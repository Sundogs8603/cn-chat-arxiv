<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JLSPCADL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Johnson-Lindenstrauss&#24341;&#29702;&#36873;&#25321;&#19968;&#20010;&#32500;&#25968;&#30340;&#36716;&#25442;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#21028;&#21035;&#24335;&#23383;&#20856;&#23398;&#20064;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13991</link><description>&lt;p&gt;
JL-&#24341;&#29702;&#25512;&#23548;&#30340;&#29992;&#20110;&#21028;&#21035;&#24335;&#23383;&#20856;&#23398;&#20064;&#30340;&#26368;&#20248;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
JL-lemma derived Optimal Projections for Discriminative Dictionary Learning. (arXiv:2308.13991v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JLSPCADL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;Johnson-Lindenstrauss&#24341;&#29702;&#36873;&#25321;&#19968;&#20010;&#32500;&#25968;&#30340;&#36716;&#25442;&#31354;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#21028;&#21035;&#24335;&#23383;&#20856;&#23398;&#20064;&#24182;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20811;&#26381;&#22312;&#20855;&#26377;&#22823;&#37327;&#31867;&#21035;&#30340;&#22823;&#32500;&#24230;&#25968;&#25454;&#20998;&#31867;&#20013;&#30340;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JLSPCADL&#30340;&#26032;&#26041;&#27861;&#12290;&#26412;&#25991;&#21033;&#29992;Johnson-Lindenstrauss(JL)&#24341;&#29702;&#65292;&#22312;&#19968;&#20010;&#36716;&#25442;&#31354;&#38388;&#20013;&#36873;&#25321;&#19968;&#20010;&#32500;&#25968;&#65292;&#21487;&#20197;&#22312;&#20854;&#20013;&#23398;&#20064;&#29992;&#20110;&#20449;&#21495;&#20998;&#31867;&#30340;&#21028;&#21035;&#24335;&#23383;&#20856;&#12290;&#19982;&#36890;&#24120;&#20351;&#29992;JL&#36827;&#34892;&#38477;&#32500;&#30340;&#38543;&#26426;&#25237;&#24433;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;Modified Supervised PC Analysis (M-SPCA)&#25512;&#23548;&#24471;&#20986;&#30340;&#25237;&#24433;&#36716;&#25442;&#30697;&#38453;&#65292;&#20854;&#32500;&#25968;&#36981;&#24490;JL&#30340;&#35268;&#23450;&#12290;JLSPCADL&#25552;&#20379;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25512;&#26029;&#36866;&#24403;&#30340;&#22833;&#30495;&#27700;&#24179;&#21644;&#30456;&#24212;&#30340;&#23383;&#20856;&#21407;&#23376;&#30340;&#36866;&#24403;&#25551;&#36848;&#38271;&#24230;(SDL)&#65292;&#20197;&#25512;&#23548;&#20986;&#19968;&#20010;&#26368;&#20248;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#20998;&#31867;&#30340;&#23383;&#20856;&#21407;&#23376;&#30340;SDL&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#38477;&#32500;&#30340;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#20174;M-SPCA&#20013;&#19968;&#27493;&#24471;&#20986;&#30340;&#25237;&#24433;&#36716;&#25442;&#30697;&#38453;&#25552;&#20379;&#20102;&#26368;&#22823;&#30340;&#29305;&#24449;-&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To overcome difficulties in classifying large dimensionality data with a large number of classes, we propose a novel approach called JLSPCADL. This paper uses the Johnson-Lindenstrauss (JL) Lemma to select the dimensionality of a transformed space in which a discriminative dictionary can be learned for signal classification. Rather than reducing dimensionality via random projections, as is often done with JL, we use a projection transformation matrix derived from Modified Supervised PC Analysis (M-SPCA) with the JL-prescribed dimension.  JLSPCADL provides a heuristic to deduce suitable distortion levels and the corresponding Suitable Description Length (SDL) of dictionary atoms to derive an optimal feature space and thus the SDL of dictionary atoms for better classification. Unlike state-of-the-art dimensionality reduction-based dictionary learning methods, a projection transformation matrix derived in a single step from M-SPCA provides maximum feature-label consistency of the transfor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#26631;&#37327;&#21270;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#30340;&#32463;&#39564;&#20248;&#21183;&#30456;&#21453;&#65292;&#26631;&#37327;&#21270;&#26412;&#36136;&#19978;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#24179;&#34913;&#20102;paren</title><link>http://arxiv.org/abs/2308.13985</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#65306;&#19968;&#20010;&#29702;&#35770;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Revisiting Scalarization in Multi-Task Learning: A Theoretical Perspective. (arXiv:2308.13985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#26631;&#37327;&#21270;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#22768;&#31216;&#30340;&#32463;&#39564;&#20248;&#21183;&#30456;&#21453;&#65292;&#26631;&#37327;&#21270;&#26412;&#36136;&#19978;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#24179;&#34913;&#20102;paren
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26631;&#37327;&#21270;&#65292;&#21363;&#36890;&#36807;&#21152;&#26435;&#24635;&#21644;&#26469;&#32452;&#21512;&#25152;&#26377;&#25439;&#22833;&#20989;&#25968;&#65292;&#33258;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#21019;&#31435;&#20197;&#26469;&#19968;&#30452;&#26159;&#25991;&#29486;&#20013;&#30340;&#40664;&#35748;&#36873;&#25321;&#12290;&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#24320;&#21457;&#19987;&#38376;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#22120;&#65288;SMTOs&#65289;&#26469;&#22788;&#29702;MTL&#20316;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;SMTOs&#26159;&#21542;&#27604;&#26631;&#37327;&#21270;&#26377;&#26681;&#26412;&#19978;&#30340;&#20248;&#21183;&#12290;&#23454;&#38469;&#19978;&#65292;&#31038;&#21306;&#20013;&#23384;&#22312;&#23545;&#27604;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#28608;&#28872;&#35752;&#35770;&#65292;&#20027;&#35201;&#26159;&#20174;&#32463;&#39564;&#35282;&#24230;&#20986;&#21457;&#12290;&#20026;&#20102;&#22238;&#31572;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#26631;&#37327;&#21270;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;MTL&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#26631;&#37327;&#21270;&#26159;&#21542;&#33021;&#22815;&#20805;&#20998;&#25506;&#32034;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#37027;&#20123;&#22768;&#31216;&#26631;&#37327;&#21270;&#20855;&#26377;&#32463;&#39564;&#20248;&#21183;&#30340;&#26368;&#36817;&#24037;&#20316;&#30456;&#21453;&#65292;&#26631;&#37327;&#21270;&#26412;&#36136;&#19978;&#26080;&#27861;&#36827;&#34892;&#20840;&#38754;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#24179;&#34913;&#20102;paren
&lt;/p&gt;
&lt;p&gt;
Linear scalarization, i.e., combining all loss functions by a weighted sum, has been the default choice in the literature of multi-task learning (MTL) since its inception. In recent years, there is a surge of interest in developing Specialized Multi-Task Optimizers (SMTOs) that treat MTL as a multi-objective optimization problem. However, it remains open whether there is a fundamental advantage of SMTOs over scalarization. In fact, heated debates exist in the community comparing these two types of algorithms, mostly from an empirical perspective. To approach the above question, in this paper, we revisit scalarization from a theoretical perspective. We focus on linear MTL models and study whether scalarization is capable of fully exploring the Pareto front. Our findings reveal that, in contrast to recent works that claimed empirical advantages of scalarization, scalarization is inherently incapable of full exploration, especially for those Pareto optimal solutions that strike the balanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26469;&#25554;&#20540;&#23665;&#21306;&#22825;&#27668;&#39044;&#25253;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#35266;&#27979;&#25968;&#25454;&#21644;&#21608;&#22260;&#24179;&#21407;&#30340;&#39044;&#25253;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#22320;&#24418;&#20013;&#25968;&#20540;&#27169;&#25311;&#31934;&#24230;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.13983</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#23665;&#21306;&#22825;&#27668;&#39044;&#25253;&#30340;&#25554;&#20540;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Interpolation of mountain weather forecasts by machine learning. (arXiv:2308.13983v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26469;&#25554;&#20540;&#23665;&#21306;&#22825;&#27668;&#39044;&#25253;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#24403;&#21069;&#35266;&#27979;&#25968;&#25454;&#21644;&#21608;&#22260;&#24179;&#21407;&#30340;&#39044;&#25253;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#22320;&#24418;&#20013;&#25968;&#20540;&#27169;&#25311;&#31934;&#24230;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#25968;&#20540;&#27169;&#25311;&#26041;&#27861;&#30340;&#36827;&#23637;&#25552;&#39640;&#20102;&#22825;&#27668;&#39044;&#25253;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#22320;&#24418;&#22914;&#23665;&#22320;&#22320;&#21306;&#65292;&#30001;&#20110;&#25968;&#20540;&#27169;&#25311;&#20013;&#20351;&#29992;&#20102;&#20960;&#20844;&#37324;&#24179;&#26041;&#30340;&#32593;&#26684;&#65292;&#31934;&#24230;&#20250;&#38477;&#20302;&#12290;&#34429;&#28982;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30452;&#25509;&#24212;&#29992;&#38590;&#20197;&#21033;&#29992;&#29289;&#29702;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#24403;&#21069;&#35266;&#27979;&#25968;&#25454;&#21644;&#21608;&#22260;&#24179;&#21407;&#30340;&#39044;&#25253;&#25968;&#25454;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#8220;&#25554;&#20540;&#8221;&#26410;&#26469;&#23665;&#21306;&#30340;&#22825;&#27668;&#12290;&#36890;&#24120;&#65292;&#22825;&#27668;&#39044;&#27979;&#20381;&#36182;&#20110;&#25968;&#20540;&#27169;&#25311;&#65292;&#22240;&#27492;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#38388;&#25509;&#34701;&#21512;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#28151;&#21512;&#26041;&#27861;&#12290;&#36824;&#30740;&#31350;&#20102;&#22312;&#38477;&#27700;&#39044;&#27979;&#20013;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in numerical simulation methods based on physical models have enhanced the accuracy of weather forecasts. However, the precision diminishes in complex terrains like mountainous regions due to the several kilometers square grid used in numerical simulations. While statistical machine learning has also significantly advanced, its direct application is difficult to utilize physics knowledge. This paper proposes a method that employs machine learning to ``interpolate'' future weather in mountainous regions using current observed data and forecast data from surrounding plains. Generally, weather prediction relies on numerical simulations, so this approach can be considered a hybrid method that indirectly merges numerical simulation and machine learning. The use of binary cross-entropy in precipitation prediction is also examined.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#36890;&#36807;&#32500;&#25345;&#22270;&#24418;&#38388;&#30340;&#32467;&#26500;&#19968;&#33268;&#24615;&#21644;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36890;&#29992;&#22270;&#24418;&#36830;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.13982</link><description>&lt;p&gt;
&#36890;&#29992;&#22270;&#24418;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Universal Graph Continual Learning. (arXiv:2308.13982v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#36890;&#36807;&#32500;&#25345;&#22270;&#24418;&#38388;&#30340;&#32467;&#26500;&#19968;&#33268;&#24615;&#21644;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36890;&#29992;&#22270;&#24418;&#36830;&#32493;&#23398;&#20064;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#36825;&#26159;&#22240;&#20026;&#20256;&#20837;&#30340;&#25968;&#25454;&#20174;&#19968;&#20010;&#22270;&#24418;&#20998;&#24067;&#36716;&#21464;&#20026;&#21478;&#19968;&#20010;&#22270;&#24418;&#20998;&#24067;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#35299;&#20915;&#20102;&#22270;&#24418;&#36830;&#32493;&#23398;&#20064;&#30340;&#19968;&#31181;&#24773;&#20917;&#65292;&#27604;&#22914;&#22686;&#37327;&#33410;&#28857;&#20998;&#31867;&#65292;&#32780;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#28857;&#21487;&#20197;&#26159;&#19968;&#20010;&#33410;&#28857;&#25110;&#19968;&#20010;&#22270;&#24418;&#65292;&#24182;&#19988;&#20219;&#21153;&#20174;&#33410;&#28857;&#21040;&#22270;&#24418;&#20998;&#31867;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#31181;&#36890;&#29992;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32500;&#25345;&#22270;&#24418;&#38388;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#19968;&#33268;&#24615;&#26469;&#20445;&#25345;&#23545;&#36807;&#21435;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#25968;&#25454;&#38598;&#20013;&#23558;&#20854;&#19982;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#23545;&#27604;&#65292;&#22312;&#24179;&#22343;&#24615;&#33021;&#21644;&#36951;&#24536;&#24615;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address catastrophic forgetting issues in graph learning as incoming data transits from one to another graph distribution. Whereas prior studies primarily tackle one setting of graph continual learning such as incremental node classification, we focus on a universal approach wherein each data point in a task can be a node or a graph, and the task varies from node to graph classification. We propose a novel method that enables graph neural networks to excel in this universal setting. Our approach perseveres knowledge about past tasks through a rehearsal mechanism that maintains local and global structure consistency across the graphs. We benchmark our method against various continual learning baselines in real-world graph datasets and achieve significant improvement in average performance and forgetting across tasks.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22270;&#19978;&#22522;&#20110;QUBO&#20844;&#24335;&#30340;&#26368;&#22823;&#20999;&#21106;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#21644;&#21704;&#23494;&#39039;&#20989;&#25968;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#20844;&#24335;&#24418;&#24335;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;...</title><link>http://arxiv.org/abs/2308.13978</link><description>&lt;p&gt;
&#29702;&#35299;&#22522;&#20110;QUBO&#30340;&#21704;&#23494;&#39039;&#20989;&#25968;&#22312;&#22270;&#19978;&#30340;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#20351;&#29992;&#65306;&#20197;&#26368;&#22823;&#20999;&#21106;&#38382;&#39064;&#20026;&#20363;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Understanding the Usage of QUBO-based Hamiltonian Function in Combinatorial Optimization over Graphs: A Discussion Using Max Cut (MC) Problem. (arXiv:2308.13978v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13978
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22270;&#19978;&#22522;&#20110;QUBO&#20844;&#24335;&#30340;&#26368;&#22823;&#20999;&#21106;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#21644;&#21704;&#23494;&#39039;&#20989;&#25968;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#20844;&#24335;&#24418;&#24335;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#65288;QUBO&#65289;&#26159;&#19968;&#31181;&#24191;&#20041;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#21508;&#31181;NP&#22256;&#38590;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#24314;&#27169;&#20026;&#20108;&#36827;&#21046;&#21464;&#37327;&#30340;&#24418;&#24335;&#12290;&#21704;&#23494;&#39039;&#20989;&#25968;&#32463;&#24120;&#29992;&#20110;&#24418;&#25104;QUBO&#38382;&#39064;&#65292;&#20854;&#20013;&#23427;&#22312;&#20248;&#21270;&#30340;&#19978;&#19979;&#25991;&#20013;&#34987;&#29992;&#20316;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33539;&#24335;&#21644;&#21704;&#23494;&#39039;&#20989;&#25968;&#35299;&#20915;QUBO&#20844;&#24335;&#20013;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#26550;&#26500;&#22312;&#33410;&#28857;&#20043;&#38388;&#20256;&#36882;&#20449;&#24687;&#12290;&#25105;&#20204;&#20027;&#35201;&#30740;&#31350;&#20102;&#19977;&#31181;&#20844;&#24335;&#65292;Monty-Carlo Tree Search with GNN-based RL&#65288;MCTS-GNN&#65289;&#12289;DQN with GNN-based RL&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#30340;&#36890;&#29992;GNN&#65288;GRL&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Quadratic Unconstrained Binary Optimization (QUBO) is a generic technique to model various NP-hard combinatorial optimization problems in the form of binary variables. The Hamiltonian function is often used to formulate QUBO problems where it is used as the objective function in the context of optimization. In this study, we investigate how reinforcement learning-based (RL) paradigms with the presence of the Hamiltonian function can address combinatorial optimization problems over graphs in QUBO formulations. We use Graph Neural Network (GNN) as the message-passing architecture to convey the information among the nodes. We have centered our discussion on QUBO formulated Max-Cut problem but the intuitions can be extended to any QUBO supported canonical NP-Hard combinatorial optimization problems. We mainly investigate three formulations, Monty-Carlo Tree Search with GNN-based RL (MCTS-GNN), DQN with GNN-based RL, and a generic GNN with attention-based RL (GRL). Our findings state that i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#22312;&#36825;&#31181;&#35266;&#23519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#65288;DeCA&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;</title><link>http://arxiv.org/abs/2308.13976</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Label Denoising through Cross-Model Agreement. (arXiv:2308.13976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#26631;&#31614;&#21435;&#22122;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35266;&#23519;&#21457;&#29616;&#65292;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#22312;&#36825;&#31181;&#35266;&#23519;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#30340;&#26041;&#27861;&#65288;DeCA&#65289;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20174;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#23398;&#20064;&#26159;&#38750;&#24120;&#24120;&#35265;&#30340;&#12290;&#35760;&#24518;&#36825;&#20123;&#26377;&#22122;&#22768;&#30340;&#26631;&#31614;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#23398;&#20064;&#65292;&#20174;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26377;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#40065;&#26834;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#24178;&#20928;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#30456;&#23545;&#30456;&#20284;&#65292;&#32780;&#22312;&#26377;&#22122;&#22768;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#21464;&#21270;&#26356;&#22823;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36328;&#27169;&#22411;&#19968;&#33268;&#24615;&#36827;&#34892;&#21435;&#22122;&#65288;DeCA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#26368;&#23567;&#21270;&#30001;&#20004;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#25968;&#25454;&#35266;&#27979;&#30340;&#20284;&#28982;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;DeCA&#26041;&#27861;&#24212;&#29992;&#20110;&#20108;&#36827;&#21046;&#26631;&#31614;&#24773;&#26223;&#21644;&#22810;&#26631;&#31614;&#24773;&#26223;&#12290;&#23545;&#20110;&#20108;&#36827;&#21046;&#26631;&#31614;&#24773;&#26223;&#65292;&#25105;&#20204;&#36873;&#25321;&#38544;&#24335;&#21453;&#39304;&#25512;&#33616;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from corrupted labels is very common in real-world machine-learning applications. Memorizing such noisy labels could affect the learning of the model, leading to sub-optimal performances. In this work, we propose a novel framework to learn robust machine-learning models from noisy labels. Through an empirical study, we find that different models make relatively similar predictions on clean examples, while the predictions on noisy examples vary much more across different models. Motivated by this observation, we propose \em denoising with cross-model agreement \em (DeCA) which aims to minimize the KL-divergence between the true label distributions parameterized by two machine learning models while maximizing the likelihood of data observation. We employ the proposed DeCA on both the binary label scenario and the multiple label scenario. For the binary label scenario, we select implicit feedback recommendation as the downstream task and conduct experiments with four state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#33258;&#36866;&#24212;&#32852;&#37030;&#20803;&#23398;&#20064;&#65288;FAM&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#20010;&#21035;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;&#21644;&#38544;&#31169;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#39046;&#22495;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2308.13970</link><description>&lt;p&gt;
FAM&#65306;&#24555;&#36895;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FAM: fast adaptive meta-learning. (arXiv:2308.13970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#33258;&#36866;&#24212;&#32852;&#37030;&#20803;&#23398;&#20064;&#65288;FAM&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#20010;&#21035;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;&#21644;&#38544;&#31169;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#39046;&#22495;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#33258;&#36866;&#24212;&#32852;&#37030;&#20803;&#23398;&#20064;&#65288;FAM&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#20840;&#23616;&#27169;&#22411;&#65292;&#28982;&#21518;&#21487;&#20197;&#22312;&#20010;&#21035;&#23458;&#25143;&#31471;&#19978;&#20010;&#24615;&#21270;&#12290;&#32852;&#37030;&#23398;&#20064;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#25968;&#25454;&#12290;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#23458;&#25143;&#31471;&#30001;&#20110;&#25968;&#25454;&#19981;&#36275;&#25110;&#25968;&#25454;&#22810;&#26679;&#24615;&#23548;&#33268;&#23398;&#20064;&#21463;&#21040;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;&#26102;&#65292;&#23398;&#20064;&#20250;&#21463;&#21040;&#22256;&#25200;&#12290;&#26377;&#24517;&#35201;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#20351;&#29992;&#23458;&#25143;&#31471;&#29305;&#23450;&#20449;&#24687;&#36827;&#34892;&#33258;&#36866;&#24212;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#23458;&#25143;&#31471;&#19978;&#21019;&#24314;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;MRI&#25968;&#25454;&#23384;&#22312;&#36825;&#20010;&#38382;&#39064;&#65292;&#31532;&#19968;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#25361;&#25112;&#65292;&#22312;&#26576;&#20010;&#22320;&#28857;&#30340;&#26412;&#22320;&#25968;&#25454;&#36275;&#20197;&#35757;&#32451;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#31532;&#20108;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#26377;&#25968;&#25454;&#20849;&#20139;&#38480;&#21046;&#65292;&#31532;&#19977;&#65292;&#30001;&#20110;&#23458;&#25143;&#31471;&#31449;&#28857;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#21464;&#65292;&#38656;&#35201;&#23545;&#23398;&#20064;&#30340;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a fast adaptive federated meta-learning (FAM) framework for collaboratively learning a single global model, which can then be personalized locally on individual clients. Federated learning enables multiple clients to collaborate to train a model without sharing data. Clients with insufficient data or data diversity participate in federated learning to learn a model with superior performance. Nonetheless, learning suffers when data distributions diverge. There is a need to learn a global model that can be adapted using client's specific information to create personalised models on clients is required. MRI data suffers from this problem, wherein, one, due to data acquisition challenges, local data at a site is sufficient for training an accurate model and two, there is a restriction of data sharing due to privacy concerns and three, there is a need for personalization of a learnt shared global model on account of domain shift across client sites. The global model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13969</link><description>&lt;p&gt;
&#27880;&#37325;&#27880;&#24847;&#21147;&#65306;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Fixating on Attention: Integrating Human Eye Tracking into Vision Transformers. (arXiv:2308.13969v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22522;&#20110;Transformer&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#22810;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20851;&#38190;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#22270;&#20687;&#35299;&#37322;&#21644;&#33258;&#21160;&#39550;&#39542;&#65292;&#20173;&#28982;&#38656;&#35201;&#20381;&#36182;&#20154;&#31867;&#21028;&#26029;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#31867;&#35270;&#35273;&#36755;&#20837;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#30524;&#21160;&#20202;&#25910;&#38598;&#21040;&#30340;&#27880;&#35270;&#28857;&#65292;&#38598;&#25104;&#21040;Transformer&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#21644;Vision Transformer&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#27880;&#35270;&#21306;&#22495;&#22312;&#24038;&#21491;&#39550;&#39542;&#20915;&#31574;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#27880;&#35270;&#22270;&#21644;ViT&#27880;&#24847;&#21147;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21333;&#20010;&#22836;&#37096;&#21644;&#23618;&#20043;&#38388;&#30340;&#37325;&#21472;&#21160;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#37325;&#21472;&#21160;&#24577;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#30340;&#20462;&#21098;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#39550;&#39542;&#22330;&#26223;&#20449;&#24687;&#19982;&#27880;&#35270;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#37319;&#29992;&#8220;&#32852;&#21512;&#31354;&#38388;-&#27880;&#35270;&#8221;&#65288;JSF&#65289;&#30340;&#27880;&#24847;&#21147;&#35774;&#32622;&#12290;&#26368;&#21518;
&lt;/p&gt;
&lt;p&gt;
Modern transformer-based models designed for computer vision have outperformed humans across a spectrum of visual tasks. However, critical tasks, such as medical image interpretation or autonomous driving, still require reliance on human judgments. This work demonstrates how human visual input, specifically fixations collected from an eye-tracking device, can be integrated into transformer models to improve accuracy across multiple driving situations and datasets. First, we establish the significance of fixation regions in left-right driving decisions, as observed in both human subjects and a Vision Transformer (ViT). By comparing the similarity between human fixation maps and ViT attention weights, we reveal the dynamics of overlap across individual heads and layers. This overlap is exploited for model pruning without compromising accuracy. Thereafter, we incorporate information from the driving scene with fixation data, employing a "joint space-fixation" (JSF) attention setup. Lastly
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#32593;&#32476;&#65288;DA-Net&#65289;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;DA-Net&#33021;&#22815;&#21516;&#26102;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#20174;&#32780;&#35782;&#21035;&#20851;&#38190;&#30340;&#23616;&#37096;&#24207;&#21015;&#29255;&#27573;&#24182;&#24314;&#31435;&#20840;&#23616;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.13968</link><description>&lt;p&gt;
&#24102;&#26377;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series classification with dual attention network. (arXiv:2308.13968v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13968
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#32593;&#32476;&#65288;DA-Net&#65289;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;DA-Net&#33021;&#22815;&#21516;&#26102;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#20174;&#32780;&#35782;&#21035;&#20851;&#38190;&#30340;&#23616;&#37096;&#24207;&#21015;&#29255;&#27573;&#24182;&#24314;&#31435;&#20840;&#23616;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#30456;&#20851;&#30340;&#39046;&#22495;&#12290;&#30446;&#21069;&#30340;&#25216;&#26415;&#38598;&#20013;&#20110;&#35782;&#21035;&#23616;&#37096;&#37325;&#35201;&#30340;&#24207;&#21015;&#29255;&#27573;&#25110;&#32773;&#24314;&#31435;&#20840;&#23616;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20102;&#26469;&#33258;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#30340;&#21512;&#24182;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#32476;&#65288;DA-Net&#65289;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#20197;&#25552;&#21462;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#12290;DA-Net&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#23618;&#65292;&#21363;Squeeze-Excitation Window Attention&#65288;SEWA&#65289;&#23618;&#21644;Sparse Self-Attention within Windows&#65288;SSAW&#65289;&#23618;&#12290;DA-Net&#33021;&#22815;&#22522;&#20110;&#36825;&#20004;&#20010;&#25193;&#23637;&#23618;&#25552;&#21462;&#24517;&#35201;&#30340;&#23616;&#37096;&#24207;&#21015;&#29255;&#27573;&#65292;&#24314;&#31435;&#20840;&#23616;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the topics in machine learning that is becoming more and more relevant is multivariate time series classification. Current techniques concentrate on identifying the local important sequence segments or establishing the global long-range dependencies. They frequently disregard the merged data from both global and local features, though. Using dual attention, we explore a novel network (DA-Net) in this research to extract local and global features for multivariate time series classification. The two distinct layers that make up DA-Net are the Squeeze-Excitation Window Attention (SEWA) layer and the Sparse Self-Attention within Windows (SSAW) layer. DA- Net can mine essential local sequence fragments that are necessary for establishing global long-range dependencies based on the two expanded layers.
&lt;/p&gt;</description></item><item><title>&#31232;&#30095;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#21487;&#20197;&#29992;&#20110;&#22238;&#24402;&#12289;&#20998;&#31867;&#12289;&#22270;&#27169;&#22411;&#36873;&#25321;&#12289;&#31232;&#30095;&#20272;&#35745;&#21644;&#32500;&#24230;&#38477;&#20302;&#31561;&#20219;&#21153;&#65292;&#24182;&#22312;&#21457;&#29616;&#39044;&#27979;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.13960</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sparse Models for Machine Learning. (arXiv:2308.13960v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13960
&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#21487;&#20197;&#29992;&#20110;&#22238;&#24402;&#12289;&#20998;&#31867;&#12289;&#22270;&#27169;&#22411;&#36873;&#25321;&#12289;&#31232;&#30095;&#20272;&#35745;&#21644;&#32500;&#24230;&#38477;&#20302;&#31561;&#20219;&#21153;&#65292;&#24182;&#22312;&#21457;&#29616;&#39044;&#27979;&#27169;&#24335;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24314;&#27169;&#26159;&#25429;&#25417;&#31616;&#27905;&#21407;&#29702;&#30340;&#26126;&#26174;&#34920;&#29616;&#65292;&#31232;&#30095;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#20110;&#32479;&#35745;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#20449;&#24687;&#31185;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#12289;&#35745;&#31639;&#25968;&#23398;&#31561;&#39046;&#22495;&#12290;&#22312;&#32479;&#35745;&#23398;&#20013;&#65292;&#31232;&#30095;&#24314;&#27169;&#30340;&#35768;&#22810;&#24212;&#29992;&#21253;&#25324;&#22238;&#24402;&#12289;&#20998;&#31867;&#20219;&#21153;&#12289;&#22270;&#27169;&#22411;&#36873;&#25321;&#12289;&#31232;&#30095;M-&#20272;&#35745;&#21644;&#31232;&#30095;&#32500;&#24230;&#38477;&#20302;&#12290;&#23427;&#22312;&#35768;&#22810;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#29305;&#21035;&#26377;&#25928;&#65292;&#20854;&#20013;&#20027;&#35201;&#30446;&#26631;&#26159;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#39044;&#27979;&#27169;&#24335;&#65292;&#20197;&#22686;&#24378;&#25105;&#20204;&#23545;&#24213;&#23618;&#29289;&#29702;&#12289;&#29983;&#29289;&#21644;&#20854;&#20182;&#33258;&#28982;&#36807;&#31243;&#30340;&#29702;&#35299;&#21644;&#25511;&#21046;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26500;&#24314;&#20934;&#30830;&#30340;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#12290;&#24120;&#35265;&#20363;&#23376;&#21253;&#25324;&#22312;&#29983;&#29289;&#36807;&#31243;&#20013;&#36873;&#25321;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22312;&#22522;&#20110;fMRI&#25968;&#25454;&#30340;&#22823;&#33041;&#29366;&#24577;&#21644;&#36807;&#31243;&#30340;&#39044;&#27979;&#20013;&#25214;&#21040;&#30456;&#20851;&#30340;&#33041;&#27963;&#21160;&#20301;&#32622;&#65292;&#20197;&#21450;&#35782;&#21035;&#26368;&#33021;&#35299;&#37322;&#31471;&#21040;&#31471;&#24615;&#33021;&#30340;&#32593;&#32476;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sparse modeling is an evident manifestation capturing the parsimony principle just described, and sparse models are widespread in statistics, physics, information sciences, neuroscience, computational mathematics, and so on. In statistics the many applications of sparse modeling span regression, classification tasks, graphical model selection, sparse M-estimators and sparse dimensionality reduction. It is also particularly effective in many statistical and machine learning areas where the primary goal is to discover predictive patterns from data which would enhance our understanding and control of underlying physical, biological, and other natural processes, beyond just building accurate outcome black-box predictors. Common examples include selecting biomarkers in biological procedures, finding relevant brain activity locations which are predictive about brain states and processes based on fMRI data, and identifying network bottlenecks best explaining end-to-end performance. Moreov
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#27169;&#22359;&#21270;&#26435;&#37325;&#21644;&#39046;&#22495;&#36801;&#31227;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#26435;&#37325;&#25513;&#30721;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#20445;&#25345;&#28304;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#20801;&#35768;&#39640;&#25928;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13957</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#26435;&#37325;&#25513;&#30721;&#29992;&#20110;&#39046;&#22495;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Differentiable Weight Masks for Domain Transfer. (arXiv:2308.13957v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#27169;&#22359;&#21270;&#26435;&#37325;&#21644;&#39046;&#22495;&#36801;&#31227;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#26435;&#37325;&#25513;&#30721;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#20445;&#25345;&#28304;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#20801;&#35768;&#39640;&#25928;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#26080;&#27861;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#20445;&#30041;&#22810;&#20010;&#20449;&#24687;&#28304;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#20010;&#22312;&#28304;&#20219;&#21153;&#19978;&#35757;&#32451;&#36807;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#20445;&#25345;&#20854;&#22312;&#28304;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#23558;&#20854;&#37325;&#26032;&#35757;&#32451;&#21040;&#19968;&#20010;&#30456;&#20284;&#20294;&#19981;&#21516;&#30340;&#30446;&#26631;&#20219;&#21153;&#19978;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#32593;&#32476;&#26435;&#37325;&#30340;&#27169;&#22359;&#21270;&#65292;&#20197;&#23450;&#20301;&#21644;&#30830;&#23450;&#23545;&#20110;&#35302;&#21457;&#32473;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#26435;&#37325;&#38598;&#21512;&#12290;&#19968;&#20123;&#24037;&#20316;&#30740;&#31350;&#20102;&#36890;&#36807;&#23398;&#20064;&#21644;&#20998;&#26512;&#26435;&#37325;&#25513;&#30721;&#24341;&#20837;&#30340;&#32593;&#32476;&#26435;&#37325;&#30340;&#27169;&#22359;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#39046;&#22495;&#32467;&#21512;&#36215;&#26469;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#26435;&#37325;&#25513;&#30721;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#23427;&#20204;&#22312;&#32531;&#35299;&#28304;&#20219;&#21153;&#30340;&#8220;&#36951;&#24536;&#8221;&#21516;&#26102;&#20801;&#35768;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#25513;&#30721;&#25216;&#26415;&#22312;&#20445;&#30041;&#28304;&#20219;&#21153;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the major drawbacks of deep learning models for computer vision has been their inability to retain multiple sources of information in a modular fashion. For instance, given a network that has been trained on a source task, we would like to re-train this network on a similar, yet different, target task while maintaining its performance on the source task. Simultaneously, researchers have extensively studied modularization of network weights to localize and identify the set of weights culpable for eliciting the observed performance on a given task. One set of works studies the modularization induced in the weights of a neural network by learning and analysing weight masks. In this work, we combine these fields to study three such weight masking methods and analyse their ability to mitigate "forgetting'' on the source task while also allowing for efficient finetuning on the target task. We find that different masking techniques have trade-offs in retaining knowledge in the source t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#26080;&#33455;&#29255;RFID&#20256;&#24863;&#22120;&#26631;&#31614;&#30340;&#40065;&#26834;&#26816;&#27979;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#39318;&#27425;&#25253;&#21578;&#20102;&#19981;&#21516;&#26631;&#31614;&#34920;&#38754;&#24418;&#29366;&#21464;&#21270;&#23545;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.13944</link><description>&lt;p&gt;
&#29992;&#20110;&#26080;&#33455;&#29255;RFID&#20256;&#24863;&#22120;&#26631;&#31614;&#30340;&#28145;&#24230;&#23398;&#20064;&#36741;&#21161;&#30340;&#40065;&#26834;&#26816;&#27979;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Deep learning assisted robust detection techniques for a chipless RFID sensor tag. (arXiv:2308.13944v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#26080;&#33455;&#29255;RFID&#20256;&#24863;&#22120;&#26631;&#31614;&#30340;&#40065;&#26834;&#26816;&#27979;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#35774;&#35745;&#21644;&#23454;&#29616;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#39318;&#27425;&#25253;&#21578;&#20102;&#19981;&#21516;&#26631;&#31614;&#34920;&#38754;&#24418;&#29366;&#21464;&#21270;&#23545;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#26080;&#33455;&#29255;RFID&#20256;&#24863;&#22120;&#26631;&#31614;&#20013;&#31283;&#23450;&#35835;&#21462;&#35782;&#21035;&#21644;&#20256;&#24863;&#25968;&#25454;&#12290;&#39318;&#27425;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#22238;&#24402;&#24314;&#27169;&#25216;&#26415;&#24212;&#29992;&#20110;&#30001;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#27979;&#37327;&#30340;&#23450;&#21046;3&#20301;&#26080;&#33455;&#29255;RFID&#20256;&#24863;&#22120;&#26631;&#31614;&#30340;&#38647;&#36798;&#25955;&#23556;&#25130;&#38754;&#65288;RCS&#65289;&#25968;&#25454;&#38598;&#12290;&#35813;&#26426;&#22120;&#20154;&#31995;&#32479;&#37319;&#29992;ur16e&#34892;&#19994;&#26631;&#20934;&#26426;&#22120;&#20154;&#65292;&#20351;&#29992;&#39318;&#21019;&#30340;&#33258;&#21160;&#25968;&#25454;&#37319;&#38598;&#26041;&#27861;&#23454;&#29616;&#12290;&#20351;&#29992;&#33258;&#21160;&#31995;&#32479;&#25910;&#38598;&#30340;9600&#20010;&#30005;&#30913;&#65288;EM&#65289;RCS&#31614;&#21517;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#22235;&#31181;ML&#27169;&#22411;&#21644;&#22235;&#31181;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;1D CNN&#65289;&#26550;&#26500;&#12290;&#39318;&#27425;&#25253;&#36947;&#20102;&#29992;&#20110;&#20351;&#29992;ML/DL&#27169;&#22411;&#36827;&#34892;&#35782;&#21035;&#65288;ID&#65289;&#21644;&#20256;&#24863;&#25968;&#25454;&#30340;&#40065;&#26834;&#26816;&#27979;&#30340;&#31471;&#21040;&#31471;&#35774;&#35745;&#21644;&#23454;&#29616;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#39318;&#27425;&#25253;&#36947;&#20102;&#19981;&#21516;&#26631;&#31614;&#34920;&#38754;&#24418;&#29366;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new approach for robust reading of identification and sensor data from chipless RFID sensor tags. For the first time, Machine Learning (ML) and Deep Learning (DL) regression modelling techniques are applied to a dataset of measured Radar Cross Section (RCS) data that has been derived from large-scale robotic measurements of custom-designed, 3-bit chipless RFID sensor tags. The robotic system is implemented using the first-of-its-kind automated data acquisition method using an ur16e industry-standard robot. A large data set of 9,600 Electromagnetic (EM) RCS signatures collected using the automated system is used to train and validate four ML models and four 1-dimensional Convolutional Neural Network (1D CNN) architectures. For the first time, we report an end-to-end design and implementation methodology for robust detection of identification (ID) and sensing data using ML/DL models. Also, we report, for the first time, the effect of varying tag surface shapes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#24211;&#65292;&#21253;&#21547;&#20102;&#36830;&#32493;&#30340;&#21457;&#22768;&#22120;&#23448;&#21644;&#22768;&#23398;&#35821;&#38899;&#25968;&#25454;&#65292;&#29992;&#36229;&#22768;&#22270;&#20687;&#21644;&#22768;&#38899;&#25968;&#25454;&#26469;&#30740;&#31350;&#35328;&#35821;&#29983;&#25104;&#36807;&#31243;&#20013;&#33292;&#22836;&#19978;&#36718;&#24275;&#30340;&#21487;&#35270;&#21270;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#26159;&#30001;&#21733;&#20262;&#27604;&#20122;&#22307;&#22374;&#24503;&#21306;&#30340;17&#21517;&#24180;&#36731;&#34987;&#35797;&#23436;&#25104;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.13941</link><description>&lt;p&gt;
&#19968;&#20010;&#21253;&#21547;&#22768;&#36947;&#21160;&#24577;&#36229;&#22768;&#22270;&#20687;&#24207;&#21015;&#30340;&#23567;&#22411;&#35789;&#27719;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
A small vocabulary database of ultrasound image sequences of vocal tract dynamics. (arXiv:2308.13941v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#24211;&#65292;&#21253;&#21547;&#20102;&#36830;&#32493;&#30340;&#21457;&#22768;&#22120;&#23448;&#21644;&#22768;&#23398;&#35821;&#38899;&#25968;&#25454;&#65292;&#29992;&#36229;&#22768;&#22270;&#20687;&#21644;&#22768;&#38899;&#25968;&#25454;&#26469;&#30740;&#31350;&#35328;&#35821;&#29983;&#25104;&#36807;&#31243;&#20013;&#33292;&#22836;&#19978;&#36718;&#24275;&#30340;&#21487;&#35270;&#21270;&#12290;&#36825;&#20010;&#25968;&#25454;&#24211;&#26159;&#30001;&#21733;&#20262;&#27604;&#20122;&#22307;&#22374;&#24503;&#21306;&#30340;17&#21517;&#24180;&#36731;&#34987;&#35797;&#23436;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#36830;&#32493;&#30340;&#21457;&#22768;&#22120;&#23448;&#21644;&#22768;&#23398;&#35821;&#38899;&#25968;&#25454;&#30340;&#26032;&#25968;&#25454;&#24211;&#12290;&#21457;&#22768;&#22120;&#23448;&#25968;&#25454;&#23545;&#24212;&#20110;&#22768;&#36947;&#21160;&#24577;&#30340;&#36229;&#22768;&#35270;&#39057;&#65292;&#21487;&#20197;&#22312;&#35328;&#35821;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#35270;&#21270;&#33292;&#22836;&#19978;&#36718;&#24275;&#12290;&#22768;&#23398;&#25968;&#25454;&#30001;&#23450;&#21521;&#24515;&#33039;&#40614;&#20811;&#39118;&#33719;&#21462;&#65292;&#21253;&#25324;30&#20010;&#30701;&#21477;&#23376;&#12290;&#35813;&#25968;&#25454;&#24211;&#21253;&#25324;&#26469;&#33258;&#21733;&#20262;&#27604;&#20122;&#22307;&#22374;&#24503;&#21306;&#30340;17&#21517;&#24180;&#36731;&#34987;&#35797;&#65288;&#30007;&#24615;8&#21517;&#65292;&#22899;&#24615;9&#21517;&#65289;&#65292;&#20182;&#20204;&#25253;&#21578;&#27809;&#26377;&#20219;&#20309;&#35328;&#35821;&#30149;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new database consisting of concurrent articulatory and acoustic speech data. The articulatory data correspond to ultrasound videos of the vocal tract dynamics, which allow the visualization of the tongue upper contour during the speech production process. Acoustic data is composed of 30 short sentences that were acquired by a directional cardioid microphone. This database includes data from 17 young subjects (8 male and 9 female) from the Santander region in Colombia, who reported not having any speech pathology.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36816;&#36755;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#36827;&#34892;&#38745;&#24577;&#27169;&#22411;&#21442;&#25968;&#30340;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#21253;&#25324;&#24178;&#25200;&#21442;&#25968;&#30340;&#22797;&#26434;&#22122;&#22768;&#27169;&#22411;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#20165;&#20316;&#20026;&#40657;&#31665;&#30340;&#27491;&#21521;&#27169;&#22411;&#12290;&#25968;&#20540;&#24212;&#29992;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#30005;&#23548;&#29575;&#27979;&#37327;&#26469;&#34920;&#24449;&#20912;&#21402;&#24230;&#30340;&#24773;&#20917;&#19979;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.13940</link><description>&lt;p&gt;
&#20351;&#29992;&#36816;&#36755;&#26041;&#27861;&#36827;&#34892;&#39034;&#24207;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
A transport approach to sequential simulation-based inference. (arXiv:2308.13940v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36816;&#36755;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#36827;&#34892;&#38745;&#24577;&#27169;&#22411;&#21442;&#25968;&#30340;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#21253;&#25324;&#24178;&#25200;&#21442;&#25968;&#30340;&#22797;&#26434;&#22122;&#22768;&#27169;&#22411;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#20165;&#20316;&#20026;&#40657;&#31665;&#30340;&#27491;&#21521;&#27169;&#22411;&#12290;&#25968;&#20540;&#24212;&#29992;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20351;&#29992;&#30005;&#23548;&#29575;&#27979;&#37327;&#26469;&#34920;&#24449;&#20912;&#21402;&#24230;&#30340;&#24773;&#20917;&#19979;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36816;&#36755;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#36827;&#34892;&#38745;&#24577;&#27169;&#22411;&#21442;&#25968;&#30340;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#35813;&#31574;&#30053;&#22522;&#20110;&#20174;&#21442;&#25968;&#21644;&#25968;&#25454;&#30340;&#32852;&#21512;&#20998;&#24067;&#20013;&#25552;&#21462;&#26465;&#20214;&#20998;&#24067;&#65292;&#36890;&#36807;&#20272;&#35745;&#32467;&#26500;&#21270;&#65288;&#20363;&#22914;&#65292;&#22359;&#19977;&#35282;&#24418;&#65289;&#36816;&#36755;&#26144;&#23556;&#26469;&#23454;&#29616;&#12290;&#36825;&#20026;&#20284;&#28982;&#20989;&#25968;&#21450;&#20854;&#26799;&#24230;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#36825;&#20801;&#35768;&#22312;&#27169;&#22411;&#26080;&#20851;&#12289;&#22312;&#32447;&#38454;&#27573;&#36890;&#36807;&#36816;&#36755;&#26144;&#23556;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#21518;&#39564;&#23494;&#24230;&#34920;&#24449;&#12290;&#36825;&#20010;&#26694;&#26550;&#38750;&#24120;&#36866;&#29992;&#20110;&#22797;&#26434;&#22122;&#22768;&#27169;&#22411;&#65288;&#21253;&#25324;&#24178;&#25200;&#21442;&#25968;&#65289;&#21644;&#20165;&#20316;&#20026;&#40657;&#31665;&#30340;&#27491;&#21521;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#30005;&#23548;&#29575;&#27979;&#37327;&#26469;&#34920;&#24449;&#20912;&#21402;&#24230;&#30340;&#24773;&#20917;&#19979;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#25968;&#20540;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new transport-based approach to efficiently perform sequential Bayesian inference of static model parameters. The strategy is based on the extraction of conditional distribution from the joint distribution of parameters and data, via the estimation of structured (e.g., block triangular) transport maps. This gives explicit surrogate models for the likelihood functions and their gradients. This allow gradient-based characterizations of posterior density via transport maps in a model-free, online phase. This framework is well suited for parameter estimation in case of complex noise models including nuisance parameters and when the forward model is only known as a black box. The numerical application of this method is performed in the context of characterization of ice thickness with conductivity measurements.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23556;&#39057;&#30340;&#26080;&#20154;&#26426;&#26816;&#27979;&#19982;&#35782;&#21035;&#30340;&#20108;&#32500;&#28145;&#24230;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#25552;&#21462;&#20102;&#21253;&#21547;&#26102;&#22495;&#21644;&#39057;&#22495;&#20449;&#24687;&#30340;&#20108;&#32500;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#25193;&#23637;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#24179;&#34913;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13906</link><description>&lt;p&gt;
&#22522;&#20110;&#23556;&#39057;&#30340;&#26080;&#20154;&#26426;&#26816;&#27979;&#19982;&#35782;&#21035;&#30340;&#20108;&#32500;&#28145;&#24230;&#32593;&#32476;&#29992;&#20110;&#23433;&#20840;&#35206;&#30422;&#24310;&#20280;
&lt;/p&gt;
&lt;p&gt;
A Two-Dimensional Deep Network for RF-based Drone Detection and Identification Towards Secure Coverage Extension. (arXiv:2308.13906v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13906
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#23556;&#39057;&#30340;&#26080;&#20154;&#26426;&#26816;&#27979;&#19982;&#35782;&#21035;&#30340;&#20108;&#32500;&#28145;&#24230;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#25552;&#21462;&#20102;&#21253;&#21547;&#26102;&#22495;&#21644;&#39057;&#22495;&#20449;&#24687;&#30340;&#20108;&#32500;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#25193;&#23637;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20855;&#22791;&#33391;&#22909;&#30340;&#24179;&#34913;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26080;&#20154;&#26426;&#22312;&#20154;&#31867;&#29983;&#27963;&#20013;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#23433;&#20840;&#25285;&#24551;&#65292;&#21253;&#25324;&#26410;&#32463;&#25480;&#26435;&#30340;&#35775;&#38382;&#21644;&#25511;&#21046;&#65292;&#20197;&#21450;&#19982;&#26377;&#20154;&#39134;&#26426;&#30340;&#30896;&#25758;&#21644;&#24178;&#25200;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;&#33021;&#22815;&#20934;&#30830;&#22320;&#26816;&#27979;&#21644;&#35782;&#21035;&#19981;&#21516;&#30340;&#26080;&#20154;&#26426;&#23545;&#20110;&#35206;&#30422;&#24310;&#20280;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#24110;&#21161;&#19979;&#65292;&#23556;&#39057;&#65288;RF&#65289;&#26816;&#27979;&#21487;&#20197;&#26681;&#25454;&#37319;&#26679;&#30340;&#26080;&#20154;&#26426;&#20449;&#21495;&#35782;&#21035;&#26080;&#20154;&#26426;&#30340;&#31867;&#22411;&#21644;&#39134;&#34892;&#27169;&#24335;&#12290;&#26412;&#25991;&#39318;&#20808;&#21033;&#29992;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;STFT&#65289;&#20174;&#21407;&#22987;&#20449;&#21495;&#20013;&#25552;&#21462;&#21253;&#21547;&#26102;&#22495;&#21644;&#39057;&#22495;&#20449;&#24687;&#30340;&#20108;&#32500;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#37319;&#29992;ResNet&#32467;&#26500;&#26500;&#24314;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23454;&#29616;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;ResNet-STFT&#22312;&#25193;&#23637;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24179;&#34913;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As drones become increasingly prevalent in human life, they also raises security concerns such as unauthorized access and control, as well as collisions and interference with manned aircraft. Therefore, ensuring the ability to accurately detect and identify between different drones holds significant implications for coverage extension. Assisted by machine learning, radio frequency (RF) detection can recognize the type and flight mode of drones based on the sampled drone signals. In this paper, we first utilize Short-Time Fourier. Transform (STFT) to extract two-dimensional features from the raw signals, which contain both time-domain and frequency-domain information. Then, we employ a Convolutional Neural Network (CNN) built with ResNet structure to achieve multi-class classifications. Our experimental results show that the proposed ResNet-STFT can achieve higher accuracy and faster convergence on the extended dataset. Additionally, it exhibits balanced performance compared to other ba
&lt;/p&gt;</description></item><item><title>LMSanitator&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#28040;&#38500;Transformer&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;LMSanitator&#36890;&#36807;&#36870;&#36716;&#39044;&#23450;&#20041;&#30340;&#25915;&#20987;&#21521;&#37327;&#32780;&#19981;&#26159;&#35302;&#21457;&#22120;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#21644;&#21518;&#38376;&#26816;&#27979;&#31934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.13904</link><description>&lt;p&gt;
LMSanitator: &#38024;&#23545;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#30340;Prompt-Tuning&#38450;&#24481;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors. (arXiv:2308.13904v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13904
&lt;/p&gt;
&lt;p&gt;
LMSanitator&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#28040;&#38500;Transformer&#27169;&#22411;&#20013;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;LMSanitator&#36890;&#36807;&#36870;&#36716;&#39044;&#23450;&#20041;&#30340;&#25915;&#20987;&#21521;&#37327;&#32780;&#19981;&#26159;&#35302;&#21457;&#22120;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#21644;&#21518;&#38376;&#26816;&#27979;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-Tuning&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24341;&#20154;&#27880;&#30446;&#30340;&#33539;&#24335;&#65292;&#29992;&#20110;&#37096;&#32626;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#24378;&#22823;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#26381;&#21153;&#33021;&#21147;&#12290;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;Prompt-Tuning&#23481;&#26131;&#21463;&#21040;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#21518;&#38376;&#23384;&#22312;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#24433;&#21709;&#20219;&#24847;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#26080;&#27861;&#38450;&#24481;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#38590;&#22312;&#36870;&#36716;&#21518;&#38376;&#35302;&#21457;&#22120;&#26041;&#38754;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMSanitator&#65292;&#19968;&#31181;&#22312;Transformer&#27169;&#22411;&#19978;&#26816;&#27979;&#21644;&#21435;&#38500;&#20219;&#21153;&#19981;&#21487;&#30693;&#21518;&#38376;&#30340;&#26032;&#26041;&#27861;&#12290;LMSanitator&#19981;&#30452;&#25509;&#36870;&#36716;&#35302;&#21457;&#22120;&#65292;&#32780;&#26159;&#36870;&#36716;&#39044;&#23450;&#20041;&#30340;&#25915;&#20987;&#21521;&#37327;&#65288;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36755;&#20837;&#23884;&#20837;&#35302;&#21457;&#22120;&#26102;&#30340;&#36755;&#20986;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#33021;&#21644;&#21518;&#38376;&#26816;&#27979;&#31934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inversing the triggers, LMSanitator aims to inverse the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prom
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20998;&#21106;&#22270;&#20013;&#26631;&#31614;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;S4MC&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#36890;&#36807;&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#24182;&#25552;&#39640;&#20102;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20351;&#29992;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.13900</link><description>&lt;p&gt;
&#36890;&#36807;&#36793;&#38469;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Semantic Segmentation via Marginal Contextual Information. (arXiv:2308.13900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13900
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20998;&#21106;&#22270;&#20013;&#26631;&#31614;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;S4MC&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#36890;&#36807;&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#26041;&#24335;&#65292;&#24182;&#25552;&#39640;&#20102;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20351;&#29992;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36229;&#36234;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32622;&#20449;&#24230;&#31934;&#21270;&#26041;&#26696;&#65292;&#22686;&#24378;&#20102;&#21322;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#20266;&#26631;&#31614;&#12290;&#19982;&#24403;&#21069;&#20027;&#27969;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#30456;&#37051;&#20687;&#32032;&#20998;&#32452;&#24182;&#20849;&#21516;&#32771;&#34385;&#23427;&#20204;&#30340;&#20266;&#26631;&#31614;&#65292;&#21033;&#29992;&#20998;&#21106;&#22270;&#20013;&#26631;&#31614;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#20511;&#21161;&#36825;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;S4MC&#65292;&#22312;&#20445;&#25345;&#20266;&#26631;&#31614;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#22686;&#21152;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#36890;&#36807;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;S4MC&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20026;&#38477;&#20302;&#33719;&#24471;&#31264;&#23494;&#26631;&#27880;&#25104;&#26412;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20363;&#22914;&#65292;&#22312;PASCAL VOC 12&#19978;&#20351;&#29992;366&#20010;&#24102;&#27880;&#37322;&#22270;&#20687;&#65292;S4MC&#27604;&#21069;&#19968;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;1.29&#20010;mIoU&#12290;&#26377;&#20851;&#37325;&#29616;&#25105;&#20204;&#23454;&#39564;&#30340;&#20195;&#30721;&#21442;&#35265;...
&lt;/p&gt;
&lt;p&gt;
We present a novel confidence refinement scheme that enhances pseudo-labels in semi-supervised semantic segmentation. Unlike current leading methods, which filter pixels with low-confidence predictions in isolation, our approach leverages the spatial correlation of labels in segmentation maps by grouping neighboring pixels and considering their pseudo-labels collectively. With this contextual information, our method, named S4MC, increases the amount of unlabeled data used during training while maintaining the quality of the pseudo-labels, all with negligible computational overhead. Through extensive experiments on standard benchmarks, we demonstrate that S4MC outperforms existing state-of-the-art semi-supervised learning approaches, offering a promising solution for reducing the cost of acquiring dense annotations. For example, S4MC achieves a 1.29 mIoU improvement over the prior state-of-the-art method on PASCAL VOC 12 with 366 annotated images. The code to reproduce our experiments i
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#35745;&#31639;&#22270;&#20248;&#21270;&#30340;&#39640;&#25928;&#20869;&#23384;&#24863;&#30693;&#35843;&#24230;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#22270;&#34701;&#21512;&#21644;&#25299;&#25169;&#24863;&#30693;&#30340;&#21464;&#37327;&#20462;&#21098;&#26469;&#31616;&#21270;&#35745;&#31639;&#22270;&#24182;&#23454;&#29616;&#35843;&#24230;&#20248;&#21270;&#65292;&#26377;&#26395;&#22312;&#22797;&#26434;&#26377;&#32447;&#32593;&#32476;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#21644;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.13898</link><description>&lt;p&gt;
&#20869;&#23384;&#24863;&#30693;&#35843;&#24230;&#22312;&#22797;&#26434;&#26377;&#32447;&#32593;&#32476;&#20013;&#20351;&#29992;&#36845;&#20195;&#22270;&#20248;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Memory-aware Scheduling for Complex Wired Networks with Iterative Graph Optimization. (arXiv:2308.13898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13898
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#35745;&#31639;&#22270;&#20248;&#21270;&#30340;&#39640;&#25928;&#20869;&#23384;&#24863;&#30693;&#35843;&#24230;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#22270;&#34701;&#21512;&#21644;&#25299;&#25169;&#24863;&#30693;&#30340;&#21464;&#37327;&#20462;&#21098;&#26469;&#31616;&#21270;&#35745;&#31639;&#22270;&#24182;&#23454;&#29616;&#35843;&#24230;&#20248;&#21270;&#65292;&#26377;&#26395;&#22312;&#22797;&#26434;&#26377;&#32447;&#32593;&#32476;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#21644;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23384;&#24863;&#30693;&#32593;&#32476;&#35843;&#24230;&#23545;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#36827;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#21333;&#20803;&#32423;&#21644;&#32593;&#32476;&#32423;&#25299;&#25169;&#32467;&#26500;&#65292;&#20869;&#23384;&#24863;&#30693;&#35843;&#24230;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#35745;&#31639;&#22270;&#20248;&#21270;&#30340;&#39640;&#25928;&#20869;&#23384;&#24863;&#30693;&#35843;&#24230;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#19968;&#31181;&#36845;&#20195;&#22270;&#34701;&#21512;&#31639;&#27861;&#65292;&#21487;&#20197;&#31616;&#21270;&#35745;&#31639;&#22270;&#21516;&#26102;&#20445;&#25345;&#35843;&#24230;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#65292;&#24182;&#32467;&#21512;&#25299;&#25169;&#24863;&#30693;&#30340;&#21464;&#37327;&#20462;&#21098;&#26469;&#39640;&#25928;&#35843;&#24230;&#31616;&#21270;&#30340;&#22270;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#32593;&#32476;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#23558;&#23792;&#20540;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#20102;13.4%&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#22797;&#26434;&#25299;&#25169;&#32467;&#26500;&#30340;&#32593;&#32476;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory-aware network scheduling is becoming increasingly important for deep neural network (DNN) inference on resource-constrained devices. However, due to the complex cell-level and network-level topologies, memory-aware scheduling becomes very challenging. While previous algorithms all suffer from poor scalability, in this paper, we propose an efficient memory-aware scheduling framework based on iterative computation graph optimization. Our framework features an iterative graph fusion algorithm that simplifies the computation graph while preserving the scheduling optimality. We further propose an integer linear programming formulation together with topology-aware variable pruning to schedule the simplified graph efficiently. We evaluate our method against prior-art algorithms on different networks and demonstrate that our method outperforms existing techniques in all the benchmarks, reducing the peak memory footprint by 13.4%, and achieving better scalability for networks with comple
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;FL&#21327;&#35758;FwdLLM&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#21313;&#20159;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;&#65288;FedLLM&#65289;&#30340;&#25928;&#29575;&#12290;FwdLLM&#36890;&#36807;&#20351;&#29992;&#26080;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#35757;&#32451;&#26041;&#27861;&#20197;&#21450;&#8220;&#25200;&#21160;&#25512;&#26029;&#8221;&#26469;&#25552;&#39640;&#20869;&#23384;&#25928;&#29575;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13894</link><description>&lt;p&gt;
&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#21313;&#20159;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Federated Fine-tuning of Billion-Sized Language Models across Mobile Devices. (arXiv:2308.13894v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13894
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;FL&#21327;&#35758;FwdLLM&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36827;&#34892;&#21313;&#20159;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#24494;&#35843;&#65288;FedLLM&#65289;&#30340;&#25928;&#29575;&#12290;FwdLLM&#36890;&#36807;&#20351;&#29992;&#26080;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#35757;&#32451;&#26041;&#27861;&#20197;&#21450;&#8220;&#25200;&#21160;&#25512;&#26029;&#8221;&#26469;&#25552;&#39640;&#20869;&#23384;&#25928;&#29575;&#21644;&#26102;&#38388;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#25913;&#21464;&#31227;&#21160;&#26234;&#33021;&#30340;&#26684;&#23616;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#29992;&#20110;&#23545;&#19979;&#28216;&#31227;&#21160;&#20219;&#21153;&#36827;&#34892;LLM&#30340;&#24494;&#35843;&#65292;&#36825;&#34987;&#31216;&#20026;FedLLM&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#30001;&#24222;&#22823;&#27169;&#22411;&#22823;&#23567;&#24341;&#36215;&#30340;&#32593;&#32476;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22312;&#19982;&#31227;&#21160;&#35774;&#22791;&#30340;&#25972;&#21512;&#26041;&#38754;&#24182;&#27809;&#26377;&#23454;&#38469;&#32531;&#35299;&#35832;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#26174;&#33879;&#30340;&#20869;&#23384;&#28040;&#32791;&#21644;&#32531;&#24930;&#30340;&#27169;&#22411;&#25910;&#25947;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;FL&#21327;&#35758;FwdLLM&#65292;&#26088;&#22312;&#25552;&#39640;FedLLM&#30340;&#25928;&#29575;&#12290;FwdLLM&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#37319;&#29992;&#26080;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#35774;&#22791;&#25191;&#34892;&#8220;&#25200;&#21160;&#25512;&#26029;&#8221;&#12290;&#22240;&#27492;&#65292;FwdLLM&#20855;&#26377;&#26356;&#22909;&#30340;&#20869;&#23384;&#25928;&#29575;&#21644;&#26102;&#38388;&#25928;&#29575;&#65288;&#36890;&#36807;&#31227;&#21160;NPUs&#21644;&#25193;&#22823;&#30340;&#21442;&#19982;&#35774;&#22791;&#25968;&#32452;&#65289;&#12290;FwdLLM&#22260;&#32469;&#19977;&#20010;&#20851;&#38190;&#35774;&#35745;&#23637;&#24320;&#65306;&#65288;1&#65289;&#23558;&#26080;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#19982;p
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are transforming the landscape of mobile intelligence. Federated Learning (FL), a method to preserve user data privacy, is often employed in fine-tuning LLMs to downstream mobile tasks, an approach known as FedLLM. Though recent efforts have addressed the network issue induced by the vast model size, they have not practically mitigated vital challenges concerning integration with mobile devices, such as significant memory consumption and sluggish model convergence.  In response to these challenges, this work introduces FwdLLM, an innovative FL protocol designed to enhance the FedLLM efficiency. The key idea of FwdLLM to employ backpropagation (BP)-free training methods, requiring devices only to execute ``perturbed inferences''. Consequently, FwdLLM delivers way better memory efficiency and time efficiency (expedited by mobile NPUs and an expanded array of participant devices). FwdLLM centers around three key designs: (1) it combines BP-free training with p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;DrIVeNN&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#25972;&#21512;&#33647;&#29289;&#29305;&#24449;&#65292;&#22914;&#20998;&#23376;&#32467;&#26500;&#12289;&#33647;&#29289;-&#34507;&#30333;&#30456;&#20114;&#20316;&#29992;&#21644;&#21333;&#33647;&#21103;&#20316;&#29992;&#31561;&#26469;&#24314;&#31435;&#21644;&#35780;&#20272;&#22810;&#33647;&#24182;&#29992;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#65288;ADEs&#65289;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.13891</link><description>&lt;p&gt;
&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#21521;&#37327;&#31070;&#32463;&#32593;&#32476;: DrIVeNN
&lt;/p&gt;
&lt;p&gt;
Drug Interaction Vectors Neural Network: DrIVeNN. (arXiv:2308.13891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;DrIVeNN&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#25972;&#21512;&#33647;&#29289;&#29305;&#24449;&#65292;&#22914;&#20998;&#23376;&#32467;&#26500;&#12289;&#33647;&#29289;-&#34507;&#30333;&#30456;&#20114;&#20316;&#29992;&#21644;&#21333;&#33647;&#21103;&#20316;&#29992;&#31561;&#26469;&#24314;&#31435;&#21644;&#35780;&#20272;&#22810;&#33647;&#24182;&#29992;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#65288;ADEs&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#33647;&#24182;&#29992;&#26159;&#27835;&#30103;&#21333;&#19968;&#30149;&#30151;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#31649;&#29702;&#22810;&#31181;&#25110;&#22797;&#26434;&#30149;&#30151;&#30340;&#24739;&#32773;&#26469;&#35828;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#33647;&#29289;&#26041;&#26696;&#20013;&#22686;&#21152;&#33647;&#29289;&#30340;&#25968;&#37327;&#65292;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#65288;ADEs&#65289;&#30340;&#39118;&#38505;&#36805;&#36895;&#19978;&#21319;&#12290;&#35768;&#22810;&#19982;&#22810;&#33647;&#24182;&#29992;&#30456;&#20851;&#30340;&#20005;&#37325;ADEs&#21482;&#26377;&#22312;&#33647;&#29289;&#20351;&#29992;&#21518;&#25165;&#20250;&#34987;&#21457;&#29616;&#12290;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#27979;&#35797;&#27599;&#31181;&#21487;&#33021;&#30340;&#33647;&#29289;&#32452;&#21512;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#32769;&#24180;&#24515;&#34880;&#31649;&#30142;&#30149;&#65288;CVD&#65289;&#24739;&#32773;&#20013;&#23588;&#20026;&#24120;&#35265;&#65292;&#22810;&#33647;&#24182;&#29992;&#21644;ADEs&#24120;&#24120;&#35266;&#23519;&#21040;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#23450;&#24314;&#31435;&#21644;&#35780;&#20272;&#22810;&#33647;&#24182;&#29992;ADEs&#27169;&#22411;&#30340;&#20851;&#38190;&#33647;&#29289;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#27425;&#35201;&#30446;&#26631;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#26696;&#20363;&#30740;&#31350;&#20013;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#25324;&#20998;&#23376;&#32467;&#26500;&#12289;&#33647;&#29289;-&#34507;&#30333;&#30456;&#20114;&#20316;&#29992;&#21644;&#21333;&#33647;&#21103;&#20316;&#29992;&#31561;&#33647;&#29289;&#29305;&#24449;&#30340;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65288;DrIVeNN&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;sid&#36827;&#34892;&#20102;DrIVeNN&#30340;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polypharmacy, the concurrent use of multiple drugs to treat a single condition, is common in patients managing multiple or complex conditions. However, as more drugs are added to the treatment plan, the risk of adverse drug events (ADEs) rises rapidly. Many serious ADEs associated with polypharmacy only become known after the drugs are in use. It is impractical to test every possible drug combination during clinical trials. This issue is particularly prevalent among older adults with cardiovascular disease (CVD) where polypharmacy and ADEs are commonly observed. In this research, our primary objective was to identify key drug features to build and evaluate a model for modeling polypharmacy ADEs. Our secondary objective was to assess our model on a domain-specific case study. We developed a two-layer neural network that incorporated drug features such as molecular structure, drug-protein interactions, and mono drug side effects (DrIVeNN). We assessed DrIVeNN using publicly available sid
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20154;&#33080;&#22270;&#20687;&#21464;&#24418;&#21644;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#20013;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#20154;&#33080;&#21464;&#24418;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13888</link><description>&lt;p&gt;
&#20154;&#33080;&#22270;&#20687;&#30340;&#31070;&#32463;&#38544;&#24335;&#24418;&#21464;
&lt;/p&gt;
&lt;p&gt;
Neural Implicit Morphing of Face Images. (arXiv:2308.13888v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20154;&#33080;&#22270;&#20687;&#21464;&#24418;&#21644;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#32467;&#21512;&#32463;&#20856;&#26041;&#27861;&#20013;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#20154;&#33080;&#21464;&#24418;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#21464;&#24418;&#26159;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#20855;&#26377;&#20247;&#22810;&#33402;&#26415;&#21644;&#21462;&#35777;&#24212;&#29992;&#12290;&#30001;&#20110;&#23039;&#24577;&#12289;&#20809;&#29031;&#12289;&#24615;&#21035;&#21644;&#31181;&#26063;&#30340;&#21464;&#21270;&#65292;&#23427;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#36825;&#20010;&#20219;&#21153;&#21253;&#25324;&#29305;&#24449;&#23545;&#40784;&#30340;&#21464;&#24418;&#21644;&#26080;&#32541;&#36807;&#28193;&#30340;&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22522;&#20110;&#22352;&#26631;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#34920;&#31034;&#20154;&#33080;&#22270;&#20687;&#30340;&#36825;&#31181;&#21464;&#24418;&#21644;&#28151;&#21512;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32593;&#32476;&#30340;&#24179;&#28369;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#31163;&#25955;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26102;&#38388;&#20381;&#36182;&#30340;&#65292;&#20801;&#35768;&#23545;&#30446;&#26631;&#22270;&#20687;&#36827;&#34892;&#36830;&#32493;&#30340;&#21464;&#24418;&#21644;&#28151;&#21512;&#12290;&#22312;&#21464;&#24418;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#26102;&#38388;&#20381;&#36182;&#21464;&#24418;&#30340;&#30452;&#25509;&#21644;&#36870;&#21464;&#25442;&#12290;&#21069;&#32773;&#36127;&#36131;&#23558;&#30446;&#26631;&#22270;&#20687;&#21464;&#24418;&#20026;&#28304;&#22270;&#20687;&#65292;&#32780;&#21518;&#32773;&#21017;&#29992;&#20110;&#22312;&#30456;&#21453;&#26041;&#21521;&#36827;&#34892;&#21464;&#24418;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#21464;&#24418;&#32593;&#32476;&#20855;&#26377;&#39640;&#25928;&#12289;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face morphing is one of the seminal problems in computer graphics, with numerous artistic and forensic applications. It is notoriously challenging due to pose, lighting, gender, and ethnicity variations. Generally, this task consists of a warping for feature alignment and a blending for a seamless transition between the warped images.  We propose to leverage coordinate-based neural networks to represent such warpings and blendings of face images. During training, we exploit the smoothness and flexibility of such networks, by combining energy functionals employed in classical approaches without discretizations. Additionally, our method is time-dependent, allowing a continuous warping, and blending of the target images.  During warping inference, we need both direct and inverse transformations of the time-dependent warping. The first is responsible for morphing the target image into the source image, while the inverse is used for morphing in the opposite direction. Our neural warping sto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#29983;&#29289;&#21512;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#30340;&#20840;&#38754;&#35299;&#37322;&#21644;&#24212;&#29992;&#30340;&#29616;&#29366;&#21450;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13877</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#29983;&#29289;&#21512;&#25104;&#20013;&#30340;&#24212;&#29992;&#65306;&#25552;&#39640;&#25928;&#29575;&#21644;&#33539;&#22260;&#30340;&#20808;&#36827;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Applications of machine Learning to improve the efficiency and range of microbial biosynthesis: a review of state-of-art techniques. (arXiv:2308.13877v1 [q-bio.SC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24494;&#29983;&#29289;&#29983;&#29289;&#21512;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#30340;&#20840;&#38754;&#35299;&#37322;&#21644;&#24212;&#29992;&#30340;&#29616;&#29366;&#21450;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#19990;&#30028;&#20013;&#65292;&#25216;&#26415;&#36798;&#21040;&#20102;&#39030;&#23792;&#12290;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#32534;&#31243;&#21644;&#25216;&#26415;&#36884;&#24452;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#12289;&#33258;&#21160;&#21270;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#12290;&#26426;&#22120;&#23398;&#20064;&#26159;&#20248;&#21270;&#25968;&#25454;&#20998;&#26512;&#12289;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#21644;&#21152;&#36895;/&#25913;&#36827;&#29616;&#26377;&#21151;&#33021;&#30340;&#20851;&#38190;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#27491;&#22312;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#24182;&#19988;&#27491;&#22312;&#25506;&#32034;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20854;&#20013;&#19968;&#20010;&#31361;&#20986;&#30340;&#39046;&#22495;&#23601;&#26159;&#24494;&#29983;&#29289;&#29983;&#29289;&#21512;&#25104;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#24494;&#29983;&#29289;&#29983;&#29289;&#21512;&#25104;&#20013;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#24182;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#24494;&#29983;&#29289;&#29983;&#29289;&#21512;&#25104;&#39046;&#22495;&#20998;&#21035;&#36827;&#34892;&#20102;&#31616;&#35201;&#25551;&#36848;&#12290;&#35813;&#20449;&#24687;&#21253;&#25324;&#36807;&#21435;&#30340;&#36235;&#21183;&#12289;&#29616;&#20195;&#30340;&#21457;&#23637;&#12289;&#26410;&#26469;&#30340;&#25913;&#36827;&#12289;&#27969;&#31243;&#30340;&#35299;&#37322;&#20197;&#21450;&#24403;&#21069;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26803;&#29702;&#20102;&#20004;&#20010;&#20851;&#38190;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#24212;&#29992;&#30340;&#20840;&#38754;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the modern world, technology is at its peak. Different avenues in programming and technology have been explored for data analysis, automation, and robotics. Machine learning is key to optimize data analysis, make accurate predictions, and hasten/improve existing functions. Thus, presently, the field of machine learning in artificial intelligence is being developed and its uses in varying fields are being explored. One field in which its uses stand out is that of microbial biosynthesis. In this paper, a comprehensive overview of the differing machine learning programs used in biosynthesis is provided, alongside brief descriptions of the fields of machine learning and microbial biosynthesis separately. This information includes past trends, modern developments, future improvements, explanations of processes, and current problems they face. Thus, this paper's main contribution is to distill developments in, and provide a holistic explanation of, 2 key fields and their applicability to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#31867;&#21035;&#20108;&#20540;&#21270;&#25216;&#26415;&#24212;&#29992;&#20110;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#23558;&#32416;&#38169;&#36755;&#20986;&#30721;(ECOC)&#24212;&#29992;&#20110;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.13876</link><description>&lt;p&gt;
&#31867;&#21035;&#20108;&#20540;&#21270;&#21040;&#31070;&#32463;&#36827;&#21270;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Class Binarization to NeuroEvolution for Multiclass Classification. (arXiv:2308.13876v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31867;&#21035;&#20108;&#20540;&#21270;&#25216;&#26415;&#24212;&#29992;&#20110;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#23558;&#32416;&#38169;&#36755;&#20986;&#30721;(ECOC)&#24212;&#29992;&#20110;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#31867;&#21035;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31867;&#21035;&#20998;&#31867;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#39033;&#22522;&#30784;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#25216;&#26415;&#21487;&#20998;&#20026;&#19977;&#31867;&#65306;(i)&#23558;&#22810;&#31867;&#21035;&#20998;&#31867;&#20998;&#35299;&#20026;&#20108;&#20540;&#21270;&#20998;&#31867;&#65292;&#20351;&#29992;&#20108;&#20540;&#21270;&#20998;&#31867;&#22120;&#39640;&#25928;&#35299;&#20915;&#12290;(ii)&#20174;&#20108;&#20540;&#21270;&#20998;&#31867;&#20013;&#25193;&#23637;&#32780;&#26469;&#12290;(iii)&#23618;&#27425;&#20998;&#31867;&#12290;&#26412;&#25991;&#23558;&#31867;&#21035;&#20108;&#20540;&#21270;&#25216;&#26415;&#24212;&#29992;&#20110;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;NeuroEvolution of Augmenting Topologies (NEAT)&#65292;&#20197;&#29983;&#25104;&#29992;&#20110;&#22810;&#31867;&#21035;&#20998;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#32416;&#38169;&#36755;&#20986;&#30721; (ECOC) &#24212;&#29992;&#20110;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#31867;&#21035;&#20108;&#20540;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiclass classification is a fundamental and challenging task in machine learning. The existing techniques of multiclass classification can be categorized as (i) decomposition into binary (ii) extension from binary and (iii) hierarchical classification. Decomposing multiclass classification into a set of binary classifications that can be efficiently solved by using binary classifiers, called class binarization, which is a popular technique for multiclass classification. Neuroevolution, a general and powerful technique for evolving the structure and weights of neural networks, has been successfully applied to binary classification. In this paper, we apply class binarization techniques to a neuroevolution algorithm, NeuroEvolution of Augmenting Topologies (NEAT), that is used to generate neural networks for multiclass classification. We propose a new method that applies Error-Correcting Output Codes (ECOC) to design the class binarization strategies on the neuroevolution for multiclas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#24378;&#22823;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#20687;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#33258;&#28982;&#30005;&#24433;&#36827;&#34892;&#34920;&#24449;&#30699;&#27491;&#12290;&#36825;&#31181;&#30699;&#27491;&#22312;&#29983;&#29289;&#35270;&#35273;&#20013;&#26159;&#24120;&#35265;&#30340;&#65292;&#20294;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26410;&#33021;&#21576;&#29616;&#36825;&#31181;&#29616;&#35937;&#12290;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#65292;&#36825;&#31181;&#30699;&#27491;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#20013;&#38388;&#30005;&#24433;&#24103;&#12290;</title><link>http://arxiv.org/abs/2308.13870</link><description>&lt;p&gt;
&#20687;&#22823;&#33041;&#19968;&#26679;&#30340;&#34920;&#24449;&#30699;&#27491;&#22312;&#24378;&#22823;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#33258;&#28982;&#30005;&#24433;&#36827;&#34892;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Brain-like representational straightening of natural movies in robust feedforward neural networks. (arXiv:2308.13870v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#24378;&#22823;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#20687;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#33258;&#28982;&#30005;&#24433;&#36827;&#34892;&#34920;&#24449;&#30699;&#27491;&#12290;&#36825;&#31181;&#30699;&#27491;&#22312;&#29983;&#29289;&#35270;&#35273;&#20013;&#26159;&#24120;&#35265;&#30340;&#65292;&#20294;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26410;&#33021;&#21576;&#29616;&#36825;&#31181;&#29616;&#35937;&#12290;&#36890;&#36807;&#32447;&#24615;&#25554;&#20540;&#65292;&#36825;&#31181;&#30699;&#27491;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#20013;&#38388;&#30005;&#24433;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#30699;&#27491;&#26159;&#25351;&#20174;&#33258;&#28982;&#30005;&#24433;&#20013;&#36830;&#32493;&#37319;&#38598;&#30340;&#24103;&#30340;&#35270;&#35273;&#29305;&#24449;&#34920;&#24449;&#26354;&#29575;&#30340;&#20943;&#23569;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#28789;&#38271;&#31867;&#21160;&#29289;&#30340;&#21021;&#32423;&#35270;&#35273;&#30382;&#23618;&#65288;V1&#65289;&#30340;&#34920;&#24449;&#30699;&#27491;&#21644;&#20154;&#31867;&#34892;&#20026;&#20013;&#30340;&#24863;&#30693;&#30699;&#27491;&#20316;&#20026;&#29983;&#29289;&#35270;&#35273;&#30340;&#26631;&#24535;&#65292;&#32780;&#20154;&#24037;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#24182;&#26410;&#34920;&#29616;&#20986;&#36825;&#31181;&#29616;&#35937;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#26126;&#30830;&#20248;&#21270;&#20197;&#20135;&#29983;&#26102;&#38388;&#21487;&#39044;&#27979;&#30340;&#30005;&#24433;&#34920;&#24449;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36755;&#20837;&#22270;&#20687;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#21487;&#20197;&#22312;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#20135;&#29983;&#34920;&#24449;&#30699;&#27491;&#12290;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#21644;Random Smoothing&#65288;RS&#65289;&#30340;&#22522;&#30784;&#20998;&#31867;&#22120;&#37117;&#21487;&#20197;&#20135;&#29983;&#26126;&#26174;&#30699;&#27491;&#30340;&#29305;&#24449;&#32534;&#30721;&#12290;&#22312;&#33258;&#28982;&#30005;&#24433;&#39046;&#22495;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#65292;&#36825;&#20123;&#32534;&#30721;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#31354;&#38388;&#30340;&#32447;&#24615;&#25554;&#20540;&#26469;&#21453;&#25512;&#20013;&#38388;&#30005;&#24433;&#24103;&#65292;&#21363;&#20351;&#23427;&#20204;&#27809;&#26377;&#22312;&#36825;&#20123;&#24103;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representational straightening refers to a decrease in curvature of visual feature representations of a sequence of frames taken from natural movies. Prior work established straightening in neural representations of the primate primary visual cortex (V1) and perceptual straightening in human behavior as a hallmark of biological vision in contrast to artificial feedforward neural networks which did not demonstrate this phenomenon as they were not explicitly optimized to produce temporally predictable movie representations. Here, we show robustness to noise in the input image can produce representational straightening in feedforward neural networks. Both adversarial training (AT) and base classifiers for Random Smoothing (RS) induced remarkably straightened feature codes. Demonstrating their utility within the domain of natural movies, these codes could be inverted to generate intervening movie frames by linear interpolation in the feature space even though they were not trained on these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20572;&#27490;&#23398;&#20064;&#65292;&#36890;&#36807;&#31227;&#38500;&#39640;&#27010;&#29575;&#38169;&#35823;&#26631;&#35760;&#31034;&#20363;&#26469;&#32553;&#23567;&#22122;&#22768;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#30041;&#22823;&#22810;&#25968;&#24178;&#20928;&#22256;&#38590;&#31034;&#20363;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26368;&#20248;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13862</link><description>&lt;p&gt;
&#20572;&#27490;&#23398;&#20064;&#65306;&#36991;&#20813;&#33258;&#20449;&#22320;&#20174;&#38169;&#35823;&#26631;&#35760;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Late Stopping: Avoiding Confidently Learning from Mislabeled Examples. (arXiv:2308.13862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#20572;&#27490;&#23398;&#20064;&#65292;&#36890;&#36807;&#31227;&#38500;&#39640;&#27010;&#29575;&#38169;&#35823;&#26631;&#35760;&#31034;&#20363;&#26469;&#32553;&#23567;&#22122;&#22768;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20445;&#30041;&#22823;&#22810;&#25968;&#24178;&#20928;&#22256;&#38590;&#31034;&#20363;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#26368;&#20248;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23398;&#20064;&#20013;&#65292;&#26679;&#26412;&#36873;&#25321;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#23558;&#23567;&#25439;&#22833;&#30340;&#25968;&#25454;&#35270;&#20026;&#27491;&#30830;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#20855;&#26377;&#36739;&#22823;&#25439;&#22833;&#30340;&#24178;&#20928;&#22256;&#38590;&#31034;&#20363;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#27169;&#22411;&#25509;&#36817;&#26368;&#20248;&#27867;&#21270;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#20572;&#27490;&#23398;&#20064;&#65292;&#36890;&#36807;&#19968;&#20010;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#22312;&#40065;&#26834;&#23398;&#20064;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20572;&#27490;&#23398;&#20064;&#36890;&#36807;&#36880;&#28176;&#31227;&#38500;&#39640;&#27010;&#29575;&#38169;&#35823;&#26631;&#35760;&#31034;&#20363;&#26469;&#32553;&#23567;&#22122;&#22768;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#30041;&#22823;&#22810;&#25968;&#24178;&#20928;&#22256;&#38590;&#31034;&#20363;&#22312;&#35757;&#32451;&#38598;&#20013;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20013;&#35266;&#23519;&#21040;&#38169;&#35823;&#26631;&#35760;&#21644;&#24178;&#20928;&#31034;&#20363;&#22312;&#34987;&#19968;&#33268;&#21644;&#27491;&#30830;&#20998;&#31867;&#25152;&#38656;&#30340;&#21608;&#26399;&#25968;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#22240;&#27492;&#21487;&#20197;&#31227;&#38500;&#39640;&#27010;&#29575;&#38169;&#35823;&#26631;&#35760;&#31034;&#20363;&#12290;&#22312;&#22522;&#20934;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22122;&#22768;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Sample selection is a prevalent method in learning with noisy labels, where small-loss data are typically considered as correctly labeled data. However, this method may not effectively identify clean hard examples with large losses, which are critical for achieving the model's close-to-optimal generalization performance. In this paper, we propose a new framework, Late Stopping, which leverages the intrinsic robust learning ability of DNNs through a prolonged training process. Specifically, Late Stopping gradually shrinks the noisy dataset by removing high-probability mislabeled examples while retaining the majority of clean hard examples in the training set throughout the learning process. We empirically observe that mislabeled and clean examples exhibit differences in the number of epochs required for them to be consistently and correctly classified, and thus high-probability mislabeled examples can be removed. Experimental results on benchmark-simulated and real-world noisy datasets 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#19982;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#38750;&#30149;&#29702;&#24615;&#20559;&#31227;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#20559;&#31227;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.13861</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#22312;&#33041;&#37096;MRI&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bias in Unsupervised Anomaly Detection in Brain MRI. (arXiv:2308.13861v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20559;&#24046;&#38382;&#39064;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#19982;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#38750;&#30149;&#29702;&#24615;&#20559;&#31227;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#20559;&#31227;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20026;&#30417;&#30563;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#28789;&#27963;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#38761;&#26032;&#21307;&#23398;&#25195;&#25551;&#20998;&#26512;&#65292;&#25552;&#39640;&#35786;&#26029;&#24615;&#33021;&#12290;&#24403;&#21069;&#29615;&#22659;&#20013;&#65292;&#20154;&#20204;&#24120;&#24120;&#20551;&#35774;&#27979;&#35797;&#26679;&#26412;&#19982;&#35757;&#32451;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#20165;&#24402;&#22240;&#20110;&#30149;&#29702;&#26465;&#20214;&#65292;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#24046;&#24322;&#37117;&#34987;&#35270;&#20026;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#24120;&#24120;&#24573;&#35270;&#20102;&#20854;&#20182;&#21487;&#33021;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;&#30340;&#28508;&#22312;&#22240;&#32032;&#65292;&#21253;&#25324;&#25195;&#25551;&#20202;&#12289;&#24180;&#40836;&#12289;&#24615;&#21035;&#25110;&#31181;&#26063;&#12290;&#36825;&#20123;&#20559;&#31227;&#20250;&#20005;&#37325;&#24433;&#21709;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;&#19968;&#20123;&#31361;&#20986;&#30340;&#22833;&#36133;&#26696;&#20363;&#24341;&#21457;&#20102;&#23545;&#24322;&#24120;&#26816;&#27979;&#30340;&#20559;&#35265;&#12289;&#21487;&#20449;&#24230;&#21644;&#20844;&#24179;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#23545;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20559;&#24046;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#20998;&#26512;&#12290;&#36890;&#36807;&#26816;&#26597;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#21487;&#33021;&#30340;&#38750;&#30149;&#29702;&#24615;&#20998;&#24067;&#20559;&#31227;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#20559;&#31227;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomaly detection methods offer a promising and flexible alternative to supervised approaches, holding the potential to revolutionize medical scan analysis and enhance diagnostic performance.  In the current landscape, it is commonly assumed that differences between a test case and the training distribution are attributed solely to pathological conditions, implying that any disparity indicates an anomaly. However, the presence of other potential sources of distributional shift, including scanner, age, sex, or race, is frequently overlooked. These shifts can significantly impact the accuracy of the anomaly detection task. Prominent instances of such failures have sparked concerns regarding the bias, credibility, and fairness of anomaly detection.  This work presents a novel analysis of biases in unsupervised anomaly detection. By examining potential non-pathological distributional shifts between the training and testing distributions, we shed light on the extent of these bi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#21644;&#20998;&#35010;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24322;&#26500;&#24615;&#24102;&#26469;&#30340;&#35757;&#32451;&#36895;&#24230;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13849</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65306;&#22522;&#20110;&#37197;&#23545;&#21644;&#20998;&#35010;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Effectively Heterogeneous Federated Learning: A Pairing and Split Learning Based Approach. (arXiv:2308.13849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#21644;&#20998;&#35010;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24322;&#26500;&#24615;&#24102;&#26469;&#30340;&#35757;&#32451;&#36895;&#24230;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#20998;&#24067;&#24335;&#35774;&#22791;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#26102;&#36991;&#20813;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#31471;&#30340;&#24322;&#26500;&#24615;&#65292;&#32852;&#37030;&#23398;&#20064;&#23384;&#22312;&#35757;&#32451;&#36895;&#24230;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#24310;&#36831;&#21644;&#26381;&#21153;&#22120;&#32858;&#21512;&#30340;&#24310;&#36831;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#35745;&#31639;&#36164;&#28304;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36890;&#20449;&#36895;&#29575;&#23558;&#23458;&#25143;&#31471;&#36827;&#34892;&#37197;&#23545;&#65292;&#24182;&#22312;&#36923;&#36753;&#23618;&#23558;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#25104;&#20004;&#37096;&#20998;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#21482;&#35745;&#31639;&#20854;&#20998;&#37197;&#30340;&#37096;&#20998;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#35010;&#23398;&#20064;&#23454;&#29616;&#21069;&#21521;&#25512;&#29702;&#21644;&#21518;&#21521;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;&#23458;&#25143;&#31471;&#37197;&#23545;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#23146;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26500;&#35757;&#32451;&#24310;&#36831;&#20248;&#21270;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a promising paradigm federated Learning (FL) is widely used in privacy-preserving machine learning, which allows distributed devices to collaboratively train a model while avoiding data transmission among clients. Despite its immense potential, the FL suffers from bottlenecks in training speed due to client heterogeneity, leading to escalated training latency and straggling server aggregation. To deal with this challenge, a novel split federated learning (SFL) framework that pairs clients with different computational resources is proposed, where clients are paired based on computing resources and communication rates among clients, meanwhile the neural network model is split into two parts at the logical level, and each client only computes the part assigned to it by using the SL to achieve forward inference and backward training. Moreover, to effectively deal with the client pairing problem, a heuristic greedy algorithm is proposed by reconstructing the optimization of training late
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#26032;&#30340;&#20943;&#23567;&#27169;&#22411;&#65288;ROM&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;Sinkhorn&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#31934;&#30830;&#23398;&#20064;&#20943;&#23569;&#30340;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#12290;</title><link>http://arxiv.org/abs/2308.13840</link><description>&lt;p&gt;
&#21463;&#26368;&#20248;&#20256;&#36755;&#21551;&#21457;&#30340;&#24930;&#34928;&#20943;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65306;&#21033;&#29992;Sinkhorn&#25439;&#22833;&#21644;Wasserstein&#26680;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport-inspired Deep Learning Framework for Slow-Decaying Problems: Exploiting Sinkhorn Loss and Wasserstein Kernel. (arXiv:2308.13840v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#26032;&#30340;&#20943;&#23567;&#27169;&#22411;&#65288;ROM&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;Sinkhorn&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#31934;&#30830;&#23398;&#20064;&#20943;&#23569;&#30340;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23567;&#27169;&#22411;&#65288;ROMs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#31185;&#23398;&#35745;&#31639;&#20013;&#20197;&#22788;&#29702;&#39640;&#32500;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ROM&#26041;&#27861;&#21487;&#33021;&#21482;&#33021;&#37096;&#20998;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#22266;&#26377;&#20960;&#20309;&#29305;&#24449;&#12290;&#36825;&#20123;&#29305;&#24449;&#21253;&#25324;&#24213;&#23618;&#32467;&#26500;&#12289;&#20851;&#31995;&#21644;&#23545;&#31934;&#30830;&#24314;&#27169;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#29702;&#35770;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;ROM&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;Wasserstein&#36317;&#31163;&#20026;&#33258;&#23450;&#20041;&#26680;&#30340;&#26680;Proper&#27491;&#20132;&#20998;&#35299;&#65288;kPOD&#65289;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;Sinkhorn&#31639;&#27861;&#39640;&#25928;&#22320;&#35757;&#32451;&#24471;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;OT&#30340;&#38750;&#32447;&#24615;&#38477;&#32500;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#23398;&#20064;&#20943;&#23569;&#30340;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#20256;&#32479;&#30340;&#22343;&#26041;&#35823;&#24046;&#25110;&#20132;&#21449;&#29109;&#31561;&#24230;&#37327;&#26631;&#20934;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
Reduced order models (ROMs) are widely used in scientific computing to tackle high-dimensional systems. However, traditional ROM methods may only partially capture the intrinsic geometric characteristics of the data. These characteristics encompass the underlying structure, relationships, and essential features crucial for accurate modeling.  To overcome this limitation, we propose a novel ROM framework that integrates optimal transport (OT) theory and neural network-based methods. Specifically, we investigate the Kernel Proper Orthogonal Decomposition (kPOD) method exploiting the Wasserstein distance as the custom kernel, and we efficiently train the resulting neural network (NN) employing the Sinkhorn algorithm. By leveraging an OT-based nonlinear reduction, the presented framework can capture the geometric structure of the data, which is crucial for accurate learning of the reduced solution manifold. When compared with traditional metrics such as mean squared error or cross-entropy,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#25913;&#21892;&#20102;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.13838</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#23545;&#32852;&#21512;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Price-Discrimination Game for Distributed Resource Management in Federated Learning. (arXiv:2308.13838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#25913;&#21892;&#20102;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#38477;&#20302;&#20102;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#21487;&#20197;&#24418;&#25104;&#20856;&#22411;&#30340;&#20080;&#26041;&#24066;&#22330;&#65292;&#20854;&#20013;PS/&#20080;&#23478;&#25968;&#37327;&#36828;&#36828;&#23569;&#20110;&#23458;&#25143;&#31471;/&#21334;&#23478;&#25968;&#37327;&#12290;&#20026;&#20102;&#25913;&#21892;&#32852;&#21512;&#23398;&#20064;&#30340;&#24615;&#33021;&#24182;&#20943;&#23569;&#28608;&#21169;&#23458;&#25143;&#21442;&#19982;&#32852;&#21512;&#23398;&#20064;&#30340;&#25104;&#26412;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30340;&#26381;&#21153;&#36827;&#34892;&#23450;&#20215;&#24046;&#24322;&#21270;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#22320;&#20026;&#19981;&#21516;&#23458;&#25143;&#25552;&#20379;&#30456;&#21516;&#30340;&#26381;&#21153;&#23450;&#20215;&#12290;&#20215;&#26684;&#24046;&#24322;&#21270;&#22522;&#20110;&#23545;&#32852;&#21512;&#23398;&#20064;&#24102;&#26469;&#30340;&#24615;&#33021;&#25913;&#36827;&#21644;&#35745;&#31639;&#36890;&#20449;&#33021;&#21147;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20215;&#26684;&#24046;&#24322;&#21270;&#28216;&#25103;&#65288;PDG&#65289;&#65292;&#20840;&#38754;&#35299;&#20915;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#22810;&#30446;&#26631;&#26435;&#34913;&#12289;&#23458;&#25143;&#31471;&#36873;&#25321;&#21644;&#28608;&#21169;&#26426;&#21046;&#12290;&#30001;&#20110;PDG&#26159;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#35268;&#21010;&#65288;MINLP&#65289;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#20998;&#24067;&#24335;&#21322;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In vanilla federated learning (FL) such as FedAvg, the parameter server (PS) and multiple distributed clients can form a typical buyer's market, where the number of PS/buyers of FL services is far less than the number of clients/sellers. In order to improve the performance of FL and reduce the cost of motivating clients to participate in FL, this paper proposes to differentiate the pricing for services provided by different clients rather than simply providing the same service pricing for different clients. The price is differentiated based on the performance improvements brought to FL and their heterogeneity in computing and communication capabilities. To this end, a price-discrimination game (PDG) is formulated to comprehensively address the distributed resource management problems in FL, including multi-objective trade-off, client selection, and incentive mechanism. As the PDG is a mixed-integer nonlinear programming (MINLP) problem, a distributed semi-heuristic algorithm with low c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31867;&#21035;&#32422;&#26463;&#30340;t-SNE&#65292;&#29992;&#20110;&#32467;&#21512;&#25968;&#25454;&#29305;&#24449;&#21644;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#38477;&#32500;&#20998;&#26512;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24179;&#34913;&#20195;&#20215;&#20989;&#25968;&#20013;&#30340;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#20248;&#21270;&#25968;&#25454;&#28857;&#30340;&#20301;&#32622;&#21644;&#20195;&#34920;&#24615;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.13837</link><description>&lt;p&gt;
Class-constrained t-SNE&#65306;&#32467;&#21512;&#25968;&#25454;&#29305;&#24449;&#21644;&#31867;&#21035;&#27010;&#29575;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Class-constrained t-SNE: Combining Data Features and Class Probabilities. (arXiv:2308.13837v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31867;&#21035;&#32422;&#26463;&#30340;t-SNE&#65292;&#29992;&#20110;&#32467;&#21512;&#25968;&#25454;&#29305;&#24449;&#21644;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#38477;&#32500;&#20998;&#26512;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24179;&#34913;&#20195;&#20215;&#20989;&#25968;&#20013;&#30340;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#20248;&#21270;&#25968;&#25454;&#28857;&#30340;&#20301;&#32622;&#21644;&#20195;&#34920;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#29305;&#24449;&#21644;&#31867;&#21035;&#27010;&#29575;&#26159;&#35780;&#20272;&#27169;&#22411;&#32467;&#26524;&#21644;&#35782;&#21035;&#38382;&#39064;&#39033;&#26102;&#30340;&#20004;&#20010;&#20027;&#35201;&#35270;&#35282;&#12290;&#31867;&#21035;&#27010;&#29575;&#34920;&#31034;&#27599;&#20010;&#23454;&#20363;&#23646;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#21487;&#33021;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#27010;&#29575;&#20998;&#31867;&#22120;&#25110;&#20154;&#24037;&#26631;&#27880;&#26469;&#20135;&#29983;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#35270;&#35282;&#37117;&#26159;&#22810;&#32500;&#25968;&#25454;&#65292;&#24120;&#24120;&#20351;&#29992;&#38477;&#32500;&#25216;&#26415;&#26469;&#20174;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#20165;&#20851;&#27880;&#25968;&#25454;&#29305;&#24449;&#65292;&#35201;&#20040;&#20381;&#36182;&#31867;&#21035;&#27010;&#29575;&#20272;&#35745;&#26469;&#25351;&#23548;&#38477;&#32500;&#36807;&#31243;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;"&#31867;&#21035;&#32422;&#26463;&#30340;t-SNE"&#65292;&#23558;&#25968;&#25454;&#29305;&#24449;&#21644;&#31867;&#21035;&#27010;&#29575;&#21512;&#24182;&#22312;&#21516;&#19968;&#20010;&#38477;&#32500;&#32467;&#26524;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#20195;&#20215;&#20989;&#25968;&#20013;&#24179;&#34913;&#20004;&#20010;&#30456;&#24212;&#30340;&#32452;&#25104;&#37096;&#20998;&#26469;&#20248;&#21270;&#25968;&#25454;&#28857;&#30340;&#20301;&#32622;&#21644;&#20195;&#34920;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data features and class probabilities are two main perspectives when, e.g., evaluating model results and identifying problematic items. Class probabilities represent the likelihood that each instance belongs to a particular class, which can be produced by probabilistic classifiers or even human labeling with uncertainty. Since both perspectives are multi-dimensional data, dimensionality reduction (DR) techniques are commonly used to extract informative characteristics from them. However, existing methods either focus solely on the data feature perspective or rely on class probability estimates to guide the DR process. In contrast to previous work where separate views are linked to conduct the analysis, we propose a novel approach, class-constrained t-SNE, that combines data features and class probabilities in the same DR result. Specifically, we combine them by balancing two corresponding components in a cost function to optimize the positions of data points and iconic representation o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20026;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#31995;&#32479;&#21457;&#29616;&#20102;&#20445;&#25345;&#32467;&#26500;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#23884;&#20837;&#65292;&#36890;&#36807;&#36763;&#21464;&#25442;&#33719;&#24471;&#32039;&#20945;&#36763;&#23884;&#20837;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20854;&#21160;&#21147;&#23398;&#30340;&#26377;&#30028;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13835</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#20445;&#25345;&#32467;&#26500;&#30340;&#38750;&#32447;&#24615;&#26631;&#20934;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#31283;&#23450;&#24211;&#26222;&#26364;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Structure-Preserving Universal Stable Koopman-Inspired Embeddings for Nonlinear Canonical Hamiltonian Dynamics. (arXiv:2308.13835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20026;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#31995;&#32479;&#21457;&#29616;&#20102;&#20445;&#25345;&#32467;&#26500;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#23884;&#20837;&#65292;&#36890;&#36807;&#36763;&#21464;&#25442;&#33719;&#24471;&#32039;&#20945;&#36763;&#23884;&#20837;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20854;&#21160;&#21147;&#23398;&#30340;&#26377;&#30028;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#36866;&#24403;&#22352;&#26631;&#21464;&#25442;&#21487;&#20197;&#26500;&#24314;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#20415;&#20110;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#39044;&#27979;&#12289;&#25511;&#21046;&#21644;&#20248;&#21270;&#12290;&#20026;&#27492;&#65292;&#24211;&#26222;&#26364;&#31639;&#23376;&#29702;&#35770;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#30340;&#26694;&#26550;&#65292;&#20174;&#32780;&#20801;&#35768;&#20351;&#29992;&#32447;&#24615;&#24037;&#20855;&#36827;&#34892;&#35774;&#35745;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#36890;&#36807;&#36763;&#21464;&#25442;&#35782;&#21035;&#26631;&#20934;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#23884;&#20837;&#12290;&#23613;&#31649;&#36825;&#20010;&#20219;&#21153;&#36890;&#24120;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#33021;&#21147;&#26469;&#21457;&#29616;&#25152;&#38656;&#30340;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20811;&#26381;&#24211;&#26222;&#26364;&#31639;&#23376;&#22312;&#20855;&#26377;&#36830;&#32493;&#39057;&#35889;&#30340;&#31995;&#32479;&#20013;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#25260;&#21319;&#21407;&#29702;&#24182;&#23398;&#20064;&#20840;&#23616;&#30340;&#31435;&#26041;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30528;&#37325;&#24378;&#35843;&#20102;&#25152;&#21457;&#29616;&#23884;&#20837;&#30340;&#21160;&#21147;&#23398;&#30340;&#26377;&#30028;&#31283;&#23450;&#24615;&#30340;&#24378;&#21046;&#24615;&#35201;&#27714;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33719;&#21462;&#32039;&#20945;&#36763;&#23884;&#20837;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering a suitable coordinate transformation for nonlinear systems enables the construction of simpler models, facilitating prediction, control, and optimization for complex nonlinear systems. To that end, Koopman operator theory offers a framework for global linearization for nonlinear systems, thereby allowing the usage of linear tools for design studies. In this work, we focus on the identification of global linearized embeddings for canonical nonlinear Hamiltonian systems through a symplectic transformation. While this task is often challenging, we leverage the power of deep learning to discover the desired embeddings. Furthermore, to overcome the shortcomings of Koopman operators for systems with continuous spectra, we apply the lifting principle and learn global cubicized embeddings. Additionally, a key emphasis is paid to enforce the bounded stability for the dynamics of the discovered embeddings. We demonstrate the capabilities of deep learning in acquiring compact symplect
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#23545;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23457;&#35270;&#65292;&#26088;&#22312;&#32416;&#27491;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#21644;&#20195;&#34920;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13821</link><description>&lt;p&gt;
&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#32508;&#36848;&#65306;&#38382;&#39064;&#12289;&#25216;&#26415;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey of Imbalanced Learning on Graphs: Problems, Techniques, and Future Directions. (arXiv:2308.13821v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#23545;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23457;&#35270;&#65292;&#26088;&#22312;&#32416;&#27491;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#21644;&#20195;&#34920;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#19982;&#19990;&#30028;&#21508;&#31181;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#30456;&#20114;&#36830;&#25509;&#30340;&#32467;&#26500;&#12290;&#26377;&#25928;&#30340;&#22270;&#20998;&#26512;&#25216;&#26415;&#65292;&#22914;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#20174;&#22270;&#25968;&#25454;&#20013;&#33719;&#24471;&#28145;&#21051;&#30340;&#27934;&#23519;&#21147;&#65292;&#20026;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#36335;&#39044;&#27979;&#31561;&#21508;&#31181;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#22270;&#25968;&#25454;&#20013;&#26576;&#20123;&#29255;&#27573;&#25317;&#26377;&#22823;&#37327;&#25968;&#25454;&#32780;&#20854;&#20182;&#25968;&#25454;&#31232;&#32570;&#65292;&#20174;&#32780;&#23548;&#33268;&#20559;&#20506;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#36825;&#23601;&#38656;&#35201;&#20986;&#29616;&#20102;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#26032;&#20852;&#39046;&#22495;&#65292;&#26088;&#22312;&#32416;&#27491;&#36825;&#20123;&#25968;&#25454;&#20998;&#24067;&#20559;&#24046;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#21644;&#20195;&#34920;&#24615;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#22270;&#19978;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23457;&#35270;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#23545;&#35813;&#27010;&#24565;&#21644;&#30456;&#20851;&#26415;&#35821;&#30340;&#26126;&#30830;&#29702;&#35299;&#65292;&#20026;&#35835;&#32773;&#24314;&#31435;&#20102;&#25166;&#23454;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#65306;&#65288;1&#65289;&#38382;&#39064;&#20998;&#31867;&#27861;&#65288;Problem Taxonomy&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs represent interconnected structures prevalent in a myriad of real-world scenarios. Effective graph analytics, such as graph learning methods, enables users to gain profound insights from graph data, underpinning various tasks including node classification and link prediction. However, these methods often suffer from data imbalance, a common issue in graph data where certain segments possess abundant data while others are scarce, thereby leading to biased learning outcomes. This necessitates the emerging field of imbalanced learning on graphs, which aims to correct these data distribution skews for more accurate and representative learning outcomes. In this survey, we embark on a comprehensive review of the literature on imbalanced learning on graphs. We begin by providing a definitive understanding of the concept and related terminologies, establishing a strong foundational understanding for readers. Following this, we propose two comprehensive taxonomies: (1) the problem taxono
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31283;&#23450;&#20108;&#27425;&#27169;&#22411;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#32622;&#36866;&#24403;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#26500;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#31283;&#23450;&#24615;&#30340;&#20108;&#27425;&#31995;&#32479;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.13819</link><description>&lt;p&gt;
&#20445;&#35777;&#31283;&#23450;&#30340;&#20108;&#27425;&#27169;&#22411;&#21450;&#20854;&#22312;SINDy&#21644;&#25805;&#20316;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Stable Quadratic Models and their applications in SINDy and Operator Inference. (arXiv:2308.13819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31283;&#23450;&#20108;&#27425;&#27169;&#22411;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#32622;&#36866;&#24403;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#26500;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#31283;&#23450;&#24615;&#30340;&#20108;&#27425;&#31995;&#32479;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#23427;&#23558;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12289;&#22522;&#20110;&#29289;&#29702;&#24314;&#27169;&#21644;&#32463;&#39564;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#22312;&#24037;&#31243;&#35774;&#35745;&#24490;&#29615;&#21644;&#25968;&#23383;&#21452;&#32990;&#32974;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#19968;&#31181;&#25805;&#20316;&#25512;&#26029;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#32467;&#26500;&#19978;&#25552;&#20986;&#20808;&#39564;&#20551;&#35774;&#65288;&#36890;&#24120;&#30001;&#24050;&#30693;&#29289;&#29702;&#23398;&#25110;&#19987;&#23478;&#32473;&#20986;&#65289;&#65292;&#20174;&#32780;&#26500;&#24314;&#20302;&#32500;&#21160;&#24577;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#32622;&#36866;&#24403;&#30340;&#20248;&#21270;&#38382;&#39064;&#26469;&#23398;&#20064;&#27169;&#22411;&#30340;&#36816;&#31639;&#31526;&#20197;&#36827;&#34892;&#25512;&#26029;&#12290;&#21160;&#24577;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#23646;&#24615;&#26159;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#27169;&#22411;&#26080;&#27861;&#20445;&#35777;&#36825;&#31181;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20108;&#27425;&#27169;&#22411;&#30340;&#25512;&#26029;&#20844;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35774;&#35745;&#19978;&#26159;&#31283;&#23450;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#31283;&#23450;&#30340;&#20108;&#27425;&#31995;&#32479;&#30340;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27809;&#26377;&#31283;&#23450;&#28857;&#20294;&#26377;&#30028;&#30340;&#20108;&#27425;&#31995;&#32479;&#65306;
&lt;/p&gt;
&lt;p&gt;
Scientific machine learning for learning dynamical systems is a powerful tool that combines data-driven modeling models, physics-based modeling, and empirical knowledge. It plays an essential role in an engineering design cycle and digital twinning. In this work, we primarily focus on an operator inference methodology that builds dynamical models, preferably in low-dimension, with a prior hypothesis on the model structure, often determined by known physics or given by experts. Then, for inference, we aim to learn the operators of a model by setting up an appropriate optimization problem. One of the critical properties of dynamical systems is{stability. However, such a property is not guaranteed by the inferred models. In this work, we propose inference formulations to learn quadratic models, which are stable by design. Precisely, we discuss the parameterization of quadratic systems that are locally and globally stable. Moreover, for quadratic systems with no stable point yet bounded (e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#26469;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#32467;&#26500;&#32452;&#32455;&#65292;&#20174;&#31232;&#30095;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#33719;&#21462;&#31354;&#38388;&#20449;&#24687;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13816</link><description>&lt;p&gt;
&#21516;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Homological Convolutional Neural Networks. (arXiv:2308.13816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13816
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21516;&#35843;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#26469;&#21033;&#29992;&#34920;&#26684;&#25968;&#25454;&#30340;&#32467;&#26500;&#32452;&#32455;&#65292;&#20174;&#31232;&#30095;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#33719;&#21462;&#31354;&#38388;&#20449;&#24687;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#22312;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#65292;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#27604;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#19988;&#21516;&#26679;&#26377;&#25928;&#12290;&#36825;&#26159;&#22240;&#20026;&#34920;&#26684;&#25968;&#25454;&#20013;&#29305;&#24449;&#38388;&#30340;&#30456;&#20851;&#24615;&#27604;&#22270;&#20687;&#25110;&#33258;&#28982;&#35821;&#35328;&#30340;&#31354;&#38388;&#25110;&#35821;&#20041;&#20851;&#31995;&#35201;&#24369;&#65292;&#32780;&#19988;&#38656;&#35201;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#23545;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#26469;&#21033;&#29992;&#25968;&#25454;&#30340;&#32467;&#26500;&#32452;&#32455;&#65292;&#20174;&#31232;&#30095;&#30340;&#34920;&#26684;&#25968;&#25454;&#20013;&#33719;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#25152;&#24471;&#27169;&#22411;&#21033;&#29992;&#20102;&#21367;&#31215;&#30340;&#33021;&#21147;&#65292;&#24182;&#22260;&#32469;&#32593;&#32476;&#25299;&#25169;&#20013;&#30340;&#26377;&#38480;&#27010;&#24565;&#26469;&#20445;&#35777;&#65288;i&#65289;&#25968;&#25454;&#19968;&#33268;&#24615;&#12289;&#65288;ii&#65289;&#34920;&#31034;&#26377;&#25928;&#24615;&#12289;&#21644;&#65288;iii&#65289;&#23481;&#37327;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning methods have demonstrated outstanding performances on classification and regression tasks on homogeneous data types (e.g., image, audio, and text data). However, tabular data still poses a challenge with classic machine learning approaches being often computationally cheaper and equally effective than increasingly complex deep learning architectures. The challenge arises from the fact that, in tabular data, the correlation among features is weaker than the one from spatial or semantic relationships in images or natural languages, and the dependency structures need to be modeled without any prior information. In this work, we propose a novel deep learning architecture that exploits the data structural organization through topologically constrained network representations to gain spatial information from sparse tabular data. The resulting model leverages the power of convolutions and is centered on a limited number of concepts from network topology to guarantee (i) a data-c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SyMOT-Flow&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26410;&#30693;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#31216;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#26469;&#35757;&#32451;&#21487;&#36870;&#36716;&#25442;&#65292;&#24182;&#32467;&#21512;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#20316;&#20026;&#27491;&#21017;&#21270;&#65292;&#23558;&#26410;&#30693;&#20998;&#24067;&#36716;&#25442;&#20026;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#26356;&#31283;&#23450;&#20934;&#30830;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.13815</link><description>&lt;p&gt;
SyMOT-Flow: &#23398;&#20064;&#20004;&#20010;&#20219;&#24847;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#27969;&#21160;&#21450;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
SyMOT-Flow: Learning optimal transport flow for two arbitrary distributions with maximum mean discrepancy. (arXiv:2308.13815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SyMOT-Flow&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26410;&#30693;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#31216;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#26469;&#35757;&#32451;&#21487;&#36870;&#36716;&#25442;&#65292;&#24182;&#32467;&#21512;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#20316;&#20026;&#27491;&#21017;&#21270;&#65292;&#23558;&#26410;&#30693;&#20998;&#24067;&#36716;&#25442;&#20026;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#26356;&#31283;&#23450;&#20934;&#30830;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25214;&#21040;&#20004;&#20010;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#36716;&#25442;&#23545;&#20110;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#21644;&#25191;&#34892;&#23494;&#24230;&#20272;&#35745;&#12289;&#26679;&#26412;&#29983;&#25104;&#21644;&#32479;&#35745;&#25512;&#26029;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SyMOT-Flow&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26410;&#30693;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#31216;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#26469;&#35757;&#32451;&#19968;&#20010;&#21487;&#36870;&#36716;&#25442;&#65292;&#24182;&#32467;&#21512;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#20316;&#20026;&#27491;&#21017;&#21270;&#20197;&#33719;&#24471;&#19968;&#20010;&#30701;&#36317;&#31163;&#21644;&#21487;&#35299;&#37322;&#30340;&#36716;&#25442;&#12290;&#24471;&#21040;&#30340;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#26356;&#31283;&#23450;&#20934;&#30830;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24314;&#31435;&#20102;&#19968;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#20302;&#32500;&#31034;&#20363;&#21644;&#39640;&#32500;&#29983;&#25104;&#26679;&#26412;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a transformation between two unknown probability distributions from samples is crucial for modeling complex data distributions and perform tasks such as density estimation, sample generation, and statistical inference. One powerful framework for such transformations is normalizing flow, which transforms an unknown distribution into a standard normal distribution using an invertible network. In this paper, we introduce a novel model called SyMOT-Flow that trains an invertible transformation by minimizing the symmetric maximum mean discrepancy between samples from two unknown distributions, and we incorporate an optimal transport cost as regularization to obtain a short-distance and interpretable transformation. The resulted transformation leads to more stable and accurate sample generation. We establish several theoretical results for the proposed model and demonstrate its effectiveness with low-dimensional illustrative examples as well as high-dimensional generative samples obt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#32447;&#24615;&#21487;&#35299;&#37322;LSTM&#65288;DeLELSTM&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#26102;&#24207;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;DeLELSTM&#33021;&#22815;&#21306;&#20998;&#26032;&#25968;&#25454;&#30340;&#30636;&#26102;&#24433;&#21709;&#21644;&#21382;&#21490;&#25968;&#25454;&#30340;&#38271;&#26399;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#21253;&#21547;&#20102;&#26631;&#20934;LSTM&#21644;&#24352;&#37327;&#21270;LSTM&#20004;&#37096;&#20998;&#65292;&#20998;&#21035;&#22788;&#29702;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.13797</link><description>&lt;p&gt;
DeLELSTM: &#22522;&#20110;&#20998;&#35299;&#30340;&#32447;&#24615;&#21487;&#35299;&#37322;LSTM&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#26102;&#24207;&#25968;&#25454;&#30340;&#30636;&#26102;&#21644;&#38271;&#26399;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
DeLELSTM: Decomposition-based Linear Explainable LSTM to Capture Instantaneous and Long-term Effects in Time Series. (arXiv:2308.13797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#32447;&#24615;&#21487;&#35299;&#37322;LSTM&#65288;DeLELSTM&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#39640;&#26102;&#24207;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;DeLELSTM&#33021;&#22815;&#21306;&#20998;&#26032;&#25968;&#25454;&#30340;&#30636;&#26102;&#24433;&#21709;&#21644;&#21382;&#21490;&#25968;&#25454;&#30340;&#38271;&#26399;&#24433;&#21709;&#12290;&#36825;&#19968;&#26041;&#27861;&#21253;&#21547;&#20102;&#26631;&#20934;LSTM&#21644;&#24352;&#37327;&#21270;LSTM&#20004;&#37096;&#20998;&#65292;&#20998;&#21035;&#22788;&#29702;&#19981;&#21516;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24207;&#39044;&#27979;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#24120;&#35265;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#22312;&#26102;&#24207;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23545;&#20110;&#20851;&#38190;&#30340;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#26102;&#24207;&#27169;&#22411;&#30340;&#35299;&#37322;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#30340;&#32447;&#24615;&#21487;&#35299;&#37322;LSTM&#65288;DeLELSTM&#65289;&#26469;&#25552;&#39640;LSTM&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20256;&#32479;&#19978;&#65292;RNNs&#30340;&#21487;&#35299;&#37322;&#24615;&#21482;&#20851;&#27880;&#21464;&#37327;&#37325;&#35201;&#24615;&#21644;&#26102;&#38388;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#39069;&#22806;&#21306;&#20998;&#20102;&#26032;&#25968;&#25454;&#30340;&#30636;&#26102;&#24433;&#21709;&#21644;&#21382;&#21490;&#25968;&#25454;&#30340;&#38271;&#26399;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DeLELSTM&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#26631;&#20934;LSTM&#21644;&#24352;&#37327;&#21270;LSTM&#12290;&#24352;&#37327;&#21270;LSTM&#20026;&#27599;&#20010;&#21464;&#37327;&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#26500;&#25104;&#19968;&#20010;&#30697;&#38453; $\mathbf{h}_t$&#65292;&#26631;&#20934;LSTM&#21017;&#20351;&#29992;&#20849;&#20139;&#30340;&#38544;&#34255;&#29366;&#24577; $\mathbf{H}_t$ &#26469;&#23545;&#25152;&#26377;&#21464;&#37327;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting is prevalent in various real-world applications. Despite the promising results of deep learning models in time series forecasting, especially the Recurrent Neural Networks (RNNs), the explanations of time series models, which are critical in high-stakes applications, have received little attention. In this paper, we propose a Decomposition-based Linear Explainable LSTM (DeLELSTM) to improve the interpretability of LSTM. Conventionally, the interpretability of RNNs only concentrates on the variable importance and time importance. We additionally distinguish between the instantaneous influence of new coming data and the long-term effects of historical data. Specifically, DeLELSTM consists of two components, i.e., standard LSTM and tensorized LSTM. The tensorized LSTM assigns each variable with a unique hidden state making up a matrix $\mathbf{h}_t$, and the standard LSTM models all the variables with a shared hidden state $\mathbf{H}_t$. By decomposing the $\mathb
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#22312;&#20302;&#32500;&#25968;&#25454;&#27969;&#24418;&#19978;&#36827;&#34892;&#22806;&#22495;&#26816;&#27979;&#65292;&#36890;&#36807;&#20272;&#35745;&#23494;&#24230;&#21644;&#27979;&#37327;&#19982;&#27969;&#24418;&#30340;&#36317;&#31163;&#26469;&#21028;&#26029;&#22806;&#22495;&#25968;&#25454;&#65292;&#26377;&#25928;&#25552;&#39640;&#22806;&#22495;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13792</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#27969;&#24418;&#19978;&#30340;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#22806;&#22495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution detection using normalizing flows on the data manifold. (arXiv:2308.13792v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13792
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#22312;&#20302;&#32500;&#25968;&#25454;&#27969;&#24418;&#19978;&#36827;&#34892;&#22806;&#22495;&#26816;&#27979;&#65292;&#36890;&#36807;&#20272;&#35745;&#23494;&#24230;&#21644;&#27979;&#37327;&#19982;&#27969;&#24418;&#30340;&#36317;&#31163;&#26469;&#21028;&#26029;&#22806;&#22495;&#25968;&#25454;&#65292;&#26377;&#25928;&#25552;&#39640;&#22806;&#22495;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#22495;&#26816;&#27979;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#20272;&#35745;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#65292;&#20026;&#22806;&#22495;&#25968;&#25454;&#20998;&#37197;&#36739;&#20302;&#30340;&#21487;&#33021;&#24615;&#20540;&#12290;&#27491;&#21017;&#21270;&#27969;&#26159;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#25345;&#32500;&#24230;&#30340;&#21487;&#36870;&#21464;&#25442;&#25552;&#20379;&#21487;&#35745;&#31639;&#30340;&#23494;&#24230;&#20272;&#35745;&#12290;&#20256;&#32479;&#30340;&#27491;&#21017;&#21270;&#27969;&#22312;&#22806;&#22495;&#26816;&#27979;&#20013;&#23481;&#26131;&#22833;&#36133;&#65292;&#22240;&#20026;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#27169;&#22411;&#38754;&#20020;&#30528;&#32500;&#24230;&#35781;&#21650;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#27969;&#24418;&#20551;&#35774;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#36890;&#24120;&#20301;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#22806;&#22495;&#26816;&#27979;&#26102;&#30340;&#27969;&#24418;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#20272;&#35745;&#23494;&#24230;&#65292;&#24182;&#32467;&#21512;&#27979;&#37327;&#19982;&#27969;&#24418;&#30340;&#36317;&#31163;&#20316;&#20026;&#22806;&#22495;&#26816;&#27979;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#21333;&#29420;&#20351;&#29992;&#23427;&#20204;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#27969;&#24418;&#23398;&#20064;&#23545;&#22806;&#22495;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach for out-of-distribution detection involves estimating an underlying data distribution, which assigns a lower likelihood value to out-of-distribution data. Normalizing flows are likelihood-based generative models providing a tractable density estimation via dimension-preserving invertible transformations. Conventional normalizing flows are prone to fail in out-of-distribution detection, because of the well-known curse of dimensionality problem of the likelihood-based models. According to the manifold hypothesis, real-world data often lie on a low-dimensional manifold. This study investigates the effect of manifold learning using normalizing flows on out-of-distribution detection. We proceed by estimating the density on a low-dimensional manifold, coupled with measuring the distance from the manifold, as criteria for out-of-distribution detection. However, individually, each of them is insufficient for this task. The extensive experimental results show that manifold lea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#22522;&#20110;&#26799;&#24230;&#30340;&#28151;&#21512;&#22240;&#23376;&#20998;&#26512;&#22120;&#65288;MFA&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#36136;&#24515;&#21021;&#22987;&#21270;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#26102;&#33021;&#22815;&#31616;&#21270;&#35757;&#32451;&#21644;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#24182;&#36991;&#20813;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#26399;&#26395;-&#26368;&#22823;&#21270;&#31639;&#27861;&#65289;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13778</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#22522;&#20110;&#26799;&#24230;&#30340;&#28151;&#21512;&#22240;&#23376;&#20998;&#26512;&#22120;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Large-scale gradient-based training of Mixtures of Factor Analyzers. (arXiv:2308.13778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13778
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#22522;&#20110;&#26799;&#24230;&#30340;&#28151;&#21512;&#22240;&#23376;&#20998;&#26512;&#22120;&#65288;MFA&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#38543;&#26426;&#36136;&#24515;&#21021;&#22987;&#21270;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#26102;&#33021;&#22815;&#31616;&#21270;&#35757;&#32451;&#21644;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#24182;&#36991;&#20813;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#26399;&#26395;-&#26368;&#22823;&#21270;&#31639;&#27861;&#65289;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26159;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24120;&#29992;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#65289;&#26102;&#65292;&#30001;&#20110;&#25152;&#38656;&#30340;&#23436;&#25972;&#21327;&#26041;&#24046;&#30697;&#38453;&#65288;CM&#65289;&#30340;&#22823;&#23567;&#65292;&#23427;&#20204;&#38754;&#20020;&#38382;&#39064;&#65292;&#32780;&#23545;&#35282;&#32447;&#25110;&#29699;&#24418;CM&#30340;&#20351;&#29992;&#24448;&#24448;&#20250;&#24102;&#26469;&#36807;&#20110;&#20005;&#26684;&#30340;&#38480;&#21046;&#12290;&#28151;&#21512;&#22240;&#23376;&#20998;&#26512;&#22120;&#65288;MFA&#65289;&#27169;&#22411;&#26159;GMM&#30340;&#19968;&#20010;&#37325;&#35201;&#25193;&#23637;&#65292;&#23427;&#20801;&#35768;&#26681;&#25454;&#22240;&#23376;&#21152;&#36733;&#25968;&#24179;&#28369;&#22320;&#25554;&#20540;&#23545;&#35282;&#32447;&#21644;&#23436;&#25972;CM&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;MFA&#25104;&#21151;&#24212;&#29992;&#20110;&#24314;&#27169;&#39640;&#32500;&#22270;&#20687;&#25968;&#25454;&#12290;&#36825;&#31687;&#25991;&#31456;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#39640;&#32500;MFA&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#20174;&#38543;&#26426;&#30340;&#36136;&#24515;&#21021;&#22987;&#21270;&#24320;&#22987;&#12290;&#36825;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#35757;&#32451;&#21644;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#24182;&#36991;&#20813;&#20102;&#25209;&#22788;&#29702;&#22411;&#31639;&#27861;&#65288;&#22914;&#26399;&#26395;-&#26368;&#22823;&#21270;&#31639;&#27861;&#65289;&#22312;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#26102;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;p
&lt;/p&gt;
&lt;p&gt;
Gaussian Mixture Models (GMMs) are a standard tool in data analysis. However, they face problems when applied to high-dimensional data (e.g., images) due to the size of the required full covariance matrices (CMs), whereas the use of diagonal or spherical CMs often imposes restrictions that are too severe. The Mixture of Factor analyzers (MFA) model is an important extension of GMMs, which allows to smoothly interpolate between diagonal and full CMs based on the number of \textit{factor loadings} $l$. MFA has successfully been applied for modeling high-dimensional image data. This article contributes both a theoretical analysis as well as a new method for efficient high-dimensional MFA training by stochastic gradient descent, starting from random centroid initializations. This greatly simplifies the training and initialization process, and avoids problems of batch-type algorithms such Expectation-Maximization (EM) when training with huge amounts of data. In addition, by exploiting the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#27979;&#37327;-&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#30340;&#37319;&#26679;&#27604;&#29575;&#21644;&#30697;&#38453;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21452;&#22495;&#25439;&#22833;&#21644;&#22235;&#20010;&#24674;&#22797;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;/&#20449;&#24687;&#21033;&#29992;&#29575;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13777</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21487;&#25193;&#23637;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Scalable Deep Compressed Sensing. (arXiv:2308.13777v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#21487;&#25193;&#23637;&#28145;&#24230;&#21387;&#32553;&#24863;&#30693;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#26631;&#35760;&#30340;&#27979;&#37327;-&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#30340;&#37319;&#26679;&#27604;&#29575;&#21644;&#30697;&#38453;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#21452;&#22495;&#25439;&#22833;&#21644;&#22235;&#20010;&#24674;&#22797;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;/&#20449;&#24687;&#21033;&#29992;&#29575;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#24863;&#30693;&#65288;CS&#65289;&#26159;&#38477;&#20302;&#37319;&#26679;&#25104;&#26412;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#12290;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;CS&#26041;&#27861;&#22312;&#25910;&#38598;&#26631;&#35760;&#30340;&#27979;&#37327;-&#22320;&#38754;&#30495;&#23454;&#65288;GT&#65289;&#25968;&#25454;&#21644;&#25512;&#24191;&#21040;&#23454;&#38469;&#24212;&#29992;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#21487;&#25193;&#23637;&#28145;&#24230;CS&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#20010;&#31216;&#20026;SCL&#30340;&#23398;&#20064;&#26041;&#26696;&#21644;&#19968;&#20010;&#21517;&#20026;SCNet&#30340;&#32593;&#32476;&#31995;&#21015;&#65292;&#23427;&#19981;&#38656;&#35201;GT&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#19968;&#26086;&#22312;&#37096;&#20998;&#27979;&#37327;&#38598;&#19978;&#35757;&#32451;&#23436;&#27605;&#23601;&#21487;&#20197;&#22788;&#29702;&#20219;&#24847;&#30340;&#37319;&#26679;&#27604;&#29575;&#21644;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;SCL&#21253;&#21547;&#19968;&#20010;&#21452;&#22495;&#25439;&#22833;&#21644;&#19968;&#20010;&#22235;&#38454;&#27573;&#24674;&#22797;&#31574;&#30053;&#12290;&#21069;&#32773;&#40723;&#21169;&#20004;&#20010;&#27979;&#37327;&#37096;&#20998;&#30340;&#20132;&#21449;&#19968;&#33268;&#24615;&#20197;&#21450;&#37319;&#26679;-&#37325;&#26500;&#24490;&#29615;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#26368;&#22823;&#21270;&#25968;&#25454;/&#20449;&#24687;&#21033;&#29992;&#29575;&#12290;&#21518;&#32773;&#21487;&#20197;&#36880;&#27493;&#21033;&#29992;&#22806;&#37096;&#27979;&#37327;&#20013;&#30340;&#24120;&#35265;&#20449;&#21495;&#20808;&#39564;&#21644;&#27979;&#35797;&#26679;&#26412;&#20197;&#21450;&#23398;&#20064;&#30340;NN&#30340;&#20869;&#37096;&#29305;&#24449;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressed sensing (CS) is a promising tool for reducing sampling costs. Current deep neural network (NN)-based CS methods face challenges in collecting labeled measurement-ground truth (GT) data and generalizing to real applications. This paper proposes a novel $\mathbf{S}$elf-supervised s$\mathbf{C}$alable deep CS method, comprising a $\mathbf{L}$earning scheme called $\mathbf{SCL}$ and a family of $\mathbf{Net}$works named $\mathbf{SCNet}$, which does not require GT and can handle arbitrary sampling ratios and matrices once trained on a partial measurement set. Our SCL contains a dual-domain loss and a four-stage recovery strategy. The former encourages a cross-consistency on two measurement parts and a sampling-reconstruction cycle-consistency regarding arbitrary ratios and matrices to maximize data/information utilization. The latter can progressively leverage common signal prior in external measurements and internal characteristics of test samples and learned NNs to improve accur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Detectron2&#24211;&#20013;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#23391;&#21152;&#25289;&#35821;&#25991;&#26723;&#36827;&#34892;&#24067;&#23616;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#30740;&#31350;&#35752;&#35770;&#20102;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.13769</link><description>&lt;p&gt;
&#20351;&#29992;Detectron2&#36827;&#34892;&#23391;&#21152;&#25289;&#35821;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Bengali Document Layout Analysis with Detectron2. (arXiv:2308.13769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Detectron2&#24211;&#20013;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#23391;&#21152;&#25289;&#35821;&#25991;&#26723;&#36827;&#34892;&#24067;&#23616;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#30740;&#31350;&#35752;&#35770;&#20102;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#25968;&#23383;&#21270;&#23545;&#20110;&#20445;&#25252;&#21382;&#21490;&#35760;&#24405;&#12289;&#39640;&#25928;&#30340;&#25991;&#26723;&#31649;&#29702;&#21644;&#25512;&#36827;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#65288;DLA&#65289;&#28041;&#21450;&#23558;&#25991;&#26723;&#20998;&#21106;&#20026;&#25991;&#26412;&#26694;&#12289;&#27573;&#33853;&#12289;&#22270;&#20687;&#21644;&#34920;&#26684;&#31561;&#26377;&#24847;&#20041;&#30340;&#21333;&#20301;&#12290;&#38754;&#23545;&#21508;&#31181;&#24067;&#23616;&#12289;&#21382;&#21490;&#25991;&#20214;&#21644;&#23391;&#21152;&#25289;&#35821;&#31561;&#29305;&#27530;&#33050;&#26412;&#30340;&#25361;&#25112;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#23391;&#21152;&#25289;DLA&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;Detectron2&#24211;&#20013;&#30340;&#20808;&#36827;&#30340;Mask R-CNN&#27169;&#22411;&#25552;&#39640;&#20102;&#23391;&#21152;&#25289;&#35821;&#25991;&#26723;&#30340;DLA&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324;&#19977;&#20010;&#21464;&#31181;&#65306;Mask R-CNN R-50&#12289;R-101&#21644;X-101&#65292;&#22312;BaDLAD&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;PubLayNet&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#21644;&#19981;&#20351;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20154;&#24037;&#27880;&#37322;&#30340;&#22235;&#20010;&#31867;&#21035;&#30340;&#23391;&#21152;&#25289;&#35821;&#25991;&#26723;&#65306;&#25991;&#26412;&#26694;&#12289;&#27573;&#33853;&#12289;&#22270;&#20687;&#21644;&#34920;&#26684;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#20934;&#30830;&#20998;&#21106;&#23391;&#21152;&#25289;&#35821;&#25991;&#26723;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#20197;&#21450;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document digitization is vital for preserving historical records, efficient document management, and advancing OCR (Optical Character Recognition) research. Document Layout Analysis (DLA) involves segmenting documents into meaningful units like text boxes, paragraphs, images, and tables. Challenges arise when dealing with diverse layouts, historical documents, and unique scripts like Bengali, hindered by the lack of comprehensive Bengali DLA datasets. We improved the accuracy of the DLA model for Bengali documents by utilizing advanced Mask R-CNN models available in the Detectron2 library. Our evaluation involved three variants: Mask R-CNN R-50, R-101, and X-101, both with and without pretrained weights from PubLayNet, on the BaDLAD dataset, which contains human-annotated Bengali documents in four categories: text boxes, paragraphs, images, and tables. Results show the effectiveness of these models in accurately segmenting Bengali documents. We discuss speed-accuracy tradeoffs and unde
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#38454;&#27573;&#20248;&#21270;&#25216;&#26415;&#65292;&#20351;&#29992;&#23545;&#25239;&#24615;&#24494;&#35843;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36845;&#20195;&#30340;&#25552;&#31034;&#21644;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#21644;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2308.13768</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25932;&#23545;&#24494;&#35843;&#65306;&#38024;&#23545;&#38382;&#39064;&#20869;&#23481;&#29983;&#25104;&#21644;&#26816;&#27979;&#30340;&#36845;&#20195;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content. (arXiv:2308.13768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#38454;&#27573;&#20248;&#21270;&#25216;&#26415;&#65292;&#20351;&#29992;&#23545;&#25239;&#24615;&#24494;&#35843;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36845;&#20195;&#30340;&#25552;&#31034;&#21644;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#25345;&#32493;&#30340;&#25913;&#36827;&#21644;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#38454;&#27573;&#20248;&#21270;&#25216;&#26415;&#65292;&#20351;&#29992;&#23545;&#25239;&#24615;&#24494;&#35843;&#26469;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23545;&#25239;&#27169;&#22411;&#21644;&#21028;&#21035;&#27169;&#22411;&#20004;&#20010;&#38454;&#27573;&#65292;&#23545;&#25239;&#27169;&#22411;&#34987;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#28508;&#22312;&#26377;&#23475;&#25552;&#31034;&#65292;&#32780;&#21028;&#21035;&#27169;&#22411;&#21017;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26469;&#35782;&#21035;&#36825;&#20123;&#25552;&#31034;&#12290;&#36890;&#36807;&#23545;&#25239;&#24490;&#29615;&#65292;&#20004;&#20010;&#27169;&#22411;&#22312;&#25552;&#31034;&#38454;&#27573;&#20105;&#30456;&#36229;&#36234;&#23545;&#26041;&#65292;&#29983;&#25104;&#21253;&#21547;&#20016;&#23500;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#29992;&#20110;&#24494;&#35843;&#12290;&#36825;&#31181;&#36845;&#20195;&#24212;&#29992;&#25552;&#31034;&#21644;&#24494;&#35843;&#30340;&#26041;&#27861;&#20351;&#24471;&#25345;&#32493;&#30340;&#25913;&#36827;&#21644;&#24615;&#33021;&#25552;&#21319;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19968;&#20010;&#21253;&#21547;GPT-4&#26410;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#25552;&#31034;&#21644;&#19968;&#20123;&#26377;&#20105;&#35758;&#20294;&#26080;&#38382;&#39064;&#30340;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#35780;&#20272;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#22312;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#21028;&#21035;&#27169;&#22411;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#26377;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;SamDSK&#65292;&#32467;&#21512;&#20998;&#21106;&#20219;&#24847;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#26041;&#27861;&#21253;&#25324;&#36845;&#20195;&#30340;&#20004;&#20010;&#38454;&#27573;&#65306;&#20998;&#21106;&#27169;&#22411;&#35757;&#32451;&#21644;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25193;&#23637;&#26377;&#26631;&#31614;&#38598;&#12290;&#36890;&#36807;&#23558;&#20998;&#21106;&#24314;&#35758;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#32467;&#21512;&#65292;&#26500;&#24314;&#26080;&#26631;&#31614;&#22270;&#20687;&#30340;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.13759</link><description>&lt;p&gt;
SamDSK: &#32467;&#21512;&#20998;&#21106;&#20219;&#24847;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SamDSK: Combining Segment Anything Model with Domain-Specific Knowledge for Semi-Supervised Learning in Medical Image Segmentation. (arXiv:2308.13759v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;SamDSK&#65292;&#32467;&#21512;&#20998;&#21106;&#20219;&#24847;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#26041;&#27861;&#21253;&#25324;&#36845;&#20195;&#30340;&#20004;&#20010;&#38454;&#27573;&#65306;&#20998;&#21106;&#27169;&#22411;&#35757;&#32451;&#21644;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25193;&#23637;&#26377;&#26631;&#31614;&#38598;&#12290;&#36890;&#36807;&#23558;&#20998;&#21106;&#24314;&#35758;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#32467;&#21512;&#65292;&#26500;&#24314;&#26080;&#26631;&#31614;&#22270;&#20687;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#20998;&#21106;&#21508;&#31181;&#23545;&#35937;&#30340;&#33021;&#21147;&#65292;&#26159;&#21508;&#31181;&#19979;&#28216;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#22810;&#21151;&#33021;&#24863;&#30693;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#36890;&#24120;&#20381;&#36182;&#20110;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65288;DSK&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#65288;&#21363;SAM&#65289;&#19982;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21487;&#38752;&#22320;&#21033;&#29992;&#26080;&#26631;&#31614;&#22270;&#20687;&#26500;&#24314;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#26159;&#36845;&#20195;&#30340;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;&#65288;1&#65289;&#20998;&#21106;&#27169;&#22411;&#35757;&#32451;&#65307;&#65288;2&#65289;&#20351;&#29992;&#35757;&#32451;&#22909;&#30340;&#20998;&#21106;&#27169;&#22411;&#12289;&#26080;&#26631;&#31614;&#38598;&#12289;SAM&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25193;&#23637;&#26377;&#26631;&#31614;&#38598;&#12290;&#36825;&#20004;&#20010;&#38454;&#27573;&#37325;&#22797;&#36827;&#34892;&#65292;&#30452;&#21040;&#26080;&#27861;&#20877;&#28155;&#21152;&#26679;&#26412;&#21040;&#26377;&#26631;&#31614;&#38598;&#20026;&#27490;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#23558;SAM&#29983;&#25104;&#30340;&#20998;&#21106;&#24314;&#35758;&#19982;&#20687;&#32032;&#32423;&#21644;&#22270;&#20687;&#32423;&#30340;DSK&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#26080;&#26631;&#31614;&#22270;&#20687;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) exhibits a capability to segment a wide array of objects in natural images, serving as a versatile perceptual tool for various downstream image segmentation tasks. In contrast, medical image segmentation tasks often rely on domain-specific knowledge (DSK). In this paper, we propose a novel method that combines the segmentation foundation model (i.e., SAM) with domain-specific knowledge for reliable utilization of unlabeled images in building a medical image segmentation model. Our new method is iterative and consists of two main stages: (1) segmentation model training; (2) expanding the labeled set by using the trained segmentation model, an unlabeled set, SAM, and domain-specific knowledge. These two stages are repeated until no more samples are added to the labeled set. A novel optimal-matching-based method is developed for combining the SAM-generated segmentation proposals and pixel-level and image-level DSK for constructing annotations of unlabeled 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PE-MED&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#20132;&#20114;&#24335;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#25104;&#21151;&#29575;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#33258;&#24490;&#29615;&#31574;&#30053;&#21644;&#25552;&#31034;&#27880;&#24847;&#21147;&#23398;&#20064;&#27169;&#22359;&#26469;&#25913;&#21892;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#20449;&#24687;&#30340;&#21033;&#29992;&#65292;&#20174;&#32780;&#38450;&#27490;&#19981;&#21033;&#24773;&#20917;&#30340;&#21457;&#29983;&#65292;&#24182;&#25552;&#39640;&#20998;&#21106;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13746</link><description>&lt;p&gt;
PE-MED: &#25552;&#39640;&#20132;&#20114;&#24335;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#25552;&#31034;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
PE-MED: Prompt Enhancement for Interactive Medical Image Segmentation. (arXiv:2308.13746v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PE-MED&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#20132;&#20114;&#24335;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#25104;&#21151;&#29575;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#33258;&#24490;&#29615;&#31574;&#30053;&#21644;&#25552;&#31034;&#27880;&#24847;&#21147;&#23398;&#20064;&#27169;&#22359;&#26469;&#25913;&#21892;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#20449;&#24687;&#30340;&#21033;&#29992;&#65292;&#20174;&#32780;&#38450;&#27490;&#19981;&#21033;&#24773;&#20917;&#30340;&#21457;&#29983;&#65292;&#24182;&#25552;&#39640;&#20998;&#21106;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26159;&#25351;&#36890;&#36807;&#29992;&#25143;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#65288;&#20363;&#22914;&#65292;&#28857;&#20987;&#65289;&#20934;&#30830;&#22320;&#20998;&#21106;&#20986;&#24863;&#20852;&#36259;&#30340;&#30446;&#26631;&#12290;&#36817;&#24180;&#26469;&#65292;&#23427;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#19981;&#22826;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#27604;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#20998;&#21106;&#26356;&#21152;&#28789;&#27963;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#36824;&#27809;&#26377;&#20805;&#20998;&#25506;&#32034;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#28857;&#65289;&#65292;&#21253;&#25324;&#22312;&#19968;&#20010;&#20132;&#20114;&#20013;&#25366;&#25496;&#30340;&#30693;&#35782;&#20197;&#21450;&#22810;&#20010;&#20132;&#20114;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35013;&#22791;&#26377;&#25552;&#31034;&#22686;&#24378;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#21517;&#20026;PE-MED&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#24490;&#29615;&#31574;&#30053;&#65292;&#22522;&#20110;&#31532;&#19968;&#20010;&#25552;&#31034;&#29983;&#25104;&#28201;&#26262;&#30340;&#21021;&#22987;&#20998;&#21106;&#32467;&#26524;&#12290;&#23427;&#21487;&#20197;&#38450;&#27490;&#20986;&#29616;&#39640;&#24230;&#19981;&#21033;&#30340;&#24773;&#20917;&#65292;&#27604;&#22914;&#22312;&#31532;&#19968;&#27425;&#20132;&#20114;&#21518;&#36935;&#21040;&#31354;&#30333;&#36974;&#32617;&#20316;&#20026;&#21021;&#22987;&#36755;&#20837;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#27880;&#24847;&#21147;&#23398;&#20064;&#27169;&#22359;&#65288;PALM&#65289;&#65292;&#29992;&#20110;&#25366;&#25496;&#26377;&#29992;&#30340;&#25552;&#31034;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive medical image segmentation refers to the accurate segmentation of the target of interest through interaction (e.g., click) between the user and the image. It has been widely studied in recent years as it is less dependent on abundant annotated data and more flexible than fully automated segmentation. However, current studies have not fully explored user-provided prompt information (e.g., points), including the knowledge mined in one interaction, and the relationship between multiple interactions. Thus, in this paper, we introduce a novel framework equipped with prompt enhancement, called PE-MED, for interactive medical image segmentation. First, we introduce a Self-Loop strategy to generate warm initial segmentation results based on the first prompt. It can prevent the highly unfavorable scenarios, such as encountering a blank mask as the initial input after the first interaction. Second, we propose a novel Prompt Attention Learning Module (PALM) to mine useful prompt infor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#30701;&#26399;&#35843;&#25972;MCMC&#38142;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20248;&#21270;&#26356;&#32039;&#30340;&#21464;&#20998;&#36793;&#30028;&#30340;&#21516;&#26102;&#65292;&#36866;&#24212;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#25552;&#26696;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20351;&#27169;&#22411;&#24471;&#21040;&#26356;&#39640;&#30340;&#20445;&#30041;&#23545;&#25968;&#20284;&#28982;&#21644;&#25913;&#36827;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13731</link><description>&lt;p&gt;
&#36890;&#36807;MCMC&#36895;&#24230;&#24230;&#37327;&#23398;&#20064;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning variational autoencoders via MCMC speed measures. (arXiv:2308.13731v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#30701;&#26399;&#35843;&#25972;MCMC&#38142;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20248;&#21270;&#26356;&#32039;&#30340;&#21464;&#20998;&#36793;&#30028;&#30340;&#21516;&#26102;&#65292;&#36866;&#24212;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#25552;&#26696;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20351;&#27169;&#22411;&#24471;&#21040;&#26356;&#39640;&#30340;&#20445;&#30041;&#23545;&#25968;&#20284;&#28982;&#21644;&#25913;&#36827;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#19979;&#30028;&#65288;ELBO&#65289;&#26469;&#26377;&#25928;&#35757;&#32451;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#32039;&#30340;&#21464;&#20998;&#36793;&#30028;&#21644;&#26356;&#39640;&#30340;&#29983;&#25104;&#24615;&#33021;&#65292;&#25913;&#36827;&#21464;&#20998;&#20998;&#24067;&#30340;&#34920;&#36798;&#33021;&#21147;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#26500;&#24314;&#20102;&#21464;&#20998;&#23494;&#24230;&#65292;&#20294;&#38024;&#23545;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#35843;&#25972;&#25552;&#26696;&#20998;&#24067;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#30701;&#26399;&#35843;&#25972;Metropolis-adjusted Langevin&#65288;MALA&#65289;&#25110;Hamiltonian Monte Carlo&#65288;HMC&#65289;&#38142;&#30340;&#26041;&#27861;&#65292;&#24182;&#20248;&#21270;&#26356;&#32039;&#30340;&#21464;&#20998;&#36793;&#30028;&#20197;&#33719;&#24471;&#23545;&#25968;&#20284;&#28982;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#26356;&#39640;&#30340;&#20445;&#30041;&#23545;&#25968;&#20284;&#28982;&#20197;&#21450;&#25913;&#36827;&#30340;&#29983;&#25104;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#38544;&#24335;&#21464;&#20998;&#23494;&#24230;&#33021;&#22815;&#36866;&#24212;&#28508;&#22312;&#23618;&#27425;&#34920;&#31034;&#30340;&#22797;&#26434;&#21518;&#39564;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are popular likelihood-based generative models which can be efficiently trained by maximizing an Evidence Lower Bound (ELBO). There has been much progress in improving the expressiveness of the variational distribution to obtain tighter variational bounds and increased generative performance. Whilst previous work has leveraged Markov chain Monte Carlo (MCMC) methods for the construction of variational densities, gradient-based methods for adapting the proposal distributions for deep latent variable models have received less attention. This work suggests an entropy-based adaptation for a short-run Metropolis-adjusted Langevin (MALA) or Hamiltonian Monte Carlo (HMC) chain while optimising a tighter variational bound to the log-evidence. Experiments show that this approach yields higher held-out log-likelihoods as well as improved generative metrics. Our implicit variational density can adapt to complicated posterior geometries of latent hierarchical repres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Muffin&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#29616;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#22810;&#32500;&#24230;&#20154;&#24037;&#26234;&#33021;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#39318;&#20808;&#25581;&#31034;&#20102;&#19981;&#21516;&#19981;&#20844;&#24179;&#23646;&#24615;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#32500;&#24230;&#20844;&#24179;&#24615;&#26694;&#26550;Muffin&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#33258;&#21160;&#24037;&#20855;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13730</link><description>&lt;p&gt;
Muffin:&#36890;&#36807;&#25972;&#21512;&#29616;&#25104;&#27169;&#22411;&#23454;&#29616;&#22810;&#32500;&#24230;&#20154;&#24037;&#26234;&#33021;&#20844;&#24179;&#24615;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Muffin: A Framework Toward Multi-Dimension AI Fairness by Uniting Off-the-Shelf Models. (arXiv:2308.13730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Muffin&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#29616;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#22810;&#32500;&#24230;&#20154;&#24037;&#26234;&#33021;&#20844;&#24179;&#24615;&#12290;&#30740;&#31350;&#39318;&#20808;&#25581;&#31034;&#20102;&#19981;&#21516;&#19981;&#20844;&#24179;&#23646;&#24615;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#32500;&#24230;&#20844;&#24179;&#24615;&#26694;&#26550;Muffin&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#33258;&#21160;&#24037;&#20855;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20844;&#24179;&#24615;&#65288;&#21363;&#20559;&#35265;&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#19968;&#20010;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#33021;&#20250;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#65292;&#22914;&#26524;&#23545;&#26497;&#31471;&#24773;&#20917;&#65288;&#22914;&#26497;&#31471;&#22825;&#27668;&#65289;&#19981;&#33021;&#20844;&#27491;&#23545;&#24453;&#65307;&#25110;&#32773;&#22914;&#26524;AI&#27169;&#22411;&#35823;&#35786;&#26576;&#19968;&#32676;&#20307;&#65288;&#22914;&#26837;&#33394;&#21644;&#40657;&#33394;&#30382;&#32932;&#65289;&#65292;&#23558;&#23548;&#33268;&#21307;&#30103;&#24046;&#24322;&#12290;&#36817;&#24180;&#26469;&#65292;&#26377;&#35768;&#22810;&#20851;&#20110;&#35299;&#20915;&#19981;&#20844;&#24179;&#24615;&#30340;&#30740;&#31350;&#24037;&#20316;&#28044;&#29616;&#20986;&#26469;&#65292;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#30340;&#19981;&#20844;&#24179;&#23646;&#24615;&#65292;&#22914;&#32932;&#33394;&#65307;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#22810;&#20010;&#23646;&#24615;&#65292;&#20854;&#20013;&#19981;&#20844;&#24179;&#24615;&#21487;&#20197;&#23384;&#22312;&#20110;&#22810;&#20010;&#23646;&#24615;&#20013;&#65292;&#31216;&#20026;&#8220;&#22810;&#32500;&#24230;&#20844;&#24179;&#24615;&#8221;&#12290;&#26412;&#25991;&#39318;&#20808;&#25581;&#31034;&#20102;&#19981;&#21516;&#19981;&#20844;&#24179;&#23646;&#24615;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#21363;&#22312;&#19968;&#20010;&#23646;&#24615;&#19978;&#20248;&#21270;&#20844;&#24179;&#24615;&#20250;&#23548;&#33268;&#20854;&#20182;&#23646;&#24615;&#30340;&#23849;&#28291;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#32500;&#24230;&#20844;&#24179;&#24615;&#26694;&#26550;Muffin&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#33258;&#21160;&#24037;&#20855;&#26469;&#25972;&#21512;&#29616;&#25104;&#27169;&#22411;&#26469;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model fairness (a.k.a., bias) has become one of the most critical problems in a wide range of AI applications. An unfair model in autonomous driving may cause a traffic accident if corner cases (e.g., extreme weather) cannot be fairly regarded; or it will incur healthcare disparities if the AI model misdiagnoses a certain group of people (e.g., brown and black skin). In recent years, there have been emerging research works on addressing unfairness, and they mainly focus on a single unfair attribute, like skin tone; however, real-world data commonly have multiple attributes, among which unfairness can exist in more than one attribute, called 'multi-dimensional fairness'. In this paper, we first reveal a strong correlation between the different unfair attributes, i.e., optimizing fairness on one attribute will lead to the collapse of others. Then, we propose a novel Multi-Dimension Fairness framework, namely Muffin, which includes an automatic tool to unite off-the-shelf models to improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#31639;&#27861;&#30340;&#20248;&#21270;&#29256;&#26412;&#65288;OPT-DMD&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;ExB&#31561;&#31163;&#23376;&#20307;&#30340;&#25968;&#25454;&#39537;&#21160;&#38477;&#38454;&#24314;&#27169;&#65292;&#24182;&#35780;&#20272;&#20102;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#29289;&#29702;&#21442;&#25968;&#19979;&#39044;&#27979;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13727</link><description>&lt;p&gt;
&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#29992;&#20110;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;ExB&#31561;&#31163;&#23376;&#20307;&#20998;&#26512;&#21644;&#38477;&#38454;&#24314;&#27169;&#65306;II. &#21160;&#21147;&#23398;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic Mode Decomposition for data-driven analysis and reduced-order modelling of ExB plasmas: II. dynamics forecasting. (arXiv:2308.13727v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#31639;&#27861;&#30340;&#20248;&#21270;&#29256;&#26412;&#65288;OPT-DMD&#65289;&#65292;&#29992;&#20110;&#20998;&#26512;ExB&#31561;&#31163;&#23376;&#20307;&#30340;&#25968;&#25454;&#39537;&#21160;&#38477;&#38454;&#24314;&#27169;&#65292;&#24182;&#35780;&#20272;&#20102;&#35813;&#31639;&#27861;&#22312;&#19981;&#21516;&#29289;&#29702;&#21442;&#25968;&#19979;&#39044;&#27979;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31532;&#19968;&#37096;&#20998;&#30340;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#37327;&#25237;&#24433;&#20248;&#21270;&#30340;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#31639;&#27861;&#30340;&#21464;&#31181;&#65292;&#31216;&#20026;&#20248;&#21270;DMD&#65288;OPT-DMD&#65289;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#29289;&#29702;&#21442;&#25968;&#30340;ExB&#27169;&#25311;&#37197;&#32622;&#20013;&#40065;&#26834;&#22320;&#35782;&#21035;&#25968;&#25454;&#20013;&#28508;&#22312;&#30340;&#26102;&#31354;&#30456;&#24178;&#27169;&#24335;&#12290;&#30001;&#20110;OPT-DMD&#21487;&#20197;&#36890;&#36807;&#26500;&#36896;&#34987;&#38480;&#21046;&#20026;&#20135;&#29983;&#31283;&#23450;&#30340;&#38477;&#38454;&#27169;&#22411;&#65288;ROMs&#65289;&#65292;&#22240;&#27492;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;OPT-DMD&#30340;&#24212;&#29992;&#65292;&#24182;&#30740;&#31350;&#20102;&#35813;&#31639;&#27861;&#20013;&#32447;&#24615;ROMs&#22312;&#31867;&#20284;&#20110;Hall&#25512;&#21147;&#22120;&#30340;&#24452;&#21521;-&#26041;&#20301;&#21644;&#36724;&#21521;-&#26041;&#20301;&#25130;&#38754;&#20197;&#21450;&#27599;&#20010;&#27979;&#35797;&#26696;&#20363;&#30340;&#19968;&#31995;&#21015;&#27169;&#25311;&#21442;&#25968;&#19979;&#39044;&#27979;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#30340;&#33021;&#21147;&#12290;OPT-DMD ROM&#30340;&#39044;&#27979;&#33021;&#21147;&#20027;&#35201;&#36890;&#36807;&#30701;&#26399;&#21160;&#21147;&#23398;&#39044;&#27979;&#36827;&#34892;&#35780;&#20272;&#65292;&#25110;&#32773;&#25442;&#21477;&#35805;&#35828;&#65292;&#29992;&#20110;&#35757;&#32451;-&#27979;&#35797;&#25968;&#25454;&#27604;&#22823;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In part I of the article, we demonstrated that a variant of the Dynamic Mode Decomposition (DMD) algorithm based on variable projection optimization, called Optimized DMD (OPT-DMD), enables a robust identification of the dominant spatiotemporally coherent modes underlying the data across various test cases representing different physical parameters in an ExB simulation configuration. As the OPT-DMD can be constrained to produce stable reduced-order models (ROMs) by construction, in this paper, we extend the application of the OPT-DMD and investigate the capabilities of the linear ROM from this algorithm toward forecasting in time of the plasma dynamics in configurations representative of the radial-azimuthal and axial-azimuthal cross-sections of a Hall thruster and over a range of simulation parameters in each test case. The predictive capacity of the OPT-DMD ROM is assessed primarily in terms of short-term dynamics forecast or, in other words, for large ratios of training-to-test data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;&#65288;DMD&#65289;&#31639;&#27861;&#22312;ExB&#31561;&#31163;&#23376;&#20307;&#21160;&#24577;&#20998;&#26512;&#21644;&#38477;&#38454;&#24314;&#27169;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;DMD&#22312;&#25552;&#21462;&#39640;&#32500;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#26102;&#31354;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13726</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;&#29992;&#20110;ExB&#31561;&#31163;&#23376;&#20307;&#30340;&#20998;&#26512;&#21644;&#38477;&#38454;&#24314;&#27169;&#65306;I.&#25552;&#21462;&#26102;&#31354;&#21327;&#21516;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Dynamic Mode Decomposition for data-driven analysis and reduced-order modelling of ExB plasmas: I. Extraction of spatiotemporally coherent patterns. (arXiv:2308.13726v1 [physics.plasm-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;&#65288;DMD&#65289;&#31639;&#27861;&#22312;ExB&#31561;&#31163;&#23376;&#20307;&#21160;&#24577;&#20998;&#26512;&#21644;&#38477;&#38454;&#24314;&#27169;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;DMD&#22312;&#25552;&#21462;&#39640;&#32500;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#26102;&#31354;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#20004;&#37096;&#20998;&#30340;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21160;&#24577;&#27169;&#24335;&#20998;&#35299;&#65288;DMD&#65289;&#31639;&#27861;&#22312;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20132;&#21449;&#22330;ExB&#37197;&#32622;&#31561;&#31163;&#23376;&#20307;&#21160;&#24577;&#20998;&#26512;&#21644;&#38477;&#38454;&#24314;&#27169;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;DMD&#31639;&#27861;&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040;&#25551;&#36848;&#25968;&#25454;&#20013;&#26102;&#31354;&#21327;&#21516;&#32467;&#26500;&#65288;&#27169;&#24335;&#65289;&#30340;&#26102;&#38388;&#28436;&#21270;&#30340;&#26368;&#20339;&#25311;&#21512;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#39640;&#25928;&#38477;&#38454;PIC&#26041;&#26696;&#30340;&#31890;&#23376;-&#32593;&#26684;&#65288;PIC&#65289;&#20195;&#30721;&#29983;&#25104;&#20102;&#22823;&#37327;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#65292;&#24182;&#23558;DMD&#24212;&#29992;&#20110;&#36825;&#20123;&#25968;&#25454;&#12290;&#22312;&#26412;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;DMD&#30340;&#27010;&#24565;&#21450;&#20854;&#22522;&#30784;&#30340;&#27491;&#20132;&#27169;&#24577;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#20171;&#32461;&#20102;DMD&#30340;&#20004;&#31181;&#20027;&#35201;&#21464;&#20307;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#21644;&#35752;&#35770;&#20102;DMD&#24212;&#29992;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#22312;&#21508;&#31181;&#27169;&#25311;&#26465;&#20214;&#19979;&#65292;&#20174;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#20013;&#35782;&#21035;&#21644;&#25552;&#21462;&#20027;&#35201;&#26102;&#31354;&#27169;&#24335;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;DMD&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#21462;&#39640;&#32500;&#31561;&#31163;&#23376;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#26102;&#31354;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this two-part article, we evaluate the utility and the generalizability of the Dynamic Mode Decomposition (DMD) algorithm for data-driven analysis and reduced-order modelling of plasma dynamics in cross-field ExB configurations. The DMD algorithm is an interpretable data-driven method that finds a best-fit linear model describing the time evolution of spatiotemporally coherent structures (patterns) in data. We have applied the DMD to extensive high-fidelity datasets generated using a particle-in-cell (PIC) code based on a cost-efficient reduced-order PIC scheme. In this part, we first provide an overview of the concept of DMD and its underpinning Proper Orthogonal and Singular Value Decomposition methods. Two of the main DMD variants are next introduced. We then present and discuss the results of the DMD application in terms of the identification and extraction of the dominant spatiotemporal modes from high-fidelity data over a range of simulation conditions. We demonstrate that the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Time-to-Pattern&#30340;&#26102;&#38388;&#24207;&#21015;&#25688;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#30340;&#20449;&#24687;&#23884;&#20837;&#65292;&#25214;&#21040;&#19968;&#32452;&#19981;&#21516;&#30340;&#27169;&#24335;&#20197;&#32534;&#30721;&#26368;&#26174;&#33879;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.13722</link><description>&lt;p&gt;
Time-to-Pattern: &#20449;&#24687;&#35770;&#26080;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#21487;&#25193;&#23637;&#30340;&#26102;&#38388;&#24207;&#21015;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Time-to-Pattern: Information-Theoretic Unsupervised Learning for Scalable Time Series Summarization. (arXiv:2308.13722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13722
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Time-to-Pattern&#30340;&#26102;&#38388;&#24207;&#21015;&#25688;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#30340;&#20449;&#24687;&#23884;&#20837;&#65292;&#25214;&#21040;&#19968;&#32452;&#19981;&#21516;&#30340;&#27169;&#24335;&#20197;&#32534;&#30721;&#26368;&#26174;&#33879;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#25688;&#35201;&#26159;&#20174;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#21487;&#35299;&#37322;&#21644;&#20195;&#34920;&#24615;&#23376;&#38598;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#25688;&#35201;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#19968;&#32452;&#25163;&#21160;&#35774;&#35745;&#30340;&#30456;&#20284;&#24230;&#20989;&#25968;&#26469;&#25628;&#32034;&#37325;&#22797;&#23376;&#24207;&#21015;&#20197;&#25688;&#35201;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#26469;&#33258;&#35814;&#23613;&#25628;&#32034;&#21644;&#21551;&#21457;&#24335;&#23450;&#20041;&#24207;&#21015;&#30456;&#20284;&#24230;&#30340;&#38480;&#21046;&#12290;&#36825;&#20123;&#26041;&#27861;&#24433;&#21709;&#20102;&#29983;&#25104;&#30340;&#25968;&#25454;&#25688;&#35201;&#30340;&#22810;&#26679;&#24615;&#21644;&#20840;&#38754;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26102;&#38388;&#24207;&#21015;&#25688;&#35201;&#26041;&#27861;&#65292;&#31216;&#20026;Time-to-Pattern&#65288;T2P&#65289;&#65292;&#23427;&#26088;&#22312;&#25214;&#21040;&#19968;&#32452;&#19981;&#21516;&#30340;&#27169;&#24335;&#65292;&#20849;&#21516;&#32534;&#30721;&#26368;&#26174;&#33879;&#30340;&#20449;&#24687;&#65292;&#36981;&#24490;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30340;&#27010;&#24565;&#12290;T2P&#26159;&#19968;&#20010;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#22312;&#35774;&#35745;&#25104;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#19978;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#30340;&#20449;&#24687;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#35777;&#26126;T2P&#21487;&#20197;&#25552;&#39640;&#25688;&#35201;&#36136;&#37327;&#24182;&#20445;&#25345;&#22810;&#26679;&#24615;&#21644;&#20840;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data summarization is the process of generating interpretable and representative subsets from a dataset. Existing time series summarization approaches often search for recurring subsequences using a set of manually devised similarity functions to summarize the data. However, such approaches are fraught with limitations stemming from an exhaustive search coupled with a heuristic definition of series similarity. Such approaches affect the diversity and comprehensiveness of the generated data summaries. To mitigate these limitations, we introduce an approach to time series summarization, called Time-to-Pattern (T2P), which aims to find a set of diverse patterns that together encode the most salient information, following the notion of minimum description length. T2P is implemented as a deep generative model that learns informative embeddings of the discrete time series on a latent space specifically designed to be interpretable. Our synthetic and real-world experiments reveal that T2P dis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#32852;&#37030;&#23398;&#20064;&#19982;&#20256;&#32479;&#38598;&#20013;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#38544;&#31169;&#21644;&#25968;&#25454;&#20998;&#24067;&#22788;&#29702;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25552;&#20379;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#24403;&#21069;&#25361;&#25112;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.13714</link><description>&lt;p&gt;
&#25581;&#31034;&#32852;&#37030;&#23398;&#20064;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#20248;&#21183;&#19982;&#25361;&#25112;&#65306;&#19968;&#20010;&#32508;&#36848;&#25991;&#29486;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Uncovering Promises and Challenges of Federated Learning to Detect Cardiovascular Diseases: A Scoping Literature Review. (arXiv:2308.13714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13714
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#29616;&#29366;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;&#32852;&#37030;&#23398;&#20064;&#19982;&#20256;&#32479;&#38598;&#20013;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#38544;&#31169;&#21644;&#25968;&#25454;&#20998;&#24067;&#22788;&#29702;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25552;&#20379;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#24403;&#21069;&#25361;&#25112;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#26159;&#20840;&#29699;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#26089;&#26399;&#26816;&#27979;&#21487;&#26174;&#33879;&#25913;&#21892;&#24739;&#32773;&#30340;&#39044;&#21518;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#26089;&#26399;&#35786;&#26029;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#20294;&#20854;&#24615;&#33021;&#21463;&#38480;&#20110;&#29992;&#20110;&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#21307;&#30103;&#20445;&#25252;&#38544;&#31169;&#30340;&#38544;&#31169;&#38382;&#39064;&#20351;&#24471;&#33719;&#21462;&#20934;&#30830;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#26356;&#21152;&#22256;&#38590;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#22312;&#19981;&#25439;&#23475;&#20010;&#20307;&#25968;&#25454;&#25152;&#26377;&#32773;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#25968;&#25454;&#12290;&#26412;&#32508;&#36848;&#25991;&#29486;&#25552;&#20379;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#29616;&#29366;&#27010;&#36848;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#22312;&#21508;&#31181;&#35770;&#25991;&#20013;&#25552;&#20986;&#30340;&#19981;&#21516;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#38544;&#31169;&#21644;&#25968;&#25454;&#20998;&#24067;&#22788;&#29702;&#33021;&#21147;&#31561;&#26041;&#38754;&#31361;&#20986;&#20102;&#24046;&#24322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#32852;&#37030;&#23398;&#20064;&#30446;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular diseases (CVD) are the leading cause of death globally, and early detection can significantly improve outcomes for patients. Machine learning (ML) models can help diagnose CVDs early, but their performance is limited by the data available for model training. Privacy concerns in healthcare make it harder to acquire data to train accurate ML models. Federated learning (FL) is an emerging approach to machine learning that allows models to be trained on data from multiple sources without compromising the privacy of the individual data owners. This survey paper provides an overview of the current state-of-the-art in FL for CVD detection. We review the different FL models proposed in various papers and discuss their advantages and challenges. We also compare FL with traditional centralized learning approaches and highlight the differences in terms of model accuracy, privacy, and data distribution handling capacity. Finally, we provide a critical analysis of FL's current challe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21097;&#20313;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;RDDM&#65289;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#27531;&#24046;&#26469;&#34920;&#31034;&#20174;&#30446;&#26631;&#22495;&#21040;&#36755;&#20837;&#22495;&#30340;&#26041;&#21521;&#24615;&#25193;&#25955;&#65292;&#24182;&#21516;&#26102;&#20272;&#35745;&#22122;&#22768;&#26469;&#32771;&#34385;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2308.13712</link><description>&lt;p&gt;
&#21097;&#20313;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Residual Denoising Diffusion Models. (arXiv:2308.13712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13712
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21097;&#20313;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;RDDM&#65289;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#27531;&#24046;&#26469;&#34920;&#31034;&#20174;&#30446;&#26631;&#22495;&#21040;&#36755;&#20837;&#22495;&#30340;&#26041;&#21521;&#24615;&#25193;&#25955;&#65292;&#24182;&#21516;&#26102;&#20272;&#35745;&#22122;&#22768;&#26469;&#32771;&#34385;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;&#23558;&#36864;&#21270;&#30340;&#36755;&#20837;&#22270;&#20687;&#20316;&#20026;&#26465;&#20214;&#36755;&#20837;&#21040;&#22122;&#22768;&#20272;&#35745;&#32593;&#32476;&#20013;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#36825;&#20010;&#25193;&#25955;&#36807;&#31243;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#22522;&#26412;&#19978;&#26159;&#20174;&#22122;&#22768;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#19988;&#26356;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#29983;&#25104;&#21644;&#24674;&#22797;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21097;&#20313;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;RDDM&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;&#20363;&#22914;DDPM&#25110;DDIM&#65289;&#20165;&#19987;&#27880;&#20110;&#22122;&#22768;&#20272;&#35745;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;RDDM&#39044;&#27979;&#27531;&#24046;&#26469;&#34920;&#31034;&#20174;&#30446;&#26631;&#22495;&#21040;&#36755;&#20837;&#22495;&#30340;&#26041;&#21521;&#24615;&#25193;&#25955;&#65292;&#24182;&#21516;&#26102;&#20272;&#35745;&#22122;&#22768;&#26469;&#32771;&#34385;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#25200;&#21160;&#12290;&#27531;&#24046;&#30340;&#24341;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#37325;&#26032;&#23450;&#20041;&#27491;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#30446;&#26631;&#22270;&#20687;&#36880;&#28176;&#25193;&#25955;&#25104;&#19968;&#20010;&#32431;&#22122;&#22768;&#22270;&#20687;&#25110;&#25658;&#24102;&#22122;&#22768;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#22270;&#20687;&#29983;&#25104;&#21644;&#24674;&#22797;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#36807;&#31243;&#19982;&#30495;&#23454;&#30340;&#30446;&#26631;&#22270;&#20687;&#20998;&#24067;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current diffusion-based image restoration methods feed degraded input images as conditions into the noise estimation network. However, interpreting this diffusion process is challenging since it essentially generates the target image from the noise. To establish a unified and more interpretable model for image generation and restoration, we propose residual denoising diffusion models (RDDM). In contrast to existing diffusion models (e.g., DDPM or DDIM) that focus solely on noise estimation, our RDDM predicts residuals to represent directional diffusion from the target domain to the input domain, while concurrently estimating noise to account for random perturbations in the diffusion process. The introduction of residuals allows us to redefine the forward diffusion process, wherein the target image progressively diffuses into a purely noisy image or a noise-carrying input image, thus unifying image generation and restoration. We demonstrate that our sampling process is consistent with t
&lt;/p&gt;</description></item><item><title>PAITS&#26159;&#38024;&#23545;&#31232;&#30095;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#21644;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;NLP&#21551;&#21457;&#24335;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#21450;&#38543;&#26426;&#25628;&#32034;&#26469;&#25214;&#21040;&#36866;&#21512;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13703</link><description>&lt;p&gt;
PAITS: &#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#21644;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
PAITS: Pretraining and Augmentation for Irregularly-Sampled Time Series. (arXiv:2308.13703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13703
&lt;/p&gt;
&lt;p&gt;
PAITS&#26159;&#38024;&#23545;&#31232;&#30095;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#21644;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;NLP&#21551;&#21457;&#24335;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#21450;&#38543;&#26426;&#25628;&#32034;&#26469;&#25214;&#21040;&#36866;&#21512;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#21453;&#26144;&#20102;&#39034;&#24207;&#20154;&#31867;&#34892;&#20026;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#32463;&#24120;&#26159;&#29420;&#29305;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#31232;&#30095;&#30340;&#65292;&#26102;&#38388;&#21644;&#23454;&#20307;&#19978;&#30340;&#37319;&#26679;&#38750;&#24120;&#19981;&#22343;&#21248;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#21644;&#22686;&#24378;&#26041;&#27861;&#24182;&#19981;&#20855;&#22791;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#30340;&#19987;&#38376;&#35774;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PAITS&#65288;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#35757;&#32451;&#21644;&#22686;&#24378;&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#36866;&#21512;&#31232;&#30095;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#12290;PAITS&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#21512;&#20102;NLP&#21551;&#21457;&#24335;&#39044;&#35757;&#32451;&#20219;&#21153;&#21644;&#22686;&#24378;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#38543;&#26426;&#25628;&#32034;&#26469;&#20026;&#32473;&#23450;&#25968;&#25454;&#38598;&#25214;&#21040;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21463;&#30410;&#20110;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#36873;&#25321;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#25345;&#32493;&#25913;&#36827;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; \url{https://github.com/google-research/google-research/tree/master/irregul} &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world time series data that commonly reflect sequential human behavior are often uniquely irregularly sampled and sparse, with highly nonuniform sampling over time and entities. Yet, commonly-used pretraining and augmentation methods for time series are not specifically designed for such scenarios. In this paper, we present PAITS (Pretraining and Augmentation for Irregularly-sampled Time Series), a framework for identifying suitable pretraining strategies for sparse and irregularly sampled time series datasets. PAITS leverages a novel combination of NLP-inspired pretraining tasks and augmentations, and a random search to identify an effective strategy for a given dataset. We demonstrate that different datasets benefit from different pretraining choices. Compared with prior methods, our approach is better able to consistently improve pretraining across multiple datasets and domains. Our code is available at \url{https://github.com/google-research/google-research/tree/master/irregul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#20826;&#27966;&#39044;&#27979;&#20570;&#27861;&#30340;&#32508;&#21512;&#35843;&#26597;&#21644;&#23454;&#35777;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#25110;&#36229;&#36234;&#20043;&#65292;&#28982;&#32780;&#21364;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.13699</link><description>&lt;p&gt;
Twitter&#19978;&#30340;&#25919;&#20826;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Party Prediction for Twitter. (arXiv:2308.13699v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#20826;&#27966;&#39044;&#27979;&#20570;&#27861;&#30340;&#32508;&#21512;&#35843;&#26597;&#21644;&#23454;&#35777;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#25110;&#36229;&#36234;&#20043;&#65292;&#28982;&#32780;&#21364;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#30740;&#31350;&#27604;&#36739;&#20102;&#26469;&#33258;&#19981;&#21516;&#25919;&#20826;&#30340;&#29992;&#25143;&#30340;&#34892;&#20026;&#12290;&#20316;&#20026;&#22522;&#26412;&#27493;&#39588;&#65292;&#20182;&#20204;&#20351;&#29992;&#39044;&#27979;&#27169;&#22411;&#26469;&#25512;&#26029;&#20182;&#20204;&#30340;&#25919;&#27835;&#27966;&#21035;&#12290;&#36825;&#20010;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#19979;&#28216;&#20998;&#26512;&#30340;&#32467;&#35770;&#65292;&#28982;&#32780;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#36873;&#25321;&#20284;&#20046;&#26159;&#20219;&#24847;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#20826;&#27966;&#39044;&#27979;&#20570;&#27861;&#30340;&#32508;&#21512;&#35843;&#26597;&#21644;&#23454;&#35777;&#27604;&#36739;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#25110;&#36229;&#36234;&#20043;&#65292;&#28982;&#32780;&#21364;&#38656;&#35201;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20826;&#27966;&#39044;&#27979;&#27169;&#22411;&#20381;&#36182;&#20110;&#29992;&#25143;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;&#20363;&#22914;&#25512;&#25991;&#25991;&#26412;&#65289;&#65292;&#20182;&#20204;&#30340;&#20851;&#31995;&#65288;&#20363;&#22914;&#20182;&#20204;&#20851;&#27880;&#35841;&#65289;&#65292;&#25110;&#32773;&#20182;&#20204;&#30340;&#27963;&#21160;&#21644;&#20114;&#21160;&#65288;&#20363;&#22914;&#20182;&#20204;&#21916;&#27426;&#21738;&#20123;&#25512;&#25991;&#65289;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#25152;&#26377;&#36825;&#20123;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#20826;&#27966;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#20449;&#21495;&#24378;&#24230;&#12290;&#26412;&#25991;&#35753;&#20174;&#19994;&#32773;&#21487;&#20197;&#20174;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#20013;&#36827;&#34892;&#36873;&#25321;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#37117;&#26159;---
&lt;/p&gt;
&lt;p&gt;
A large number of studies on social media compare the behaviour of users from different political parties. As a basic step, they employ a predictive model for inferring their political affiliation. The accuracy of this model can change the conclusions of a downstream analysis significantly, yet the choice between different models seems to be made arbitrarily. In this paper, we provide a comprehensive survey and an empirical comparison of the current party prediction practices and propose several new approaches which are competitive with or outperform state-of-the-art methods, yet require less computational resources. Party prediction models rely on the content generated by the users (e.g., tweet texts), the relations they have (e.g., who they follow), or their activities and interactions (e.g., which tweets they like). We examine all of these and compare their signal strength for the party prediction task. This paper lets the practitioner select from a wide range of data types that all
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#22823;&#23567;&#21644;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#21542;&#28085;&#30422;&#30693;&#35782;&#22270;&#35889;&#30340;&#22797;&#26434;&#25299;&#25169;&#21644;&#35821;&#20041;&#23646;&#24615;&#65292;&#36825;&#23545;&#20110;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.13676</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31526;&#21495;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Rethinking Language Models as Symbolic Knowledge Graphs. (arXiv:2308.13676v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#19981;&#21516;&#22823;&#23567;&#21644;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#21542;&#28085;&#30422;&#30693;&#35782;&#22270;&#35889;&#30340;&#22797;&#26434;&#25299;&#25169;&#21644;&#35821;&#20041;&#23646;&#24615;&#65292;&#36825;&#23545;&#20110;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#30693;&#35782;&#22270;&#35889;&#22312;&#25628;&#32034;&#12289;&#38382;&#31572;&#21644;&#25512;&#33616;&#31561;&#20197;&#30693;&#35782;&#20026;&#20013;&#24515;&#30340;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#38543;&#30528;&#24403;&#20195;&#22522;&#20110;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#26159;&#21542;&#33021;&#22815;&#19982;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#30693;&#35782;&#30456;&#21305;&#37197;&#12290;&#21508;&#31181;&#26041;&#27861;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#25110;&#35757;&#32451;&#25968;&#25454;&#37327;&#21487;&#20197;&#22686;&#24378;&#20854;&#26816;&#32034;&#31526;&#21495;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#36890;&#24120;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#24037;&#30417;&#30563;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#28085;&#30422;&#30693;&#35782;&#22270;&#35889;&#30340;&#22797;&#26434;&#25299;&#25169;&#21644;&#35821;&#20041;&#23646;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#36825;&#20123;&#23646;&#24615;&#23545;&#20110;&#25512;&#29702;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#22823;&#23567;&#21644;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20061;&#20010;&#23450;&#24615;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#23646;&#24615;&#65292;&#21253;&#25324;&#23545;&#31216;&#24615;&#12289;&#19981;&#23545;&#31216;&#24615;&#12289;
&lt;/p&gt;
&lt;p&gt;
Symbolic knowledge graphs (KGs) play a pivotal role in knowledge-centric applications such as search, question answering and recommendation. As contemporary language models (LMs) trained on extensive textual data have gained prominence, researchers have extensively explored whether the parametric knowledge within these models can match up to that present in knowledge graphs. Various methodologies have indicated that enhancing the size of the model or the volume of training data enhances its capacity to retrieve symbolic knowledge, often with minimal or no human supervision. Despite these advancements, there is a void in comprehensively evaluating whether LMs can encompass the intricate topological and semantic attributes of KGs, attributes crucial for reasoning processes. In this work, we provide an exhaustive evaluation of language models of varying sizes and capabilities. We construct nine qualitative benchmarks that encompass a spectrum of attributes including symmetry, asymmetry, h
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;&#32447;&#24615;&#25391;&#21160;&#65288;LoC&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#23558;&#32447;&#24615;&#36712;&#36857;&#21644;&#25391;&#33633;&#20559;&#24046;&#26080;&#32541;&#34701;&#21512;&#65292;&#25429;&#25417;&#21040;&#20102;&#8220;&#22256;&#24785;&#30340;&#37325;&#35201;&#24615;&#8221;&#30340;&#26412;&#36136;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38656;&#35201;&#21306;&#20998;&#24494;&#22937;&#27169;&#24335;&#30340;&#24773;&#22659;&#20013;&#65292;&#20351;&#29992;LoC&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13670</link><description>&lt;p&gt;
&#32447;&#24615;&#25391;&#21160;&#65306;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#30340;&#22256;&#24785;&#32654;&#23398;
&lt;/p&gt;
&lt;p&gt;
Linear Oscillation: The Aesthetics of Confusion for Vision Transformer. (arXiv:2308.13670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13670
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#8212;&#8212;&#32447;&#24615;&#25391;&#21160;&#65288;LoC&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#23427;&#36890;&#36807;&#23558;&#32447;&#24615;&#36712;&#36857;&#21644;&#25391;&#33633;&#20559;&#24046;&#26080;&#32541;&#34701;&#21512;&#65292;&#25429;&#25417;&#21040;&#20102;&#8220;&#22256;&#24785;&#30340;&#37325;&#35201;&#24615;&#8221;&#30340;&#26412;&#36136;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38656;&#35201;&#21306;&#20998;&#24494;&#22937;&#27169;&#24335;&#30340;&#24773;&#22659;&#20013;&#65292;&#20351;&#29992;LoC&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#26159;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#38190;&#65292;&#28145;&#21051;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#35757;&#32451;&#21160;&#21147;&#23398;&#12290;&#23427;&#20204;&#19981;&#20165;&#22609;&#36896;&#20102;&#34920;&#31034;&#30340;&#24615;&#36136;&#65292;&#36824;&#20248;&#21270;&#20102;&#25910;&#25947;&#36895;&#24230;&#24182;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#37492;&#20110;&#36825;&#19968;&#20851;&#38190;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32447;&#24615;&#25391;&#21160;&#65288;LoC&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#23450;&#20041;&#20026;$f(x) = x \times \sin(\alpha x + \beta)$&#12290;&#19982;&#20256;&#32479;&#30340;&#28608;&#27963;&#20989;&#25968;&#19981;&#21516;&#65292;LoC&#23558;&#32447;&#24615;&#36712;&#36857;&#19982;&#25391;&#33633;&#20559;&#24046;&#26080;&#32541;&#34701;&#21512;&#12290;&#21629;&#21517;&#20026;&#8220;&#32447;&#24615;&#25391;&#21160;&#8221;&#26159;&#23545;&#20854;&#29420;&#29305;&#23646;&#24615;&#30340;&#33268;&#25964;&#65292;&#21363;&#36890;&#36807;&#21644;&#35856;&#30340;&#25391;&#21160;&#34701;&#20837;&#32447;&#24615;&#28608;&#27963;&#65292;&#25429;&#25417;&#8220;&#22256;&#24785;&#30340;&#37325;&#35201;&#24615;&#8221;&#30340;&#26412;&#36136;&#12290;&#32593;&#32476;&#28608;&#27963;&#20013;&#30340;&#8220;&#25511;&#21046;&#24615;&#22256;&#24785;&#8221;&#27010;&#24565;&#34987;&#35748;&#20026;&#33021;&#22815;&#20419;&#36827;&#26356;&#31283;&#20581;&#30340;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#21306;&#20998;&#24494;&#22937;&#27169;&#24335;&#30340;&#24773;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;LoC&#28608;&#27963;&#20989;&#25968;&#26102;&#65292;&#32593;&#32476;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#38544;&#34255;&#30340;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activation functions are the linchpins of deep learning, profoundly influencing both the representational capacity and training dynamics of neural networks. They shape not only the nature of representations but also optimize convergence rates and enhance generalization potential. Appreciating this critical role, we present the Linear Oscillation (LoC) activation function, defined as $f(x) = x \times \sin(\alpha x + \beta)$. Distinct from conventional activation functions which primarily introduce non-linearity, LoC seamlessly blends linear trajectories with oscillatory deviations. The nomenclature ``Linear Oscillation'' is a nod to its unique attribute of infusing linear activations with harmonious oscillations, capturing the essence of the 'Importance of Confusion'. This concept of ``controlled confusion'' within network activations is posited to foster more robust learning, particularly in contexts that necessitate discerning subtle patterns. Our empirical studies reveal that, when i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#36924;&#36817;&#38543;&#26426;&#28216;&#36208;&#30340;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#32858;&#31867;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13663</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#30095;&#36924;&#36817;&#38543;&#26426;&#28216;&#36208;&#30340;&#32593;&#32476;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Network Embedding Using Sparse Approximations of Random Walks. (arXiv:2308.13663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#36924;&#36817;&#38543;&#26426;&#28216;&#36208;&#30340;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#32858;&#31867;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#21220;&#26102;&#38388;&#30340;&#32593;&#32476;&#23884;&#20837;&#30340;&#39640;&#25928;&#25968;&#20540;&#23454;&#29616;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#36890;&#36807;&#25913;&#36827;&#30340;&#25193;&#25955;&#23567;&#27874;&#31639;&#27861;&#22312;&#32593;&#32476;&#19978;&#33719;&#24471;&#30340;&#31232;&#30095;&#25193;&#25955;&#36807;&#31243;&#30340;&#36924;&#36817;&#12290;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#21644;&#20302;&#32500;&#34920;&#31034;&#30340;&#32511;&#20989;&#25968;&#37319;&#26679;&#65292;&#35745;&#31639;&#33410;&#28857;&#30340;&#23884;&#20837;&#65292;&#36890;&#36807;&#22810;&#20010;&#31034;&#20363;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#32858;&#31867;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#36824;&#35752;&#35770;&#20102;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#29702;&#35770;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an efficient numerical implementation of Network Embedding based on commute times, using sparse approximation of a diffusion process on the network obtained by a modified version of the diffusion wavelet algorithm. The node embeddings are computed by optimizing the cross entropy loss via the stochastic gradient descent method with sampling of low-dimensional representations of green functions. We demonstrate the efficacy of this method for data clustering and multi-label classification through several examples, and compare its performance over existing methods in terms of efficiency and accuracy. Theoretical issues justifying the scheme are also discussed.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#21098;&#26525;&#25216;&#26415;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#20013;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25361;&#25112;&#65292;&#24182;&#22312;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#21644;&#24615;&#33021;&#30340;&#21516;&#26102;&#36866;&#24212;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.13662</link><description>&lt;p&gt;
&#24322;&#26500;&#21644;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#36164;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Resource-Efficient Federated Learning for Heterogenous and Resource-Constrained Environments. (arXiv:2308.13662v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13662
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#21464;&#21098;&#26525;&#25216;&#26415;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#20013;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25361;&#25112;&#65292;&#24182;&#22312;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#21644;&#24615;&#33021;&#30340;&#21516;&#26102;&#36866;&#24212;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#23376;&#39046;&#22495;&#65292;&#23427;&#23558;&#27169;&#22411;&#24102;&#21040;&#29992;&#25143;&#35774;&#22791;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20849;&#20139;&#20010;&#20154;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;FL&#20013;&#30340;&#20854;&#20182;&#25361;&#25112;&#65292;&#22914;&#35774;&#22791;&#24322;&#36136;&#24615;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RE-FL&#65292;&#36825;&#26159;&#19968;&#31181;&#38754;&#21521;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#35745;&#31639;&#21644;&#36890;&#20449;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#21487;&#21464;&#21098;&#26525;&#25216;&#26415;&#36890;&#36807;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#35745;&#31639;&#33021;&#21147;&#36827;&#34892;&#21098;&#26525;&#20248;&#21270;&#36164;&#28304;&#21033;&#29992;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#20943;&#23569;&#24102;&#23485;&#28040;&#32791;&#21644;&#36890;&#20449;&#36718;&#25968;&#12290;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#30340;&#25968;&#25454;&#38544;&#31169;&#21644;&#24615;&#33021;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-enforcing sub-domain of machine learning that brings the model to the user's device for training, avoiding the need to share personal data with a central server. While existing works address data heterogeneity, they overlook other challenges in FL, such as device heterogeneity and communication efficiency. In this paper, we propose RE-FL, a novel approach that tackles computational and communication challenges in resource-constrained devices. Our variable pruning technique optimizes resource utilization by adapting pruning to each client's computational capabilities. We also employ knowledge distillation to reduce bandwidth consumption and communication rounds. Experimental results on image classification tasks demonstrate the effectiveness of our approach in resource-constrained environments, maintaining data privacy and performance while accommodating heterogeneous model architectures.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;GoBI&#65288;&#36229;&#36234;&#24819;&#35937;&#65289;&#20869;&#22312;&#22870;&#21169;&#35774;&#35745;&#65292;&#32467;&#21512;&#20256;&#32479;&#30340;&#32456;&#36523;&#26032;&#39062;&#24615;&#21160;&#26426;&#21644;&#24773;&#33410;&#20869;&#22312;&#22870;&#21169;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#19990;&#30028;&#27169;&#22411;&#26368;&#22823;&#21270;&#20102;&#36880;&#27493;&#21487;&#36798;&#25193;&#23637;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#22312;&#36855;&#20320;&#32593;&#26684;&#23548;&#33322;&#20219;&#21153;&#21644;DeepMind&#25511;&#21046;&#22871;&#20214;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13661</link><description>&lt;p&gt;
&#36229;&#36234;&#24819;&#35937;&#65306;&#36890;&#36807;&#19990;&#30028;&#27169;&#22411;&#26368;&#22823;&#21270;&#24773;&#33410;&#21487;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
Go Beyond Imagination: Maximizing Episodic Reachability with World Models. (arXiv:2308.13661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13661
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;GoBI&#65288;&#36229;&#36234;&#24819;&#35937;&#65289;&#20869;&#22312;&#22870;&#21169;&#35774;&#35745;&#65292;&#32467;&#21512;&#20256;&#32479;&#30340;&#32456;&#36523;&#26032;&#39062;&#24615;&#21160;&#26426;&#21644;&#24773;&#33410;&#20869;&#22312;&#22870;&#21169;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#19990;&#30028;&#27169;&#22411;&#26368;&#22823;&#21270;&#20102;&#36880;&#27493;&#21487;&#36798;&#25193;&#23637;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#22312;&#36855;&#20320;&#32593;&#26684;&#23548;&#33322;&#20219;&#21153;&#21644;DeepMind&#25511;&#21046;&#22871;&#20214;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#39640;&#25928;&#25506;&#32034;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#12290;&#20026;&#20102;&#24212;&#23545;&#22870;&#21169;&#31232;&#30095;&#24615;&#65292;&#20154;&#20204;&#36890;&#24120;&#20250;&#24212;&#29992;&#20869;&#22312;&#22870;&#21169;&#26469;&#28608;&#21169;&#26234;&#33021;&#20307;&#26377;&#25928;&#22320;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20869;&#22312;&#22870;&#21169;&#35774;&#35745;&#65292;&#31216;&#20026;GoBI - &#36229;&#36234;&#24819;&#35937;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#32456;&#36523;&#26032;&#39062;&#24615;&#21160;&#26426;&#19982;&#19968;&#20010;&#24773;&#33410;&#20869;&#22312;&#22870;&#21169;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#36880;&#27493;&#21487;&#36798;&#24615;&#25193;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24212;&#29992;&#23398;&#20064;&#30340;&#19990;&#30028;&#27169;&#22411;&#26469;&#29983;&#25104;&#20855;&#26377;&#38543;&#26426;&#21160;&#20316;&#30340;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#12290;&#37027;&#20123;&#20855;&#26377;&#26356;&#22810;&#29420;&#29305;&#39044;&#27979;&#19988;&#19981;&#22312;&#24773;&#33410;&#35760;&#24518;&#20013;&#30340;&#29366;&#24577;&#23558;&#34987;&#20998;&#37197;&#39640;&#20869;&#22312;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;12&#20010;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#36855;&#20320;&#32593;&#26684;&#23548;&#33322;&#20219;&#21153;&#20013;&#22823;&#22823;&#36229;&#36234;&#20102;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#39640;&#20102;DeepMind Control Suite&#20013;&#36816;&#21160;&#20219;&#21153;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient exploration is a challenging topic in reinforcement learning, especially for sparse reward tasks. To deal with the reward sparsity, people commonly apply intrinsic rewards to motivate agents to explore the state space efficiently. In this paper, we introduce a new intrinsic reward design called GoBI - Go Beyond Imagination, which combines the traditional lifelong novelty motivation with an episodic intrinsic reward that is designed to maximize the stepwise reachability expansion. More specifically, we apply learned world models to generate predicted future states with random actions. States with more unique predictions that are not in episodic memory are assigned high intrinsic rewards. Our method greatly outperforms previous state-of-the-art methods on 12 of the most challenging Minigrid navigation tasks and improves the sample efficiency on locomotion tasks from DeepMind Control Suite.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#65292;&#24403;&#36817;&#20284;&#12289;&#31616;&#21270;&#30340;&#27169;&#22411;&#30340;&#26368;&#20248;&#35299;&#27604;&#26356;&#20934;&#30830;&#27169;&#22411;&#30340;&#36817;&#20284;&#35299;&#26356;&#22909;&#26102;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#36924;&#36817;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13654</link><description>&lt;p&gt;
&#30456;&#23545;&#20934;&#30830;&#27169;&#22411;&#20013;&#36817;&#20284;&#35299;&#20309;&#26102;&#20248;&#20110;&#36817;&#20284;&#27169;&#22411;&#8212;&#8212;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pretty darn good control: when are approximate solutions better than approximate models. (arXiv:2308.13654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#65292;&#24403;&#36817;&#20284;&#12289;&#31616;&#21270;&#30340;&#27169;&#22411;&#30340;&#26368;&#20248;&#35299;&#27604;&#26356;&#20934;&#30830;&#27169;&#22411;&#30340;&#36817;&#20284;&#35299;&#26356;&#22909;&#26102;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#36924;&#36817;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#22788;&#29702;&#23454;&#38469;&#31995;&#32479;&#20013;&#36935;&#21040;&#30340;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#32500;&#24230;&#12289;&#36807;&#31243;&#35823;&#24046;&#12289;&#27169;&#22411;&#20559;&#24046;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20542;&#21521;&#20110;&#31616;&#21270;&#27169;&#22411;&#26469;&#36866;&#24212;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35299;&#20915;&#31995;&#32479;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#36817;&#20284;&#12289;&#31616;&#21270;&#30340;&#27169;&#22411;&#30340;&#26368;&#20248;&#35299;&#27604;&#26356;&#20934;&#30830;&#27169;&#22411;&#30340;&#36817;&#20284;&#35299;&#26356;&#22909;&#26102;&#65292;&#25105;&#20204;&#21448;&#35813;&#22914;&#20309;&#36873;&#25321;&#21602;&#65311;&#34429;&#28982;&#36825;&#20010;&#38382;&#39064;&#19968;&#30452;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#65292;&#22240;&#20026;&#25214;&#21040;&#22797;&#26434;&#27169;&#22411;&#30340;&#36817;&#20284;&#35299;&#38750;&#24120;&#22256;&#38590;&#65292;&#20294;&#26159;&#26368;&#36817;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#31639;&#27861;&#21644;&#35745;&#31639;&#26041;&#38754;&#30340;&#36827;&#23637;&#25110;&#35768;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;DRL&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#28216;&#25103;&#25110;&#26426;&#22120;&#20154;&#21147;&#23398;&#31561;&#31934;&#30830;&#24050;&#30693;&#35268;&#21017;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;DRL&#31639;&#27861;&#25104;&#21151;&#36924;&#36817;&#35299;&#20915;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods for optimal control struggle to deal with the complexity commonly encountered in real-world systems, including dimensionality, process error, model bias and data heterogeneity. Instead of tackling these system complexities directly, researchers have typically sought to simplify models to fit optimal control methods. But when is the optimal solution to an approximate, stylized model better than an approximate solution to a more accurate model? While this question has largely gone unanswered owing to the difficulty of finding even approximate solutions for complex models, recent algorithmic and computational advances in deep reinforcement learning (DRL) might finally allow us to address these questions. DRL methods have to date been applied primarily in the context of games or robotic mechanics, which operate under precisely known rules. Here, we demonstrate the ability for DRL algorithms using deep neural networks to successfully approximate solutions (the "policy funct
&lt;/p&gt;</description></item><item><title>GRASP&#26159;&#19968;&#31181;&#26032;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#65292;&#26681;&#25454;&#26679;&#26412;&#30340;&#20195;&#34920;&#24615;&#36873;&#25321;&#26368;&#36866;&#21512;&#23398;&#20064;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#32447;&#28176;&#36827;&#24335;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13646</link><description>&lt;p&gt;
GRASP: &#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#28176;&#36827;&#24335;&#23398;&#20064;&#30340;&#37325;&#28436;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
GRASP: A Rehearsal Policy for Efficient Online Continual Learning. (arXiv:2308.13646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13646
&lt;/p&gt;
&lt;p&gt;
GRASP&#26159;&#19968;&#31181;&#26032;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#65292;&#26681;&#25454;&#26679;&#26412;&#30340;&#20195;&#34920;&#24615;&#36873;&#25321;&#26368;&#36866;&#21512;&#23398;&#20064;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#32447;&#28176;&#36827;&#24335;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28176;&#36827;&#23398;&#20064;&#28041;&#21450;&#20174;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#27969;&#20013;&#36880;&#27493;&#32047;&#31215;&#30693;&#35782;&#12290;&#28176;&#36827;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#38750;&#24179;&#31283;&#30340;&#25968;&#25454;&#27969;&#20250;&#23548;&#33268;&#20043;&#21069;&#23398;&#21040;&#30340;&#33021;&#21147;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#37325;&#28436;&#26159;&#19968;&#31181;&#24120;&#29992;&#19988;&#26377;&#25928;&#30340;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#36807;&#21435;&#30340;&#35266;&#27979;&#32467;&#26524;&#23384;&#20648;&#22312;&#32531;&#20914;&#21306;&#20013;&#65292;&#24182;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23558;&#23427;&#20204;&#19982;&#26032;&#30340;&#35266;&#27979;&#32467;&#26524;&#28151;&#21512;&#12290;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24212;&#35813;&#36873;&#25321;&#21738;&#20123;&#23384;&#20648;&#26679;&#26412;&#36827;&#34892;&#37325;&#28436;&#65311;&#36873;&#25321;&#26368;&#36866;&#21512;&#23398;&#20064;&#30340;&#26679;&#26412;&#32780;&#19981;&#26159;&#38543;&#26426;&#36873;&#25321;&#26679;&#26412;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#23398;&#20064;&#36895;&#24230;&#26174;&#33879;&#21152;&#24555;&#12290;&#23545;&#20110;&#31867;&#22686;&#37327;&#23398;&#20064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#31616;&#21333;&#30340;&#31867;&#22343;&#34913;&#38543;&#26426;&#36873;&#25321;&#31574;&#30053;&#20248;&#20110;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#19968;&#31181;&#26032;&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;GRASP&#37325;&#26032;&#24605;&#32771;&#36825;&#20010;&#38382;&#39064;&#12290;GRASP&#39318;&#20808;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#36880;&#28176;&#36873;&#25321;&#36739;&#19981;&#20855;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) in deep neural networks (DNNs) involves incrementally accumulating knowledge in a DNN from a growing data stream. A major challenge in CL is that non-stationary data streams cause catastrophic forgetting of previously learned abilities. Rehearsal is a popular and effective way to mitigate this problem, which is storing past observations in a buffer and mixing them with new observations during learning. This leads to a question: Which stored samples should be selected for rehearsal? Choosing samples that are best for learning, rather than simply selecting them at random, could lead to significantly faster learning. For class incremental learning, prior work has shown that a simple class balanced random selection policy outperforms more sophisticated methods. Here, we revisit this question by exploring a new sample selection policy called GRASP. GRASP selects the most prototypical (class representative) samples first and then gradually selects less prototypical (h
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#23545;&#20110;&#24314;&#27169;&#25915;&#20987;&#30340;&#20316;&#29992;&#65292;&#20351;&#29992;&#29575;&#38400;&#20540;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25361;&#25112;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#23454;&#29616;&#8220;&#24555;&#36895;&#8221;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#26377;&#25928;&#22320;&#23398;&#20064;&#21046;&#36896;&#30340;PUF&#12290;</title><link>http://arxiv.org/abs/2308.13645</link><description>&lt;p&gt;
Arbiter PUF&#23545;&#24555;&#36895;&#21644;&#24930;&#36895;&#24314;&#27169;&#25915;&#20987;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active learning for fast and slow modeling attacks on Arbiter PUFs. (arXiv:2308.13645v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#23545;&#20110;&#24314;&#27169;&#25915;&#20987;&#30340;&#20316;&#29992;&#65292;&#20351;&#29992;&#29575;&#38400;&#20540;&#20989;&#25968;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25361;&#25112;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#23454;&#29616;&#8220;&#24555;&#36895;&#8221;&#23398;&#20064;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#26377;&#25928;&#22320;&#23398;&#20064;&#21046;&#36896;&#30340;PUF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#25915;&#20987;&#26159;&#25351;&#23545;&#22522;&#20110;&#30828;&#20214;&#30340;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968; (PUF) &#36827;&#34892;&#24314;&#27169;&#30340;&#25915;&#20987;&#65292;&#36825;&#23545;&#36825;&#20123;&#30828;&#20214;&#23433;&#20840;&#21407;&#35821;&#30340;&#21487;&#34892;&#24615;&#26500;&#25104;&#20102;&#24040;&#22823;&#23041;&#32961;&#12290;&#22312;&#22823;&#22810;&#25968;&#24314;&#27169;&#25915;&#20987;&#20013;&#65292;&#19968;&#37096;&#20998;&#38543;&#26426;&#30340;&#25361;&#25112;-&#21709;&#24212;&#23545; (CRP) &#34987;&#29992;&#20316;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#38024;&#23545;&#38400;&#20540;&#20989;&#25968;&#24102;&#26469;&#30340; aribiter-PUF &#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35813;&#38408;&#20540;&#20989;&#25968;&#30001;&#20110;&#21046;&#36896;&#32570;&#38519;&#32780;&#20855;&#26377;&#38543;&#26426;&#26435;&#37325;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25903;&#25345;&#21521;&#37327;&#26426; (SVM) &#23398;&#20064;&#20013;&#20027;&#21160;&#23398;&#20064;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#36890;&#36807;&#25361;&#25112;&#36873;&#25321;&#26469;&#24110;&#21161; SVM &#31639;&#27861;&#23454;&#29616;&#8220;&#24555;&#36895;&#8221;&#23398;&#20064;&#21644;&#8220;&#24930;&#36895;&#8221;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26500;&#24314;&#25361;&#25112;&#65292;&#32780;&#19981;&#26159;&#20687;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#20381;&#36182;&#26679;&#26412;&#27744;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#23454;&#29616;&#8220;&#24555;&#36895;&#8221;&#23398;&#20064;&#65288;&#25581;&#31034;&#26356;&#23569;&#30340; CRPs&#65292;&#24471;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65289;&#21487;&#33021;&#26377;&#21161;&#20110;&#21046;&#36896;&#21830;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#21046;&#36896;&#30340; PUF&#65292;&#25110;&#32773;&#22312;&#25915;&#20987;&#32773;&#21487;&#20197;&#26597;&#35810;&#24182;&#25910;&#38598;&#36739;&#23569;&#30340;CRPs&#20449;&#24687;&#26102;&#24418;&#25104;&#26356;&#24378;&#22823;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling attacks, in which an adversary uses machine learning techniques to model a hardware-based Physically Unclonable Function (PUF) pose a great threat to the viability of these hardware security primitives. In most modeling attacks, a random subset of challenge-response-pairs (CRPs) are used as the labeled data for the machine learning algorithm. Here, for the arbiter-PUF, a delay based PUF which may be viewed as a linear threshold function with random weights (due to manufacturing imperfections), we investigate the role of active learning in Support Vector Machine (SVM) learning. We focus on challenge selection to help SVM algorithm learn ``fast'' and learn ``slow''. Our methods construct challenges rather than relying on a sample pool of challenges as in prior work. Using active learning to learn ``fast'' (less CRPs revealed, higher accuracies) may help manufacturers learn the manufactured PUFs more efficiently, or may form a more powerful attack when the attacker may query the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#20195;&#20113;&#26381;&#21153;&#20013;&#33258;&#21160;&#32034;&#24341;&#35843;&#20248;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24037;&#20316;&#36127;&#36733;&#36873;&#25321;&#12289;&#20505;&#36873;&#32034;&#24341;&#36807;&#28388;&#12289;&#21152;&#36895;&#32034;&#24341;&#37197;&#32622;&#25628;&#32034;&#12289;&#20943;&#23569;&#26597;&#35810;&#20248;&#21270;&#22120;&#35843;&#29992;&#30340;&#25968;&#37327;&#21644;&#38477;&#20302;&#24615;&#33021;&#22238;&#24402;&#26426;&#20250;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.13641</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#32034;&#24341;&#35843;&#20248;&#65306;&#26368;&#26032;&#36827;&#23637;&#19982;&#24320;&#25918;&#25361;&#25112;&#30340;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
ML-Powered Index Tuning: An Overview of Recent Progress and Open Challenges. (arXiv:2308.13641v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#29616;&#20195;&#20113;&#26381;&#21153;&#20013;&#33258;&#21160;&#32034;&#24341;&#35843;&#20248;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24037;&#20316;&#36127;&#36733;&#36873;&#25321;&#12289;&#20505;&#36873;&#32034;&#24341;&#36807;&#28388;&#12289;&#21152;&#36895;&#32034;&#24341;&#37197;&#32622;&#25628;&#32034;&#12289;&#20943;&#23569;&#26597;&#35810;&#20248;&#21270;&#22120;&#35843;&#29992;&#30340;&#25968;&#37327;&#21644;&#38477;&#20302;&#24615;&#33021;&#22238;&#24402;&#26426;&#20250;&#31561;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20113;&#26381;&#21153;&#20013;&#24037;&#20316;&#36127;&#36733;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#33258;&#21160;&#32034;&#24341;&#35843;&#20248;&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#8212;&#8212;&#22312;&#20445;&#25345;&#32034;&#24341;&#35843;&#20248;&#21487;&#25193;&#23637;&#24615;&#30340;&#21516;&#26102;&#25512;&#33616;&#39640;&#36136;&#37327;&#30340;&#32034;&#24341;&#12290;&#36825;&#19968;&#25361;&#25112;&#36827;&#19968;&#27493;&#21463;&#21040;&#33258;&#21160;&#32034;&#24341;&#23454;&#29616;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#24341;&#20837;&#26368;&#23567;&#26597;&#35810;&#24615;&#33021;&#22238;&#24402;&#30340;&#35201;&#27714;&#30340;&#24433;&#21709;&#65292;&#36825;&#26500;&#25104;&#20102;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#21644;&#20840;&#33258;&#21160;&#21270;&#30340;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#20851;&#27880;&#33258;&#21160;&#32034;&#24341;&#35843;&#20248;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#25552;&#20379;&#30340;&#26032;&#26426;&#36935;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#22312;&#24037;&#20316;&#36127;&#36733;&#36873;&#25321;&#12289;&#20505;&#36873;&#32034;&#24341;&#36807;&#28388;&#12289;&#21152;&#36895;&#32034;&#24341;&#37197;&#32622;&#25628;&#32034;&#12289;&#20943;&#23569;&#26597;&#35810;&#20248;&#21270;&#22120;&#35843;&#29992;&#30340;&#25968;&#37327;&#20197;&#21450;&#38477;&#20302;&#24615;&#33021;&#22238;&#24402;&#26426;&#20250;&#26041;&#38754;&#24320;&#23637;&#30340;&#26368;&#26032;&#24037;&#20316;&#12290;&#25105;&#20204;&#24378;&#35843;&#36825;&#20123;&#24037;&#20316;&#30340;&#20851;&#38190;&#35201;&#28857;&#65292;&#24182;&#24378;&#35843;&#20854;&#21019;&#26032;&#19982;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scale and complexity of workloads in modern cloud services have brought into sharper focus a critical challenge in automated index tuning -- the need to recommend high-quality indexes while maintaining index tuning scalability. This challenge is further compounded by the requirement for automated index implementations to introduce minimal query performance regressions in production deployments, representing a significant barrier to achieving scalability and full automation. This paper directs attention to these challenges within automated index tuning and explores ways in which machine learning (ML) techniques provide new opportunities in their mitigation. In particular, we reflect on recent efforts in developing ML techniques for workload selection, candidate index filtering, speeding up index configuration search, reducing the amount of query optimizer calls, and lowering the chances of performance regressions. We highlight the key takeaways from these efforts and underline the g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#20351;&#29992;&#24555;&#36895;&#22686;&#30410;&#35843;&#21046;&#21644;&#24930;&#36895;&#31361;&#35302;&#21487;&#22609;&#24615;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21464;&#21270;&#30340;&#24863;&#35273;&#32479;&#35745;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.13633</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#30333;&#21270;&#65306;&#24555;&#36895;&#22686;&#30410;&#35843;&#21046;&#21644;&#24930;&#36895;&#31361;&#35302;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adaptive whitening with fast gain modulation and slow synaptic plasticity. (arXiv:2308.13633v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#20351;&#29992;&#24555;&#36895;&#22686;&#30410;&#35843;&#21046;&#21644;&#24930;&#36895;&#31361;&#35302;&#21487;&#22609;&#24615;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#26469;&#36866;&#24212;&#21464;&#21270;&#30340;&#24863;&#35273;&#32479;&#35745;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#24863;&#35273;&#21306;&#30340;&#31070;&#32463;&#20803;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#21464;&#21270;&#30340;&#24863;&#35273;&#32479;&#35745;&#20449;&#24687;&#65292;&#36890;&#36807;&#23545;&#20854;&#20010;&#20307;&#21709;&#24212;&#30340;&#26041;&#24046;&#36827;&#34892;&#24402;&#19968;&#21270;&#20197;&#21450;&#20943;&#23569;&#21709;&#24212;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#36716;&#25442;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#30333;&#21270;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#30333;&#21270;&#30340;&#26426;&#21046;&#27169;&#22411;&#21482;&#20351;&#29992;&#31361;&#35302;&#21487;&#22609;&#24615;&#25110;&#22686;&#30410;&#35843;&#21046;&#20316;&#20026;&#36866;&#24212;&#30340;&#29983;&#29289;&#22522;&#36136;&#65292;&#28982;&#32780;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#26377;&#26174;&#33879;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#33539;&#24615;&#30340;&#22810;&#26102;&#38388;&#23610;&#24230;&#26426;&#21046;&#27169;&#22411;&#65292;&#36890;&#36807;&#31361;&#35302;&#21487;&#22609;&#24615;&#21644;&#22686;&#30410;&#35843;&#21046;&#30340;&#35745;&#31639;&#35282;&#33394;&#26469;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#30333;&#21270;&#12290;&#22686;&#30410;&#22312;&#24555;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#26681;&#25454;&#24403;&#21069;&#30340;&#32479;&#35745;&#24773;&#20917;&#36827;&#34892;&#35843;&#25972;&#65292;&#32780;&#31361;&#35302;&#22312;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#35843;&#25972;&#65292;&#23398;&#20064;&#36755;&#20837;&#32479;&#35745;&#20013;&#19982;&#24773;&#22659;&#26080;&#20851;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26469;&#33258;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26102;&#38388;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to learn structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#26469;&#20272;&#35745;RIS-&#21551;&#29992;&#30340;&#27627;&#31859;&#27874;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#32852;&#21512;&#20272;&#35745;&#29992;&#25143;&#35774;&#22791;&#21040;RIS&#21644;RIS&#21040;&#22522;&#31449;&#30340;&#36890;&#36947;&#65292;&#20943;&#23569;&#20102;&#20449;&#21495;&#22797;&#26434;&#24615;&#21644;&#30636;&#26102;&#20449;&#36947;&#30340;&#39640;&#20449;&#20196;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2308.13616</link><description>&lt;p&gt;
RIS-&#21551;&#29992;&#30340;&#27627;&#31859;&#27874;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#20272;&#35745;&#65306;&#19968;&#31181;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Channel Estimation in RIS-Enabled mmWave Wireless Systems: A Variational Inference Approach. (arXiv:2308.13616v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#26469;&#20272;&#35745;RIS-&#21551;&#29992;&#30340;&#27627;&#31859;&#27874;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#36890;&#36807;&#32852;&#21512;&#20272;&#35745;&#29992;&#25143;&#35774;&#22791;&#21040;RIS&#21644;RIS&#21040;&#22522;&#31449;&#30340;&#36890;&#36947;&#65292;&#20943;&#23569;&#20102;&#20449;&#21495;&#22797;&#26434;&#24615;&#21644;&#30636;&#26102;&#20449;&#36947;&#30340;&#39640;&#20449;&#20196;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#23436;&#20840;&#34987;&#21160;&#30340;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;(RIS)&#36741;&#21161;&#30340;&#27627;&#31859;&#27874;&#21333;&#29992;&#25143;&#21333;&#36755;&#20837;&#22810;&#36755;&#20986;(SIMO)&#36890;&#20449;&#31995;&#32479;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#25512;&#26029;(VI)&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;(CSI)&#20272;&#35745;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;VI&#30340;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#19978;&#34892;&#35757;&#32451;&#20449;&#21495;&#26469;&#20272;&#35745;&#29992;&#25143;&#35774;&#22791;(UE)&#21040;RIS(UE-RIS)&#21644;RIS&#21040;&#22522;&#31449;(RIS-BS)&#20449;&#36947;&#22312;&#34987;&#21160;RIS&#35774;&#32622;&#20013;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#30636;&#26102;CSI(I-CSI)&#26356;&#26032;&#30456;&#31227;&#20250;&#23548;&#33268;&#39640;&#20449;&#20196;&#24320;&#38144;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;UE-RIS&#20449;&#36947;&#30340;&#30701;&#30456;&#24178;&#22359;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20943;&#23569;&#20449;&#20196;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;VI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;RIS-BS&#20449;&#36947;&#20197;&#21450;&#23545;&#20110;&#27604;&#30636;&#26102;UE-RIS&#20449;&#36947;&#26356;&#38271;&#26102;&#38388;&#20445;&#25345;&#20934;&#38745;&#24577;&#30340;UE-RIS&#20449;&#36947;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#22312;VI&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#26041;&#20415;&#30340;&#20998;&#24067;&#20989;&#25968;&#26469;&#36817;&#20284;&#20449;&#36947;&#22686;&#30410;/&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a variational inference (VI)-based channel state information (CSI) estimation approach in a fully-passive reconfigurable intelligent surface (RIS)-aided mmWave single-user single-input multiple-output (SIMO) communication system. Specifically, we first propose a VI-based joint channel estimation method to estimate the user-equipment (UE) to RIS (UE-RIS) and RIS to base station (RIS-BS) channels using uplink training signals in a passive RIS setup. However, updating the phase-shifts based on the instantaneous CSI (I-CSI) leads to a high signaling overhead especially due to the short coherence block of the UE-RIS channel. Therefore, to reduce the signaling complexity, we propose a VI-based method to estimate the RIS-BS channel along with the covariance matrix of the UE-RIS channel that remains quasi-static for a longer period than the instantaneous UE-RIS channel. In the VI framework, we approximate the posterior of the channel gains/covariance matrix with convenient distribut
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30340;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#24182;&#27604;&#36739;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65292;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#22914;&#20309;&#36890;&#36807;AI&#24037;&#20855;&#25903;&#25345;&#30002;&#29366;&#33146;&#30284;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;</title><link>http://arxiv.org/abs/2308.13592</link><description>&lt;p&gt;
&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;: &#25216;&#26415;&#12289;&#36235;&#21183;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
AI in Thyroid Cancer Diagnosis: Techniques, Trends, and Future Directions. (arXiv:2308.13592v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#22312;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30340;&#30456;&#20851;&#30740;&#31350;&#12290;&#36890;&#36807;&#25552;&#20986;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#24182;&#27604;&#36739;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65292;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#22914;&#20309;&#36890;&#36807;AI&#24037;&#20855;&#25903;&#25345;&#30002;&#29366;&#33146;&#30284;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21019;&#24314;&#26234;&#33021;&#35786;&#26029;&#31995;&#32479;&#20197;&#21327;&#21161;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#20998;&#26512;&#21644;&#22788;&#29702;&#27835;&#30103;&#26080;&#27861;&#27835;&#24840;&#30142;&#30149;&#30340;&#22823;&#25968;&#25454;&#26041;&#38754;&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#26816;&#27979;&#30002;&#29366;&#33146;&#30284;&#65292;&#22312;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#36827;&#34892;&#30002;&#29366;&#33146;&#30284;&#39044;&#21518;&#35780;&#20272;&#21644;&#30830;&#23450;&#24739;&#32773;&#24694;&#24615;&#39118;&#38505;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#24635;&#32467;&#20102;&#19982;&#22312;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30456;&#20851;&#30340;&#22823;&#37327;&#25991;&#31456;&#12290;&#30456;&#24212;&#22320;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#26681;&#25454;&#25152;&#20351;&#29992;&#30340;AI&#31639;&#27861;&#12289;&#26694;&#26550;&#30340;&#30446;&#30340;&#21644;&#35745;&#31639;&#24179;&#21488;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#26681;&#25454;&#20854;&#29305;&#24449;&#27604;&#36739;&#20102;&#29616;&#26377;&#30340;&#30002;&#29366;&#33146;&#30284;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#26377;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#25110;&#28151;&#21512;&#30340;AI&#24037;&#20855;&#22914;&#20309;&#25903;&#25345;&#30002;&#29366;&#33146;&#30284;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a growing interest in creating intelligent diagnostic systems to assist medical professionals in analyzing and processing big data for the treatment of incurable diseases. One of the key challenges in this field is detecting thyroid cancer, where advancements have been made using machine learning (ML) and big data analytics to evaluate thyroid cancer prognosis and determine a patient's risk of malignancy. This review paper summarizes a large collection of articles related to artificial intelligence (AI)-based techniques used in the diagnosis of thyroid cancer. Accordingly, a new classification was introduced to classify these techniques based on the AI algorithms used, the purpose of the framework, and the computing platforms used. Additionally, this study compares existing thyroid cancer datasets based on their features. The focus of this study is on how AI-based tools can support the diagnosis and treatment of thyroid cancer, through supervised, unsupervised, or hybrid
&lt;/p&gt;</description></item><item><title>GeoExplainer&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#25903;&#25345;&#20998;&#26512;&#20154;&#21592;&#21019;&#24314;&#24635;&#32467;&#21644;&#24773;&#22659;&#21270;&#31354;&#38388;&#20998;&#26512;&#30340;&#35299;&#37322;&#24615;&#25991;&#26723;&#12290;</title><link>http://arxiv.org/abs/2308.13588</link><description>&lt;p&gt;
GeoExplainer: &#19968;&#31181;&#29992;&#20110;&#31354;&#38388;&#24314;&#27169;&#24773;&#22659;&#21270;&#21644;&#25253;&#21578;&#29983;&#25104;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GeoExplainer: A Visual Analytics Framework for Spatial Modeling Contextualization and Report Generation. (arXiv:2308.13588v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13588
&lt;/p&gt;
&lt;p&gt;
GeoExplainer&#26159;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#25903;&#25345;&#20998;&#26512;&#20154;&#21592;&#21019;&#24314;&#24635;&#32467;&#21644;&#24773;&#22659;&#21270;&#31354;&#38388;&#20998;&#26512;&#30340;&#35299;&#37322;&#24615;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#22238;&#24402;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#35782;&#21035;&#31354;&#38388;&#20998;&#24067;&#35266;&#27979;&#32467;&#26524;&#20013;&#30340;&#27169;&#24335;&#21644;&#24322;&#24120;&#12290;&#36825;&#31181;&#31867;&#22411;&#30340;&#20998;&#26512;&#26088;&#22312;&#22238;&#31572;&#20851;&#20110;&#24213;&#23618;&#31354;&#38388;&#29616;&#35937;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#20026;&#20160;&#20040;&#29359;&#32618;&#29575;&#22312;&#36825;&#20010;&#22320;&#21306;&#36739;&#39640;&#65292;&#20026;&#20160;&#20040;&#19968;&#20010;&#23398;&#21306;&#30340;&#23398;&#29983;&#34920;&#29616;&#20248;&#20110;&#21478;&#19968;&#20010;&#23398;&#21306;&#30340;&#23398;&#29983;&#31561;&#12290;&#36825;&#20123;&#38382;&#39064;&#30340;&#31572;&#26696;&#38656;&#35201;&#23545;&#27169;&#22411;&#32467;&#26500;&#12289;&#21442;&#25968;&#36873;&#25321;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#38656;&#35201;&#23558;&#30740;&#31350;&#32467;&#26524;&#19982;&#22320;&#29702;&#32972;&#26223;&#36827;&#34892;&#24773;&#22659;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;GeoExplainer&#65292;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#20998;&#26512;&#20154;&#21592;&#21019;&#24314;&#33021;&#22815;&#24635;&#32467;&#21644;&#24773;&#22659;&#21270;&#31354;&#38388;&#20998;&#26512;&#30340;&#35299;&#37322;&#24615;&#25991;&#26723;&#12290;&#22312;&#20998;&#26512;&#20154;&#21592;&#21019;&#24314;&#31354;&#38388;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20250;&#26631;&#35760;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#20013;&#30340;&#28508;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geographic regression models of various descriptions are often applied to identify patterns and anomalies in the determinants of spatially distributed observations. These types of analyses focus on answering why questions about underlying spatial phenomena, e.g., why is crime higher in this locale, why do children in one school district outperform those in another, etc.? Answers to these questions require explanations of the model structure, the choice of parameters, and contextualization of the findings with respect to their geographic context. This is particularly true for local forms of regression models which are focused on the role of locational context in determining human behavior. In this paper, we present GeoExplainer, a visual analytics framework designed to support analysts in creating explanative documentation that summarizes and contextualizes their spatial analyses. As analysts create their spatial models, our framework flags potential issues with model parameter selectio
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;&#24037;&#35780;&#20272;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#35780;&#20215;&#25351;&#26631;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.13577</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Text Style Transfer Evaluation Using Large Language Models. (arXiv:2308.13577v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13577
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;&#24037;&#35780;&#20272;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#35780;&#20215;&#25351;&#26631;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#65288;TST&#65289;&#30340;&#35780;&#20272;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#34920;&#29616;&#22312;&#22810;&#20010;&#26041;&#38754;&#65292;&#27599;&#20010;&#26041;&#38754;&#37117;&#24456;&#38590;&#21333;&#29420;&#34913;&#37327;&#65306;&#39118;&#26684;&#36716;&#25442;&#20934;&#30830;&#24615;&#12289;&#20869;&#23481;&#20445;&#30041;&#21644;&#25972;&#20307;&#27969;&#30021;&#24615;&#12290;&#20154;&#24037;&#35780;&#20272;&#26159;TST&#35780;&#20272;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#28982;&#32780;&#65292;&#23427;&#36153;&#26102;&#36153;&#21147;&#65292;&#24182;&#19988;&#32467;&#26524;&#38590;&#20197;&#37325;&#22797;&#12290;&#35768;&#22810;&#33258;&#21160;&#21270;&#25351;&#26631;&#34987;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20316;&#20026;&#20154;&#24037;&#35780;&#20272;&#30340;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#33258;&#21160;&#21270;&#25351;&#26631;&#19982;&#20154;&#24037;&#35780;&#20272;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#23545;&#23427;&#20204;&#20316;&#20026;&#21487;&#38752;&#22522;&#20934;&#30340;&#25928;&#26524;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#19981;&#20165;&#33021;&#22815;&#21305;&#37197;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#26410;&#35265;&#20219;&#21153;&#20013;&#36824;&#33021;&#36229;&#36807;&#24179;&#22343;&#20154;&#31867;&#34920;&#29616;&#12290;&#36825;&#34920;&#26126;LLMs&#26377;&#28508;&#21147;&#25104;&#20026;&#20154;&#24037;&#35780;&#20272;&#21644;&#20854;&#20182;&#33258;&#21160;&#21270;&#25351;&#26631;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Text Style Transfer (TST) is challenging to evaluate because the quality of the generated text manifests itself in multiple aspects, each of which is hard to measure individually: style transfer accuracy, content preservation, and overall fluency of the text. Human evaluation is the gold standard in TST evaluation; however, it is expensive, and the results are difficult to reproduce. Numerous automated metrics are employed to assess performance in these aspects, serving as substitutes for human evaluation. However, the correlation between many of these automated metrics and human evaluations remains unclear, raising doubts about their effectiveness as reliable benchmarks. Recent advancements in Large Language Models (LLMs) have demonstrated their ability to not only match but also surpass the average human performance across a wide range of unseen tasks. This suggests that LLMs have the potential to serve as a viable alternative to human evaluation and other automated metrics. We asses
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13570</link><description>&lt;p&gt;
&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38543;&#26426;&#37197;&#32622;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#65288;IAI&#65289;&#20013;&#65292;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#24314;&#27169;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24378;&#22823;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20197;&#24378;&#35843;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#21644;&#26377;&#20215;&#20540;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;&#19982;&#20855;&#26377;&#20108;&#20540;&#21270;&#23454;&#29616;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30456;&#27604;&#65292;SCMs&#30340;&#27169;&#22411;&#23384;&#20648;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#38500;&#20102;SCM&#23398;&#20064;&#22120;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20316;&#20026;&#26412;&#25991;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;SCMs&#30340;&#23398;&#20064;&#33021;&#21147;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23454;&#39564;&#30740;&#31350;&#20063;&#36827;&#34892;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#24515;&#29702;&#20581;&#24247;&#30740;&#31350;&#35770;&#25991;&#65292;&#37319;&#29992;&#33258;&#23450;&#20041;&#23884;&#20837;&#27169;&#22411;&#65292;&#35782;&#21035;&#20986;&#35813;&#39046;&#22495;&#30340;&#19968;&#33324;&#36235;&#21183;&#21644;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13569</link><description>&lt;p&gt;
&#29992;&#20027;&#39064;&#24314;&#27169;&#21457;&#29616;&#24515;&#29702;&#20581;&#24247;&#30740;&#31350;&#35838;&#39064;
&lt;/p&gt;
&lt;p&gt;
Discovering Mental Health Research Topics with Topic Modeling. (arXiv:2308.13569v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#24515;&#29702;&#20581;&#24247;&#30740;&#31350;&#35770;&#25991;&#65292;&#37319;&#29992;&#33258;&#23450;&#20041;&#23884;&#20837;&#27169;&#22411;&#65292;&#35782;&#21035;&#20986;&#35813;&#39046;&#22495;&#30340;&#19968;&#33324;&#36235;&#21183;&#21644;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#20581;&#24247;&#26174;&#33879;&#24433;&#21709;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20854;&#37325;&#35201;&#24615;&#22312;&#30740;&#31350;&#30028;&#21644;&#22823;&#20247;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#35748;&#21487;&#65292;&#29305;&#21035;&#26159;&#22312;COVID-19&#22823;&#27969;&#34892;&#20043;&#21518;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#20986;&#29256;&#29289;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#36825;&#31181;&#27987;&#21402;&#30340;&#20852;&#36259;&#20063;&#20307;&#29616;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35770;&#25991;&#20013;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20998;&#26512;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#30740;&#31350;&#35770;&#25991;&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;&#35813;&#39046;&#22495;&#30340;&#19968;&#33324;&#36235;&#21183;&#65292;&#24182;&#25214;&#20986;&#20855;&#26377;&#39640;&#24433;&#21709;&#21147;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;&#21508;&#20010;&#25968;&#25454;&#24211;&#25910;&#38598;&#20102;&#25688;&#35201;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;BERTopic&#26694;&#26550;&#30340;&#33258;&#23450;&#20041;Sentence-BERT&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;96,676&#31687;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#23427;&#20204;&#30340;&#25688;&#35201;&#26469;&#30740;&#31350;&#19981;&#21516;&#20027;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#21478;&#22806;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#65306;Top2Vec&#27169;&#22411;&#21644;LDA-BERT&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mental health significantly influences various aspects of our daily lives, and its importance has been increasingly recognized by the research community and the general public, particularly in the wake of the COVID-19 pandemic. This heightened interest is evident in the growing number of publications dedicated to mental health in the past decade. In this study, our goal is to identify general trends in the field and pinpoint high-impact research topics by analyzing a large dataset of mental health research papers. To accomplish this, we collected abstracts from various databases and trained a customized Sentence-BERT based embedding model leveraging the BERTopic framework. Our dataset comprises 96,676 research papers pertaining to mental health, enabling us to examine the relationships between different topics using their abstracts. To evaluate the effectiveness of the model, we compared it against two other state-of-the-art methods: Top2Vec model and LDA-BERT model. The model demonstr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#21306;&#22495;&#35299;&#32806;&#25193;&#25955;&#27169;&#22411; (RDDM)&#65292;&#29992;&#20110;&#23558;PPG&#36716;&#25442;&#20026;ECG&#20449;&#21495;&#12290;RDDM&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21521;&#24863;&#20852;&#36259;&#21306;&#22495;&#28155;&#21152;&#22122;&#22768;&#26469;&#25429;&#25417;ECG&#20449;&#21495;&#30340;&#22797;&#26434;&#26102;&#38388;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2308.13568</link><description>&lt;p&gt;
&#21306;&#22495;&#35299;&#32806;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;PPG&#21040;ECG&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Region-Disentangled Diffusion Model for High-Fidelity PPG-to-ECG Translation. (arXiv:2308.13568v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13568
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#21306;&#22495;&#35299;&#32806;&#25193;&#25955;&#27169;&#22411; (RDDM)&#65292;&#29992;&#20110;&#23558;PPG&#36716;&#25442;&#20026;ECG&#20449;&#21495;&#12290;RDDM&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21521;&#24863;&#20852;&#36259;&#21306;&#22495;&#28155;&#21152;&#22122;&#22768;&#26469;&#25429;&#25417;ECG&#20449;&#21495;&#30340;&#22797;&#26434;&#26102;&#38388;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#39640;&#21457;&#29575;&#38656;&#35201;&#20415;&#25463;&#19988;&#32463;&#27982;&#26377;&#25928;&#30340;&#36830;&#32493;&#24515;&#33039;&#30417;&#27979;&#24037;&#20855;&#12290;&#23613;&#31649;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#40644;&#37329;&#26631;&#20934;&#65292;&#20294;&#36830;&#32493;&#30417;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#27492;&#20154;&#20204;&#24320;&#22987;&#25506;&#32034;&#20809;&#30005;&#23481;&#23481;&#31215;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#22522;&#26412;&#30340;&#21487;&#29992;&#20110;&#28040;&#36153;&#32773;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#36825;&#31181;&#24819;&#27861;&#26368;&#36817;&#24341;&#36215;&#20102;&#23558;PPG&#36716;&#21270;&#20026;ECG&#20449;&#21495;&#30340;&#20852;&#36259;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21306;&#22495;&#35299;&#32806;&#25193;&#25955;&#27169;&#22411;&#65288;RDDM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35774;&#35745;&#29992;&#20110;&#25429;&#25417;ECG&#22797;&#26434;&#26102;&#38388;&#21160;&#24577;&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#22914;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;&#25429;&#25417;&#36825;&#31181;&#32454;&#24494;&#24046;&#21035;&#26102;&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#25972;&#20010;&#20449;&#21495;&#19978;&#30340;&#22122;&#22768;&#28155;&#21152;&#36807;&#31243;&#26159;&#19981;&#21152;&#36873;&#25321;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;RDDM&#36890;&#36807;&#23558;&#22122;&#22768;&#26377;&#36873;&#25321;&#22320;&#28155;&#21152;&#21040;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#65288;&#22914;ECG&#20449;&#21495;&#20013;&#30340;QRS&#22797;&#21512;&#20307;&#65289;&#30340;&#26032;&#39062;&#27491;&#21521;&#36807;&#31243;&#21644;&#21453;&#21521;&#36807;&#31243;&#26469;&#20811;&#26381;&#27492;&#31867;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high prevalence of cardiovascular diseases (CVDs) calls for accessible and cost-effective continuous cardiac monitoring tools. Despite Electrocardiography (ECG) being the gold standard, continuous monitoring remains a challenge, leading to the exploration of Photoplethysmography (PPG), a promising but more basic alternative available in consumer wearables. This notion has recently spurred interest in translating PPG to ECG signals. In this work, we introduce Region-Disentangled Diffusion Model (RDDM), a novel diffusion model designed to capture the complex temporal dynamics of ECG. Traditional Diffusion models like Denoising Diffusion Probabilistic Models (DDPM) face challenges in capturing such nuances due to the indiscriminate noise addition process across the entire signal. Our proposed RDDM overcomes such limitations by incorporating a novel forward process that selectively adds noise to specific regions of interest (ROI) such as QRS complex in ECG signals, and a reverse proces
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLLM-DataEngine&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#24369;&#28857;&#65292;&#29983;&#25104;&#36866;&#24403;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.13566</link><description>&lt;p&gt;
MLLM-DataEngine&#65306;&#19968;&#31181;MLLM&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MLLM-DataEngine: An Iterative Refinement Approach for MLLM. (arXiv:2308.13566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MLLM-DataEngine&#30340;&#36845;&#20195;&#25913;&#36827;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#24369;&#28857;&#65292;&#29983;&#25104;&#36866;&#24403;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25351;&#23548;&#25968;&#25454;&#38598;&#26500;&#24314;&#21644;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#29420;&#31435;&#24615;&#20351;&#24471;&#24403;&#21069;&#30340;MLLM&#24456;&#38590;&#22312;&#30456;&#23545;&#36739;&#20302;&#30340;&#20154;&#21147;&#25104;&#26412;&#19979;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23553;&#38381;&#24490;&#29615;&#31995;&#32479;MLLM-DataEngine&#65292;&#23427;&#36830;&#25509;&#20102;&#25968;&#25454;&#29983;&#25104;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#22312;&#27599;&#20010;&#24490;&#29615;&#36845;&#20195;&#20013;&#65292;MLLM-DataEngine&#39318;&#20808;&#26681;&#25454;&#35780;&#20272;&#32467;&#26524;&#20998;&#26512;&#27169;&#22411;&#30340;&#24369;&#28857;&#65292;&#28982;&#21518;&#29983;&#25104;&#21512;&#36866;&#30340;&#22686;&#37327;&#25968;&#25454;&#38598;&#29992;&#20110;&#19979;&#19968;&#27425;&#35757;&#32451;&#36845;&#20195;&#65292;&#24182;&#36845;&#20195;&#22320;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#19982;&#20808;&#21069;&#19982;&#22522;&#20934;&#27979;&#35797;&#20998;&#31163;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#30456;&#27604;&#65292;MLLM-DataEngine&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;&#23450;&#20301;&#12289;&#36136;&#37327;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#37117;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental datas
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24191;&#20041;&#30697;&#26041;&#27861;&#65288;SGMM&#65289;&#65292;&#29992;&#20110;&#20272;&#35745;&#21644;&#25512;&#26029;&#30697;&#38480;&#21046;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#22312;&#32447;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.13564</link><description>&lt;p&gt;
SGMM: &#24191;&#20041;&#30697;&#26041;&#27861;&#30340;&#38543;&#26426;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
SGMM: Stochastic Approximation to Generalized Method of Moments. (arXiv:2308.13564v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13564
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#24191;&#20041;&#30697;&#26041;&#27861;&#65288;SGMM&#65289;&#65292;&#29992;&#20110;&#20272;&#35745;&#21644;&#25512;&#26029;&#30697;&#38480;&#21046;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#22312;&#32447;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#31867;&#65292;&#38543;&#26426;&#24191;&#20041;&#30697;&#26041;&#27861;&#65288;SGMM&#65289;&#65292;&#29992;&#20110;&#20272;&#35745;&#21644;&#25512;&#26029;&#65288;&#36229;&#35782;&#21035;&#65289;&#30697;&#38480;&#21046;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;SGMM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#36924;&#36817;&#26041;&#27861;&#65292;&#26367;&#20195;&#20102;&#27969;&#34892;&#30340;Hansen&#65288;1982&#24180;&#65289;&#30340;&#65288;&#31163;&#32447;&#65289;GMM&#65292;&#24182;&#25552;&#20379;&#20102;&#24555;&#36895;&#21644;&#21487;&#25193;&#23637;&#30340;&#23454;&#26102;&#27969;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SGMM&#23545;&#20110;&#25928;&#29575;&#19981;&#39640;&#30340;&#22312;&#32447;2SLS&#21644;&#39640;&#25928;&#30340;SGMM&#20855;&#26377;&#20960;&#20046;&#30830;&#23450;&#30340;&#25910;&#25947;&#24615;&#21644;&#65288;&#20989;&#25968;&#65289;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Durbin-Wu-Hausman&#21644;Sargan-Hansen&#27979;&#35797;&#30340;&#22312;&#32447;&#29256;&#26412;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;SGMM&#26694;&#26550;&#20013;&#12290;&#24191;&#27867;&#30340;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#38543;&#30528;&#26679;&#26412;&#37327;&#30340;&#22686;&#21152;&#65292;SGMM&#22312;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#19982;&#26631;&#20934;&#65288;&#31163;&#32447;&#65289;GMM&#30456;&#21305;&#37197;&#65292;&#24182;&#26174;&#31034;&#20986;&#22312;&#22823;&#35268;&#27169;&#21644;&#22312;&#32447;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#31034;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of algorithms, Stochastic Generalized Method of Moments (SGMM), for estimation and inference on (overidentified) moment restriction models. Our SGMM is a novel stochastic approximation alternative to the popular Hansen (1982) (offline) GMM, and offers fast and scalable implementation with the ability to handle streaming datasets in real time. We establish the almost sure convergence, and the (functional) central limit theorem for the inefficient online 2SLS and the efficient SGMM. Moreover, we propose online versions of the Durbin-Wu-Hausman and Sargan-Hansen tests that can be seamlessly integrated within the SGMM framework. Extensive Monte Carlo simulations show that as the sample size increases, the SGMM matches the standard (offline) GMM in terms of estimation accuracy and gains over computational efficiency, indicating its practical value for both large-scale and online datasets. We demonstrate the efficacy of our approach by a proof of concept using two we
&lt;/p&gt;</description></item><item><title>&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25509;&#21475;(ChatGPT, BARD&#21644;GPT4)&#22312;&#20998;&#26512;&#20107;&#25925;&#21465;&#36848;&#20013;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#25552;&#21462;&#20107;&#25925;&#30456;&#20851;&#20449;&#24687;&#21644;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#37117;&#20855;&#26377;&#19968;&#23450;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.13563</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#20107;&#25925;&#21465;&#36848;&#20013;&#30340;&#24212;&#29992;&#8212;&#8212;ChatGPT&#12289;BARD&#21644;GPT-4&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Analyzing Crash Narratives -- A Comparative Study of ChatGPT, BARD and GPT-4. (arXiv:2308.13563v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13563
&lt;/p&gt;
&lt;p&gt;
&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25509;&#21475;(ChatGPT, BARD&#21644;GPT4)&#22312;&#20998;&#26512;&#20107;&#25925;&#21465;&#36848;&#20013;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#25552;&#21462;&#20107;&#25925;&#30456;&#20851;&#20449;&#24687;&#21644;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#26041;&#38754;&#37117;&#20855;&#26377;&#19968;&#23450;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#36890;&#23433;&#20840;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#25991;&#26412;&#20998;&#26512;&#20174;&#20107;&#25925;&#21465;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20102;&#35299;&#27969;&#34892;&#30340;LLM&#25509;&#21475;&#22312;&#20998;&#31867;&#25110;&#20174;&#20107;&#25925;&#21465;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#30340;&#34920;&#29616;&#23558;&#38750;&#24120;&#26377;&#29992;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20351;&#29992;&#20102;&#30446;&#21069;&#26368;&#27969;&#34892;&#30340;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;LLM&#25509;&#21475;&#8212;&#8212;ChatGPT&#12289;BARD&#21644;GPT4&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23427;&#20204;&#22312;&#25552;&#21462;&#20449;&#24687;&#21644;&#22238;&#31572;&#19982;&#20107;&#25925;&#26377;&#20851;&#30340;&#26597;&#35810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#12290;&#30740;&#31350;&#20174;&#29233;&#33655;&#21326;&#24030;&#21644;&#22570;&#33832;&#26031;&#24030;&#30340;100&#20010;&#20107;&#25925;&#21465;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#23545;&#26597;&#35810;&#30340;&#21709;&#24212;&#12290;&#20116;&#20010;&#19982;&#21465;&#36848;&#30456;&#20851;&#30340;&#38382;&#39064;&#34987;&#25552;&#20986;&#65306;1&#65289;&#35841;&#26159;&#36131;&#20219;&#26041;&#65311;2&#65289;&#30896;&#25758;&#26041;&#24335;&#26159;&#20160;&#20040;&#65311;3&#65289;&#20107;&#25925;&#21457;&#29983;&#22312;&#24037;&#20316;&#21306;&#21527;&#65311;4&#65289;&#20107;&#25925;&#28041;&#21450;&#34892;&#20154;&#21527;&#65311;5&#65289;&#20107;&#25925;&#20013;&#26377;&#23475;&#20107;&#20214;&#30340;&#39034;&#24207;&#26159;&#20160;&#20040;&#65311;&#23545;&#20110;&#31532;1&#21040;&#31532;4&#20010;&#38382;&#39064;&#65292;&#19977;&#20010;LLM&#25509;&#21475;&#30340;&#22238;&#31572;&#37117;&#32463;&#36807;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In traffic safety research, extracting information from crash narratives using text analysis is a common practice. With recent advancements of large language models (LLM), it would be useful to know how the popular LLM interfaces perform in classifying or extracting information from crash narratives. To explore this, our study has used the three most popular publicly available LLM interfaces- ChatGPT, BARD and GPT4. This study investigated their usefulness and boundaries in extracting information and answering queries related to accidents from 100 crash narratives from Iowa and Kansas. During the investigation, their capabilities and limitations were assessed and their responses to the queries were compared. Five questions were asked related to the narratives: 1) Who is at-fault? 2) What is the manner of collision? 3) Has the crash occurred in a work-zone? 4) Did the crash involve pedestrians? and 5) What are the sequence of harmful events in the crash? For questions 1 through 4, the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#21435;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#20542;&#21521;&#24471;&#20998;&#21305;&#37197;&#21644;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#30740;&#31350;&#20351;&#29992;Lalonde&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#21435;&#23398;&#20064;&#20197;&#25552;&#39640;&#22240;&#26524;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13559</link><description>&lt;p&gt;
&#26426;&#22120;&#21435;&#23398;&#20064;&#29992;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning for Causal Inference. (arXiv:2308.13559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;&#21435;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#20542;&#21521;&#24471;&#20998;&#21305;&#37197;&#21644;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#12290;&#30740;&#31350;&#20351;&#29992;Lalonde&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#21435;&#23398;&#20064;&#20197;&#25552;&#39640;&#22240;&#26524;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#21644;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#27934;&#23519;&#21147;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#22240;&#26524;&#25512;&#26029;&#12290;&#20026;&#20102;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24536;&#35760;&#26377;&#20851;&#32473;&#23450;&#29992;&#25143;&#30340;&#19968;&#20123;&#23398;&#20064;/&#25429;&#25417;&#21040;&#30340;&#20449;&#24687;&#26159;&#24456;&#37325;&#35201;&#30340;&#65288;&#26426;&#22120;&#21435;&#23398;&#20064;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#29305;&#21035;&#26159;&#20542;&#21521;&#24471;&#20998;&#21305;&#37197;&#21644;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#65292;&#26088;&#22312;&#22312;&#28385;&#36275;&#19978;&#36848;&#21435;&#23398;&#20064;&#35201;&#27714;&#30340;&#21069;&#25552;&#19979;&#65292;&#25913;&#36827;&#21644;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20542;&#21521;&#24471;&#20998;&#27169;&#22411;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#26159;Lalonde&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#32844;&#19994;&#22521;&#35757;&#35745;&#21010;&#30340;&#26377;&#25928;&#24615;&#65288;&#21363;&#22788;&#29702;&#25928;&#24212;&#65289;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21021;&#22987;&#20542;&#21521;&#24471;&#20998;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#21019;&#24314;&#36951;&#24536;&#38598;&#26469;&#36827;&#34892;&#26426;&#22120;&#21435;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models play a vital role in making predictions and deriving insights from data and are being increasingly used for causal inference. To preserve user privacy, it is important to enable the model to forget some of its learning/captured information about a given user (machine unlearning). This paper introduces the concept of machine unlearning for causal inference, particularly propensity score matching and treatment effect estimation, which aims to refine and improve the performance of machine learning models for causal analysis given the above unlearning requirements. The paper presents a methodology for machine unlearning using a neural network-based propensity score model. The dataset used in the study is the Lalonde dataset, a widely used dataset for evaluating the effectiveness i.e. the treatment effect of job training programs. The methodology involves training an initial propensity score model on the original dataset and then creating forget sets by selectively r
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20114;&#21160;&#24335;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#25968;&#25454;&#25193;&#20805;&#20013;&#24341;&#20837;&#30340;&#20559;&#35265;&#65292;&#20197;&#21450;&#29992;&#20110;&#24230;&#37327;&#20559;&#35265;&#21152;&#37325;&#31243;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#26377;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#21487;&#29992;&#65292;&#20294;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#24230;&#37327;&#19981;&#21516;&#22270;&#20687;&#39046;&#22495;&#20013;&#30340;&#20559;&#35265;&#21152;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.13554</link><description>&lt;p&gt;
&#12298;&#23545;GAN&#22686;&#24378;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#36827;&#34892;&#23450;&#37327;&#21270;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#12299;&#30340;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study on Quantifying Bias in GAN-Augmented Data. (arXiv:2308.13554v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13554
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20114;&#21160;&#24335;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#25968;&#25454;&#25193;&#20805;&#20013;&#24341;&#20837;&#30340;&#20559;&#35265;&#65292;&#20197;&#21450;&#29992;&#20110;&#24230;&#37327;&#20559;&#35265;&#21152;&#37325;&#31243;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#26377;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#21487;&#29992;&#65292;&#20294;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#24230;&#37327;&#19981;&#21516;&#22270;&#20687;&#39046;&#22495;&#20013;&#30340;&#20559;&#35265;&#21152;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26368;&#36817;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#32773;&#20351;&#29992;&#30340;&#27969;&#34892;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#23384;&#22312;&#25152;&#35859;&#30340;&#27169;&#24335;&#23849;&#28291;&#25925;&#38556;&#27169;&#24335;&#65292;&#20351;&#20854;&#23481;&#26131;&#21152;&#21095;&#24050;&#32463;&#20559;&#26012;&#25968;&#25454;&#38598;&#19978;&#30340;&#20559;&#35265;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#25968;&#25454;&#20998;&#24067;&#27604;&#35757;&#32451;&#20998;&#24067;&#26356;&#19981;&#22810;&#26679;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23450;&#37327;&#21270;&#27169;&#24335;&#23849;&#28291;&#31243;&#24230;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#19968;&#39033;&#31995;&#32479;&#24615;&#30340;&#24037;&#20316;&#65292;&#37325;&#28857;&#35780;&#20272;&#21487;&#33021;&#23545;GAN&#22686;&#24378;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#23450;&#37327;&#21270;&#30340;&#26368;&#26032;&#24230;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#26377;&#20960;&#31181;&#26041;&#27861;&#21487;&#29992;&#65292;&#20294;&#27809;&#26377;&#21333;&#19968;&#30340;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#21487;&#38752;&#22320;&#23450;&#37327;&#21270;&#19981;&#21516;&#22270;&#20687;&#39046;&#22495;&#30340;&#20559;&#35265;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) have recently become a popular data augmentation technique used by machine learning practitioners. However, they have been shown to suffer from the so-called mode collapse failure mode, which makes them vulnerable to exacerbating biases on already skewed datasets, resulting in the generated data distribution being less diverse than the training distribution. To this end, we address the problem of quantifying the extent to which mode collapse occurs. This study is a systematic effort focused on the evaluation of state-of-the-art metrics that can potentially quantify biases in GAN-augmented data. We show that, while several such methods are available, there is no single metric that quantifies bias exacerbation reliably over the span of different image domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#33258;&#21160;&#32534;&#30721;&#21644;&#25945;&#24072;&#36755;&#20837;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#24322;&#27493;&#22312;&#32447;&#35752;&#35770;&#30340;ENA&#21487;&#35270;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;ENA&#27169;&#22411;&#19982;&#20154;&#24037;&#32534;&#30721;&#21592;&#26500;&#24314;&#30340;&#27169;&#22411;&#27809;&#26377;&#32479;&#35745;&#23398;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;ENA&#21487;&#20316;&#20026;&#19968;&#31181;&#24110;&#21161;&#25945;&#24072;&#35780;&#20272;&#24322;&#27493;&#22312;&#32447;&#35752;&#35770;&#30340;&#26377;&#25928;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.13549</link><description>&lt;p&gt;
&#32467;&#21512;&#33258;&#21160;&#32534;&#30721;&#21644;&#25945;&#24072;&#36755;&#20837;&#20197;&#29983;&#25104;&#29992;&#20110;&#24322;&#27493;&#22312;&#32447;&#35752;&#35770;&#30340;ENA&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Automatic Coding and Instructor Input to Generate ENA Visualizations for Asynchronous Online Discussion. (arXiv:2308.13549v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#33258;&#21160;&#32534;&#30721;&#21644;&#25945;&#24072;&#36755;&#20837;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#24322;&#27493;&#22312;&#32447;&#35752;&#35770;&#30340;ENA&#21487;&#35270;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#29983;&#25104;&#30340;ENA&#27169;&#22411;&#19982;&#20154;&#24037;&#32534;&#30721;&#21592;&#26500;&#24314;&#30340;&#27169;&#22411;&#27809;&#26377;&#32479;&#35745;&#23398;&#24046;&#24322;&#65292;&#36825;&#34920;&#26126;ENA&#21487;&#20316;&#20026;&#19968;&#31181;&#24110;&#21161;&#25945;&#24072;&#35780;&#20272;&#24322;&#27493;&#22312;&#32447;&#35752;&#35770;&#30340;&#26377;&#25928;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#22312;&#32447;&#35752;&#35770;&#26159;&#20419;&#36827;&#28151;&#21512;&#21644;&#22312;&#32447;&#35838;&#31243;&#20013;&#31038;&#20132;&#20114;&#21160;&#30340;&#24120;&#35265;&#22522;&#30784;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#25945;&#24072;&#32570;&#20047;&#24037;&#20855;&#26469;&#23436;&#25104;&#35780;&#20272;&#24322;&#27493;&#22312;&#32447;&#35752;&#35770;&#27963;&#21160;&#30340;&#32321;&#37325;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#26512;&#65288;LDA&#65289;&#21644;&#25945;&#24072;&#30340;&#20851;&#38190;&#35789;&#20174;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#25552;&#21462;&#20195;&#30721;&#12290;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#30340;&#20195;&#30721;&#26500;&#24314;&#20102;&#19968;&#20010;Epistemic Network Analysis&#65288;ENA&#65289;&#27169;&#22411;&#65292;&#24182;&#23558;&#35813;&#27169;&#22411;&#19982;&#20154;&#24037;&#32534;&#30721;&#21592;&#26500;&#24314;&#30340;&#20043;&#21069;&#30340;ENA&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#27809;&#26377;&#32479;&#35745;&#23398;&#24046;&#24322;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;ENA&#20316;&#20026;&#21487;&#35270;&#21270;&#24037;&#20855;&#24110;&#21161;&#25945;&#24072;&#35780;&#20272;&#24322;&#27493;&#22312;&#32447;&#35752;&#35770;&#30340;&#28508;&#22312;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
Asynchronous online discussions are a common fundamental tool to facilitate social interaction in hybrid and online courses. However, instructors lack the tools to accomplish the overwhelming task of evaluating asynchronous online discussion activities. In this paper we present an approach that uses Latent Dirichlet Analysis (LDA) and the instructor's keywords to automatically extract codes from a relatively small dataset. We use the generated codes to build an Epistemic Network Analysis (ENA) model and compare this model with a previous ENA model built by human coders. The results show that there is no statistical difference between the two models. We present an analysis of these models and discuss the potential use of ENA as a visualization to help instructors evaluating asynchronous online discussions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36229;&#25195;&#25551;&#25216;&#26415;&#65292;&#24341;&#20837;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25506;&#31350;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#24341;&#21457;&#30340;&#24773;&#32490;&#20256;&#26579;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#24773;&#32490;&#20256;&#26579;&#19982;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.13546</link><description>&lt;p&gt;
Hyperscanning EEG&#30340;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#25581;&#31034;&#20102;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#24341;&#21457;&#30340;&#24773;&#32490;&#20256;&#26579;
&lt;/p&gt;
&lt;p&gt;
Functional Graph Contrastive Learning of Hyperscanning EEG Reveals Emotional Contagion Evoked by Stereotype-Based Stressors. (arXiv:2308.13546v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36229;&#25195;&#25551;&#25216;&#26415;&#65292;&#24341;&#20837;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25506;&#31350;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#24341;&#21457;&#30340;&#24773;&#32490;&#20256;&#26579;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#24773;&#32490;&#20256;&#26579;&#19982;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#24773;&#32490;&#20256;&#26579;&#30340;&#32454;&#24494;&#24046;&#24322;&#21450;&#20854;&#23545;&#21452;&#20154;&#20114;&#21160;&#20013;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30740;&#31350;&#32858;&#28966;&#20110;&#22899;&#24615;&#23545;&#30340;&#21512;&#20316;&#35299;&#20915;&#38382;&#39064;&#20219;&#21153;&#20013;&#22522;&#20110;&#21051;&#26495;&#21360;&#35937;&#30340;&#21387;&#21147;&#32972;&#26223;&#12290;&#36890;&#36807;&#23545;&#24773;&#32490;&#20256;&#26579;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#25581;&#31034;&#20854;&#28508;&#22312;&#26426;&#21046;&#21644;&#24433;&#21709;&#12290;&#21033;&#29992;&#22522;&#20110;EEG&#30340;&#36229;&#25195;&#25551;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#21151;&#33021;&#24615;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;fGCL&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#25552;&#21462;&#20027;&#20307;&#19981;&#21464;&#30340;&#31070;&#32463;&#27963;&#21160;&#27169;&#24335;&#34920;&#31034;&#12290;&#36825;&#20123;&#34920;&#31034;&#36827;&#19968;&#27493;&#24212;&#29992;&#21160;&#24577;&#22270;&#20998;&#31867;&#65288;DGC&#65289;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#26088;&#22312;&#21078;&#26512;&#24773;&#32490;&#20256;&#26579;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#23545;&#33041;&#37096;&#21516;&#27493;&#21644;&#36830;&#25509;&#24615;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#24773;&#32490;&#20256;&#26579;&#19982;&#35748;&#30693;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#32467;&#26524;&#24378;&#35843;&#24773;&#32490;&#20256;&#26579;&#22312;&#22609;&#36896;&#36712;&#36857;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study delves into the intricacies of emotional contagion and its impact on performance within dyadic interactions. Specifically, it focuses on the context of stereotype-based stress (SBS) during collaborative problem-solving tasks among female pairs. Through an exploration of emotional contagion, the research seeks to unveil its underlying mechanisms and effects. Leveraging EEG-based hyperscanning technology, the study introduces an innovative approach known as functional Graph Contrastive Learning (fGCL), which extracts subject-invariant representations of neural activity patterns. These representations are further subjected to analysis using the Dynamic Graph Classification (DGC) model, aimed at dissecting the process of emotional contagion. By scrutinizing brain synchronization and connectivity, the study reveals the intricate interplay between emotional contagion and cognitive functioning. The results underscore the substantial role of emotional contagion in shaping the trajec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#23391;&#21152;&#25289;&#25991;&#26412;&#20998;&#31867;&#20013;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#24182;&#25910;&#38598;&#12289;&#27880;&#37322;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#20135;&#29983;&#20102;&#26368;&#20339;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.13545</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23391;&#21152;&#25289;&#25991;&#26412;&#20998;&#31867;&#30340;&#29305;&#24449;&#25552;&#21462;&#65288;arXiv:2308.13545v1 [cs.IR]&#65289;
&lt;/p&gt;
&lt;p&gt;
Feature Extraction Using Deep Generative Models for Bangla Text Classification on a New Comprehensive Dataset. (arXiv:2308.13545v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#23391;&#21152;&#25289;&#25991;&#26412;&#20998;&#31867;&#20013;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#27861;&#65292;&#24182;&#25910;&#38598;&#12289;&#27880;&#37322;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#20135;&#29983;&#20102;&#26368;&#20339;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#26159;&#25991;&#26412;&#25366;&#25496;&#21644;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#12290;&#23613;&#31649;&#23391;&#21152;&#25289;&#35821;&#26159;&#19990;&#30028;&#19978;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#31532;&#20845;&#22823;&#35821;&#35328;&#65292;&#20294;&#30001;&#20110;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#65292;&#23427;&#19968;&#30452;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#12289;&#27880;&#37322;&#21644;&#20934;&#22791;&#20102;&#19968;&#20010;&#21253;&#21547;212,184&#20010;&#23391;&#21152;&#25289;&#25991;&#26723;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#19971;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#24182;&#23558;&#20854;&#20844;&#24320;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65306;LSTM&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;LSTM VAE&#65289;&#12289;&#36741;&#21161;&#20998;&#31867;&#22120;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;AC-GAN&#65289;&#21644;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#65288;AAE&#65289;&#26469;&#25552;&#21462;&#25991;&#26412;&#29305;&#24449;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#24212;&#29992;&#26368;&#21021;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21457;&#29616;&#30340;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#36825;&#19977;&#20010;&#27169;&#22411;&#65292;&#24182;&#22312;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#20102;&#24471;&#21040;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#27169;&#22411;&#20135;&#29983;&#20102;&#26368;&#20339;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The selection of features for text classification is a fundamental task in text mining and information retrieval. Despite being the sixth most widely spoken language in the world, Bangla has received little attention due to the scarcity of text datasets. In this research, we collected, annotated, and prepared a comprehensive dataset of 212,184 Bangla documents in seven different categories and made it publicly accessible. We implemented three deep learning generative models: LSTM variational autoencoder (LSTM VAE), auxiliary classifier generative adversarial network (AC-GAN), and adversarial autoencoder (AAE) to extract text features, although their applications are initially found in the field of computer vision. We utilized our dataset to train these three models and used the feature space obtained in the document classification task. We evaluated the performance of the classifiers and found that the adversarial autoencoder model produced the best feature space.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#39033;&#27604;&#36739;&#29992;&#25143;&#27979;&#35797;&#35780;&#20272;&#20102;&#22320;&#29702;&#23450;&#20301;&#25968;&#25454;&#23545;&#22686;&#24378;&#29616;&#23454;&#21487;&#29992;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#19982;&#25511;&#21046;&#32452;&#30456;&#27604;&#65292;&#20351;&#29992;RTK&#25968;&#25454;&#30340;&#23454;&#39564;&#32452;&#30340;&#22320;&#29702;&#23450;&#20301;&#25968;&#25454;&#24179;&#22343;&#20934;&#30830;&#24615;&#26356;&#20302;&#65292;&#21487;&#29992;&#24615;&#35780;&#20998;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2308.13544</link><description>&lt;p&gt;
&#22320;&#29702;&#23450;&#20301;&#25968;&#25454;&#23545;&#22686;&#24378;&#29616;&#23454;&#21487;&#29992;&#24615;&#30340;&#24433;&#21709;&#65306;&#19968;&#39033;&#27604;&#36739;&#29992;&#25143;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Impact of geolocation data on augmented reality usability: A comparative user test. (arXiv:2308.13544v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#39033;&#27604;&#36739;&#29992;&#25143;&#27979;&#35797;&#35780;&#20272;&#20102;&#22320;&#29702;&#23450;&#20301;&#25968;&#25454;&#23545;&#22686;&#24378;&#29616;&#23454;&#21487;&#29992;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#19982;&#25511;&#21046;&#32452;&#30456;&#27604;&#65292;&#20351;&#29992;RTK&#25968;&#25454;&#30340;&#23454;&#39564;&#32452;&#30340;&#22320;&#29702;&#23450;&#20301;&#25968;&#25454;&#24179;&#22343;&#20934;&#30830;&#24615;&#26356;&#20302;&#65292;&#21487;&#29992;&#24615;&#35780;&#20998;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#20301;&#32622;&#30340;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#22312;&#25945;&#32946;&#20013;&#30340;&#20351;&#29992;&#24050;&#32463;&#35777;&#26126;&#23545;&#21442;&#19982;&#32773;&#30340;&#21160;&#26426;&#12289;&#21442;&#19982;&#24230;&#21644;&#36523;&#20307;&#27963;&#21160;&#26377;&#30410;&#22788;&#65292;&#20294;&#22320;&#29702;&#23450;&#20301;&#25968;&#25454;&#30340;&#19981;&#20934;&#30830;&#24615;&#23548;&#33268;&#22686;&#24378;&#23545;&#35937;&#25238;&#21160;&#25110;&#28418;&#31227;&#65292;&#36825;&#26159;&#38477;&#20302;&#29992;&#25143;&#20307;&#39564;&#30340;&#19968;&#20010;&#22240;&#32032;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20813;&#36153;&#24320;&#28304;&#30340;&#32593;&#32476;AR&#24212;&#29992;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#39033;&#27604;&#36739;&#29992;&#25143;&#27979;&#35797;&#65288;n = 54&#65289;&#65292;&#20197;&#35780;&#20272;&#22320;&#29702;&#23450;&#20301;&#25968;&#25454;&#23545;&#21487;&#29992;&#24615;&#12289;&#25506;&#32034;&#21644;&#20851;&#27880;&#30340;&#24433;&#21709;&#12290;&#25511;&#21046;&#32452;&#20351;&#29992;&#20102;&#31995;&#32479;&#19982;&#20869;&#23884;&#30340;GNSS&#25968;&#25454;&#32467;&#21512;&#65292;&#25506;&#32034;&#33258;&#28982;&#30028;&#30340;&#29983;&#29289;&#22810;&#26679;&#24615;&#65292;&#23454;&#39564;&#32452;&#20351;&#29992;&#20102;&#22806;&#37096;&#27169;&#22359;&#36827;&#34892;RTK&#25968;&#25454;&#12290;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#35760;&#24405;&#20102;&#30524;&#21160;&#25968;&#25454;&#12289;&#22320;&#29702;&#23450;&#20301;&#36712;&#36857;&#21644;&#24212;&#29992;&#20869;&#29992;&#25143;&#35302;&#21457;&#30340;&#20107;&#20214;&#12290;&#21442;&#19982;&#32773;&#22238;&#31572;&#20102;&#21487;&#29992;&#24615;&#38382;&#21367;&#65288;SUS&#12289;UEQ&#12289;HARUS&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23454;&#39564;&#32452;&#26292;&#38706;&#30340;RTK&#32452;&#22320;&#29702;&#23450;&#20301;&#25968;&#25454;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#20302;&#20110;&#25511;&#21046;&#32452;&#12290;&#23454;&#39564;&#32452;&#25253;&#21578;&#30340;&#21487;&#29992;&#24615;&#24471;&#20998;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract. While the use of location-based augmented reality (AR) for education has demonstrated benefits on participants' motivation, engagement, and on their physical activity, geolocation data inaccuracy causes augmented objects to jitter or drift, which is a factor in downgrading user experience. We developed a free and open source web AR application and conducted a comparative user test (n = 54) in order to assess the impact of geolocation data on usability, exploration, and focus. A control group explored biodiversity in nature using the system in combination with embedded GNSS data, and an experimental group used an external module for RTK data. During the test, eye tracking data, geolocated traces, and in-app user-triggered events were recorded. Participants answered usability questionnaires (SUS, UEQ, HARUS).We found that the geolocation data the RTK group was exposed to was less accurate in average than that of the control group. The RTK group reported lower usability scores o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20813;&#36153;&#30340;&#23545;&#25239;&#24615;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#35268;&#33539;&#29992;&#25143;/&#39033;&#30446;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;SharpCF&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.13541</link><description>&lt;p&gt;
&#38754;&#21521;&#20813;&#36153;&#30340;&#23545;&#25239;&#24615;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Adversarial Collaborative Filtering for Free. (arXiv:2308.13541v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13541
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#20813;&#36153;&#30340;&#23545;&#25239;&#24615;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#35268;&#33539;&#29992;&#25143;/&#39033;&#30446;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;SharpCF&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#24050;&#25104;&#21151;&#29992;&#20110;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CF&#26041;&#27861;&#23384;&#22312;&#22122;&#22768;&#25968;&#25454;&#38382;&#39064;&#65292;&#36825;&#23545;&#25512;&#33616;&#30340;&#36136;&#37327;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#35268;&#33539;&#29992;&#25143;/&#39033;&#30446;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26694;&#26550;&#19979;&#23398;&#20064;&#23545;&#25239;&#25200;&#21160;&#21644;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;1&#65289;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#26080;&#27861;&#35299;&#37322;&#20026;&#20160;&#20040;&#28155;&#21152;&#25200;&#21160;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65307;2&#65289;&#35299;&#20915;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#38656;&#35201;&#32791;&#26102;&#12290;&#38500;&#20102;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#65292;&#27599;&#27425;&#36845;&#20195;&#36824;&#38656;&#35201;&#39069;&#22806;&#35745;&#31639;&#26469;&#26356;&#26032;&#25200;&#21160;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#35268;&#27169;&#22823;&#30340;&#24037;&#19994;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Sharpness-aware Collaborative Filtering&#65288;SharpCF&#65289;&#26041;&#27861;&#65292;&#23427;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Filtering (CF) has been successfully used to help users discover the items of interest. Nevertheless, existing CF methods suffer from noisy data issue, which negatively impacts the quality of recommendation. To tackle this problem, many prior studies leverage adversarial learning to regularize the representations of users/items, which improves both generalizability and robustness. Those methods often learn adversarial perturbations and model parameters under min-max optimization framework. However, there still have two major drawbacks: 1) Existing methods lack theoretical guarantees of why adding perturbations improve the model generalizability and robustness; 2) Solving min-max optimization is time-consuming. In addition to updating the model parameters, each iteration requires additional computations to update the perturbations, making them not scalable for industry-scale datasets.  In this paper, we present Sharpness-aware Collaborative Filtering (SharpCF), a simple ye
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;STEM&#30340;&#26032;&#33539;&#20363;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#25512;&#33616;&#20013;&#30340;&#36127;&#20256;&#36882;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;STEM&#36890;&#36807;&#26681;&#25454;&#26679;&#26412;&#20013;&#27491;&#21453;&#39304;&#25968;&#37327;&#30340;&#30456;&#23545;&#27604;&#20363;&#36827;&#34892;&#32454;&#20998;&#65292;&#28145;&#20837;&#30740;&#31350;&#26679;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13537</link><description>&lt;p&gt;
STEM:&#37322;&#25918;Embedding&#22312;&#22810;&#20219;&#21153;&#25512;&#33616;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
STEM: Unleashing the Power of Embeddings for Multi-task Recommendation. (arXiv:2308.13537v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;STEM&#30340;&#26032;&#33539;&#20363;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#25512;&#33616;&#20013;&#30340;&#36127;&#20256;&#36882;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;STEM&#36890;&#36807;&#26681;&#25454;&#26679;&#26412;&#20013;&#27491;&#21453;&#39304;&#25968;&#37327;&#30340;&#30456;&#23545;&#27604;&#20363;&#36827;&#34892;&#32454;&#20998;&#65292;&#28145;&#20837;&#30740;&#31350;&#26679;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#12290;MTL&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#36127;&#20256;&#36882;&#30340;&#21457;&#29983;&#65292;&#21363;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#30340;&#20914;&#31361;&#23548;&#33268;&#26576;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#36807;&#23558;&#25152;&#26377;&#26679;&#26412;&#35270;&#20026;&#19968;&#20010;&#25972;&#20307;&#26469;&#25506;&#32034;&#36127;&#20256;&#36882;&#65292;&#24573;&#35270;&#20102;&#20854;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26681;&#25454;&#20219;&#21153;&#20043;&#38388;&#27491;&#21453;&#39304;&#30340;&#30456;&#23545;&#25968;&#37327;&#23558;&#26679;&#26412;&#36827;&#34892;&#32454;&#20998;&#65292;&#28145;&#20837;&#30740;&#31350;&#26679;&#26412;&#30340;&#22797;&#26434;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#29616;&#26377;MTL&#26041;&#27861;&#22312;&#25910;&#21040;&#21508;&#20219;&#21153;&#31867;&#20284;&#21453;&#39304;&#30340;&#26679;&#26412;&#19978;&#20173;&#28982;&#23384;&#22312;&#36127;&#20256;&#36882;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20849;&#20139;&#23884;&#20837;&#30340;&#33539;&#20363;&#65292;&#24182;&#19988;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#30340;&#22833;&#36133;&#21487;&#20197;&#24402;&#22240;&#20110;&#20351;&#29992;&#36825;&#31181;&#36890;&#29992;&#23884;&#20837;&#26469;&#24314;&#27169;&#19981;&#21516;&#29992;&#25143;&#20559;&#22909;&#30340;&#26377;&#38480;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) has gained significant popularity in recommendation systems as it enables the simultaneous optimization of multiple objectives. A key challenge in MTL is the occurrence of negative transfer, where the performance of certain tasks deteriorates due to conflicts between tasks. Existing research has explored negative transfer by treating all samples as a whole, overlooking the inherent complexities within them. To this end, we delve into the intricacies of samples by splitting them based on the relative amount of positive feedback among tasks. Surprisingly, negative transfer still occurs in existing MTL methods on samples that receive comparable feedback across tasks. It is worth noting that existing methods commonly employ a shared-embedding paradigm, and we hypothesize that their failure can be attributed to the limited capacity of modeling diverse user preferences across tasks using such universal embeddings.  In this paper, we introduce a novel paradigm called
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#24335;&#30340;ZCA&#30333;&#21270;&#20316;&#29992;&#65292;&#20197;&#21450;&#20854;&#23545;&#20302;&#32500;&#29289;&#21697;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13536</link><description>&lt;p&gt;
&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38544;&#24335;ZCA&#30333;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Implicit ZCA Whitening Effects of Linear Autoencoders for Recommendation. (arXiv:2308.13536v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#24335;&#30340;ZCA&#30333;&#21270;&#20316;&#29992;&#65292;&#20197;&#21450;&#20854;&#23545;&#20302;&#32500;&#29289;&#21697;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#32447;&#24615;&#22238;&#24402;&#65288;&#33258;&#21160;&#32534;&#30721;&#22120;&#65289;&#27169;&#22411;&#34987;&#30740;&#31350;&#20316;&#20026;&#23398;&#20064;&#29289;&#21697;&#30456;&#20284;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#19982;&#25512;&#33616;&#25968;&#25454;&#30340;ZCA&#30333;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#23545;&#20598;&#24418;&#24335;&#35299;&#22312;&#29289;&#21697;&#30340;&#29305;&#24449;&#21521;&#37327;&#19978;&#23454;&#38469;&#19978;&#20855;&#26377;ZCA&#30333;&#21270;&#25928;&#26524;&#65292;&#32780;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;/&#22238;&#24402;&#27169;&#22411;&#30340;&#21407;&#22987;&#38382;&#39064;&#20013;&#65292;&#29289;&#21697;&#34987;&#35270;&#20026;&#36755;&#20837;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23558;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#24212;&#29992;&#20110;&#20351;&#29992;Item2vec&#31561;&#23884;&#20837;&#26041;&#27861;&#33719;&#24471;&#30340;&#20302;&#32500;&#29289;&#21697;&#21521;&#37327;&#26469;&#20272;&#35745;&#29289;&#21697;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#21021;&#27493;&#32467;&#26524;&#65292;&#34920;&#26126;&#30333;&#21270;&#20302;&#32500;&#29289;&#21697;&#23884;&#20837;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, in the field of recommendation systems, linear regression (autoencoder) models have been investigated as a way to learn item similarity. In this paper, we show a connection between a linear autoencoder model and ZCA whitening for recommendation data. In particular, we show that the dual form solution of a linear autoencoder model actually has ZCA whitening effects on feature vectors of items, while items are considered as input features in the primal problem of the autoencoder/regression model. We also show the correctness of applying a linear autoencoder to low-dimensional item vectors obtained using embedding methods such as Item2vec to estimate item-item similarities. Our experiments provide preliminary results indicating the effectiveness of whitening low-dimensional item embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#32447;&#24615;&#36172;&#21338;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#32447;&#35745;&#31639;&#30340;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#22312;&#22122;&#22768;&#34928;&#33853;&#20449;&#36947;&#19978;&#36827;&#34892;&#30340;&#27169;&#25311;&#20449;&#21495;&#20256;&#36755;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#38477;&#20302;&#32047;&#31215;&#36951;&#25022;&#26041;&#38754;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13298</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#32447;&#35745;&#31639;&#23454;&#29616;&#32852;&#37030;&#32447;&#24615;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Linear Bandit Learning via Over-the-Air Computation. (arXiv:2308.13298v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#32852;&#37030;&#32447;&#24615;&#36172;&#21338;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26080;&#32447;&#35745;&#31639;&#30340;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#36890;&#36807;&#22312;&#22122;&#22768;&#34928;&#33853;&#20449;&#36947;&#19978;&#36827;&#34892;&#30340;&#27169;&#25311;&#20449;&#21495;&#20256;&#36755;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#38477;&#20302;&#32047;&#31215;&#36951;&#25022;&#26041;&#38754;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30001;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#35774;&#22791;&#32452;&#25104;&#30340;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#32852;&#37030;&#32972;&#26223;&#19979;&#30340;&#32447;&#24615;&#36172;&#21338;&#23398;&#20064;&#12290;&#27599;&#20010;&#35774;&#22791;&#19982;&#29615;&#22659;&#20132;&#20114;&#65292;&#22312;&#25509;&#25910;&#21040;&#22870;&#21169;&#21518;&#36873;&#25321;&#19968;&#20010;&#21160;&#20316;&#65292;&#24182;&#23558;&#27169;&#22411;&#26356;&#26032;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#26368;&#23567;&#21270;&#25152;&#26377;&#35774;&#22791;&#30340;&#32047;&#31215;&#36951;&#25022;&#12290;&#20026;&#20102;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#35774;&#22791;&#36890;&#36807;&#26080;&#32447;&#35745;&#31639;&#65288;AirComp&#65289;&#22312;&#22122;&#22768;&#34928;&#33853;&#20449;&#36947;&#19978;&#19982;&#26381;&#21153;&#22120;&#36890;&#20449;&#65292;&#20854;&#20013;&#36890;&#36947;&#22122;&#22768;&#21487;&#33021;&#20250;&#25197;&#26354;&#20449;&#21495;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#32852;&#37030;&#32447;&#24615;&#36172;&#21338;&#26041;&#26696;&#65292;&#20854;&#20013;&#27599;&#20010;&#35774;&#22791;&#20256;&#36755;&#19968;&#20010;&#27169;&#25311;&#20449;&#21495;&#65292;&#26381;&#21153;&#22120;&#25509;&#25910;&#21040;&#30340;&#26159;&#36825;&#20123;&#20449;&#21495;&#30340;&#21472;&#21152;&#65292;&#21463;&#21040;&#20449;&#36947;&#22122;&#22768;&#30340;&#25197;&#26354;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#25968;&#23398;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#35813;&#26041;&#26696;&#30340;&#36951;&#25022;&#19978;&#38480;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;&#24615;&#33021;&#26041;&#38754;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate federated contextual linear bandit learning within a wireless system that comprises a server and multiple devices. Each device interacts with the environment, selects an action based on the received reward, and sends model updates to the server. The primary objective is to minimize cumulative regret across all devices within a finite time horizon. To reduce the communication overhead, devices communicate with the server via over-the-air computation (AirComp) over noisy fading channels, where the channel noise may distort the signals. In this context, we propose a customized federated linear bandits scheme, where each device transmits an analog signal, and the server receives a superposition of these signals distorted by channel noise. A rigorous mathematical analysis is conducted to determine the regret bound of the proposed scheme. Both theoretical analysis and numerical experiments demonstrate the competitive performance of our proposed scheme in terms o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HDUS&#30340;&#20998;&#24067;&#24335;&#36951;&#24536;&#26694;&#26550;&#65292;&#20351;&#29992;&#31181;&#23376;&#27169;&#22411;&#33976;&#39311;&#26500;&#24314;&#21487;&#25830;&#38500;&#30340;&#27169;&#22411;&#38598;&#25104;&#65292;&#36866;&#29992;&#20110;&#24322;&#26500;&#30340;&#35774;&#22791;&#31471;&#27169;&#22411;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13269</link><description>&lt;p&gt;
&#24322;&#26500;&#20998;&#24067;&#24335;&#26426;&#22120;&#36951;&#24536;&#21644;&#31181;&#23376;&#27169;&#22411;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Decentralized Machine Unlearning with Seed Model Distillation. (arXiv:2308.13269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HDUS&#30340;&#20998;&#24067;&#24335;&#36951;&#24536;&#26694;&#26550;&#65292;&#20351;&#29992;&#31181;&#23376;&#27169;&#22411;&#33976;&#39311;&#26500;&#24314;&#21487;&#25830;&#38500;&#30340;&#27169;&#22411;&#38598;&#25104;&#65292;&#36866;&#29992;&#20110;&#24322;&#26500;&#30340;&#35774;&#22791;&#31471;&#27169;&#22411;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19968;&#20123;&#26368;&#36817;&#30340;&#20449;&#24687;&#23433;&#20840;&#27861;&#35268;&#36171;&#20104;&#29992;&#25143;&#23545;&#20219;&#20309;&#32463;&#36807;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25317;&#26377;&#34987;&#36951;&#24536;&#30340;&#26080;&#26465;&#20214;&#26435;&#21033;&#65292;&#20010;&#24615;&#21270;&#29289;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#21830;&#24517;&#39035;&#32771;&#34385;&#21040;&#36951;&#24536;&#21151;&#33021;&#12290;&#21462;&#28040;&#23398;&#20064;&#29992;&#25143;&#36129;&#29486;&#30340;&#26368;&#30452;&#25509;&#26041;&#27861;&#26159;&#20174;&#21021;&#22987;&#29366;&#24577;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#22312;&#39057;&#32321;&#30340;&#36951;&#24536;&#35831;&#27714;&#20013;&#65292;&#36825;&#22312;&#39640;&#21534;&#21520;&#37327;&#24212;&#29992;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#19968;&#20123;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#26469;&#21152;&#36895;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#36866;&#24212;&#20998;&#24067;&#24335;&#23398;&#20064;&#22330;&#26223;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;HDUS&#30340;&#20998;&#24067;&#24335;&#36951;&#24536;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#33976;&#39311;&#30340;&#31181;&#23376;&#27169;&#22411;&#20026;&#25152;&#26377;&#23458;&#25143;&#31471;&#26500;&#24314;&#21487;&#25830;&#38500;&#30340;&#38598;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35813;&#26694;&#26550;&#19982;&#24322;&#26500;&#30340;&#35774;&#22791;&#31471;&#27169;&#22411;&#20860;&#23481;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#21487;&#20280;&#32553;&#24615;&#65292;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;HDUS&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As some recent information security legislation endowed users with unconditional rights to be forgotten by any trained machine learning model, personalized IoT service providers have to put unlearning functionality into their consideration. The most straightforward method to unlearn users' contribution is to retrain the model from the initial state, which is not realistic in high throughput applications with frequent unlearning requests. Though some machine unlearning frameworks have been proposed to speed up the retraining process, they fail to match decentralized learning scenarios. In this paper, we design a decentralized unlearning framework called HDUS, which uses distilled seed models to construct erasable ensembles for all clients. Moreover, the framework is compatible with heterogeneous on-device models, representing stronger scalability in real-world applications. Extensive experiments on three real-world datasets show that our HDUS achieves state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Laplace-LoRA&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26469;&#22686;&#24378;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13111</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20302;&#31209;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bayesian low-rank adaptation for large language models. (arXiv:2308.13111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Laplace-LoRA&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#26469;&#22686;&#24378;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#39640;&#25928;&#24494;&#35843;&#30340;&#26032;&#33539;&#24335;&#65292;&#20854;&#20013;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#24448;&#24448;&#21464;&#24471;&#36807;&#20110;&#33258;&#20449;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#20855;&#26377;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#21487;&#20316;&#20026;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#24182;&#22686;&#24378;&#26657;&#20934;&#33021;&#21147;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Laplace-LoRA&#65292;&#19968;&#31181;&#30452;&#35266;&#32780;&#26377;&#25928;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23427;&#23558;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#24212;&#29992;&#20110;LoRA&#21442;&#25968;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#30340;&#26657;&#20934;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs), with low-rank adaptation (LoRA) being a widely adopted choice. However, fine-tuned LLMs often become overconfident especially on when fine-tuned on smaller datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, a straightforward yet effective Bayesian method, which applies the Laplace approximation to the LoRA parameters and, considerably boosts the calibration of fine-tuned LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12902</link><description>&lt;p&gt;
CDAN: &#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDAN&#30340;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65292;&#29992;&#20110;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#12290;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#12289;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#22270;&#20687;&#20197;&#19981;&#36275;&#30340;&#29031;&#26126;&#20026;&#29305;&#24449;&#65292;&#38754;&#20020;&#28165;&#26224;&#24230;&#20943;&#24369;&#12289;&#39068;&#33394;&#26263;&#28129;&#21644;&#32454;&#33410;&#20943;&#23569;&#30340;&#25361;&#25112;&#12290;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#25913;&#21892;&#20142;&#24230;&#12289;&#23545;&#27604;&#24230;&#21644;&#25972;&#20307;&#24863;&#30693;&#36136;&#37327;&#26469;&#32416;&#27491;&#36825;&#20123;&#38382;&#39064;&#65292;&#20174;&#32780;&#20419;&#36827;&#20934;&#30830;&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#21367;&#31215;&#31264;&#23494;&#27880;&#24847;&#21147;&#24341;&#23548;&#32593;&#32476;&#65288;CDAN&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#20302;&#20809;&#22270;&#20687;&#12290;CDAN&#23558;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#19982;&#21367;&#31215;&#21644;&#31264;&#23494;&#22359;&#30456;&#32467;&#21512;&#65292;&#37197;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#36339;&#36291;&#36830;&#25509;&#12290;&#35813;&#26550;&#26500;&#30830;&#20445;&#20102;&#26377;&#25928;&#30340;&#20449;&#24687;&#20256;&#36882;&#21644;&#29305;&#24449;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#19987;&#38376;&#30340;&#21518;&#22788;&#29702;&#38454;&#27573;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#33394;&#24425;&#24179;&#34913;&#21644;&#23545;&#27604;&#24230;&#12290;&#19982;&#20302;&#20809;&#22270;&#20687;&#22686;&#24378;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-light images, characterized by inadequate illumination, pose challenges of diminished clarity, muted colors, and reduced details. Low-light image enhancement, an essential task in computer vision, aims to rectify these issues by improving brightness, contrast, and overall perceptual quality, thereby facilitating accurate analysis and interpretation. This paper introduces the Convolutional Dense Attention-guided Network (CDAN), a novel solution for enhancing low-light images. CDAN integrates an autoencoder-based architecture with convolutional and dense blocks, complemented by an attention mechanism and skip connections. This architecture ensures efficient information propagation and feature learning. Furthermore, a dedicated post-processing phase refines color balance and contrast. Our approach demonstrates notable progress compared to state-of-the-art results in low-light image enhancement, showcasing its robustness across a wide range of challenging scenarios. Our model performs 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2308.12143</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIA)&#36890;&#36807;&#26597;&#35810;&#27169;&#22411;&#26469;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35760;&#24405;&#12290;&#23545;&#32463;&#20856;&#20998;&#31867;&#27169;&#22411;&#30340;MIA&#24050;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#23558;MIA&#24212;&#29992;&#21040;&#29983;&#25104;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;&#29983;&#25104;&#27169;&#22411;&#30340;MIA&#20027;&#35201;&#20381;&#36182;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#36807;&#25311;&#21512;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#36991;&#20813;&#65292;&#32780;&#29616;&#26377;&#30340;MIA&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#19982;&#36807;&#25311;&#21512;&#19981;&#21516;&#65292;&#35760;&#24518;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20026;&#26222;&#36941;&#30340;&#29616;&#35937;&#12290;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#23548;&#33268;&#29983;&#25104;&#35760;&#24405;&#30340;&#27010;&#29575;&#20998;&#24067;&#21576;&#29616;&#20986;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#23427;&#26159;&#19968;&#31181;&#40657;&#30418;MIA&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#27874;&#21160;&#26469;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26680;&#23398;&#20064;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#26694;&#26550;(MKL-$L_{0/1}$-SVM)&#65292;&#36890;&#36807;&#24320;&#21457;&#24555;&#36895;&#30340;ADMM&#27714;&#35299;&#22120;&#22788;&#29702;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#39046;&#20808;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12016</link><description>&lt;p&gt;
MKL-$L_{0/1}$-SVM: &#19968;&#31181;&#22810;&#26680;&#23398;&#20064;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MKL-$L_{0/1}$-SVM. (arXiv:2308.12016v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26680;&#23398;&#20064;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#26694;&#26550;(MKL-$L_{0/1}$-SVM)&#65292;&#36890;&#36807;&#24320;&#21457;&#24555;&#36895;&#30340;ADMM&#27714;&#35299;&#22120;&#22788;&#29702;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#39046;&#20808;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;$(0, 1)$&#25439;&#22833;&#20989;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#22810;&#26680;&#23398;&#20064;&#65288;MKL&#65289;&#26694;&#26550;&#12290;&#39318;&#20808;&#32473;&#20986;&#20102;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#28982;&#21518;&#21033;&#29992;&#23427;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#24555;&#36895;&#30340;ADMM&#27714;&#35299;&#22120;&#26469;&#22788;&#29702;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#35814;&#32454;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;MKL-$L_{0/1}$-SVM&#30340;&#24615;&#33021;&#19982;&#19968;&#31181;&#21517;&#20026;SimpleMKL&#30340;&#39046;&#20808;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a Multiple Kernel Learning (abbreviated as MKL) framework for the Support Vector Machine (SVM) with the $(0, 1)$ loss function. Some first-order optimality conditions are given and then exploited to develop a fast ADMM solver to deal with the nonconvex and nonsmooth optimization problem. Extensive numerical experiments on synthetic and real datasets show that the performance of our MKL-$L_{0/1}$-SVM is comparable with the one of the leading approaches called SimpleMKL developed by Rakotomamonjy, Bach, Canu, and Grandvalet [Journal of Machine Learning Research, vol. 9, pp. 2491-2521, 2008].
&lt;/p&gt;</description></item><item><title>ProAgent&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20027;&#21160;&#21512;&#20316;&#30340;AI&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#38431;&#21451;&#30340;&#20915;&#31574;&#24182;&#20026;&#33258;&#24049;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11339</link><description>&lt;p&gt;
ProAgent&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20027;&#21160;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
ProAgent: Building Proactive Cooperative AI with Large Language Models. (arXiv:2308.11339v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11339
&lt;/p&gt;
&lt;p&gt;
ProAgent&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20027;&#21160;&#21512;&#20316;&#30340;AI&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#38431;&#21451;&#30340;&#20915;&#31574;&#24182;&#20026;&#33258;&#24049;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AGI&#30740;&#31350;&#20013;&#65292;&#26500;&#24314;&#20855;&#26377;&#33258;&#36866;&#24212;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#20197;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#20851;&#27880;&#28857;&#12290;&#30446;&#21069;&#65292;&#24320;&#21457;&#21512;&#20316;&#20195;&#29702;&#20154;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25919;&#31574;&#27867;&#21270;&#20005;&#37325;&#20381;&#36182;&#20110;&#19982;&#29305;&#23450;&#38431;&#21451;&#30340;&#36807;&#21435;&#20114;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#38480;&#21046;&#20102;&#20195;&#29702;&#20154;&#22312;&#38754;&#23545;&#26032;&#30340;&#38431;&#21451;&#26102;&#37325;&#26032;&#26657;&#20934;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#39044;&#27979;&#38431;&#21451;&#26410;&#26469;&#20915;&#31574;&#33021;&#21147;&#21644;&#20026;&#33258;&#36523;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#33021;&#21147;&#30340;&#20027;&#21160;&#20195;&#29702;&#12290;ProAgent&#22312;&#21512;&#20316;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#21160;&#24577;&#35843;&#25972;&#34892;&#20026;&#20197;&#22686;&#24378;&#19982;&#38431;&#21451;&#30340;&#21327;&#20316;&#21162;&#21147;&#12290;&#27492;&#22806;&#65292;ProAgent&#26694;&#26550;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20415;&#20110;&#26080;&#32541;&#38598;&#25104;&#65292;&#20197;&#24212;&#23545;&#21508;&#31181;&#21327;&#35843;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34892;&#21160;&#21644;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#26234;&#33021;&#20307;&#65292;&#20854;&#33021;&#22815;&#21516;&#26102;&#20316;&#20026;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#65292;&#36890;&#36807;&#34892;&#21160;&#28436;&#31034;&#21644;&#35821;&#35328;&#25351;&#20196;&#22686;&#24378;&#20102;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#32467;&#21512;&#34892;&#21160;&#21644;&#35821;&#35328;&#27807;&#36890;&#27169;&#24335;&#23545;&#23398;&#20064;&#32467;&#26524;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.10842</link><description>&lt;p&gt;
&#36890;&#36807;&#34892;&#21160;&#21644;&#35821;&#35328;&#25552;&#21319;&#26234;&#33021;&#20307;&#30340;&#27807;&#36890;&#21644;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Agent Communication and Learning through Action and Language. (arXiv:2308.10842v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10842
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34892;&#21160;&#21644;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#26234;&#33021;&#20307;&#65292;&#20854;&#33021;&#22815;&#21516;&#26102;&#20316;&#20026;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#65292;&#36890;&#36807;&#34892;&#21160;&#28436;&#31034;&#21644;&#35821;&#35328;&#25351;&#20196;&#22686;&#24378;&#20102;&#27807;&#36890;&#25928;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#32467;&#21512;&#34892;&#21160;&#21644;&#35821;&#35328;&#27807;&#36890;&#27169;&#24335;&#23545;&#23398;&#20064;&#32467;&#26524;&#30340;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;GC&#26234;&#33021;&#20307;&#31867;&#21035;&#65292;&#33021;&#22815;&#21516;&#26102;&#20805;&#24403;&#25945;&#24072;&#21644;&#23398;&#20064;&#32773;&#30340;&#35282;&#33394;&#12290;&#20511;&#21161;&#22522;&#20110;&#34892;&#21160;&#30340;&#28436;&#31034;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#22686;&#24378;&#20102;&#27807;&#36890;&#25928;&#29575;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20154;&#31867;&#27807;&#36890;&#21644;&#30446;&#26631;&#23454;&#29616;&#20013;&#30340;&#37325;&#35201;&#20803;&#32032;&#8212;&#8212;&#25945;&#32946;&#23398;&#21644;&#23454;&#29992;&#20027;&#20041;&#30340;&#34701;&#20837;&#65292;&#25552;&#21319;&#20102;&#26234;&#33021;&#20307;&#30340;&#25945;&#23398;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32467;&#21512;&#34892;&#21160;&#21644;&#35821;&#35328;&#27807;&#36890;&#27169;&#24335;&#23545;&#23398;&#20064;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#22810;&#27169;&#24335;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel category of GC-agents capable of functioning as both teachers and learners. Leveraging action-based demonstrations and language-based instructions, these agents enhance communication efficiency. We investigate the incorporation of pedagogy and pragmatism, essential elements in human communication and goal achievement, enhancing the agents' teaching and learning capabilities. Furthermore, we explore the impact of combining communication modes (action and language) on learning outcomes, highlighting the benefits of a multi-modal approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#30340;&#26032;&#32452;&#20214;&#65292;&#22312;&#26222;&#36890;&#30340;Transformer&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20132;&#36890;&#39044;&#27979;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#31354;&#20851;&#31995;&#21644;&#26102;&#38388;&#20449;&#24687;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10425</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#65292;&#26222;&#36890;Transformer&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting. (arXiv:2308.10425v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#30340;&#26032;&#32452;&#20214;&#65292;&#22312;&#26222;&#36890;&#30340;Transformer&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20132;&#36890;&#39044;&#27979;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#31354;&#20851;&#31995;&#21644;&#26102;&#38388;&#20449;&#24687;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20934;&#30830;&#30340;&#20132;&#36890;&#39044;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20854;&#26680;&#24515;&#29942;&#39048;&#22312;&#20110;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20132;&#36890;&#27169;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20855;&#26377;&#22797;&#26434;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#27493;&#36935;&#21040;&#20102;&#24615;&#33021;&#25910;&#30410;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#30340;&#26032;&#32452;&#20214;&#65292;&#21487;&#20197;&#22312;&#26222;&#36890;&#30340;Transformer&#20013;&#33719;&#24471;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;Spatio-Temporal Adaptive Embedding transformer&#65288;STAEformer&#65289;&#22312;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20132;&#36890;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#36890;&#36807;&#26377;&#25928;&#25429;&#25417;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20869;&#22312;&#26102;&#31354;&#20851;&#31995;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of the Intelligent Transportation System (ITS), accurate traffic forecasting has emerged as a critical challenge. The key bottleneck lies in capturing the intricate spatio-temporal traffic patterns. In recent years, numerous neural networks with complicated architectures have been proposed to address this issue. However, the advancements in network architectures have encountered diminishing performance gains. In this study, we present a novel component called spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. Our proposed Spatio-Temporal Adaptive Embedding transformer (STAEformer) achieves state-of-the-art performance on five real-world traffic forecasting datasets. Further experiments demonstrate that spatio-temporal adaptive embedding plays a crucial role in traffic forecasting by effectively capturing intrinsic spatio-temporal relations and chronological information in traffic time series.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Wasserstein&#20960;&#20309;&#29983;&#25104;&#22120;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#29983;&#25104;&#32473;&#23450;&#29305;&#23450;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#23398;&#20064;&#35266;&#23519;&#22495;&#30340;&#26465;&#20214;&#20998;&#24067;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#12290;&#22312;&#20154;&#33080;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.10145</link><description>&lt;p&gt;
Wasserstein&#20960;&#20309;&#29983;&#25104;&#22120;&#29992;&#20110;&#26465;&#20214;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Geodesic Generator for Conditional Distributions. (arXiv:2308.10145v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10145
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Wasserstein&#20960;&#20309;&#29983;&#25104;&#22120;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#29983;&#25104;&#32473;&#23450;&#29305;&#23450;&#26631;&#31614;&#30340;&#26679;&#26412;&#12290;&#20351;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#23398;&#20064;&#35266;&#23519;&#22495;&#30340;&#26465;&#20214;&#20998;&#24067;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#12290;&#22312;&#20154;&#33080;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#32473;&#23450;&#29305;&#23450;&#26631;&#31614;&#30340;&#26679;&#26412;&#38656;&#35201;&#20272;&#35745;&#26465;&#20214;&#20998;&#24067;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#21487;&#22788;&#29702;&#30340;&#19978;&#30028;&#65292;&#20197;&#24314;&#31435;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#22522;&#20110;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#29983;&#25104;&#31639;&#27861;&#65292;&#20854;&#20013;&#26465;&#20214;&#20998;&#24067;&#23436;&#20840;&#30001;&#30001;&#32479;&#35745;&#36317;&#31163;&#23450;&#20041;&#30340;&#24230;&#37327;&#31354;&#38388;&#26469;&#34920;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;Wasserstein&#20960;&#20309;&#29983;&#25104;&#22120;&#65292;&#19968;&#31181;&#23398;&#20064;Wasserstein&#20960;&#20309;&#30340;&#26032;&#30340;&#26465;&#20214;&#29983;&#25104;&#22120;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#35266;&#23519;&#22495;&#30340;&#26465;&#20214;&#20998;&#24067;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#12290;&#32473;&#23450;&#20004;&#20010;&#35266;&#23519;&#22495;&#26631;&#31614;&#65292;&#26410;&#35266;&#23519;&#21040;&#30340;&#20013;&#38388;&#22495;&#30340;&#26465;&#20214;&#20998;&#24067;&#20301;&#20110;&#32473;&#23450;&#30340;&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#20960;&#20309;&#20013;&#12290;&#22312;&#20197;&#20809;&#29031;&#26465;&#20214;&#20026;&#22495;&#26631;&#31614;&#30340;&#20154;&#33080;&#22270;&#20687;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating samples given a specific label requires estimating conditional distributions. We derive a tractable upper bound of the Wasserstein distance between conditional distributions to lay the theoretical groundwork to learn conditional distributions. Based on this result, we propose a novel conditional generation algorithm where conditional distributions are fully characterized by a metric space defined by a statistical distance. We employ optimal transport theory to propose the \textit{Wasserstein geodesic generator}, a new conditional generator that learns the Wasserstein geodesic. The proposed method learns both conditional distributions for observed domains and optimal transport maps between them. The conditional distributions given unobserved intermediate domains are on the Wasserstein geodesic between conditional distributions given two observed domain labels. Experiments on face images with light conditions as domain labels demonstrate the efficacy of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20984;&#20248;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#35774;&#35745;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#36807;&#28388;&#22120;&#33021;&#22815;&#25429;&#33719;&#24314;&#27169;&#35823;&#24046;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26469;&#25628;&#32034;&#21487;&#20197;&#20445;&#35777;&#32422;&#26463;&#28385;&#36275;&#30340;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#22312;&#38750;&#32447;&#24615;&#25670;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08086</link><description>&lt;p&gt;
&#36890;&#36807;&#20984;&#20248;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#35774;&#35745;&#23433;&#20840;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
Safety Filter Design for Neural Network Systems via Convex Optimization. (arXiv:2308.08086v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20984;&#20248;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#35774;&#35745;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#36807;&#28388;&#22120;&#33021;&#22815;&#25429;&#33719;&#24314;&#27169;&#35823;&#24046;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26469;&#25628;&#32034;&#21487;&#20197;&#20445;&#35777;&#32422;&#26463;&#28385;&#36275;&#30340;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#22312;&#38750;&#32447;&#24615;&#25670;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#21487;&#29992;&#24615;&#30340;&#22686;&#21152;&#65292;&#24050;&#32463;&#24191;&#27867;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#33021;&#22815;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#20934;&#30830;&#25429;&#33719;&#22797;&#26434;&#30340;&#31995;&#32479;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;NN&#30340;&#26550;&#26500;&#22797;&#26434;&#24615;&#21644;&#38750;&#32447;&#24615;&#20351;&#24471;&#23545;&#20854;&#21512;&#25104;&#19968;&#20010;&#33021;&#22815;&#34987;&#35777;&#26126;&#26159;&#23433;&#20840;&#30340;&#25511;&#21046;&#22120;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#23427;&#20381;&#38752;&#20984;&#20248;&#21270;&#26469;&#30830;&#20445;NN&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#33021;&#22815;&#25429;&#33719;&#24314;&#27169;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;NN&#39564;&#35777;&#24037;&#20855;&#26469;&#20351;&#29992;&#19968;&#32452;&#32447;&#24615;&#36793;&#30028;&#26469;&#36817;&#20284;NN&#30340;&#21160;&#24577;&#65292;&#28982;&#21518;&#24212;&#29992;&#40065;&#26834;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;robust linear MPC&#65289;&#26469;&#23547;&#25214;&#21487;&#20197;&#20445;&#35777;&#40065;&#26834;&#32422;&#26463;&#28385;&#36275;&#30340;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#22312;&#38750;&#32447;&#24615;&#25670;&#31995;&#32479;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increase in data availability, it has been widely demonstrated that neural networks (NN) can capture complex system dynamics precisely in a data-driven manner. However, the architectural complexity and nonlinearity of the NNs make it challenging to synthesize a provably safe controller. In this work, we propose a novel safety filter that relies on convex optimization to ensure safety for a NN system, subject to additive disturbances that are capable of capturing modeling errors. Our approach leverages tools from NN verification to over-approximate NN dynamics with a set of linear bounds, followed by an application of robust linear MPC to search for controllers that can guarantee robust constraint satisfaction. We demonstrate the efficacy of the proposed framework numerically on a nonlinear pendulum system.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#19978;&#19979;&#25991;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#19982;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#65292;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#24178;&#39044;&#25514;&#26045;&#12290;&#35813;&#31639;&#27861;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07843</link><description>&lt;p&gt;
Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG]) &#35813;&#35770;&#25991;&#26631;&#39064;&#24050;&#32763;&#35793;&#65306;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#19978;&#19979;&#25991;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#19982;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#65292;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#24178;&#39044;&#25514;&#26045;&#12290;&#35813;&#31639;&#27861;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#21307;&#30103;&#26088;&#22312;&#36890;&#36807;&#22312;&#20010;&#20154;&#26085;&#24120;&#29983;&#27963;&#20013;&#25552;&#20379;&#24178;&#39044;&#26469;&#25552;&#39640;&#20581;&#24247;&#32467;&#26524;&#12290;&#29031;&#39038;&#20276;&#20387;&#21644;&#31038;&#20250;&#25903;&#25345;&#32593;&#32476;&#30340;&#21442;&#19982;&#32463;&#24120;&#22312;&#24110;&#21161;&#20010;&#20154;&#31649;&#29702;&#32321;&#37325;&#30340;&#21307;&#30103;&#26465;&#20214;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20026;&#31227;&#21160;&#21307;&#30103;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#35774;&#35745;&#38024;&#23545;&#20108;&#20803;&#20851;&#31995;&#8212;&#8212;&#30446;&#26631;&#20154;&#21644;&#20854;&#29031;&#39038;&#20276;&#20387;&#20043;&#38388;&#20851;&#31995;&#8212;&#8212;&#20197;&#25552;&#39640;&#31038;&#20250;&#25903;&#25345;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;Dyadic RL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#21450;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#20010;&#24615;&#21270;&#24178;&#39044;&#25514;&#26045;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#22810;&#32452;&#24178;&#39044;&#25514;&#26045;&#24433;&#21709;&#30528;&#20108;&#20803;&#20851;&#31995;&#22312;&#22810;&#20010;&#26102;&#38388;&#38388;&#38548;&#20869;&#12290;&#24320;&#21457;&#30340;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#12290;&#25105;&#20204;&#27491;&#24335;&#20171;&#32461;&#20102;&#38382;&#39064;&#35774;&#23450;&#65292;&#24320;&#21457;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#24182;&#30830;&#23450;&#20102;&#36951;&#25022;&#36793;&#30028;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation st
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;Electra Transformer&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#21019;&#26032;&#38382;&#39064;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#34701;&#21512;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#33719;&#24471;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06828</link><description>&lt;p&gt;
&#38382;&#39064;&#20998;&#31867;&#30340;&#38598;&#25104;&#26041;&#27861;&#65306;&#34701;&#21512;Electra Transformer&#12289;GloVe&#21644;LSTM
&lt;/p&gt;
&lt;p&gt;
An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM. (arXiv:2308.06828v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;Electra Transformer&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#21019;&#26032;&#38382;&#39064;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#34701;&#21512;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#33719;&#24471;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24050;&#32463;&#25104;&#20026;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#23427;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#20998;&#31867;&#26041;&#38754;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#39046;&#22495;&#65292;&#38382;&#39064;&#20998;&#31867;&#19987;&#27880;&#20110;&#30830;&#23450;&#25152;&#38656;&#20449;&#24687;&#30340;&#31867;&#22411;&#65292;&#36825;&#26159;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#31561;&#19979;&#28216;&#24212;&#29992;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38382;&#39064;&#20998;&#31867;&#38598;&#25104;&#26041;&#27861;&#65292;&#23558;Electra&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;&#35813;&#27169;&#22411;&#22312;&#33879;&#21517;&#30340;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#25972;&#21512;&#36825;&#20123;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#24471;&#21040;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;Electra&#25552;&#20379;&#20102;&#22522;&#20110;transformer&#30340;&#22797;&#26434;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;GloVe&#25552;&#20379;&#20102;&#20840;&#23616;&#21521;&#37327;&#34920;&#31034;&#20197;&#25429;&#25417;&#35789;&#32423;&#35821;&#20041;&#65292;LSTM&#21017;&#36129;&#29486;&#20102;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#20197;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) has emerged as a crucial technology for understanding and generating human language, playing an essential role in tasks such as machine translation, sentiment analysis, and more pertinently, question classification. As a subfield within NLP, question classification focuses on determining the type of information being sought, a fundamental step for downstream applications like question answering systems. This study presents an innovative ensemble approach for question classification, combining the strengths of Electra, GloVe, and LSTM models. Rigorously tested on the well-regarded TREC dataset, the model demonstrates how the integration of these disparate technologies can lead to superior results. Electra brings in its transformer-based capabilities for complex language understanding, GloVe offers global vector representations for capturing word-level semantics, and LSTM contributes its sequence learning abilities to model long-term dependencies. By fus
&lt;/p&gt;</description></item><item><title>BarlowRL&#36890;&#36807;&#23558;Barlow Twins&#21644;DER&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;Atari 100k&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#20449;&#24687;&#25193;&#25955;&#36991;&#20813;&#20102;&#32500;&#24230;&#25240;&#21472;&#65292;&#20351;&#24471;RL&#31639;&#27861;&#33021;&#22815;&#21033;&#29992;&#22343;&#21248;&#20998;&#24067;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04263</link><description>&lt;p&gt;
BarlowRL: Barlow Twins&#29992;&#20110;&#25968;&#25454;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BarlowRL: Barlow Twins for Data-Efficient Reinforcement Learning. (arXiv:2308.04263v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04263
&lt;/p&gt;
&lt;p&gt;
BarlowRL&#36890;&#36807;&#23558;Barlow Twins&#21644;DER&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;Atari 100k&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#20449;&#24687;&#25193;&#25955;&#36991;&#20813;&#20102;&#32500;&#24230;&#25240;&#21472;&#65292;&#20351;&#24471;RL&#31639;&#27861;&#33021;&#22815;&#21033;&#29992;&#22343;&#21248;&#20998;&#24067;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BarlowRL&#65292;&#19968;&#31181;&#23558;Barlow Twins&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#19982;DER&#65288;Data-Efficient Rainbow&#65289;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#25968;&#25454;&#25928;&#29575;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;BarlowRL&#22312;Atari 100k&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;DER&#21644;&#23545;&#27604;&#31639;&#27861;CURL&#12290;BarlowRL&#36890;&#36807;&#30830;&#20445;&#20449;&#24687;&#25193;&#25955;&#21040;&#25972;&#20010;&#31354;&#38388;&#26469;&#36991;&#20813;&#32500;&#24230;&#25240;&#21472;&#12290;&#36825;&#26377;&#21161;&#20110;RL&#31639;&#27861;&#21033;&#29992;&#22343;&#21248;&#20998;&#24067;&#30340;&#29366;&#24577;&#34920;&#31034;&#65292;&#26368;&#32456;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;Barlow Twins&#19982;DER&#30340;&#25972;&#21512;&#22686;&#24378;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;RL&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;BarlowRL&#23637;&#31034;&#20102;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#32435;&#20837;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces BarlowRL, a data-efficient reinforcement learning agent that combines the Barlow Twins self-supervised learning framework with DER (Data-Efficient Rainbow) algorithm. BarlowRL outperforms both DER and its contrastive counterpart CURL on the Atari 100k benchmark. BarlowRL avoids dimensional collapse by enforcing information spread to the whole space. This helps RL algorithms to utilize uniformly spread state representation that eventually results in a remarkable performance. The integration of Barlow Twins with DER enhances data efficiency and achieves superior performance in the RL tasks. BarlowRL demonstrates the potential of incorporating self-supervised learning techniques to improve RL algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PMU&#27979;&#37327;&#30340;&#30005;&#21147;&#31995;&#32479;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#35299;&#20915;&#20102;&#25299;&#25169;&#21464;&#21270;&#12289;&#26679;&#26412;&#26631;&#27880;&#21644;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#22788;&#29702;&#31561;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#27169;&#22411;&#35780;&#20272;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03953</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#30340;PMU&#27979;&#37327;&#30340;&#30005;&#21147;&#31995;&#32479;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PMU measurements based short-term voltage stability assessment of power systems via deep transfer learning. (arXiv:2308.03953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PMU&#27979;&#37327;&#30340;&#30005;&#21147;&#31995;&#32479;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#35299;&#20915;&#20102;&#25299;&#25169;&#21464;&#21270;&#12289;&#26679;&#26412;&#26631;&#27880;&#21644;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#22788;&#29702;&#31561;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#27169;&#22411;&#35780;&#20272;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#30005;&#21147;&#31995;&#32479;&#30701;&#26399;&#30005;&#21387;&#31283;&#23450;&#24615;&#35780;&#20272;&#65288;STVSA&#65289;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;STVSA&#26041;&#27861;&#22312;&#36866;&#24212;&#25299;&#25169;&#21464;&#21270;&#12289;&#26679;&#26412;&#26631;&#27880;&#21644;&#22788;&#29702;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PMU&#27979;&#37327;&#30340;STVSA&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;PMUs&#25429;&#33719;&#30340;&#23454;&#26102;&#21160;&#24577;&#20449;&#24687;&#21019;&#24314;&#21021;&#22987;&#25968;&#25454;&#38598;&#12290;&#23427;&#37319;&#29992;&#26102;&#38388;&#38598;&#25104;&#36827;&#34892;&#26679;&#26412;&#26631;&#27880;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#20108;&#20056;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;LSGAN&#65289;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#23454;&#29616;&#23545;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#28145;&#24230;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#25925;&#38556;&#20043;&#38388;&#30340;&#36830;&#25509;&#22686;&#24378;&#20102;&#23545;&#25299;&#25169;&#21464;&#21270;&#30340;&#36866;&#24212;&#24615;&#12290;&#22312;IEEE 39&#33410;&#28857;&#27979;&#35797;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#35780;&#20272;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has emerged as an effective solution for addressing the challenges of short-term voltage stability assessment (STVSA) in power systems. However, existing deep learning-based STVSA approaches face limitations in adapting to topological changes, sample labeling, and handling small datasets. To overcome these challenges, this paper proposes a novel phasor measurement unit (PMU) measurements-based STVSA method by using deep transfer learning. The method leverages the real-time dynamic information captured by PMUs to create an initial dataset. It employs temporal ensembling for sample labeling and utilizes least squares generative adversarial networks (LSGAN) for data augmentation, enabling effective deep learning on small-scale datasets. Additionally, the method enhances adaptability to topological changes by exploring connections between different faults. Experimental results on the IEEE 39-bus test system demonstrate that the proposed method improves model evaluation accura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20195;&#30721;&#20013;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#31243;&#24207;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#23618;&#26469;&#25552;&#39640;&#22312;&#31243;&#24207;&#20998;&#26512;&#21644;&#24314;&#27169;&#20013;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.03312</link><description>&lt;p&gt;
&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#31243;&#24207;&#34920;&#31034;&#27861;&#29992;&#20110;&#23398;&#20064;&#20195;&#30721;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Symmetry-Preserving Program Representations for Learning Code Semantics. (arXiv:2308.03312v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20195;&#30721;&#20013;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#31243;&#24207;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24490;&#29615;&#23618;&#26469;&#25552;&#39640;&#22312;&#31243;&#24207;&#20998;&#26512;&#21644;&#24314;&#27169;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#21160;&#31243;&#24207;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36825;&#26159;&#35768;&#22810;&#23433;&#20840;&#20219;&#21153;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#20195;&#30721;&#30340;LLM&#26550;&#26500;&#36890;&#24120;&#20174;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#20511;&#29992;&#65292;&#24341;&#21457;&#23545;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#23545;&#26410;&#30693;&#20195;&#30721;&#30340;&#20581;&#22766;&#24615;&#30340;&#25285;&#24551;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#27867;&#21270;&#25361;&#25112;&#26159;&#23558;&#20195;&#30721;&#35821;&#20041;&#30340;&#30693;&#35782;&#65292;&#21253;&#25324;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#32435;&#20837;LLM&#26550;&#26500;&#20013;&#12290;&#21463;&#21040;&#21033;&#29992;&#24179;&#31227;&#23545;&#31216;&#24615;&#30340;&#21367;&#31215;&#23618;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20195;&#30721;&#23545;&#31216;&#24615;&#22914;&#20309;&#22686;&#24378;&#31243;&#24207;&#20998;&#26512;&#21644;&#24314;&#27169;&#30340;LLM&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#32676;&#35770;&#26694;&#26550;&#65292;&#24418;&#24335;&#21270;&#22320;&#23450;&#20041;&#20102;&#20195;&#30721;&#23545;&#31216;&#24615;&#20316;&#20026;&#20445;&#25345;&#35821;&#20041;&#30340;&#21464;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;LLM&#26550;&#26500;&#20013;&#31934;&#30830;&#25512;&#29702;&#23545;&#31216;&#24615;&#20445;&#25345;&#30340;&#25216;&#26415;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20445;&#25345;&#31243;&#24207;&#23545;&#31216;&#24615;&#30340;&#26032;&#22411;&#33258;&#27880;&#24847;&#21147;&#21464;&#20307;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise in automated program reasoning, a crucial aspect of many security tasks. However, existing LLM architectures for code are often borrowed from other domains like natural language processing, raising concerns about their generalization and robustness to unseen code. A key generalization challenge is to incorporate the knowledge of code semantics, including control and data flow, into the LLM architectures.  Drawing inspiration from examples of convolution layers exploiting translation symmetry, we explore how code symmetries can enhance LLM architectures for program analysis and modeling. We present a rigorous group-theoretic framework that formally defines code symmetries as semantics-preserving transformations and provides techniques for precisely reasoning about symmetry preservation within LLM architectures. Using this framework, we introduce a novel variant of self-attention that preserves program symmetries, demonstrating its effectiv
&lt;/p&gt;</description></item><item><title>DLSIA&#26159;&#19968;&#31181;Python&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#20026;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#23450;&#21046;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21152;&#36895;&#31185;&#23398;&#22270;&#20687;&#20998;&#26512;&#30740;&#31350;&#65292;&#24182;&#20419;&#36827;&#36328;&#23398;&#31185;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2308.02559</link><description>&lt;p&gt;
DLSIA: &#29992;&#20110;&#31185;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DLSIA: Deep Learning for Scientific Image Analysis. (arXiv:2308.02559v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02559
&lt;/p&gt;
&lt;p&gt;
DLSIA&#26159;&#19968;&#31181;Python&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#20026;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#23450;&#21046;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21152;&#36895;&#31185;&#23398;&#22270;&#20687;&#20998;&#26512;&#30740;&#31350;&#65292;&#24182;&#20419;&#36827;&#36328;&#23398;&#31185;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DLSIA&#65288;Deep Learning for Scientific Image Analysis&#65289;&#30340;&#22522;&#20110;Python&#30340;&#26426;&#22120;&#23398;&#20064;&#24211;&#65292;&#35813;&#24211;&#20026;&#31185;&#23398;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#21487;&#23450;&#21046;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#24191;&#27867;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#20197;&#36827;&#34892;&#19979;&#28216;&#25968;&#25454;&#22788;&#29702;&#25110;&#23454;&#39564;&#20013;&#30340;&#35745;&#31639;&#29615;&#22659;&#12290;DLSIA&#21253;&#21547;&#26131;&#20110;&#20351;&#29992;&#30340;&#26550;&#26500;&#65292;&#22914;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#21487;&#35843;&#30340;U-Net&#21644;&#21442;&#25968;&#31934;&#31616;&#30340;&#28151;&#21512;&#23610;&#24230;&#31264;&#23494;&#32593;&#32476;&#65288;MSDNets&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20351;&#29992;&#38543;&#26426;&#22270;&#21644;&#31232;&#30095;&#36830;&#25509;&#29983;&#25104;&#30340;&#31232;&#30095;&#28151;&#21512;&#23610;&#24230;&#32593;&#32476;&#65288;SMSNets&#65289;&#12290;&#38543;&#30528;&#23454;&#39564;&#25968;&#25454;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;DLSIA&#25552;&#20379;&#20102;&#26131;&#20110;&#35775;&#38382;&#30340;CNN&#26500;&#24314;&#21644;&#25277;&#35937;CNN&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#31185;&#23398;&#23478;&#33021;&#22815;&#23450;&#21046;&#20182;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21152;&#36895;&#21457;&#29616;&#65292;&#20419;&#36827;&#36328;&#23398;&#31185;&#21512;&#20316;&#24182;&#25512;&#21160;&#31185;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DLSIA (Deep Learning for Scientific Image Analysis), a Python-based machine learning library that empowers scientists and researchers across diverse scientific domains with a range of customizable convolutional neural network (CNN) architectures for a wide variety of tasks in image analysis to be used in downstream data processing, or for experiment-in-the-loop computing scenarios. DLSIA features easy-to-use architectures such as autoencoders, tunable U-Nets, and parameter-lean mixed-scale dense networks (MSDNets). Additionally, we introduce sparse mixed-scale networks (SMSNets), generated using random graphs and sparse connections. As experimental data continues to grow in scale and complexity, DLSIA provides accessible CNN construction and abstracts CNN complexities, allowing scientists to tailor their machine learning approaches, accelerate discoveries, foster interdisciplinary collaboration, and advance research in scientific image analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2308.01674</link><description>&lt;p&gt;
&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC. (arXiv:2308.01674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32463;&#27982;&#38750;&#32447;&#24615;MPC&#30340;Koopman&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#65288;&#32463;&#27982;&#65289;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;&#65288;e&#65289;NMPC&#65289;&#38656;&#35201;&#22312;&#25152;&#26377;&#30456;&#20851;&#29366;&#24577;&#31354;&#38388;&#21306;&#22495;&#37117;&#20855;&#26377;&#36275;&#22815;&#20934;&#30830;&#24615;&#30340;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#36824;&#24517;&#39035;&#35745;&#31639;&#25104;&#26412;&#36275;&#22815;&#20302;&#20197;&#30830;&#20445;&#23454;&#26102;&#21487;&#34892;&#24615;&#12290;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#26426;&#21046;&#27169;&#22411;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#65288;e&#65289;NMPC&#30340;&#35745;&#31639;&#36127;&#25285;&#65307;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#31995;&#32479;&#36776;&#35782;&#20197;&#22312;&#27169;&#25311;&#26679;&#26412;&#19978;&#33719;&#24471;&#26368;&#22823;&#24179;&#22343;&#39044;&#27979;&#20934;&#30830;&#24615;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20316;&#20026;&#23454;&#38469;&#65288;e&#65289;NMPC&#30340;&#19968;&#37096;&#20998;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#26368;&#20339;&#65288;e&#65289;NMPC&#24615;&#33021;&#30340;&#21160;&#24577;&#26367;&#20195;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#20855;&#26377;&#25511;&#21046;&#24615;&#33021;&#21644;&#35745;&#31639;&#38656;&#27714;&#20043;&#38388;&#33391;&#22909;&#24179;&#34913;&#30340;&#39044;&#27979;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;&#24050;&#24314;&#31435;&#30340;&#38750;&#32447;&#24615;&#36830;&#32493;&#25605;&#25292;&#21453;&#24212;&#22120;&#27169;&#22411;&#30340;&#24212;&#29992;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
(Economic) nonlinear model predictive control ((e)NMPC) requires dynamic system models that are sufficiently accurate in all relevant state-space regions. These models must also be computationally cheap enough to ensure real-time tractability. Data-driven surrogate models for mechanistic models can be used to reduce the computational burden of (e)NMPC; however, such models are typically trained by system identification for maximum average prediction accuracy on simulation samples and perform suboptimally as part of actual (e)NMPC. We present a method for end-to-end reinforcement learning of dynamic surrogate models for optimal performance in (e)NMPC applications, resulting in predictive controllers that strike a favorable balance between control performance and computational demand. We validate our method on two applications derived from an established nonlinear continuous stirred-tank reactor model. We compare the controller performance to that of MPCs utilizing models trained by the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#26684;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#25968;&#25454;&#24402;&#19968;&#21270;&#12289;&#38544;&#31169;&#21644;&#35780;&#20272;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.15424</link><description>&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12289;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#21644;&#24046;&#20998;&#38544;&#31169;&#65306;&#32508;&#36848;&#19982;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models, Synthetic Tabular Data, and Differential Privacy: An Overview and Synthesis. (arXiv:2307.15424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#26684;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#25968;&#25454;&#24402;&#19968;&#21270;&#12289;&#38544;&#31169;&#21644;&#35780;&#20272;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#32508;&#36848;&#20102;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#34920;&#26684;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#29305;&#21035;&#27010;&#36848;&#20102;&#22312;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#32972;&#26223;&#19979;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#35814;&#32454;&#35299;&#37322;&#20102;&#21253;&#25324;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#29983;&#25104;&#27169;&#22411;&#22312;&#20869;&#30340;&#22522;&#26412;&#27010;&#24565;&#12290;&#35770;&#25991;&#28085;&#30422;&#20102;&#22312;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#38598;&#26102;&#28041;&#21450;&#30340;&#25361;&#25112;&#21644;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#25968;&#25454;&#24402;&#19968;&#21270;&#12289;&#38544;&#31169;&#38382;&#39064;&#21644;&#27169;&#22411;&#35780;&#20272;&#12290;&#26412;&#32508;&#36848;&#20026;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21450;&#20854;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a comprehensive synthesis of the recent developments in synthetic data generation via deep generative models, focusing on tabular datasets. We specifically outline the importance of synthetic data generation in the context of privacy-sensitive data. Additionally, we highlight the advantages of using deep generative models over other methods and provide a detailed explanation of the underlying concepts, including unsupervised learning, neural networks, and generative models. The paper covers the challenges and considerations involved in using deep generative models for tabular datasets, such as data normalization, privacy concerns, and model evaluation. This review provides a valuable resource for researchers and practitioners interested in synthetic data generation and its applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;</title><link>http://arxiv.org/abs/2307.12057</link><description>&lt;p&gt;
&#22806;&#37096;&#25512;&#29702;&#65306;&#26397;&#30528;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20114;&#25442;&#36741;&#21161;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback. (arXiv:2307.12057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#34987;&#35748;&#20026;&#26159;&#20351;&#28023;&#39532;&#20307;&#21644;&#33041;&#31070;&#32463;&#20803;&#20869;&#20445;&#25345;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#12289;&#38543;&#21518;&#29992;&#20110;&#35299;&#20915;&#36890;&#36807;&#23398;&#20064;&#19968;&#29983;&#20013;&#36935;&#21040;&#30340;&#29616;&#23454;&#25361;&#25112;&#30340;&#20851;&#38190;&#20154;&#31867;&#33021;&#21147;&#12290;&#36890;&#36807;&#24212;&#29992;&#24050;&#33719;&#24471;&#30340;&#30693;&#35782;&#35299;&#20915;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19968;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20687;GPT-3.5&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#20132;&#20114;&#21644;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#24191;&#27867;&#12289;&#19981;&#26029;&#28436;&#21464;&#30340;&#30693;&#35782;&#24211;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory is identified as a crucial human faculty that allows for the retention of visual and linguistic information within the hippocampus and neurons in the brain, which can subsequently be retrieved to address real-world challenges that arise through a lifetime of learning. The resolution of complex AI tasks through the application of acquired knowledge represents a stride toward the realization of artificial general intelligence. However, despite the prevalence of Large Language Models (LLMs) like GPT-3.5 and GPT-4 , which have displayed remarkable capabilities in language comprehension, generation, interaction, and reasoning, they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases. This paper proposes that LLMs could be augmented through the selective integration of knowledge from external repositories, and in doing so, introduces a novel methodology for External Reasoning, exemplified by ChatPDF. Central to
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32422;&#26463;&#27714;&#35299;&#26041;&#27861;NeuralSAT&#65292;&#29992;&#20110;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art.</title><link>http://arxiv.org/abs/2307.10266</link><description>&lt;p&gt;
&#29992;&#20110;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;DPLL(T)&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A DPLL(T) Framework for Verifying Deep Neural Networks. (arXiv:2307.10266v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10266
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32422;&#26463;&#27714;&#35299;&#26041;&#27861;NeuralSAT&#65292;&#29992;&#20110;&#39564;&#35777;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#24037;&#32534;&#20889;&#30340;&#36719;&#20214;&#19968;&#26679;&#65292;&#33258;&#21160;&#29983;&#25104;&#30340;DNNs&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#24182;&#21463;&#21040;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#22312;&#24320;&#21457;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;DNN&#39564;&#35777;&#25216;&#26415;&#21644;&#24037;&#20855;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;NeuralSAT&#65292;&#19968;&#31181;&#29992;&#20110;DNN&#39564;&#35777;&#30340;&#26032;&#30340;&#32422;&#26463;&#27714;&#35299;&#26041;&#27861;&#12290;NeuralSAT&#30340;&#35774;&#35745;&#36981;&#24490;&#20102;&#29992;&#20110;&#29616;&#20195;SMT&#27714;&#35299;&#30340;DPLL(T)&#31639;&#27861;&#65292;&#21253;&#25324;&#65288;&#20914;&#31361;&#65289;&#23376;&#21477;&#23398;&#20064;&#12289;&#25277;&#35937;&#21644;&#29702;&#35770;&#27714;&#35299;&#65292;&#22240;&#27492;NeuralSAT&#21487;&#20197;&#34987;&#35270;&#20026;DNNs&#30340;&#19968;&#20010;SMT&#26694;&#26550;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;NeuralSAT&#21407;&#22411;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#36866;&#24403;&#30340;&#20248;&#21270;&#21644;&#24037;&#31243;&#21270;&#65292;NeuralSAT&#33021;&#22815;&#23558;&#29616;&#20195;SAT/SMT&#27714;&#35299;&#22120;&#30340;&#33021;&#21147;&#21644;&#25104;&#21151;&#24102;&#21040;DNN&#39564;&#35777;&#20013;&#12290;NeuralSAT&#21487;&#20174;&#20197;&#19979;&#38142;&#25509;&#33719;&#21462;&#65306;https://github.com/dynaroars/neuralsat-solver
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have emerged as an effective approach to tackling real-world problems. However, like human-written software, automatically-generated DNNs can have bugs and be attacked. This thus attracts many recent interests in developing effective and scalable DNN verification techniques and tools. In this work, we introduce a NeuralSAT, a new constraint solving approach to DNN verification. The design of NeuralSAT follows the DPLL(T) algorithm used modern SMT solving, which includes (conflict) clause learning, abstraction, and theory solving, and thus NeuralSAT can be considered as an SMT framework for DNNs. Preliminary results show that the NeuralSAT prototype is competitive to the state-of-the-art. We hope, with proper optimization and engineering, NeuralSAT will carry the power and success of modern SAT/SMT solvers to DNN verification. NeuralSAT is avaliable from: https://github.com/dynaroars/neuralsat-solver
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#21472;&#37096;&#20998;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#29992;&#20110;&#25903;&#25345;&#35813;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.07887</link><description>&lt;p&gt;
&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#65306;&#19968;&#20010;&#31614;&#21517;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#22312;&#37325;&#21472;&#37096;&#20998;&#25552;&#39640;&#20998;&#21106;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#29992;&#20110;&#25903;&#25345;&#35813;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#26512;&#25195;&#25551;&#25991;&#26723;&#26102;&#65292;&#25163;&#20889;&#25991;&#26412;&#21487;&#33021;&#35206;&#30422;&#25171;&#21360;&#25991;&#26412;&#12290;&#36825;&#22312;&#25991;&#26723;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#21644;&#25968;&#23383;&#21270;&#36807;&#31243;&#20013;&#36896;&#25104;&#22256;&#38590;&#65292;&#24182;&#19988;&#36827;&#32780;&#24433;&#21709;&#21040;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#20165;&#20851;&#27880;&#25163;&#20889;&#25991;&#26412;&#30340;&#20108;&#20998;&#31867;&#65292;&#35201;&#20040;&#36827;&#34892;&#19977;&#31867;&#25991;&#26723;&#30340;&#20998;&#21106;&#65292;&#21363;&#25163;&#20889;&#12289;&#25171;&#21360;&#21644;&#32972;&#26223;&#20687;&#32032;&#30340;&#35782;&#21035;&#12290;&#36825;&#23548;&#33268;&#25163;&#20889;&#21644;&#25171;&#21360;&#37325;&#21472;&#30340;&#20687;&#32032;&#21482;&#34987;&#20998;&#37197;&#21040;&#19968;&#20010;&#31867;&#21035;&#20013;&#65292;&#22240;&#27492;&#22312;&#21478;&#19968;&#20010;&#31867;&#21035;&#20013;&#19981;&#34987;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#20889;&#21644;&#25171;&#21360;&#25991;&#26412;&#20998;&#21106;&#30340;&#25361;&#25112;&#65292;&#30446;&#26631;&#26159;&#23436;&#25972;&#22320;&#24674;&#22797;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#29305;&#21035;&#26159;&#25552;&#39640;&#37325;&#21472;&#37096;&#20998;&#30340;&#20998;&#21106;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#39033;&#20219;&#21153;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SignaTR6K&#65292;&#35813;&#25968;&#25454;&#38598;&#25910;&#38598;&#33258;&#30495;&#23454;&#30340;&#27861;&#24459;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
While analyzing scanned documents, handwritten text can overlay printed text. This causes difficulties during the optical character recognition (OCR) and digitization process of documents, and subsequently, hurts downstream NLP tasks. Prior research either focuses only on the binary classification of handwritten text, or performs a three-class segmentation of the document, i.e., recognition of handwritten, printed, and background pixels. This results in the assignment of the handwritten and printed overlapping pixels to only one of the classes, and thus, they are not accounted for in the other class. Thus, in this research, we develop novel approaches for addressing the challenges of handwritten and printed text segmentation with the goal of recovering text in different classes in whole, especially improving the segmentation performance on the overlapping parts. As such, to facilitate with this task, we introduce a new dataset, SignaTR6K, collected from real legal documents, as well as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;C-VAE&#27169;&#22411;&#26469;&#29983;&#25104;&#24179;&#28369;&#19988;&#36924;&#30495;&#30340;&#26102;&#31354;&#28436;&#21464;&#34920;&#31034;&#65292;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#37325;&#24314;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.06243</link><description>&lt;p&gt;
&#29992;C-VAEs&#37325;&#24314;&#26102;&#31354;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Reconstructing Spatiotemporal Data with C-VAEs. (arXiv:2307.06243v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;C-VAE&#27169;&#22411;&#26469;&#29983;&#25104;&#24179;&#28369;&#19988;&#36924;&#30495;&#30340;&#26102;&#31354;&#28436;&#21464;&#34920;&#31034;&#65292;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#37325;&#24314;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#25968;&#25454;&#30340;&#36830;&#32493;&#34920;&#31034;&#36890;&#24120;&#20381;&#36182;&#20110;&#20351;&#29992;&#25277;&#35937;&#25968;&#25454;&#31867;&#22411;&#65292;&#20363;&#22914;&#31227;&#21160;&#21306;&#22495;&#65292;&#26469;&#34920;&#31034;&#24418;&#29366;&#21644;&#20301;&#32622;&#22312;&#26102;&#38388;&#19978;&#36830;&#32493;&#21464;&#21270;&#30340;&#23454;&#20307;&#12290;&#20174;&#31163;&#25955;&#30340;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#24555;&#29031;&#21019;&#24314;&#36825;&#31181;&#34920;&#31034;&#38656;&#35201;&#20351;&#29992;&#25554;&#20540;&#26041;&#27861;&#26469;&#35745;&#31639;&#20013;&#38388;&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#20272;&#35745;&#24863;&#20852;&#36259;&#23545;&#35937;&#22312;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#20301;&#32622;&#21644;&#24418;&#29366;&#12290;&#29616;&#26377;&#30340;&#21306;&#22495;&#25554;&#20540;&#26041;&#27861;&#24120;&#24120;&#26080;&#27861;&#29983;&#25104;&#24179;&#28369;&#21644;&#36924;&#30495;&#30340;&#21306;&#22495;&#28436;&#21464;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#25581;&#31034;&#20102;&#22522;&#20110;&#31163;&#25955;&#35266;&#27979;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#36890;&#36807;&#38544;&#24335;&#29305;&#24449;&#23398;&#20064;&#21487;&#20197;&#25429;&#25417;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;C-VAE&#65289;&#27169;&#22411;&#29983;&#25104;&#31227;&#21160;&#21306;&#22495;&#30340;&#26102;&#31354;&#28436;&#21464;&#24179;&#28369;&#21644;&#36924;&#30495;&#34920;&#31034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continuous representation of spatiotemporal data commonly relies on using abstract data types, such as \textit{moving regions}, to represent entities whose shape and position continuously change over time. Creating this representation from discrete snapshots of real-world entities requires using interpolation methods to compute in-between data representations and estimate the position and shape of the object of interest at arbitrary temporal points. Existing region interpolation methods often fail to generate smooth and realistic representations of a region's evolution. However, recent advancements in deep learning techniques have revealed the potential of deep models trained on discrete observations to capture spatiotemporal dependencies through implicit feature learning.  In this work, we explore the capabilities of Conditional Variational Autoencoder (C-VAE) models to generate smooth and realistic representations of the spatiotemporal evolution of moving regions. We evaluate our
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#36793;&#32536;&#23384;&#20648;&#31649;&#29702;&#21644;&#36947;&#36335;&#24322;&#24120;&#26816;&#27979;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23384;&#20648;&#39640;&#20445;&#30495;&#38899;&#39057;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.04298</link><description>&lt;p&gt;
&#24102;&#26377;&#38646;-shot&#25968;&#25454;&#21387;&#32553;&#30340;&#36793;&#32536;&#23384;&#20648;&#31649;&#29702;&#37197;&#26041;&#29992;&#20110;&#36947;&#36335;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Edge Storage Management Recipe with Zero-Shot Data Compression for Road Anomaly Detection. (arXiv:2307.04298v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04298
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#65292;&#29992;&#20110;&#36793;&#32536;&#23384;&#20648;&#31649;&#29702;&#21644;&#36947;&#36335;&#24322;&#24120;&#26816;&#27979;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23384;&#20648;&#39640;&#20445;&#30495;&#38899;&#39057;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#22522;&#20110;&#36793;&#32536;&#35745;&#31639;&#30340;&#36947;&#36335;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#35745;&#31639;&#26426;&#30340;&#25968;&#25454;&#23384;&#20648;&#31354;&#38388;&#24456;&#23567;&#65292;&#20294;&#25105;&#20204;&#38656;&#35201;&#38271;&#26102;&#38388;&#23384;&#20648;&#25910;&#38598;&#21040;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#20197;&#20415;&#26356;&#26032;&#29616;&#26377;&#27169;&#22411;&#25110;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#35813;&#32771;&#34385;&#19968;&#31181;&#22312;&#20445;&#25345;&#39640;&#20445;&#30495;&#38899;&#39057;&#30340;&#21516;&#26102;&#36827;&#34892;&#39640;&#25928;&#23384;&#20648;&#31649;&#29702;&#30340;&#26041;&#27861;&#12290;&#20174;&#30828;&#20214;&#35282;&#24230;&#26469;&#30475;&#65292;&#20351;&#29992;&#20302;&#20998;&#36776;&#29575;&#40614;&#20811;&#39118;&#21487;&#20197;&#30452;&#35266;&#22320;&#20943;&#23567;&#25991;&#20214;&#22823;&#23567;&#65292;&#20294;&#19981;&#25512;&#33616;&#36825;&#31181;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20250;&#20174;&#26681;&#26412;&#19978;&#21066;&#24369;&#39640;&#39057;&#32452;&#20214;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35745;&#31639;&#25991;&#20214;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#23558;&#25910;&#38598;&#21040;&#30340;&#39640;&#20998;&#36776;&#29575;&#38899;&#39057;&#32534;&#30721;&#20026;&#32039;&#20945;&#30340;&#20195;&#30721;&#65292;&#22240;&#27492;&#25512;&#33616;&#20351;&#29992;&#35813;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#36824;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#30721;&#26041;&#27861;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#30340;&#25968;&#25454;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show edge computing-based road anomaly detection systems which may also conduct data collection simultaneously. However, the edge computers will have small data storage but we need to store the collected audio samples for a long time in order to update existing models or develop a novel method. Therefore, we should consider an approach for efficient storage management methods while preserving high-fidelity audio. A hardware-perspective approach, such as using a low-resolution microphone, is an intuitive way to reduce file size but is not recommended because it fundamentally cuts off high-frequency components. On the other hand, a computational file compression approach that encodes collected high-resolution audio into a compact code should be recommended because it also provides a corresponding decoding method. Motivated by this, we propose a way of simple yet effective pre-trained autoencoder-based data compression method. The pre-trained autoencoder is trained for the 
&lt;/p&gt;</description></item><item><title>inTformer&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#20851;&#27880;&#26426;&#21046;Transformer&#65292;&#29992;&#20110;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#39044;&#27979;&#36335;&#21475;&#20107;&#25925;&#21487;&#33021;&#24615;&#12290;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;inTformer&#20855;&#26377;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#24615;&#12289;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.03854</link><description>&lt;p&gt;
inTformer: &#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#20851;&#27880;&#26426;&#21046;Transformer&#29992;&#20110;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#30340;&#36335;&#21475;&#20107;&#25925;&#21487;&#33021;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
inTformer: A Time-Embedded Attention-Based Transformer for Crash Likelihood Prediction at Intersections Using Connected Vehicle Data. (arXiv:2307.03854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03854
&lt;/p&gt;
&lt;p&gt;
inTformer&#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#30340;&#20851;&#27880;&#26426;&#21046;Transformer&#65292;&#29992;&#20110;&#20351;&#29992;&#36830;&#25509;&#36710;&#36742;&#25968;&#25454;&#39044;&#27979;&#36335;&#21475;&#20107;&#25925;&#21487;&#33021;&#24615;&#12290;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;inTformer&#20855;&#26377;&#22788;&#29702;&#38271;&#26399;&#20381;&#36182;&#24615;&#12289;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#20107;&#25925;&#21487;&#33021;&#24615;&#39044;&#27979;&#27169;&#22411;&#26159;&#20027;&#21160;&#20132;&#36890;&#23433;&#20840;&#31649;&#29702;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22810;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#26500;&#24314;&#20107;&#25925;&#21487;&#33021;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#20132;&#36890;&#23433;&#20840;&#65292;&#20294;&#20027;&#35201;&#38598;&#20013;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#12290;&#22312;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#37319;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#26469;&#35782;&#21035;&#20107;&#25925;&#28508;&#22312;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;Transformer&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#28508;&#22312;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#22522;&#26412;&#21407;&#29702;&#26159;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36827;&#34892;&#25805;&#20316;&#12290;Transformer&#22312;&#21151;&#33021;&#19978;&#27604;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;LSTM&#65292;CNN&#31561;&#65289;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;Transformer&#21487;&#20197;&#36731;&#26494;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;Transformer&#21487;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#24182;&#34892;&#22788;&#29702;&#25968;&#25454;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#20803;&#32032;&#12290;&#26368;&#21518;&#65292;Transformer&#19981;&#23384;&#22312;&#26799;&#24230;&#28040;&#22833;&#30340;&#38382;&#39064;&#12290;&#35748;&#35782;&#21040;Transformer&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
The real-time crash likelihood prediction model is an essential component of the proactive traffic safety management system. Over the years, numerous studies have attempted to construct a crash likelihood prediction model in order to enhance traffic safety, but mostly on freeways. In the majority of the existing studies, researchers have primarily employed a deep learning-based framework to identify crash potential. Lately, Transformer has emerged as a potential deep neural network that fundamentally operates through attention-based mechanisms. Transformer has several functional benefits over extant deep learning models such as Long Short-Term Memory (LSTM), Convolution Neural Network (CNN), etc. Firstly, Transformer can readily handle long-term dependencies in a data sequence. Secondly, Transformer can parallelly process all elements in a data sequence during training. Finally, Transformer does not have the vanishing gradient issue. Realizing the immense possibility of Transformer, th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#32780;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.03380</link><description>&lt;p&gt;
&#20851;&#20110;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#24418;&#24335;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#21450;&#20854;&#36817;&#20284;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#32780;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#34429;&#28982;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;ML&#27169;&#22411;&#33030;&#24369;&#24615;&#65292;&#20844;&#24179;&#24615;&#20197;&#21450;&#35299;&#37322;&#24615;&#30340;&#32570;&#20047;&#31561;&#37325;&#35201;&#38382;&#39064;&#38656;&#35201;&#31215;&#26497;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#24418;&#24335;&#21270;&#30340;ML&#27169;&#22411;&#39564;&#35777;&#12290;XAI&#30340;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#21253;&#25324;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;Anchors&#65289;&#21644;&#29305;&#24449;&#24402;&#22240;&#25216;&#26415;&#65288;&#20363;&#22914;&#65292;LIME&#21644;SHAP&#65289;&#12290;&#23613;&#31649;&#26377;&#24076;&#26395;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#24402;&#22240;&#26041;&#27861;&#37117;&#23481;&#26131;&#20986;&#29616;&#19968;&#31995;&#21015;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#35299;&#37322;&#19981;&#27491;&#30830;&#21644;&#36229;&#20986;&#20998;&#24067;&#37319;&#26679;&#12290;&#36817;&#26399;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;XAI&#26041;&#27861;&#65288;FXAI&#65289;&#34429;&#28982;&#20316;&#20026;&#20197;&#19978;&#26041;&#27861;&#30340;&#26367;&#20195;&#21697;&#24182;&#36991;&#20813;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#20363;&#22914;&#65292;&#38500;&#20102;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#22806;&#65292;&#36825;&#31181;&#24418;&#24335;&#21270;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#29305;&#24449;&#24402;&#22240;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include feature selection methods, e.g. Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution prob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#20013;&#24213;&#23618;&#38382;&#39064;&#20026;&#24378;&#20984;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#25509;&#36817;&#26368;&#20248;&#30340;&#36895;&#29575;&#25910;&#25947;&#12290;&#36825;&#31181;&#31639;&#27861;&#36991;&#20813;&#20102;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#25110;&#20195;&#20215;&#26114;&#36149;&#30340;Hessian-&#21521;&#37327;&#20056;&#31215;&#39044;&#35328;&#26426;&#30340;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.14853</link><description>&lt;p&gt;
&#36817;&#20284;&#26368;&#20248;&#38750;&#20984;-&#24378;&#20984;&#21452;&#23618;&#20248;&#21270;&#19982;&#20840;&#19968;&#38454;&#39044;&#35328;&#26426;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Nonconvex-Strongly-Convex Bilevel Optimization with Fully First-Order Oracles. (arXiv:2306.14853v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#20013;&#24213;&#23618;&#38382;&#39064;&#20026;&#24378;&#20984;&#30340;&#24773;&#20917;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20197;&#25509;&#36817;&#26368;&#20248;&#30340;&#36895;&#29575;&#25910;&#25947;&#12290;&#36825;&#31181;&#31639;&#27861;&#36991;&#20813;&#20102;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#25110;&#20195;&#20215;&#26114;&#36149;&#30340;Hessian-&#21521;&#37327;&#20056;&#31215;&#39044;&#35328;&#26426;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#12289;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#20803;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#24212;&#29992;&#12290;&#35774;&#35745;&#39640;&#25928;&#30340;&#21452;&#23618;&#20248;&#21270;&#31639;&#27861;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#24213;&#23618;&#38382;&#39064;&#36890;&#36807;&#21478;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#38544;&#24335;&#23450;&#20041;&#20102;&#19968;&#20010;&#21487;&#34892;&#24615;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#26131;&#20110;&#22788;&#29702;&#30340;&#24773;&#20917;&#65292;&#21363;&#24213;&#23618;&#38382;&#39064;&#26159;&#24378;&#20984;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;Hessian-&#21521;&#37327;&#20056;&#31215;&#39044;&#35328;&#26426;&#65292;&#21487;&#20197;&#22312;$\tilde{\mathcal{O}}(\epsilon^{-2})$&#20010;&#39044;&#35328;&#35843;&#29992;&#20869;&#21487;&#38752;&#22320;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#19968;&#38454;&#31283;&#23450;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;Hessian-&#21521;&#37327;&#20056;&#31215;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#25110;&#20195;&#20215;&#26114;&#36149;&#12290;Kwon&#31561;&#20154;&#65288;ICML 2023&#65289;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#19968;&#38454;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20197;&#36739;&#24930;&#30340;$\tilde{\mathcal{O}}(\epsilon^{-3})$&#30340;&#36895;&#29575;&#23454;&#29616;&#30456;&#21516;&#30340;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26356;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20197;&#25509;&#36817;&#26368;&#20248;&#30340;$\tilde {\mathcal{O}}(\epsilon^{-2})$&#30340;&#36895;&#29575;&#20687;&#20108;&#38454;&#26041;&#27861;&#19968;&#26679;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization has wide applications such as hyperparameter tuning, neural architecture search, and meta-learning. Designing efficient algorithms for bilevel optimization is challenging because the lower-level problem defines a feasibility set implicitly via another optimization problem. In this work, we consider one tractable case when the lower-level problem is strongly convex. Recent works show that with a Hessian-vector product oracle, one can provably find an $\epsilon$-first-order stationary point within $\tilde{\mathcal{O}}(\epsilon^{-2})$ oracle calls. However, Hessian-vector product may be inaccessible or expensive in practice. Kwon et al. (ICML 2023) addressed this issue by proposing a first-order method that can achieve the same goal at a slower rate of $\tilde{\mathcal{O}}(\epsilon^{-3})$. In this work, we provide a tighter analysis demonstrating that this method can converge at the near-optimal $\tilde {\mathcal{O}}(\epsilon^{-2})$ rate as second-order methods. Our a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#22312;&#32473;&#23450;&#22270;&#19978;&#39044;&#27979;&#33410;&#28857;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#26356;&#39640;&#32423;&#20219;&#21153;&#30456;&#20284;&#24615;&#26469;&#21152;&#24378;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#23558;&#20219;&#21153;&#20998;&#32452;&#20197;&#24212;&#23545;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14009</link><description>&lt;p&gt;
&#36890;&#36807;&#26356;&#39640;&#32423;&#20219;&#21153;&#30456;&#20284;&#24615;&#21152;&#24378;&#22270;&#19978;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Multitask Learning on Graphs through Higher-Order Task Affinities. (arXiv:2306.14009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#22312;&#32473;&#23450;&#22270;&#19978;&#39044;&#27979;&#33410;&#28857;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#26356;&#39640;&#32423;&#20219;&#21153;&#30456;&#20284;&#24615;&#26469;&#21152;&#24378;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#23558;&#20219;&#21153;&#20998;&#32452;&#20197;&#24212;&#23545;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#22270;&#19978;&#39044;&#27979;&#33410;&#28857;&#26631;&#31614;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#23376;&#22270;&#39044;&#27979;&#12290;&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#27492;&#38382;&#39064;&#65292;&#32771;&#34385;&#21516;&#26102;&#22312;&#22270;&#19978;&#39044;&#27979;&#22810;&#20010;&#33410;&#28857;&#26631;&#31614;&#20989;&#25968;&#12290;&#20026;&#20102;&#20855;&#20307;&#35828;&#26126;&#65292;&#32771;&#34385;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#65306;&#27599;&#20010;&#31038;&#21306;&#25104;&#21592;&#36523;&#20221;&#26159;&#19968;&#20010;&#20108;&#36827;&#21046;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#30001;&#20110;&#22797;&#26434;&#30340;&#37325;&#21472;&#27169;&#24335;&#65292;&#24403;&#25105;&#20204;&#23558;&#22810;&#20010;&#31038;&#21306;&#26816;&#27979;&#24212;&#29992;&#21040;naive&#22810;&#20219;&#21153;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#36127;&#36801;&#31227;&#24456;&#26222;&#36941;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#33410;&#28857;&#26631;&#31614;&#20043;&#38388;&#30340;&#20219;&#21153;&#20851;&#31995;&#39640;&#24230;&#38750;&#32447;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22522;&#20110;&#26356;&#39640;&#32423;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#23558;&#20219;&#21153;&#20998;&#32452;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#27599;&#20010;&#20219;&#21153;&#32452;&#19978;&#25311;&#21512;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#20135;&#29983;&#22312;&#22522;&#32447;&#27169;&#22411;&#19978;&#30340;&#22686;&#24378;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#26356;&#39640;&#32423;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#24230;&#37327;&#20272;&#35745;&#20026;&#39044;&#27979;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting node labels on a given graph is a widely studied problem with many applications, including community detection and molecular graph prediction. This paper considers predicting multiple node labeling functions on graphs simultaneously and revisits this problem from a multitask learning perspective. For a concrete example, consider overlapping community detection: each community membership is a binary node classification task. Due to complex overlapping patterns, we find that negative transfer is prevalent when we apply naive multitask learning to multiple community detection, as task relationships are highly nonlinear across different node labeling. To address the challenge, we develop an algorithm to cluster tasks into groups based on a higher-order task affinity measure. We then fit a multitask model on each task group, resulting in a boosting procedure on top of the baseline model. We estimate the higher-order task affinity measure between two tasks as the prediction loss o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#22914;&#20309;&#22312;&#27809;&#26377;&#20840;&#23616;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#35757;&#32451;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21327;&#35843;&#34892;&#21160;&#65292;&#23454;&#29616;&#26356;&#24555;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#20219;&#21153;&#25191;&#34892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12926</link><description>&lt;p&gt;
&#20855;&#26377;&#20840;&#23616;&#29366;&#24577;&#39044;&#27979;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Agent Reinforcement Learning with Global State Prediction. (arXiv:2306.12926v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#22914;&#20309;&#22312;&#27809;&#26377;&#20840;&#23616;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#35757;&#32451;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#21327;&#35843;&#34892;&#21160;&#65292;&#23454;&#29616;&#26356;&#24555;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#20219;&#21153;&#25191;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#25511;&#21046;&#21333;&#20010;&#26426;&#22120;&#20154;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23558;DRL&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#32676;&#20307;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#38750;&#38745;&#24577;&#24615;&#65292;&#21363;&#24403;&#20004;&#20010;&#25110;&#26356;&#22810;&#26426;&#22120;&#20154;&#21516;&#26102;&#26356;&#26032;&#20010;&#20307;&#25110;&#20849;&#20139;&#25919;&#31574;&#26102;&#65292;&#20250;&#36827;&#20837;&#19968;&#20010;&#30456;&#20114;&#20381;&#23384;&#30340;&#22521;&#35757;&#36807;&#31243;&#65292;&#24182;&#19988;&#19981;&#20445;&#35777;&#25910;&#25947;&#12290;&#20811;&#26381;&#38750;&#38745;&#24577;&#24615;&#36890;&#24120;&#28041;&#21450;&#20351;&#29992;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#20840;&#23616;&#20449;&#24687;&#26469;&#35757;&#32451;&#26426;&#22120;&#20154;&#65292;&#20363;&#22914;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#21644;/&#25110;&#34892;&#21160;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#28040;&#38500;&#20840;&#23616;&#20449;&#24687;&#30340;&#38656;&#27714;&#12290;&#30001;&#20110;&#32570;&#20047;&#20854;&#20182;&#20449;&#24687;&#20307;&#30340;&#20840;&#23616;&#30693;&#35782;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#25551;&#36848;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#20197;&#38598;&#20307;&#36816;&#36755;&#20026;&#27979;&#35797;&#22330;&#26223;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#22810;&#26234;&#33021;&#20307;&#22521;&#35757;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#31181;&#26041;&#27861;&#20013;&#65292;&#26426;&#22120;&#20154;&#19981;&#20132;&#25442;&#20449;&#24687;&#65292;&#24182;&#19988;&#34987;&#35757;&#32451;&#20381;&#38752;&#36890;&#36807;&#25512;&#65288;push&#65289;&#21644;&#25289;&#65288;pull&#65289;&#29289;&#20307;&#36827;&#34892;&#38544;&#24335;&#36890;&#20449;&#12290;&#22312;&#31532;&#20108;&#31181;&#26041;&#27861;&#20013;&#65292;&#26426;&#22120;&#20154;&#24444;&#27492;&#20849;&#20139;&#29366;&#24577;&#39044;&#27979;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#26174;&#24335;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#21327;&#35843;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20849;&#20139;&#39044;&#27979;&#21487;&#20197;&#20351;&#26234;&#33021;&#20307;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#65292;&#21516;&#26102;&#22312;&#38656;&#35201;&#26356;&#23569;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#20219;&#21153;&#25191;&#34892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) has seen remarkable success in the control of single robots. However, applying DRL to robot swarms presents significant challenges. A critical challenge is non-stationarity, which occurs when two or more robots update individual or shared policies concurrently, thereby engaging in an interdependent training process with no guarantees of convergence. Circumventing non-stationarity typically involves training the robots with global information about other agents' states and/or actions. In contrast, in this paper we explore how to remove the need for global information. We pose our problem as a Partially Observable Markov Decision Process, due to the absence of global knowledge on other agents. Using collective transport as a testbed scenario, we study two approaches to multi-agent training. In the first, the robots exchange no messages, and are trained to rely on implicit communication through push-and-pull on the object to transport. In the second appro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.11167</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34987;&#35823;&#23548;&#65306;&#20351;&#29992;Only Connect Wall&#25968;&#25454;&#38598;&#25506;&#32034;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#21644;Einstellung&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset. (arXiv:2306.11167v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#34987;&#35823;&#23548;&#65292;&#20986;&#29616;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20154;&#24037;&#26234;&#33021;&#35806;&#29983;&#20197;&#26469;&#65292;&#23545;&#20154;&#31867;&#20223;&#30495;&#26234;&#33021;&#30340;&#36861;&#27714;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#25345;&#20037;&#35805;&#39064;&#12290;&#26368;&#26032;&#19968;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25216;&#26415;&#28436;&#36827;&#21644;&#26032;&#20852;&#33021;&#21147;&#23558;&#36825;&#20010;&#20027;&#39064;&#20174;&#23398;&#26415;&#30028;&#24102;&#21040;&#20102;&#25991;&#21270;&#26102;&#20195;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;NLP&#35780;&#20272;&#22522;&#20934;&#20219;&#21153;&#27979;&#35797;&#20102;&#20154;&#31867;&#20223;&#30495;&#34892;&#20026;&#30340;&#19968;&#20123;&#26041;&#38754;&#65288;&#20363;&#22914;BIG-bench&#30340;&#8220;&#31867;&#20154;&#34892;&#20026;&#8221;&#20219;&#21153;&#65289;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#19968;&#20010;&#20219;&#21153;&#32771;&#23519;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20154;&#31867;&#30340;&#21019;&#36896;&#24615;&#38382;&#39064;&#35299;&#20915;&#26159;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#30740;&#31350;&#36739;&#20026;&#28145;&#20837;&#30340;&#20027;&#39064;&#65292;&#26631;&#20934;&#21270;&#27979;&#35797;&#20027;&#35201;&#20351;&#29992;&#23558;&#32447;&#32034;&#35789;&#20043;&#38388;&#30340;&#65288;&#24322;&#26500;&#65289;&#36830;&#25509;&#33021;&#21147;&#20316;&#20026;&#21019;&#36896;&#24615;&#30340;&#24230;&#37327;&#12290;&#22312;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#65292;&#26263;&#31034;&#24615;&#30340;&#35823;&#23548;&#24615;&#21050;&#28608;-&#34987;&#31216;&#20026;&#8220;&#35825;&#23548;&#35823;&#35299;&#8221;&#30340;&#24178;&#25200;&#22240;&#32032;-&#36890;&#36807;&#22266;&#23450;&#25928;&#24212;&#21644;Einstellung&#33539;&#24335;&#38459;&#30861;&#20102;&#20154;&#31867;&#30340;&#34920;&#29616;&#12290;&#22312;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20107;&#20808;&#35753;&#21442;&#19982;&#32773;&#25509;&#35302;&#21040;&#26377;&#30456;&#20284;&#25340;&#20889;&#30340;&#38169;&#35823;&#22240;&#32032;&#26469;&#23454;&#39564;&#24615;&#22320;&#35825;&#23548;&#36825;&#26679;&#30340;&#22266;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeitgeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine creative problem solving abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli - distractors dubbed red herrings - impede human performance in such tasks via the fixation effect and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#24050;&#25104;&#20026;&#32452;&#32455;&#21307;&#23398;&#30693;&#35782;&#30340;&#26377;&#32467;&#26500;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21307;&#23398;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#24378;&#35843;&#20102;&#22312;HKG&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#32508;&#36848;&#26159;HKG&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;HKG&#26500;&#24314;&#30340;&#27969;&#31243;&#21644;&#20851;&#38190;&#25216;&#26415;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#21644;&#36890;&#36807;&#38598;&#25104;&#65289;&#65292;&#20197;&#21450;&#24120;&#35265;&#30340;&#21033;&#29992;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#22522;&#20110;&#27169;&#22411;&#65289;&#12290;&#20026;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#25429;&#33719;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65288;&#35813;&#36164;&#28304;&#23384;&#20648;&#20110;https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase&#65289;&#32452;&#32455;&#20102;&#29616;&#26377;&#30340;HKG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#24212;&#29992;&#37096;&#20998;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
&lt;/p&gt;</description></item><item><title>&#20174;&#31070;&#32463;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#20855;&#22791;&#21754;&#20083;&#21160;&#29289;&#24847;&#35782;&#24863;&#30693;&#30456;&#20851;&#30340;&#19992;&#33041;&#30382;&#23618;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#32570;&#20047;&#21608;&#22260;&#19990;&#30028;&#30340;&#20855;&#20307;&#23884;&#20837;&#24335;&#20449;&#24687;&#65292;&#19988;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#26080;&#27861;&#20570;&#21040;&#23384;&#22312;&#30340;&#20381;&#36182;&#20110;&#20854;&#34892;&#20026;&#65292;&#36825;&#24847;&#21619;&#30528;&#20154;&#24037;&#24847;&#35782;&#30340;&#21487;&#34892;&#24615;&#23384;&#22312;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2306.00915</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#30340;&#35270;&#35282;&#25506;&#31350;&#20154;&#24037;&#24847;&#35782;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
The feasibility of artificial consciousness through the lens of neuroscience. (arXiv:2306.00915v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00915
&lt;/p&gt;
&lt;p&gt;
&#20174;&#31070;&#32463;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#20855;&#22791;&#21754;&#20083;&#21160;&#29289;&#24847;&#35782;&#24863;&#30693;&#30456;&#20851;&#30340;&#19992;&#33041;&#30382;&#23618;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#32570;&#20047;&#21608;&#22260;&#19990;&#30028;&#30340;&#20855;&#20307;&#23884;&#20837;&#24335;&#20449;&#24687;&#65292;&#19988;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#26080;&#27861;&#20570;&#21040;&#23384;&#22312;&#30340;&#20381;&#36182;&#20110;&#20854;&#34892;&#20026;&#65292;&#36825;&#24847;&#21619;&#30528;&#20154;&#24037;&#24847;&#35782;&#30340;&#21487;&#34892;&#24615;&#23384;&#22312;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24341;&#21457;&#20102;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#30340;&#29468;&#27979;&#12290;&#20174;&#31070;&#32463;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#35266;&#28857;&#24456;&#38590;&#34987;&#35777;&#23454;&#12290;&#39318;&#20808;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#32570;&#23569;&#21754;&#20083;&#21160;&#29289;&#24847;&#35782;&#24863;&#30693;&#30456;&#20851;&#30340;&#19992;&#33041;&#30382;&#23618;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#32570;&#20047;&#25105;&#20204;&#19982;&#21608;&#22260;&#19990;&#30028;&#30340;&#24863;&#23448;&#25509;&#35302;&#30340;&#20855;&#26377;&#20307;&#39564;&#12289;&#23884;&#20837;&#24335;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#34429;&#28982;&#21069;&#20004;&#20010;&#35770;&#28857;&#22312;&#26410;&#26469;&#30340;AI&#31995;&#32479;&#20013;&#21487;&#20197;&#34987;&#20811;&#26381;&#65292;&#20294;&#31532;&#19977;&#20010;&#21487;&#33021;&#26356;&#38590;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#36328;&#36234;&#12290;&#25442;&#35328;&#20043;&#65292;&#25105;&#20204;&#35748;&#20026;&#24847;&#35782;&#21487;&#33021;&#21462;&#20915;&#20110;&#26159;&#21542;&#22312;&#8220;&#28216;&#25103;&#20013;&#26377;&#30382;&#32932;&#8221;&#65292;&#21363;&#31995;&#32479;&#30340;&#23384;&#22312;&#26159;&#21542;&#21462;&#20915;&#20110;&#20854;&#34892;&#20026;&#65292;&#32780;&#36825;&#22312;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#24182;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions with large language models have led to the suggestion that these models may be conscious. From the perspective of neuroscience, this position is difficult to defend. For one, the architecture of large language models is missing key features of the thalamocortical system that have been linked to conscious awareness in mammals. Secondly, the inputs to large language models lack the embodied, embedded information content characteristic of our sensory contact with the world around us. Finally, while the previous two arguments can be overcome in future AI systems, the third one might be harder to bridge in the near future. Namely, we argue that consciousness might depend on having 'skin in the game', in that the existence of the system depends on its actions, which is not true for present-day artificial intelligence.
&lt;/p&gt;</description></item><item><title>SimFBO&#21644;&#20854;ShroFBO&#21464;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;FBO&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.19442</link><description>&lt;p&gt;
SimFBO&#65306;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#21452;&#23618;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning. (arXiv:2305.19442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19442
&lt;/p&gt;
&lt;p&gt;
SimFBO&#21644;&#20854;ShroFBO&#21464;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#28789;&#27963;&#19988;&#36890;&#20449;&#39640;&#25928;&#30340;FBO&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#30001;&#20110;&#20803;&#23398;&#20064;&#12289;&#24494;&#35843;&#12289;&#36229;&#21442;&#25968;&#35843;&#25972;&#31561;&#39046;&#22495;&#20013;&#23884;&#22871;&#20248;&#21270;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32852;&#37030;&#21452;&#23618;&#20248;&#21270;&#65288;FBO&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#20013;&#26174;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FBO&#31639;&#27861;&#24448;&#24448;&#28041;&#21450;&#22797;&#26434;&#30340;&#35745;&#31639;&#65292;&#24182;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#22810;&#20010;&#23376;&#24490;&#29615;&#65292;&#27599;&#20010;&#23376;&#24490;&#29615;&#21253;&#21547;&#22810;&#20010;&#36890;&#20449;&#36718;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SimFBO&#30340;&#31616;&#21333;&#28789;&#27963;&#30340;FBO&#26694;&#26550;&#65292;&#23427;&#26131;&#20110;&#23454;&#29616;&#65292;&#19981;&#38656;&#35201;&#23376;&#24490;&#29615;&#65292;&#24182;&#21253;&#25324;&#19968;&#31181;&#24191;&#20041;&#30340;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#21644;&#26356;&#26032;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#31995;&#32479;&#32423;&#24322;&#26500;&#40065;&#26834;FBO&#65288;ShroFBO&#65289;&#20316;&#20026;SimFBO&#30340;&#21464;&#20307;&#65292;&#20854;&#23545;&#26412;&#22320;&#35745;&#31639;&#30340;&#24322;&#26500;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#21644;&#26080;&#26367;&#25442;&#30340;&#23458;&#25143;&#31471;&#37319;&#26679;&#19979;&#65292;SimFBO&#21644;ShroFBO&#21487;&#20197;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#21152;&#36895;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#26679;&#26412;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#20803;&#23398;&#20064;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demons
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#32676;&#20307;&#20013;&#30456;&#20284;&#30340;&#20010;&#20307;&#26469;&#35299;&#20915;&#32676;&#20307;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20542;&#21521;&#24471;&#20998;&#65292;&#35782;&#21035;&#23545;&#31561;&#20010;&#20307;&#65292;&#36991;&#20813;&#20102;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#20010;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18160</link><description>&lt;p&gt;
&#23545;&#31561;&#20844;&#24179;&#24615;&#8212;&#8212;&#35299;&#20915;&#20844;&#24179;&#35780;&#20272;&#20013;&#32676;&#20307;&#20043;&#38388;&#31995;&#32479;&#24046;&#24322;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Counterpart Fairness -- Addressing Systematic between-group Differences in Fairness Evaluation. (arXiv:2305.18160v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#32676;&#20307;&#20013;&#30456;&#20284;&#30340;&#20010;&#20307;&#26469;&#35299;&#20915;&#32676;&#20307;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20542;&#21521;&#24471;&#20998;&#65292;&#35782;&#21035;&#23545;&#31561;&#20010;&#20307;&#65292;&#36991;&#20813;&#20102;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#20010;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#20844;&#24179;&#24615;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#20915;&#31574;&#26102;&#65292;&#30830;&#20445;&#31639;&#27861;&#20915;&#31574;&#20844;&#24179;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#21363;&#19981;&#27495;&#35270;&#29305;&#23450;&#20010;&#20307;/&#32676;&#20307;&#65292;&#23588;&#20854;&#26159;&#26469;&#33258;&#24369;&#21183;&#32676;&#20307;&#30340;&#20154;&#12290;&#29616;&#26377;&#30340;&#32676;&#20307;&#20844;&#24179;&#26041;&#27861;&#35201;&#27714;&#36827;&#34892;&#24179;&#31561;&#30340;&#32676;&#20307;&#27979;&#37327;&#65292;&#20294;&#26410;&#32771;&#34385;&#32676;&#20307;&#20043;&#38388;&#30340;&#31995;&#32479;&#24046;&#24322;&#12290;&#28151;&#28102;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#34429;&#28982;&#19982;&#25935;&#24863;&#21464;&#37327;&#26080;&#20851;&#65292;&#20294;&#34920;&#29616;&#20986;&#31995;&#32479;&#24046;&#24322;&#65292;&#20250;&#23545;&#20844;&#24179;&#35780;&#20272;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#20844;&#24179;&#27979;&#37327;&#24212;&#35813;&#22522;&#20110;&#19981;&#21516;&#32676;&#20307;&#20013;&#30456;&#20284;&#20110;&#24863;&#20852;&#36259;&#20219;&#21153;&#30340;&#23545;&#31561;&#20154;&#65288;&#21363;&#24444;&#27492;&#30456;&#20284;&#30340;&#20010;&#20307;&#65289;&#20043;&#38388;&#30340;&#27604;&#36739;&#65292;&#20854;&#32676;&#20307;&#36523;&#20221;&#19981;&#21487;&#36890;&#36807;&#25506;&#32034;&#28151;&#28102;&#22240;&#32032;&#31639;&#27861;&#22320;&#21306;&#20998;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#20542;&#21521;&#24471;&#20998;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#23545;&#31561;&#20010;&#20307;&#65292;&#20197;&#36991;&#20813;&#20844;&#24179;&#35780;&#20272;&#27604;&#36739;&#8220;&#27225;&#23376;&#8221;&#21644;&#8220;&#33529;&#26524;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
When using machine learning (ML) to aid decision-making, it is critical to ensure that an algorithmic decision is fair, i.e., it does not discriminate against specific individuals/groups, particularly those from underprivileged populations. Existing group fairness methods require equal group-wise measures, which however fails to consider systematic between-group differences. The confounding factors, which are non-sensitive variables but manifest systematic differences, can significantly affect fairness evaluation. To tackle this problem, we believe that a fairness measurement should be based on the comparison between counterparts (i.e., individuals who are similar to each other with respect to the task of interest) from different groups, whose group identities cannot be distinguished algorithmically by exploring confounding factors. We have developed a propensity-score-based method for identifying counterparts, which prevents fairness evaluation from comparing "oranges" with "apples". 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#23548;&#21644;&#35780;&#20272;&#20102;&#22235;&#31181;&#36125;&#21494;&#26031;&#25209;&#27425;&#36172;&#21338;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#30830;&#23450;&#27969;&#37327;&#20998;&#37197;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#20449;&#24230;&#12289;&#25935;&#24863;&#24615;&#21644;&#21518;&#24724;&#24230;&#65292;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.14704</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#25277;&#26679;&#31639;&#27861;&#30340;&#22312;&#32447;&#33258;&#36866;&#24212;&#27969;&#37327;&#23454;&#39564;&#30340;&#23454;&#29992;&#25209;&#27425;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation. (arXiv:2305.14704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#23548;&#21644;&#35780;&#20272;&#20102;&#22235;&#31181;&#36125;&#21494;&#26031;&#25209;&#27425;&#36172;&#21338;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#30830;&#23450;&#27969;&#37327;&#20998;&#37197;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#20449;&#24230;&#12289;&#25935;&#24863;&#24615;&#21644;&#21518;&#24724;&#24230;&#65292;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21152;&#36895;&#22312;&#32447;&#27979;&#35797;&#65292;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25910;&#38598;&#25968;&#25454;&#32780;&#34987;&#20316;&#20026;&#22266;&#23450;&#26102;&#38388;A/B&#27979;&#35797;&#30340;&#37325;&#35201;&#34917;&#20805;&#26041;&#24335;&#19981;&#26029;&#25552;&#39640;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110;&#33258;&#36866;&#24212;&#25910;&#38598;&#25968;&#25454;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#21644;&#32479;&#35745;&#25512;&#26029;&#30340;&#30740;&#31350;&#65292;&#25512;&#23548;&#21644;&#35780;&#20272;&#20102;&#22235;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25209;&#27425;&#36172;&#21338;&#31639;&#27861;&#65288;NB-TS&#65292;WB-TS&#65292;NB-TTTS&#65292;WB-TTTS&#65289;&#65292;&#23427;&#20204;&#26159;&#20004;&#31181;&#21152;&#26435;&#25209;&#27425;&#65288;Naive Batch&#21644;Weighted Batch&#65289;&#21644;&#20004;&#31181;&#36125;&#21494;&#26031;&#25277;&#26679;&#31574;&#30053;&#65288;Thompson Sampling&#21644;Top-Two Thompson Sampling&#65289;&#30340;&#32452;&#21512;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#30830;&#23450;&#27969;&#37327;&#20998;&#37197;&#12290;&#26412;&#25991;&#25552;&#20379;&#30340;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#32479;&#35745;&#22870;&#21169;&#24230;&#37327;&#30340;&#36125;&#21494;&#26031;&#25277;&#26679;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#24471;&#20197;&#24212;&#29992;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#30340;&#20854;&#20013;&#19968;&#20010;&#32452;&#21512;WB-TTTS&#20284;&#20046;&#26159;&#26368;&#26032;&#35752;&#35770;&#30340;&#12290;&#23545;&#36825;&#22235;&#31181;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#25277;&#26679;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#27979;&#35797;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12289;&#25935;&#24863;&#24615;&#21644;&#21518;&#24724;&#24230;&#12290;&#27492;&#22806;&#65292;&#35780;&#20272;&#36824;&#32771;&#34385;&#20102;&#25209;&#27425;&#20869;&#22870;&#21169;&#24230;&#37327;&#30340;&#26041;&#24046;&#20197;&#21450;&#25209;&#27425;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#36172;&#21338;&#31639;&#27861;&#65288;&#20363;&#22914;UCB1&#65292;TS&#21644;Exp3&#65289;&#30456;&#27604;&#65292;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
To speed up online testing, adaptive traffic experimentation through multi-armed bandit algorithms is rising as an essential complementary alternative to the fixed horizon A/B testing. Based on recent research on best arm identification and statistical inference with adaptively collected data, this paper derives and evaluates four Bayesian batch bandit algorithms (NB-TS, WB-TS, NB-TTTS, WB-TTTS), which are combinations of two ways of weighting batches (Naive Batch and Weighted Batch) and two Bayesian sampling strategies (Thompson Sampling and Top-Two Thompson Sampling) to adaptively determine traffic allocation. These derived Bayesian sampling algorithms are practically based on summary batch statistics of a reward metric for pilot experiments, where one of the combination WB-TTTS in this paper seems to be newly discussed. The comprehensive evaluation on the four Bayesian sampling algorithms covers trustworthiness, sensitivity and regret of a testing methodology. Moreover, the evaluati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#36827;&#34892;&#35843;&#26597;&#65292;&#20998;&#31867;&#23427;&#20204;&#30340;&#24050;&#30693;&#28431;&#27934;&#65292;&#23558;&#20854;&#20998;&#20026;&#22266;&#26377;&#38382;&#39064;&#12289;&#26377;&#24847;&#25915;&#20987;&#21644;&#24847;&#22806;&#38169;&#35823;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#22235;&#31181;&#20114;&#34917;&#25216;&#26415;&#20197;&#25552;&#20379;LLM&#21450;&#20854;&#24212;&#29992;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2305.11391</link><description>&lt;p&gt;
&#36890;&#36807;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#36827;&#34892;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation. (arXiv:2305.11391v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11391
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39564;&#35777;&#21644;&#39564;&#35777;&#30340;&#35270;&#35282;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#36827;&#34892;&#35843;&#26597;&#65292;&#20998;&#31867;&#23427;&#20204;&#30340;&#24050;&#30693;&#28431;&#27934;&#65292;&#23558;&#20854;&#20998;&#20026;&#22266;&#26377;&#38382;&#39064;&#12289;&#26377;&#24847;&#25915;&#20987;&#21644;&#24847;&#22806;&#38169;&#35823;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#22235;&#31181;&#20114;&#34917;&#25216;&#26415;&#20197;&#25552;&#20379;LLM&#21450;&#20854;&#24212;&#29992;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#22312;&#35768;&#22810;&#30693;&#35782;&#39046;&#22495;&#20013;&#20026;&#32456;&#31471;&#29992;&#25143;&#25552;&#20379;&#35814;&#32454;&#21644;&#26377;&#26465;&#29702;&#30340;&#31572;&#26696;&#65292;&#24182;&#33021;&#22815;&#36827;&#34892;&#20154;&#31867;&#32423;&#21035;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#24341;&#21457;&#20102;AI&#30340;&#19968;&#27874;&#26032;&#28909;&#28526;&#12290;&#20026;&#20102;&#24212;&#23545;&#23427;&#20204;&#22312;&#35768;&#22810;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#24555;&#36895;&#37319;&#29992;&#65292;&#26412;&#27425;&#35843;&#26597;&#20851;&#27880;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22238;&#39038;LLM&#30340;&#24050;&#30693;&#28431;&#27934;&#65292;&#23558;&#23427;&#20204;&#20998;&#31867;&#20026;&#22266;&#26377;&#38382;&#39064;&#12289;&#26377;&#24847;&#25915;&#20987;&#21644;&#24847;&#22806;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#23558;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#20256;&#32479;&#36719;&#20214;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#39564;&#35777;&#21644;&#39564;&#35777;&#65288;V&#65286;V&#65289;&#25216;&#26415;&#65292;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;LLM&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#65292;&#20197;&#25552;&#20379;&#20005;&#26684;&#30340;&#20998;&#26512;&#65292;&#30830;&#20445;LLM&#21450;&#20854;&#24212;&#29992;&#30340;&#23433;&#20840;&#21644;&#21487;&#20449;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#22235;&#31181;&#20114;&#34917;&#25216;&#26415;&#65306;&#34394;&#20551;&#24615;&#21644;&#35780;&#20272;&#12289;&#39564;&#35777;&#12289;&#36816;&#34892;&#26102;&#30417;&#35270;&#21644;&#36947;&#24503;&#20351;&#29992;&#12290;&#32771;&#34385;&#21040;LLM&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exploded a new heatwave of AI, for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities of the LLMs, categorising them into inherent issues, intended attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V&amp;V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and ethical use. Considering the fast development of 
&lt;/p&gt;</description></item><item><title>TDC'22&#26159;&#31532;&#19968;&#23626;&#38754;&#21521;ICDs&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#21019;&#26032;&#31454;&#36187;&#12290;&#26412;&#27425;&#31454;&#36187;&#30340;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.05105</link><description>&lt;p&gt;
&#38754;&#21521;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#24494;&#23567;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
TinyML Design Contest for Life-Threatening Ventricular Arrhythmia Detection. (arXiv:2305.05105v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05105
&lt;/p&gt;
&lt;p&gt;
TDC'22&#26159;&#31532;&#19968;&#23626;&#38754;&#21521;ICDs&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#21019;&#26032;&#31454;&#36187;&#12290;&#26412;&#27425;&#31454;&#36187;&#30340;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#23626;ACM/IEEE&#24494;&#23567;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#31454;&#36187;&#65288;TDC&#65289;&#20110;2022&#24180;&#22312;&#31532;41&#23626;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#22269;&#38469;&#20250;&#35758;&#65288;ICCAD&#65289;&#19978;&#20030;&#34892;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26376;&#30740;&#21457;&#31454;&#36187;&#12290;TDC'22&#19987;&#27880;&#20110;&#38656;&#35201;&#22312;&#21487;&#26893;&#20837;&#35774;&#22791;&#19978;&#21019;&#26032;&#21644;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#30340;&#30495;&#23454;&#21307;&#30103;&#38382;&#39064;&#12290;TDC'22&#30340;&#25361;&#25112;&#38382;&#39064;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#24515;&#33039;&#38500;&#39076;&#22120;&#65288;ICDs&#65289;&#19978;&#20351;&#29992;&#30340;&#20302;&#21151;&#29575;&#24494;&#25511;&#21046;&#22120;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;90&#20010;&#21463;&#35797;&#32773;&#30340;8&#31181;&#19981;&#21516;&#24515;&#24459;&#31867;&#22411;&#30340;&#36229;&#36807;38,000&#20010;5&#31186;&#24515;&#20869;&#30005;&#22270;&#65288;IEGM&#65289;&#29255;&#27573;&#12290;&#19987;&#29992;&#30828;&#20214;&#24179;&#21488;&#26159;STMicroelectronics&#21046;&#36896;&#30340;NUCLEO-L432KC&#12290;TDC'22&#38754;&#21521;&#20840;&#29699;&#22810;&#20154;&#22242;&#38431;&#65292;&#21560;&#24341;&#20102;&#26469;&#33258;50&#22810;&#20010;&#32452;&#32455;&#30340;150&#22810;&#25903;&#38431;&#20237;&#21442;&#36187;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#36825;&#19968;&#21307;&#30103;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The first ACM/IEEE TinyML Design Contest (TDC) held at the 41st International Conference on Computer-Aided Design (ICCAD) in 2022 is a challenging, multi-month, research and development competition. TDC'22 focuses on real-world medical problems that require the innovation and implementation of artificial intelligence/machine learning (AI/ML) algorithms on implantable devices. The challenge problem of TDC'22 is to develop a novel AI/ML-based real-time detection algorithm for life-threatening ventricular arrhythmia over low-power microcontrollers utilized in Implantable Cardioverter-Defibrillators (ICDs). The dataset contains more than 38,000 5-second intracardiac electrograms (IEGMs) segments over 8 different types of rhythm from 90 subjects. The dedicated hardware platform is NUCLEO-L432KC manufactured by STMicroelectronics. TDC'22, which is open to multi-person teams world-wide, attracted more than 150 teams from over 50 organizations. This paper first presents the medical problem, da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#21453;&#39304;&#30340;&#26080;&#38480;&#26102;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02527</link><description>&lt;p&gt;
&#12298;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Delayed, Composite, and Partially Anonymous Reward. (arXiv:2305.02527v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#21453;&#39304;&#30340;&#26080;&#38480;&#26102;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#65292;&#24182;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#24310;&#36831;&#12289;&#22797;&#21512;&#21644;&#37096;&#20998;&#21311;&#21517;&#22870;&#21169;&#21453;&#39304;&#30340;&#26080;&#38480;&#26102;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#12290;&#22870;&#21169;&#30340;&#24310;&#36831;&#21644;&#22797;&#26434;&#24615;&#24847;&#21619;&#30528;&#22312;&#32473;&#23450;&#29366;&#24577;&#19979;&#37319;&#21462;&#34892;&#21160;&#29983;&#25104;&#30340;&#22870;&#21169;&#34987;&#20998;&#35299;&#20026;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#24310;&#36831;&#30340;&#26102;&#38388;&#23454;&#20363;&#20013;&#34987;&#39034;&#24207;&#23454;&#29616;&#12290;&#37096;&#20998;&#21311;&#21517;&#23646;&#24615;&#24847;&#21619;&#30528;&#23545;&#20110;&#27599;&#20010;&#29366;&#24577;&#65292;&#23398;&#20064;&#32773;&#21482;&#35266;&#23519;&#21040;&#22312;&#35813;&#29366;&#24577;&#19979;&#37319;&#21462;&#19981;&#21516;&#34892;&#21160;&#20135;&#29983;&#30340;&#36807;&#21435;&#22870;&#21169;&#32452;&#25104;&#37096;&#20998;&#30340;&#24635;&#21644;&#65292;&#20294;&#26159;&#22312;&#35266;&#23519;&#23454;&#20363;&#20013;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\mathrm{DUCRL2}$&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33719;&#24471;&#27492;&#35774;&#32622;&#30340;&#36817;&#20284;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#34920;&#26126;&#23427;&#23454;&#29616;&#20102;$\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ &#30340;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;$S$&#21644;$A$&#20998;&#21035;&#26159;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;$D$&#26159;MDP&#30340;&#30452;&#24452;&#65292;$d$&#26159;&#19968;&#20010;&#30001;&#26368;&#22823;&#22870;&#21169;&#24310;&#36831;&#38480;&#21046;&#30340;&#21442;&#25968;&#65292;$T$&#34920;&#31034;&#26102;&#38388;&#30340;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate an infinite-horizon average reward Markov Decision Process (MDP) with delayed, composite, and partially anonymous reward feedback. The delay and compositeness of rewards mean that rewards generated as a result of taking an action at a given state are fragmented into different components, and they are sequentially realized at delayed time instances. The partial anonymity attribute implies that a learner, for each state, only observes the aggregate of past reward components generated as a result of different actions taken at that state, but realized at the observation instance. We propose an algorithm named $\mathrm{DUCRL2}$ to obtain a near-optimal policy for this setting and show that it achieves a regret bound of $\tilde{\mathcal{O}}\left(DS\sqrt{AT} + d (SA)^3\right)$ where $S$ and $A$ are the sizes of the state and action spaces, respectively, $D$ is the diameter of the MDP, $d$ is a parameter upper bounded by the maximum reward delay, and $T$ denotes the time horizon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23384;&#20648;&#30340;&#36716;&#25442;&#32452;&#32455;&#25104;&#19968;&#31181;&#31616;&#27905;&#30340;&#29615;&#22659;&#27169;&#22411;&#32593;&#32476;&#65292;&#20197;&#22312;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#30340;&#21516;&#26102;&#22686;&#21152;&#27599;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.02054</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#65306;&#24378;&#21270;&#23398;&#20064;&#20013;&#36951;&#24536;&#29616;&#35937;&#30340;&#20869;&#23384;&#33410;&#32422;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Map-based Experience Replay: A Memory-Efficient Solution to Catastrophic Forgetting in Reinforcement Learning. (arXiv:2305.02054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23384;&#20648;&#30340;&#36716;&#25442;&#32452;&#32455;&#25104;&#19968;&#31181;&#31616;&#27905;&#30340;&#29615;&#22659;&#27169;&#22411;&#32593;&#32476;&#65292;&#20197;&#22312;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#30340;&#21516;&#26102;&#22686;&#21152;&#27599;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#35757;&#32451;&#26032;&#25968;&#25454;&#26102;&#24120;&#24120;&#20250;&#36973;&#21463;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#65292;&#36951;&#24536;&#20808;&#21069;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22238;&#25918;&#35760;&#24518;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#23427;&#20250;&#23545;&#26087;&#21644;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#21435;&#20851;&#32852;&#21644;&#28151;&#27927;&#12290;&#20182;&#20204;&#22825;&#30495;&#22320;&#25353;&#29031;&#29366;&#24577;&#36807;&#28193;&#30340;&#39034;&#24207;&#23384;&#20648;&#29366;&#24577;&#36716;&#21464;&#65292;&#32780;&#19981;&#32771;&#34385;&#20887;&#20313;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Grow-When-Required&#65288;GWR&#65289;&#33258;&#32452;&#32455;&#32593;&#32476;&#30340;&#26032;&#22411;&#35748;&#30693;&#21551;&#21457;&#24335;&#22238;&#25918;&#20869;&#23384;&#26041;&#27861;&#65292;&#23427;&#31867;&#20284;&#20110;&#19968;&#31181;&#22522;&#20110;&#22320;&#22270;&#30340;&#19990;&#30028;&#35748;&#30693;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#23384;&#20648;&#30340;&#36716;&#25442;&#32452;&#32455;&#25104;&#19968;&#20010;&#31616;&#27905;&#30340;&#29615;&#22659;&#27169;&#22411;&#32593;&#32476;&#65292;&#23558;&#30456;&#20284;&#30340;&#26679;&#26412;&#21512;&#24182;&#20197;&#20943;&#23569;&#20869;&#23384;&#22823;&#23567;&#24182;&#22686;&#21152;&#26679;&#26412;&#20043;&#38388;&#30340;&#20004;&#20004;&#36317;&#31163;&#65292;&#20174;&#32780;&#22686;&#21152;&#27599;&#20010;&#26679;&#26412;&#30340;&#30456;&#20851;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#34920;&#26126;&#65292;&#22522;&#20110;&#22320;&#22270;&#30340;&#32463;&#39564;&#22238;&#25918;&#20801;&#35768;&#26174;&#30528;&#20943;&#23569;&#20869;&#23384;&#65292;&#21482;&#20250;&#20135;&#29983;&#36731;&#24494;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning agents often suffer from catastrophic forgetting, forgetting previously found solutions in parts of the input space when training on new data. Replay Memories are a common solution to the problem, decorrelating and shuffling old and new training samples. They naively store state transitions as they come in, without regard for redundancy. We introduce a novel cognitive-inspired replay memory approach based on the Grow-When-Required (GWR) self-organizing network, which resembles a map-based mental model of the world. Our approach organizes stored transitions into a concise environment-model-like network of state-nodes and transition-edges, merging similar samples to reduce the memory size and increase pair-wise distance among samples, which increases the relevancy of each sample. Overall, our paper shows that map-based experience replay allows for significant memory reduction with only small performance decreases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25239;&#22122;&#22768;&#30340;&#22768;&#23398;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#19982;&#21507;&#33609;&#21644;&#21453;&#21005;&#30456;&#20851;&#30340;&#37492;&#23450;&#19979;&#39066;&#36816;&#21160;&#20107;&#20214;&#30340;&#22266;&#23450;&#38271;&#24230;&#27573;&#65292;&#29992;&#20110;&#35782;&#21035;&#29275;&#30340;&#35269;&#39135;&#27963;&#21160;&#65292;&#24182;&#22312;&#29615;&#22659;&#21644;&#33258;&#28982;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.14824</link><description>&lt;p&gt;
&#19968;&#31181;&#25239;&#22122;&#22768;&#30340;&#22768;&#23398;&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#29275;&#30340;&#35269;&#39135;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
A noise-robust acoustic method for recognition of foraging activities of grazing cattle. (arXiv:2304.14824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25239;&#22122;&#22768;&#30340;&#22768;&#23398;&#26041;&#27861;&#65292;&#33021;&#22815;&#20998;&#26512;&#19982;&#21507;&#33609;&#21644;&#21453;&#21005;&#30456;&#20851;&#30340;&#37492;&#23450;&#19979;&#39066;&#36816;&#21160;&#20107;&#20214;&#30340;&#22266;&#23450;&#38271;&#24230;&#27573;&#65292;&#29992;&#20110;&#35782;&#21035;&#29275;&#30340;&#35269;&#39135;&#27963;&#21160;&#65292;&#24182;&#22312;&#29615;&#22659;&#21644;&#33258;&#28982;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#20083;&#21046;&#21697;&#24066;&#22330;&#20013;&#20445;&#25345;&#31454;&#20105;&#21147;&#65292;&#20892;&#27665;&#24517;&#39035;&#19981;&#26029;&#25913;&#36827;&#20182;&#20204;&#30340;&#30044;&#29287;&#29983;&#20135;&#31995;&#32479;&#12290;&#31934;&#30830;&#30044;&#29287;&#19994;&#25216;&#26415;&#25552;&#20379;&#20102;&#21830;&#19994;&#20892;&#22330;&#21160;&#29289;&#20010;&#20307;&#21270;&#30417;&#27979;&#65292;&#20248;&#21270;&#30044;&#29287;&#29983;&#20135;&#12290;&#36830;&#32493;&#30340;&#22768;&#23398;&#30417;&#27979;&#26159;&#19968;&#31181;&#24191;&#27867;&#25509;&#21463;&#30340;&#24863;&#24212;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#33258;&#30001;&#25918;&#29287;&#29275;&#30340;&#26085;&#21453;&#21005;&#21644;&#21507;&#33609;&#26102;&#38388;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#29287;&#22330;&#19978;&#30340;&#20856;&#22411;&#29615;&#22659;&#21644;&#33258;&#28982;&#22122;&#22768;&#26126;&#26174;&#24433;&#21709;&#24403;&#21069;&#22768;&#23398;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22768;&#23398;&#26041;&#27861;&#65292;&#31216;&#20026;&#25239;&#22122;&#22768;&#35269;&#39135;&#27963;&#21160;&#35782;&#21035;&#22120; (NRFAR)&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#19982;&#21507;&#33609;&#21644;&#21453;&#21005;&#30456;&#20851;&#30340;&#37492;&#23450;&#19979;&#39066;&#36816;&#21160;&#20107;&#20214;&#30340;&#22266;&#23450;&#38271;&#24230;&#27573;&#65292;&#30830;&#23450;&#35269;&#39135;&#27963;&#21160;&#30340;&#31361;&#21457;&#12290;NRFAR &#30340;&#21152;&#24615;&#22122;&#22768;&#40065;&#26834;&#24615;&#20351;&#29992;&#38745;&#24577;&#39640;&#26031;&#30333;&#22122;&#22768;&#21644;&#22235;&#31181;&#19981;&#21516;&#30340;&#38750;&#38745;&#24577;&#33258;&#28982;&#22122;&#22768;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
To stay competitive in the growing dairy market, farmers must continuously improve their livestock production systems. Precision livestock farming technologies provide individualised monitoring of animals on commercial farms, optimising livestock production. Continuous acoustic monitoring is a widely accepted sensing technique used to estimate the daily rumination and grazing time budget of free-ranging cattle. However, typical environmental and natural noises on pasture noticeably affect the performance and generalisation of current acoustic methods. In this study, we present an acoustic method called Noise-Robust Foraging Activity Recognizer (NRFAR). The proposed method determines foraging activity bouts by analysing fixed-length segments of identified jaw movement events associated with grazing and rumination. The additive noise robustness of NRFAR was evaluated for several signal-to-noise ratios, using stationary Gaussian white noise and four different non-stationary natural noise 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#32447;&#24615;&#31995;&#32479;&#30340;&#31232;&#30095;&#30697;&#38453;&#20026;&#40657;&#30333;&#22270;&#20687;&#65292;&#21033;&#29992;&#27744;&#21270;&#25805;&#20316;&#23558;&#20854;&#36716;&#25442;&#20026;&#23567;&#30340;&#22810;&#36890;&#36947;&#22270;&#20687;&#65292;&#20174;&#32780;&#35843;&#25972;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#20013;&#30340;&#24378;&#38376;&#27099;&#21442;&#25968;&#12290;&#35813;&#31639;&#27861;&#26368;&#23567;&#21270;&#20102;AMG&#26041;&#27861;&#22312;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#35299;&#20915;&#19977;&#32500;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;AMG&#27714;&#35299;&#22120;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2304.10832</link><description>&lt;p&gt;
&#19968;&#31181;&#21152;&#36895;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20013;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning algorithm to accelerate Algebraic Multigrid methods in Finite Element solvers of 3D elliptic PDEs. (arXiv:2304.10832v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10832
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#32447;&#24615;&#31995;&#32479;&#30340;&#31232;&#30095;&#30697;&#38453;&#20026;&#40657;&#30333;&#22270;&#20687;&#65292;&#21033;&#29992;&#27744;&#21270;&#25805;&#20316;&#23558;&#20854;&#36716;&#25442;&#20026;&#23567;&#30340;&#22810;&#36890;&#36947;&#22270;&#20687;&#65292;&#20174;&#32780;&#35843;&#25972;&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#20013;&#30340;&#24378;&#38376;&#27099;&#21442;&#25968;&#12290;&#35813;&#31639;&#27861;&#26368;&#23567;&#21270;&#20102;AMG&#26041;&#27861;&#22312;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#35299;&#20915;&#19977;&#32500;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#26102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;AMG&#27714;&#35299;&#22120;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#25968;&#22810;&#37325;&#32593;&#26684;&#26041;&#27861;&#26159;&#35299;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#26368;&#26377;&#25928;&#30340;&#27714;&#35299;&#22120;&#20043;&#19968;&#65292;&#24191;&#27867;&#29992;&#20110;&#31163;&#25955;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#23384;&#22312;&#39640;&#24230;&#20381;&#36182;&#20110;&#38656;&#35201;&#35843;&#20248;&#30340;&#21442;&#25968;&#65292;&#23588;&#20854;&#26159;&#24378;&#38376;&#27099;&#21442;&#25968;&#65292;&#36825;&#26159;&#26500;&#24314;&#22810;&#37325;&#32593;&#26684;&#25152;&#38656;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25226;&#32447;&#24615;&#31995;&#32479;&#30340;&#31232;&#30095;&#30697;&#38453;&#35299;&#37322;&#20026;&#40657;&#30333;&#22270;&#20687;&#65292;&#21033;&#29992;&#27744;&#21270;&#25805;&#20316;&#23558;&#23427;&#36716;&#25442;&#20026;&#23567;&#30340;&#22810;&#36890;&#36947;&#22270;&#20687;&#65292;&#20174;&#32780;&#35843;&#25972;&#24378;&#38376;&#27099;&#21442;&#25968;&#65292;&#26368;&#23567;&#21270;&#20102;AMG&#26041;&#27861;&#22312;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#23545;&#20110;&#35299;&#20915;&#19977;&#32500;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36895;&#24230;&#26174;&#33879;&#24555;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;AMG&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algebraic multigrid (AMG) methods are among the most efficient solvers for linear systems of equations and they are widely used for the solution of problems stemming from the discretization of Partial Differential Equations (PDEs). The most severe limitation of AMG methods is the dependence on parameters that require to be fine-tuned. In particular, the strong threshold parameter is the most relevant since it stands at the basis of the construction of successively coarser grids needed by the AMG methods. We introduce a novel Deep Learning algorithm that minimizes the computational cost of the AMG method when used as a finite element solver. We show that our algorithm requires minimal changes to any existing code. The proposed Artificial Neural Network (ANN) tunes the value of the strong threshold parameter by interpreting the sparse matrix of the linear system as a black-and-white image and exploiting a pooling operator to transform it into a small multi-channel image. We experimentall
&lt;/p&gt;</description></item><item><title>&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#38656;&#35201;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#65292;&#32780;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#26159;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.06634</link><description>&lt;p&gt;
PGTask&#65306;&#20171;&#32461;&#20174;&#23545;&#35805;&#20013;&#29983;&#25104;&#26723;&#26696;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
PGTask: Introducing the Task of Profile Generation from Dialogues. (arXiv:2304.06634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06634
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#38656;&#35201;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#65292;&#32780;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#26159;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23558;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#26469;&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#20449;&#24687;&#31232;&#23569;&#19988;&#38590;&#20197;&#33719;&#21462;&#65292;&#36825;&#20351;&#24471;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;/&#29983;&#25104;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#25104;&#20026;&#19968;&#39033;&#22522;&#26412;&#38656;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26723;&#26696;&#29983;&#25104;&#20219;&#21153;&#65288;PGTask&#65289;&#12290;&#25105;&#20204;&#20026;&#27492;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#30456;&#20851;&#35805;&#35821;&#23545;&#40784;&#30340;&#26723;&#26696;&#21477;&#23376;&#65292;&#20174;&#23545;&#35805;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20026;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#20010;&#26723;&#26696;&#29983;&#25104;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26723;&#26696;&#29983;&#25104;&#30340;&#25361;&#25112;&#65292;&#24182;&#24076;&#26395;&#36825;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent approaches have attempted to personalize dialogue systems by leveraging profile information into models. However, this knowledge is scarce and difficult to obtain, which makes the extraction/generation of profile information from dialogues a fundamental asset. To surpass this limitation, we introduce the Profile Generation Task (PGTask). We contribute with a new dataset for this problem, comprising profile sentences aligned with related utterances, extracted from a corpus of dialogues. Furthermore, using state-of-the-art methods, we provide a benchmark for profile generation on this novel dataset. Our experiments disclose the challenges of profile generation, and we hope that this introduces a new research direction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#24212;&#23545;&#20256;&#24863;&#22120;&#22122;&#22768;&#21450;&#19981;&#30830;&#23450;&#24615;&#30340;&#24863;&#30693;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#24863;&#30693;&#22320;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#25511;&#21046;&#35774;&#35745;&#20013;&#65292;&#35745;&#31639;&#26377;&#25928;&#30340;&#29366;&#24577;&#20272;&#35745;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#30340;&#37319;&#26679;&#25968;&#25454;&#25511;&#21046;&#65292;&#30830;&#20445;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00194</link><description>&lt;p&gt;
&#22522;&#20110;&#32622;&#20449;&#24230;&#39044;&#27979;&#30340;&#38543;&#26426;&#20256;&#24863;&#22120;&#19981;&#30830;&#23450;&#24615;&#19979;&#23433;&#20840;&#30340;&#24863;&#30693;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Safe Perception-Based Control under Stochastic Sensor Uncertainty using Conformal Prediction. (arXiv:2304.00194v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#24212;&#23545;&#20256;&#24863;&#22120;&#22122;&#22768;&#21450;&#19981;&#30830;&#23450;&#24615;&#30340;&#24863;&#30693;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#24863;&#30693;&#22320;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#25511;&#21046;&#35774;&#35745;&#20013;&#65292;&#35745;&#31639;&#26377;&#25928;&#30340;&#29366;&#24577;&#20272;&#35745;&#21306;&#22495;&#65292;&#20174;&#32780;&#23454;&#29616;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#30340;&#37319;&#26679;&#25968;&#25454;&#25511;&#21046;&#65292;&#30830;&#20445;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#21033;&#29992;&#36890;&#36807;&#23398;&#20064;&#22686;&#24378;&#30340;&#24863;&#30693;&#22320;&#22270;&#20174;&#39640;&#32500;&#20256;&#24863;&#22120;&#27979;&#37327;&#20013;&#33719;&#24471;&#30340;&#29366;&#24577;&#20272;&#35745;&#30340;&#24863;&#30693;&#25511;&#21046;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#24863;&#30693;&#22320;&#22270;&#24182;&#19981;&#23436;&#32654;&#65292;&#20250;&#23548;&#33268;&#29366;&#24577;&#20272;&#35745;&#35823;&#24046;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#31995;&#32479;&#34892;&#20026;&#12290;&#38543;&#26426;&#20256;&#24863;&#22120;&#22122;&#22768;&#20250;&#20351;&#24773;&#20917;&#21464;&#24471;&#26356;&#31967;&#65292;&#24182;&#23548;&#33268;&#36981;&#24490;&#26410;&#30693;&#20998;&#24067;&#30340;&#20272;&#35745;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24863;&#30693;&#25511;&#21046;&#26694;&#26550;&#65292;&#23427;: i&#65289;&#37327;&#21270;&#20102;&#24863;&#30693;&#22320;&#22270;&#30340;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;ii&#65289;&#23558;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#38598;&#25104;&#21040;&#25511;&#21046;&#35774;&#35745;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#32622;&#20449;&#24230;&#39044;&#27979;&#26469;&#35745;&#31639;&#26377;&#25928;&#30340;&#29366;&#24577;&#20272;&#35745;&#21306;&#22495;&#65292;&#36825;&#20123;&#21306;&#22495;&#26159;&#39640;&#27010;&#29575;&#21253;&#21547;&#26410;&#30693;&#29366;&#24577;&#30340;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#27979;&#37327;&#40065;&#26834;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30340;&#27010;&#24565;&#35774;&#35745;&#20102;&#36830;&#32493;&#26102;&#38388;&#31995;&#32479;&#30340;&#37319;&#26679;&#25968;&#25454;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#30340;&#25511;&#21046;&#22120;&#20351;&#29992;&#20102;&#33258;&#35302;&#21457;&#25511;&#21046;&#30340;&#24605;&#24819;&#65292;&#24182;&#20351;&#25105;&#20204;&#36991;&#20813;&#20351;&#29992;&#38543;&#26426;&#24494;&#31215;&#20998;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26159;&#19968;&#31181;&#26131;&#20110;&#23454;&#29616;&#21644;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider perception-based control using state estimates that are obtained from high-dimensional sensor measurements via learning-enabled perception maps. However, these perception maps are not perfect and result in state estimation errors that can lead to unsafe system behavior. Stochastic sensor noise can make matters worse and result in estimation errors that follow unknown distributions. We propose a perception-based control framework that i) quantifies estimation uncertainty of perception maps, and ii) integrates these uncertainty representations into the control design. To do so, we use conformal prediction to compute valid state estimation regions, which are sets that contain the unknown state with high probability. We then devise a sampled-data controller for continuous-time systems based on the notion of measurement robust control barrier functions. Our controller uses idea from self-triggered control and enables us to avoid using stochastic calculus. Our framework is agnost
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.12091</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning. (arXiv:2303.12091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#26631;&#35760;&#25968;&#25454;&#12289;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#12290;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#32771;&#34385;&#21040;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#21253;&#21547;&#26631;&#35760;&#25968;&#25454;&#20013;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#31867;&#21035;&#65288;&#24322;&#24120;&#20540;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;ANEDL&#65289;&#65292;&#20197;&#24212;&#23545;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#22914;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#21644;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#20316;&#20026;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#22120;&#26469;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#20351;EDL&#26356;&#21152;&#36866;&#21512;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;ANEDL&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrat
&lt;/p&gt;</description></item><item><title>SIESTA&#26159;&#19968;&#31181;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#38656;&#22238;&#24518;&#12289;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#26356;&#26032;&#35268;&#21017;&#65292;&#22312;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#33021;&#28304;&#28040;&#32791;&#19979;&#39640;&#25928;&#22320;&#26356;&#26032;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#35757;&#32451;&#20241;&#30496;/&#35273;&#37266;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#35774;&#22791;&#31471;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.10725</link><description>&lt;p&gt;
SIESTA: &#39640;&#25928;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#19982;&#20241;&#30496; (arXiv:2303.10725v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
SIESTA: Efficient Online Continual Learning with Sleep. (arXiv:2303.10725v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10725
&lt;/p&gt;
&lt;p&gt;
SIESTA&#26159;&#19968;&#31181;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#38656;&#22238;&#24518;&#12289;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#26356;&#26032;&#35268;&#21017;&#65292;&#22312;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#33021;&#28304;&#28040;&#32791;&#19979;&#39640;&#25928;&#22320;&#26356;&#26032;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#35757;&#32451;&#20241;&#30496;/&#35273;&#37266;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#35774;&#22791;&#31471;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#24335;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#36890;&#36807;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#25454;&#27969;&#36827;&#34892;&#26356;&#26032;&#12290;&#19982;&#25968;&#25454;&#31163;&#32447;&#24773;&#20917;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#33021;&#23545;&#25968;&#25454;&#27969;&#36827;&#34892;&#20219;&#20309;&#20998;&#24067;&#20551;&#35774;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#21482;&#38656;&#35201;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#19968;&#27425;&#36941;&#21382;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#26465;&#20214;&#65292;&#21516;&#26102;&#20063;&#26080;&#27861;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35757;&#32451;&#20241;&#30496;/&#35273;&#37266;&#26694;&#26550;&#30340;&#26032;&#22411;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;SIESTA&#65292;&#35813;&#26041;&#27861;&#31526;&#21512;&#35774;&#22791;&#31471;&#23398;&#20064;&#30340;&#38656;&#27714;&#12290;SIESTA&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25913;&#36827;&#35745;&#31639;&#25928;&#29575;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#33021;&#28304;&#28040;&#32791;&#19979;&#39640;&#25928;&#22320;&#26356;&#26032;DNN&#12290;SIESTA&#30340;&#20027;&#35201;&#21019;&#26032;&#28857;&#26377;&#65306;&#22312;&#35273;&#37266;&#38454;&#27573;&#20351;&#29992;&#26080;&#38656;&#22238;&#24518;&#12289;&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#32593;&#32476;&#26356;&#26032;&#35268;&#21017;&#36827;&#34892;&#24555;&#36895;&#22312;&#32447;&#26356;&#26032;&#65292;&#20197;&#21450;&#24555;&#36895;&#25910;&#25947;&#30340;Wake/Sleep&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In supervised continual learning, a deep neural network (DNN) is updated with an ever-growing data stream. Unlike the offline setting where data is shuffled, we cannot make any distributional assumptions about the data stream. Ideally, only one pass through the dataset is needed for computational efficiency. However, existing methods are inadequate and make many assumptions that cannot be made for real-world applications, while simultaneously failing to improve computational efficiency. In this paper, we propose a novel online continual learning method, SIESTA based on wake/sleep framework for training, which is well aligned to the needs of on-device learning. The major goal of SIESTA is to advance compute efficient continual learning so that DNNs can be updated efficiently using far less time and energy. The principal innovations of SIESTA are: 1) rapid online updates using a rehearsal-free, backpropagation-free, and data-driven network update rule during its wake phase, and 2) expedi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#21512;&#20316;&#23398;&#20064;&#20013;&#20998;&#31867;&#22120;&#20559;&#24046;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#21512;&#25104;&#30340;ETF&#20998;&#31867;&#22120;&#65292;&#20351;&#24471;&#25152;&#26377;&#23458;&#25143;&#31471;&#33021;&#22815;&#23398;&#20064;&#21040;&#32479;&#19968;&#30340;&#26368;&#20248;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.10058</link><description>&lt;p&gt;
&#19981;&#24597;&#20998;&#31867;&#22120;&#20559;&#24046;&#65306;&#20197;&#31070;&#32463;&#23849;&#28291;&#20026;&#28789;&#24863;&#30340;&#21512;&#20316;&#23398;&#20064;&#20013;&#20351;&#29992;&#21512;&#25104;&#21644;&#22266;&#23450;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier. (arXiv:2303.10058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#21512;&#20316;&#23398;&#20064;&#20013;&#20998;&#31867;&#22120;&#20559;&#24046;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#21512;&#25104;&#30340;ETF&#20998;&#31867;&#22120;&#65292;&#20351;&#24471;&#25152;&#26377;&#23458;&#25143;&#31471;&#33021;&#22815;&#23398;&#20064;&#21040;&#32479;&#19968;&#30340;&#26368;&#20248;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24322;&#26500;&#24615;&#26159;&#22256;&#25200;&#21512;&#20316;&#23398;&#20064;&#24615;&#33021;&#30340;&#20869;&#22312;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#26412;&#22320;&#27169;&#22411;&#30340;&#20559;&#32622;&#20998;&#31867;&#22120;&#26159;&#20851;&#38190;&#29942;&#39048;&#12290;&#20197;&#21069;&#30340;&#23581;&#35797;&#21033;&#29992;FL&#35757;&#32451;&#21518;&#36827;&#34892;&#20998;&#31867;&#22120;&#26657;&#20934;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26410;&#33021;&#25913;&#21892;&#35757;&#32451;&#26102;&#20998;&#31867;&#22120;&#20559;&#24046;&#23548;&#33268;&#30340;&#24046;&#21155;&#29305;&#24449;&#34920;&#31034;&#12290;&#35299;&#20915;FL&#20013;&#20998;&#31867;&#22120;&#20559;&#24046;&#22256;&#22659;&#38656;&#35201;&#20805;&#20998;&#29702;&#35299;&#20998;&#31867;&#22120;&#32972;&#21518;&#30340;&#26426;&#21046;&#12290;&#31070;&#32463;&#23849;&#28291;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#22312;&#23436;&#32654;&#30340;&#35757;&#32451;&#22330;&#26223;&#19979;&#65292;&#20998;&#31867;&#22120;&#21644;&#29305;&#24449;&#21407;&#22411;&#23849;&#28291;&#20026;&#19968;&#31181;&#31216;&#20026;simplex equiangular tight frame(ETF)&#30340;&#26368;&#20248;&#32467;&#26500;&#12290;&#22522;&#20110;&#36825;&#31181;&#31070;&#32463;&#23849;&#28291;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;FL&#20998;&#31867;&#22120;&#20559;&#24046;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#21512;&#25104;&#21644;&#22266;&#23450;&#30340;ETF&#20998;&#31867;&#22120;&#12290;&#26368;&#20248;&#20998;&#31867;&#22120;&#32467;&#26500;&#20351;&#24471;&#25152;&#26377;&#23458;&#25143;&#31471;&#29978;&#33267;&#22312;&#26497;&#31471;&#24322;&#26500;&#25968;&#25454;&#19979;&#20063;&#33021;&#23398;&#21040;&#32479;&#19968;&#30340;&#21644;&#26368;&#20248;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data heterogeneity is an inherent challenge that hinders the performance of federated learning (FL). Recent studies have identified the biased classifiers of local models as the key bottleneck. Previous attempts have used classifier calibration after FL training, but this approach falls short in improving the poor feature representations caused by training-time classifier biases. Resolving the classifier bias dilemma in FL requires a full understanding of the mechanisms behind the classifier. Recent advances in neural collapse have shown that the classifiers and feature prototypes under perfect training scenarios collapse into an optimal structure called simplex equiangular tight frame (ETF). Building on this neural collapse insight, we propose a solution to the FL's classifier bias problem by utilizing a synthetic and fixed ETF classifier during training. The optimal classifier structure enables all clients to learn unified and optimal feature representations even under extremely hete
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#25554;&#30005;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#30340;MIMO&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#25569;&#25163;&#31574;&#30053;&#21644;&#22810;&#30446;&#26631;&#20989;&#25968;&#20248;&#21270;&#20840;&#23616;&#25511;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#29123;&#26009;&#28040;&#32791;&#12289;SOC&#21464;&#21270;&#21644;&#21151;&#29575;&#38480;&#21046;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09658</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#25554;&#30005;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Energy Management of Multi-mode Plug-in Hybrid Electric Vehicle using Multi-agent Deep Reinforcement Learning. (arXiv:2303.09658v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#25554;&#30005;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#30340;MIMO&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#25569;&#25163;&#31574;&#30053;&#21644;&#22810;&#30446;&#26631;&#20989;&#25968;&#20248;&#21270;&#20840;&#23616;&#25511;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#29123;&#26009;&#28040;&#32791;&#12289;SOC&#21464;&#21270;&#21644;&#21151;&#29575;&#38480;&#21046;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#30340;&#22810;&#27169;&#24335;&#25554;&#30005;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;(PHEV)&#25216;&#26415;&#26159;&#20943;&#23569;&#30899;&#25490;&#25918;&#30340;&#36884;&#24452;&#20043;&#19968;&#65292;&#20854;&#33021;&#37327;&#31649;&#29702;&#38656;&#35201;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;(MIMO)&#25511;&#21046;&#12290;&#30446;&#21069;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;MIMO&#25511;&#21046;&#35299;&#32806;&#20026;&#21333;&#36755;&#20986;(MISO)&#25511;&#21046;&#65292;&#24182;&#19988;&#21482;&#33021;&#23454;&#29616;&#20854;&#23616;&#37096;&#26368;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#20840;&#23616;&#20248;&#21270;&#22810;&#27169;&#24335;&#36710;&#36742;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(MADRL)&#30340;&#22810;&#27169;&#24335;PHEV&#33021;&#37327;&#31649;&#29702;&#30340;MIMO&#25511;&#21046;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#30456;&#20851;&#27604;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25569;&#25163;&#31574;&#30053;&#65292;&#20351;&#24471;&#20004;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;MADRL&#26694;&#26550;&#19979;&#20351;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;(DDPG)&#31639;&#27861;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#24433;&#21709;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#22240;&#32032;&#36827;&#34892;&#28789;&#25935;&#24230;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;DDPG&#26234;&#33021;&#20307;&#30340;&#32479;&#19968;&#35774;&#32622;&#12290;&#25569;&#25163;&#31574;&#30053;&#30340;&#26368;&#20248;&#24037;&#20316;&#27169;&#24335;&#36890;&#36807;&#22810;&#30446;&#26631;&#20989;&#25968;&#24471;&#21040;&#65292;&#32771;&#34385;&#29123;&#26009;&#28040;&#32791;&#12289;&#30005;&#27744;SOC&#21464;&#21270;&#21644;&#21151;&#29575;&#38480;&#21046;&#36829;&#35268;&#12290;&#22522;&#20110;&#30828;&#20214;&#22312;&#29615;(HIL)&#20223;&#30495;&#22120;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MADRL&#33021;&#37327;&#31649;&#29702;&#26041;&#27861;&#22312;&#29123;&#26009;&#28040;&#32791;&#12289;SOC&#21464;&#21270;&#21644;&#21151;&#29575;&#38480;&#21046;&#36829;&#35268;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#21333;&#20010;&#26234;&#33021;&#20307;RL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently emerging multi-mode plug-in hybrid electric vehicle (PHEV) technology is one of the pathways making contributions to decarbonization, and its energy management requires multiple-input and multiple-output (MIMO) control. At the present, the existing methods usually decouple the MIMO control into single-output (MISO) control and can only achieve its local optimal performance. To optimize the multi-mode vehicle globally, this paper studies a MIMO control method for energy management of the multi-mode PHEV based on multi-agent deep reinforcement learning (MADRL). By introducing a relevance ratio, a hand-shaking strategy is proposed to enable two learning agents to work collaboratively under the MADRL framework using the deep deterministic policy gradient (DDPG) algorithm. Unified settings for the DDPG agents are obtained through a sensitivity analysis of the influencing factors to the learning performance. The optimal working mode for the hand-shaking strategy is attained thro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;&#12290;</title><link>http://arxiv.org/abs/2303.00396</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
Controlling Class Layout for Deep Ordinal Classification via Constrained Proxies Learning. (arXiv:2303.00396v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25511;&#21046;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#31867;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20219;&#21153;&#65292;&#23398;&#20064;&#29305;&#23450;&#20110;&#24207;&#25968;&#20998;&#31867;&#30340;&#33391;&#22909;&#32467;&#26500;&#21270;&#29305;&#24449;&#31354;&#38388;&#26377;&#21161;&#20110;&#24688;&#24403;&#22320;&#25429;&#25417;&#31867;&#20043;&#38388;&#30340;&#24207;&#25968;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#20195;&#29702;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20026;&#27599;&#20010;&#24207;&#25968;&#31867;&#23398;&#20064;&#19968;&#20010;&#20195;&#29702;&#65292;&#28982;&#21518;&#36890;&#36807;&#38480;&#21046;&#36825;&#20123;&#20195;&#29702;&#26469;&#35843;&#25972;&#31867;&#30340;&#20840;&#23616;&#24067;&#23616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#30828;&#24067;&#23616;&#32422;&#26463;&#21644;&#36719;&#24067;&#23616;&#32422;&#26463;&#12290;&#30828;&#24067;&#23616;&#32422;&#26463;&#36890;&#36807;&#30452;&#25509;&#25511;&#21046;&#20195;&#29702;&#30340;&#29983;&#25104;&#26469;&#23454;&#29616;&#65292;&#20197;&#24378;&#21046;&#23558;&#20854;&#25918;&#32622;&#22312;&#20005;&#26684;&#30340;&#32447;&#24615;&#24067;&#23616;&#25110;&#21322;&#22278;&#24418;&#24067;&#23616;&#65288;&#21363;&#20005;&#26684;&#24207;&#25968;&#24067;&#23616;&#30340;&#20004;&#31181;&#23454;&#20363;&#65289;&#20013;&#12290;&#36719;&#24067;&#23616;&#32422;&#26463;&#36890;&#36807;&#24341;&#20837;&#27491;&#21017;&#21270;&#39033;&#21040;&#25439;&#22833;&#20989;&#25968;&#20013;&#26469;&#23454;&#29616;&#65292;&#35813;&#39033;&#24809;&#32602;&#20559;&#31163;&#29702;&#24819;&#24207;&#25968;&#24067;&#23616;&#30340;&#24773;&#20917;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;CPL&#26041;&#27861;&#22312;&#28145;&#24230;&#24207;&#25968;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
For deep ordinal classification, learning a well-structured feature space specific to ordinal classification is helpful to properly capture the ordinal nature among classes. Intuitively, when Euclidean distance metric is used, an ideal ordinal layout in feature space would be that the sample clusters are arranged in class order along a straight line in space. However, enforcing samples to conform to a specific layout in the feature space is a challenging problem. To address this problem, in this paper, we propose a novel Constrained Proxies Learning (CPL) method, which can learn a proxy for each ordinal class and then adjusts the global layout of classes by constraining these proxies. Specifically, we propose two kinds of strategies: hard layout constraint and soft layout constraint. The hard layout constraint is realized by directly controlling the generation of proxies to force them to be placed in a strict linear layout or semicircular layout (i.e., two instantiations of strict ordi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GCNs&#30340;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65288;SBAG&#65289;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#33410;&#28857;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#24182;&#27880;&#20837;&#38544;&#34255;&#30340;&#21518;&#38376;&#26469;&#25915;&#20987;GCNs&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.14353</link><description>&lt;p&gt;
&#23545;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A semantic backdoor attack against Graph Convolutional Networks. (arXiv:2302.14353v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GCNs&#30340;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65288;SBAG&#65289;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#20013;&#30340;&#29305;&#23450;&#33410;&#28857;&#20316;&#20026;&#35302;&#21457;&#22120;&#65292;&#24182;&#27880;&#20837;&#38544;&#34255;&#30340;&#21518;&#38376;&#26469;&#25915;&#20987;GCNs&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#22312;&#35299;&#20915;&#21508;&#31181;&#22270;&#32467;&#26500;&#30456;&#20851;&#20219;&#21153;&#65288;&#22914;&#33410;&#28857;&#20998;&#31867;&#21644;&#22270;&#20998;&#31867;&#65289;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;GCNs&#23481;&#26131;&#21463;&#21040;&#19968;&#31181;&#26032;&#22411;&#23041;&#32961;&#65292;&#31216;&#20026;&#21518;&#38376;&#25915;&#20987;&#12290;&#25915;&#20987;&#32773;&#21487;&#20197;&#23558;&#38544;&#34255;&#30340;&#21518;&#38376;&#27880;&#20837;GCNs&#20013;&#65292;&#20351;&#24471;&#25915;&#20987;&#27169;&#22411;&#22312;&#33391;&#24615;&#26679;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#26159;&#22914;&#26524;&#25915;&#20987;&#32773;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#28608;&#27963;&#20102;&#38544;&#34255;&#30340;&#21518;&#38376;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#23558;&#34987;&#24694;&#24847;&#22320;&#20462;&#25913;&#20026;&#25915;&#20987;&#32773;&#25351;&#23450;&#30340;&#30446;&#26631;&#26631;&#31614;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;GCNs&#26159;&#21542;&#23481;&#26131;&#21463;&#21040;&#36825;&#31181;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;GCNs&#30340;&#35821;&#20041;&#21518;&#38376;&#25915;&#20987;&#65288;SBAG&#65289;&#26469;&#25581;&#31034;GCNs&#20013;&#23384;&#22312;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;SBAG&#20351;&#29992;&#26679;&#26412;&#20013;&#30340;&#26576;&#31181;&#33410;&#28857;&#20316;&#20026;&#21518;&#38376;&#35302;&#21457;&#22120;&#65292;&#24182;&#36890;&#36807;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#23558;&#38544;&#34255;&#30340;&#21518;&#38376;&#27880;&#20837;&#21040;GCNs&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCNs) have been very effective in addressing the issue of various graph-structured related tasks, such as node classification and graph classification. However, recent research has shown that GCNs are vulnerable to a new type of threat called the backdoor attack, where the adversary can inject hidden backdoor into the GCNs so that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed to the attacker-specified target label if the hidden backdoor is activated by the attacker-defined trigger. In this paper, we investigate whether such semantic backdoor attacks are possible for GCNs and propose a Semantic Backdoor Attack against GCNs(SBAG) under the context of graph classification to reveal the existence of this security vulnerability in GCNs. The SBAG uses a certain type of node in the samples as a backdoor trigger and injects hidden backdoor into GCNs models through poisoning training data. The backdoor will b
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;</title><link>http://arxiv.org/abs/2302.13268</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#38761;&#26032;&#22522;&#22240;&#32452;&#23398;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Genomics with Reinforcement Learning Techniques. (arXiv:2302.13268v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13268
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#26032;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#22522;&#22240;&#32452;&#23398;&#39046;&#22495;&#20013;&#35299;&#20915;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#38477;&#20302;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36866;&#29992;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#26412;&#35843;&#26597;&#37325;&#28857;&#20851;&#27880;&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#12289;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#20986;&#29616;&#22312;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#20915;&#31574;&#21644;&#22522;&#22240;&#32452;&#23398;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#30340;&#21407;&#22987;&#22522;&#22240;&#32452;&#25968;&#25454;&#25351;&#25968;&#22686;&#38271;&#24050;&#32463;&#36229;&#20986;&#20102;&#25163;&#21160;&#20998;&#26512;&#30340;&#33021;&#21147;&#65292;&#36825;&#23548;&#33268;&#23545;&#33258;&#21160;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#22823;&#12290;RL&#31639;&#27861;&#33021;&#22815;&#22312;&#26368;&#23567;&#30340;&#20154;&#24037;&#30417;&#30563;&#19979;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#22522;&#22240;&#32452;&#25968;&#25454;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;&#20351;&#29992;RL&#30340;&#19968;&#20010;&#20851;&#38190;&#22909;&#22788;&#26159;&#38477;&#20302;&#20102;&#25910;&#38598;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#25104;&#26412;&#65292;&#36825;&#26159;&#30417;&#30563;&#23398;&#20064;&#25152;&#38656;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#22522;&#22240;&#32452;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#26412;&#35843;&#26597;&#20165;&#19987;&#27880;&#20110;&#22312;&#21508;&#31181;&#22522;&#22240;&#32452;&#30740;&#31350;&#39046;&#22495;&#65288;&#21253;&#25324;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#65292;&#22522;&#22240;&#32452;&#32452;&#35013;&#21644;&#24207;&#21015;&#27604;&#23545;&#65289;&#20013;&#20351;&#29992;RL&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30740;&#31350;&#30340;&#25216;&#26415;&#32454;&#33410;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Reinforcement Learning (RL) has emerged as a powerful tool for solving a wide range of problems, including decision-making and genomics. The exponential growth of raw genomic data over the past two decades has exceeded the capacity of manual analysis, leading to a growing interest in automatic data analysis and processing. RL algorithms are capable of learning from experience with minimal human supervision, making them well-suited for genomic data analysis and interpretation. One of the key benefits of using RL is the reduced cost associated with collecting labeled training data, which is required for supervised learning. While there have been numerous studies examining the applications of Machine Learning (ML) in genomics, this survey focuses exclusively on the use of RL in various genomics research fields, including gene regulatory networks (GRNs), genome assembly, and sequence alignment. We present a comprehensive technical overview of existing studies on the applic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#20934;&#30830;&#30340;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;LDL&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#24418;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#36741;&#21161;&#24674;&#22797;&#29702;&#24819;&#30340;&#26631;&#31614;&#20998;&#24067;&#12290;LDL&#26041;&#27861;&#30340;&#20808;&#21069;&#30740;&#31350;&#20551;&#35774;&#35757;&#32451;&#23454;&#20363;&#30340;&#26631;&#31614;&#20998;&#24067;&#26159;&#20934;&#30830;&#30340;&#65292;&#32780;&#29616;&#23454;&#20013;&#30340;&#26631;&#31614;&#20998;&#24067;&#36890;&#24120;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#24182;&#21463;&#21040;&#26631;&#27880;&#38169;&#35823;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2302.13000</link><description>&lt;p&gt;
&#19981;&#20934;&#30830;&#30340;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inaccurate Label Distribution Learning. (arXiv:2302.13000v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#20934;&#30830;&#30340;&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;LDL&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#24418;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#36741;&#21161;&#24674;&#22797;&#29702;&#24819;&#30340;&#26631;&#31614;&#20998;&#24067;&#12290;LDL&#26041;&#27861;&#30340;&#20808;&#21069;&#30740;&#31350;&#20551;&#35774;&#35757;&#32451;&#23454;&#20363;&#30340;&#26631;&#31614;&#20998;&#24067;&#26159;&#20934;&#30830;&#30340;&#65292;&#32780;&#29616;&#23454;&#20013;&#30340;&#26631;&#31614;&#20998;&#24067;&#36890;&#24120;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#24182;&#21463;&#21040;&#26631;&#27880;&#38169;&#35823;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#20998;&#24067;&#23398;&#20064;&#65288;LDL&#65289;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#19968;&#32452;&#26631;&#31614;&#65288;&#31216;&#20026;&#26631;&#31614;&#20998;&#24067;&#65288;LD&#65289;&#65289;&#19982;&#23454;&#20363;&#30340;&#30456;&#20851;&#24615;&#12290;&#20197;&#21069;&#30340;LDL&#26041;&#27861;&#37117;&#20551;&#35774;&#35757;&#32451;&#23454;&#20363;&#30340;LD&#26159;&#20934;&#30830;&#30340;&#12290;&#28982;&#32780;&#65292;&#20026;&#35757;&#32451;&#23454;&#20363;&#27880;&#37322;&#39640;&#24230;&#20934;&#30830;&#30340;LD&#32791;&#26102;&#19988;&#38750;&#24120;&#26114;&#36149;&#65292;&#22312;&#29616;&#23454;&#20013;&#25910;&#38598;&#21040;&#30340;LD&#36890;&#24120;&#26159;&#19981;&#20934;&#30830;&#19988;&#21463;&#27880;&#37322;&#38169;&#35823;&#24178;&#25200;&#30340;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#19981;&#20934;&#30830;&#30340;LDL&#38382;&#39064;&#65292;&#21363;&#24320;&#21457;&#19968;&#20010;&#20855;&#26377;&#22122;&#22768;LD&#30340;LDL&#27169;&#22411;&#12290;&#25105;&#20204;&#20551;&#35774;&#22122;&#22768;LD&#30697;&#38453;&#26159;&#29702;&#24819;LD&#30697;&#38453;&#21644;&#31232;&#30095;&#22122;&#22768;&#30697;&#38453;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#22240;&#27492;&#65292;&#19981;&#20934;&#30830;&#30340;LDL&#38382;&#39064;&#25104;&#20026;&#19968;&#20010;&#36870;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#20174;&#22122;&#22768;LD&#20013;&#24674;&#22797;&#29702;&#24819;&#30340;LD&#21644;&#22122;&#22768;&#30697;&#38453;&#12290;&#25105;&#20204;&#20551;&#35774;&#29702;&#24819;&#30340;LD&#30697;&#38453;&#30001;&#20110;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#20855;&#26377;&#20302;&#31209;&#24615;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#25429;&#25417;&#21040;&#30340;&#23454;&#20363;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#26469;&#24110;&#21161;&#24674;&#22797;&#29702;&#24819;&#30340;LD&#12290;
&lt;/p&gt;
&lt;p&gt;
Label distribution learning (LDL) trains a model to predict the relevance of a set of labels (called label distribution (LD)) to an instance. The previous LDL methods all assumed the LDs of the training instances are accurate. However, annotating highly accurate LDs for training instances is time-consuming and very expensive, and in reality the collected LD is usually inaccurate and disturbed by annotating errors. For the first time, this paper investigates the problem of inaccurate LDL, i.e., developing an LDL model with noisy LDs. We assume that the noisy LD matrix is a linear combination of an ideal LD matrix and a sparse noise matrix. Consequently, the problem of inaccurate LDL becomes an inverse problem, where the objective is to recover the ideal LD and noise matrices from the noisy LDs. We hypothesize that the ideal LD matrix is low-rank due to the correlation of labels and utilize the local geometric structure of instances captured by a graph to assist in recovering the ideal L
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#21363;&#20351;&#22312;&#20002;&#24323;&#25935;&#24863;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20173;&#23384;&#22312;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#26816;&#27979;&#40657;&#30418;&#39044;&#27979;&#22120;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2302.08204</link><description>&lt;p&gt;
&#22312;&#26080;&#24847;&#35782;&#20844;&#24179;&#24615;&#35774;&#32622;&#20013;&#65292;&#23545;&#20559;&#35265;&#35780;&#20272;&#21644;&#26816;&#27979;&#36827;&#34892;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Reasoning for Bias Evaluation and Detection in a Fairness under Unawareness setting. (arXiv:2302.08204v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#21363;&#20351;&#22312;&#20002;&#24323;&#25935;&#24863;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20173;&#23384;&#22312;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#26816;&#27979;&#40657;&#30418;&#39044;&#27979;&#22120;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;AI&#27861;&#35268;&#35201;&#27714;&#22312;&#31639;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#20013;&#20002;&#24323;&#25935;&#24863;&#29305;&#24449;&#65288;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#23447;&#25945;&#65289;&#65292;&#20197;&#38450;&#27490;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#35757;&#32451;&#38598;&#20013;&#27809;&#26377;&#25935;&#24863;&#29305;&#24449;&#65292;&#31639;&#27861;&#20173;&#21487;&#33021;&#25345;&#32493;&#36827;&#34892;&#27495;&#35270;&#12290;&#20107;&#23454;&#19978;&#65292;&#22312;&#25935;&#24863;&#29305;&#24449;&#34987;&#24573;&#30053;&#30340;&#24773;&#20917;&#19979;&#65288;&#26080;&#24847;&#35782;&#20844;&#24179;&#24615;&#65289;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#21487;&#20197;&#36890;&#36807;&#25152;&#35859;&#30340;&#20195;&#29702;&#29305;&#24449;&#25512;&#27979;&#20986;&#36825;&#20123;&#25935;&#24863;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25581;&#31034;&#21363;&#20351;&#22312;&#20002;&#24323;&#25935;&#24863;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20173;&#23384;&#22312;&#30340;&#28508;&#22312;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#65292;&#21487;&#20197;&#25581;&#31034;&#20986;&#40657;&#30418;&#39044;&#27979;&#22120;&#26159;&#21542;&#20173;&#23384;&#22312;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#39044;&#27979;&#22120;&#25552;&#20379;&#36127;&#38754;&#20998;&#31867;&#32467;&#26524;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20026;&#34987;&#27495;&#35270;&#30340;&#29992;&#25143;&#31867;&#21035;&#26500;&#24314;&#21453;&#20107;&#23454;&#31034;&#20363;&#65292;&#20197;&#33719;&#24471;&#27491;&#38754;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#30456;&#21516;&#30340;&#21453;&#20107;&#23454;&#26679;&#26412;&#34987;&#29992;&#20110;&#22806;&#37096;&#20998;&#31867;&#22120;&#65288;&#38024;&#23545;&#25935;&#24863;&#29305;&#24449;&#65289;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#20197;&#35780;&#20272;&#40657;&#30418;&#39044;&#27979;&#22120;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current AI regulations require discarding sensitive features (e.g., gender, race, religion) in the algorithm's decision-making process to prevent unfair outcomes. However, even without sensitive features in the training set, algorithms can persist in discrimination. Indeed, when sensitive features are omitted (fairness under unawareness), they could be inferred through non-linear relations with the so called proxy features. In this work, we propose a way to reveal the potential hidden bias of a machine learning model that can persist even when sensitive features are discarded. This study shows that it is possible to unveil whether the black-box predictor is still biased by exploiting counterfactual reasoning. In detail, when the predictor provides a negative classification outcome, our approach first builds counterfactual examples for a discriminated user category to obtain a positive outcome. Then, the same counterfactual samples feed an external classifier (that targets a sensitive f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#30340;&#31354;&#38388;&#30456;&#20851;&#34928;&#20943;&#24615;&#36136;&#23454;&#29616;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.07938</link><description>&lt;p&gt;
&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-Agent Reinforcement Learning with General Utilities. (arXiv:2302.07938v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24335;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#30340;&#31354;&#38388;&#30456;&#20851;&#34928;&#20943;&#24615;&#36136;&#23454;&#29616;&#20102;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#21487;&#25193;&#23637;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#20854;&#20013;&#36890;&#29992;&#25928;&#29992;&#34987;&#23450;&#20041;&#20026;&#22242;&#38431;&#38271;&#26399;&#29366;&#24577;-&#21160;&#20316;&#21344;&#26377;&#29575;&#27979;&#24230;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#23616;&#37096;&#31574;&#30053;&#65292;&#26368;&#22823;&#21270;&#22242;&#38431;&#23616;&#37096;&#25928;&#29992;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;&#32780;&#19981;&#38656;&#35201;&#23436;&#20840;&#35266;&#27979;&#22242;&#38431;&#20013;&#30340;&#27599;&#20010;&#26234;&#33021;&#20307;&#12290;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#32467;&#26500;&#30340;&#31354;&#38388;&#30456;&#20851;&#34928;&#20943;&#24615;&#36136;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#38452;&#24433;&#22870;&#21169;&#20272;&#35745;&#65292;&#65288;2&#65289;&#25130;&#26029;&#38452;&#24433;Q&#20989;&#25968;&#20272;&#35745;&#65292;&#20197;&#21450;&#65288;3&#65289;&#25130;&#26029;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#21644;&#31574;&#30053;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#25910;&#25947;&#20110;$\epsilon$-&#31283;&#23450;&#24615;&#65292;&#39640;&#27010;&#29575;&#19979;&#38656;&#35201;$\widetilde{\mathcal{O}}(\epsilon^{-2})$&#20010;&#26679;&#26412;&#65292;&#30452;&#21040;&#19968;&#23450;&#31243;&#24230;&#19978;&#30340;&#36817;&#20284;&#35823;&#24046;&#20197;&#25351;&#25968;&#36895;&#24230;&#20943;&#23567;&#21040;&#36890;&#20449;&#21322;&#24452;&#20869;&#12290;&#36825;&#26159;&#20851;&#20110;&#20855;&#26377;&#36890;&#29992;&#25928;&#29992;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#29486;&#20013;&#30340;&#39318;&#20010;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the scalable multi-agent reinforcement learning (MARL) with general utilities, defined as nonlinear functions of the team's long-term state-action occupancy measure. The objective is to find a localized policy that maximizes the average of the team's local utility functions without the full observability of each agent in the team. By exploiting the spatial correlation decay property of the network structure, we propose a scalable distributed policy gradient algorithm with shadow reward and localized policy that consists of three steps: (1) shadow reward estimation, (2) truncated shadow Q-function estimation, and (3) truncated policy gradient estimation and policy update. Our algorithm converges, with high probability, to $\epsilon$-stationarity with $\widetilde{\mathcal{O}}(\epsilon^{-2})$ samples up to some approximation error that decreases exponentially in the communication radius. This is the first result in the literature on multi-agent RL with general utilities that does
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04391</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#26032;&#26631;&#31614;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#39044;&#27979;&#26469;&#36741;&#21161;&#20154;&#31867;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31867;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#23384;&#22312;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#24320;&#21457;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;90&#20998;&#20197;&#19978;&#30340;&#25104;&#32489;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#20986;&#22122;&#22768;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#37319;&#29992;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#20154;&#31867;&#26631;&#35760;&#30340;&#21442;&#32771;&#26469;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#25968;&#25454;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24819;&#27861;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24207;&#21015;&#26631;&#35760;&#12289;&#29289;&#20307;&#26816;&#27979;&#12289;&#24207;&#21015;&#29983;&#25104;&#12289;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GINSEW &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31192;&#23494;&#20449;&#21495;&#27880;&#20837;&#21040;&#27599;&#20010;&#30446;&#26631;&#26631;&#35760;&#30340;&#35299;&#30721;&#27493;&#39588;&#30340;&#27010;&#29575;&#21521;&#37327;&#20013;&#65292;&#20445;&#25252;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#26377;&#25928;&#35782;&#21035;&#20986;&#20405;&#26435;&#34892;&#20026;&#65292;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2302.03162</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24418;&#27700;&#21360;&#20445;&#25252;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Protecting Language Generation Models via Invisible Watermarking. (arXiv:2302.03162v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GINSEW &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31192;&#23494;&#20449;&#21495;&#27880;&#20837;&#21040;&#27599;&#20010;&#30446;&#26631;&#26631;&#35760;&#30340;&#35299;&#30721;&#27493;&#39588;&#30340;&#27010;&#29575;&#21521;&#37327;&#20013;&#65292;&#20445;&#25252;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#26377;&#25928;&#35782;&#21035;&#20986;&#20405;&#26435;&#34892;&#20026;&#65292;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#26159;&#35768;&#22810;&#24212;&#29992;&#30340;&#26377;&#21147;&#25903;&#25345;&#32773;&#12290;&#35768;&#22810;&#36825;&#26679;&#30340;&#27169;&#22411;&#25552;&#20379;&#20813;&#36153;&#25110;&#32463;&#27982;&#23454;&#24800;&#30340; API &#35775;&#38382;&#65292;&#36825;&#20351;&#23427;&#20204;&#21487;&#33021;&#21463;&#21040;&#27169;&#22411;&#25277;&#21462;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#27491;&#20351;&#29992;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#20363;&#22914;&#35789;&#27719;&#27700;&#21360;&#21644;&#21516;&#20041;&#35789;&#26367;&#25442;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#34987;&#26126;&#26174;&#30340;&#23545;&#31574;&#22914;&#8220;&#21516;&#20041;&#35789;&#38543;&#26426;&#21270;&#8221;&#31561;&#25152;&#25269;&#28040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; GINSEW&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#33976;&#39311;&#20445;&#25252;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#31192;&#23494;&#20449;&#21495;&#27880;&#20837;&#21040;&#27599;&#20010;&#30446;&#26631;&#26631;&#35760;&#30340;&#35299;&#30721;&#27493;&#39588;&#30340;&#27010;&#29575;&#21521;&#37327;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25506;&#27979;&#23244;&#30097;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;&#31192;&#23494;&#28040;&#24687;&#26159;&#21542;&#30001;&#21463;&#20445;&#25252;&#30340;&#27169;&#22411;&#33976;&#39311;&#32780;&#26469;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GINSEW &#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20986;&#20405;&#26435;&#34892;&#20026;&#65292;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;&#26497;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as "synonym randomization". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#26469;&#25552;&#21319;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23547;&#25214;&#36830;&#25509;&#19981;&#21516;&#31867;&#21035;&#23376;&#20154;&#21475;&#20998;&#24067;&#30340;&#27979;&#22320;&#32447;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;Wasserstein barycenter&#26469;&#22686;&#21152;&#25968;&#25454;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#20197;&#33719;&#24471;&#26356;&#24179;&#28369;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#23454;&#65292;&#24182;&#25913;&#36827;&#20102;&#22522;&#32447;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#21644;&#32463;&#39564;&#40065;&#26834;&#24615;&#12290;&#35813;&#30740;&#31350;&#20174;Wasserstein&#27979;&#22320;&#32447;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02092</link><description>&lt;p&gt;
&#23545;&#40065;&#26834;&#24615;&#23398;&#20064;&#30340;&#25554;&#20540;&#26041;&#27861;&#65306;&#22522;&#20110;Wasserstein&#27979;&#22320;&#32447;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Interpolation for Robust Learning: Data Augmentation on Wasserstein Geodesics. (arXiv:2302.02092v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#26469;&#25552;&#21319;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23547;&#25214;&#36830;&#25509;&#19981;&#21516;&#31867;&#21035;&#23376;&#20154;&#21475;&#20998;&#24067;&#30340;&#27979;&#22320;&#32447;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;Wasserstein barycenter&#26469;&#22686;&#21152;&#25968;&#25454;&#65292;&#24182;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#20197;&#33719;&#24471;&#26356;&#24179;&#28369;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#23454;&#65292;&#24182;&#25913;&#36827;&#20102;&#22522;&#32447;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#21644;&#32463;&#39564;&#40065;&#26834;&#24615;&#12290;&#35813;&#30740;&#31350;&#20174;Wasserstein&#27979;&#22320;&#32447;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#26469;&#30740;&#31350;&#21644;&#25552;&#21319;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#25214;&#21040;&#36830;&#25509;&#19981;&#21516;&#31867;&#21035;&#23376;&#20154;&#21475;&#20998;&#24067;&#30340;&#27979;&#22320;&#32447;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;Wasserstein barycenter&#26469;&#22686;&#21152;&#25968;&#25454;&#65307;&#25105;&#20204;&#23545;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20351;&#20854;&#22312;&#36830;&#25509;&#23376;&#20154;&#21475;&#20998;&#24067;&#30340;&#36830;&#32493;&#27979;&#22320;&#32447;&#36335;&#24452;&#19978;&#20855;&#26377;&#26356;&#24179;&#28369;&#30340;&#24615;&#33021;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35770;&#20445;&#35777;&#40065;&#26834;&#24615;&#25913;&#36827;&#24182;&#30740;&#31350;&#27979;&#22320;&#32447;&#20301;&#32622;&#21644;&#26679;&#26412;&#22823;&#23567;&#30340;&#36129;&#29486;&#12290;&#22312;&#21253;&#25324;CIFAR-100&#21644;ImageNet&#22312;&#20869;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CIFAR10&#19978;&#25552;&#39640;&#20102;&#22522;&#32447;&#30340;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#36798;&#21040;7.7%&#65292;&#22312;CIFAR-100&#19978;&#25552;&#39640;&#20102;16.8%&#30340;&#32463;&#39564;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;Wasserstein&#27979;&#22320;&#32447;&#25581;&#31034;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to study and promote the robustness of a model as per its performance through the interpolation of training data distributions. Specifically, (1) we augment the data by finding the worst-case Wasserstein barycenter on the geodesic connecting subpopulation distributions of different categories. (2) We regularize the model for smoother performance on the continuous geodesic path connecting subpopulation distributions. (3) Additionally, we provide a theoretical guarantee of robustness improvement and investigate how the geodesic location and the sample size contribute, respectively. Experimental validations of the proposed strategy on \textit{four} datasets, including CIFAR-100 and ImageNet, establish the efficacy of our method, e.g., our method improves the baselines' certifiable robustness on CIFAR10 up to $7.7\%$, with $16.8\%$ on empirical robustness on CIFAR-100. Our work provides a new perspective of model robustness through the lens of Wasserstein geodesic-based interpol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;"&#27169;&#22359;&#21270;&#32487;&#20219;&#29305;&#24449;&#36924;&#36817;&#22120;"&#65288;MSFA&#65289;&#65292;&#36890;&#36807;&#35753;&#27169;&#22359;&#21457;&#29616;&#26377;&#29992;&#20110;&#39044;&#27979;&#30340;&#29305;&#24449;&#24182;&#23398;&#20064;&#33258;&#24049;&#30340;&#39044;&#27979;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.12305</link><description>&lt;p&gt;
&#29992;&#27169;&#22359;&#21270;&#30340;&#32487;&#20219;&#29305;&#24449;&#36924;&#36817;&#22120;&#32452;&#21512;&#20219;&#21153;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Composing Task Knowledge with Modular Successor Feature Approximators. (arXiv:2301.12305v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;"&#27169;&#22359;&#21270;&#32487;&#20219;&#29305;&#24449;&#36924;&#36817;&#22120;"&#65288;MSFA&#65289;&#65292;&#36890;&#36807;&#35753;&#27169;&#22359;&#21457;&#29616;&#26377;&#29992;&#20110;&#39044;&#27979;&#30340;&#29305;&#24449;&#24182;&#23398;&#20064;&#33258;&#24049;&#30340;&#39044;&#27979;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#32487;&#20219;&#29305;&#24449;&#21644;&#24191;&#20041;&#31574;&#30053;&#25913;&#36827;&#65288;SF&amp;GPI&#65289;&#26694;&#26550;&#20316;&#20026;&#23398;&#20064;&#12289;&#32452;&#21512;&#21644;&#36716;&#31227;&#39044;&#27979;&#30693;&#35782;&#21644;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;SF&amp;GPI&#36890;&#36807;&#35753;&#20195;&#29702;&#23398;&#20064;&#33021;&#22815;&#32467;&#21512;GPI&#36827;&#34892;&#20219;&#21153;&#36716;&#31227;&#30340;&#39044;&#27979;&#34920;&#31034;&#65288;SFs&#65289;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26377;&#25928;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#26377;&#29992;&#20110;&#39044;&#27979;&#30340;&#29366;&#24577;&#29305;&#24449;&#65292;&#32780;&#36825;&#20123;&#29366;&#24577;&#29305;&#24449;&#36890;&#24120;&#26159;&#25163;&#24037;&#35774;&#35745;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;"&#27169;&#22359;&#21270;&#32487;&#20219;&#29305;&#24449;&#36924;&#36817;&#22120;"&#65288;MSFA&#65289;&#65292;&#20854;&#20013;&#27169;&#22359;&#26082;&#21487;&#20197;&#21457;&#29616;&#26377;&#29992;&#20110;&#39044;&#27979;&#30340;&#29305;&#24449;&#65292;&#20063;&#21487;&#20197;&#23398;&#20064;&#33258;&#24049;&#30340;&#39044;&#27979;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MSFA&#30456;&#27604;&#22522;&#20934;&#26550;&#26500;&#26469;&#23398;&#20064;SFs&#21644;&#27169;&#22359;&#21270;&#26550;&#26500;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the Successor Features and Generalized Policy Improvement (SF&amp;GPI) framework has been proposed as a method for learning, composing, and transferring predictive knowledge and behavior. SF&amp;GPI works by having an agent learn predictive representations (SFs) that can be combined for transfer to new tasks with GPI. However, to be effective this approach requires state features that are useful to predict, and these state-features are typically hand-designed. In this work, we present a novel neural network architecture, "Modular Successor Feature Approximators" (MSFA), where modules both discover what is useful to predict, and learn their own predictive representations. We show that MSFA is able to better generalize compared to baseline architectures for learning SFs and modular architectures
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;LMPNN&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38598;&#21512;&#25805;&#20316;&#20998;&#35299;&#25104;&#31070;&#32463;&#38598;&#21512;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#65292;&#24182;&#23558;&#23558;&#21407;&#23376;&#20844;&#24335;&#30340;&#23616;&#37096;&#21333;&#36339;&#25512;&#29702;&#36830;&#25509;&#21040;&#20840;&#23616;&#36923;&#36753;&#25512;&#29702;&#20013;&#12290;</title><link>http://arxiv.org/abs/2301.08859</link><description>&lt;p&gt;
&#24102;&#26377;&#21407;&#23376;&#20844;&#24335;&#19968;&#36339;&#25512;&#29702;&#30340;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Logical Message Passing Networks with One-hop Inference on Atomic Formulas. (arXiv:2301.08859v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;LMPNN&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38598;&#21512;&#25805;&#20316;&#20998;&#35299;&#25104;&#31070;&#32463;&#38598;&#21512;&#25805;&#20316;&#31526;&#26469;&#36827;&#34892;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#65292;&#24182;&#23558;&#23558;&#21407;&#23376;&#20844;&#24335;&#30340;&#23616;&#37096;&#21333;&#36339;&#25512;&#29702;&#36830;&#25509;&#21040;&#20840;&#23616;&#36923;&#36753;&#25512;&#29702;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#32771;&#34385;&#21040;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#27169;&#22411;&#36890;&#36807;&#23558;&#38598;&#21512;&#25805;&#20316;&#21442;&#25968;&#21270;&#20026;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#26469;&#22238;&#31572;&#36923;&#36753;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#22823;&#37327;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#23884;&#20837;&#26469;&#35757;&#32451;&#31070;&#32463;&#38598;&#21512;&#25805;&#20316;&#31526;&#65292;&#32780;&#23884;&#20837;&#25110;&#31070;&#32463;&#38598;&#21512;&#25805;&#20316;&#31526;&#22914;&#20309;&#23545;&#24615;&#33021;&#20570;&#20986;&#36129;&#29486;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#26694;&#26550;&#65292;&#20174;&#31070;&#32463;&#38598;&#21512;&#25805;&#20316;&#31526;&#20013;&#20998;&#35299;&#20986;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#12290;&#25105;&#20204;&#23558;&#22797;&#26434;&#26597;&#35810;&#34920;&#31034;&#20026;&#26597;&#35810;&#22270;&#65292;&#24182;&#22312;&#26597;&#35810;&#22270;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#36923;&#36753;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476; (LMPNN)&#65292;&#23558;&#21407;&#23376;&#20844;&#24335;&#30340;&#23616;&#37096;&#21333;&#36339;&#25512;&#29702;&#36830;&#25509;&#21040;&#22797;&#26434;&#26597;&#35810;&#22238;&#31572;&#30340;&#20840;&#23616;&#36923;&#36753;&#25512;&#29702;&#20013;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#26377;&#25928;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex Query Answering (CQA) over Knowledge Graphs (KGs) has attracted a lot of attention to potentially support many applications. Given that KGs are usually incomplete, neural models are proposed to answer the logical queries by parameterizing set operators with complex neural networks. However, such methods usually train neural set operators with a large number of entity and relation embeddings from the zero, where whether and how the embeddings or the neural set operators contribute to the performance remains not clear. In this paper, we propose a simple framework for complex query answering that decomposes the KG embeddings from neural set operators. We propose to represent the complex queries into the query graph. On top of the query graph, we propose the Logical Message Passing Neural Network (LMPNN) that connects the local one-hop inferences on atomic formulas to the global logical reasoning for complex query answering. We leverage existing effective KG embeddings to conduct o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20102;762&#20010;&#24050;&#30830;&#35748;&#30340;&#22806;&#34892;&#26143;&#21644;&#20843;&#20010;&#22826;&#38451;&#31995;&#34892;&#26143;&#30340;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#24040;&#22823;&#34892;&#26143;&#20855;&#26377;&#36739;&#20302;&#30340;&#23494;&#24230;&#65292;&#20027;&#35201;&#30001;&#27682;&#21644;&#27686;&#26500;&#25104;&#65292;&#32780;&#23567;&#34892;&#26143;&#26356;&#23494;&#38598;&#65292;&#20027;&#35201;&#30001;&#26356;&#37325;&#30340;&#20803;&#32032;&#26500;&#25104;&#12290;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#36136;&#37327;&#12289;&#36712;&#36947;&#21608;&#26399;&#21644;&#24658;&#26143;&#37329;&#23646;&#20016;&#24230;&#23545;&#22806;&#34892;&#26143;&#21322;&#24452;&#30340;&#39044;&#27979;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.07143</link><description>&lt;p&gt;
&#37325;&#28201;&#22806;&#34892;&#26143;&#31181;&#32676;&#30340;&#36136;&#37327;-&#21322;&#24452;&#20851;&#31995;&#65306;&#26426;&#22120;&#23398;&#20064;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revisiting mass-radius relationships for exoplanet populations: a machine learning insight. (arXiv:2301.07143v2 [astro-ph.EP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07143
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20102;762&#20010;&#24050;&#30830;&#35748;&#30340;&#22806;&#34892;&#26143;&#21644;&#20843;&#20010;&#22826;&#38451;&#31995;&#34892;&#26143;&#30340;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#24040;&#22823;&#34892;&#26143;&#20855;&#26377;&#36739;&#20302;&#30340;&#23494;&#24230;&#65292;&#20027;&#35201;&#30001;&#27682;&#21644;&#27686;&#26500;&#25104;&#65292;&#32780;&#23567;&#34892;&#26143;&#26356;&#23494;&#38598;&#65292;&#20027;&#35201;&#30001;&#26356;&#37325;&#30340;&#20803;&#32032;&#26500;&#25104;&#12290;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#36136;&#37327;&#12289;&#36712;&#36947;&#21608;&#26399;&#21644;&#24658;&#26143;&#37329;&#23646;&#20016;&#24230;&#23545;&#22806;&#34892;&#26143;&#21322;&#24452;&#30340;&#39044;&#27979;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#34892;&#26143;&#30340;&#21457;&#29616;&#25968;&#37327;&#27491;&#22312;&#22686;&#38271;&#65292;&#24182;&#19988;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#20026;&#25506;&#32034;&#21644;&#29702;&#35299;&#25105;&#20204;&#22826;&#38451;&#31995;&#20197;&#22806;&#30340;&#19990;&#30028;&#30340;&#29305;&#24615;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#20998;&#26512;&#21253;&#25324;762&#20010;&#24050;&#30830;&#35748;&#30340;&#22806;&#34892;&#26143;&#21644;&#20843;&#20010;&#22826;&#38451;&#31995;&#34892;&#26143;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#34920;&#24449;&#23427;&#20204;&#30340;&#22522;&#26412;&#24615;&#36136;&#12290;&#36890;&#36807;&#24212;&#29992;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#20998;&#25104;&#20102;&#20004;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#8216;&#23567;&#8217;&#34892;&#26143;&#21644;&#8216;&#24040;&#22823;&#8217;&#34892;&#26143;&#65292;&#20999;&#21106;&#20540;&#20998;&#21035;&#20026;$R_{p}=8.13R_{\oplus}$&#21644;$M_{p}=52.48M_{\oplus}$&#12290;&#36825;&#31181;&#20998;&#31867;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21306;&#21035;&#65306;&#24040;&#22823;&#34892;&#26143;&#30340;&#23494;&#24230;&#36739;&#20302;&#65292;&#26263;&#31034;&#23427;&#20204;&#20855;&#26377;&#26356;&#39640;&#30340;&#27682;-&#27686;&#36136;&#37327;&#20998;&#25968;&#65292;&#32780;&#23567;&#34892;&#26143;&#26356;&#23494;&#38598;&#65292;&#20027;&#35201;&#30001;&#26356;&#37325;&#30340;&#20803;&#32032;&#32452;&#25104;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#21508;&#31181;&#22238;&#24402;&#27169;&#22411;&#26469;&#25581;&#31034;&#29289;&#29702;&#21442;&#25968;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#34892;&#26143;&#21322;&#24452;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24378;&#35843;&#20102;&#34892;&#26143;&#36136;&#37327;&#12289;&#36712;&#36947; p&#233;riode &#21644;&#24658;&#26143;&#37329;&#23646;&#20016;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing number of exoplanet discoveries and advances in machine learning techniques have opened new avenues for exploring and understanding the characteristics of worlds beyond our Solar System. In this study, we employ efficient machine learning approaches to analyze a dataset comprising 762 confirmed exoplanets and eight Solar System planets, aiming to characterize their fundamental quantities. By applying different unsupervised clustering algorithms, we classify the data into two main classes: 'small' and 'giant' planets, with cut-off values at $R_{p}=8.13R_{\oplus}$ and $M_{p}=52.48M_{\oplus}$. This classification reveals an intriguing distinction: giant planets have lower densities, suggesting higher H-He mass fractions, while small planets are denser, composed mainly of heavier elements. We apply various regression models to uncover correlations between physical parameters and their predictive power for exoplanet radius. Our analysis highlights that planetary mass, orbital pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#39118;&#33021;&#21644;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#30340;&#24066;&#22330;&#21442;&#19982;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#21327;&#35843;&#22797;&#26434;&#24615;&#21644;&#33021;&#28304;&#20215;&#26684;&#38543;&#26426;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#34913;&#29616;&#22330;&#39118;&#30005;&#21066;&#20943;&#21644;&#24066;&#22330;&#25237;&#26631;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2212.13368</link><description>&lt;p&gt;
&#39118;&#33021;&#21644;&#33021;&#37327;&#20648;&#23384;&#21327;&#35843;&#22312;&#25209;&#21457;&#33021;&#28304;&#21644;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#20013;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Wind and Energy Storage Coordination in Wholesale Energy and Ancillary Service Markets. (arXiv:2212.13368v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#39118;&#33021;&#21644;&#33021;&#37327;&#23384;&#20648;&#31995;&#32479;&#30340;&#24066;&#22330;&#21442;&#19982;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#21327;&#35843;&#22797;&#26434;&#24615;&#21644;&#33021;&#28304;&#20215;&#26684;&#38543;&#26426;&#24615;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#34913;&#29616;&#22330;&#39118;&#30005;&#21066;&#20943;&#21644;&#24066;&#22330;&#25237;&#26631;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#33021;&#20316;&#20026;&#20943;&#32531;&#27668;&#20505;&#21464;&#21270;&#30340;&#25163;&#27573;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#39118;&#33021;&#30340;&#21487;&#21464;&#24615;&#23548;&#33268;&#20102;&#39118;&#30005;&#21378;&#30340;&#21066;&#20943;&#65292;&#32473;&#39118;&#30005;&#21378;&#20027;&#24102;&#26469;&#20102;&#21487;&#35266;&#30340;&#32463;&#27982;&#25439;&#22833;&#12290;&#36890;&#36807;&#23558;&#30005;&#27744;&#20648;&#33021;&#31995;&#32479; (BESS) &#29992;&#20316;&#29616;&#22330;&#22791;&#29992;&#28304;&#65292;&#21487;&#20197;&#20943;&#23569;&#39118;&#30005;&#21066;&#20943;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36741;&#21161;&#35282;&#33394;&#21487;&#33021;&#20250;&#20005;&#37325;&#21066;&#24369;BESS&#22312;&#33021;&#28304;&#20132;&#26131;&#20013;&#30340;&#32463;&#27982;&#28508;&#21147;&#12290;&#29702;&#24819;&#30340;BESS&#35843;&#24230;&#24212;&#35813;&#24179;&#34913;&#29616;&#22330;&#39118;&#30005;&#21066;&#20943;&#21644;&#24066;&#22330;&#25237;&#26631;&#65292;&#20294;&#30001;&#20110;&#21327;&#35843;&#22797;&#26434;&#24615;&#21644;&#33021;&#28304;&#20215;&#26684;&#21644;&#39118;&#21457;&#30005;&#30340;&#38543;&#26426;&#24615;&#65292;&#23454;&#38469;&#23454;&#26045;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21516;&#22320;&#39118;&#30005;-&#30005;&#27744;&#31995;&#32479;&#22312;&#29616;&#36135;&#21644;&#35843;&#39057;&#39057;&#29575;&#25511;&#21046;&#36741;&#21161;&#26381;&#21153;&#24066;&#22330;&#20013;&#30340;&#32852;&#21512;&#24066;&#22330;&#25237;&#26631;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#31995;&#32479;&#30340;&#24066;&#22330;&#21442;&#19982;&#20998;&#35299;&#20026;&#27599;&#20010;&#35774;&#26045;&#30340;&#20004;&#20010;&#30456;&#20851;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20351;BESS&#33021;&#22815;&#21560;&#25910;&#29616;&#22330;&#39118;&#33021;
&lt;/p&gt;
&lt;p&gt;
Wind energy has been increasingly adopted to mitigate climate change. However, the variability of wind energy causes wind curtailment, resulting in considerable economic losses for wind farm owners. Wind curtailment can be reduced using battery energy storage systems (BESS) as onsite backup sources. Yet, this auxiliary role may significantly weaken the economic potential of BESS in energy trading. Ideal BESS scheduling should balance onsite wind curtailment reduction and market bidding, but practical implementation is challenging due to coordination complexity and the stochastic nature of energy prices and wind generation. We investigate the joint-market bidding strategy of a co-located wind-battery system in the spot and Regulation Frequency Control Ancillary Service markets. We propose a novel deep reinforcement learning-based approach that decouples the system's market participation into two related Markov decision processes for each facility, enabling the BESS to absorb onsite wind
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#31169;&#26377;&#31574;&#30053;&#21644;&#39044;&#27979;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#65292;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#25104;&#21151;&#36827;&#34892;&#39044;&#27979;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.12561</link><description>&lt;p&gt;
&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An active learning method for solving competitive multi-agent decision-making and control problems. (arXiv:2212.12561v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12561
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31454;&#20105;&#24615;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21644;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#37325;&#26500;&#31169;&#26377;&#31574;&#30053;&#21644;&#39044;&#27979;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#65292;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#25104;&#21151;&#36827;&#34892;&#39044;&#27979;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#37325;&#26500;&#30001;&#30456;&#20114;&#20316;&#29992;&#20195;&#29702;&#20154;&#32676;&#20307;&#25191;&#34892;&#30340;&#31169;&#26377;&#31574;&#30053;&#65292;&#24182;&#39044;&#27979;&#24213;&#23618;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#36807;&#31243;&#30340;&#30830;&#20999;&#32467;&#26524;&#65292;&#36825;&#37324;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#31283;&#23450;&#30340;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#12290;&#25105;&#20204;&#35774;&#24819;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#22312;&#36825;&#20010;&#22330;&#26223;&#20013;&#65292;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#31243;&#24207;&#30340;&#22806;&#37096;&#35266;&#23519;&#32773;&#21487;&#20197;&#36890;&#36807;&#31169;&#26377;&#30340;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#36827;&#34892;&#26597;&#35810;&#21644;&#35266;&#23519;&#20195;&#29702;&#20154;&#30340;&#21453;&#24212;&#65292;&#38598;&#20307;&#30340;&#19981;&#21160;&#28857;&#23545;&#24212;&#20110;&#19968;&#20010;&#31283;&#24577;&#37197;&#32622;&#25991;&#20214;&#12290;&#36890;&#36807;&#36845;&#20195;&#22320;&#25910;&#38598;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#21644;&#26356;&#26032;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30340;&#28176;&#36817;&#24615;&#36136;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#20415;&#22914;&#26524;&#25910;&#25947;&#21457;&#29983;&#65292;&#23427;&#21482;&#33021;&#26397;&#21521;&#19968;&#20010;&#31283;&#24577;&#34892;&#21160;&#37197;&#32622;&#25991;&#20214;&#12290;&#36825;&#19968;&#20107;&#23454;&#23548;&#33268;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#26524;&#65306;i&#65289;&#23398;&#20064;&#23616;&#37096;&#31934;&#30830;&#30340;&#34892;&#21160;-&#21453;&#24212;&#26144;&#23556;&#26367;&#20195;&#29289;&#20351;&#24471;&#22806;&#37096;&#35266;&#23519;&#32773;&#33021;&#22815;&#25104;&#21151;&#23436;&#25104;&#20854;&#39044;&#27979;&#20219;&#21153;&#65292;ii&#65289;&#19982;&#20195;&#29702;&#20154;&#30340;&#20114;&#21160;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#31574;&#30053;&#20197;&#36798;&#21040;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a scheme based on active learning to reconstruct private strategies executed by a population of interacting agents and predict an exact outcome of the underlying multi-agent interaction process, here identified as a stationary action profile. We envision a scenario where an external observer, endowed with a learning procedure, can make queries and observe the agents' reactions through private action-reaction mappings, whose collective fixed point corresponds to a stationary profile. By iteratively collecting sensible data and updating parametric estimates of the action-reaction mappings, we establish sufficient conditions to assess the asymptotic properties of the proposed active learning methodology so that, if convergence happens, it can only be towards a stationary action profile. This fact yields two main consequences: i) learning locally-exact surrogates of the action-reaction mappings allows the external observer to succeed in its prediction task, and ii) working with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23637;&#24320;&#30340;&#21152;&#26435;&#24179;&#22343;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24322;&#36136;&#29615;&#22659;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30452;&#25509;&#23558;&#29615;&#22659;&#30340;&#24322;&#36136;&#24615;&#32435;&#20837;&#32858;&#21512;&#26435;&#37325;&#30340;&#35843;&#25972;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#26410;&#30693;&#31867;&#24179;&#34913;&#25968;&#25454;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.12191</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23637;&#24320;&#30340;&#24322;&#26500;&#29615;&#22659;&#32852;&#37030;&#23398;&#20064;&#21152;&#26435;&#24179;&#22343;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Unfolding-based Weighted Averaging for Federated Learning in Heterogeneous Environments. (arXiv:2212.12191v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23637;&#24320;&#30340;&#21152;&#26435;&#24179;&#22343;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24322;&#36136;&#29615;&#22659;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30452;&#25509;&#23558;&#29615;&#22659;&#30340;&#24322;&#36136;&#24615;&#32435;&#20837;&#32858;&#21512;&#26435;&#37325;&#30340;&#35843;&#25972;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33719;&#24471;&#26356;&#39640;&#30340;&#26410;&#30693;&#31867;&#24179;&#34913;&#25968;&#25454;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26032;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#32858;&#21512;&#26469;&#36845;&#20195;&#27169;&#22411;&#26356;&#26032;&#12290;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#35774;&#22791;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#27492;&#22312;&#26381;&#21153;&#22120;&#30340;&#32858;&#21512;&#38454;&#27573;&#24212;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#20998;&#37197;&#36866;&#24403;&#30340;&#32858;&#21512;&#26435;&#37325;&#12290;&#20026;&#20102;&#35843;&#25972;&#32858;&#21512;&#26435;&#37325;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#28145;&#24230;&#23637;&#24320;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#30340;&#33021;&#21147;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#23558;&#24863;&#20852;&#36259;&#29615;&#22659;&#30340;&#24322;&#36136;&#24615;&#32435;&#20837;&#32858;&#21512;&#26435;&#37325;&#30340;&#35843;&#25972;&#20013;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#21508;&#31181;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#21551;&#21457;&#24335;&#21152;&#26435;&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#26410;&#30693;&#31867;&#24179;&#34913;&#25968;&#25454;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a collaborative model training method that iterates model updates by multiple clients and aggregation of the updates by a central server. Device and statistical heterogeneity of participating clients cause significant performance degradation so that an appropriate aggregation weight should be assigned to each client in the aggregation phase of the server. To adjust the aggregation weights, this paper employs deep unfolding, which is known as the parameter tuning method that leverages both learning capability using training data like deep learning and domain knowledge. This enables us to directly incorporate the heterogeneity of the environment of interest into the tuning of the aggregation weights. The proposed approach can be combined with various federated learning algorithms. The results of numerical experiments indicate that a higher test accuracy for unknown class-balanced data can be obtained with the proposed method than that with conventional heuristic wei
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20449;&#21495;&#21644;&#21452;&#36793;&#20449;&#24687;&#19981;&#23545;&#31216;&#19979;&#30340;&#25215;&#35834;&#38382;&#39064;&#65292;&#22312;&#36125;&#21494;&#26031;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#65292;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#20449;&#21495;&#35013;&#32622;&#65292;&#39046;&#23548;&#32773;&#21487;&#20197;&#21033;&#29992;&#20449;&#24687;&#20248;&#21183;&#33719;&#24471;&#26356;&#39640;&#30340;&#39044;&#26399;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.11446</link><description>&lt;p&gt;
&#20449;&#21495;&#21644;&#21452;&#36793;&#20449;&#24687;&#19981;&#23545;&#31216;&#19979;&#30340;&#25215;&#35834;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Commitment with Signaling under Double-sided Information Asymmetry. (arXiv:2212.11446v3 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20449;&#21495;&#21644;&#21452;&#36793;&#20449;&#24687;&#19981;&#23545;&#31216;&#19979;&#30340;&#25215;&#35834;&#38382;&#39064;&#65292;&#22312;&#36125;&#21494;&#26031;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#65292;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#20449;&#21495;&#35013;&#32622;&#65292;&#39046;&#23548;&#32773;&#21487;&#20197;&#21033;&#29992;&#20449;&#24687;&#20248;&#21183;&#33719;&#24471;&#26356;&#39640;&#30340;&#39044;&#26399;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#20013;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#20351;&#24471;&#20855;&#26377;&#20449;&#24687;&#20248;&#21183;&#30340;&#29609;&#23478;&#21487;&#20197;&#36890;&#36807;&#25112;&#30053;&#24615;&#22320;&#21521;&#20854;&#20182;&#29609;&#23478;&#36879;&#38706;&#20449;&#24687;&#26469;&#25805;&#32437;&#20854;&#20449;&#24565;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#36125;&#21494;&#26031;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#30340;&#21452;&#36793;&#20449;&#24687;&#19981;&#23545;&#31216;&#65292;&#39046;&#23548;&#32773;&#30340;&#23454;&#38469;&#34892;&#21160;&#36890;&#36807;&#28151;&#21512;&#31574;&#30053;&#25215;&#35834;&#20174;&#36861;&#38543;&#32773;&#38544;&#34255;&#36215;&#26469;&#12290;&#30456;&#21453;&#65292;&#36861;&#38543;&#32773;&#25345;&#26377;&#20851;&#20110;&#20854;&#25910;&#30410;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#22312;&#21452;&#26041;&#30340;&#38750;&#23545;&#31216;&#20449;&#24687;&#32473;&#20986;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#39046;&#23548;&#32773;&#30340;&#20449;&#24687;&#20248;&#21183;&#26159;&#21542;&#21387;&#20498;&#36861;&#38543;&#32773;&#30340;&#20449;&#24687;&#20248;&#21183;&#65311;&#26412;&#30740;&#31350;&#32943;&#23450;&#22320;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#36866;&#24403;&#35774;&#35745;&#19968;&#20010;&#20449;&#21495;&#35013;&#32622;&#65292;&#21521;&#36861;&#38543;&#32773;&#36879;&#38706;&#26377;&#20851;&#39046;&#23548;&#32773;&#23454;&#38469;&#34892;&#21160;&#30340;&#37096;&#20998;&#20449;&#24687;&#65292;&#39046;&#23548;&#32773;&#21487;&#20197;&#33719;&#24471;&#27604;&#27809;&#26377;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#26356;&#39640;&#30340;&#39044;&#26399;&#25928;&#29992;&#12290;&#27492;&#22806;&#65292;&#19982;&#20197;&#24448;&#30340;&#36125;&#21494;&#26031;&#26031;&#22612;&#20811;&#23572;&#36125;&#26684;&#21338;&#24328;&#20013;&#21033;&#29992;&#25968;&#23398;&#35268;&#21010;&#24037;&#20855;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#21338;&#24328;&#30340;&#35299;&#37322;&#26159;
&lt;/p&gt;
&lt;p&gt;
Information asymmetry in games enables players with the information advantage to manipulate others' beliefs by strategically revealing information to other players. This work considers a double-sided information asymmetry in a Bayesian Stackelberg game, where the leader's realized action, sampled from the mixed strategy commitment, is hidden from the follower. In contrast, the follower holds private information about his payoff. Given asymmetric information on both sides, an important question arises: \emph{Does the leader's information advantage outweigh the follower's?} We answer this question affirmatively in this work, where we demonstrate that by adequately designing a signaling device that reveals partial information regarding the leader's realized action to the follower, the leader can achieve a higher expected utility than that without signaling. Moreover, unlike previous works on the Bayesian Stackelberg game where mathematical programming tools are utilized, we interpret the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;Lipschitz&#36172;&#24466;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{UniformMesh-N}&#30340;&#31639;&#27861;&#12290;&#20351;&#29992;&#20391;&#38754;&#35266;&#23519;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2212.07524</link><description>&lt;p&gt;
&#19981;&#21464;Lipschitz&#36172;&#24466;&#65306;&#19968;&#20010;&#20391;&#35266;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Invariant Lipschitz Bandits: A Side Observation Approach. (arXiv:2212.07524v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21464;Lipschitz&#36172;&#24466;&#35774;&#32622;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{UniformMesh-N}&#30340;&#31639;&#27861;&#12290;&#20351;&#29992;&#20391;&#38754;&#35266;&#23519;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#20986;&#29616;&#22312;&#35768;&#22810;&#20248;&#21270;&#21644;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#24182;&#21560;&#24341;&#20102;&#20248;&#21270;&#30028;&#30340;&#30456;&#24403;&#20851;&#27880;&#65306;&#36890;&#36807;&#21033;&#29992;&#36825;&#26679;&#30340;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#23547;&#25214;&#26368;&#20248;&#35299;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#23545;&#31216;&#24615;&#22312;&#65288;&#31163;&#32447;&#65289;&#20248;&#21270;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#22312;&#22312;&#32447;&#20248;&#21270;&#35774;&#32622;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36172;&#24466;&#25991;&#29486;&#20013;&#65292;&#20854;&#21033;&#29992;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21464;Lipschitz&#36172;&#24466;&#35774;&#32622;&#65292;&#36825;&#26159;Lipschitz&#36172;&#24466;&#30340;&#19968;&#20010;&#23376;&#31867;&#65292;&#22312;&#35813;&#23376;&#31867;&#20013;&#65292;&#22870;&#21169;&#20989;&#25968;&#21644;&#33218;&#38598;&#22312;&#19968;&#32452;&#21464;&#25442;&#19979;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{UniformMesh-N}&#30340;&#31639;&#27861;&#65292;&#23427;&#33258;&#28982;&#22320;&#23558;&#20391;&#38754;&#35266;&#23519;&#20351;&#29992;&#32676;&#36712;&#36947;&#25972;&#21512;&#21040;\texttt{UniformMesh}&#31639;&#27861;&#65288;\cite{Kleinberg2005_UniformMesh}&#65289;&#20013;&#65292;&#35813;&#31639;&#27861;&#22343;&#21248;&#22320;&#20998;&#21106;&#20102;&#33218;&#30340;&#38598;&#21512;&#12290;&#36890;&#36807;&#20391;&#38754;&#35266;&#23519;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#20854;&#21462;&#20915;&#20110;&#22522;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry arises in many optimization and decision-making problems, and has attracted considerable attention from the optimization community: By utilizing the existence of such symmetries, the process of searching for optimal solutions can be improved significantly. Despite its success in (offline) optimization, the utilization of symmetries has not been well examined within the online optimization settings, especially in the bandit literature. As such, in this paper we study the invariant Lipschitz bandit setting, a subclass of the Lipschitz bandits where the reward function and the set of arms are preserved under a group of transformations. We introduce an algorithm named \texttt{UniformMesh-N}, which naturally integrates side observations using group orbits into the \texttt{UniformMesh} algorithm (\cite{Kleinberg2005_UniformMesh}), which uniformly discretizes the set of arms. Using the side-observation approach, we prove an improved regret upper bound, which depends on the cardinalit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20110;&#21407;&#22987;&#22522;&#31449;LTE&#25968;&#25454;&#36827;&#34892;5G&#22522;&#31449;&#27969;&#37327;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.15220</link><description>&lt;p&gt;
&#38754;&#21521;5G&#22522;&#31449;&#27969;&#37327;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning for 5G Base Station Traffic Forecasting. (arXiv:2211.15220v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20110;&#21407;&#22987;&#22522;&#31449;LTE&#25968;&#25454;&#36827;&#34892;5G&#22522;&#31449;&#27969;&#37327;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#21160;5G&#31227;&#21160;&#32593;&#32476;&#23454;&#29616;&#26234;&#33021;&#39640;&#25928;&#22522;&#30784;&#35774;&#26045;&#35268;&#21010;&#21644;&#31649;&#29702;&#30340;&#36807;&#31243;&#20013;&#65292;&#32454;&#32990;&#27969;&#37327;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#21487;&#29992;&#30340;&#25968;&#25454;&#20165;&#38480;&#20110;&#22522;&#31449;&#26085;&#24535;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#35757;&#32451;&#26041;&#27861;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#21442;&#19982;&#26041;&#30340;&#26032;&#35266;&#27979;&#20013;&#12290;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#20174;&#22810;&#20010;&#22522;&#31449;&#25910;&#38598;&#27979;&#37327;&#25968;&#25454;&#65292;&#23558;&#20854;&#20256;&#36755;&#21040;&#20013;&#22830;&#23454;&#20307;&#65292;&#24182;&#20351;&#29992;&#33719;&#21462;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#25805;&#20316;&#12290;&#26412;&#22320;&#35266;&#27979;&#32467;&#26524;&#30340;&#20256;&#25773;&#24341;&#21457;&#20102;&#20851;&#20110;&#20445;&#23494;&#24615;&#21644;&#24615;&#33021;&#30340;&#25285;&#24551;&#65292;&#36825;&#24433;&#21709;&#20102;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36866;&#29992;&#24615;&#12290;&#23613;&#31649;&#24050;&#25552;&#20986;&#21508;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#22312;&#27969;&#37327;&#39044;&#27979;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#28982;&#24456;&#23569;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#20110;&#21407;&#22987;&#22522;&#31449;LTE&#25968;&#25454;&#36827;&#34892;&#26102;&#24207;&#27969;&#37327;&#39044;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular traffic prediction is of great importance on the path of enabling 5G mobile networks to perform intelligent and efficient infrastructure planning and management. However, available data are limited to base station logging information. Hence, training methods for generating high-quality predictions that can generalize to new observations across diverse parties are in demand. Traditional approaches require collecting measurements from multiple base stations, transmitting them to a central entity and conducting machine learning operations using the acquire data. The dissemination of local observations raises concerns regarding confidentiality and performance, which impede the applicability of machine learning techniques. Although various distributed learning methods have been proposed to address this issue, their application to traffic prediction remains highly unexplored. In this work, we investigate the efficacy of federated learning applied to raw base station LTE data for tim
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#20219;&#21153;&#20013;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#29109;&#65292;&#24182;&#21457;&#29616;&#31574;&#30053;&#20248;&#21270;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20855;&#26377;&#20302;&#29109;&#31574;&#30053;&#65292;&#28982;&#32780;Q&#23398;&#20064;&#26234;&#33021;&#20307;&#23545;&#27492;&#24433;&#21709;&#36739;&#23567;&#65292;&#36890;&#24120;&#20445;&#25345;&#39640;&#29109;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2211.11869</link><description>&lt;p&gt;
&#30740;&#31350;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#22312;&#20010;&#24615;&#21270;&#20219;&#21153;&#20013;&#30340;&#31574;&#30053;&#29109;
&lt;/p&gt;
&lt;p&gt;
Examining Policy Entropy of Reinforcement Learning Agents for Personalization Tasks. (arXiv:2211.11869v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11869
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#20219;&#21153;&#20013;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#29109;&#65292;&#24182;&#21457;&#29616;&#31574;&#30053;&#20248;&#21270;&#26234;&#33021;&#20307;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20855;&#26377;&#20302;&#29109;&#31574;&#30053;&#65292;&#28982;&#32780;Q&#23398;&#20064;&#26234;&#33021;&#20307;&#23545;&#27492;&#24433;&#21709;&#36739;&#23567;&#65292;&#36890;&#24120;&#20445;&#25345;&#39640;&#29109;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#22312;&#20010;&#24615;&#21270;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#35814;&#32454;&#25551;&#36848;&#20102;&#19981;&#21516;&#23398;&#20064;&#31639;&#27861;&#25152;&#20851;&#32852;&#30340;&#31574;&#30053;&#29109;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#31574;&#30053;&#20248;&#21270;&#26234;&#33021;&#20307;&#24448;&#24448;&#20855;&#26377;&#20302;&#29109;&#31574;&#30053;&#65292;&#23454;&#38469;&#19978;&#23548;&#33268;&#26234;&#33021;&#20307;&#20248;&#20808;&#32771;&#34385;&#26576;&#20123;&#21160;&#20316;&#32780;&#36991;&#20813;&#20854;&#20182;&#21160;&#20316;&#12290;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#20063;&#34920;&#26126;&#20102;Q&#23398;&#20064;&#26234;&#33021;&#20307;&#23545;&#36825;&#31181;&#34892;&#20026;&#30340;&#24433;&#21709;&#35201;&#23567;&#24471;&#22810;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#39640;&#29109;&#31574;&#30053;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#26356;&#21487;&#21462;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21508;&#31181;&#25968;&#20540;&#23454;&#39564;&#20197;&#21450;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20197;&#34920;&#26126;&#36825;&#20123;&#29109;&#24046;&#24322;&#26159;&#30001;&#25152;&#37319;&#29992;&#30340;&#23398;&#20064;&#31867;&#22411;&#25152;&#23548;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This effort is focused on examining the behavior of reinforcement learning systems in personalization environments and detailing the differences in policy entropy associated with the type of learning algorithm utilized. We demonstrate that Policy Optimization agents often possess low-entropy policies during training, which in practice results in agents prioritizing certain actions and avoiding others. Conversely, we also show that Q-Learning agents are far less susceptible to such behavior and generally maintain high-entropy policies throughout training, which is often preferable in real-world applications. We provide a wide range of numerical experiments as well as theoretical justification to show that these differences in entropy are due to the type of learning being employed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#24310;&#36831;&#33258;&#36866;&#24212;&#32534;&#30721;&#33033;&#20914;&#26694;&#26550;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#32534;&#30721;&#22120;&#28789;&#27963;&#24615;&#12289;&#24310;&#36831;&#21644;&#33021;&#37327;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#21644;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2211.11760</link><description>&lt;p&gt;
&#19968;&#20010;&#20302;&#24310;&#36831;&#33258;&#36866;&#24212;&#32534;&#30721;&#33033;&#20914;&#26694;&#26550;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Low Latency Adaptive Coding Spiking Framework for Deep Reinforcement Learning. (arXiv:2211.11760v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#24310;&#36831;&#33258;&#36866;&#24212;&#32534;&#30721;&#33033;&#20914;&#26694;&#26550;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#32534;&#30721;&#22120;&#28789;&#27963;&#24615;&#12289;&#24310;&#36831;&#21644;&#33021;&#37327;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#21644;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20302;&#21151;&#32791;&#21644;&#20107;&#20214;&#39537;&#21160;&#29305;&#24615;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#34987;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#22266;&#23450;&#32534;&#30721;&#26041;&#27861;&#23548;&#33268;&#30340;&#33033;&#20914;&#24378;&#21270;&#23398;&#20064;&#65288;SRL&#65289;&#20173;&#28982;&#38754;&#20020;&#39640;&#24310;&#36831;&#21644;&#36739;&#24046;&#30340;&#28789;&#27963;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#30697;&#38453;&#20056;&#27861;&#23545;&#33033;&#20914;&#36827;&#34892;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#25552;&#39640;&#32534;&#30721;&#22120;&#30340;&#28789;&#27963;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#24310;&#36831;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#30452;&#25509;&#35757;&#32451;&#26041;&#27861;&#35757;&#32451;SNNs&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#32467;&#26500;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#25317;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#31639;&#27861;&#21644;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24310;&#36831;&#26497;&#20302;&#65288;&#20165;&#20026;&#20854;&#20182;SRL&#26041;&#27861;&#30340;0.8%&#65289;&#19988;&#20855;&#26377;&#26497;&#39640;&#30340;&#33021;&#37327;&#25928;&#29575;&#65288;&#39640;&#36798;DNNs&#30340;5&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, spiking neural networks (SNNs) have been used in reinforcement learning (RL) due to their low power consumption and event-driven features. However, spiking reinforcement learning (SRL), which suffers from fixed coding methods, still faces the problems of high latency and poor versatility. In this paper, we use learnable matrix multiplication to encode and decode spikes, improving the flexibility of the coders and thus reducing latency. Meanwhile, we train the SNNs using the direct training method and use two different structures for online and offline RL algorithms, which gives our model a wider range of applications. Extensive experiments have revealed that our method achieves optimal performance with ultra-low latency (as low as 0.8% of other SRL methods) and excellent energy efficiency (up to 5X the DNNs) in different algorithms and different environments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;X&#23556;&#32447;&#22270;&#20687;&#19978;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#30142;&#30149;&#20998;&#31867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;RepVGG&#31639;&#27861;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#32954;&#37096;&#30142;&#30149;&#26816;&#27979;&#65292;&#20197;&#23454;&#29616;&#26089;&#26399;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#39044;&#38450;&#36827;&#19968;&#27493;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2211.08244</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;X&#23556;&#32447;&#22270;&#20687;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#30142;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Automatic Detection and Classification Disease on the X-Ray Images. (arXiv:2211.08244v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;X&#23556;&#32447;&#22270;&#20687;&#19978;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#30142;&#30149;&#20998;&#31867;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;RepVGG&#31639;&#27861;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#32954;&#37096;&#30142;&#30149;&#26816;&#27979;&#65292;&#20197;&#23454;&#29616;&#26089;&#26399;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#39044;&#38450;&#36827;&#19968;&#27493;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#21644;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;X&#23556;&#32447;&#22270;&#20687;&#36827;&#34892;&#30142;&#30149;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#26159;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#26680;&#24515;&#20219;&#21153;&#20043;&#19968;&#12290;&#30001;&#20110;&#36817;&#26399;&#23545;&#25918;&#23556;&#23398;&#22270;&#20687;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#39640;&#24230;&#20851;&#27880;&#65292;&#36890;&#36807;X&#23556;&#32447;&#22270;&#20687;&#26089;&#26399;&#26816;&#27979;&#30142;&#30149;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65292;&#20197;&#39044;&#38450;&#36827;&#19968;&#27493;&#20256;&#25773;&#21644;&#21387;&#24179;&#26354;&#32447;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#21644;&#38761;&#21629;&#20026;&#24555;&#36895;&#20934;&#30830;&#22320;&#35786;&#26029;&#31579;&#26597;&#21644;&#26816;&#27979;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#65288;CXR&#65289;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#35757;&#32451;&#31639;&#27861;RepVGG&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#32954;&#30142;&#30149;&#24555;&#36895;&#26816;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;X&#23556;&#32447;&#22270;&#20687;&#20316;&#20026;&#31034;&#20363;&#26469;&#23637;&#31034;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;X&#23556;&#32447;&#22270;&#20687;&#20998;&#20026;Covid-19&#12289;&#32954;&#28814;&#21644;&#27491;&#24120;&#30340;X&#23556;&#32447;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#37319;&#29992;ROI&#23545;&#35937;&#26469;&#25552;&#39640;&#32954;&#37096;&#25552;&#21462;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#38543;&#21518;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#22686;&#24378;&#12290;&#25105;&#20204;&#27491;&#22312;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
Detecting and classifying diseases using X-ray images is one of the more challenging core tasks in the medical and research world. Due to the recent high interest in radiological images and AI, early detection of diseases in X-ray images has become notably more essential to prevent further spreading and flatten the curve. Innovations and revolutions of Computer Vision with Deep learning methods offer great promise for fast and accurate diagnosis of screening and detection from chest X-ray images (CXR). This work presents rapid detection of diseases in the lung using the efficient Deep learning pre-trained RepVGG algorithm for deep feature extraction and classification. We used X-ray images as an example to show the model's efficiency. To perform this task, we classify X-Ray images into Covid-19, Pneumonia, and Normal X-Ray images. Employ ROI object to improve the detection accuracy for lung extraction, followed by data pre-processing and augmentation. We are applying Artificial Intelli
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#23618;QuadConv&#65292;&#36890;&#36807;&#23450;&#31215;&#20998;&#23545;&#36830;&#32493;&#21367;&#31215;&#36827;&#34892;&#36817;&#20284;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38750;&#22343;&#21248;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#21387;&#32553;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#27169;&#25311;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#19982;&#26631;&#20934;&#31163;&#25955;&#21367;&#31215;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#33021;&#22815;&#22312;&#38750;&#22343;&#21248;&#25968;&#25454;&#19978;&#20445;&#25345;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#12290;QuadConv&#36824;&#20248;&#20110;&#20854;&#20182;&#38750;&#32467;&#26500;&#21270;&#21367;&#31215;&#26041;&#27861;&#22914;&#22270;&#21367;&#31215;&#12290;</title><link>http://arxiv.org/abs/2211.05151</link><description>&lt;p&gt;
QuadConv&#65306;&#22522;&#20110;&#23450;&#31215;&#20998;&#30340;&#21367;&#31215;&#19982;&#38750;&#22343;&#21248;PDE&#25968;&#25454;&#21387;&#32553;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
QuadConv: Quadrature-Based Convolutions with Applications to Non-Uniform PDE Data Compression. (arXiv:2211.05151v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#23618;QuadConv&#65292;&#36890;&#36807;&#23450;&#31215;&#20998;&#23545;&#36830;&#32493;&#21367;&#31215;&#36827;&#34892;&#36817;&#20284;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#38750;&#22343;&#21248;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#21387;&#32553;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#27169;&#25311;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#19982;&#26631;&#20934;&#31163;&#25955;&#21367;&#31215;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#29978;&#33267;&#33021;&#22815;&#22312;&#38750;&#22343;&#21248;&#25968;&#25454;&#19978;&#20445;&#25345;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#12290;QuadConv&#36824;&#20248;&#20110;&#20854;&#20182;&#38750;&#32467;&#26500;&#21270;&#21367;&#31215;&#26041;&#27861;&#22914;&#22270;&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#21367;&#31215;&#23618;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;QuadConv&#8212;&#8212;&#36890;&#36807;&#23450;&#31215;&#20998;&#23545;&#36830;&#32493;&#21367;&#31215;&#36827;&#34892;&#36817;&#20284;&#12290;&#25105;&#20204;&#30340;&#25805;&#20316;&#31526;&#19987;&#38376;&#38024;&#23545;&#38750;&#22343;&#21248;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#25968;&#25454;&#36827;&#34892;&#24320;&#21457;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#22312;&#20219;&#24847;&#20301;&#32622;&#36827;&#34892;&#37319;&#26679;&#30340;&#36830;&#32493;&#26680;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25805;&#20316;&#31526;&#30340;&#26500;&#24314;&#36824;&#20801;&#35768;&#39640;&#25928;&#30340;&#23454;&#29616;&#65292;&#25105;&#20204;&#22312;&#25991;&#20013;&#35814;&#32454;&#20171;&#32461;&#24182;&#26500;&#36896;&#20102;&#27492;&#23454;&#29616;&#12290;&#20316;&#20026;&#23545;&#25105;&#20204;&#30340;&#25805;&#20316;&#31526;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20174;&#22266;&#23450;&#32593;&#26684;&#20013;&#21387;&#32553;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#27169;&#25311;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;QuadConv&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;QCAE&#65289;&#19982;&#26631;&#20934;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CAE&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;QuadConv&#33021;&#22815;&#19982;&#26631;&#20934;&#31163;&#25955;&#21367;&#31215;&#22312;&#22343;&#21248;&#32593;&#26684;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;QCAE&#21363;&#20351;&#22312;&#38750;&#22343;&#21248;&#25968;&#25454;&#19978;&#20063;&#33021;&#22815;&#20445;&#25345;&#36825;&#31181;&#20934;&#30830;&#24615;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;QuadConv&#36824;&#20248;&#20110;&#22270;&#21367;&#31215;&#31561;&#20854;&#20182;&#38750;&#32467;&#26500;&#21270;&#21367;&#31215;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new convolution layer for deep learning architectures which we call QuadConv -- an approximation to continuous convolution via quadrature. Our operator is developed explicitly for use on non-uniform, mesh-based data, and accomplishes this by learning a continuous kernel that can be sampled at arbitrary locations. Moreover, the construction of our operator admits an efficient implementation which we detail and construct. As an experimental validation of our operator, we consider the task of compressing partial differential equation (PDE) simulation data from fixed meshes. We show that QuadConv can match the performance of standard discrete convolutions on uniform grid data by comparing a QuadConv autoencoder (QCAE) to a standard convolutional autoencoder (CAE). Further, we show that the QCAE can maintain this accuracy even on non-uniform data. In both cases, QuadConv also outperforms alternative unstructured convolution methods such as graph convolution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#37051;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.64&#30340;&#24179;&#22343;IOU&#65292;&#23613;&#31649;&#23384;&#22312;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2211.00646</link><description>&lt;p&gt;
&#20174;&#30456;&#37051;&#30340;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#23398;&#20064;&#40657;&#33394;&#32032;&#32454;&#32990;&#25513;&#33180;
&lt;/p&gt;
&lt;p&gt;
Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#37051;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.64&#30340;&#24179;&#22343;IOU&#65292;&#23613;&#31649;&#23384;&#22312;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#33394;&#32032;&#30244;&#26159;&#26368;&#20855;&#20405;&#34989;&#24615;&#30340;&#30382;&#32932;&#30284;&#20043;&#19968;&#65292;&#23548;&#33268;&#22823;&#37096;&#20998;&#30382;&#32932;&#30284;&#27515;&#20129;&#12290;&#28982;&#32780;&#65292;&#30149;&#29702;&#23398;&#23478;&#23545;&#40657;&#33394;&#32032;&#30244;&#30340;&#35786;&#26029;&#21487;&#38752;&#24615;&#36739;&#20302;&#12290;&#30001;&#20110;&#40657;&#33394;&#32032;&#30244;&#26159;&#40657;&#33394;&#32032;&#32454;&#32990;&#30340;&#32959;&#30244;&#65292;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#19982;&#30149;&#29702;&#23398;&#23478;&#30340;&#24046;&#24322;&#26080;&#20851;&#24182;&#33021;&#33258;&#21160;&#36827;&#34892;&#20687;&#32032;&#32423;&#27880;&#37322;&#30340;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#30149;&#29702;&#23398;&#23478;&#26631;&#27880;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#37051;&#36817;&#32452;&#32455;&#20999;&#29255;&#19978;&#30340;&#20598;&#32852;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#26579;&#33394;&#29255;&#65292;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#65292;&#34429;&#28982;&#24456;&#38590;&#26377;&#23436;&#32654;&#30340;&#26631;&#31614;&#65292;&#20294;&#36798;&#21040;&#20102;0.64&#30340;&#24179;&#22343;IOU&#12290;
&lt;/p&gt;
&lt;p&gt;
Melanoma is one of the most aggressive forms of skin cancer, causing a large proportion of skin cancer deaths. However, melanoma diagnoses by pathologists shows low interrater reliability. As melanoma is a cancer of the melanocyte, there is a clear need to develop a melanocytic cell segmentation tool that is agnostic to pathologist variability and automates pixel-level annotation. Gigapixel-level pathologist labeling, however, is impractical. Herein, we propose a means to train deep neural networks for melanocytic cell segmentation from hematoxylin and eosin (H&amp;E) stained slides using paired immunohistochemical (IHC) slides of adjacent tissue sections, achieving a mean IOU of 0.64 despite imperfect ground-truth labels.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#36817;&#26399;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#22312;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#37327;&#23376;&#35745;&#31639;&#26426;&#25512;&#29702;&#30340;&#26032;&#22411;&#24207;&#21015;&#32534;&#30721;&#22120;&#27169;&#22411;QNet&#65292;&#20197;&#21450;&#19968;&#20010;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#27169;&#22411;ResQNet&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#32463;&#20856;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#21442;&#25968;&#25968;&#37327;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2210.17262</link><description>&lt;p&gt;
QNet: &#19968;&#31181;&#21407;&#29983;&#20110;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24207;&#21015;&#32534;&#30721;&#22120;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
QNet: A Quantum-native Sequence Encoder Architecture. (arXiv:2210.17262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#36817;&#26399;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#22312;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#37327;&#23376;&#35745;&#31639;&#26426;&#25512;&#29702;&#30340;&#26032;&#22411;&#24207;&#21015;&#32534;&#30721;&#22120;&#27169;&#22411;QNet&#65292;&#20197;&#21450;&#19968;&#20010;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#27169;&#22411;ResQNet&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#32463;&#20856;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#21442;&#25968;&#25968;&#37327;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#37327;&#23376;&#35745;&#31639;&#26426;&#25512;&#29702;&#30340;&#26032;&#22411;&#24207;&#21015;&#32534;&#30721;&#22120;&#27169;&#22411;QNet&#65292;&#20351;&#29992;&#26368;&#23569;&#25968;&#37327;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#28857;&#31215;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(n^2 \cdot d)$&#65292;&#32780;QNet&#30340;&#37327;&#23376;&#30005;&#36335;&#28145;&#24230;&#20165;&#20026;$O(n+d)$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ResQNet&#65292;&#19968;&#20010;&#30001;&#22810;&#20010;&#30001;&#27531;&#24046;&#36830;&#25509;&#30456;&#36830;&#30340;QNet&#27169;&#22359;&#32452;&#25104;&#30340;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#27169;&#22411;&#65292;&#20316;&#20026;&#21516;&#26500;Transformer&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23545;&#25105;&#20204;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#35780;&#20998;&#39044;&#27979;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#32463;&#20856;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#24615;&#33021;&#65292;&#21442;&#25968;&#25968;&#37327;&#23569;&#20102;&#19968;&#21315;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes QNet, a novel sequence encoder model that entirely inferences on the quantum computer using a minimum number of qubits. Let $n$ and $d$ represent the length of the sequence and the embedding size, respectively. The dot-product attention mechanism requires a time complexity of $O(n^2 \cdot d)$, while QNet has merely $O(n+d)$ quantum circuit depth. In addition, we introduce ResQNet, a quantum-classical hybrid model composed of several QNet blocks linked by residual connections, as an isomorph Transformer Encoder. We evaluated our work on various natural language processing tasks, including text classification, rating score prediction, and named entity recognition. Our models exhibit compelling performance over classical state-of-the-art models with a thousand times fewer parameters. In summary, this work investigates the advantage of machine learning on near-term quantum computers in sequential data by experimenting with natural language processing tasks.
&lt;/p&gt;</description></item><item><title>TuneUp&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#35838;&#31243;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#23614;&#33410;&#28857;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.14843</link><description>&lt;p&gt;
TuneUp:&#19968;&#31181;&#31616;&#21333;&#30340;&#25913;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
TuneUp: A Simple Improved Training Strategy for Graph Neural Networks. (arXiv:2210.14843v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14843
&lt;/p&gt;
&lt;p&gt;
TuneUp&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#35838;&#31243;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#25913;&#36827;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#23614;&#33410;&#28857;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#36817;&#26399;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#31574;&#30053;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20256;&#32479;&#30340;&#35757;&#32451;&#31574;&#30053;&#23545;&#21407;&#22987;&#22270;&#20013;&#30340;&#25152;&#26377;&#33410;&#28857;&#36827;&#34892;&#24179;&#31561;&#23398;&#20064;&#65292;&#36825;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#26576;&#20123;&#33410;&#28857;&#24448;&#24448;&#27604;&#20854;&#20182;&#33410;&#28857;&#26356;&#38590;&#23398;&#20064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TuneUp&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#35838;&#31243;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29992;&#20110;&#25552;&#39640;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;TuneUp&#23558;GNN&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;TuneUp&#24212;&#29992;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#33719;&#24471;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#30784;GNN&#12290;&#22522;&#30784;GNN&#22312;&#22836;&#33410;&#28857;&#65288;&#20855;&#26377;&#22823;&#24230;&#25968;&#30340;&#33410;&#28857;&#65289;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#23614;&#33410;&#28857;&#65288;&#20855;&#26377;&#23567;&#24230;&#25968;&#30340;&#33410;&#28857;&#65289;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#22240;&#27492;&#65292;TuneUp&#30340;&#31532;&#20108;&#38454;&#27573;&#20391;&#37325;&#20110;&#36890;&#36807;&#36827;&#19968;&#27493;&#35757;&#32451;&#22522;&#30784;GNN&#20197;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#23614;&#33410;&#28857;&#19978;&#25552;&#39640;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;TuneUp&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22815;&#25913;&#21892;&#23614;&#33410;&#28857;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;TuneUp&#23454;&#29616;&#31616;&#21333;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances in Graph Neural Networks (GNNs), their training strategies remain largely under-explored. The conventional training strategy learns over all nodes in the original graph(s) equally, which can be sub-optimal as certain nodes are often more difficult to learn than others. Here we present TuneUp, a simple curriculum-based training strategy for improving the predictive performance of GNNs. TuneUp trains a GNN in two stages. In the first stage, TuneUp applies conventional training to obtain a strong base GNN. The base GNN tends to perform well on head nodes (nodes with large degrees) but less so on tail nodes (nodes with small degrees). Therefore, the second stage of TuneUp focuses on improving prediction on the difficult tail nodes by further training the base GNN on synthetically generated tail node data. We theoretically analyze TuneUp and show it provably improves generalization performance on tail nodes. TuneUp is simple to implement and applicable to a broad ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;&#65292;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#23398;&#20064;&#20102;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#27979;&#35797;&#38598;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.13533</link><description>&lt;p&gt;
&#20998;&#24067;&#36716;&#31227;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sufficient Invariant Learning for Distribution Shift. (arXiv:2210.13533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;&#65292;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#23398;&#20064;&#20102;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#27979;&#35797;&#38598;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#35777;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36328;&#32452;&#25110;&#39046;&#22495;&#30340;&#19981;&#21464;&#29305;&#24449;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#37096;&#20998;&#22320;&#23398;&#20064;&#20102;&#19981;&#21464;&#29305;&#24449;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#26377;&#38480;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#20294;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#30001;&#20110;&#21482;&#26377;&#35757;&#32451;&#38598;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#23384;&#22312;&#20110;&#20998;&#24067;&#36716;&#31227;&#26102;&#30340;&#27979;&#35797;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#25552;&#39640;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#35748;&#20026;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#20805;&#20998;&#30340;&#19981;&#21464;&#29305;&#24449;&#23545;&#20110;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have shown remarkable performance in diverse applications. However, it is still challenging to guarantee performance in distribution shifts when distributions of training and test datasets are different. There have been several approaches to improve the performance in distribution shift cases by learning invariant features across groups or domains. However, we observe that the previous works only learn invariant features partially. While the prior works focus on the limited invariant features, we first raise the importance of the sufficient invariant features. Since only training sets are given empirically, the learned partial invariant features from training sets might not be present in the test sets under distribution shift. Therefore, the performance improvement on distribution shifts might be limited. In this paper, we argue that learning sufficient invariant features from the training set is crucial for the distribution shift case. Concretely, we newly 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#21487;&#24494;&#32422;&#26463;&#27169;&#20223;&#23398;&#20064;&#19982;&#30828;&#32422;&#26463;&#30340;&#30452;&#25509;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#28436;&#31034;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#20026;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2210.11796</link><description>&lt;p&gt;
&#21487;&#24494;&#32422;&#26463;&#27169;&#20223;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentiable Constrained Imitation Learning for Robot Motion Planning and Control. (arXiv:2210.11796v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#21487;&#24494;&#32422;&#26463;&#27169;&#20223;&#23398;&#20064;&#19982;&#30828;&#32422;&#26463;&#30340;&#30452;&#25509;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#31163;&#32447;&#28436;&#31034;&#20013;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#20026;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#35268;&#21010;&#21644;&#25511;&#21046;&#26159;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#31995;&#32479;&#21160;&#21147;&#23398;&#21644;&#23433;&#20840;&#36793;&#30028;&#65288;&#22914;&#38556;&#30861;&#29289;&#65289;&#31561;&#26102;&#31354;&#38480;&#21046;&#38480;&#21046;&#20102;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#12290;&#26412;&#30740;&#31350;&#23558;&#21487;&#24494;&#32422;&#26463;&#27169;&#20223;&#23398;&#20064;&#19982;&#30828;&#32422;&#26463;&#30340;&#32467;&#21512;&#65292;&#36890;&#36807;&#20174;&#31163;&#32447;&#28436;&#31034;&#20013;&#23398;&#20064;&#20915;&#31574;&#21046;&#23450;&#30340;&#30452;&#25509;&#26041;&#27861;&#65292;&#20026;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#35268;&#21010;&#21644;&#25511;&#21046;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion planning and control are crucial components of robotics applications like automated driving. Here, spatio-temporal hard constraints like system dynamics and safety boundaries (e.g., obstacles) restrict the robot's motions. Direct methods from optimal control solve a constrained optimization problem. However, in many applications finding a proper cost function is inherently difficult because of the weighting of partially conflicting objectives. On the other hand, Imitation Learning (IL) methods such as Behavior Cloning (BC) provide an intuitive framework for learning decision-making from offline demonstrations and constitute a promising avenue for planning and control in complex robot applications. Prior work primarily relied on soft constraint approaches, which use additional auxiliary loss terms describing the constraints. However, catastrophic safety-critical failures might occur in out-of-distribution (OOD) scenarios. This work integrates the flexibility of IL with hard const
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#35270;&#35282;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#33258;&#19978;&#32780;&#19979;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#25512;&#29702;&#35270;&#35282;&#65292;&#20197;&#21450;&#22810;&#31181;&#31561;&#20215;&#30340;&#26041;&#31243;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#26356;&#23436;&#25972;&#30340;&#35821;&#20041;&#21040;&#26041;&#31243;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2210.11694</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#25512;&#29702;&#65306;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#25968;&#23398;&#24212;&#29992;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-View Reasoning: Consistent Contrastive Learning for Math Word Problem. (arXiv:2210.11694v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11694
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#35270;&#35282;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#26469;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#33258;&#19978;&#32780;&#19979;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#25512;&#29702;&#35270;&#35282;&#65292;&#20197;&#21450;&#22810;&#31181;&#31561;&#20215;&#30340;&#26041;&#31243;&#24418;&#24335;&#65292;&#23454;&#29616;&#20102;&#26356;&#23436;&#25972;&#30340;&#35821;&#20041;&#21040;&#26041;&#31243;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#24212;&#29992;&#38382;&#39064;&#27714;&#35299;&#22120;&#38656;&#35201;&#23545;&#25991;&#26412;&#20013;&#30340;&#25968;&#37327;&#36827;&#34892;&#31934;&#30830;&#30340;&#20851;&#31995;&#25512;&#29702;&#21644;&#21487;&#38752;&#30340;&#26041;&#31243;&#29983;&#25104;&#12290;&#24403;&#21069;&#30340;&#24207;&#21015;&#21040;&#26641;&#25110;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#21482;&#20174;&#19968;&#20010;&#22266;&#23450;&#35270;&#35282;&#30475;&#24453;&#36825;&#20010;&#38382;&#39064;&#65292;&#24456;&#38590;&#21516;&#26102;&#22788;&#29702;&#22797;&#26434;&#30340;&#35821;&#20041;&#21644;&#22810;&#26679;&#30340;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#35299;&#39064;&#33258;&#28982;&#22320;&#28041;&#21450;&#20004;&#31181;&#19968;&#33268;&#30340;&#25512;&#29702;&#35270;&#35282;&#65306;&#33258;&#19978;&#32780;&#19979;&#21644;&#33258;&#19979;&#32780;&#19978;&#65292;&#23601;&#20687;&#25968;&#23398;&#26041;&#31243;&#20063;&#21487;&#20197;&#29992;&#22810;&#31181;&#31561;&#20215;&#24418;&#24335;&#34920;&#31034;&#65306;&#21069;&#24207;&#21644;&#21518;&#24207;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#29992;&#20110;&#26356;&#23436;&#25972;&#30340;&#35821;&#20041;&#21040;&#26041;&#31243;&#30340;&#26144;&#23556;&#12290;&#25972;&#20010;&#36807;&#31243;&#34987;&#20998;&#35299;&#20026;&#20004;&#20010;&#29420;&#31435;&#20294;&#19968;&#33268;&#30340;&#35270;&#35282;&#65306;&#33258;&#19978;&#32780;&#19979;&#30340;&#20998;&#35299;&#21644;&#33258;&#19979;&#32780;&#19978;&#30340;&#26500;&#24314;&#65292;&#24182;&#19988;&#20004;&#31181;&#25512;&#29702;&#35270;&#35282;&#22312;&#22810;&#31890;&#24230;&#19978;&#23545;&#40784;&#20197;&#20445;&#25345;&#19968;&#33268;&#24615;&#65292;&#22686;&#24378;&#20840;&#23616;&#29983;&#25104;&#21644;&#31934;&#30830;&#25512;&#29702;&#12290;&#22312;&#20004;&#31181;&#35821;&#35328;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Math word problem solver requires both precise relation reasoning about quantities in the text and reliable generation for the diverse equation. Current sequence-to-tree or relation extraction methods regard this only from a fixed view, struggling to simultaneously handle complex semantics and diverse equations. However, human solving naturally involves two consistent reasoning views: top-down and bottom-up, just as math equations also can be expressed in multiple equivalent forms: pre-order and post-order. We propose a multi-view consistent contrastive learning for a more complete semantics-to-equation mapping. The entire process is decoupled into two independent but consistent views: top-down decomposition and bottom-up construction, and the two reasoning views are aligned in multi-granularity for consistency, enhancing global generation and precise reasoning. Experiments on multiple datasets across two languages show our approach significantly outperforms the existing baselines, esp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;MD&#27169;&#25311;&#22522;&#20934;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#30446;&#21069;&#24120;&#29992;&#30340;&#21147;&#31934;&#24230;&#27979;&#35797;&#19982;&#27169;&#25311;&#32467;&#26524;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2210.07237</link><description>&lt;p&gt;
&#21147;&#37327;&#24182;&#19981;&#36275;&#22815;&#65306;&#29992;&#20998;&#23376;&#27169;&#25311;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#20851;&#38190;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Forces are not Enough: Benchmark and Critical Evaluation for Machine Learning Force Fields with Molecular Simulations. (arXiv:2210.07237v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07237
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;MD&#27169;&#25311;&#22522;&#20934;&#22871;&#20214;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#21147;&#22330;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#30446;&#21069;&#24120;&#29992;&#30340;&#21147;&#31934;&#24230;&#27979;&#35797;&#19982;&#27169;&#25311;&#32467;&#26524;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#31185;&#23398;&#24212;&#29992;&#20013;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21147;&#22330;&#65288;FF&#65289;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24320;&#22987;&#21462;&#20195;&#20174;&#21407;&#23376;&#32467;&#26500;&#30452;&#25509;&#39044;&#27979;&#21147;&#30340;&#20174;&#22836;&#31639;&#27861;&#27169;&#25311;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#20027;&#35201;&#36890;&#36807;&#21147;/&#33021;&#39044;&#27979;&#35823;&#24046;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#23613;&#31649;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#26159;&#20135;&#29983;&#36924;&#30495;&#30340;MD&#36712;&#36857;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23398;&#20064;MD&#27169;&#25311;&#22522;&#20934;&#22871;&#20214;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#20195;&#34920;&#24615;&#30340;MD&#31995;&#32479;&#65292;&#21253;&#25324;&#27700;&#65292;&#26377;&#26426;&#20998;&#23376;&#65292;&#32957;&#21644;&#26448;&#26009;&#65292;&#24182;&#35774;&#35745;&#19982;&#21508;&#20010;&#31995;&#32479;&#30340;&#31185;&#23398;&#30446;&#26631;&#30456;&#23545;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#26368;&#26032;&#30340;ML FF&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#38416;&#26126;&#20102;&#36890;&#24120;&#34987;&#22522;&#20934;&#27979;&#35797;&#30340;&#21147;&#31934;&#24230;&#19982;&#30456;&#20851;&#27169;&#25311;&#25351;&#26631;&#19981;&#19968;&#33268;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36873;&#25321;&#30340;SOTA&#26041;&#27861;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#22833;&#36133;&#65292;
&lt;/p&gt;
&lt;p&gt;
Molecular dynamics (MD) simulation techniques are widely used for various natural science applications. Increasingly, machine learning (ML) force field (FF) models begin to replace ab-initio simulations by predicting forces directly from atomic structures. Despite significant progress in this area, such techniques are primarily benchmarked by their force/energy prediction errors, even though the practical use case would be to produce realistic MD trajectories. We aim to fill this gap by introducing a novel benchmark suite for learned MD simulation. We curate representative MD systems, including water, organic molecules, a peptide, and materials, and design evaluation metrics corresponding to the scientific objectives of respective systems. We benchmark a collection of state-of-the-art (SOTA) ML FF models and illustrate, in particular, how the commonly benchmarked force accuracy is not well aligned with relevant simulation metrics. We demonstrate when and how selected SOTA methods fail,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27491;&#24335;&#21270;&#20102;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#30340;&#26368;&#20339;&#36873;&#25321;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;BAE&#65289;&#65292;BAE&#33021;&#22815;&#23558;&#25968;&#25454;&#25237;&#23556;&#21040;&#26368;&#20248;&#30340;&#27969;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#21387;&#32553;&#21644;&#26356;&#21152;&#31283;&#23450;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2210.00637</link><description>&lt;p&gt;
&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Benign Autoencoders. (arXiv:2210.00637v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27491;&#24335;&#21270;&#20102;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#30340;&#26368;&#20339;&#36873;&#25321;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;BAE&#65289;&#65292;BAE&#33021;&#22815;&#23558;&#25968;&#25454;&#25237;&#23556;&#21040;&#26368;&#20248;&#30340;&#27969;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#21387;&#32553;&#21644;&#26356;&#21152;&#31283;&#23450;&#30340;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#24456;&#22810;&#36827;&#23637;&#65292;&#20854;&#20013;&#24120;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#23454;&#29616;&#25968;&#25454;&#30340;&#39640;&#25928;&#34920;&#31034;&#12290;&#26412;&#35770;&#25991;&#27491;&#24335;&#21270;&#20102;&#23547;&#25214;&#26368;&#20339;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#23545;&#30340;&#25968;&#23398;&#38382;&#39064;&#24182;&#34920;&#24449;&#20854;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#8220;&#33391;&#24615;&#33258;&#32534;&#30721;&#22120;&#8221;&#65288;BAE&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;BAE&#23558;&#25968;&#25454;&#25237;&#23556;&#21040;&#19968;&#20010;&#27969;&#22411;&#19978;&#65292;&#20854;&#32500;&#25968;&#20026;&#29983;&#25104;&#38382;&#39064;&#30340;&#26368;&#20339;&#21487;&#21387;&#32553;&#32500;&#24230;&#12290;&#25105;&#20204;&#24378;&#35843;BAE&#19982;&#20154;&#24037;&#26234;&#33021;&#20013;&#20960;&#20010;&#26368;&#36817;&#21457;&#23637;&#30340;&#26041;&#21521;&#20043;&#38388;&#30340;&#24778;&#20154;&#32852;&#31995;&#65292;&#22914;&#26377;&#26465;&#20214;&#30340;GAN&#65292;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#65292;&#31283;&#23450;&#25193;&#25955;&#65292;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BAE&#22914;&#20309;&#25214;&#21040;&#26368;&#20248;&#30340;&#20302;&#32500;&#28508;&#22312;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#25552;&#39640;&#37492;&#21035;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21387;&#32553;&#8220;&#24694;&#24615;&#8221;&#25968;&#25454;&#32500;&#24230;&#65292;BAE&#23548;&#33268;&#26799;&#24230;&#26356;&#21152;&#24179;&#28369;&#21644;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in Generative Artificial Intelligence (AI) relies on efficient data representations, often featuring encoder-decoder architectures. We formalize the mathematical problem of finding the optimal encoder-decoder pair and characterize its solution, which we name the "benign autoencoder" (BAE). We prove that BAE projects data onto a manifold whose dimension is the optimal compressibility dimension of the generative problem. We highlight surprising connections between BAE and several recent developments in AI, such as conditional GANs, context encoders, stable diffusion, stacked autoencoders, and the learning capabilities of generative models. As an illustration, we show how BAE can find optimal, low-dimensional latent representations that improve the performance of a discriminator under a distribution shift. By compressing "malignant" data dimensions, BAE leads to smoother and more stable gradients.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#32467;&#21512;&#35757;&#32451;&#38454;&#27573;&#33719;&#21462;&#30340;&#22320;&#22270;&#20449;&#24687;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36991;&#20813;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#24182;&#20351;&#29992;SAC&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2209.14271</link><description>&lt;p&gt;
&#22522;&#20110;&#22870;&#21169;&#22609;&#36896;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generalization in Deep Reinforcement Learning for Robotic Navigation by Reward Shaping. (arXiv:2209.14271v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#26426;&#22120;&#20154;&#23548;&#33322;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#32467;&#21512;&#35757;&#32451;&#38454;&#27573;&#33719;&#21462;&#30340;&#22320;&#22270;&#20449;&#24687;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36991;&#20813;&#23616;&#37096;&#26497;&#23567;&#20540;&#65292;&#24182;&#20351;&#29992;SAC&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#23616;&#37096;&#23548;&#33322;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#22312;&#26410;&#30693;&#19988;&#26434;&#20081;&#30340;&#24037;&#20316;&#21306;&#22495;&#20013;&#65292;&#21482;&#37197;&#22791;&#26377;&#21463;&#38480;&#33539;&#22260;&#22806;&#37096;&#24863;&#30693;&#20256;&#24863;&#22120;&#65288;&#22914;LiDAR&#65289;&#65292;&#21521;&#30446;&#26631;&#20301;&#32622;&#31227;&#21160;&#12290;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30896;&#25758;&#36991;&#20813;&#31574;&#30053;&#20855;&#26377;&#19968;&#20123;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#23545;&#23616;&#37096;&#26497;&#23567;&#20540;&#38750;&#24120;&#25935;&#24863;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#22312;&#20256;&#24863;&#22120;&#33539;&#22260;&#20869;&#23398;&#20064;&#36866;&#24403;&#30340;&#21160;&#20316;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#26426;&#22120;&#20154;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#25191;&#34892;&#20219;&#21153;&#65292;&#23547;&#27714;&#33021;&#22815;&#36991;&#20813;&#23616;&#37096;&#26497;&#23567;&#20540;&#30340;&#27867;&#21270;&#23616;&#37096;&#23548;&#33322;&#31574;&#30053;&#23545;&#25105;&#20204;&#26469;&#35828;&#38750;&#24120;&#26377;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#35757;&#32451;&#22330;&#26223;&#20013;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#34701;&#21512;&#20102;&#22312;&#35757;&#32451;&#38454;&#27573;&#33719;&#21462;&#30340;&#22320;&#22270;&#20449;&#24687;&#65292;&#22686;&#21152;&#20102;Agent&#30340;&#35880;&#24910;&#32771;&#34385;&#26368;&#20339;&#34892;&#21160;&#26041;&#26696;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;SAC&#31639;&#27861;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;ANN&#65292;&#35813;&#31639;&#27861;&#22312;&#26368;&#26032;&#30340;&#25991;&#29486;&#20013;&#26174;&#31034;&#20986;&#27604;&#20854;&#20182;&#31639;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the application of DRL algorithms in the context of local navigation problems, in which a robot moves towards a goal location in unknown and cluttered workspaces equipped only with limited-range exteroceptive sensors, such as LiDAR. Collision avoidance policies based on DRL present some advantages, but they are quite susceptible to local minima, once their capacity to learn suitable actions is limited to the sensor range. Since most robots perform tasks in unstructured environments, it is of great interest to seek generalized local navigation policies capable of avoiding local minima, especially in untrained scenarios. To do so, we propose a novel reward function that incorporates map information gained in the training stage, increasing the agent's capacity to deliberate about the best course of action. Also, we use the SAC algorithm for training our ANN, which shows to be more effective than others in the state-of-the-art literature. A set of sim-to-sim and sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#23545;&#38750;&#26377;&#30446;&#26631;&#25968;&#25454;&#27745;&#26579;&#30340;&#40065;&#26834;&#24615;</title><link>http://arxiv.org/abs/2209.14013</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#23545;&#38750;&#26377;&#30446;&#26631;&#25968;&#25454;&#27745;&#26579;&#30340;&#40065;&#26834;&#24615;&#65306;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of Random Forest Against Untargeted Data Poisoning: An Ensemble-Based Approach. (arXiv:2209.14013v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#23545;&#38750;&#26377;&#30446;&#26631;&#25968;&#25454;&#27745;&#26579;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27491;&#22312;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#20174;&#37329;&#34701;&#21040;&#21307;&#23398;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27491;&#22312;&#25552;&#39640;&#20915;&#31574;&#36807;&#31243;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#22411;&#21644;&#30456;&#24212;&#39044;&#27979;&#30340;&#23433;&#20840;&#24615;&#26041;&#38754;&#65292;&#30456;&#23545;&#39044;&#27979;&#36136;&#37327;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#30340;&#24773;&#20917;&#24182;&#27809;&#26377;&#30456;&#24212;&#30340;&#24471;&#21040;&#20445;&#35777;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#23545;&#27745;&#26579;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;
&lt;/p&gt;
&lt;p&gt;
Machine learning is becoming ubiquitous. From finance to medicine, machine learning models are boosting decision-making processes and even outperforming humans in some tasks. This huge progress in terms of prediction quality does not however find a counterpart in the security of such models and corresponding predictions, where perturbations of fractions of the training set (poisoning) can seriously undermine the model accuracy. Research on poisoning attacks and defenses received increasing attention in the last decade, leading to several promising solutions aiming to increase the robustness of machine learning. Among them, ensemble-based defenses, where different models are trained on portions of the training set and their predictions are then aggregated, provide strong theoretical guarantees at the price of a linear overhead. Surprisingly, ensemble-based defenses, which do not pose any restrictions on the base model, have not been applied to increase the robustness of random forest mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20027;&#21160;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550; LADA&#65292;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#19981;&#19968;&#33268;&#24615;&#26631;&#20934;&#36873;&#25321;&#20449;&#24687;&#37327;&#26356;&#20016;&#23500;&#30340;&#30446;&#26631;&#26679;&#26412;&#65292;&#24182;&#20197;&#31867;&#24179;&#34913;&#30340;&#26041;&#24335;&#36880;&#27493;&#22686;&#21152;&#26631;&#27880;&#30340;&#30446;&#26631;&#25968;&#25454;&#19982;&#33258;&#20449;&#30340;&#37051;&#23621;&#12290;&#23454;&#39564;&#35777;&#23454; LADA &#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#26368;&#36817;&#30340; ADA &#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.12856</link><description>&lt;p&gt;
&#23616;&#37096;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20027;&#21160;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Local Context-Aware Active Domain Adaptation. (arXiv:2208.12856v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20027;&#21160;&#22495;&#33258;&#36866;&#24212;&#26694;&#26550; LADA&#65292;&#36890;&#36807;&#24341;&#20837;&#23616;&#37096;&#19981;&#19968;&#33268;&#24615;&#26631;&#20934;&#36873;&#25321;&#20449;&#24687;&#37327;&#26356;&#20016;&#23500;&#30340;&#30446;&#26631;&#26679;&#26412;&#65292;&#24182;&#20197;&#31867;&#24179;&#34913;&#30340;&#26041;&#24335;&#36880;&#27493;&#22686;&#21152;&#26631;&#27880;&#30340;&#30446;&#26631;&#25968;&#25454;&#19982;&#33258;&#20449;&#30340;&#37051;&#23621;&#12290;&#23454;&#39564;&#35777;&#23454; LADA &#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#26368;&#36817;&#30340; ADA &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#22495;&#33258;&#36866;&#24212;&#65288;ADA&#65289;&#36890;&#36807;&#26597;&#35810;&#23569;&#37327;&#36873;&#25321;&#30340;&#30446;&#26631;&#26679;&#26412;&#30340;&#26631;&#31614;&#26469;&#24110;&#21161;&#23558;&#27169;&#22411;&#20174;&#28304;&#22495;&#36866;&#24212;&#21040;&#30446;&#26631;&#22495;&#12290;&#24403;&#23384;&#22312;&#36739;&#22823;&#30340;&#22495;&#24046;&#36317;&#26102;&#65292;&#26597;&#35810;&#25968;&#25454;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ADA&#26041;&#27861;&#24182;&#27809;&#26377;&#20805;&#20998;&#25506;&#32034;&#36825;&#19968;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LADA&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;ADA&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#36873;&#25321;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#30446;&#26631;&#26679;&#26412;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#23616;&#37096;&#19981;&#19968;&#33268;&#24615;&#30340;&#26032;&#26631;&#20934;&#12290;&#30001;&#20110;&#26631;&#27880;&#39044;&#31639;&#36890;&#24120;&#24456;&#23567;&#65292;&#20165;&#22312;&#26597;&#35810;&#25968;&#25454;&#19978;&#24494;&#35843;&#27169;&#22411;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#20197;&#31867;&#24179;&#34913;&#30340;&#26041;&#24335;&#36880;&#27493;&#22686;&#21152;&#26631;&#27880;&#30340;&#30446;&#26631;&#25968;&#25454;&#19982;&#33258;&#20449;&#30340;&#37051;&#23621;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25152;&#25552;&#20986;&#30340;&#26631;&#20934;&#36873;&#25321;&#30340;&#30446;&#26631;&#26679;&#26412;&#27604;&#29616;&#26377;&#30340;&#20027;&#21160;&#36873;&#25321;&#31574;&#30053;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23436;&#25972;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#26368;&#36817;&#30340;ADA&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/tsu&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Domain Adaptation (ADA) queries the labels of a small number of selected target samples to help adapting a model from a source domain to a target domain. The local context of queried data is important, especially when the domain gap is large. However, this has not been fully explored by existing ADA works. In this paper, we propose a Local context-aware ADA framework, named LADA, to address this issue. To select informative target samples, we devise a novel criterion based on the local inconsistency of model predictions. Since the labeling budget is usually small, fine-tuning model on only queried data can be inefficient. We progressively augment labeled target data with the confident neighbors in a class-balanced manner. Experiments validate that the proposed criterion chooses more informative target samples than existing active selection strategies. Furthermore, our full method clearly surpasses recent ADA arts on various benchmarks. Code is available at https://github.com/tsu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31574;&#30053;&#34920;&#31034;&#20026;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Diffusion Q-learning&#65288;Diffusion-QL&#65289;&#65292;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#34920;&#31034;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#21160;&#20316;&#20540;&#26469;&#23547;&#27714;&#25509;&#36817;&#34892;&#20026;&#31574;&#30053;&#30340;&#26368;&#20248;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2208.06193</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#31574;&#30053;&#20316;&#20026;&#34920;&#36798;&#24615;&#31574;&#30053;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning. (arXiv:2208.06193v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31574;&#30053;&#34920;&#31034;&#20026;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Diffusion Q-learning&#65288;Diffusion-QL&#65289;&#65292;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#34920;&#31034;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#21160;&#20316;&#20540;&#26469;&#23547;&#27714;&#25509;&#36817;&#34892;&#20026;&#31574;&#30053;&#30340;&#26368;&#20248;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26159;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#25910;&#38598;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#30340;&#37325;&#35201;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#12290;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#22312;&#20998;&#24067;&#19981;&#21305;&#37197;&#30340;&#34892;&#20026;&#19978;&#23384;&#22312;&#20989;&#25968;&#36924;&#36817;&#35823;&#24046;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#21463;&#38480;&#20110;&#20855;&#26377;&#26377;&#38480;&#34920;&#36798;&#33021;&#21147;&#30340;&#31574;&#30053;&#31867;&#65292;&#21487;&#33021;&#23548;&#33268;&#39640;&#24230;&#27425;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#31574;&#30053;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#36817;&#26399;&#20986;&#29616;&#30340;&#39640;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#31867;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25193;&#25955;Q-learning&#65288;Diffusion-QL&#65289;&#65292;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#34920;&#31034;&#31574;&#30053;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#21160;&#20316;&#20540;&#20989;&#25968;&#65292;&#24182;&#23558;&#26368;&#22823;&#21270;&#21160;&#20316;&#20540;&#30340;&#39033;&#21152;&#20837;&#21040;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25439;&#22833;&#20013;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#23547;&#27714;&#25509;&#36817;&#34892;&#20026;&#31574;&#30053;&#30340;&#26368;&#20248;&#21160;&#20316;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#25955;&#31574;&#30053;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL), which aims to learn an optimal policy using a previously collected static dataset, is an important paradigm of RL. Standard RL methods often perform poorly in this regime due to the function approximation errors on out-of-distribution actions. While a variety of regularization methods have been proposed to mitigate this issue, they are often constrained by policy classes with limited expressiveness that can lead to highly suboptimal solutions. In this paper, we propose representing the policy as a diffusion model, a recent class of highly-expressive deep generative models. We introduce Diffusion Q-learning (Diffusion-QL) that utilizes a conditional diffusion model to represent the policy. In our approach, we learn an action-value function and we add a term maximizing action-values into the training loss of the conditional diffusion model, which results in a loss that seeks optimal actions that are near the behavior policy. We show the expressiveness
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MDLatLRRv2&#30340;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22810;&#32423;&#20998;&#35299;&#26041;&#27861;&#24182;&#20805;&#20998;&#21033;&#29992;LatLRR&#25552;&#21462;&#30340;&#21508;&#31181;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#30340;&#26368;&#20808;&#36827;&#34701;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.15179</link><description>&lt;p&gt;
&#22522;&#20110;MDLatLRRv2&#30340;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Medical Image Fusion Method based on MDLatLRRv2. (arXiv:2206.15179v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15179
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MDLatLRRv2&#30340;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22810;&#32423;&#20998;&#35299;&#26041;&#27861;&#24182;&#20805;&#20998;&#21033;&#29992;LatLRR&#25552;&#21462;&#30340;&#21508;&#31181;&#22270;&#20687;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#30340;&#26368;&#20808;&#36827;&#34701;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;MDLatLRR&#20165;&#32771;&#34385;&#20102;&#36890;&#36807;&#28508;&#22312;&#20302;&#31209;&#34920;&#31034;&#65288;LatLRR&#65289;&#25552;&#21462;&#30340;&#36755;&#20837;&#22270;&#20687;&#30340;&#35814;&#32454;&#37096;&#20998;&#65288;&#26174;&#33879;&#29305;&#24449;&#65289;&#65292;&#27809;&#26377;&#26377;&#25928;&#22320;&#21033;&#29992;LatLRR&#25552;&#21462;&#30340;&#22522;&#30784;&#37096;&#20998;&#65288;&#20027;&#35201;&#29305;&#24449;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#32423;&#20998;&#35299;&#26041;&#27861;&#65292;&#31216;&#20026;MDLatLRRv2&#65292;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#20998;&#26512;&#21644;&#21033;&#29992;LatLRR&#33719;&#21462;&#30340;&#25152;&#26377;&#22270;&#20687;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;MDLatLRRv2&#24212;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#34701;&#21512;&#12290;&#22522;&#30784;&#37096;&#20998;&#36890;&#36807;&#24179;&#22343;&#31574;&#30053;&#36827;&#34892;&#34701;&#21512;&#65292;&#35814;&#32454;&#37096;&#20998;&#36890;&#36807;&#26680;&#33539;&#25968;&#25805;&#20316;&#36827;&#34892;&#34701;&#21512;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#27604;&#36739;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#34701;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since MDLatLRR only considers detailed parts (salient features) of input images extracted by latent low-rank representation (LatLRR), it doesn't use base parts (principal features) extracted by LatLRR effectively. Therefore, we proposed an improved multi-level decomposition method called MDLatLRRv2 which effectively analyzes and utilizes all the image features obtained by LatLRR. Then we apply MDLatLRRv2 to medical image fusion. The base parts are fused by average strategy and the detail parts are fused by nuclear-norm operation. The comparison with the existing methods demonstrates that the proposed method can achieve state-of-the-art fusion performance in objective and subjective assessment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26641;&#38598;&#21512;&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#35268;&#21017;&#21015;&#34920;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#27169;&#22411;&#23545;&#20110;&#23569;&#25968;&#31867;&#21035;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TE2Rules&#26041;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#21015;&#34920;&#20934;&#30830;&#24615;&#36739;&#39640;&#65292;&#24182;&#19988;&#36816;&#34892;&#26102;&#38388;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2206.14359</link><description>&lt;p&gt;
TE2Rules: &#20351;&#29992;&#35268;&#21017;&#35299;&#37322;&#26641;&#38598;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TE2Rules: Explaining Tree Ensembles using Rules. (arXiv:2206.14359v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26641;&#38598;&#21512;&#27169;&#22411;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#35268;&#21017;&#21015;&#34920;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#37322;&#27169;&#22411;&#23545;&#20110;&#23569;&#25968;&#31867;&#21035;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TE2Rules&#26041;&#27861;&#29983;&#25104;&#30340;&#35268;&#21017;&#21015;&#34920;&#20934;&#30830;&#24615;&#36739;&#39640;&#65292;&#24182;&#19988;&#36816;&#34892;&#26102;&#38388;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#38598;&#21512;&#65288;&#22914;&#26799;&#24230;&#25552;&#21319;&#26641;&#65289;&#36890;&#24120;&#30456;&#27604;&#21333;&#26869;&#20915;&#31574;&#26641;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#26641;&#38598;&#21512;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20154;&#31867;&#38590;&#20197;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#36923;&#36753;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;&#26641;&#38598;&#21512;&#27169;&#22411;&#36716;&#25442;&#20026;&#25509;&#36817;&#26641;&#38598;&#21512;&#30340;&#21487;&#35299;&#37322;&#35268;&#21017;&#21015;&#34920;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#35299;&#37322;&#27169;&#22411;&#23545;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#23569;&#25968;&#31867;&#21035;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TE2Rules&#29983;&#25104;&#30340;&#35268;&#21017;&#21015;&#34920;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;TE2Rules&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#20854;&#20182;&#31867;&#20284;&#22522;&#32447;&#26041;&#27861;&#30456;&#24403;&#65292;TE2Rules&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#21487;&#20197;&#20197;&#31245;&#24494;&#38477;&#20302;&#30340;&#20934;&#30830;&#24615;&#20026;&#20195;&#20215;&#36827;&#34892;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tree Ensemble (TE) models (like Gradient Boosted Trees) often provide higher prediction performance compared to single decision trees. However, TE models generally lack transparency and interpretability, as humans have difficulty understanding their decision logic. This paper presents a novel approach to convert a TE trained for a binary classification task, to a rule list (RL) that closely approximates the TE and is interpretable for a human. This RL can effectively explain the model even on the minority class predicted by the model. Experiments on benchmark datasets demonstrate that, (i) predictions from the RL generated by TE2Rules have higher fidelity (with respect to the original TE) compared to state-of-the-art methods, (ii) the run-time of TE2Rules is comparable to that of some other similar baselines and (iii) the run-time of TE2Rules algorithm can be traded off at the cost of a slightly lower fidelity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#34920;&#31034;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#25506;&#32034;&#19982;&#26631;&#31614;&#24378;&#30456;&#20851;&#21644;&#19982;&#26631;&#31614;&#24369;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#26469;&#21306;&#20998;&#20869;&#37096;&#21644;&#38750;&#20998;&#24067;&#26679;&#26412;&#65292;&#36827;&#32780;&#25552;&#39640;&#38750;&#20998;&#24067;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.09387</link><description>&lt;p&gt;
&#23545;&#20110;&#38750;&#20998;&#24067;&#26816;&#27979;&#30340;&#21452;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dual Representation Learning for Out-of-Distribution Detection. (arXiv:2206.09387v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#34920;&#31034;&#23398;&#20064;&#65288;DRL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#25506;&#32034;&#19982;&#26631;&#31614;&#24378;&#30456;&#20851;&#21644;&#19982;&#26631;&#31614;&#24369;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#26469;&#21306;&#20998;&#20869;&#37096;&#21644;&#38750;&#20998;&#24067;&#26679;&#26412;&#65292;&#36827;&#32780;&#25552;&#39640;&#38750;&#20998;&#24067;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23558;&#20869;&#37096;&#20998;&#24067;&#26679;&#26412;&#20998;&#31867;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25506;&#32034;&#19982;&#26631;&#31614;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#26681;&#25454;&#20449;&#24687;&#29942;&#39048;&#21407;&#21017;&#33293;&#24323;&#19982;&#26631;&#31614;&#24369;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#19982;&#20869;&#37096;&#20998;&#24067;&#26679;&#26412;&#19981;&#21516;&#30340;&#38750;&#20998;&#24067;&#26679;&#26412;&#21487;&#33021;&#34987;&#36171;&#20104;&#24847;&#22806;&#30340;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#33719;&#24471;&#26368;&#23567;&#30340;&#19982;&#26631;&#31614;&#24378;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#21306;&#20998;&#20869;&#37096;&#21644;&#38750;&#20998;&#24067;&#26679;&#26412;&#65292;&#21452;&#34920;&#31034;&#23398;&#20064; &#65288;DRL&#65289;&#36890;&#36807;&#20174;&#20869;&#37096;&#20998;&#24067;&#26679;&#26412;&#20013;&#21516;&#26102;&#25506;&#32034;&#19982;&#26631;&#31614;&#24378;&#30456;&#20851;&#21644;&#19982;&#26631;&#31614;&#24369;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#20351;&#38750;&#20998;&#24067;&#26679;&#26412;&#26356;&#38590;&#33719;&#24471;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#12290;&#23545;&#20110;&#19968;&#20010;&#25506;&#32034;&#19982;&#26631;&#31614;&#24378;&#30456;&#20851;&#20449;&#24687;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#26469;&#23398;&#20064;&#26631;&#31614;&#37492;&#21035;&#24615;&#34920;&#31034;&#65292;DRL&#35757;&#32451;&#20854;&#25506;&#32034;&#21097;&#20313;&#19982;&#26631;&#31614;&#24369;&#30456;&#20851;&#20449;&#24687;&#30340;&#36741;&#21161;&#32593;&#32476;&#26469;&#23398;&#20064;&#20998;&#24067;&#37492;&#21035;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
To classify in-distribution samples, deep neural networks explore strongly label-related information and discard weakly label-related information according to the information bottleneck. Out-of-distribution samples drawn from distributions differing from that of in-distribution samples could be assigned with unexpected high-confidence predictions because they could obtain minimum strongly label-related information. To distinguish in- and out-of-distribution samples, Dual Representation Learning (DRL) makes out-of-distribution samples harder to have high-confidence predictions by exploring both strongly and weakly label-related information from in-distribution samples. For a pretrained network exploring strongly label-related information to learn label-discriminative representations, DRL trains its auxiliary network exploring the remaining weakly label-related information to learn distribution-discriminative representations. Specifically, for a label-discriminative representation, DRL c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#37051;&#36817;&#20998;&#24067;&#20013;&#36873;&#25321;&#38750;&#20998;&#24067;&#26679;&#26412;&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20998;&#24067;&#22806;&#26679;&#26412;&#30340;&#35782;&#21035;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#24341;&#20837;&#20102;&#20132;&#21449;&#31867;&#25509;&#36817;&#20998;&#24067;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#20551;&#35774;&#28151;&#21512;&#22810;&#20010;&#20998;&#24067;&#20869;&#26679;&#26412;&#29983;&#25104;&#30340;&#20998;&#24067;&#22806;&#26679;&#26412;&#19982;&#20854;&#32452;&#25104;&#37096;&#20998;&#20855;&#26377;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.09385</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#21449;&#31867;&#25509;&#36817;&#20998;&#24067;&#30340;&#35757;&#32451;&#25968;&#25454;&#26816;&#27979;&#38750;&#20998;&#24067;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution Detection by Cross-class Vicinity Distribution of In-distribution Data. (arXiv:2206.09385v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#35757;&#32451;&#25968;&#25454;&#30340;&#37051;&#36817;&#20998;&#24067;&#20013;&#36873;&#25321;&#38750;&#20998;&#24067;&#26679;&#26412;&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20998;&#24067;&#22806;&#26679;&#26412;&#30340;&#35782;&#21035;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#24341;&#20837;&#20102;&#20132;&#21449;&#31867;&#25509;&#36817;&#20998;&#24067;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#20551;&#35774;&#28151;&#21512;&#22810;&#20010;&#20998;&#24067;&#20869;&#26679;&#26412;&#29983;&#25104;&#30340;&#20998;&#24067;&#22806;&#26679;&#26412;&#19982;&#20854;&#32452;&#25104;&#37096;&#20998;&#20855;&#26377;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21482;&#23398;&#20064;&#23558;&#35757;&#32451;&#20013;&#30340;&#20998;&#24067;&#36755;&#20837;&#26144;&#23556;&#21040;&#23545;&#24212;&#30340;&#27491;&#30830;&#26631;&#31614;&#65292;&#32780;&#26080;&#27861;&#21306;&#20998;&#20998;&#24067;&#22806;&#26679;&#26412;&#21644;&#20998;&#24067;&#20869;&#26679;&#26412;&#12290;&#36825;&#26159;&#22240;&#20026;&#20551;&#35774;&#25152;&#26377;&#26679;&#26412;&#37117;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#24182;&#27809;&#26377;&#20998;&#24067;&#21306;&#21035;&#12290;&#22240;&#27492;&#65292;&#20174;&#20998;&#24067;&#20869;&#26679;&#26412;&#20013;&#23398;&#20064;&#21040;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#20250;&#23558;&#20998;&#24067;&#22806;&#26679;&#26412;&#35270;&#20026;&#20998;&#24067;&#20869;&#26679;&#26412;&#65292;&#22312;&#27979;&#35797;&#38454;&#27573;&#23545;&#20854;&#36827;&#34892;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#35757;&#32451;&#20998;&#24067;&#20869;&#26679;&#26412;&#30340;&#37051;&#36817;&#20998;&#24067;&#20013;&#25277;&#21462;&#20986;&#20998;&#24067;&#22806;&#26679;&#26412;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#20998;&#24067;&#22806;&#36755;&#20837;&#19978;&#25298;&#32477;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#8220;&#20132;&#21449;&#31867;&#25509;&#36817;&#20998;&#24067;&#8221;&#65292;&#20551;&#35774;&#36890;&#36807;&#28151;&#21512;&#22810;&#20010;&#20998;&#24067;&#20869;&#26679;&#26412;&#29983;&#25104;&#30340;&#20998;&#24067;&#22806;&#26679;&#26412;&#19982;&#20854;&#32452;&#25104;&#37096;&#20998;&#19981;&#20849;&#20139;&#30456;&#21516;&#30340;&#31867;&#21035;&#12290;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21306;&#20998;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks for image classification only learn to map in-distribution inputs to their corresponding ground truth labels in training without differentiating out-of-distribution samples from in-distribution ones. This results from the assumption that all samples are independent and identically distributed (IID) without distributional distinction. Therefore, a pretrained network learned from in-distribution samples treats out-of-distribution samples as in-distribution and makes high-confidence predictions on them in the test phase. To address this issue, we draw out-of-distribution samples from the vicinity distribution of training in-distribution samples for learning to reject the prediction on out-of-distribution inputs. A \textit{Cross-class Vicinity Distribution} is introduced by assuming that an out-of-distribution sample generated by mixing multiple in-distribution samples does not share the same classes of its constituents. We thus improve the discriminability of a pretra
&lt;/p&gt;</description></item><item><title>NNV12&#26159;&#31532;&#19968;&#20010;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#20248;&#21270;&#20919;&#21551;&#21160;&#25512;&#29702;&#33021;&#21147;&#30340;&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#20869;&#26680;&#12289;&#32531;&#23384;&#26435;&#37325;&#36716;&#25442;&#21518;&#30340;&#32467;&#26524;&#20197;&#21450;&#22312;&#24322;&#26500;&#22788;&#29702;&#22120;&#19978;&#36827;&#34892;&#27969;&#27700;&#32447;&#25191;&#34892;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#25552;&#21319;&#26368;&#39640;&#36798;&#21040;15&#20493;&#12290;</title><link>http://arxiv.org/abs/2206.07446</link><description>&lt;p&gt;
&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#25552;&#21319;DNN&#20919;&#21551;&#21160;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting DNN Cold Inference on Edge Devices. (arXiv:2206.07446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07446
&lt;/p&gt;
&lt;p&gt;
NNV12&#26159;&#31532;&#19968;&#20010;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#20248;&#21270;&#20919;&#21551;&#21160;&#25512;&#29702;&#33021;&#21147;&#30340;&#25512;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#20869;&#26680;&#12289;&#32531;&#23384;&#26435;&#37325;&#36716;&#25442;&#21518;&#30340;&#32467;&#26524;&#20197;&#21450;&#22312;&#24322;&#26500;&#22788;&#29702;&#22120;&#19978;&#36827;&#34892;&#27969;&#27700;&#32447;&#25191;&#34892;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#25552;&#21319;&#26368;&#39640;&#36798;&#21040;15&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;DNN&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#21313;&#20998;&#24120;&#35265;&#12290;&#38543;&#30528;&#20854;&#37325;&#35201;&#24615;&#21644;&#20351;&#29992;&#26696;&#20363;&#30340;&#22686;&#21152;&#65292;&#19981;&#22826;&#21487;&#33021;&#23558;&#25152;&#26377;&#30340;DNN&#25171;&#21253;&#21040;&#35774;&#22791;&#20869;&#23384;&#20013;&#65292;&#24182;&#26399;&#26395;&#27599;&#20010;&#25512;&#29702;&#37117;&#24050;&#32463;&#39044;&#28909;&#12290;&#22240;&#27492;&#65292;&#20919;&#21551;&#21160;&#25512;&#29702;&#65292;&#21363;&#35835;&#21462;&#12289;&#21021;&#22987;&#21270;&#21644;&#25191;&#34892;DNN&#27169;&#22411;&#30340;&#36807;&#31243;&#65292;&#21464;&#24471;&#26222;&#36941;&#36215;&#26469;&#65292;&#24182;&#36843;&#20999;&#38656;&#35201;&#20248;&#21270;&#20854;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NNV12&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#20919;&#21551;&#21160;&#25512;&#29702;&#36827;&#34892;&#20248;&#21270;&#30340;&#22312;&#35774;&#22791;&#19978;&#25512;&#29702;&#24341;&#25806;&#12290;NNV12&#24314;&#31435;&#22312;&#19977;&#20010;&#26032;&#39062;&#30340;&#20248;&#21270;&#31574;&#30053;&#19978;&#65306;&#20026;&#27599;&#20010;DNN&#31639;&#23376;&#36873;&#25321;&#36866;&#21512;&#30340;&#20869;&#26680;&#65288;&#23454;&#29616;&#26041;&#24335;&#65289;&#65292;&#36890;&#36807;&#23558;&#21518;&#36716;&#25442;&#30340;&#26435;&#37325;&#32531;&#23384;&#22312;&#30913;&#30424;&#19978;&#32469;&#36807;&#26435;&#37325;&#36716;&#25442;&#36807;&#31243;&#65292;&#24182;&#22312;&#24322;&#26500;&#22788;&#29702;&#22120;&#19978;&#36827;&#34892;&#22810;&#20010;&#20869;&#26680;&#30340;&#27969;&#27700;&#32447;&#25191;&#34892;&#12290;&#20026;&#20102;&#24212;&#23545;&#24040;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;NNV12&#37319;&#29992;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#26041;&#26696;&#33719;&#21462;&#36817;&#20284;&#26368;&#20248;&#30340;&#20869;&#26680;&#35843;&#24230;&#35745;&#21010;&#12290;&#25105;&#20204;&#23436;&#20840;&#23454;&#29616;&#20102;NNV12&#30340;&#21407;&#22411;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;NNV12&#30340;&#24615;&#33021;&#25552;&#21319;&#26368;&#39640;&#36798;&#21040;15&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNNs are ubiquitous on edge devices nowadays. With its increasing importance and use cases, it's not likely to pack all DNNs into device memory and expect that each inference has been warmed up. Therefore, cold inference, the process to read, initialize, and execute a DNN model, is becoming commonplace and its performance is urgently demanded to be optimized. To this end, we present NNV12, the first on-device inference engine that optimizes for cold inference NNV12 is built atop 3 novel optimization knobs: selecting a proper kernel (implementation) for each DNN operator, bypassing the weights transformation process by caching the post-transformed weights on disk, and pipelined execution of many kernels on asymmetric processors. To tackle with the huge search space, NNV12 employs a heuristic-based scheme to obtain a near-optimal kernel scheduling plan. We fully implement a prototype of NNV12 and evaluate its performance across extensive experiments. It shows that NNV12 achieves up to 15
&lt;/p&gt;</description></item><item><title>ReCo&#26159;&#19968;&#20010;&#29992;&#20110;&#20303;&#23429;&#31038;&#21306;&#24067;&#23616;&#35268;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.04678</link><description>&lt;p&gt;
ReCo: &#19968;&#31181;&#29992;&#20110;&#20303;&#23429;&#31038;&#21306;&#24067;&#23616;&#35268;&#21010;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ReCo: A Dataset for Residential Community Layout Planning. (arXiv:2206.04678v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04678
&lt;/p&gt;
&lt;p&gt;
ReCo&#26159;&#19968;&#20010;&#29992;&#20110;&#20303;&#23429;&#31038;&#21306;&#24067;&#23616;&#35268;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23616;&#35268;&#21010;&#22312;&#24314;&#31569;&#21644;&#22478;&#24066;&#35774;&#35745;&#39046;&#22495;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#25215;&#36733;&#22478;&#24066;&#21151;&#33021;&#30340;&#21508;&#31181;&#22522;&#26412;&#21333;&#20301;&#20013;&#65292;&#20303;&#23429;&#31038;&#21306;&#23545;&#25903;&#25345;&#20154;&#31867;&#29983;&#27963;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#20303;&#23429;&#31038;&#21306;&#30340;&#24067;&#23616;&#35268;&#21010;&#19968;&#30452;&#21463;&#21040;&#20851;&#27880;&#65292;&#24182;&#33258;&#28145;&#24230;&#23398;&#20064;&#38382;&#19990;&#20197;&#26469;&#65292;&#23588;&#20854;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#28145;&#24230;&#23398;&#20064;&#26377;&#21161;&#20110;&#33258;&#21160;&#24067;&#23616;&#29983;&#25104;&#21644;&#31354;&#38388;&#27169;&#24335;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#39046;&#22495;&#26222;&#36941;&#38754;&#20020;&#20303;&#23429;&#31038;&#21306;&#24067;&#23616;&#22522;&#20934;&#25110;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#65292;&#36825;&#38459;&#30861;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20303;&#23429;&#31038;&#21306;&#24067;&#23616;&#35268;&#21010;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#20027;&#35201;&#26159;&#30001;&#20110;&#22823;&#35268;&#27169;&#23454;&#38469;&#20303;&#23429;&#25968;&#25454;&#37319;&#38598;&#21644;&#38271;&#26399;&#19987;&#23478;&#31579;&#36873;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25512;&#36827;&#26234;&#24935;&#22478;&#24066;&#21457;&#23637;&#20013;&#21508;&#31181;&#26234;&#33021;&#31354;&#38388;&#35774;&#35745;&#21644;&#20998;&#26512;&#24212;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ReCo&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layout planning is centrally important in the field of architecture and urban design. Among the various basic units carrying urban functions, residential community plays a vital part for supporting human life. Therefore, the layout planning of residential community has always been of concern, and has attracted particular attention since the advent of deep learning that facilitates the automated layout generation and spatial pattern recognition. However, the research circles generally suffer from the insufficiency of residential community layout benchmark or high-quality datasets, which hampers the future exploration of data-driven methods for residential community layout planning. The lack of datasets is largely due to the difficulties of large-scale real-world residential data acquisition and long-term expert screening. In order to address the issues and advance a benchmark dataset for various intelligent spatial design and analysis applications in the development of smart city, we in
&lt;/p&gt;</description></item><item><title>Diffusion-GAN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21069;&#21521;&#25193;&#25955;&#38142;&#29983;&#25104;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#30340;&#23454;&#20363;&#22122;&#22768;&#65292;&#22312;&#35757;&#32451;&#20013;&#35299;&#20915;&#20102;GAN&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.02262</link><description>&lt;p&gt;
Diffusion-GAN: &#20351;&#29992;&#25193;&#25955;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Diffusion-GAN: Training GANs with Diffusion. (arXiv:2206.02262v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02262
&lt;/p&gt;
&lt;p&gt;
Diffusion-GAN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21069;&#21521;&#25193;&#25955;&#38142;&#29983;&#25104;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#30340;&#23454;&#20363;&#22122;&#22768;&#65292;&#22312;&#35757;&#32451;&#20013;&#35299;&#20915;&#20102;GAN&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#31283;&#23450;&#35757;&#32451;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#23558;&#23454;&#20363;&#22122;&#22768;&#27880;&#20837;&#37492;&#21035;&#22120;&#36755;&#20837;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#21313;&#20998;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-GAN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;&#65292;&#21033;&#29992;&#21069;&#21521;&#25193;&#25955;&#38142;&#29983;&#25104;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#30340;&#23454;&#20363;&#22122;&#22768;&#12290;Diffusion-GAN&#21253;&#25324;&#19977;&#20010;&#32452;&#20214;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#25193;&#25955;&#36807;&#31243;&#12289;&#26102;&#38388;&#27493;&#20381;&#36182;&#30340;&#21028;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#12290;&#35266;&#23519;&#21040;&#30340;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#37117;&#36890;&#36807;&#30456;&#21516;&#30340;&#33258;&#36866;&#24212;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#25193;&#25955;&#12290;&#22312;&#27599;&#20010;&#25193;&#25955;&#26102;&#38388;&#27493;&#20013;&#65292;&#26377;&#19981;&#21516;&#30340;&#22122;&#22768;&#21040;&#25968;&#25454;&#27604;&#20363;&#65292;&#26102;&#38388;&#27493;&#20381;&#36182;&#30340;&#21028;&#21035;&#22120;&#23398;&#20064;&#21306;&#20998;&#25193;&#25955;&#30340;&#30495;&#23454;&#25968;&#25454;&#21644;&#25193;&#25955;&#30340;&#29983;&#25104;&#25968;&#25454;&#12290;&#29983;&#25104;&#22120;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#25193;&#25955;&#38142;&#30340;&#38271;&#24230;&#26469;&#24179;&#34913;&#22122;&#22768;&#21644;&#25968;&#25454;&#27700;&#24179;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#21028;&#21035;&#22120;&#30340;&#25910;&#25947;&#24615;&#21644;&#29983;&#25104;&#22120;&#30340;&#25910;&#25947;&#24615;&#65292;&#21516;&#26102;&#22312;&#19968;&#20123;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;Diffusion-GAN&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) are challenging to train stably, and a promising remedy of injecting instance noise into the discriminator input has not been very effective in practice. In this paper, we propose Diffusion-GAN, a novel GAN framework that leverages a forward diffusion chain to generate Gaussian-mixture distributed instance noise. Diffusion-GAN consists of three components, including an adaptive diffusion process, a diffusion timestep-dependent discriminator, and a generator. Both the observed and generated data are diffused by the same adaptive diffusion process. At each diffusion timestep, there is a different noise-to-data ratio and the timestep-dependent discriminator learns to distinguish the diffused real data from the diffused generated data. The generator learns from the discriminator's feedback by backpropagating through the forward diffusion chain, whose length is adaptively adjusted to balance the noise and data levels. We theoretically show that the dis
&lt;/p&gt;</description></item><item><title>ES-GNN&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#36793;&#20998;&#21106;&#23558;&#22270;&#20998;&#21106;&#20026;&#20004;&#20010;&#23376;&#22270;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23545;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#22270;&#36793;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;GNN&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13700</link><description>&lt;p&gt;
ES-GNN: &#36890;&#36807;&#36793;&#20998;&#21106;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#24322;&#36136;&#22270;
&lt;/p&gt;
&lt;p&gt;
ES-GNN: Generalizing Graph Neural Networks Beyond Homophily with Edge Splitting. (arXiv:2205.13700v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13700
&lt;/p&gt;
&lt;p&gt;
ES-GNN&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#36793;&#20998;&#21106;&#23558;&#22270;&#20998;&#21106;&#20026;&#20004;&#20010;&#23376;&#22270;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23545;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#22270;&#36793;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;GNN&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#22270;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#29616;&#20195;&#21464;&#20307;&#20027;&#35201;&#20381;&#36182;&#20110;&#21516;&#36136;&#24615;&#30340;&#24378;&#24402;&#32435;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#36890;&#24120;&#21516;&#26102;&#26174;&#31034;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#30340;&#38142;&#25509;&#27169;&#24335;&#65292;&#20854;&#20013;&#30456;&#37051;&#33410;&#28857;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#23646;&#24615;&#21644;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;GNN&#22312;&#25972;&#20307;&#19978;&#24179;&#28369;&#33410;&#28857;&#25509;&#36817;&#24615;&#21487;&#33021;&#20250;&#32858;&#21512;&#20219;&#21153;&#30456;&#20851;&#21644;&#19981;&#30456;&#20851;&#65288;&#29978;&#33267;&#26377;&#23475;&#65289;&#30340;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#25512;&#24191;&#21040;&#24322;&#36136;&#22270;&#30340;&#33021;&#21147;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#38750;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36793;&#20998;&#21106;GNN&#65288;ES-GNN&#65289;&#26694;&#26550;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#21306;&#20998;&#23545;&#23398;&#20064;&#20219;&#21153;&#30456;&#20851;&#25110;&#19981;&#30456;&#20851;&#30340;&#22270;&#36793;&#12290;&#36825;&#23558;&#21407;&#22987;&#22270;&#36716;&#21270;&#20026;&#20004;&#20010;&#20855;&#26377;&#30456;&#21516;&#33410;&#28857;&#38598;&#20294;&#20855;&#26377;&#29420;&#21344;&#36793;&#38598;&#30340;&#23376;&#22270;&#12290;&#22312;&#36825;&#20004;&#20010;&#23376;&#22270;&#19978;&#20998;&#21035;&#36827;&#34892;&#20449;&#24687;&#20256;&#25773;&#21644;&#36793;&#20998;&#21106;&#65292;&#20174;&#32780;&#20351;&#20449;&#24687;&#20256;&#25773;&#21644;&#36793;&#20998;&#21106;&#20132;&#26367;&#36827;&#34892;&#65292;&#23454;&#29616;&#20102;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Graph Neural Networks (GNNs) have achieved enormous success in multiple graph analytical tasks, modern variants mostly rely on the strong inductive bias of homophily. However, real-world networks typically exhibit both homophilic and heterophilic linking patterns, wherein adjacent nodes may share dissimilar attributes and distinct labels. Therefore, GNNs smoothing node proximity holistically may aggregate both task-relevant and irrelevant (even harmful) information, limiting their ability to generalize to heterophilic graphs and potentially causing non-robustness. In this work, we propose a novel edge splitting GNN (ES-GNN) framework to adaptively distinguish between graph edges either relevant or irrelevant to learning tasks. This essentially transfers the original graph into two subgraphs with the same node set but exclusive edge sets dynamically. Given that, information propagation separately on these subgraphs and edge splitting are alternatively conducted, thus disentangling
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#21644;&#31169;&#23494;&#30340;&#32852;&#21512;&#31070;&#32463;&#24433;&#20687;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20102;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2205.05249</link><description>&lt;p&gt;
&#23433;&#20840;&#21644;&#31169;&#23494;&#30340;&#32852;&#21512;&#31070;&#32463;&#24433;&#20687;&#23398;
&lt;/p&gt;
&lt;p&gt;
Secure &amp; Private Federated Neuroimaging. (arXiv:2205.05249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23433;&#20840;&#21644;&#31169;&#23494;&#30340;&#32852;&#21512;&#31070;&#32463;&#24433;&#20687;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#20102;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#12289;&#38544;&#31169;&#21644;&#30417;&#31649;&#38382;&#39064;&#65292;&#20174;&#22810;&#20010;&#31449;&#28857;&#25910;&#38598;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#20998;&#26512;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#65292;&#23427;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#28304;&#19978;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#20849;&#20139;&#25968;&#25454;&#12290;&#27599;&#20010;&#31449;&#28857;&#22312;&#20854;&#31169;&#26377;&#25968;&#25454;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19968;&#27573;&#26102;&#38388;&#65292;&#28982;&#21518;&#23558;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65288;&#21363;&#26435;&#37325;&#12289;&#26799;&#24230;&#65289;&#19982;&#32852;&#37030;&#25511;&#21046;&#22120;&#20849;&#20139;&#65292;&#32852;&#37030;&#25511;&#21046;&#22120;&#20877;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#65292;&#23558;&#32467;&#26524;&#27169;&#22411;&#21457;&#36865;&#22238;&#27599;&#20010;&#31449;&#28857;&#65292;&#36825;&#20010;&#36807;&#31243;&#19981;&#26029;&#37325;&#22797;&#12290;&#25105;&#20204;&#30340;&#32852;&#37030;&#23398;&#20064;&#26550;&#26500;MetisFL&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#39318;&#20808;&#65292;&#26679;&#26412;&#25968;&#25454;&#27704;&#36828;&#19981;&#20250;&#31163;&#24320;&#31449;&#28857;&#12290;&#20854;&#27425;&#65292;&#22312;&#20256;&#36755;&#20043;&#21069;&#23545;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#36827;&#34892;&#21152;&#23494;&#65292;&#24182;&#19988;&#20840;&#21516;&#24577;&#21152;&#23494;&#19979;&#35745;&#31639;&#20840;&#23616;&#31070;&#32463;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#26041;&#27861;&#26469;&#38480;&#21046;&#20449;&#24687;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
The amount of biomedical data continues to grow rapidly. However, collecting data from multiple sites for joint analysis remains challenging due to security, privacy, and regulatory concerns. To overcome this challenge, we use Federated Learning, which enables distributed training of neural network models over multiple data sources without sharing data. Each site trains the neural network over its private data for some time, then shares the neural network parameters (i.e., weights, gradients) with a Federation Controller, which in turn aggregates the local models, sends the resulting community model back to each site, and the process repeats. Our Federated Learning architecture, MetisFL, provides strong security and privacy. First, sample data never leaves a site. Second, neural network parameters are encrypted before transmission and the global neural model is computed under fully-homomorphic encryption. Finally, we use information-theoretic methods to limit information leakage from t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#22686;&#24378;&#26680;&#24515;&#22270;&#20687;&#20998;&#31867;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20808;&#36827;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#20998;&#21106;&#23721;&#24515;&#22270;&#20687;&#20013;&#30340;&#26680;&#24515;&#21644;&#27934;&#65292;&#24182;&#21033;&#29992;&#24378;&#22823;&#30340;GANs&#25216;&#26415;&#22635;&#34917;&#23721;&#24515;&#22270;&#20687;&#20013;&#30340;&#27934;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#20026;&#27833;&#27668;&#21208;&#25506;&#34892;&#19994;&#24102;&#26469;&#37325;&#22823;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2204.14224</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#22686;&#24378;&#26680;&#24515;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Enhancing Core Image Classification Using Generative Adversarial Networks (GANs). (arXiv:2204.14224v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.14224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#22686;&#24378;&#26680;&#24515;&#22270;&#20687;&#20998;&#31867;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20808;&#36827;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#20998;&#21106;&#23721;&#24515;&#22270;&#20687;&#20013;&#30340;&#26680;&#24515;&#21644;&#27934;&#65292;&#24182;&#21033;&#29992;&#24378;&#22823;&#30340;GANs&#25216;&#26415;&#22635;&#34917;&#23721;&#24515;&#22270;&#20687;&#20013;&#30340;&#27934;&#12290;&#36825;&#39033;&#30740;&#31350;&#23558;&#20026;&#27833;&#27668;&#21208;&#25506;&#34892;&#19994;&#24102;&#26469;&#37325;&#22823;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20852;&#22859;&#20154;&#24515;&#30340;&#27833;&#27668;&#21208;&#25506;&#19990;&#30028;&#20013;&#65292;&#23721;&#24515;&#26679;&#21697;&#26159;&#35299;&#38145;&#22320;&#36136;&#20449;&#24687;&#20197;&#23547;&#25214;&#26377;&#21033;&#21487;&#22270;&#30340;&#27833;&#27668;&#30719;&#24202;&#30340;&#20851;&#38190;&#12290;&#23613;&#31649;&#36825;&#20123;&#26679;&#21697;&#30340;&#37325;&#35201;&#24615;&#65292;&#20256;&#32479;&#30340;&#23721;&#24515;&#35760;&#24405;&#25216;&#26415;&#34987;&#35748;&#20026;&#26159;&#32791;&#26102;&#19988;&#20027;&#35266;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#35813;&#34892;&#19994;&#24050;&#32463;&#37319;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;-&#23721;&#24515;&#25104;&#20687;&#65292;&#23427;&#21487;&#20197;&#23545;&#22823;&#37327;&#23721;&#24515;&#36827;&#34892;&#26080;&#25439;&#21644;&#38750;&#20405;&#20837;&#24615;&#30340;&#24555;&#36895;&#34920;&#24449;&#12290;&#25105;&#20204;&#26480;&#20986;&#30340;&#30740;&#31350;&#35770;&#25991;&#26088;&#22312;&#35299;&#20915;&#23721;&#24515;&#26816;&#27979;&#21644;&#20998;&#31867;&#30340;&#32039;&#36843;&#38382;&#39064;&#12290;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31361;&#30772;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#25913;&#21464;&#35813;&#34892;&#19994;&#12290;&#25105;&#20204;&#39318;&#20808;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#23721;&#24515;&#24182;&#20998;&#21106;&#20986;&#23380;&#27934;&#65292;&#25105;&#20204;&#23558;&#20998;&#21035;&#20351;&#29992;Faster RCNN&#21644;Mask RCNN&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#21033;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GANs)&#21644;Contextual Residual&#26469;&#35299;&#20915;&#22635;&#34917;&#23721;&#24515;&#22270;&#20687;&#20013;&#30340;&#27934;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the thrilling world of oil exploration, drill core samples are key to unlocking geological information critical to finding lucrative oil deposits. Despite the importance of these samples, traditional core logging techniques are known to be laborious and, worse still, subjective. Thankfully, the industry has embraced an innovative solution core imaging that allows for nondestructive and noninvasive rapid characterization of large quantities of drill cores. Our preeminent research paper aims to tackle the pressing problem of core detection and classification. Using state-of-the-art techniques, we present a groundbreaking solution that will transform the industry. Our first challenge is detecting the cores and segmenting the holes in images, which we will achieve using the Faster RCNN and Mask RCNN models, respectively. Then, we will address the problem of filling the hole in the core image, utilizing the powerful Generative Adversarial Networks (GANs) and employing Contextual Residual
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22810;&#23610;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#27169;&#25311;&#31895;&#31890;&#21270;&#20998;&#23376;&#21160;&#21147;&#23398;&#26469;&#21152;&#36895;&#27169;&#25311;&#36807;&#31243;&#65292;&#20351;&#29992;&#38750;&#24120;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#21644;&#26032;&#39062;&#30340;&#32454;&#21270;&#27169;&#22359;&#26469;&#25552;&#39640;&#31283;&#23450;&#24615;&#12290;&#22312;&#20004;&#20010;&#22797;&#26434;&#31995;&#32479;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#21270;&#23398;&#32452;&#25104;&#30340;&#31995;&#32479;&#20013;&#21487;&#20197;&#20934;&#30830;&#24674;&#22797;&#32467;&#26500;&#21644;&#21160;&#21147;&#23398;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2204.10348</link><description>&lt;p&gt;
&#27169;&#25311;&#26102;&#38388;&#31215;&#20998;&#30340;&#31895;&#31890;&#21270;&#20998;&#23376;&#21160;&#21147;&#23398;&#19982;&#22810;&#23610;&#24230;&#22270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Simulate Time-integrated Coarse-grained Molecular Dynamics with Multi-Scale Graph Networks. (arXiv:2204.10348v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22810;&#23610;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#27169;&#25311;&#31895;&#31890;&#21270;&#20998;&#23376;&#21160;&#21147;&#23398;&#26469;&#21152;&#36895;&#27169;&#25311;&#36807;&#31243;&#65292;&#20351;&#29992;&#38750;&#24120;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#21644;&#26032;&#39062;&#30340;&#32454;&#21270;&#27169;&#22359;&#26469;&#25552;&#39640;&#31283;&#23450;&#24615;&#12290;&#22312;&#20004;&#20010;&#22797;&#26434;&#31995;&#32479;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#21270;&#23398;&#32452;&#25104;&#30340;&#31995;&#32479;&#20013;&#21487;&#20197;&#20934;&#30830;&#24674;&#22797;&#32467;&#26500;&#21644;&#21160;&#21147;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#23545;&#20110;&#21508;&#31181;&#31185;&#23398;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#22522;&#20110;&#23398;&#20064;&#30340;&#21147;&#22330;&#22312;&#21152;&#36895;&#20174;&#22836;&#31639;MD&#27169;&#25311;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#22823;&#31995;&#32479;&#21644;&#23567;&#26102;&#38388;&#27493;&#38271;&#65288;&#39134;&#31186;&#32423;&#65289;&#30340;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#25512;&#29702;&#36895;&#24230;&#20173;&#28982;&#22826;&#24930;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#22810;&#23610;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#30452;&#25509;&#27169;&#25311;&#31895;&#31890;&#21270;MD&#65292;&#20351;&#29992;&#38750;&#24120;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#65288;&#32435;&#31186;&#32423;&#65289;&#65292;&#24182;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#39062;&#32454;&#21270;&#27169;&#22359;&#26469;&#20943;&#36731;&#27169;&#25311;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#22797;&#26434;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65306;&#21333;&#38142;&#31895;&#31890;&#21270;&#32858;&#21512;&#29289;&#21644;&#22810;&#32452;&#20998;&#38146;&#31163;&#23376;&#32858;&#21512;&#29289;&#30005;&#35299;&#36136;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#27169;&#25311;&#20102;&#27604;&#35757;&#32451;&#36712;&#36857;&#38271;&#24471;&#22810;&#30340;&#36712;&#36857;&#65292;&#36866;&#29992;&#20110;&#27169;&#22411;&#26410;&#32463;&#35757;&#32451;&#30340;&#20855;&#26377;&#19981;&#21516;&#21270;&#23398;&#32452;&#25104;&#30340;&#31995;&#32479;&#12290;&#32467;&#26500;&#21644;&#21160;&#21147;&#23398;&#24615;&#36136;&#21487;&#20197;&#20934;&#30830;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular dynamics (MD) simulation is essential for various scientific domains but computationally expensive. Learning-based force fields have made significant progress in accelerating ab-initio MD simulation but are not fast enough for many real-world applications due to slow inference for large systems and small time steps (femtosecond-level). We aim to address these challenges by learning a multi-scale graph neural network that directly simulates coarse-grained MD with a very large time step (nanosecond-level) and a novel refinement module based on diffusion models to mitigate simulation instability. The effectiveness of our method is demonstrated in two complex systems: single-chain coarse-grained polymers and multi-component Li-ion polymer electrolytes. For evaluation, we simulate trajectories much longer than the training trajectories for systems with different chemical compositions that the model is not trained on. Structural and dynamical properties can be accurately recovered 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#29992;&#25143;&#30340;&#28436;&#21270;&#20559;&#22909;&#65292;&#32500;&#25252;&#23384;&#20648;&#22120;&#26469;&#23384;&#20648;&#25152;&#26377;&#29992;&#25143;&#21644;&#20803;&#32032;&#30340;&#29366;&#24577;&#65292;&#24182;&#21033;&#29992;&#36890;&#29992;&#24207;&#21015;&#25353;&#26102;&#38388;&#39034;&#24207;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2204.05490</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Continuous-Time User Preference Modelling for Temporal Sets Prediction. (arXiv:2204.05490v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05490
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26174;&#24335;&#24314;&#27169;&#29992;&#25143;&#30340;&#28436;&#21270;&#20559;&#22909;&#65292;&#32500;&#25252;&#23384;&#20648;&#22120;&#26469;&#23384;&#20648;&#25152;&#26377;&#29992;&#25143;&#21644;&#20803;&#32032;&#30340;&#29366;&#24577;&#65292;&#24182;&#21033;&#29992;&#36890;&#29992;&#24207;&#21015;&#25353;&#26102;&#38388;&#39034;&#24207;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#31995;&#21015;&#38598;&#21512;&#65292;&#27599;&#20010;&#38598;&#21512;&#37117;&#26377;&#19968;&#20010;&#26102;&#38388;&#25139;&#24182;&#21253;&#21547;&#20219;&#24847;&#25968;&#37327;&#30340;&#20803;&#32032;&#65292;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26088;&#22312;&#39044;&#27979;&#21518;&#32493;&#38598;&#21512;&#20013;&#30340;&#20803;&#32032;&#12290;&#20043;&#21069;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20803;&#32032;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#19982;&#20803;&#32032;&#30340;&#20114;&#21160;&#38388;&#25509;&#34920;&#31034;&#27599;&#20010;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#20559;&#22909;&#32463;&#24120;&#19981;&#26029;&#28436;&#21464;&#65292;&#19981;&#33021;&#23436;&#20840;&#29992;&#38388;&#25509;&#23398;&#20064;&#20559;&#22909;&#30340;&#33539;&#24335;&#26469;&#25429;&#25417;&#28436;&#21270;&#36235;&#21183;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#29992;&#25143;&#20559;&#22909;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#23384;&#20648;&#25152;&#26377;&#29992;&#25143;&#21644;&#20803;&#32032;&#29366;&#24577;&#30340;&#23384;&#20648;&#22120;&#26469;&#26174;&#24335;&#24314;&#27169;&#27599;&#20010;&#29992;&#25143;&#30340;&#28436;&#21270;&#20559;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20197;&#38750;&#38477;&#24207;&#30340;&#26102;&#38388;&#39034;&#24207;&#25490;&#21015;&#25152;&#26377;&#29992;&#25143;-&#38598;&#21512;&#20132;&#20114;&#26469;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#24207;&#21015;&#65292;&#28982;&#21518;&#25353;&#26102;&#38388;&#39034;&#24207;&#20174;&#27599;&#20010;&#29992;&#25143;-&#38598;&#21512;&#20132;&#20114;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#23545;&#20110;&#27599;&#20010;&#20132;&#20114;&#65292;&#25105;&#20204;&#36830;&#32493;&#22320;&#24314;&#27169;&#29992;&#25143;&#20559;&#22909;&#24182;&#20351;&#29992;&#35760;&#24518;&#23384;&#20648;&#22120;&#36827;&#34892;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a sequence of sets, where each set has a timestamp and contains an arbitrary number of elements, temporal sets prediction aims to predict the elements in the subsequent set. Previous studies for temporal sets prediction mainly focus on the modelling of elements and implicitly represent each user's preference based on his/her interacted elements. However, user preferences are often continuously evolving and the evolutionary trend cannot be fully captured with the indirect learning paradigm of user preferences. To this end, we propose a continuous-time user preference modelling framework for temporal sets prediction, which explicitly models the evolving preference of each user by maintaining a memory bank to store the states of all the users and elements. Specifically, we first construct a universal sequence by arranging all the user-set interactions in a non-descending temporal order, and then chronologically learn from each user-set interaction. For each interaction, we continuou
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OUR-GAN&#30340;&#19968;&#27425;&#24615;&#36229;&#39640;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#21333;&#20010;&#35757;&#32451;&#22270;&#20687;&#29983;&#25104;&#38750;&#37325;&#22797;&#30340;16K&#22270;&#20687;&#12290;&#23427;&#36890;&#36807;&#36880;&#27493;&#22686;&#21152;&#32454;&#33410;&#21644;&#36229;&#20998;&#36776;&#29575;&#26469;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#33021;&#21512;&#25104;&#20855;&#26377;&#32454;&#33410;&#21644;&#19968;&#33268;&#24615;&#30340;&#22823;&#22411;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#36890;&#36807;&#22402;&#30452;&#20301;&#31227;&#26469;&#25552;&#39640;&#35270;&#35273;&#19968;&#33268;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.13799</link><description>&lt;p&gt;
&#29992;&#21333;&#20010;GPU&#21512;&#25104;16K&#22270;&#20687;&#30340;&#19968;&#27425;&#24615;&#36229;&#39640;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
One-shot Ultra-high-Resolution Generative Adversarial Network That Synthesizes 16K Images On A Single GPU. (arXiv:2202.13799v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13799
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OUR-GAN&#30340;&#19968;&#27425;&#24615;&#36229;&#39640;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#21333;&#20010;&#35757;&#32451;&#22270;&#20687;&#29983;&#25104;&#38750;&#37325;&#22797;&#30340;16K&#22270;&#20687;&#12290;&#23427;&#36890;&#36807;&#36880;&#27493;&#22686;&#21152;&#32454;&#33410;&#21644;&#36229;&#20998;&#36776;&#29575;&#26469;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#33021;&#21512;&#25104;&#20855;&#26377;&#32454;&#33410;&#21644;&#19968;&#33268;&#24615;&#30340;&#22823;&#22411;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#36890;&#36807;&#22402;&#30452;&#20301;&#31227;&#26469;&#25552;&#39640;&#35270;&#35273;&#19968;&#33268;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27425;&#24615;&#36229;&#39640;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;OUR-GAN&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#21333;&#20010;&#35757;&#32451;&#22270;&#20687;&#29983;&#25104;&#38750;&#37325;&#22797;&#30340;16K&#65288;16,384 x 8,640&#65289;&#22270;&#20687;&#65292;&#24182;&#21487;&#20197;&#22312;&#21333;&#20010;&#28040;&#36153;&#32423;GPU&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;OUR-GAN&#22312;&#20302;&#20998;&#36776;&#29575;&#19979;&#29983;&#25104;&#19968;&#20010;&#35270;&#35273;&#19978;&#21512;&#29702;&#19988;&#24418;&#29366;&#21508;&#24322;&#30340;&#21021;&#22987;&#22270;&#20687;&#65292;&#28982;&#21518;&#36890;&#36807;&#36229;&#20998;&#36776;&#29575;&#36880;&#28176;&#22686;&#21152;&#32454;&#33410;&#26469;&#25552;&#39640;&#20998;&#36776;&#29575;&#12290;&#30001;&#20110;OUR-GAN&#20174;&#30495;&#23454;&#30340;&#36229;&#39640;&#20998;&#36776;&#29575;&#65288;UHR&#65289;&#22270;&#20687;&#20013;&#23398;&#20064;&#65292;&#23427;&#21487;&#20197;&#21512;&#25104;&#20855;&#26377;&#32454;&#33410;&#21644;&#38271;&#31243;&#19968;&#33268;&#24615;&#30340;&#22823;&#22411;&#24418;&#29366;&#65292;&#32780;&#20256;&#32479;&#30340;&#20381;&#36182;&#20110;&#20174;&#30456;&#23545;&#36739;&#23567;&#22270;&#20687;&#23398;&#20064;&#30340;&#20998;&#22359;&#20998;&#24067;&#30340;&#29983;&#25104;&#27169;&#22411;&#38590;&#20197;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;OUR-GAN&#21487;&#20197;&#20351;&#29992;12.5 GB&#30340;GPU&#20869;&#23384;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;16K&#22270;&#20687;&#65292;&#21482;&#38656;&#35201;4.29 GB&#21363;&#21487;&#21512;&#25104;4K&#22270;&#20687;&#65292;&#22240;&#20026;&#23427;&#36890;&#36807;&#26080;&#32541;&#23376;&#21306;&#22495;&#36229;&#20998;&#36776;&#29575;&#36880;&#37096;&#20998;&#21512;&#25104;UHR&#22270;&#20687;&#12290;&#21478;&#22806;&#65292;OUR-GAN&#36890;&#36807;&#24212;&#29992;&#22402;&#30452;&#20301;&#31227;&#26469;&#25552;&#39640;&#35270;&#35273;&#19968;&#33268;&#24615;&#24182;&#20445;&#25345;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a one-shot ultra-high-resolution generative adversarial network (OUR-GAN) framework that generates non-repetitive 16K (16, 384 x 8, 640) images from a single training image and is trainable on a single consumer GPU. OUR-GAN generates an initial image that is visually plausible and varied in shape at low resolution, and then gradually increases the resolution by adding detail through super-resolution. Since OUR-GAN learns from a real ultra-high-resolution (UHR) image, it can synthesize large shapes with fine details and long-range coherence, which is difficult to achieve with conventional generative models that rely on the patch distribution learned from relatively small images. OUR-GAN can synthesize high-quality 16K images with 12.5 GB of GPU memory and 4K images with only 4.29 GB as it synthesizes a UHR image part by part through seamless subregion-wise super-resolution. Additionally, OUR-GAN improves visual coherence while maintaining diversity by applying vertical positi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22806;&#29983;&#38750;&#24179;&#31283;&#21464;&#21270;&#23384;&#22312;&#19979;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#12290;&#25552;&#20986;&#20102;&#26080;&#20559;&#27748;&#26222;&#26862;&#25277;&#26679;(DTS)&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#22312;&#38754;&#23545;&#38750;&#24179;&#31283;&#22806;&#29983;&#22240;&#32032;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;DTS&#31639;&#27861;&#36890;&#36807;&#25511;&#21046;&#32972;&#26223;&#20449;&#24687;&#39044;&#27979;&#19968;&#20010;&#33218;&#30340;&#20154;&#21475;&#23618;&#32423;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#20869;&#21644;&#23454;&#39564;&#21518;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#26174;&#31034;&#20102;&#20854;&#23545;&#22806;&#29983;&#21464;&#24322;&#30340;&#24377;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.09036</link><description>&lt;p&gt;
&#22312;&#22806;&#29983;&#38750;&#24179;&#31283;&#21464;&#21270;&#23384;&#22312;&#19979;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Adaptive Experimentation in the Presence of Exogenous Nonstationary Variation. (arXiv:2202.09036v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22806;&#29983;&#38750;&#24179;&#31283;&#21464;&#21270;&#23384;&#22312;&#19979;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#12290;&#25552;&#20986;&#20102;&#26080;&#20559;&#27748;&#26222;&#26862;&#25277;&#26679;(DTS)&#31639;&#27861;&#26469;&#35299;&#20915;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#22312;&#38754;&#23545;&#38750;&#24179;&#31283;&#22806;&#29983;&#22240;&#32032;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;DTS&#31639;&#27861;&#36890;&#36807;&#25511;&#21046;&#32972;&#26223;&#20449;&#24687;&#39044;&#27979;&#19968;&#20010;&#33218;&#30340;&#20154;&#21475;&#23618;&#32423;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#20869;&#21644;&#23454;&#39564;&#21518;&#30340;&#36951;&#25022;&#30028;&#38480;&#65292;&#26174;&#31034;&#20102;&#20854;&#23545;&#22806;&#29983;&#21464;&#24322;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#35774;&#35745;&#29992;&#20110;&#36873;&#25321;&#20154;&#21475;&#37096;&#32626;&#27835;&#30103;&#26041;&#26696;&#30340;&#23454;&#39564;&#12290;&#22810;&#33218;&#32769;&#34382;&#26426;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#21453;&#39304;&#21160;&#24577;&#20998;&#37197;&#27979;&#37327;&#24037;&#20316;&#37327;&#21040;&#34920;&#29616;&#26356;&#22909;&#30340;&#33218;&#19978;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21160;&#24577;&#24615;&#21487;&#33021;&#23548;&#33268;&#38754;&#23545;&#24433;&#21709;&#23454;&#39564;&#20013;&#33218;&#34920;&#29616;&#30340;&#38750;&#24179;&#31283;&#22806;&#29983;&#22240;&#32032;&#26102;&#20986;&#29616;&#33030;&#24369;&#34892;&#20026;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#20559;&#27748;&#26222;&#26862;&#25277;&#26679;(DTS)&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#31283;&#20581;&#30340;&#33879;&#21517;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#38543;&#30528;&#35266;&#23519;&#32467;&#26524;&#30340;&#31215;&#32047;&#65292;DTS&#20250;&#25511;&#21046;&#35266;&#23519;&#21040;&#30340;&#27835;&#30103;&#20915;&#31574;&#30340;&#32972;&#26223;&#65292;&#21516;&#26102;&#39044;&#27979;&#19968;&#20010;&#33218;&#30340;&#20154;&#21475;&#23618;&#32423;&#34920;&#29616;&#12290;&#36825;&#37324;&#30340;&#32972;&#26223;&#21487;&#20197;&#25429;&#25417;&#21040;&#19968;&#20010;&#21487;&#29702;&#35299;&#30340;&#21464;&#21270;&#28304;&#65292;&#27604;&#22914;&#19968;&#20010;&#21463;&#27835;&#30103;&#20010;&#20307;&#30340;&#22269;&#23478;&#65292;&#25110;&#32773;&#20165;&#20165;&#26159;&#35760;&#24405;&#27835;&#30103;&#26102;&#38388;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;DTS&#22312;&#23454;&#39564;&#20869;&#21644;&#23454;&#39564;&#21518;&#36951;&#25022;&#30340;&#30028;&#38480;&#65292;&#35828;&#26126;&#23427;&#23545;&#20110;&#22806;&#29983;&#21464;&#24322;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate experiments that are designed to select a treatment arm for population deployment. Multi-armed bandit algorithms can enhance efficiency by dynamically allocating measurement effort towards higher performing arms based on observed feedback. However, such dynamics can result in brittle behavior in the face of nonstationary exogenous factors influencing arms' performance during the experiment. To counter this, we propose deconfounded Thompson sampling (DTS), a more robust variant of the prominent Thompson sampling algorithm. As observations accumulate, DTS projects the population-level performance of an arm while controlling for the context within which observed treatment decisions were made. Contexts here might capture a comprehensible source of variation, such as the country of a treated individual, or simply record the time of treatment. We provide bounds on both within-experiment and post-experiment regret of DTS, illustrating its resilience to exogenous variation and t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#29992;&#20110;&#20248;&#21270;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#24179;&#31283;&#20998;&#24067;&#12290;&#31639;&#27861;&#36890;&#36807;&#20272;&#35745;&#24179;&#31283;&#20998;&#24067;&#30340;&#26799;&#24230;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#20256;&#25773;&#36827;&#34892;&#36830;&#32493;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#25910;&#25947;&#33267;&#26368;&#38497;&#19979;&#38477;&#26041;&#21521;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#22312;&#32447;&#27491;&#21521;&#20256;&#25773;&#31639;&#27861;&#22312;&#32447;&#24615;&#27169;&#22411;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#38750;&#32447;&#24615;&#31034;&#20363;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2202.06637</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#29992;&#20110;&#20248;&#21270;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24179;&#31283;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Continuous-time stochastic gradient descent for optimizing over the stationary distribution of stochastic differential equations. (arXiv:2202.06637v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06637
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#29992;&#20110;&#20248;&#21270;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#24179;&#31283;&#20998;&#24067;&#12290;&#31639;&#27861;&#36890;&#36807;&#20272;&#35745;&#24179;&#31283;&#20998;&#24067;&#30340;&#26799;&#24230;&#65292;&#24182;&#20351;&#29992;&#27491;&#21521;&#20256;&#25773;&#36827;&#34892;&#36830;&#32493;&#26356;&#26032;&#21442;&#25968;&#65292;&#23454;&#29616;&#25910;&#25947;&#33267;&#26368;&#38497;&#19979;&#38477;&#26041;&#21521;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#22312;&#32447;&#27491;&#21521;&#20256;&#25773;&#31639;&#27861;&#22312;&#32447;&#24615;&#27169;&#22411;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#38750;&#32447;&#24615;&#31034;&#20363;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#30340;&#24179;&#31283;&#20998;&#24067;&#12290;&#31639;&#27861;&#20351;&#29992;&#24179;&#31283;&#20998;&#24067;&#30340;&#26799;&#24230;&#20272;&#35745;&#36830;&#32493;&#26356;&#26032;SDE&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#26799;&#24230;&#20272;&#35745;&#21516;&#26102;&#20351;&#29992;SDE&#29366;&#24577;&#23548;&#25968;&#30340;&#27491;&#21521;&#20256;&#25773;&#36827;&#34892;&#26356;&#26032;&#65292;&#28176;&#36817;&#22320;&#25910;&#25947;&#21040;&#26368;&#38497;&#19979;&#38477;&#26041;&#21521;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#22312;&#32447;&#27491;&#21521;&#20256;&#25773;&#31639;&#27861;&#22312;&#32447;&#24615;SDE&#27169;&#22411;&#65288;&#22914;&#22810;&#32500;Ornstein-Uhlenbeck&#36807;&#31243;&#65289;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#21576;&#29616;&#20102;&#38750;&#32447;&#24615;&#31034;&#20363;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#35777;&#26126;&#38656;&#35201;&#23545;&#21442;&#25968;&#28436;&#21270;&#22312;&#26368;&#38497;&#19979;&#38477;&#26041;&#21521;&#38468;&#36817;&#30340;&#27874;&#21160;&#36827;&#34892;&#20998;&#26512;&#12290;&#30001;&#20110;&#31639;&#27861;&#30340;&#22312;&#32447;&#24615;&#36136;&#65292;&#33719;&#24471;&#27874;&#21160;&#30340;&#30028;&#38480;&#24456;&#20855;&#25361;&#25112;&#24615;&#65288;&#20363;&#22914;&#65292;&#38543;&#30528;&#21442;&#25968;&#30340;&#21464;&#21270;&#65292;&#31283;&#23450;&#20998;&#24067;&#23558;&#25345;&#32493;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a new continuous-time stochastic gradient descent method for optimizing over the stationary distribution of stochastic differential equation (SDE) models. The algorithm continuously updates the SDE model's parameters using an estimate for the gradient of the stationary distribution. The gradient estimate is simultaneously updated using forward propagation of the SDE state derivatives, asymptotically converging to the direction of steepest descent. We rigorously prove convergence of the online forward propagation algorithm for linear SDE models (i.e., the multi-dimensional Ornstein-Uhlenbeck process) and present its numerical results for nonlinear examples. The proof requires analysis of the fluctuations of the parameter evolution around the direction of steepest descent. Bounds on the fluctuations are challenging to obtain due to the online nature of the algorithm (e.g., the stationary distribution will continuously change as the parameters change). We prove bounds for the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#22270;&#35889;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#32986;&#32974;&#20998;&#21106;&#21644;&#31354;&#38388;&#23545;&#40784;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20154;&#20307;&#32986;&#32974;&#22312;&#31532;&#19968;&#23395;&#24230;3D&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#29983;&#38271;&#21644;&#21457;&#32946;&#36827;&#34892;&#30417;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#23558;&#32986;&#32974;&#27880;&#20876;&#21040;&#22270;&#35889;&#20013;&#65292;&#20351;&#29992;&#20102;&#22810;&#20010;&#23381;&#40836;&#33539;&#22260;&#20869;&#30340;&#36229;&#22768;&#22270;&#20687;&#65292;&#32463;&#36807;&#20998;&#21106;&#21644;&#31354;&#38388;&#23545;&#40784;&#21518;&#24471;&#21040;&#26631;&#20934;&#26041;&#21521;&#19978;&#30340;&#32986;&#32974;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2202.06599</link><description>&lt;p&gt;
&#20154;&#20307;&#32986;&#32974;&#22312;&#31532;&#19968;&#23395;&#24230;3D&#36229;&#22768;&#20013;&#30340;&#22810;&#22270;&#35889;&#20998;&#21106;&#19982;&#31354;&#38388;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Multi-Atlas Segmentation and Spatial Alignment of the Human Embryo in First Trimester 3D Ultrasound. (arXiv:2202.06599v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#22810;&#22270;&#35889;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#32986;&#32974;&#20998;&#21106;&#21644;&#31354;&#38388;&#23545;&#40784;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20154;&#20307;&#32986;&#32974;&#22312;&#31532;&#19968;&#23395;&#24230;3D&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#29983;&#38271;&#21644;&#21457;&#32946;&#36827;&#34892;&#30417;&#27979;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#23398;&#20064;&#23558;&#32986;&#32974;&#27880;&#20876;&#21040;&#22270;&#35889;&#20013;&#65292;&#20351;&#29992;&#20102;&#22810;&#20010;&#23381;&#40836;&#33539;&#22260;&#20869;&#30340;&#36229;&#22768;&#22270;&#20687;&#65292;&#32463;&#36807;&#20998;&#21106;&#21644;&#31354;&#38388;&#23545;&#40784;&#21518;&#24471;&#21040;&#26631;&#20934;&#26041;&#21521;&#19978;&#30340;&#32986;&#32974;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#21629;&#30340;&#36825;&#19968;&#20851;&#38190;&#26102;&#26399;&#65292;&#23545;&#31532;&#19968;&#23395;&#24230;&#32974;&#20799;&#29983;&#38271;&#21644;&#21457;&#32946;&#36827;&#34892;&#30417;&#27979;&#65292;&#36229;&#22768;&#25104;&#20687;&#25968;&#25454;&#30340;&#20998;&#21106;&#21644;&#31354;&#38388;&#23545;&#40784;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#26159;&#25163;&#21160;&#30340;&#65292;&#35201;&#20040;&#26159;&#21322;&#33258;&#21160;&#30340;&#65292;&#38750;&#24120;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#36825;&#20123;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#22270;&#35889;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#26368;&#23567;&#30417;&#30563;&#26469;&#23454;&#29616;&#32986;&#32974;&#30340;&#33258;&#21160;&#20998;&#21106;&#21644;&#31354;&#38388;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23398;&#20064;&#23558;&#32986;&#32974;&#27880;&#20876;&#21040;&#19968;&#20010;&#22270;&#35889;&#20013;&#65292;&#35813;&#22270;&#35889;&#30001;&#22312;&#19981;&#21516;&#23381;&#40836;&#33539;&#22260;&#20869;&#33719;&#21462;&#30340;&#36229;&#22768;&#22270;&#20687;&#32463;&#36807;&#20998;&#21106;&#21644;&#31354;&#38388;&#23545;&#40784;&#21518;&#24418;&#25104;&#30340;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#32986;&#32974;&#30340;&#20998;&#21106;&#32467;&#26524;&#24182;&#23558;&#20854;&#25918;&#32622;&#22312;&#26631;&#20934;&#26041;&#21521;&#19978;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;8+0&#33267;12+6&#21608;&#30340;&#23381;&#40836;&#30340;&#36229;&#22768;&#22270;&#20687;&#65292;&#24182;&#36873;&#25321;&#20102;&#20843;&#20010;&#34987;&#35797;&#20316;&#20026;&#22270;&#35889;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#31574;&#30053;&#26469;&#32467;&#21512;&#22810;&#20010;&#22270;&#35889;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation and spatial alignment of ultrasound (US) imaging data acquired in the in first trimester are crucial for monitoring human embryonic growth and development throughout this crucial period of life. Current approaches are either manual or semi-automatic and are therefore very time-consuming and prone to errors. To automate these tasks, we propose a multi-atlas framework for automatic segmentation and spatial alignment of the embryo using deep learning with minimal supervision. Our framework learns to register the embryo to an atlas, which consists of the US images acquired at a range of gestational age (GA), segmented and spatially aligned to a predefined standard orientation. From this, we can derive the segmentation of the embryo and put the embryo in standard orientation. US images acquired at 8+0 till 12+6 weeks GA were used and eight subjects were selected as atlas. We evaluated different fusion strategies to incorporate multiple atlases: 1) training the framework using a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#28151;&#21512;&#26041;&#27861;&#65288;NNP/MM&#65289;&#65292;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#21183;&#21644;&#20998;&#23376;&#21147;&#23398;&#65292;&#20197;&#21152;&#36895;&#29983;&#29289;&#20998;&#23376;&#27169;&#25311;&#12290;&#36890;&#36807;&#23545;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#36827;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#23545;&#37197;&#20307;&#36827;&#34892;&#20803;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#25311;&#36895;&#24230;&#24182;&#23454;&#29616;&#26368;&#38271;&#30340;&#27169;&#25311;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2201.08110</link><description>&lt;p&gt;
NNP/MM: &#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21183;&#21644;&#20998;&#23376;&#21147;&#23398;&#21152;&#36895;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
NNP/MM: Accelerating molecular dynamics simulations with machine learning potentials and molecular mechanic. (arXiv:2201.08110v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08110
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#28151;&#21512;&#26041;&#27861;&#65288;NNP/MM&#65289;&#65292;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#21183;&#21644;&#20998;&#23376;&#21147;&#23398;&#65292;&#20197;&#21152;&#36895;&#29983;&#29289;&#20998;&#23376;&#27169;&#25311;&#12290;&#36890;&#36807;&#23545;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#36827;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#21644;&#23545;&#37197;&#20307;&#36827;&#34892;&#20803;&#21160;&#21147;&#23398;&#27169;&#25311;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#27169;&#25311;&#36895;&#24230;&#24182;&#23454;&#29616;&#26368;&#38271;&#30340;&#27169;&#25311;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21183;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#29983;&#29289;&#20998;&#23376;&#27169;&#25311;&#20934;&#30830;&#24615;&#30340;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#20998;&#23376;&#21147;&#23398;&#30456;&#27604;&#65292;&#23427;&#20204;&#30340;&#24212;&#29992;&#21463;&#21040;&#22823;&#37327;&#21442;&#25968;&#24102;&#26469;&#30340;&#26174;&#33879;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28151;&#21512;&#26041;&#27861;&#65288;NNP/MM&#65289;&#30340;&#20248;&#21270;&#23454;&#29616;&#65292;&#23427;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#21183;&#65288;NNP&#65289;&#21644;&#20998;&#23376;&#21147;&#23398;&#65288;MM&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;NNP&#26469;&#27169;&#25311;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#65288;&#22914;&#23567;&#20998;&#23376;&#65289;&#65292;&#21516;&#26102;&#20351;&#29992;MM&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#34507;&#30333;&#36136;-&#37197;&#20307;&#22797;&#21512;&#29289;&#36827;&#34892;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#21644;&#23545;&#37197;&#20307;&#36827;&#34892;&#20803;&#21160;&#21147;&#23398;&#65288;MTD&#65289;&#27169;&#25311;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;NNP/MM&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#27169;&#25311;&#36895;&#24230;&#25552;&#39640;&#20102;5&#20493;&#65292;&#24182;&#22312;&#27599;&#20010;&#22797;&#21512;&#29289;&#19978;&#23454;&#29616;&#20102;1&#24494;&#31186;&#30340;&#32852;&#21512;&#37319;&#26679;&#65292;&#25104;&#20026;&#36804;&#20170;&#20026;&#27490;&#25253;&#36947;&#30340;&#36825;&#31867;&#27169;&#25311;&#20013;&#26368;&#38271;&#30340;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning potentials have emerged as a means to enhance the accuracy of biomolecular simulations. However, their application is constrained by the significant computational cost arising from the vast number of parameters compared to traditional molecular mechanics. To tackle this issue, we introduce an optimized implementation of the hybrid method (NNP/MM), which combines neural network potentials (NNP) and molecular mechanics (MM). This approach models a portion of the system, such as a small molecule, using NNP while employing MM for the remaining system to boost efficiency. By conducting molecular dynamics (MD) simulations on various protein-ligand complexes and metadynamics (MTD) simulations on a ligand, we showcase the capabilities of our implementation of NNP/MM. It has enabled us to increase the simulation speed by 5 times and achieve a combined sampling of one microsecond for each complex, marking the longest simulations ever reported for this class of simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#19981;&#23436;&#20840;&#29305;&#24449;&#21644;&#26631;&#31614;&#12289;&#22122;&#22768;&#35270;&#22270;&#21644;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#19981;&#23436;&#20840;&#30340;&#35270;&#22270;&#21644;&#24369;&#26631;&#31614;&#23884;&#20837;&#21040;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26435;&#37325;&#21644;&#23884;&#20837;&#30697;&#38453;&#24046;&#24322;&#26469;&#20943;&#23569;&#20887;&#20313;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.01079</link><description>&lt;p&gt;
&#19981;&#23436;&#20840;&#30340;&#22810;&#35270;&#35282;&#24369;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incomplete Multi-View Weak-Label Learning. (arXiv:2201.01079v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#19981;&#23436;&#20840;&#29305;&#24449;&#21644;&#26631;&#31614;&#12289;&#22122;&#22768;&#35270;&#22270;&#21644;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#19981;&#23436;&#20840;&#30340;&#35270;&#22270;&#21644;&#24369;&#26631;&#31614;&#23884;&#20837;&#21040;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26435;&#37325;&#21644;&#23884;&#20837;&#30697;&#38453;&#24046;&#24322;&#26469;&#20943;&#23569;&#20887;&#20313;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24212;&#29992;&#20013;&#23384;&#22312;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#65292;&#27599;&#20010;&#26679;&#26412;&#20855;&#26377;&#22810;&#20010;&#35270;&#22270;&#29305;&#24449;&#65292;&#22810;&#20010;&#26631;&#31614;&#36890;&#36807;&#20849;&#21516;&#30340;&#35270;&#22270;&#30456;&#20851;&#32852;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#19981;&#33021;&#30452;&#25509;&#22788;&#29702;&#27599;&#20010;&#26679;&#26412;&#21482;&#35266;&#23519;&#21040;&#37096;&#20998;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#24182;&#24573;&#30053;&#20102;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#23384;&#22312;&#30340;&#22122;&#22768;&#35270;&#22270;&#21644;&#19981;&#22343;&#34913;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#23427;&#23558;&#19981;&#23436;&#20840;&#30340;&#35270;&#22270;&#21644;&#24369;&#26631;&#31614;&#19968;&#36215;&#23884;&#20837;&#21040;&#33258;&#36866;&#24212;&#26435;&#37325;&#30340;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#65288;HSIC&#65289;&#30340;&#23884;&#20837;&#26435;&#37325;&#30697;&#38453;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#20943;&#23569;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#32858;&#28966;&#25439;&#22833;&#33258;&#36866;&#24212;&#23398;&#20064;&#35270;&#22270;&#30340;&#37325;&#35201;&#24615;&#20197;&#26816;&#27979;&#22122;&#22768;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#32858;&#28966;&#25439;&#22833;&#20943;&#36731;&#26631;&#31614;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23545;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#22810;&#35270;&#22270;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A variety of modern applications exhibit multi-view multi-label learning, where each sample has multi-view features, and multiple labels are correlated via common views. Current methods usually fail to directly deal with the setting where only a subset of features and labels are observed for each sample, and ignore the presence of noisy views and imbalanced labels in real-world problems. In this paper, we propose a novel method to overcome the limitations. It jointly embeds incomplete views and weak labels into a low-dimensional subspace with adaptive weights, and facilitates the difference between embedding weight matrices via auto-weighted Hilbert-Schmidt Independence Criterion (HSIC) to reduce the redundancy. Moreover, it adaptively learns view-wise importance for embedding to detect noisy views, and mitigates the label imbalance problem by focal loss. Experimental results on four real-world multi-view multi-label datasets demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#22352;&#26631;&#21464;&#25442;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23618;&#32423;&#24352;&#37327;&#31215;&#23637;&#24320;&#26469;&#36924;&#36817;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#25237;&#24433;&#31995;&#25968;&#36827;&#34892;&#26816;&#27979;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.01729</link><description>&lt;p&gt;
&#38543;&#26426;&#22352;&#26631;&#21464;&#25442;&#21450;&#20854;&#22312;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stochastic coordinate transformations with applications to robust machine learning. (arXiv:2110.01729v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#22352;&#26631;&#21464;&#25442;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23618;&#32423;&#24352;&#37327;&#31215;&#23637;&#24320;&#26469;&#36924;&#36817;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#25237;&#24433;&#31995;&#25968;&#36827;&#34892;&#26816;&#27979;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#26032;&#30340;&#29305;&#24449;&#65292;&#21033;&#29992;Karhunen-Loeve&#23637;&#24320;&#27861;&#26469;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#28508;&#22312;&#38543;&#26426;&#34892;&#20026;&#12290;&#36825;&#20123;&#26032;&#29305;&#24449;&#26159;&#36890;&#36807;&#22522;&#20110;&#26368;&#36817;&#30340;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#29702;&#35770;&#36827;&#34892;&#30340;&#22352;&#26631;&#21464;&#25442;&#26500;&#24314;&#30340;&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#12290;&#30456;&#20851;&#30340;&#20449;&#21495;&#20998;&#35299;&#26159;&#29992;&#24050;&#30693;&#20248;&#21270;&#23646;&#24615;&#30340;&#23618;&#32423;&#24352;&#37327;&#31215;&#23637;&#24320;&#26469;&#36924;&#36817;&#20855;&#26377;&#26377;&#38480;&#21151;&#33021;&#31354;&#38388;&#30340;&#38543;&#26426;&#36807;&#31243;&#65288;&#38543;&#26426;&#22330;&#65289;&#12290;&#21407;&#21017;&#19978;&#65292;&#36825;&#20123;&#20302;&#32500;&#31354;&#38388;&#21487;&#20197;&#25429;&#25417;&#32473;&#23450;&#21517;&#20041;&#31867;&#21035;&#30340;'&#24213;&#23618;&#20449;&#21495;'&#30340;&#22823;&#37096;&#20998;&#38543;&#26426;&#21464;&#21270;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#26469;&#33258;&#20854;&#23427;&#31867;&#21035;&#30340;&#20449;&#21495;&#25298;&#32477;&#20026;&#38543;&#26426;&#24322;&#24120;&#12290;&#36890;&#36807;&#21517;&#20041;&#31867;&#21035;&#30340;&#23618;&#32423;&#26377;&#38480;&#32500;&#23637;&#24320;&#65292;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#26816;&#27979;&#24322;&#24120;&#20449;&#21495;&#32452;&#20214;&#30340;&#27491;&#20132;&#23884;&#22871;&#23376;&#31354;&#38388;&#12290;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#23376;&#31354;&#38388;&#20013;&#30340;&#25237;&#24433;&#31995;&#25968;&#26469;&#35757;&#32451;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#32988;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce a set of novel features for identifying underlying stochastic behavior of input data using the Karhunen-Loeve expansion. These novel features are constructed by applying a coordinate transformation based on the recent Functional Data Analysis theory for anomaly detection. The associated signal decomposition is an exact hierarchical tensor product expansion with known optimality properties for approximating stochastic processes (random fields) with finite dimensional function spaces. In principle these low dimensional spaces can capture most of the stochastic behavior of `underlying signals' in a given nominal class, and can reject signals in alternative classes as stochastic anomalies. Using a hierarchical finite dimensional expansion of the nominal class, a series of orthogonal nested subspaces is constructed for detecting anomalous signal components. Projection coefficients of input data in these subspaces are then used to train a Machine Learning (ML) clas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38544;&#24335;&#29983;&#25104;&#22120;&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36776;&#21035;&#22120;&#22312;&#20998;&#24067;&#19978;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#21644;&#24809;&#32602;&#29305;&#23450;&#30340;&#20998;&#24067;&#22806;&#26679;&#26412;&#26469;&#25552;&#39640;&#36776;&#21035;&#22120;&#22312;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#26679;&#26412;&#19978;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2108.09976</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#29983;&#25104;&#22120;&#25581;&#31034;&#36776;&#21035;&#22120;&#30340;&#20998;&#24067;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revealing the Distributional Vulnerability of Discriminators by Implicit Generators. (arXiv:2108.09976v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.09976
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#29983;&#25104;&#22120;&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36776;&#21035;&#22120;&#22312;&#20998;&#24067;&#19978;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#21644;&#24809;&#32602;&#29305;&#23450;&#30340;&#20998;&#24067;&#22806;&#26679;&#26412;&#26469;&#25552;&#39640;&#36776;&#21035;&#22120;&#22312;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#26679;&#26412;&#19978;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#22312;&#20998;&#24067;&#26679;&#26412;&#19978;&#35757;&#32451;&#30340;&#36776;&#21035;&#22120;&#21487;&#33021;&#23545;&#20998;&#24067;&#22806;&#26679;&#26412;&#20570;&#20986;&#39640;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#12290;&#36825;&#23545;&#20110;&#24378;&#22823;&#12289;&#21487;&#38752;&#21644;&#23433;&#20840;&#30340;&#28145;&#24230;&#23398;&#20064;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#20027;&#35201;&#26159;&#30001;&#20110;&#22312;&#35757;&#32451;&#36776;&#21035;&#22120;&#26102;&#65292;&#30001;&#20110;&#20998;&#24067;&#22806;&#26679;&#26412;&#19981;&#21487;&#29992;&#65292;&#21482;&#33021;&#35266;&#27979;&#21040;&#26377;&#38480;&#30340;&#20998;&#24067;&#20869;&#26679;&#26412;&#25152;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#29983;&#25104;&#22120;&#36827;&#34892;&#36776;&#21035;&#22120;&#24494;&#35843;&#30340;&#36890;&#29992;&#26041;&#27861;&#65288;FIG&#65289;&#12290;FIG&#20197;&#20449;&#24687;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#24182;&#36866;&#29992;&#20110;&#26631;&#20934;&#36776;&#21035;&#22120;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#21644;&#24809;&#32602;&#29305;&#23450;&#30340;&#20998;&#24067;&#22806;&#26679;&#26412;&#26469;&#25552;&#39640;&#26631;&#20934;&#36776;&#21035;&#22120;&#22312;&#21306;&#20998;&#20998;&#24067;&#20869;&#26679;&#26412;&#21644;&#20998;&#24067;&#22806;&#26679;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26681;&#25454;&#39321;&#20892;&#29109;&#65292;&#25105;&#20204;&#20174;&#36776;&#21035;&#22120;&#20013;&#25512;&#26029;&#20986;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#38544;&#24335;&#29983;&#25104;&#22120;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#28982;&#21518;&#65292;Langevin&#21160;&#21147;&#23398;&#37319;&#26679;&#22120;&#20026;&#38544;&#24335;&#29983;&#25104;&#22120;&#32472;&#21046;&#29305;&#23450;&#30340;&#20998;&#24067;&#22806;&#26679;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31526;&#21512;&#35774;&#35745;&#21407;&#21017;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#36866;&#24212;&#35813;&#26041;&#27861;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep neural learning, a discriminator trained on in-distribution (ID) samples may make high-confidence predictions on out-of-distribution (OOD) samples. This triggers a significant matter for robust, trustworthy and safe deep learning. The issue is primarily caused by the limited ID samples observable in training the discriminator when OOD samples are unavailable. We propose a general approach for \textit{fine-tuning discriminators by implicit generators} (FIG). FIG is grounded on information theory and applicable to standard discriminators without retraining. It improves the ability of a standard discriminator in distinguishing ID and OOD samples by generating and penalizing its specific OOD samples. According to the Shannon entropy, an energy-based implicit generator is inferred from a discriminator without extra training costs. Then, a Langevin dynamic sampler draws specific OOD samples for the implicit generator. Lastly, we design a regularizer fitting the design principle of th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20803;&#26657;&#20934;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#24494;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#20195;&#29702;&#25351;&#26631;&#21644;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#26657;&#20934;&#36136;&#37327;&#30340;&#30452;&#25509;&#20248;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#19982;&#29616;&#26377;&#26657;&#20934;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#35813;&#26694;&#26550;&#20026;&#36827;&#19968;&#27493;&#35299;&#20915;&#26657;&#20934;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2106.09613</link><description>&lt;p&gt;
&#20803;&#26657;&#20934;&#65306;&#20351;&#29992;&#21487;&#24494;&#20043;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#23398;&#20064;&#27169;&#22411;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Meta-Calibration: Learning of Model Calibration Using Differentiable Expected Calibration Error. (arXiv:2106.09613v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.09613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20803;&#26657;&#20934;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#24494;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#20195;&#29702;&#25351;&#26631;&#21644;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#26657;&#20934;&#36136;&#37327;&#30340;&#30452;&#25509;&#20248;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#36798;&#21040;&#19982;&#29616;&#26377;&#26657;&#20934;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#35813;&#26694;&#26550;&#20026;&#36827;&#19968;&#27493;&#35299;&#20915;&#26657;&#20934;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#24403;&#20351;&#29992;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#19982;&#27491;&#30830;&#39044;&#27979;&#30340;&#27010;&#29575;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#26126;&#26174;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#25913;&#21892;&#26657;&#20934;&#65292;&#20294;&#20934;&#30830;&#30340;&#26657;&#20934;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21253;&#21547;&#20004;&#20010;&#36129;&#29486;&#65306;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20195;&#29702;&#25351;&#26631;&#65292;&#29992;&#20110;&#30452;&#25509;&#20248;&#21270;&#26657;&#20934;&#36136;&#37327;&#30340;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046; (DECE)&#65292;&#20197;&#21450;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;DECE&#26681;&#25454;&#27169;&#22411;&#36229;&#21442;&#25968;&#20248;&#21270;&#39564;&#35777;&#38598;&#26657;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26657;&#20934;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#22788;&#29702;&#26657;&#20934;&#38382;&#39064;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#21644;&#24037;&#20855;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#23558;&#28608;&#21457;&#26356;&#22810;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibration of neural networks is a topical problem that is becoming more and more important as neural networks increasingly underpin real-world applications. The problem is especially noticeable when using modern neural networks, for which there is a significant difference between the confidence of the model and the probability of correct prediction. Various strategies have been proposed to improve calibration, yet accurate calibration remains challenging. We propose a novel framework with two contributions: introducing a new differentiable surrogate for expected calibration error (DECE) that allows calibration quality to be directly optimised, and a meta-learning framework that uses DECE to optimise for validation set calibration with respect to model hyper-parameters. The results show that we achieve competitive performance with existing calibration approaches. Our framework opens up a new avenue and toolset for tackling calibration, which we believe will inspire further work on thi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#32593;&#32476;&#12289;&#28216;&#25103;&#21644;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#29702;&#35299;&#32593;&#32476;&#19978;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#36873;&#25321;&#24615;&#30340;&#21338;&#24328;&#29702;&#35770;&#23398;&#20064;&#31639;&#27861;&#27010;&#36848;&#21644;&#22312;&#29616;&#20195;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2105.08158</link><description>&lt;p&gt;
&#32593;&#32476;&#12289;&#28216;&#25103;&#21644;&#23398;&#20064;&#30340;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
The Confluence of Networks, Games and Learning. (arXiv:2105.08158v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.08158
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#32593;&#32476;&#12289;&#28216;&#25103;&#21644;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#29702;&#35299;&#32593;&#32476;&#19978;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#36873;&#25321;&#24615;&#30340;&#21338;&#24328;&#29702;&#35770;&#23398;&#20064;&#31639;&#27861;&#27010;&#36848;&#21644;&#22312;&#29616;&#20195;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29616;&#20195;&#32593;&#32476;&#24212;&#29992;&#30340;&#25216;&#26415;&#21644;&#26381;&#21153;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#26234;&#33021;&#30005;&#32593;&#31649;&#29702;&#12289;&#26080;&#32447;&#36890;&#20449;&#12289;&#32593;&#32476;&#23433;&#20840;&#20197;&#21450;&#22810;&#26234;&#33021;&#20307;&#33258;&#20027;&#31995;&#32479;&#12290;&#32771;&#34385;&#21040;&#32593;&#32476;&#23454;&#20307;&#30340;&#24322;&#26500;&#24615;&#65292;&#26032;&#20852;&#32593;&#32476;&#24212;&#29992;&#38656;&#35201;&#21338;&#24328;&#29702;&#35770;&#27169;&#22411;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#21019;&#24314;&#23545;&#21160;&#24577;&#25110;&#23545;&#25239;&#29615;&#22659;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#24178;&#25200;&#20316;&#20986;&#21709;&#24212;&#30340;&#20998;&#24067;&#24335;&#32593;&#32476;&#26234;&#33021;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#32593;&#32476;&#12289;&#28216;&#25103;&#21644;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#29702;&#35299;&#32593;&#32476;&#19978;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#22880;&#23450;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#22312;&#38543;&#26426;&#36924;&#36817;&#29702;&#35770;&#30340;&#26694;&#26550;&#20869;&#25552;&#20379;&#20102;&#21338;&#24328;&#29702;&#35770;&#23398;&#20064;&#31639;&#27861;&#30340;&#36873;&#25321;&#24615;&#27010;&#36848;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#29616;&#20195;&#32593;&#32476;&#31995;&#32479;&#30340;&#19968;&#20123;&#20195;&#34920;&#24615;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#19979;&#19968;&#20195;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#12289;&#26234;&#33021;&#30005;&#32593;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed significant advances in technologies and services in modern network applications, including smart grid management, wireless communication, cybersecurity as well as multi-agent autonomous systems. Considering the heterogeneous nature of networked entities, emerging network applications call for game-theoretic models and learning-based approaches in order to create distributed network intelligence that responds to uncertainties and disruptions in a dynamic or an adversarial environment. This paper articulates the confluence of networks, games and learning, which establishes a theoretical underpinning for understanding multi-agent decision-making over networks. We provide an selective overview of game-theoretic learning algorithms within the framework of stochastic approximation theory, and associated applications in some representative contexts of modern network systems, such as the next generation wireless communication networks, the smart grid and distribute
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#35299;&#20915;&#20998;&#24067;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Hilbert-Schmidt&#31639;&#23376;&#23558;&#20989;&#25968;&#22495;&#20043;&#38388;&#30340;&#38543;&#26426;&#26144;&#23556;&#36827;&#34892;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#22788;&#29702;&#20989;&#25968;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2102.03895</link><description>&lt;p&gt;
&#21151;&#33021;&#24615;&#26368;&#20248;&#36755;&#36816;&#65306;&#21151;&#33021;&#25968;&#25454;&#30340;&#26144;&#23556;&#20272;&#35745;&#21644;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Functional optimal transport: map estimation and domain adaptation for functional data. (arXiv:2102.03895v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.03895
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#35299;&#20915;&#20998;&#24067;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Hilbert-Schmidt&#31639;&#23376;&#23558;&#20989;&#25968;&#22495;&#20043;&#38388;&#30340;&#38543;&#26426;&#26144;&#23556;&#36827;&#34892;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#22788;&#29702;&#20989;&#25968;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#23545;&#20998;&#24067;&#36827;&#34892;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#34920;&#36798;&#65292;&#20854;&#20013;&#20989;&#25968;&#22495;&#20043;&#38388;&#30340;&#38543;&#26426;&#26144;&#23556;&#21487;&#20197;&#37096;&#20998;&#22320;&#34920;&#31034;&#20026;&#19968;&#20010;&#65288;&#26080;&#38480;&#32500;&#65289;Hilbert-Schmidt&#31639;&#23376;&#65292;&#23558;&#19968;&#20010;&#20989;&#25968;&#30340;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26144;&#23556;&#21040;&#21478;&#19968;&#20010;&#20989;&#25968;&#19978;&#12290;&#23545;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#25968;&#25454;&#21487;&#20197;&#33258;&#28982;&#22320;&#35270;&#20026;&#20174;&#20989;&#25968;&#31354;&#38388;&#20013;&#37319;&#26679;&#24471;&#21040;&#30340;&#65292;&#20363;&#22914;&#26354;&#32447;&#21644;&#26354;&#38754;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#12290;&#21151;&#33021;&#25968;&#25454;&#20998;&#26512;&#30340;&#26368;&#20248;&#36755;&#36816;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#39046;&#22495;&#36827;&#34892;&#22788;&#29702;&#30340;&#26377;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a formulation of optimal transport problem for distributions on function spaces, where the stochastic map between functional domains can be partially represented in terms of an (infinite-dimensional) Hilbert-Schmidt operator mapping a Hilbert space of functions to another. For numerous machine learning tasks, data can be naturally viewed as samples drawn from spaces of functions, such as curves and surfaces, in high dimensions. Optimal transport for functional data analysis provides a useful framework of treatment for such domains. { Since probability measures in infinite dimensional spaces generally lack absolute continuity (that is, with respect to non-degenerate Gaussian measures), the Monge map in the standard optimal transport theory for finite dimensional spaces may not exist. Our approach to the optimal transport problem in infinite dimensions is by a suitable regularization technique -- we restrict the class of transport maps to be a Hilbert-Schmidt space of operat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#23548;&#25968;&#35780;&#20272;&#25968;&#25454;&#21644;&#20808;&#39564;&#21327;&#26041;&#24046;&#26680;&#30340;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#23545;&#20219;&#24847;&#38454;Taylor&#23637;&#24320;&#30340;&#25130;&#26029;&#22797;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2102.00877</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#39640;&#26031;&#36807;&#31243;&#30340;&#27010;&#29575;Taylor&#23637;&#24320;
&lt;/p&gt;
&lt;p&gt;
A probabilistic Taylor expansion with Gaussian processes. (arXiv:2102.00877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.00877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#23548;&#25968;&#35780;&#20272;&#25968;&#25454;&#21644;&#20808;&#39564;&#21327;&#26041;&#24046;&#26680;&#30340;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#23545;&#20219;&#24847;&#38454;Taylor&#23637;&#24320;&#30340;&#25130;&#26029;&#22797;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#39640;&#26031;&#36807;&#31243;&#65292;&#23545;&#20110;&#29305;&#23450;&#36873;&#25321;&#30340;&#25968;&#25454;&#65292;&#21518;&#39564;&#22343;&#20540;&#22797;&#21046;&#20102;&#25130;&#26029;&#30340;&#20219;&#24847;&#38454;Taylor&#23637;&#24320;&#12290;&#25968;&#25454;&#30001;&#22312;&#23637;&#24320;&#28857;&#22788;&#30340;&#23548;&#25968;&#35780;&#20272;&#32452;&#25104;&#65292;&#20808;&#39564;&#21327;&#26041;&#24046;&#26680;&#23646;&#20110;Taylor&#26680;&#30340;&#31867;&#65292;&#21487;&#20197;&#29992;&#19968;&#23450;&#30340;&#24130;&#32423;&#25968;&#24418;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#35752;&#35770;&#24182;&#35777;&#26126;&#20102;&#20851;&#20110;Taylor&#26680;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#19968;&#20123;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#22522;&#20110;&#20855;&#26377;&#22312;&#21327;&#26041;&#24046;&#26680;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#27491;&#20132;&#25968;&#25454;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#29305;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a class of Gaussian processes for which the posterior mean, for a particular choice of data, replicates a truncated Taylor expansion of any order. The data consist of derivative evaluations at the expansion point and the prior covariance kernel belongs to the class of Taylor kernels, which can be written in a certain power series form. We discuss and prove some results on maximum likelihood estimation of parameters of Taylor kernels. The proposed framework is a special case of Gaussian process regression based on data that is orthogonal in the reproducing kernel Hilbert space of the covariance kernel.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20540;&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#20272;&#35745;&#22120;&#65292;&#21487;&#24191;&#27867;&#36866;&#29992;&#20110;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#22810;&#32500;&#25903;&#25345;&#21644;&#20219;&#24847;&#28151;&#21512;&#20559;&#23548;&#25968;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#24378;&#30340;&#35823;&#24046;&#30028;&#12290;</title><link>http://arxiv.org/abs/2006.01350</link><description>&lt;p&gt;
&#20351;&#29992;&#25554;&#20540;&#26680;&#23725;&#22238;&#24402;&#20272;&#35745;&#23548;&#25968;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Estimation of Derivatives Using Plug-in Kernel Ridge Regression Estimators. (arXiv:2006.01350v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.01350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25554;&#20540;&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#20272;&#35745;&#22120;&#65292;&#21487;&#24191;&#27867;&#36866;&#29992;&#20110;&#38750;&#21442;&#25968;&#22238;&#24402;&#20013;&#30340;&#22810;&#32500;&#25903;&#25345;&#21644;&#20219;&#24847;&#28151;&#21512;&#20559;&#23548;&#25968;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#24378;&#30340;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#22238;&#24402;&#20989;&#25968;&#30340;&#23548;&#25968;&#36827;&#34892;&#20272;&#35745;&#30340;&#38382;&#39064;&#65292;&#36825;&#22312;&#26410;&#30693;&#20989;&#25968;&#30340;&#38750;&#21442;&#25968;&#21270;&#21151;&#33021;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26631;&#20934;&#30340;&#20998;&#26512;&#21487;&#33021;&#38024;&#23545;&#29305;&#23450;&#30340;&#23548;&#25968;&#38454;&#25968;&#36827;&#34892;&#35843;&#25972;&#65292;&#32780;&#21442;&#25968;&#35843;&#20248;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#38454;&#23548;&#25968;&#26469;&#35828;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25554;&#20540;&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#20855;&#26377;&#38543;&#26426;&#35774;&#35745;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#22810;&#32500;&#25903;&#25345;&#21644;&#20219;&#24847;&#28151;&#21512;&#20559;&#23548;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#38750;&#28176;&#36817;&#20998;&#26512;&#65292;&#20197;&#32479;&#19968;&#22320;&#30740;&#31350;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#30340;&#34892;&#20026;&#65292;&#21253;&#25324;&#22238;&#24402;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#65292;&#22312;&#24378;L&#8734;&#33539;&#25968;&#19979;&#23548;&#33268;&#20102;&#19968;&#20010;&#19968;&#33324;&#31867;&#30340;&#26680;&#20989;&#25968;&#30340;&#20004;&#20010;&#35823;&#24046;&#30028;&#12290;&#22312;&#19968;&#20010;&#20855;&#20307;&#30340;&#20363;&#23376;&#20013;&#65292;&#35813;&#20272;&#35745;&#22120;&#19987;&#38376;&#38024;&#23545;&#20855;&#26377;&#22810;&#39033;&#24335;&#34928;&#20943;&#29305;&#24449;&#20540;&#30340;&#26680;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#21270;&#30340;&#26368;&#20248;&#36895;&#29575;&#65292;&#21482;&#26377;&#19968;&#20010;&#23545;&#25968;&#22240;&#23376;&#21487;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
We study the problem of estimating the derivatives of a regression function, which has a wide range of applications as a key nonparametric functional of unknown functions. Standard analysis may be tailored to specific derivative orders, and parameter tuning remains a daunting challenge particularly for high-order derivatives. In this article, we propose a simple plug-in kernel ridge regression (KRR) estimator in nonparametric regression with random design that is broadly applicable for multi-dimensional support and arbitrary mixed-partial derivatives. We provide a non-asymptotic analysis to study the behavior of the proposed estimator in a unified manner that encompasses the regression function and its derivatives, leading to two error bounds for a general class of kernels under the strong $L_\infty$ norm. In a concrete example specialized to kernels with polynomially decaying eigenvalues, the proposed estimator recovers the minimax optimal rate up to a logarithmic factor for estimatin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#26465;&#20214;&#22270;&#29983;&#25104;&#26041;&#27861;(TSGG-GAN)&#65292;&#36890;&#36807;&#32467;&#21512;&#20016;&#23500;&#30340;&#33410;&#28857;&#32423;&#19978;&#19979;&#25991;&#32467;&#26500;&#26469;&#25512;&#26029;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#22270;&#12290;</title><link>http://arxiv.org/abs/2003.01436</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#23398;&#20064;&#29983;&#25104;&#26102;&#38388;&#31995;&#21015;&#26465;&#20214;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Time Series Conditioned Graphs with Generative Adversarial Nets. (arXiv:2003.01436v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.01436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#26465;&#20214;&#22270;&#29983;&#25104;&#26041;&#27861;(TSGG-GAN)&#65292;&#36890;&#36807;&#32467;&#21512;&#20016;&#23500;&#30340;&#33410;&#28857;&#32423;&#19978;&#19979;&#25991;&#32467;&#26500;&#26469;&#25512;&#26029;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#24314;&#27169;&#21644;&#29983;&#25104;&#31526;&#21512;&#19981;&#21516;&#20998;&#24067;&#30340;&#22270;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#26159;&#22522;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#12289;&#26080;&#26465;&#20214;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#25110;&#32773;&#20165;&#22522;&#20110;&#22270;&#32423;&#19978;&#19979;&#25991;&#26465;&#20214;&#29983;&#25104;&#65292;&#36825;&#19982;&#20016;&#23500;&#30340;&#35821;&#20041;&#33410;&#28857;&#32423;&#19978;&#19979;&#25991;&#26080;&#20851;&#12290;&#19981;&#21516;&#22320;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#21517;&#20026;&#26102;&#38388;&#24207;&#21015;&#26465;&#20214;&#22270;&#29983;&#25104;&#30340;&#26032;&#38382;&#39064;&#24863;&#20852;&#36259;&#65306;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25512;&#26029;&#19968;&#20010;&#30446;&#26631;&#20851;&#31995;&#22270;&#65292;&#35813;&#22270;&#24314;&#27169;&#20102;&#26102;&#38388;&#24207;&#21015;&#20043;&#38388;&#30340;&#28508;&#22312;&#30456;&#20114;&#20851;&#31995;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#23545;&#24212;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#30740;&#31350;&#20197;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#26465;&#20214;&#30340;&#26576;&#31181;&#30142;&#30149;&#30340;&#22522;&#22240;&#35843;&#25511;&#32593;&#32476;&#20013;&#22522;&#22240;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24207;&#21015;&#26465;&#20214;&#22270;&#29983;&#25104;-&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;TSGG-GAN&#65289;&#26469;&#22788;&#29702;&#20016;&#23500;&#30340;&#33410;&#28857;&#32423;&#19978;&#19979;&#25991;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning based approaches have been utilized to model and generate graphs subjected to different distributions recently. However, they are typically unsupervised learning based and unconditioned generative models or simply conditioned on the graph-level contexts, which are not associated with rich semantic node-level contexts. Differently, in this paper, we are interested in a novel problem named Time Series Conditioned Graph Generation: given an input multivariate time series, we aim to infer a target relation graph modeling the underlying interrelationships between time series with each node corresponding to each time series. For example, we can study the interrelationships between genes in a gene regulatory network of a certain disease conditioned on their gene expression data recorded as time series. To achieve this, we propose a novel Time Series conditioned Graph Generation-Generative Adversarial Networks (TSGG-GAN) to handle challenges of rich node-level context structures 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20915;&#31574;&#26641;&#26041;&#27861;&#36873;&#25321;&#21512;&#36866;&#20013;&#24515;&#24230;&#24230;&#37327;&#26041;&#27861;&#30340;&#32553;&#20943;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#20915;&#31574;&#26641;&#35843;&#26597;&#65292;&#32467;&#21512;&#19987;&#23478;&#20559;&#22909;&#65292;&#33021;&#22815;&#22312;&#36739;&#23567;&#30340;&#22270;&#24418;&#25968;&#37327;&#19979;&#24555;&#36895;&#31579;&#36873;&#20986;&#26368;&#21512;&#36866;&#30340;&#20013;&#24515;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2003.01052</link><description>&lt;p&gt;
&#22914;&#20309;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#20013;&#24515;&#24230;&#27979;&#37327;&#26041;&#27861;&#65311;&#19968;&#31181;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to choose the most appropriate centrality measure? A decision tree approach. (arXiv:2003.01052v5 [physics.soc-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.01052
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20915;&#31574;&#26641;&#26041;&#27861;&#36873;&#25321;&#21512;&#36866;&#20013;&#24515;&#24230;&#24230;&#37327;&#26041;&#27861;&#30340;&#32553;&#20943;&#26041;&#27861;&#12290;&#36890;&#36807;&#26500;&#24314;&#20915;&#31574;&#26641;&#35843;&#26597;&#65292;&#32467;&#21512;&#19987;&#23478;&#20559;&#22909;&#65292;&#33021;&#22815;&#22312;&#36739;&#23567;&#30340;&#22270;&#24418;&#25968;&#37327;&#19979;&#24555;&#36895;&#31579;&#36873;&#20986;&#26368;&#21512;&#36866;&#30340;&#20013;&#24515;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24515;&#24230;&#24230;&#37327;&#22312;&#32593;&#32476;&#20998;&#26512;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;400&#22810;&#31181;&#25552;&#20986;&#30340;&#25351;&#26631;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;-&#22522;&#20110;&#27169;&#22411;&#12289;&#25968;&#25454;&#39537;&#21160;&#21644;&#20844;&#29702;&#24615;-&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32553;&#20943;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#23545;&#31616;&#21333;&#22270;&#20013;&#30340;&#20013;&#24515;&#24230;&#34892;&#20026;&#30340;&#20559;&#22909;&#12290;&#23427;&#28041;&#21450;&#24418;&#25104;&#19968;&#32452;&#20505;&#36873;&#27979;&#37327;&#26041;&#27861;&#65292;&#29983;&#25104;&#23613;&#21487;&#33021;&#23567;&#30340;&#22270;&#24418;&#20197;&#8220;&#20998;&#31163;&#8221;&#21508;&#31181;&#27979;&#37327;&#26041;&#27861;&#65292;&#26500;&#24314;&#20915;&#31574;&#26641;&#35843;&#26597;&#65292;&#24182;&#30830;&#23450;&#19982;&#19987;&#23478;&#22238;&#31572;&#19968;&#33268;&#30340;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#21253;&#25324;&#26032;&#30340;&#22522;&#20110;&#26680;&#30340;&#27979;&#37327;&#26041;&#27861;&#22312;&#20869;&#30340;40&#31181;&#19981;&#21516;&#20013;&#24515;&#24230;&#65292;&#21516;&#26102;&#19982;&#20844;&#29702;&#24615;&#26041;&#27861;&#32467;&#21512;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21482;&#26377;13&#20010;&#23567;&#22411;1-&#26641;&#36275;&#20197;&#20998;&#31163;&#25152;&#26377;40&#20010;&#27979;&#37327;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#25509;&#36817;&#30340;&#19968;&#23545;&#12290;&#32553;&#20943;&#26041;&#27861;&#22312;&#21171;&#21160;&#21147;&#21644;&#26102;&#38388;&#26041;&#38754;&#25552;&#20379;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#24050;&#26377;&#30340;&#27979;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Centrality metrics are vital for network analysis, but selecting the most appropriate measures for specific applications remains challenging among the 400+ proposed indices. Existing approaches -- model-based, data-driven, and axiomatic -- have limitations. To address this, we introduce the culling method, leveraging expert preferences regarding centrality behavior on simple graphs. It involves forming a set of candidate measures, generating a list of as small graphs as possible needed to ``separate'' measures from each other, constructing a decision-tree survey, and identifying the measure consistent with expert responses. We apply this method to a diverse set of 40 centralities, including new kernel-based measures, and combine it with the axiomatic approach. Remarkably, only 13 small 1-trees suffice to separate all 40 measures, among which there are pairs of close ones. The culling method offers a low-cost solution in terms of labor and time, complements existing methods for measure 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;&#30340;&#29616;&#26377;&#20316;&#21697;&#65292;&#20171;&#32461;&#20102;&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;&#30340;&#32972;&#26223;&#12289;&#22522;&#20934;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#27969;&#34892;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/1901.07474</link><description>&lt;p&gt;
&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;: &#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Pedestrian Attribute Recognition: A Survey. (arXiv:1901.07474v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1901.07474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;&#30340;&#29616;&#26377;&#20316;&#21697;&#65292;&#20171;&#32461;&#20102;&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;&#30340;&#32972;&#26223;&#12289;&#22522;&#20934;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#20197;&#21450;&#27969;&#34892;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#35270;&#39057;&#30417;&#25511;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#35782;&#21035;&#34892;&#20154;&#23646;&#24615;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#35768;&#22810;&#31639;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#22788;&#29702;&#36825;&#20010;&#20219;&#21153;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#22238;&#39038;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#25110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#29616;&#26377;&#20316;&#21697;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;&#30340;&#32972;&#26223;&#65292;&#21253;&#25324;&#34892;&#20154;&#23646;&#24615;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#30456;&#24212;&#30340;&#25361;&#25112;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#29616;&#26377;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26631;&#20934;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#24182;&#35299;&#37322;&#20102;&#36825;&#20004;&#31181;&#23398;&#20064;&#31639;&#27861;&#19982;&#34892;&#20154;&#23646;&#24615;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#19968;&#20123;&#27969;&#34892;&#32593;&#32476;&#26550;&#26500;&#12290;&#31532;&#22235;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27492;&#20219;&#21153;&#30340;&#27969;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#23646;&#24615;&#32452;&#21644;&#22522;&#20110;&#37096;&#20998;&#30340;&#26041;&#27861;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing pedestrian attributes is an important task in the computer vision community due to it plays an important role in video surveillance. Many algorithms have been proposed to handle this task. The goal of this paper is to review existing works using traditional methods or based on deep learning networks. Firstly, we introduce the background of pedestrian attribute recognition (PAR, for short), including the fundamental concepts of pedestrian attributes and corresponding challenges. Secondly, we introduce existing benchmarks, including popular datasets and evaluation criteria. Thirdly, we analyze the concept of multi-task learning and multi-label learning and also explain the relations between these two learning algorithms and pedestrian attribute recognition. We also review some popular network architectures which have been widely applied in the deep learning community. Fourthly, we analyze popular solutions for this task, such as attributes group, part-based, etc. Fifthly, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#23481;&#37327;&#22823;&#12289;&#22797;&#26434;&#24615;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#31639;&#27861;&#19981;&#31283;&#23450;&#24615;&#12289;&#38750;&#40065;&#26834;&#24615;&#21644;&#23574;&#38160;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/1710.05468</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Generalization in Deep Learning. (arXiv:1710.05468v8 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1710.05468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#23481;&#37327;&#22823;&#12289;&#22797;&#26434;&#24615;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#31639;&#27861;&#19981;&#31283;&#23450;&#24615;&#12289;&#38750;&#40065;&#26834;&#24615;&#21644;&#23574;&#38160;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#28145;&#24230;&#23398;&#20064;&#33021;&#22815;&#22312;&#23481;&#37327;&#22823;&#12289;&#22797;&#26434;&#24615;&#39640;&#12289;&#21487;&#33021;&#23384;&#22312;&#31639;&#27861;&#19981;&#31283;&#23450;&#24615;&#12289;&#38750;&#40065;&#26834;&#24615;&#21644;&#23574;&#38160;&#26497;&#23567;&#20540;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#65292;&#22238;&#24212;&#20102;&#25991;&#29486;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25552;&#20379;&#28145;&#24230;&#23398;&#20064;&#38750;&#34394;&#31354;&#27867;&#21270;&#20445;&#35777;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#29702;&#35770;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#30740;&#31350;&#32467;&#26524;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.
&lt;/p&gt;</description></item></channel></rss>