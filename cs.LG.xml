<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32806;&#36807;&#31243;&#26469;&#21457;&#29616;&#21487;&#20197;&#19982;&#20027;&#20219;&#21153;&#19968;&#36215;&#21033;&#29992;&#30340;&#19981;&#30456;&#20851;&#30340;&#20998;&#31867;&#20219;&#21153;&#21644;&#30456;&#20851;&#26631;&#31614;&#65292;&#20174;&#32780;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20419;&#36827;&#36741;&#21161;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.09278</link><description>&lt;p&gt;
&#35299;&#32806;&#28508;&#22312;&#31354;&#38388;&#20419;&#36827;&#25968;&#25454;&#39537;&#21160;&#30340;&#36741;&#21161;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Latent Spaces Facilitate Data-Driven Auxiliary Learning. (arXiv:2310.09278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#32806;&#36807;&#31243;&#26469;&#21457;&#29616;&#21487;&#20197;&#19982;&#20027;&#20219;&#21153;&#19968;&#36215;&#21033;&#29992;&#30340;&#19981;&#30456;&#20851;&#30340;&#20998;&#31867;&#20219;&#21153;&#21644;&#30456;&#20851;&#26631;&#31614;&#65292;&#20174;&#32780;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20419;&#36827;&#36741;&#21161;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#36741;&#21161;&#30446;&#26631;&#24120;&#24120;&#34987;&#29992;&#26469;&#22312;&#25968;&#25454;&#31232;&#32570;&#25110;&#32773;&#20027;&#35201;&#20219;&#21153;&#38750;&#24120;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20419;&#36827;&#23398;&#20064;&#12290;&#36825;&#20010;&#24819;&#27861;&#20027;&#35201;&#21463;&#21040;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#24102;&#26469;&#30340;&#25913;&#36827;&#27867;&#21270;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#24378;&#22823;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25214;&#21040;&#33021;&#20135;&#29983;&#26399;&#26395;&#25913;&#36827;&#30340;&#26368;&#20248;&#36741;&#21161;&#20219;&#21153;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#30340;&#25216;&#24039;&#25110;&#32773;&#26114;&#36149;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Detaux&#65292;&#36890;&#36807;&#24369;&#30417;&#30563;&#30340;&#35299;&#32806;&#36807;&#31243;&#22312;&#20219;&#20309;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#27169;&#22411;&#20013;&#21457;&#29616;&#21487;&#20197;&#19982;&#20027;&#35201;&#20219;&#21153;&#19968;&#36215;&#21033;&#29992;&#30340;&#19981;&#30456;&#20851;&#30340;&#20998;&#31867;&#20219;&#21153;&#21644;&#30456;&#20851;&#26631;&#31614;&#12290;&#35299;&#32806;&#36807;&#31243;&#22312;&#34920;&#31034;&#23618;&#38754;&#24037;&#20316;&#65292;&#23558;&#19982;&#20027;&#35201;&#20219;&#21153;&#30456;&#20851;&#30340;&#19968;&#20010;&#23376;&#31354;&#38388;&#19982;&#20219;&#24847;&#25968;&#37327;&#30340;&#27491;&#20132;&#23376;&#31354;&#38388;&#20998;&#31163;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, auxiliary objectives are often used to facilitate learning in situations where data is scarce, or the principal task is extremely complex. This idea is primarily inspired by the improved generalization capability induced by solving multiple tasks simultaneously, which leads to a more robust shared representation. Nevertheless, finding optimal auxiliary tasks that give rise to the desired improvement is a crucial problem that often requires hand-crafted solutions or expensive meta-learning approaches. In this paper, we propose a novel framework, dubbed Detaux, whereby a weakly supervised disentanglement procedure is used to discover new unrelated classification tasks and the associated labels that can be exploited with the principal task in any Multi-Task Learning (MTL) model. The disentanglement procedure works at a representation level, isolating a subspace related to the principal task, plus an arbitrary number of orthogonal subspaces. In the most disentangled subsp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28151;&#21512;&#38543;&#26426;&#26862;&#26519;-&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35780;&#20272;&#25233;&#37057;&#24739;&#32773;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#26469;&#20998;&#31867;&#25233;&#37057;&#30151;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;80%&#12290;</title><link>http://arxiv.org/abs/2310.09277</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25233;&#37057;&#30151;&#20998;&#31867;&#30340;&#28151;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#36816;&#21160;&#27963;&#21160;&#20449;&#21495;&#30340;&#38543;&#26426;&#26862;&#26519;-&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Approach for Depression Classification: Random Forest-ANN Ensemble on Motor Activity Signals. (arXiv:2310.09277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28151;&#21512;&#38543;&#26426;&#26862;&#26519;-&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35780;&#20272;&#25233;&#37057;&#24739;&#32773;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#26469;&#20998;&#31867;&#25233;&#37057;&#30151;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#24403;&#20170;&#31038;&#20250;&#24739;&#26377;&#24515;&#29702;&#20581;&#24247;&#30142;&#30149;&#30340;&#20154;&#25968;&#19981;&#26029;&#22686;&#21152;&#65292;&#24515;&#29702;&#20581;&#24247;&#30340;&#37325;&#35201;&#24615;&#19981;&#35328;&#32780;&#21947;&#12290;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#65292;&#36825;&#20123;&#20256;&#24863;&#22120;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#36861;&#36394;&#21644;&#29702;&#35299;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;&#36825;&#20123;&#35774;&#22791;&#19981;&#20165;&#30417;&#27979;&#26085;&#24120;&#27963;&#21160;&#65292;&#36824;&#25345;&#32493;&#35760;&#24405;&#35832;&#22914;&#24515;&#29575;&#31561;&#29983;&#21629;&#20307;&#24449;&#65292;&#21487;&#33021;&#25552;&#20379;&#26377;&#20851;&#19968;&#20010;&#20154;&#24515;&#29702;&#29366;&#24577;&#30340;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#36825;&#20123;&#20256;&#24863;&#22120;&#19982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#65292;&#35782;&#21035;&#19982;&#19981;&#21516;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#30456;&#20851;&#30340;&#27169;&#24335;&#65292;&#31361;&#26174;&#20102;&#36825;&#20123;&#25968;&#25454;&#22312;&#31616;&#21333;&#27963;&#21160;&#30417;&#27979;&#20197;&#22806;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#35813;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#28151;&#21512;&#38543;&#26426;&#26862;&#26519; - &#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#25233;&#37057;&#24739;&#32773;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#29305;&#27530;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#22312;&#21253;&#25324;&#21333;&#30456;&#21644;&#21452;&#30456;&#25233;&#37057;&#24739;&#32773;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;80%&#30340;&#26174;&#33879;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regarding the rising number of people suffering from mental health illnesses in today's society, the importance of mental health cannot be overstated. Wearable sensors, which are increasingly widely available, provide a potential way to track and comprehend mental health issues. These gadgets not only monitor everyday activities but also continuously record vital signs like heart rate, perhaps providing information on a person's mental state. Recent research has used these sensors in conjunction with machine learning methods to identify patterns relating to different mental health conditions, highlighting the immense potential of this data beyond simple activity monitoring. In this research, we present a novel algorithm called the Hybrid Random forest - Neural network that has been tailored to evaluate sensor data from depressed patients. Our method has a noteworthy accuracy of 80\% when evaluated on a special dataset that included both unipolar and bipolar depressive patients as well 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36870;&#21512;&#25104;&#20219;&#21153;&#22312;&#23454;&#39564;&#23460;&#25191;&#34892;&#21487;&#34892;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#36807;&#31243;&#30340;&#34920;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Retro-fallback &#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#22823;&#21270;&#23454;&#39564;&#23460;&#21487;&#25191;&#34892;&#30340;&#21512;&#25104;&#35745;&#21010;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.09270</link><description>&lt;p&gt;
Retro-fallback: &#38754;&#21521;&#19981;&#30830;&#23450;&#19990;&#30028;&#30340;&#36870;&#21512;&#25104;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Retro-fallback: retrosynthetic planning in an uncertain world. (arXiv:2310.09270v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36870;&#21512;&#25104;&#20219;&#21153;&#22312;&#23454;&#39564;&#23460;&#25191;&#34892;&#21487;&#34892;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#36807;&#31243;&#30340;&#34920;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Retro-fallback &#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#22823;&#21270;&#23454;&#39564;&#23460;&#21487;&#25191;&#34892;&#30340;&#21512;&#25104;&#35745;&#21010;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21512;&#25104;&#26159;&#36890;&#36807;&#25552;&#20986;&#19968;&#31995;&#21015;&#21270;&#23398;&#21453;&#24212;&#20174;&#26356;&#31616;&#21333;&#12289;&#21487;&#36141;&#20080;&#30340;&#20998;&#23376;&#21019;&#24314;&#25152;&#38656;&#20998;&#23376;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#31639;&#27861;&#26469;&#23547;&#25214;&#19968;&#31995;&#21015;&#24230;&#37327;&#25351;&#26631;&#65288;&#20363;&#22914;&#26368;&#30701;&#36335;&#24452;&#12289;&#26368;&#20302;&#25104;&#26412;&#65289;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#24573;&#35270;&#20102;&#25105;&#20204;&#23545;&#21487;&#33021;&#21453;&#24212;&#31354;&#38388;&#30340;&#19981;&#23436;&#20840;&#20102;&#35299;&#65292;&#36825;&#24847;&#21619;&#30528;&#31639;&#27861;&#29983;&#25104;&#30340;&#35745;&#21010;&#21487;&#33021;&#22312;&#23454;&#39564;&#23460;&#20013;&#26080;&#27861;&#23454;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#36807;&#31243;&#30340;&#36870;&#21512;&#25104;&#26032;&#39062;&#34920;&#36848;&#65292;&#20197;&#32771;&#34385;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36138;&#23146;&#31639;&#27861;&#31216;&#20026; Retro-fallback&#65292;&#26368;&#22823;&#21270;&#33267;&#23569;&#26377;&#19968;&#31181;&#21512;&#25104;&#35745;&#21010;&#33021;&#22312;&#23454;&#39564;&#23460;&#20013;&#25191;&#34892;&#30340;&#27010;&#29575;&#12290;&#20351;&#29992;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126; Retro-fallback &#36890;&#24120;&#29983;&#25104;&#27604;&#27969;&#34892;&#30340; MCTS &#21644; retro* &#31639;&#27861;&#26356;&#22909;&#30340;&#19968;&#32452;&#21512;&#25104;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36951;&#20256;&#31639;&#27861;&#26159;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#30340;&#24378;&#22823;&#22522;&#20934;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;GA&#20934;&#21017;&#65292;&#35201;&#27714;&#26032;&#30340;&#31639;&#27861;&#24517;&#39035;&#26126;&#26174;&#20248;&#20110;&#36951;&#20256;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09267</link><description>&lt;p&gt;
&#36951;&#20256;&#31639;&#27861;&#26159;&#20998;&#23376;&#29983;&#25104;&#30340;&#24378;&#22823;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Genetic algorithms are strong baselines for molecule generation. (arXiv:2310.09267v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36951;&#20256;&#31639;&#27861;&#26159;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#30340;&#24378;&#22823;&#22522;&#20934;&#32447;&#65292;&#24182;&#25552;&#20986;&#20102;GA&#20934;&#21017;&#65292;&#35201;&#27714;&#26032;&#30340;&#31639;&#27861;&#24517;&#39035;&#26126;&#26174;&#20248;&#20110;&#36951;&#20256;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20998;&#23376;&#65292;&#26080;&#35770;&#26159;&#26377;&#21521;&#36824;&#26159;&#26080;&#21521;&#30340;&#26041;&#24335;&#65292;&#26159;&#33647;&#29289;&#30740;&#21457;&#27969;&#31243;&#20013;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#36951;&#20256;&#31639;&#27861;&#65288;GAs&#65289;&#36890;&#36807;&#38543;&#26426;&#20462;&#25913;&#24050;&#30693;&#20998;&#23376;&#26469;&#29983;&#25104;&#20998;&#23376;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#20110;&#36825;&#31181;&#20219;&#21153;&#65292;GAs&#26159;&#38750;&#24120;&#24378;&#22823;&#30340;&#31639;&#27861;&#65292;&#32988;&#36807;&#35768;&#22810;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#19968;&#32467;&#26524;&#21487;&#33021;&#35753;&#24456;&#22810;&#30740;&#31350;&#20154;&#21592;&#24863;&#21040;&#24778;&#35766;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#22362;&#25345;&#35201;&#27714;&#26032;&#30340;&#31639;&#27861;&#24517;&#39035;&#26126;&#26174;&#20248;&#20110;GAs&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;GA&#20934;&#21017;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#35768;&#22810;&#20851;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#30740;&#31350;&#24212;&#35813;&#37325;&#26032;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating molecules, both in a directed and undirected fashion, is a huge part of the drug discovery pipeline. Genetic algorithms (GAs) generate molecules by randomly modifying known molecules. In this paper we show that GAs are very strong algorithms for such tasks, outperforming many complicated machine learning methods: a result which many researchers may find surprising. We therefore propose insisting during peer review that new algorithms must have some clear advantage over GAs, which we call the GA criterion. Ultimately our work suggests that a lot of research in molecule generation should be re-assessed.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;&#65292;&#21457;&#29616;LLMs&#23545;&#20110;&#21508;&#31181;&#24494;&#35843;&#25968;&#25454;&#38598;&#37117;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31163;&#32676;&#29992;&#25143;&#21644;&#36129;&#29486;&#22823;&#37327;&#25968;&#25454;&#30340;&#29992;&#25143;&#12290;&#36825;&#23545;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.09266</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
User Inference Attacks on Large Language Models. (arXiv:2310.09266v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;&#65292;&#21457;&#29616;LLMs&#23545;&#20110;&#21508;&#31181;&#24494;&#35843;&#25968;&#25454;&#38598;&#37117;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31163;&#32676;&#29992;&#25143;&#21644;&#36129;&#29486;&#22823;&#37327;&#25968;&#25454;&#30340;&#29992;&#25143;&#12290;&#36825;&#23545;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23450;&#21046;&#20026;&#19987;&#19994;&#20219;&#21153;&#21644;&#24212;&#29992;&#30340;&#24120;&#35265;&#26377;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#24494;&#35843;LLMs&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#31216;&#20026;&#29992;&#25143;&#25512;&#29702;&#30340;&#29616;&#23454;&#23041;&#32961;&#27169;&#22411;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#25512;&#26029;&#20986;&#29992;&#25143;&#30340;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#20110;&#24494;&#35843;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#31181;&#23041;&#32961;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#21482;&#38656;&#35201;&#20174;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#19968;&#23567;&#32452;&#26679;&#26412;&#65288;&#21487;&#33021;&#19982;&#29992;&#20110;&#35757;&#32451;&#30340;&#26679;&#26412;&#19981;&#21516;&#65289;&#21644;&#23545;&#24494;&#35843;LLM&#30340;&#40657;&#30418;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#22312;&#21508;&#31181;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#26131;&#21463;&#29992;&#25143;&#25512;&#29702;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#26377;&#26102;&#25915;&#20987;&#25104;&#21151;&#29575;&#25509;&#36817;&#23436;&#32654;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21738;&#20123;&#29305;&#24615;&#20351;&#29992;&#25143;&#23481;&#26131;&#21463;&#21040;&#29992;&#25143;&#25512;&#29702;&#30340;&#25915;&#20987;&#65292;&#21457;&#29616;&#31163;&#32676;&#29992;&#25143;&#65288;&#21363;&#25968;&#25454;&#20998;&#24067;&#19982;&#20854;&#20182;&#29992;&#25143;&#26126;&#26174;&#19981;&#21516;&#65289;&#21644;&#36129;&#29486;&#22823;&#37327;&#25968;&#25454;&#30340;&#29992;&#25143;&#26356;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35299;&#20915;&#36825;&#31181;&#25915;&#20987;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is a common and effective method for tailoring large language models (LLMs) to specialized tasks and applications. In this paper, we study the privacy implications of fine-tuning LLMs on user data. To this end, we define a realistic threat model, called user inference, wherein an attacker infers whether or not a user's data was used for fine-tuning. We implement attacks for this threat model that require only a small set of samples from a user (possibly different from the samples used for training) and black-box access to the fine-tuned LLM. We find that LLMs are susceptible to user inference attacks across a variety of fine-tuning datasets, at times with near perfect attack success rates. Further, we investigate which properties make users vulnerable to user inference, finding that outlier users (i.e. those with data distributions sufficiently different from other users) and users who contribute large quantities of data are most susceptible to attack. Finally, we explore s
&lt;/p&gt;</description></item><item><title>PromptRE&#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#25991;&#26723;&#32423;&#21035;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#8220;&#27809;&#26377;&#20851;&#31995;&#8221;&#30340;&#23454;&#20363;&#25968;&#37327;&#19981;&#24179;&#34913;&#21644;&#30452;&#25509;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09265</link><description>&lt;p&gt;
PromptRE: &#22522;&#20110;&#25552;&#31034;&#30340;&#25968;&#25454;&#32534;&#31243;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
PromptRE: Weakly-Supervised Document-Level Relation Extraction via Prompting-Based Data Programming. (arXiv:2310.09265v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09265
&lt;/p&gt;
&lt;p&gt;
PromptRE&#26159;&#19968;&#31181;&#24369;&#30417;&#30563;&#25991;&#26723;&#32423;&#21035;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#8220;&#27809;&#26377;&#20851;&#31995;&#8221;&#30340;&#23454;&#20363;&#25968;&#37327;&#19981;&#24179;&#34913;&#21644;&#30452;&#25509;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#23558;&#20004;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#20998;&#31867;&#21040;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21477;&#32423;&#21035;&#30340;&#20851;&#31995;&#25277;&#21462;&#19978;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#33539;&#22260;&#25193;&#22823;&#21040;&#25991;&#26723;&#32423;&#21035;&#30340;&#20851;&#31995;&#25277;&#21462;&#12290;&#20256;&#32479;&#30340;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20943;&#23569;&#23545;&#25163;&#21160;&#26631;&#27880;&#30340;&#38656;&#27714;&#65292;&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#24369;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#21477;&#32423;&#21035;&#30340;&#20851;&#31995;&#25277;&#21462;&#65292;&#20294;&#22312;&#25991;&#26723;&#32423;&#21035;&#30340;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#20173;&#28982;&#26377;&#38480;&#30340;&#24037;&#20316;&#12290;&#24369;&#30417;&#30563;&#30340;&#25991;&#26723;&#32423;&#21035;&#20851;&#31995;&#25277;&#21462;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#27604;&#22914;&#8220;&#27809;&#26377;&#20851;&#31995;&#8221;&#30340;&#23454;&#20363;&#25968;&#37327;&#19981;&#24179;&#34913;&#65292;&#20197;&#21450;&#30452;&#25509;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#22833;&#36133;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptRE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#24369;&#30417;&#30563;&#25991;&#26723;&#32423;&#21035;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims to classify the relationships between two entities into pre-defined categories. While previous research has mainly focused on sentence-level relation extraction, recent studies have expanded the scope to document-level relation extraction. Traditional relation extraction methods heavily rely on human-annotated training data, which is time-consuming and labor-intensive. To mitigate the need for manual annotation, recent weakly-supervised approaches have been developed for sentence-level relation extraction while limited work has been done on document-level relation extraction. Weakly-supervised document-level relation extraction faces significant challenges due to an imbalanced number "no relation" instances and the failure of directly probing pretrained large language models for document relation extraction. To address these challenges, we propose PromptRE, a novel weakly-supervised document-level relation extraction method that combines prompting-based techniq
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21517;&#20026;QUIK&#30340;&#28151;&#21512;&#37327;&#21270;&#31574;&#30053;&#65292;&#22312;&#20445;&#25345;&#33391;&#22909;&#31934;&#24230;&#30340;&#21516;&#26102;&#23454;&#29616;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#38469;&#36895;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#23558;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#36716;&#25442;&#20026;4&#20301;&#65292;&#24182;&#25552;&#20379;&#39640;&#25928;&#29575;&#30340;&#36880;&#23618;&#36816;&#34892;&#26102;GPU&#20869;&#26680;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;3.1&#20493;&#30340;&#23454;&#38469;&#31471;&#21040;&#31471;&#21534;&#21520;&#37327;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.09259</link><description>&lt;p&gt;
&#36808;&#21521;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;4&#20301;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards End-to-end 4-Bit Inference on Generative Large Language Models. (arXiv:2310.09259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21517;&#20026;QUIK&#30340;&#28151;&#21512;&#37327;&#21270;&#31574;&#30053;&#65292;&#22312;&#20445;&#25345;&#33391;&#22909;&#31934;&#24230;&#30340;&#21516;&#26102;&#23454;&#29616;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#38469;&#36895;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#23558;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#36716;&#25442;&#20026;4&#20301;&#65292;&#24182;&#25552;&#20379;&#39640;&#25928;&#29575;&#30340;&#36880;&#23618;&#36816;&#34892;&#26102;GPU&#20869;&#26680;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;3.1&#20493;&#30340;&#23454;&#38469;&#31471;&#21040;&#31471;&#21534;&#21520;&#37327;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20687;LLaMA&#21644;OPT&#36825;&#26679;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#22823;&#22810;&#25968;&#25512;&#29702;&#35745;&#31639;&#21487;&#20197;&#36890;&#36807;&#23558;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#37117;&#36716;&#25442;&#20026;4&#20301;&#26469;&#23436;&#25104;&#65292;&#36825;&#31181;&#26041;&#24335;&#21487;&#20197;&#22312;&#20445;&#25345;&#33391;&#22909;&#31934;&#24230;&#30340;&#21516;&#26102;&#23454;&#29616;&#23454;&#38469;&#36895;&#24230;&#25552;&#21319;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;QUIK&#30340;&#28151;&#21512;&#37327;&#21270;&#31574;&#30053;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#65292;&#35813;&#31574;&#30053;&#23558;&#22823;&#37096;&#20998;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#21387;&#32553;&#20026;4&#20301;&#65292;&#21516;&#26102;&#20445;&#30041;&#19968;&#20123;&#31163;&#32676;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#30340;&#36739;&#39640;&#31934;&#24230;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#32771;&#34385;&#21040;&#20102;&#35745;&#31639;&#25928;&#29575;&#65306;&#25105;&#20204;&#25552;&#20379;&#20102;&#39640;&#25928;&#29575;&#30340;&#36880;&#23618;&#36816;&#34892;&#26102;GPU&#20869;&#26680;&#65292;&#30456;&#23545;&#20110;FP16&#25191;&#34892;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;3.1&#20493;&#30340;&#23454;&#38469;&#31471;&#21040;&#31471;&#21534;&#21520;&#37327;&#25552;&#21319;&#12290;&#25105;&#20204;&#22312;https://github.com/IST-DASLab/QUIK&#19978;&#25552;&#20379;&#20102;&#20195;&#30721;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the majority of the inference computations for large generative models such as LLaMA and OPT can be performed with both weights and activations being cast to 4 bits, in a way that leads to practical speedups while at the same time maintaining good accuracy. We achieve this via a hybrid quantization strategy called QUIK, which compresses most of the weights and activations to 4-bit, while keeping some outlier weights and activations in higher-precision. Crucially, our scheme is designed with computational efficiency in mind: we provide GPU kernels with highly-efficient layer-wise runtimes, which lead to practical end-to-end throughput improvements of up to 3.1x relative to FP16 execution. Code and models are provided at https://github.com/IST-DASLab/QUIK.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#27979;&#24230;&#21040;&#27979;&#24230;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#12289;&#30830;&#23450;&#24615;&#33945;&#26684;&#26144;&#23556;&#12289;&#26144;&#23556;&#36328;&#19981;&#21487;&#27604;&#36739;&#31354;&#38388;&#21644;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.09254</link><description>&lt;p&gt;
&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#31354;&#38388;&#20869;&#22806;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Entropic Neural Optimal Transport To Map Within and Across Spaces. (arXiv:2310.09254v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09254
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#29109;&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;&#22312;&#27979;&#24230;&#21040;&#27979;&#24230;&#26144;&#23556;&#20013;&#30340;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#12289;&#30830;&#23450;&#24615;&#33945;&#26684;&#26144;&#23556;&#12289;&#26144;&#23556;&#36328;&#19981;&#21487;&#27604;&#36739;&#31354;&#38388;&#21644;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27979;&#24230;&#21040;&#27979;&#24230;&#30340;&#26144;&#23556;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#23588;&#20854;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#21344;&#25454;&#37325;&#35201;&#22320;&#20301;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#21551;&#21457;&#30340;&#25216;&#26415;&#19981;&#26029;&#28044;&#29616;&#12290;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#26041;&#27861;&#32479;&#31216;&#20026;"&#31070;&#32463;&#26368;&#20248;&#20256;&#36755;"&#65292;&#23558;&#26368;&#20248;&#20256;&#36755;&#20316;&#20026;&#24402;&#32435;&#20559;&#22909;&#65306;&#36825;&#20123;&#26144;&#23556;&#24212;&#35813;&#38024;&#23545;&#32473;&#23450;&#30340;&#25104;&#26412;&#20989;&#25968;&#26159;&#26368;&#20248;&#30340;&#65292;&#33021;&#20197;&#33410;&#32422;&#30340;&#26041;&#24335;&#65288;&#36890;&#36807;&#26368;&#23567;&#21270;&#20301;&#31227;&#65289;&#22312;&#31354;&#38388;&#20869;&#25110;&#31354;&#38388;&#38388;&#31227;&#21160;&#28857;&#12290;&#36825;&#19968;&#21407;&#21017;&#22312;&#30452;&#35266;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#20294;&#24448;&#24448;&#38754;&#20020;&#20960;&#20010;&#23454;&#38469;&#25361;&#25112;&#65292;&#38656;&#35201;&#35843;&#25972;&#26368;&#20248;&#20256;&#36755;&#24037;&#20855;&#31665;&#65306;&#22788;&#29702;&#20854;&#20182;&#38750;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#25104;&#26412;&#30340;&#25361;&#25112;&#65292;&#30830;&#23450;&#24615;&#29366;&#20917;&#19979;&#30340;&#33945;&#26684;&#26144;&#23556;&#20844;&#24335;&#20250;&#38480;&#21046;&#28789;&#27963;&#24615;&#65292;&#26144;&#23556;&#22312;&#19981;&#21487;&#27604;&#36739;&#30340;&#31354;&#38388;&#20013;&#20250;&#24102;&#26469;&#22810;&#20010;&#25361;&#25112;&#65292;&#26368;&#20248;&#20256;&#36755;&#22266;&#26377;&#30340;&#36136;&#37327;&#23432;&#24658;&#32422;&#26463;&#21487;&#33021;&#23545;&#24322;&#24120;&#25968;&#25454;&#32473;&#20104;&#36807;&#22810;&#30340;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning measure-to-measure mappings is a crucial task in machine learning, featured prominently in generative modeling. Recent years have witnessed a surge of techniques that draw inspiration from optimal transport (OT) theory. Combined with neural network models, these methods collectively known as \textit{Neural OT} use optimal transport as an inductive bias: such mappings should be optimal w.r.t. a given cost function, in the sense that they are able to move points in a thrifty way, within (by minimizing displacements) or across spaces (by being isometric). This principle, while intuitive, is often confronted with several practical challenges that require adapting the OT toolbox: cost functions other than the squared-Euclidean cost can be challenging to handle, the deterministic formulation of Monge maps leaves little flexibility, mapping across incomparable spaces raises multiple challenges, while the mass conservation constraint inherent to OT can provide too much credit to outli
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#27169;&#22411;&#38598;&#21512;&#20013;&#65292;&#23545;&#20110;&#27491;&#30830;&#20998;&#31867;&#30340;&#26679;&#26412;&#28857;&#65292;&#20559;&#24046;&#21644;&#26041;&#24046;&#22312;&#26679;&#26412;&#32423;&#21035;&#19978;&#26159;&#23545;&#40784;&#30340;&#12290;</title><link>http://arxiv.org/abs/2310.09250</link><description>&lt;p&gt;
&#23427;&#26159;&#19968;&#31181;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#26435;&#34913;&#65306;&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
It's an Alignment, Not a Trade-off: Revisiting Bias and Variance in Deep Models. (arXiv:2310.09250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09250
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#27169;&#22411;&#38598;&#21512;&#20013;&#65292;&#23545;&#20110;&#27491;&#30830;&#20998;&#31867;&#30340;&#26679;&#26412;&#28857;&#65292;&#20559;&#24046;&#21644;&#26041;&#24046;&#22312;&#26679;&#26412;&#32423;&#21035;&#19978;&#26159;&#23545;&#40784;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26234;&#24935;&#35748;&#20026;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#20998;&#35299;&#20026;&#20559;&#24046;&#21644;&#26041;&#24046;&#65292;&#24182;&#19988;&#36825;&#20004;&#20010;&#26415;&#35821;&#20043;&#38388;&#23384;&#22312;&#30528;"&#26435;&#34913;"&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#27169;&#22411;&#38598;&#21512;&#20013;&#65292;&#20559;&#24046;&#21644;&#26041;&#24046;&#22312;&#26679;&#26412;&#32423;&#21035;&#19978;&#26159;"&#23545;&#40784;"&#30340;&#65292;&#20854;&#20013;&#23545;&#20110;&#27491;&#30830;&#20998;&#31867;&#30340;&#26679;&#26412;&#28857;&#65292;&#22343;&#26041;&#20559;&#24046;&#22823;&#32422;&#31561;&#20110;&#26041;&#24046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#32463;&#39564;&#35777;&#25454;&#26469;&#35777;&#23454;&#36825;&#19968;&#29616;&#35937;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#26657;&#20934;&#21644;&#31070;&#32463;&#23849;&#28291;&#30340;&#20004;&#20010;&#29702;&#35770;&#35270;&#35282;&#30740;&#31350;&#20102;&#35813;&#29616;&#35937;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#22312;&#27169;&#22411;&#33391;&#22909;&#26657;&#20934;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#35266;&#23519;&#21040;&#20559;&#24046;-&#26041;&#24046;&#30340;&#23545;&#40784;&#12290;&#20854;&#27425;&#65292;&#22312;&#31070;&#32463;&#23849;&#28291;&#29702;&#35770;&#25552;&#20379;&#30340;&#22270;&#26223;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20559;&#24046;&#21644;&#26041;&#24046;&#20043;&#38388;&#30340;&#36817;&#20284;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical wisdom in machine learning holds that the generalization error can be decomposed into bias and variance, and these two terms exhibit a \emph{trade-off}. However, in this paper, we show that for an ensemble of deep learning based classification models, bias and variance are \emph{aligned} at a sample level, where squared bias is approximately \emph{equal} to variance for correctly classified sample points. We present empirical evidence confirming this phenomenon in a variety of deep learning models and datasets. Moreover, we study this phenomenon from two theoretical perspectives: calibration and neural collapse. We first show theoretically that under the assumption that the models are well calibrated, we can observe the bias-variance alignment. Second, starting from the picture provided by the neural collapse theory, we show an approximate correlation between bias and variance.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;WordNet&#23618;&#27425;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#19978;&#20041;&#35789;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65292;&#33021;&#22815;&#23450;&#37327;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#22256;&#38590;&#30340;&#35789;&#27719;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#21253;&#25324;GLIDE&#12289;Latent Diffusion&#21644;Stable Diffusion&#12290;</title><link>http://arxiv.org/abs/2310.09247</link><description>&lt;p&gt;
&#36890;&#36807;WordNet&#23618;&#27425;&#32467;&#26500;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#19978;&#20041;&#35789;&#29702;&#35299;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy. (arXiv:2310.09247v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;WordNet&#23618;&#27425;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#19978;&#20041;&#35789;&#20851;&#31995;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65292;&#33021;&#22815;&#23450;&#37327;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#22256;&#38590;&#30340;&#35789;&#27719;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#21253;&#25324;GLIDE&#12289;Latent Diffusion&#21644;Stable Diffusion&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#21644;&#20247;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#25512;&#29702;&#20986;&#32473;&#23450;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#30340;&#25552;&#31034;&#34920;&#36798;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34913;&#37327;&#20102;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#20110;&#19978;&#20041;&#35789;&#65288;&#25110;&#8220;&#26159;&#19968;&#20010;&#8221;&#20851;&#31995;&#65289;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#22522;&#20110;WordNet&#35821;&#20041;&#23618;&#27425;&#21644;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;&#29616;&#26377;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#12290;&#36825;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#22343;&#33021;&#22815;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#36827;&#34892;&#24191;&#27867;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#25214;&#21040;&#32454;&#31890;&#24230;&#23450;&#24615;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#23545;&#20110;&#27169;&#22411;&#26469;&#35828;&#26410;&#30693;&#30340;&#21333;&#35789;&#65292;&#22240;&#27492;&#24456;&#38590;&#32472;&#21046;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#21253;&#25324;GLIDE&#12289;Latent Diffusion&#21644;Stable Diffusion&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image synthesis has recently attracted widespread attention due to rapidly improving quality and numerous practical applications. However, the language understanding capabilities of text-to-image models are still poorly understood, which makes it difficult to reason about prompt formulations that a given model would understand well. In this work, we measure the capability of popular text-to-image models to understand $\textit{hypernymy}$, or the "is-a" relation between words. We design two automatic metrics based on the WordNet semantic hierarchy and existing image classifiers pretrained on ImageNet. These metrics both enable broad quantitative comparison of linguistic capabilities for text-to-image models and offer a way of finding fine-grained qualitative differences, such as words that are unknown to models and thus are difficult for them to draw. We comprehensively evaluate popular text-to-image models, including GLIDE, Latent Diffusion, and Stable Diffusion, showing how ou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;1D&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(Time CNN)&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;MEG&#25968;&#25454;&#20013;&#30340;&#30315;&#30187;&#23574;&#23792;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#24182;&#19988;&#20351;&#29992;GCN&#26469;&#32771;&#34385;MEG&#20256;&#24863;&#22120;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#22312;&#20020;&#24202;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#37117;&#21462;&#24471;&#20102;&#20248;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09236</link><description>&lt;p&gt;
&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;MEG&#25968;&#25454;&#20013;&#30340;&#30315;&#30187;&#23574;&#23792;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Time CNN and Graph Convolution Network for Epileptic Spike Detection in MEG Data. (arXiv:2310.09236v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09236
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;1D&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(Time CNN)&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;MEG&#25968;&#25454;&#20013;&#30340;&#30315;&#30187;&#23574;&#23792;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#24182;&#19988;&#20351;&#29992;GCN&#26469;&#32771;&#34385;MEG&#20256;&#24863;&#22120;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#22312;&#20020;&#24202;&#25968;&#25454;&#38598;&#21644;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#37117;&#21462;&#24471;&#20102;&#20248;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#24739;&#32773;&#30340;&#33041;&#30913;&#22270;(MEG)&#35760;&#24405;&#26174;&#31034;&#20986;&#23574;&#23792;&#65292;&#36825;&#26159;&#30149;&#29702;&#23398;&#30340;&#20856;&#22411;&#29983;&#29289;&#26631;&#24535;&#12290;&#26816;&#27979;&#36825;&#20123;&#23574;&#23792;&#21487;&#20197;&#20934;&#30830;&#22320;&#23450;&#20301;&#24341;&#21457;&#30315;&#30187;&#30340;&#33041;&#21306;&#12290;&#30446;&#21069;&#23574;&#23792;&#26816;&#27979;&#36890;&#24120;&#26159;&#25163;&#21160;&#23436;&#25104;&#30340;&#65292;&#20294;&#30001;&#20110;MEG&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#26159;&#19968;&#39033;&#32321;&#37325;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;1D&#26102;&#38388;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(Time CNN)&#32467;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#30340;&#26041;&#27861;&#65292;&#23558;MEG&#35760;&#24405;&#30340;&#30701;&#26102;&#38388;&#26694;&#26550;&#20998;&#31867;&#20026;&#21253;&#21547;&#23574;&#23792;&#25110;&#19981;&#21253;&#21547;&#23574;&#23792;&#12290;&#19982;&#20854;&#20182;&#26368;&#36817;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#38656;&#35201;&#35757;&#32451;&#30340;&#21442;&#25968;&#26356;&#23569;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;GCN&#26469;&#32771;&#34385;MEG&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20135;&#29983;&#20102;&#20020;&#24202;&#30456;&#20851;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;76.7%&#30340;&#20998;&#31867;F1&#20998;&#25968;&#65292;&#22312;&#29616;&#23454;&#20013;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;25.5%&#30340;&#20998;&#31867;F1&#20998;&#25968;&#65292;&#38024;&#23545;&#23574;&#23792;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Magnetoencephalography (MEG) recordings of patients with epilepsy exhibit spikes, a typical biomarker of the pathology. Detecting those spikes allows accurate localization of brain regions triggering seizures. Spike detection is often performed manually. However, it is a burdensome and error prone task due to the complexity of MEG data. To address this problem, we propose a 1D temporal convolutional neural network (Time CNN) coupled with a graph convolutional network (GCN) to classify short time frames of MEG recording as containing a spike or not. Compared to other recent approaches, our models have fewer parameters to train and we propose to use a GCN to account for MEG sensors spatial relationships. Our models produce clinically relevant results and outperform deep learning-based state-of-the-art methods reaching a classification f1-score of 76.7% on a balanced dataset and of 25.5% on a realistic, highly imbalanced dataset, for the spike class.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#20581;&#24247;&#20445;&#38505;&#35745;&#21010;&#30340;&#31867;&#22411;&#12289;&#21306;&#22495;&#21644;&#36153;&#29992;&#31561;&#22240;&#32032;&#65292;&#39044;&#27979;&#20854;&#26159;&#21542;&#35206;&#30422;&#25104;&#24180;&#20154;&#30340;&#24120;&#35268;&#29273;&#31185;&#26381;&#21153;&#65292;&#26088;&#22312;&#20026;&#20010;&#20154;&#21644;&#23478;&#24237;&#25552;&#20379;&#36873;&#25321;&#26368;&#21512;&#36866;&#20445;&#38505;&#30340;&#20020;&#24202;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.09229</link><description>&lt;p&gt;
&#20445;&#38556;&#24494;&#31505;&#65306;&#21033;&#29992;Spark ML&#39044;&#27979;&#24120;&#35268;&#29273;&#31185;&#20445;&#38505;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
Insuring Smiles: Predicting routine dental coverage using Spark ML. (arXiv:2310.09229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20998;&#26512;&#20581;&#24247;&#20445;&#38505;&#35745;&#21010;&#30340;&#31867;&#22411;&#12289;&#21306;&#22495;&#21644;&#36153;&#29992;&#31561;&#22240;&#32032;&#65292;&#39044;&#27979;&#20854;&#26159;&#21542;&#35206;&#30422;&#25104;&#24180;&#20154;&#30340;&#24120;&#35268;&#29273;&#31185;&#26381;&#21153;&#65292;&#26088;&#22312;&#20026;&#20010;&#20154;&#21644;&#23478;&#24237;&#25552;&#20379;&#36873;&#25321;&#26368;&#21512;&#36866;&#20445;&#38505;&#30340;&#20020;&#24202;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32654;&#22269;&#65292;&#20010;&#20154;&#21644;&#23567;&#22411;&#20225;&#19994;&#24448;&#24448;&#38590;&#20197;&#25214;&#21040;&#21512;&#36866;&#30340;&#20581;&#24247;&#20445;&#38505;&#35206;&#30422;&#12290;CMS&#25552;&#20379;&#30340;&#20581;&#24247;&#20445;&#38505;&#20132;&#27969;&#20844;&#20849;&#20351;&#29992;&#25991;&#20214;&#65288;Exchange PUFs&#65289;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#26377;&#20851;&#20581;&#24247;&#21644;&#29273;&#31185;&#25919;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#35745;&#21010;&#31867;&#22411;&#12289;&#21306;&#22495;&#12289;&#20813;&#36180;&#39069;&#12289;&#33258;&#20184;&#36153;&#29992;&#19978;&#38480;&#21644;&#20849;&#20184;&#37329;&#39069;&#31561;&#22240;&#32032;&#65292;&#39044;&#27979;&#20581;&#24247;&#20445;&#38505;&#35745;&#21010;&#26159;&#21542;&#35206;&#30422;&#25104;&#24180;&#20154;&#30340;&#24120;&#35268;&#29273;&#31185;&#26381;&#21153;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#20010;&#20154;&#21644;&#23478;&#24237;&#25552;&#20379;&#22522;&#20110;&#25910;&#20837;&#21644;&#36153;&#29992;&#30340;&#26368;&#21512;&#36866;&#30340;&#20445;&#38505;&#36873;&#25321;&#30340;&#20020;&#24202;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding suitable health insurance coverage can be challenging for individuals and small enterprises in the USA. The Health Insurance Exchange Public Use Files (Exchange PUFs) dataset provided by CMS offers valuable information on health and dental policies [1]. In this paper, we leverage machine learning algorithms to predict if a health insurance plan covers routine dental services for adults. By analyzing plan type, region, deductibles, out-of-pocket maximums, and copayments, we employ Logistic Regression, Decision Tree, Random Forest, Gradient Boost, Factorization Model and Support Vector Machine algorithms. Our goal is to provide a clinical strategy for individuals and families to select the most suitable insurance plan based on income and expenses.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#31639;&#27861;FSBN&#21644;SSBN&#65292;&#23427;&#20204;&#21033;&#29992;&#23616;&#37096;&#25628;&#32034;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#24402;&#32435;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09222</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#24555;&#36895;&#21644;&#39640;&#25928;&#22320;&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;&#65306;&#30693;&#35782;&#21457;&#29616;&#19982;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Fast &amp; Efficient Learning of Bayesian Networks from Data: Knowledge Discovery and Causality. (arXiv:2310.09222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#31639;&#27861;FSBN&#21644;SSBN&#65292;&#23427;&#20204;&#21033;&#29992;&#23616;&#37096;&#25628;&#32034;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#36136;&#37327;&#30340;&#24402;&#32435;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#23398;&#20064;&#23545;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BNs&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#21487;&#20197;&#25581;&#31034;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#33021;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#30693;&#35782;&#21457;&#29616;&#12289;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#20915;&#31574;&#12290;&#22522;&#20110;PC&#31639;&#27861;&#30340;&#20004;&#20010;&#26032;&#31639;&#27861;FSBN&#21644;SSBN&#65292;&#37319;&#29992;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;&#21644;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#32593;&#32476;&#32467;&#26500;&#12290;&#23427;&#20204;&#21033;&#29992;d-&#20998;&#31163;&#26469;&#25512;&#26029;&#39069;&#22806;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#20248;&#20808;&#32771;&#34385;&#26465;&#20214;&#38598;&#65292;&#24182;&#39640;&#25928;&#22320;&#32456;&#27490;&#25628;&#32034;&#12290;FSBN&#23454;&#29616;&#20102;&#39640;&#36798;52%&#30340;&#35745;&#31639;&#25104;&#26412;&#20943;&#23569;&#65292;&#32780;SSBN&#22312;200&#20010;&#33410;&#28857;&#30340;&#32593;&#32476;&#20013;&#36229;&#36807;&#20102;&#23427;&#65292;&#20943;&#23569;&#20102;72%&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#30001;&#20110;&#26234;&#33021;&#31574;&#30053;&#65292;SSBN&#23637;&#31034;&#20102;&#36827;&#19968;&#27493;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31639;&#27861;&#22312;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#19982;PC&#31639;&#27861;&#30340;&#24402;&#32435;&#36136;&#37327;&#30456;&#21305;&#37197;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#20351;&#23427;&#20204;&#20855;&#26377;&#24456;&#39640;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure learning is essential for Bayesian networks (BNs) as it uncovers causal relationships, and enables knowledge discovery, predictions, inferences, and decision-making under uncertainty. Two novel algorithms, FSBN and SSBN, based on the PC algorithm, employ local search strategy and conditional independence tests to learn the causal network structure from data. They incorporate d-separation to infer additional topology information, prioritize conditioning sets, and terminate the search immediately and efficiently. FSBN achieves up to 52% computation cost reduction, while SSBN surpasses it with a remarkable 72% reduction for a 200-node network. SSBN demonstrates further efficiency gains due to its intelligent strategy. Experimental studies show that both algorithms match the induction quality of the PC algorithm while significantly reducing computation costs. This enables them to offer interpretability and adaptability while reducing the computational burden, making them valuable
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#24182;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09213</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Unseen Image Synthesis with Diffusion Models. (arXiv:2310.09213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#24182;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29983;&#25104;&#39046;&#22495;&#30340;&#36235;&#21183;&#26159;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#21644;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#26469;&#23454;&#29616;&#36890;&#29992;&#39046;&#22495;&#34920;&#31034;&#65292;&#32780;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#36873;&#25321;&#30456;&#21453;&#30340;&#26041;&#21521;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#20923;&#32467;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#22312;&#21333;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28508;&#22312;&#37319;&#26679;&#21644;&#20960;&#20309;&#20248;&#21270;&#26469;&#21512;&#25104;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#21363;&#20351;&#26159;&#20165;&#22312;&#21333;&#39046;&#22495;&#22270;&#20687;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;DDPMs&#24050;&#32463;&#20855;&#22791;&#20102;&#36275;&#22815;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#21453;&#36716;&#28508;&#22312;&#32534;&#30721;&#65292;&#24182;&#32463;&#36807;&#21452;&#21521;&#30830;&#23450;&#24615;&#25193;&#25955;&#21644;&#21435;&#22122;&#36712;&#36857;&#37325;&#26500;&#20219;&#24847;&#22270;&#20687;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#30740;&#31350;&#26410;&#35265;&#36807;&#22270;&#20687;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27839;&#21435;&#22122;&#38142;&#30340;OOD&#26679;&#26412;&#30340;&#32479;&#35745;&#21644;&#20960;&#20309;&#34892;&#20026;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#37117;&#34920;&#26126;&#65292;&#21453;&#36716;&#30340;OOD&#26679;&#26412;&#20063;&#24314;&#31435;&#20102;&#39640;&#26031;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the current trend in the generative field is scaling up towards larger models and more training data for generalized domain representations, we go the opposite direction in this work by synthesizing unseen domain images without additional training. We do so via latent sampling and geometric optimization using pre-trained and frozen Denoising Diffusion Probabilistic Models (DDPMs) on single-domain datasets. Our key observation is that DDPMs pre-trained even just on single-domain images are already equipped with sufficient representation abilities to reconstruct arbitrary images from the inverted latent encoding following bi-directional deterministic diffusion and denoising trajectories. This motivates us to investigate the statistical and geometric behaviors of the Out-Of-Distribution (OOD) samples from unseen image domains in the latent spaces along the denoising chain. Notably, we theoretically and empirically show that the inverted OOD samples also establish Gaussians that are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24207;&#25968;&#37327;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#30740;&#31350;&#65292;&#23454;&#39564;&#27604;&#36739;&#20102;&#24050;&#26377;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24615;&#33021;&#26356;&#22909;&#30340;&#27491;&#21017;&#21270;OQ&#31639;&#27861;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2310.09210</link><description>&lt;p&gt;
&#23545;&#24207;&#25968;&#37327;&#21270;&#38382;&#39064;&#30340;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Regularization-Based Methods for Ordinal Quantification. (arXiv:2310.09210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24207;&#25968;&#37327;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#30740;&#31350;&#65292;&#23454;&#39564;&#27604;&#36739;&#20102;&#24050;&#26377;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24615;&#33021;&#26356;&#22909;&#30340;&#27491;&#21017;&#21270;OQ&#31639;&#27861;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#37327;&#21270;&#38382;&#39064;&#8212;&#8212;&#21363;&#22312;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#39033;&#38598;&#20013;&#35757;&#32451;&#39044;&#27979;&#22120;&#30340;&#31867;&#21035;&#26222;&#36941;&#24615;&#20540;&#8212;&#8212;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#37327;&#21270;&#30740;&#31350;&#38598;&#20013;&#22312;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#24320;&#21457;&#19978;&#65292;&#20854;&#20013;&#31867;&#21035;&#27809;&#26377;&#34987;&#25490;&#24207;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24207;&#25968;&#24773;&#20917;&#65292;&#21363;&#22312;n&gt;2&#20010;&#31867;&#21035;&#30340;&#38598;&#21512;&#19978;&#23450;&#20041;&#20102;&#19968;&#20010;&#23436;&#20840;&#30340;&#25490;&#24207;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#39046;&#22495;&#20570;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#24207;&#25968;&#35745;&#37327;&#65288;OQ&#65289;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#21487;&#29992;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23454;&#39564;&#27604;&#36739;&#20102;&#36804;&#20170;&#20026;&#27490;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#26368;&#37325;&#35201;&#30340;OQ&#31639;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#30340;&#20316;&#32773;&#25552;&#20986;&#30340;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22914;&#25968;&#25454;&#25366;&#25496;&#21644;&#22825;&#20307;&#29289;&#29702;&#23398;&#65292;&#36825;&#20123;&#20316;&#32773;&#23545;&#24444;&#27492;&#30340;&#30740;&#31350;&#36827;&#23637;&#24182;&#19981;&#30693;&#26195;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;OQ&#31639;&#27861;&#31867;&#21035;&#65292;&#23427;&#32988;&#36807;&#20102;&#20854;&#23427;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantification, i.e., the task of training predictors of the class prevalence values in sets of unlabeled data items, has received increased attention in recent years. However, most quantification research has concentrated on developing algorithms for binary and multiclass problems in which the classes are not ordered. Here, we study the ordinal case, i.e., the case in which a total order is defined on the set of n&gt;2 classes. We give three main contributions to this field. First, we create and make available two datasets for ordinal quantification (OQ) research that overcome the inadequacies of the previously available ones. Second, we experimentally compare the most important OQ algorithms proposed in the literature so far. To this end, we bring together algorithms proposed by authors from very different research fields, such as data mining and astrophysics, who were unaware of each others' developments. Third, we propose a novel class of regularized OQ algorithms, which outperforms e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SiamAF&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24515;&#30005;&#22270;&#21644;&#20809;&#30005;&#33033;&#25615;&#22270;&#20449;&#21495;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#36890;&#36807;Siamese&#32593;&#32476;&#21644;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#24378;&#20581;&#30340;&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.09203</link><description>&lt;p&gt;
SiamAF: &#23398;&#20064;&#24515;&#30005;&#22270;&#21644;&#20809;&#30005;&#33033;&#25615;&#22270;&#20449;&#21495;&#30340;&#20849;&#20139;&#20449;&#24687;&#29992;&#20110;&#24378;&#20581;&#30340;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SiamAF: Learning Shared Information from ECG and PPG Signals for Robust Atrial Fibrillation Detection. (arXiv:2310.09203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09203
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SiamAF&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24515;&#30005;&#22270;&#21644;&#20809;&#30005;&#33033;&#25615;&#22270;&#20449;&#21495;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#36890;&#36807;Siamese&#32593;&#32476;&#21644;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#24378;&#20581;&#30340;&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26159;&#26368;&#24120;&#35265;&#30340;&#24515;&#33039;&#24515;&#24459;&#22833;&#24120;&#31867;&#22411;&#65292;&#19982;&#20013;&#39118;&#12289;&#24515;&#21147;&#34928;&#31469;&#21644;&#20854;&#20182;&#24515;&#34880;&#31649;&#24182;&#21457;&#30151;&#30340;&#39118;&#38505;&#22686;&#21152;&#26377;&#20851;&#65292;&#20294;&#21487;&#20197;&#20020;&#24202;&#19978;&#26080;&#22768;&#12290;&#20329;&#25140;&#24335;&#35774;&#22791;&#36827;&#34892;&#34987;&#21160;&#24615;&#30340;AF&#30417;&#27979;&#21487;&#33021;&#26377;&#21161;&#20110;&#20943;&#23569;&#19982;AF&#30456;&#20851;&#30340;&#19981;&#33391;&#20020;&#24202;&#32467;&#26524;&#12290;&#22312;&#22024;&#26434;&#30340;&#20329;&#25140;&#24335;&#25968;&#25454;&#20013;&#26816;&#27979;AF&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#24341;&#21457;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#20808;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#21333;&#19968;&#24418;&#24577;&#23398;&#20064;&#65292;&#35201;&#20040;&#26159;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#65292;&#35201;&#20040;&#26159;&#20809;&#30005;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#29305;&#24449;&#65292;&#24182;&#20381;&#36182;&#20110;&#26356;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#25439;&#22351;&#30340;&#29305;&#24449;&#65292;&#22312;&#26576;&#20123;&#22330;&#26223;&#20013;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36136;&#37327;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#12290;&#37492;&#20110;&#20329;&#25140;&#24335;&#35774;&#22791;&#21644;&#24202;&#36793;&#30417;&#25252;&#20202;&#19978;ECG&#21644;PPG&#20449;&#21495;&#37197;&#23545;&#30340;&#26085;&#30410;&#20016;&#23500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;SiamAF&#65292;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;Siamese&#32593;&#32476;&#32467;&#26500;&#21644;&#32852;&#21512;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Atrial fibrillation (AF) is the most common type of cardiac arrhythmia. It is associated with an increased risk of stroke, heart failure, and other cardiovascular complications, but can be clinically silent. Passive AF monitoring with wearables may help reduce adverse clinical outcomes related to AF. Detecting AF in noisy wearable data poses a significant challenge, leading to the emergence of various deep learning techniques. Previous deep learning models learn from a single modality, either electrocardiogram (ECG) or photoplethysmography (PPG) signals. However, deep learning models often struggle to learn generalizable features and rely on features that are more susceptible to corruption from noise, leading to sub-optimal performances in certain scenarios, especially with low-quality signals. Given the increasing availability of ECG and PPG signal pairs from wearables and bedside monitors, we propose a new approach, SiamAF, leveraging a novel Siamese network architecture and joint le
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#21305;&#37197;&#30340;&#26080;&#35889;&#22270;&#21387;&#32553;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#21457;&#29616;&#65292;GNN&#27880;&#20837;&#21512;&#25104;&#22270;&#20013;&#30340;&#35889;&#20559;&#24046;&#23548;&#33268;&#20102;&#24615;&#33021;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09202</link><description>&lt;p&gt;
&#22270;&#23884;&#20837;&#19982;&#29305;&#24449;&#21305;&#37197;&#30340;&#22270;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Graph Condensation via Eigenbasis Matching. (arXiv:2310.09202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#21305;&#37197;&#30340;&#26080;&#35889;&#22270;&#21387;&#32553;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#21457;&#29616;&#65292;GNN&#27880;&#20837;&#21512;&#25104;&#22270;&#20013;&#30340;&#35889;&#20559;&#24046;&#23548;&#33268;&#20102;&#24615;&#33021;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#35201;&#27714;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21508;&#31181;&#22270;&#30456;&#20851;&#24212;&#29992;&#20013;&#25552;&#39640;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#26368;&#36817;&#65292;&#26032;&#20852;&#30340;&#22270;&#21387;&#32553;&#65288;GC&#65289;&#20174;&#25968;&#25454;&#35282;&#24230;&#38477;&#20302;&#20102;GNN&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23427;&#26088;&#22312;&#29992;&#19968;&#20010;&#26126;&#26174;&#36739;&#23567;&#30340;&#21512;&#25104;&#22270;&#26367;&#20195;&#30495;&#23454;&#30340;&#22823;&#22411;&#22270;&#65292;&#20351;&#24471;&#22312;&#36825;&#20004;&#20010;&#22270;&#19978;&#35757;&#32451;&#30340;GNN&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;GC&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#21363;&#22312;&#21516;&#19968;&#20010;&#21512;&#25104;&#22270;&#19978;&#35757;&#32451;&#30340;&#19981;&#21516;GNN&#24615;&#33021;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;&#26159;&#20160;&#20040;&#22240;&#32032;&#38459;&#30861;&#20102;GC&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21457;&#29616;GNN&#20250;&#23558;&#35889;&#20559;&#24046;&#27880;&#20837;&#21512;&#25104;&#22270;&#20013;&#65292;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#21305;&#37197;&#30340;&#26080;&#35889;&#22270;&#21387;&#32553;&#65292;&#31216;&#20043;&#20026;...
&lt;/p&gt;
&lt;p&gt;
The increasing amount of graph data places requirements on the efficiency and scalability of graph neural networks (GNNs), despite their effectiveness in various graph-related applications. Recently, the emerging graph condensation (GC) sheds light on reducing the computational cost of GNNs from a data perspective. It aims to replace the real large graph with a significantly smaller synthetic graph so that GNNs trained on both graphs exhibit comparable performance. However, our empirical investigation reveals that existing GC methods suffer from poor generalization, i.e., different GNNs trained on the same synthetic graph have obvious performance gaps. What factors hinder the generalization of GC and how can we mitigate it? To answer this question, we commence with a detailed analysis and observe that GNNs will inject spectrum bias into the synthetic graph, resulting in a distribution shift. To tackle this issue, we propose eigenbasis matching for spectrum-free graph condensation, name
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#19979;&#30028;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32452;&#21512;&#31639;&#27861;&#26469;&#27714;&#35299;&#26368;&#23567;&#26368;&#22823;&#30456;&#20851;&#32858;&#31867;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#23436;&#20840;&#22270;&#19978;&#36798;&#21040;4&#30340;&#36817;&#20284;&#32467;&#26524;&#12290;&#36890;&#36807;&#36138;&#23146;&#32852;&#21512;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#25193;&#23637;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38388;&#19978;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.09196</link><description>&lt;p&gt;
&#19968;&#20010;&#27714;&#35299;&#26368;&#23567;&#26368;&#22823;&#30456;&#20851;&#32858;&#31867;&#38382;&#39064;&#30340;4&#36817;&#20284;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A 4-approximation algorithm for min max correlation clustering. (arXiv:2310.09196v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#19979;&#30028;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32452;&#21512;&#31639;&#27861;&#26469;&#27714;&#35299;&#26368;&#23567;&#26368;&#22823;&#30456;&#20851;&#32858;&#31867;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#23436;&#20840;&#22270;&#19978;&#36798;&#21040;4&#30340;&#36817;&#20284;&#32467;&#26524;&#12290;&#36890;&#36807;&#36138;&#23146;&#32852;&#21512;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#25193;&#23637;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38388;&#19978;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27714;&#35299;&#26368;&#23567;&#26368;&#22823;&#30456;&#20851;&#32858;&#31867;&#38382;&#39064;&#30340;&#19979;&#30028;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#27492;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23436;&#20840;&#22270;&#30340;&#32452;&#21512;4&#36817;&#20284;&#31639;&#27861;&#12290;&#36825;&#25913;&#36827;&#20102;&#20043;&#21069;&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#20844;&#24335;&#65288;Kalhan&#31561;&#65292;2019&#65289;&#33719;&#24471;&#30340;&#36817;&#20284;&#20445;&#35777;&#20026;5&#21644;&#20351;&#29992;&#32452;&#21512;&#31639;&#27861;&#65288;Davies&#31561;&#65292;2023&#65289;&#33719;&#24471;&#30340;&#36817;&#20284;&#20445;&#35777;&#20026;4&#30340;&#26368;&#20339;&#24050;&#30693;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#36138;&#23146;&#32852;&#21512;&#21551;&#21457;&#24335;&#31639;&#27861;&#25193;&#23637;&#20102;&#35813;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a lower bounding technique for the min max correlation clustering problem and, based on this technique, a combinatorial 4-approximation algorithm for complete graphs. This improves upon the previous best known approximation guarantees of 5, using a linear program formulation (Kalhan et al., 2019), and 4, for a combinatorial algorithm (Davies et al., 2023). We extend this algorithm by a greedy joining heuristic and show empirically that it improves the state of the art in solution quality and runtime on several benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21152;&#26435;&#26679;&#26412;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#39640;&#32500;&#38750;&#21442;&#25968;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#37319;&#26679;&#20013;&#30340;&#30446;&#26631;&#20998;&#24067;&#12290;&#25152;&#24471;&#21040;&#30340;&#20998;&#24067;&#26063;&#20855;&#26377;&#19982;&#38750;&#21442;&#25968;&#27169;&#22411;&#30456;&#24403;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#27604;&#20256;&#32479;&#30340;&#39640;&#26031;&#25110;&#39640;&#26031;&#28151;&#21512;&#26063;&#26356;&#39640;&#25928;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#20808;&#39564;&#20998;&#24067;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#23398;&#20064;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09194</link><description>&lt;p&gt;
&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;&#21152;&#26435;&#26679;&#26412;&#22312;&#39640;&#32500;&#38750;&#21442;&#25968;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#37319;&#26679;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoder with weighted samples for high-dimensional non-parametric adaptive importance sampling. (arXiv:2310.09194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#21152;&#26435;&#26679;&#26412;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#39640;&#32500;&#38750;&#21442;&#25968;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#37319;&#26679;&#20013;&#30340;&#30446;&#26631;&#20998;&#24067;&#12290;&#25152;&#24471;&#21040;&#30340;&#20998;&#24067;&#26063;&#20855;&#26377;&#19982;&#38750;&#21442;&#25968;&#27169;&#22411;&#30456;&#24403;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#27604;&#20256;&#32479;&#30340;&#39640;&#26031;&#25110;&#39640;&#26031;&#28151;&#21512;&#26063;&#26356;&#39640;&#25928;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#21487;&#23398;&#20064;&#30340;&#20808;&#39564;&#20998;&#24067;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#23398;&#20064;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#21152;&#26435;&#26679;&#26412;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20272;&#35745;&#26159;&#25152;&#26377;&#33258;&#36866;&#24212;&#37325;&#35201;&#24615;&#37319;&#26679;&#31639;&#27861;&#30340;&#22522;&#30784;&#12290;&#20256;&#32479;&#19978;&#65292;&#30446;&#26631;&#20998;&#24067;&#35201;&#20040;&#36890;&#36807;&#38750;&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#36817;&#20284;&#65292;&#35201;&#20040;&#22312;&#21442;&#25968;&#21270;&#26063;&#20013;&#36827;&#34892;&#36817;&#20284;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#35201;&#20040;&#38754;&#20020;&#32500;&#24230;&#28798;&#38590;&#65292;&#35201;&#20040;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#30001;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21442;&#25968;&#21270;&#30340;&#20998;&#24067;&#20316;&#20026;&#36817;&#20284;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#23558;&#29616;&#26377;&#26694;&#26550;&#25193;&#23637;&#21040;&#21152;&#26435;&#26679;&#26412;&#30340;&#24773;&#20917;&#12290;&#25152;&#24471;&#21040;&#30340;&#20998;&#24067;&#26063;&#30340;&#28789;&#27963;&#24615;&#20351;&#20854;&#19982;&#38750;&#21442;&#25968;&#27169;&#22411;&#19968;&#26679;&#34920;&#36798;&#33021;&#21147;&#24378;&#65292;&#23613;&#31649;&#21442;&#25968;&#20272;&#35745;&#30340;&#25968;&#37327;&#38750;&#24120;&#39640;&#65292;&#20294;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#20998;&#24067;&#26063;&#27604;&#20256;&#32479;&#30340;&#39640;&#26031;&#25110;&#39640;&#26031;&#28151;&#21512;&#26063;&#35201;&#26356;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#21152;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#23398;&#20064;&#22810;&#27169;&#24577;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21487;&#23398;&#20064;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probability density function estimation with weighted samples is the main foundation of all adaptive importance sampling algorithms. Classically, a target distribution is approximated either by a non-parametric model or within a parametric family. However, these models suffer from the curse of dimensionality or from their lack of flexibility. In this contribution, we suggest to use as the approximating model a distribution parameterised by a variational autoencoder. We extend the existing framework to the case of weighted samples by introducing a new objective function. The flexibility of the obtained family of distributions makes it as expressive as a non-parametric model, and despite the very high number of parameters to estimate, this family is much more efficient in high dimension than the classical Gaussian or Gaussian mixture families. Moreover, in order to add flexibility to the model and to be able to learn multimodal distributions, we consider a learnable prior distribution fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#21387;&#32553;&#26041;&#27861;&#20013;&#23545;&#32467;&#26500;&#20449;&#24687;&#30340;&#24573;&#35270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#24191;&#25773;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#26696;&#65288;SGDD&#65289;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20445;&#30041;&#21407;&#22987;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#22312;&#36328;&#26550;&#26500;&#27867;&#21270;&#21644;&#29305;&#23450;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09192</link><description>&lt;p&gt;
&#22270;&#29366;&#21387;&#32553;&#65306;&#26159;&#21542;&#30475;&#36215;&#26469;&#20687;&#35270;&#35273;&#25968;&#25454;&#38598;&#65311;(arXiv:2310.09192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Does Graph Distillation See Like Vision Dataset Counterpart?. (arXiv:2310.09192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#21387;&#32553;&#26041;&#27861;&#20013;&#23545;&#32467;&#26500;&#20449;&#24687;&#30340;&#24573;&#35270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#24191;&#25773;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#26696;&#65288;SGDD&#65289;&#65292;&#35813;&#26041;&#26696;&#33021;&#22815;&#20445;&#30041;&#21407;&#22987;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#22312;&#36328;&#26550;&#26500;&#27867;&#21270;&#21644;&#29305;&#23450;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#23545;&#22823;&#35268;&#27169;&#22270;&#36827;&#34892;&#35757;&#32451;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#25104;&#26412;&#21644;&#23384;&#20648;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#22270;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#20248;&#21270;&#21387;&#32553;&#22270;&#30340;&#29305;&#24449;&#30697;&#38453;&#65292;&#32780;&#24573;&#35270;&#20102;&#21407;&#22987;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35843;&#26597;&#32467;&#26500;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#20174;&#35889;&#22495;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#30830;&#23450;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#37325;&#35201;&#30340;&#25289;&#26222;&#25289;&#26031;&#33021;&#37327;&#20998;&#24067;&#65288;LED&#65289;&#30340;&#20559;&#31227;&#12290;&#36825;&#31181;&#20559;&#31227;&#23548;&#33268;&#22312;&#36328;&#26550;&#26500;&#27867;&#21270;&#21644;&#29305;&#23450;&#20219;&#21153;&#65288;&#21253;&#25324;&#24322;&#24120;&#26816;&#27979;&#21644;&#38142;&#25509;&#39044;&#27979;&#65289;&#20013;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32467;&#26500;&#24191;&#25773;&#22270;&#25968;&#25454;&#38598;&#21387;&#32553;&#65288;SGDD&#65289;&#26041;&#26696;&#65292;&#23558;&#21407;&#22987;&#32467;&#26500;&#20449;&#24687;&#24191;&#25773;&#21040;&#21512;&#25104;&#22270;&#30340;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#26174;&#24335;&#22320;&#38450;&#27490;&#24573;&#35270;&#21407;&#22987;&#32467;&#26500;&#20449;&#24687;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;SGDD&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20855;&#26377;&#20445;&#30041;&#21407;&#22987;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have attracted increasing concerns. Existing graph condensation methods primarily focus on optimizing the feature matrices of condensed graphs while overlooking the impact of the structure information from the original graphs. To investigate the impact of the structure information, we conduct analysis from the spectral domain and empirically identify substantial Laplacian Energy Distribution (LED) shifts in previous works. Such shifts lead to poor performance in cross-architecture generalization and specific tasks, including anomaly detection and link prediction. In this paper, we propose a novel Structure-broadcasting Graph Dataset Distillation (SGDD) scheme for broadcasting the original structure information to the generation of the synthetic one, which explicitly prevents overlooking the original structure information. Theoretically, the synthetic graphs by SGDD 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRIOR&#30340;&#20010;&#24615;&#21270;&#20808;&#39564;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#20840;&#23616;&#27169;&#22411;&#24573;&#35270;&#29305;&#23450;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#38382;&#39064;&#12290;&#35813;&#26041;&#26696;&#20351;&#29992;&#24102;Bregman&#25955;&#24230;&#30340;PFL&#26694;&#26550;&#23558;&#20010;&#24615;&#21270;&#20808;&#39564;&#19982;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#35299;&#32806;&#65292;&#20197;&#25552;&#39640;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09183</link><description>&lt;p&gt;
PRIOR: &#20010;&#24615;&#21270;&#20808;&#39564;&#29992;&#20110;&#37325;&#26032;&#28608;&#27963;&#32852;&#37030;&#23398;&#20064;&#20013;&#34987;&#24573;&#35270;&#30340;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning. (arXiv:2310.09183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRIOR&#30340;&#20010;&#24615;&#21270;&#20808;&#39564;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30001;&#20110;&#20840;&#23616;&#27169;&#22411;&#24573;&#35270;&#29305;&#23450;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#38382;&#39064;&#12290;&#35813;&#26041;&#26696;&#20351;&#29992;&#24102;Bregman&#25955;&#24230;&#30340;PFL&#26694;&#26550;&#23558;&#20010;&#24615;&#21270;&#20808;&#39564;&#19982;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#35299;&#32806;&#65292;&#20197;&#25552;&#39640;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#24322;&#36136;&#25968;&#25454;&#29305;&#24615;&#38477;&#20302;&#20102;&#23616;&#37096;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#36890;&#36807;&#20174;&#20840;&#23616;&#27169;&#22411;&#20013;&#21512;&#25104;&#20010;&#24615;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20840;&#23616;&#27169;&#22411;&#21487;&#33021;&#24573;&#35270;&#20102;&#23458;&#25143;&#31471;&#34987;&#37319;&#26679;&#30340;&#29305;&#23450;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#20010;&#24615;&#21270;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20840;&#23616;&#27169;&#22411;&#20013;&#65292;&#35797;&#22270;&#20943;&#36731;PFL&#20013;&#24341;&#20837;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26694;&#26550;&#65292;&#21363;&#24102;Bregman&#25955;&#24230;&#65288;pFedBreD&#65289;&#30340;PFL&#65292;&#36890;&#36807;&#22312;&#20010;&#24615;&#21270;&#22330;&#26223;&#20013;&#20351;&#29992;Bregman&#25955;&#24230;&#27491;&#21017;&#21270;&#30340;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#65292;&#20351;&#20010;&#24615;&#21270;&#20808;&#39564;&#19982;&#20043;&#35299;&#32806;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#36824;&#25918;&#26494;&#20102;&#38236;&#20687;&#19979;&#38477;&#65288;RMD&#65289;&#65292;&#20197;&#26174;&#24335;&#22320;&#25552;&#21462;&#20808;&#39564;&#30693;&#35782;&#65292;&#25552;&#20379;&#21487;&#36873;&#25321;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical federated learning (FL) enables training machine learning models without sharing data for privacy preservation, but heterogeneous data characteristic degrades the performance of the localized model. Personalized FL (PFL) addresses this by synthesizing personalized models from a global model via training on local data. Such a global model may overlook the specific information that the clients have been sampled. In this paper, we propose a novel scheme to inject personalized prior knowledge into the global model in each client, which attempts to mitigate the introduced incomplete information problem in PFL. At the heart of our proposed approach is a framework, the PFL with Bregman Divergence (pFedBreD), decoupling the personalized prior from the local objective function regularized by Bregman divergence for greater adaptability in personalized scenarios. We also relax the mirror descent (RMD) to extract the prior explicitly to provide optional strategies. Additionally, our pFed
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#21046;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#40736;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#35757;&#32451;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#12289;&#25913;&#36827;&#32593;&#32476;&#26550;&#26500;&#21644;&#21442;&#25968;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;&#21475;&#26381;&#21644;&#38745;&#33033;&#32473;&#33647;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#39044;&#27979;&#26356;&#22810;&#32456;&#28857;&#21644;&#22788;&#29702;&#19981;&#21516;&#30340;&#21327;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.09167</link><description>&lt;p&gt;
&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#21046;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#22823;&#40736;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Deep Neural Network -- Mechanistic Hybrid Model to Predict Pharmacokinetics in Rat. (arXiv:2310.09167v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09167
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#21046;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#40736;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#35757;&#32451;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#12289;&#25913;&#36827;&#32593;&#32476;&#26550;&#26500;&#21644;&#21442;&#25968;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;&#21475;&#26381;&#21644;&#38745;&#33033;&#32473;&#33647;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#39044;&#27979;&#26356;&#22810;&#32456;&#28857;&#21644;&#22788;&#29702;&#19981;&#21516;&#30340;&#21327;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23567;&#20998;&#23376;&#33647;&#29289;&#25110;&#20892;&#33647;&#30340;&#30740;&#21457;&#20013;&#65292;&#37325;&#35201;&#30340;&#19968;&#20010;&#26041;&#38754;&#23601;&#26159;&#23427;&#20204;&#22312;&#38745;&#33033;&#21644;&#21475;&#26381;&#32473;&#33647;&#21518;&#30340;&#20840;&#36523;&#21487;&#29992;&#24615;&#12290;&#20174;&#20505;&#36873;&#21270;&#21512;&#29289;&#30340;&#21270;&#23398;&#32467;&#26500;&#39044;&#27979;&#20840;&#36523;&#21487;&#29992;&#24615;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#35753;&#33647;&#29289;&#25110;&#20892;&#33647;&#30340;&#30740;&#21457;&#38598;&#20013;&#22312;&#20855;&#26377;&#33391;&#22909;&#21160;&#21147;&#23398;&#29305;&#24615;&#30340;&#21270;&#21512;&#29289;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21487;&#29992;&#24615;&#26159;&#20998;&#23376;&#24615;&#36136;&#12289;&#29983;&#29289;&#23398;&#21644;&#29983;&#29702;&#23398;&#20043;&#38388;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#31232;&#32570;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#20808;&#21069;&#24320;&#21457;&#30340;&#28151;&#21512;&#27169;&#22411;[34]&#12290;&#25105;&#20204;&#23558;&#24635;&#30340;&#21475;&#26381;&#26292;&#38706;&#30340;&#20013;&#20540;&#25240;&#21472;&#35823;&#24046;&#20174;2.85&#38477;&#20302;&#21040;2.35&#65292;&#23558;&#38745;&#33033;&#32473;&#33647;&#30340;&#35823;&#24046;&#20174;1.95&#38477;&#20302;&#21040;1.62&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20197;&#21450;&#26426;&#21046;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#23454;&#29616;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#20854;&#20182;&#32456;&#28857;&#21644;&#22788;&#29702;&#19981;&#21516;&#30340;&#21327;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important aspect in the development of small molecules as drugs or agro-chemicals is their systemic availability after intravenous and oral administration.The prediction of the systemic availability from the chemical structure of a poten-tial candidate is highly desirable, as it allows to focus the drug or agrochemicaldevelopment on compounds with a favorable kinetic profile. However, such pre-dictions are challenging as the availability is the result of the complex interplaybetween molecular properties, biology and physiology and training data is rare.In this work we improve the hybrid model developed earlier [34]. We reducethe median fold change error for the total oral exposure from 2.85 to 2.35 andfor intravenous administration from 1.95 to 1.62. This is achieved by trainingon a larger data set, improving the neural network architecture as well as theparametrization of mechanistic model. Further, we extend our approach to predictadditional endpoints and to handle different covar
&lt;/p&gt;</description></item><item><title>JEI-DNN&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#36864;&#20986;&#21644;&#25512;&#26029;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#36827;&#34892;&#37096;&#20998;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#27599;&#27425;&#25512;&#26029;&#25152;&#38656;&#22823;&#37327;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09163</link><description>&lt;p&gt;
&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23398;&#20064;&#36864;&#20986;&#21644;&#25512;&#26029;&#65306;JEI-DNN
&lt;/p&gt;
&lt;p&gt;
Jointly-Learned Exit and Inference for a Dynamic Neural Network : JEI-DNN. (arXiv:2310.09163v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09163
&lt;/p&gt;
&lt;p&gt;
JEI-DNN&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#36864;&#20986;&#21644;&#25512;&#26029;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#22312;&#20013;&#38388;&#23618;&#36827;&#34892;&#37096;&#20998;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#27599;&#27425;&#25512;&#26029;&#25152;&#38656;&#22823;&#37327;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#32467;&#21512;&#24494;&#35843;&#24050;&#25104;&#20026;&#20027;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#21463;&#21040;&#27599;&#27425;&#25512;&#26029;&#25152;&#38656;&#30340;&#24040;&#22823;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;&#26089;&#26399;&#36864;&#20986;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#65288;EDNN&#65289;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#20174;&#20013;&#38388;&#23618;&#36827;&#34892;&#37096;&#20998;&#39044;&#27979;&#65288;&#21363;&#26089;&#26399;&#36864;&#20986;&#65289;&#26469;&#32469;&#36807;&#36825;&#20010;&#38382;&#39064;&#12290;&#35757;&#32451;EDNN&#26550;&#26500;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#21253;&#25324;&#20004;&#20010;&#30456;&#20114;&#20132;&#32455;&#30340;&#32452;&#20214;&#65306;&#25511;&#21046;&#26089;&#26399;&#36864;&#20986;&#20915;&#31574;&#30340;&#38376;&#25511;&#26426;&#21046;&#65288;GM&#65289;&#21644;&#25191;&#34892;&#20013;&#38388;&#34920;&#31034;&#25512;&#26029;&#30340;&#20013;&#38388;&#25512;&#26029;&#27169;&#22359;&#65288;IMs&#65289;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#38376;&#25511;&#26426;&#21046;&#30340;&#38408;&#20540;&#32622;&#20449;&#24230;&#24230;&#37327;&#65292;&#24182;&#21162;&#21147;&#25913;&#36827;&#22522;&#26412;&#30340;&#39592;&#24178;&#32593;&#32476;&#21644;&#25512;&#26029;&#27169;&#22359;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26377;&#20004;&#20010;&#22522;&#26412;&#32570;&#28857;&#65306;1&#65289;&#38376;&#25511;&#26426;&#21046;&#21644;&#20013;&#38388;&#25512;&#26029;&#27169;&#22359;&#19981;&#33021;&#20849;&#21516;&#23398;&#20064;&#21644;&#20248;&#21270;&#65292;2&#65289;GM&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;IMS&#21644;&#39592;&#24178;&#32593;&#32476;&#30340;&#36136;&#37327;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained models, coupled with fine-tuning, are slowly becoming established as the dominant architecture in machine learning. Even though these models offer impressive performance, their practical application is often limited by the prohibitive amount of resources required for every inference. Early-exiting dynamic neural networks (EDNN) circumvent this issue by allowing a model to make some of its predictions from intermediate layers (i.e., early-exit). Training an EDNN architecture is challenging as it consists of two intertwined components: the gating mechanism (GM) that controls early-exiting decisions and the intermediate inference modules (IMs) that perform inference from intermediate representations. As a result, most existing approaches rely on thresholding confidence metrics for the gating mechanism and strive to improve the underlying backbone network and the inference modules. Although successful, this approach has two fundamental shortcomings: 1) the GMs and the IMs 
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35299;&#20915;&#27668;&#20505;&#21464;&#21270;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#33021;&#28304;&#31995;&#32479;&#12289;&#27668;&#20505;&#25968;&#25454;&#39044;&#27979;&#12289;&#27668;&#20505;&#30417;&#27979;&#21644;&#21361;&#38505;&#20107;&#20214;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#30446;&#21069;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12289;&#23616;&#38480;&#20197;&#21450;&#28508;&#22312;&#26426;&#20250;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.09162</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#27668;&#20505;&#21464;&#21270;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning in Climate Change and Sustainability: a Review. (arXiv:2310.09162v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09162
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#35299;&#20915;&#27668;&#20505;&#21464;&#21270;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#33021;&#28304;&#31995;&#32479;&#12289;&#27668;&#20505;&#25968;&#25454;&#39044;&#27979;&#12289;&#27668;&#20505;&#30417;&#27979;&#21644;&#21361;&#38505;&#20107;&#20214;&#39044;&#27979;&#31561;&#39046;&#22495;&#12290;&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#30446;&#21069;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12289;&#23616;&#38480;&#20197;&#21450;&#28508;&#22312;&#26426;&#20250;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#21450;&#20854;&#23545;&#20840;&#29699;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#24433;&#21709;&#26159;&#20851;&#38190;&#25361;&#25112;&#65292;&#38656;&#35201;&#32467;&#21512;&#23574;&#31471;&#25216;&#26415;&#21644;&#31185;&#23398;&#35265;&#35299;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#21147;&#37327;&#26469;&#35299;&#20915;&#21253;&#25324;&#27668;&#20505;&#21464;&#21270;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#22312;&#20869;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#35299;&#20915;&#27668;&#20505;&#21464;&#21270;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#38382;&#39064;&#30340;&#29616;&#26377;&#25991;&#29486;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#26377;&#28508;&#21147;&#21152;&#36895;&#20943;&#30899;&#30340;QML&#26041;&#27861;&#65292;&#21253;&#25324;&#33021;&#28304;&#31995;&#32479;&#12289;&#27668;&#20505;&#25968;&#25454;&#39044;&#27979;&#12289;&#27668;&#20505;&#30417;&#27979;&#21644;&#21361;&#38505;&#20107;&#20214;&#39044;&#27979;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#30446;&#21069;&#30340;&#23616;&#38480;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#27668;&#20505;&#21464;&#21270;&#30740;&#31350;&#36825;&#19968;&#37325;&#35201;&#39046;&#22495;&#21033;&#29992;&#22522;&#20110;QML&#30340;&#26041;&#27861;&#30340;&#28508;&#22312;&#26426;&#20250;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change and its impact on global sustainability are critical challenges, demanding innovative solutions that combine cutting-edge technologies and scientific insights. Quantum machine learning (QML) has emerged as a promising paradigm that harnesses the power of quantum computing to address complex problems in various domains including climate change and sustainability. In this work, we survey existing literature that applies quantum machine learning to solve climate change and sustainability-related problems. We review promising QML methodologies that have the potential to accelerate decarbonization including energy systems, climate data forecasting, climate monitoring, and hazardous events predictions. We discuss the challenges and current limitations of quantum machine learning approaches and provide an overview of potential opportunities and future work to leverage QML-based methods in the important area of climate change research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#25214;&#21040;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;$d=2$&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#38454;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#20989;&#25968;&#20540;&#26597;&#35810;&#21363;&#21487;&#25214;&#21040;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.09157</link><description>&lt;p&gt;
&#23547;&#25214;&#38750;&#20984;&#20248;&#21270;&#20013;&#30340;&#31283;&#23450;&#28857;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Computational Complexity of Finding Stationary Points in Non-Convex Optimization. (arXiv:2310.09157v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#20984;&#20248;&#21270;&#20013;&#25214;&#21040;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;&#23545;&#20110;$d=2$&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#38454;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#20989;&#25968;&#20540;&#26597;&#35810;&#21363;&#21487;&#25214;&#21040;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#38750;&#20984;&#20294;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;$f$&#22312;&#26080;&#38480;&#21046;&#30340;$d$&#32500;&#22495;&#19978;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#65292;&#21363;&#26799;&#24230;&#36817;&#20284;&#20026;&#38646;&#30340;&#28857;&#65292;&#26159;&#32463;&#20856;&#38750;&#20984;&#20248;&#21270;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#24403;&#38382;&#39064;&#30340;&#32500;&#24230;$d$&#19982;&#36817;&#20284;&#35823;&#24046;&#29420;&#31435;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#20173;&#19981;&#21313;&#20998;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#19979;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#32467;&#26524;&#65306;1.&#22312;&#26080;&#38480;&#21046;&#30340;&#22495;&#20013;&#23547;&#25214;&#36817;&#20284;&#31283;&#23450;&#28857;&#30340;&#38382;&#39064;&#26159;PLS&#23436;&#20840;&#38382;&#39064;&#12290;2.&#23545;&#20110;$d=2$&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#38646;&#38454;&#31639;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#65292;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#20989;&#25968;&#36827;&#34892;&#26368;&#22810;$O(1/\varepsilon)$&#27425;&#20989;&#25968;&#20540;&#26597;&#35810;&#12290;3.&#25105;&#20204;&#35777;&#26126;&#24403;$d=2$&#26102;&#65292;&#20219;&#20309;&#31639;&#27861;&#33267;&#23569;&#38656;&#35201;$\Omega(1/\varepsilon)$&#27425;&#23545;&#30446;&#26631;&#20989;&#25968;&#21644;/&#25110;&#26799;&#24230;&#30340;&#26597;&#35810;&#26469;&#25214;&#21040;$\varepsilon$-&#36817;&#20284;&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding approximate stationary points, i.e., points where the gradient is approximately zero, of non-convex but smooth objective functions $f$ over unrestricted $d$-dimensional domains is one of the most fundamental problems in classical non-convex optimization. Nevertheless, the computational and query complexity of this problem are still not well understood when the dimension $d$ of the problem is independent of the approximation error. In this paper, we show the following computational and query complexity results:  1. The problem of finding approximate stationary points over unrestricted domains is PLS-complete.  2. For $d = 2$, we provide a zero-order algorithm for finding $\varepsilon$-approximate stationary points that requires at most $O(1/\varepsilon)$ value queries to the objective function.  3. We show that any algorithm needs at least $\Omega(1/\varepsilon)$ queries to the objective function and/or its gradient to find $\varepsilon$-approximate stationary points when $d=2$.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#31163;&#25955;&#21644;&#20998;&#27573;&#24120;&#25968;&#27979;&#24230;&#36827;&#34892;&#30340;&#32467;&#26500;&#36924;&#36817;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#28385;&#31209;&#30340;&#26684;&#28857;&#25353;&#27604;&#20363;&#32553;&#25918;&#21518;&#24471;&#21040;&#30340;Voronoi&#20998;&#21106;&#36924;&#36817;&#30340;&#27979;&#24230;&#35823;&#24046;&#26159;$O(h)$&#65292;&#36924;&#36817;&#30340;$N$&#39033;&#35823;&#24046;&#20026;$O(N^{-\frac1d})$&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38750;&#32039;&#25903;&#25745;&#27979;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.09149</link><description>&lt;p&gt;
&#24494;&#20998;&#27700;&#24179;&#31354;&#38388;&#20013;&#30340;&#26684;&#28857;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Lattice Approximations in Wasserstein Space. (arXiv:2310.09149v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Wasserstein&#31354;&#38388;&#20013;&#36890;&#36807;&#31163;&#25955;&#21644;&#20998;&#27573;&#24120;&#25968;&#27979;&#24230;&#36827;&#34892;&#30340;&#32467;&#26500;&#36924;&#36817;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#28385;&#31209;&#30340;&#26684;&#28857;&#25353;&#27604;&#20363;&#32553;&#25918;&#21518;&#24471;&#21040;&#30340;Voronoi&#20998;&#21106;&#36924;&#36817;&#30340;&#27979;&#24230;&#35823;&#24046;&#26159;$O(h)$&#65292;&#36924;&#36817;&#30340;$N$&#39033;&#35823;&#24046;&#20026;$O(N^{-\frac1d})$&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#38750;&#32039;&#25903;&#25745;&#27979;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;Wasserstein&#31354;&#38388;$W_p(\mathbb{R}^d)$&#20013;&#36890;&#36807;&#31163;&#25955;&#21644;&#20998;&#27573;&#24120;&#25968;&#27979;&#24230;&#26469;&#23545;&#27979;&#24230;&#36827;&#34892;&#32467;&#26500;&#36924;&#36817;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#19968;&#20010;&#28385;&#31209;&#30340;&#26684;&#28857;$\Lambda$&#25353;&#29031;$h\in(0,1]$&#30340;&#27604;&#20363;&#36827;&#34892;&#32553;&#25918;&#65292;&#37027;&#20040;&#22522;&#20110;$h\Lambda$&#30340;Voronoi&#20998;&#21106;&#24471;&#21040;&#30340;&#27979;&#24230;&#36924;&#36817;&#26159;$O(h)$&#65292;&#19981;&#35770;$d$&#25110;$p$&#30340;&#21462;&#20540;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#35206;&#30422;&#35770;&#35777;&#35777;&#26126;&#65292;&#23545;&#20110;&#32039;&#25903;&#25745;&#30340;&#27979;&#24230;&#30340;$N$&#39033;&#36924;&#36817;&#26159;$O(N^{-\frac1d})$&#65292;&#36825;&#19982;&#26368;&#20248;&#37327;&#21270;&#22120;&#21644;&#32463;&#39564;&#27979;&#24230;&#36924;&#36817;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#24050;&#30693;&#30340;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#25512;&#24191;&#21040;&#38750;&#32039;&#25903;&#25745;&#27979;&#24230;&#65292;&#35201;&#27714;&#20854;&#20855;&#26377;&#36275;&#22815;&#30340;&#34928;&#20943;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider structured approximation of measures in Wasserstein space $W_p(\mathbb{R}^d)$ for $p\in[1,\infty)$ by discrete and piecewise constant measures based on a scaled Voronoi partition of $\mathbb{R}^d$. We show that if a full rank lattice $\Lambda$ is scaled by a factor of $h\in(0,1]$, then approximation of a measure based on the Voronoi partition of $h\Lambda$ is $O(h)$ regardless of $d$ or $p$. We then use a covering argument to show that $N$-term approximations of compactly supported measures is $O(N^{-\frac1d})$ which matches known rates for optimal quantizers and empirical measure approximation in most instances. Finally, we extend these results to noncompactly supported measures with sufficient decay.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#21476;&#21704;&#29305;&#23450;&#24459;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#25928;&#24212;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#35777;&#34920;&#26126;&#22312;&#24191;&#27867;&#30340;&#29615;&#22659;&#21644;&#22870;&#21169;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#23545;&#19981;&#23436;&#32654;&#20195;&#29702;&#22870;&#21169;&#30340;&#36807;&#24230;&#20248;&#21270;&#20250;&#23548;&#33268;&#38477;&#20302;&#22312;&#30495;&#23454;&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20960;&#20309;&#35299;&#37322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#21476;&#21704;&#29305;&#23450;&#24459;&#30340;&#21457;&#29983;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36991;&#20813;&#38519;&#38449;&#30340;&#26368;&#20248;&#26089;&#20572;&#27490;&#26041;&#27861;&#65292;&#24182;&#25512;&#23548;&#20102;&#26041;&#27861;&#30340;&#29702;&#35770;&#36951;&#25022;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#28176;&#36807;&#28193;&#21040;&#30495;&#23454;&#22870;&#21169;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09144</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21476;&#21704;&#29305;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Goodhart's Law in Reinforcement Learning. (arXiv:2310.09144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#21476;&#21704;&#29305;&#23450;&#24459;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#25928;&#24212;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#35777;&#34920;&#26126;&#22312;&#24191;&#27867;&#30340;&#29615;&#22659;&#21644;&#22870;&#21169;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#23545;&#19981;&#23436;&#32654;&#20195;&#29702;&#22870;&#21169;&#30340;&#36807;&#24230;&#20248;&#21270;&#20250;&#23548;&#33268;&#38477;&#20302;&#22312;&#30495;&#23454;&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20960;&#20309;&#35299;&#37322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#21476;&#21704;&#29305;&#23450;&#24459;&#30340;&#21457;&#29983;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36991;&#20813;&#38519;&#38449;&#30340;&#26368;&#20248;&#26089;&#20572;&#27490;&#26041;&#27861;&#65292;&#24182;&#25512;&#23548;&#20102;&#26041;&#27861;&#30340;&#29702;&#35770;&#36951;&#25022;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#28176;&#36807;&#28193;&#21040;&#30495;&#23454;&#22870;&#21169;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23454;&#29616;&#23436;&#20840;&#25429;&#25417;&#22797;&#26434;&#20219;&#21153;&#30340;&#22870;&#21169;&#20989;&#25968;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#25226;&#22870;&#21169;&#20989;&#25968;&#35270;&#20026;&#30495;&#23454;&#30446;&#26631;&#30340;&#20195;&#29702;&#32780;&#38750;&#23450;&#20041;&#26159;&#21512;&#29702;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#21476;&#21704;&#29305;&#23450;&#24459;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#35813;&#23450;&#24459;&#39044;&#27979;&#22312;&#26576;&#19968;&#20020;&#30028;&#28857;&#20043;&#21518;&#65292;&#23545;&#19981;&#23436;&#32654;&#20195;&#29702;&#22870;&#21169;&#30340;&#36807;&#24230;&#20248;&#21270;&#20250;&#38477;&#20302;&#22312;&#30495;&#23454;&#30446;&#26631;&#19978;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#34913;&#37327;&#35813;&#25928;&#24212;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#35777;&#34920;&#26126;&#65292;&#22312;&#24191;&#27867;&#30340;&#29615;&#22659;&#21644;&#22870;&#21169;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#23545;&#19981;&#23436;&#32654;&#20195;&#29702;&#22870;&#21169;&#36827;&#34892;&#20248;&#21270;&#24120;&#24120;&#20250;&#23548;&#33268;&#21476;&#21704;&#29305;&#23450;&#24459;&#25152;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20960;&#20309;&#35299;&#37322;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#21457;&#29983;&#21476;&#21704;&#29305;&#23450;&#24459;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#29702;&#35770;&#27934;&#23519;&#20026;&#35813;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36991;&#20813;&#38519;&#38449;&#30340;&#26368;&#20248;&#26089;&#20572;&#27490;&#26041;&#27861;&#65292;&#24182;&#25512;&#23548;&#20102;&#35813;&#26041;&#27861;&#30340;&#29702;&#35770;&#36951;&#25022;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#20195;&#29702;&#22870;&#21169;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36880;&#27493;&#36807;&#28193;&#21040;&#30495;&#23454;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implementing a reward function that perfectly captures a complex task in the real world is impractical. As a result, it is often appropriate to think of the reward function as a proxy for the true objective rather than as its definition. We study this phenomenon through the lens of Goodhart's law, which predicts that increasing optimisation of an imperfect proxy beyond some critical point decreases performance on the true objective. First, we propose a way to quantify the magnitude of this effect and show empirically that optimising an imperfect proxy reward often leads to the behaviour predicted by Goodhart's law for a wide range of environments and reward functions. We then provide a geometric explanation for why Goodhart's law occurs in Markov decision processes. We use these theoretical insights to propose an optimal early stopping method that provably avoids the aforementioned pitfall and derive theoretical regret bounds for this method. Moreover, we derive a training method that 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#26041;&#27861;&#65292;&#23558;&#20854;&#35270;&#20026;&#35268;&#33539;&#21270;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#24207;&#21015;&#20449;&#21495;&#21338;&#24328;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#28857;&#24471;&#21040;&#20102;&#19968;&#20010;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38382;&#31572;&#21644;&#20854;&#20182;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.09139</link><description>&lt;p&gt;
&#20849;&#35782;&#28216;&#25103;&#65306;&#36890;&#36807;&#22343;&#34913;&#25628;&#32034;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Consensus Game: Language Model Generation via Equilibrium Search. (arXiv:2310.09139v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09139
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#26041;&#27861;&#65292;&#23558;&#20854;&#35270;&#20026;&#35268;&#33539;&#21270;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#24207;&#21015;&#20449;&#21495;&#21338;&#24328;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#36817;&#20284;&#22343;&#34913;&#28857;&#24471;&#21040;&#20102;&#19968;&#20010;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#38382;&#31572;&#21644;&#20854;&#20182;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24212;&#29992;&#20110;&#38382;&#31572;&#21644;&#20854;&#20182;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#24335;&#26597;&#35810;&#65288;&#36890;&#36807;&#20174;&#20854;&#36755;&#20986;&#20998;&#24067;&#20013;&#25277;&#26679;&#31572;&#26696;&#65289;&#25110;&#21028;&#21035;&#24335;&#26597;&#35810;&#65288;&#36890;&#36807;&#20351;&#29992;&#23427;&#20204;&#23545;&#19968;&#32452;&#20505;&#36873;&#36755;&#20986;&#36827;&#34892;&#35780;&#20998;&#25110;&#25490;&#24207;&#65289;&#36827;&#34892;&#26597;&#35810;&#12290;&#36825;&#20123;&#36807;&#31243;&#26377;&#26102;&#20250;&#20135;&#29983;&#38750;&#24120;&#19981;&#21516;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#22914;&#20309;&#35843;&#21644;&#20114;&#19981;&#30456;&#23481;&#30340;&#35780;&#20998;&#36807;&#31243;&#20197;&#33719;&#24471;&#36830;&#36143;&#30340;LM&#39044;&#27979;&#21602;&#65311;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#12289;&#21338;&#24328;&#35770;&#36807;&#31243;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#35270;&#20026;&#19968;&#31181;&#35268;&#33539;&#21270;&#30340;&#19981;&#23436;&#32654;&#20449;&#24687;&#24207;&#21015;&#20449;&#21495;&#21338;&#24328; - &#31216;&#20026;&#20849;&#35782;&#28216;&#25103; - &#22312;&#35813;&#21338;&#24328;&#20013;&#65292;&#19968;&#20010;&#29983;&#25104;&#22120;&#35797;&#22270;&#29992;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20256;&#36798;&#19968;&#20010;&#25277;&#35937;&#30340;&#27491;&#30830;&#24615;&#21442;&#25968;&#32473;&#19968;&#20010;&#21028;&#21035;&#22120;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#35745;&#31639;&#31243;&#24207;&#26469;&#25214;&#21040;&#36825;&#20010;&#21338;&#24328;&#30340;&#36817;&#20284;&#22343;&#34913;&#28857;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;EQUILIBRIUM-RANKING&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;&#24212;&#29992;&#20110;&#22823;&#37327;&#20219;&#21153;&#65288;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#65292;&#24120;&#35782;&#65289;
&lt;/p&gt;
&lt;p&gt;
When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate outputs). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game - which we term the CONSENSUS GAME - in which a GENERATOR seeks to communicate an abstract correctness parameter using natural language sentences to a DISCRIMINATOR. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call EQUILIBRIUM-RANKING. Applied to a large number of tasks (including reading comprehension, commonsen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#21487;&#20998;&#35299;&#27169;&#22411;&#20043;&#38388;&#36793;&#38469;&#21644;&#26465;&#20214;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#20998;&#24067;&#20013;&#31934;&#30830;&#35745;&#31639;&#24046;&#24322;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.09129</link><description>&lt;p&gt;
&#35745;&#31639;&#21487;&#20998;&#35299;&#27169;&#22411;&#20043;&#38388;&#30340;&#36793;&#38469;&#21644;&#26465;&#20214;&#24046;&#24322;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Computing Marginal and Conditional Divergences between Decomposable Models with Applications. (arXiv:2310.09129v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09129
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#21487;&#20998;&#35299;&#27169;&#22411;&#20043;&#38388;&#36793;&#38469;&#21644;&#26465;&#20214;&#24046;&#24322;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#20998;&#24067;&#20013;&#31934;&#30830;&#35745;&#31639;&#24046;&#24322;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#35745;&#31639;&#20004;&#20010;&#39640;&#32500;&#20998;&#24067;&#20043;&#38388;&#30340;&#31934;&#30830;&#24046;&#24322;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#30452;&#25509;&#35745;&#31639;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#23545;&#20110;&#20004;&#20010;&#21487;&#20998;&#35299;&#27169;&#22411;&#65288;&#21363;&#24358;&#22270;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#65289;&#30340;&#32852;&#21512;&#20998;&#24067;&#35745;&#31639;&#945;-&#946;&#24046;&#24322;&#65288;&#21253;&#25324;Kullback-Leibler&#24046;&#24322;&#21644;Hellinger&#36317;&#31163;&#65289;&#21487;&#20197;&#22312;&#25351;&#25968;&#26102;&#38388;&#20869;&#25104;&#20026;&#36825;&#20123;&#27169;&#22411;&#30340;&#26641;&#23485;&#24230;&#12290;&#28982;&#32780;&#65292;&#23558;&#20004;&#20010;&#39640;&#32500;&#23545;&#35937;&#20043;&#38388;&#30340;&#19981;&#30456;&#20284;&#24615;&#20943;&#23569;&#20026;&#21333;&#20010;&#26631;&#37327;&#20540;&#21487;&#33021;&#26159;&#19981;&#20855;&#26377;&#20449;&#24687;&#24615;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#35832;&#22914;&#30417;&#30563;&#23398;&#20064;&#30340;&#24212;&#29992;&#20013;&#65292;&#23545;&#20110;&#26465;&#20214;&#20998;&#24067;&#30340;&#24046;&#24322;&#21487;&#33021;&#26356;&#26377;&#20852;&#36259;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35745;&#31639;&#20004;&#20010;&#21487;&#20998;&#35299;&#27169;&#22411;&#30340;&#20219;&#20309;&#36793;&#38469;&#25110;&#26465;&#20214;&#20998;&#24067;&#20043;&#38388;&#30340;&#31934;&#30830;&#945;-&#946;&#24046;&#24322;&#12290;&#20197;&#21487;&#34892;&#30340;&#26041;&#24335;&#36827;&#34892;&#27492;&#35745;&#31639;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#23545;&#36825;&#20123;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#36827;&#34892;&#20998;&#35299;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to compute the exact divergence between two high-dimensional distributions is useful in many applications but doing so naively is intractable. Computing the alpha-beta divergence -- a family of divergences that includes the Kullback-Leibler divergence and Hellinger distance -- between the joint distribution of two decomposable models, i.e chordal Markov networks, can be done in time exponential in the treewidth of these models. However, reducing the dissimilarity between two high-dimensional objects to a single scalar value can be uninformative. Furthermore, in applications such as supervised learning, the divergence over a conditional distribution might be of more interest. Therefore, we propose an approach to compute the exact alpha-beta divergence between any marginal or conditional distribution of two decomposable models. Doing so tractably is non-trivial as we need to decompose the divergence between these distributions and therefore, require a decomposition over the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25237;&#24433;&#32858;&#31867;&#30340;&#23398;&#20064;&#30028;&#38480;&#38382;&#39064;&#65292;&#32473;&#20986;&#20102;&#20960;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#23545;&#20110;&#22522;&#20110;&#20013;&#24515;&#30340;&#30446;&#26631;&#65292;&#23637;&#31034;&#20102;&#25910;&#25947;&#36895;&#29575;&#20026;O(sqrt(k/n))&#12290;</title><link>http://arxiv.org/abs/2310.09127</link><description>&lt;p&gt;
&#20851;&#20110;&#25237;&#24433;&#32858;&#31867;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
On Generalization Bounds for Projective Clustering. (arXiv:2310.09127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25237;&#24433;&#32858;&#31867;&#30340;&#23398;&#20064;&#30028;&#38480;&#38382;&#39064;&#65292;&#32473;&#20986;&#20102;&#20960;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#23545;&#20110;&#22522;&#20110;&#20013;&#24515;&#30340;&#30446;&#26631;&#65292;&#23637;&#31034;&#20102;&#25910;&#25947;&#36895;&#29575;&#20026;O(sqrt(k/n))&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#32452;&#28857;&#65292;&#32858;&#31867;&#26159;&#23558;&#28857;&#38598;&#20998;&#25104;k&#20010;&#31751;&#30340;&#36807;&#31243;&#65292;&#20351;&#24471;&#27599;&#20010;&#28857;&#34987;&#20998;&#37197;&#21040;&#30340;&#20013;&#24515;&#23613;&#21487;&#33021;&#25509;&#36817;&#12290;&#26368;&#24120;&#35265;&#30340;&#26159;&#23558;&#20013;&#24515;&#28857;&#35774;&#32622;&#20026;&#28857;&#26412;&#36523;&#65292;&#36825;&#23548;&#33268;&#20102;&#33879;&#21517;&#30340;k-median&#21644;k-means&#30446;&#26631;&#12290;&#20063;&#21487;&#20197;&#36873;&#25321;&#23558;&#20013;&#24515;&#28857;&#35774;&#32622;&#20026;j&#32500;&#23376;&#31354;&#38388;&#65292;&#20174;&#32780;&#20135;&#29983;&#23376;&#31354;&#38388;&#32858;&#31867;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#23398;&#20064;&#30028;&#38480;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#32473;&#23450;&#19968;&#20010;&#20174;&#26576;&#20010;&#26410;&#30693;&#20294;&#22266;&#23450;&#20998;&#24067;D&#20013;&#29420;&#31435;&#25277;&#21462;&#30340;n&#20010;&#26679;&#26412;P&#38598;&#21512;&#65292;P&#19978;&#35745;&#31639;&#30340;&#35299;&#22914;&#20309;&#24555;&#36895;&#25910;&#25947;&#21040;D&#30340;&#26368;&#20339;&#32858;&#31867;&#65311;&#25105;&#20204;&#32473;&#20986;&#20102;&#20960;&#20010;&#36817;&#20046;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#22522;&#20110;&#20013;&#24515;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25910;&#25947;&#36895;&#29575;&#20026;O(sqrt(k/n))&#65292;&#19982;&#24050;&#30693;&#30340;&#26368;&#20248;&#30028;&#38480;[Fefferman, Mitter, and Narayanan, Journal of the Mathematical Society 2016]&#21644;[Bartlett, Linder, and Lugosi, IEEE Trans. Inf. Theory 1998]&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a set of points, clustering consists of finding a partition of a point set into $k$ clusters such that the center to which a point is assigned is as close as possible. Most commonly, centers are points themselves, which leads to the famous $k$-median and $k$-means objectives. One may also choose centers to be $j$ dimensional subspaces, which gives rise to subspace clustering. In this paper, we consider learning bounds for these problems. That is, given a set of $n$ samples $P$ drawn independently from some unknown, but fixed distribution $\mathcal{D}$, how quickly does a solution computed on $P$ converge to the optimal clustering of $\mathcal{D}$? We give several near optimal results. In particular,  For center-based objectives, we show a convergence rate of $\tilde{O}\left(\sqrt{{k}/{n}}\right)$. This matches the known optimal bounds of [Fefferman, Mitter, and Narayanan, Journal of the Mathematical Society 2016] and [Bartlett, Linder, and Lugosi, IEEE Trans. Inf. Theory 1998] fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#65288;PNNP&#65289;&#29992;&#20110;&#20934;&#30830;&#22122;&#22768;&#24314;&#27169;&#21644;&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;&#65292;&#38598;&#25104;&#20102;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#35299;&#32806;&#12289;&#29289;&#29702;&#24341;&#23548;&#20195;&#29702;&#27169;&#22411;&#21644;&#21487;&#24494;&#20998;&#20998;&#24067;&#23548;&#21521;&#25439;&#22833;&#31561;&#39640;&#25928;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.09126</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#30340;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#29992;&#20110;&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Physics-guided Noise Neural Proxy for Low-light Raw Image Denoising. (arXiv:2310.09126v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#65288;PNNP&#65289;&#29992;&#20110;&#20934;&#30830;&#22122;&#22768;&#24314;&#27169;&#21644;&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;&#65292;&#38598;&#25104;&#20102;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#35299;&#32806;&#12289;&#29289;&#29702;&#24341;&#23548;&#20195;&#29702;&#27169;&#22411;&#21644;&#21487;&#24494;&#20998;&#20998;&#24067;&#23548;&#21521;&#25439;&#22833;&#31561;&#39640;&#25928;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;&#22312;&#31227;&#21160;&#25668;&#24433;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#20026;&#20027;&#27969;&#26041;&#27861;&#12290;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#25104;&#20026;&#26367;&#20195;&#23545;&#24212;&#30495;&#23454;&#25968;&#25454;&#30340;&#39640;&#25928;&#23454;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#21463;&#22122;&#22768;&#27169;&#22411;&#31934;&#24230;&#30340;&#38480;&#21046;&#65292;&#38477;&#20302;&#20102;&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20934;&#30830;&#22122;&#22768;&#24314;&#27169;&#26694;&#26550;&#65292;&#23398;&#20064;&#19968;&#20010;&#20174;&#26263;&#22330;&#20013;&#33719;&#24471;&#30340;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#65288;PNNP&#65289;&#12290;PNNP&#38598;&#25104;&#20102;&#19977;&#31181;&#39640;&#25928;&#25216;&#26415;&#65306;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#35299;&#32806;&#65288;PND&#65289;&#65292;&#29289;&#29702;&#24341;&#23548;&#20195;&#29702;&#27169;&#22411;&#65288;PPM&#65289;&#21644;&#21487;&#24494;&#20998;&#20998;&#24067;&#23548;&#21521;&#25439;&#22833;&#65288;DDL&#65289;&#12290;PND&#23558;&#26263;&#22330;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#32452;&#20998;&#65292;&#24182;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#22788;&#29702;&#19981;&#21516;&#27700;&#24179;&#30340;&#22122;&#22768;&#65292;&#38477;&#20302;&#20102;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#30340;&#22797;&#26434;&#24230;&#12290;PPM&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#20808;&#39564;&#26377;&#25928;&#22320;&#32422;&#26463;&#29983;&#25104;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-light raw image denoising plays a crucial role in mobile photography, and learning-based methods have become the mainstream approach. Training the learning-based methods with synthetic data emerges as an efficient and practical alternative to paired real data. However, the quality of synthetic data is inherently limited by the low accuracy of the noise model, which decreases the performance of low-light raw image denoising. In this paper, we develop a novel framework for accurate noise modeling that learns a physics-guided noise neural proxy (PNNP) from dark frames. PNNP integrates three efficient techniques: physics-guided noise decoupling (PND), physics-guided proxy model (PPM), and differentiable distribution-oriented loss (DDL). The PND decouples the dark frame into different components and handles different levels of noise in a flexible manner, which reduces the complexity of the noise neural proxy. The PPM incorporates physical priors to effectively constrain the generated no
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#35757;&#32451;&#21644;&#39044;&#27979;&#35270;&#35273;&#35823;&#24046;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#39044;&#27979;&#37325;&#29992;&#30528;&#33394;&#25110;&#20351;&#29992;&#38477;&#20302;&#30528;&#33394;&#29575;&#23548;&#33268;&#30340;&#35270;&#35273;&#35823;&#24046;&#65292;&#36991;&#20813;&#20102;&#23545;&#21442;&#32771;&#25110;&#28210;&#26579;&#22270;&#20687;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2310.09125</link><description>&lt;p&gt;
&#23454;&#26102;&#24212;&#29992;&#20013;&#30340;&#35757;&#32451;&#21644;&#39044;&#27979;&#35270;&#35273;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Training and Predicting Visual Error for Real-Time Applications. (arXiv:2310.09125v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#35757;&#32451;&#21644;&#39044;&#27979;&#35270;&#35273;&#35823;&#24046;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#39044;&#27979;&#37325;&#29992;&#30528;&#33394;&#25110;&#20351;&#29992;&#38477;&#20302;&#30528;&#33394;&#29575;&#23548;&#33268;&#30340;&#35270;&#35273;&#35823;&#24046;&#65292;&#36991;&#20813;&#20102;&#23545;&#21442;&#32771;&#25110;&#28210;&#26579;&#22270;&#20687;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35823;&#24046;&#24230;&#37327;&#22312;&#37327;&#21270;&#24863;&#30693;&#22270;&#20687;&#30456;&#20284;&#24615;&#20013;&#36215;&#30528;&#22522;&#26412;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#20986;&#29616;&#20102;&#23427;&#20204;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#22914;&#20869;&#23481;&#33258;&#36866;&#24212;&#30528;&#33394;&#21644;&#30528;&#33394;&#37325;&#29992;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#25913;&#21892;&#25928;&#29575;&#12290;&#24050;&#32463;&#24314;&#31435;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20854;&#20013;&#26368;&#22797;&#26434;&#30340;&#33021;&#22815;&#25429;&#25417;&#21040;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#24863;&#30693;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#23545;&#21442;&#32771;&#22270;&#20687;&#25110;&#28210;&#26579;&#22270;&#20687;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#20351;&#24471;&#36825;&#20123;&#24212;&#29992;&#21482;&#33021;&#20351;&#29992;&#26368;&#31616;&#21333;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#38656;&#35201;&#21442;&#32771;&#25110;&#28210;&#26579;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#21508;&#31181;&#35270;&#35273;&#24230;&#37327;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#21644;&#37096;&#32626;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#37325;&#29992;&#30528;&#33394;&#25110;&#20351;&#29992;&#38477;&#20302;&#30528;&#33394;&#29575;&#23548;&#33268;&#30340;&#35270;&#35273;&#35823;&#24046;&#12290;&#26368;&#32456;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#32771;&#34385;&#21040;&#36825;&#20123;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual error metrics play a fundamental role in the quantification of perceived image similarity. Most recently, use cases for them in real-time applications have emerged, such as content-adaptive shading and shading reuse to increase performance and improve efficiency. A wide range of different metrics has been established, with the most sophisticated being capable of capturing the perceptual characteristics of the human visual system. However, their complexity, computational expense, and reliance on reference images to compare against prevent their generalized use in real-time, restricting such applications to using only the simplest available metrics. In this work, we explore the abilities of convolutional neural networks to predict a variety of visual metrics without requiring either reference or rendered images. Specifically, we train and deploy a neural network to estimate the visual error resulting from reusing shading or using reduced shading rates. The resulting models account
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#27169;&#25311;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29992;&#25143;&#28385;&#24847;&#24230;&#25351;&#26631;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#38899;&#20048;&#25773;&#25918;&#21015;&#34920;&#30340;&#33258;&#21160;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#20462;&#25913;&#29256;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#35757;&#32451;&#20986;&#30340;&#31574;&#30053;&#33021;&#22815;&#20174;&#22823;&#22411;&#21644;&#21160;&#24577;&#30340;&#20505;&#36873;&#39033;&#30446;&#38598;&#20013;&#36827;&#34892;&#25512;&#33616;&#65292;&#20197;&#26368;&#22823;&#21270;&#28040;&#36153;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.09123</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#30340;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#29983;&#25104;&#38899;&#20048;&#25773;&#25918;&#21015;&#34920;
&lt;/p&gt;
&lt;p&gt;
Automatic Music Playlist Generation via Simulation-based Reinforcement Learning. (arXiv:2310.09123v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#27169;&#25311;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29992;&#25143;&#28385;&#24847;&#24230;&#25351;&#26631;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#38899;&#20048;&#25773;&#25918;&#21015;&#34920;&#30340;&#33258;&#21160;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#20462;&#25913;&#29256;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#35757;&#32451;&#20986;&#30340;&#31574;&#30053;&#33021;&#22815;&#20174;&#22823;&#22411;&#21644;&#21160;&#24577;&#30340;&#20505;&#36873;&#39033;&#30446;&#38598;&#20013;&#36827;&#34892;&#25512;&#33616;&#65292;&#20197;&#26368;&#22823;&#21270;&#28040;&#36153;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#38899;&#20048;&#25773;&#25918;&#21015;&#34920;&#26159;&#38899;&#20048;&#27969;&#23186;&#20307;&#26381;&#21153;&#20013;&#24120;&#35265;&#30340;&#21151;&#33021;&#65292;&#20294;&#20256;&#32479;&#30340;&#25216;&#26415;&#65292;&#22914;&#21327;&#21516;&#36807;&#28388;&#65292;&#20381;&#36182;&#20110;&#23545;&#20869;&#23481;&#36136;&#37327;&#30340;&#26126;&#30830;&#20551;&#35774;&#65292;&#20197;&#23398;&#20064;&#22914;&#20309;&#36827;&#34892;&#25512;&#33616;&#12290;&#36825;&#20123;&#20551;&#35774;&#24448;&#24448;&#23548;&#33268;&#31163;&#32447;&#27169;&#22411;&#30446;&#26631;&#21644;&#22312;&#32447;&#29992;&#25143;&#28385;&#24847;&#24230;&#25351;&#26631;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#30340;&#25773;&#25918;&#21015;&#34920;&#29983;&#25104;&#29615;&#22659;&#30452;&#25509;&#20248;&#21270;&#29992;&#25143;&#28385;&#24847;&#24230;&#25351;&#26631;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#27169;&#25311;&#22120;&#24320;&#21457;&#21644;&#35757;&#32451;&#20102;&#19968;&#20010;&#20462;&#25913;&#29256;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#65292;&#31216;&#20026;AH-DQN&#65292;&#22312;&#22788;&#29702;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#22823;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#26102;&#33021;&#22815;&#35299;&#20915;&#25361;&#25112;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31574;&#30053;&#33021;&#22815;&#20174;&#22823;&#22411;&#21644;&#21160;&#24577;&#30340;&#20505;&#36873;&#39033;&#30446;&#38598;&#20013;&#36827;&#34892;&#25512;&#33616;&#65292;&#20197;&#26368;&#22823;&#21270;&#28040;&#36153;&#25351;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#29615;&#22659;&#27169;&#25311;&#36827;&#34892;&#31163;&#32447;&#35780;&#20272;&#21644;&#20998;&#26512;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization of playlists is a common feature in music streaming services, but conventional techniques, such as collaborative filtering, rely on explicit assumptions regarding content quality to learn how to make recommendations. Such assumptions often result in misalignment between offline model objectives and online user satisfaction metrics. In this paper, we present a reinforcement learning framework that solves for such limitations by directly optimizing for user satisfaction metrics via the use of a simulated playlist-generation environment. Using this simulator we develop and train a modified Deep Q-Network, the action head DQN (AH-DQN), in a manner that addresses the challenges imposed by the large state and action space of our RL formulation. The resulting policy is capable of making recommendations from large and dynamic sets of candidate items with the expectation of maximizing consumption metrics. We analyze and evaluate agents offline via simulations that use environmen
&lt;/p&gt;</description></item><item><title>DSG&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#25991;&#26723;&#35299;&#26512;&#31995;&#32479;&#65292;&#33021;&#22815;&#23558;&#28210;&#26579;&#25991;&#26723;&#26144;&#23556;&#21040;&#32467;&#26500;&#21270;&#30340;&#23618;&#27425;&#26684;&#24335;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#39640;&#25928;&#21644;&#28789;&#27963;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.09118</link><description>&lt;p&gt;
DSG: &#19968;&#31181;&#31471;&#21040;&#31471;&#25991;&#26723;&#32467;&#26500;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
DSG: An End-to-End Document Structure Generator. (arXiv:2310.09118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09118
&lt;/p&gt;
&lt;p&gt;
DSG&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#25991;&#26723;&#35299;&#26512;&#31995;&#32479;&#65292;&#33021;&#22815;&#23558;&#28210;&#26579;&#25991;&#26723;&#26144;&#23556;&#21040;&#32467;&#26500;&#21270;&#30340;&#23618;&#27425;&#26684;&#24335;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#39640;&#25928;&#21644;&#28789;&#27963;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#12289;&#30740;&#31350;&#21644;&#20844;&#20849;&#37096;&#38376;&#30340;&#20449;&#24687;&#36890;&#24120;&#23384;&#20648;&#20026;&#28210;&#26579;&#25991;&#26723;&#65288;&#22914;PDF&#25991;&#20214;&#12289;&#25195;&#25551;&#20214;&#65289;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#23454;&#29616;&#19979;&#28216;&#20219;&#21153;&#65292;&#38656;&#35201;&#23558;&#28210;&#26579;&#25991;&#26723;&#26144;&#23556;&#21040;&#32467;&#26500;&#21270;&#30340;&#23618;&#27425;&#26684;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#34987;&#21551;&#21457;&#24335;&#26041;&#27861;&#25152;&#38480;&#21046;&#65292;&#19988;&#26080;&#27861;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026; Document Structure Generator (DSG) &#30340;&#26032;&#22411;&#25991;&#26723;&#35299;&#26512;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#36827;&#34892;&#23436;&#20840;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#12290;DSG&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#26512;&#65288;i&#65289;&#25991;&#26723;&#20013;&#30340;&#23454;&#20307;&#65288;&#22914;&#22270;&#20687;&#12289;&#25991;&#26412;&#22359;&#12289;&#26631;&#39064;&#31561;&#65289;&#21644;&#65288;ii&#65289;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#39034;&#24207;&#21644;&#23884;&#22871;&#32467;&#26500;&#30340;&#20851;&#31995;&#12290;&#19982;&#20381;&#36182;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#29616;&#26377;&#31995;&#32479;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;DSG&#36827;&#34892;&#20102;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#65292;&#20351;&#20854;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#39640;&#25928;&#19988;&#28789;&#27963;&#12290;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#20010;&#21517;&#20026;E-Periodica&#30340;&#26032;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20855;&#26377;&#22797;&#26434;&#25991;&#26723;&#32467;&#26500;&#30340;&#23454;&#38469;&#26434;&#24535;&#65292;&#29992;&#20110;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Information in industry, research, and the public sector is widely stored as rendered documents (e.g., PDF files, scans). Hence, to enable downstream tasks, systems are needed that map rendered documents onto a structured hierarchical format. However, existing systems for this task are limited by heuristics and are not end-to-end trainable. In this work, we introduce the Document Structure Generator (DSG), a novel system for document parsing that is fully end-to-end trainable. DSG combines a deep neural network for parsing (i) entities in documents (e.g., figures, text blocks, headers, etc.) and (ii) relations that capture the sequence and nested structure between entities. Unlike existing systems that rely on heuristics, our DSG is trained end-to-end, making it effective and flexible for real-world applications. We further contribute a new, large-scale dataset called E-Periodica comprising real-world magazines with complex document structures for evaluation. Our results demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#21019;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#22823;&#35268;&#27169;&#21382;&#21490;&#25991;&#29486;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#37325;&#28857;&#30740;&#31350;&#20102;&#8220;Sacrobosco Collection&#8221;&#20013;&#30693;&#35782;&#30340;&#28436;&#21464;&#12290;&#36890;&#36807;&#36825;&#19968;&#30740;&#31350;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#21382;&#21490;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.09091</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;XAI&#22312;&#36229;&#36234;&#20154;&#31867;&#33021;&#21147;&#30340;&#35268;&#27169;&#19978;&#27934;&#23519;&#21382;&#21490;&#26469;&#28304;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Insightful analysis of historical sources at scales beyond human capabilities using unsupervised Machine Learning and XAI. (arXiv:2310.09091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21019;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#22823;&#35268;&#27169;&#21382;&#21490;&#25991;&#29486;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#37325;&#28857;&#30740;&#31350;&#20102;&#8220;Sacrobosco Collection&#8221;&#20013;&#30693;&#35782;&#30340;&#28436;&#21464;&#12290;&#36890;&#36807;&#36825;&#19968;&#30740;&#31350;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#21382;&#21490;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#36164;&#26009;&#20016;&#23500;&#65292;&#20294;&#22914;&#20309;&#23558;&#20154;&#31867;&#30693;&#35782;&#30340;&#28436;&#21464;&#21644;&#20256;&#25773;&#36827;&#34892;&#25972;&#21512;&#65292;&#26080;&#35770;&#26159;&#26102;&#38388;&#19978;&#36824;&#26159;&#31354;&#38388;&#19978;&#30340;&#65292;&#37117;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#21482;&#33021;&#36827;&#34892;&#26377;&#38480;&#30340;&#36873;&#25321;&#24615;&#30740;&#31350;&#12290;&#24040;&#22823;&#30340;&#21382;&#21490;&#36164;&#26009;&#37327;&#20351;&#24471;&#20840;&#38754;&#30740;&#31350;&#25104;&#20026;&#19981;&#21487;&#33021;&#65292;&#22240;&#20026;&#20154;&#31867;&#19987;&#23478;&#30340;&#25968;&#37327;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#37327;&#21382;&#21490;&#36164;&#26009;&#20197;&#25968;&#23383;&#24418;&#24335;&#21487;&#33719;&#24471;&#65292;AI&#36741;&#21161;&#21382;&#21490;&#20998;&#26512;&#26377;&#20102;&#24076;&#26395;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#22823;&#35268;&#27169;&#21382;&#21490;&#25991;&#29486;&#36827;&#34892;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#28145;&#20837;&#30340;&#21382;&#21490;&#27934;&#23519;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#8220;Sacrobosco Collection&#8221;&#20013;&#30340;&#30693;&#35782;&#28436;&#21464;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;359&#20010;&#26089;&#26399;&#29616;&#20195;&#21360;&#21047;&#29256;&#22825;&#25991;&#23398;&#25945;&#31185;&#20070;&#30340;&#25968;&#23383;&#21270;&#25910;&#34255;&#21697;&#65292;&#36825;&#20123;&#25945;&#31185;&#20070;&#22312;1472&#24180;&#33267;1650&#24180;&#38388;&#22312;&#27431;&#27954;&#22823;&#23398;&#20351;&#29992;&#65292;&#22823;&#32422;&#26377;76,000&#39029;&#65292;&#20854;&#20013;&#35768;&#22810;&#21253;&#21547;&#22825;&#25991;&#21644;&#35745;&#31639;&#34920;&#26684;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#36825;&#19968;&#25910;&#34255;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historical materials are abundant. Yet, piecing together how human knowledge has evolved and spread both diachronically and synchronically remains a challenge that can so far only be very selectively addressed. The vast volume of materials precludes comprehensive studies, given the restricted number of human specialists. However, as large amounts of historical materials are now available in digital form there is a promising opportunity for AI-assisted historical analysis. In this work, we take a pivotal step towards analyzing vast historical corpora by employing innovative machine learning (ML) techniques, enabling in-depth historical insights on a grand scale. Our study centers on the evolution of knowledge within the `Sacrobosco Collection' -- a digitized collection of 359 early modern printed editions of textbooks on astronomy used at European universities between 1472 and 1650 -- roughly 76,000 pages, many of which contain astronomic, computational tables. An ML based analysis of t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#24577;&#20248;&#21270;&#25340;&#36710;&#24179;&#21488;&#20013;&#30340;&#35746;&#21333;&#21305;&#37197;&#21644;&#36710;&#36742;&#37325;&#23450;&#20301;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#23618;&#21644;&#27169;&#22359;&#21270;&#30340;&#24314;&#27169;&#32467;&#26500;&#36827;&#34892;&#36710;&#27969;&#36716;&#31227;&#27169;&#24335;&#21644;&#24555;&#36895;&#21305;&#37197;&#37325;&#23450;&#20301;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#32593;&#32476;&#20013;&#33021;&#22815;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09071</link><description>&lt;p&gt;
&#22312;&#32447;&#30340;&#25340;&#36710;&#26381;&#21153;&#37325;&#23450;&#20301;&#21644;&#21305;&#37197;: &#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Relocating and Matching of Ride-Hailing Services: A Model-Based Modular Approach. (arXiv:2310.09071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#24577;&#20248;&#21270;&#25340;&#36710;&#24179;&#21488;&#20013;&#30340;&#35746;&#21333;&#21305;&#37197;&#21644;&#36710;&#36742;&#37325;&#23450;&#20301;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20004;&#23618;&#21644;&#27169;&#22359;&#21270;&#30340;&#24314;&#27169;&#32467;&#26500;&#36827;&#34892;&#36710;&#27969;&#36716;&#31227;&#27169;&#24335;&#21644;&#24555;&#36895;&#21305;&#37197;&#37325;&#23450;&#20301;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#32593;&#32476;&#20013;&#33021;&#22815;&#36798;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;(MMA)&#65292;&#29992;&#20110;&#22312;&#25340;&#36710;&#24179;&#21488;&#20013;&#21160;&#24577;&#20248;&#21270;&#35746;&#21333;&#21305;&#37197;&#21644;&#36710;&#36742;&#37325;&#23450;&#20301;&#12290;MMA&#21033;&#29992;&#20102;&#19968;&#20010;&#20004;&#23618;&#21644;&#27169;&#22359;&#21270;&#30340;&#24314;&#27169;&#32467;&#26500;&#12290;&#19978;&#23618;&#30830;&#23450;&#31995;&#32479;&#20869;&#36710;&#27969;&#30340;&#31354;&#38388;&#36716;&#31227;&#27169;&#24335;&#65292;&#20197;&#26368;&#22823;&#21270;&#24403;&#21069;&#21644;&#26410;&#26469;&#38454;&#27573;&#30340;&#24635;&#25910;&#20837;&#12290;&#22312;&#19978;&#23618;&#30340;&#25351;&#23548;&#19979;&#65292;&#19979;&#23618;&#36827;&#34892;&#20102;&#24555;&#36895;&#30340;&#36710;&#36742;-&#35746;&#21333;&#21305;&#37197;&#21644;&#36710;&#36742;&#37325;&#23450;&#20301;&#12290;MMA&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#37197;&#22791;&#20102;&#23450;&#21046;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#20316;&#20026;&#19968;&#31181;&#22312;&#32447;&#35746;&#21333;&#21305;&#37197;&#21644;&#36710;&#36742;&#37325;&#23450;&#20301;&#31639;&#27861;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#25968;&#21315;&#36742;&#36710;&#36742;&#12290;&#25105;&#20204;&#22312;&#31616;&#21270;&#32593;&#32476;&#19978;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#32780;&#22522;&#20110;&#29609;&#20855;&#32593;&#32476;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#25209;&#37327;&#21305;&#37197;&#21644;&#24378;&#21270;&#21305;&#37197;&#30456;&#27604;&#65292;MMA&#33021;&#22815;&#23454;&#29616;&#26356;&#20248;&#24322;&#30340;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes an innovative model-based modular approach (MMA) to dynamically optimize order matching and vehicle relocation in a ride-hailing platform. MMA utilizes a two-layer and modular modeling structure. The upper layer determines the spatial transfer patterns of vehicle flow within the system to maximize the total revenue of the current and future stages. With the guidance provided by the upper layer, the lower layer performs rapid vehicle-to-order matching and vehicle relocation. MMA is interpretable, and equipped with the customized and polynomial-time algorithm, which, as an online order-matching and vehicle-relocation algorithm, can scale past thousands of vehicles. We theoretically prove that the proposed algorithm can achieve the global optimum in stylized networks, while the numerical experiments based on both the toy network and realistic dataset demonstrate that MMA is capable of achieving superior systematic performance compared to batch matching and reinforcemen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KCTS&#30340;&#30693;&#35782;&#32422;&#26463;&#26641;&#25628;&#32034;&#35299;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#20998;&#31867;&#22120;&#21644;MCTS&#25351;&#23548;&#20923;&#32467;&#30340;LM&#29983;&#25104;&#19982;&#21442;&#32771;&#30693;&#35782;&#23545;&#40784;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#32423;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;RIPA&#12290;</title><link>http://arxiv.org/abs/2310.09044</link><description>&lt;p&gt;
KCTS&#65306;&#24102;&#26377;&#20196;&#29260;&#32423;&#24187;&#35273;&#26816;&#27979;&#30340;&#30693;&#35782;&#32422;&#26463;&#26641;&#25628;&#32034;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection. (arXiv:2310.09044v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09044
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KCTS&#30340;&#30693;&#35782;&#32422;&#26463;&#26641;&#25628;&#32034;&#35299;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#20998;&#31867;&#22120;&#21644;MCTS&#25351;&#23548;&#20923;&#32467;&#30340;LM&#29983;&#25104;&#19982;&#21442;&#32771;&#30693;&#35782;&#23545;&#40784;&#30340;&#25991;&#26412;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#32423;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;RIPA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#20154;&#31867;&#32423;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20135;&#29983;&#38169;&#35823;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#21363;&#25152;&#35859;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20854;&#37096;&#32626;&#26500;&#25104;&#37325;&#22823;&#39118;&#38505;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#36755;&#20837;&#20013;&#30340;&#30693;&#35782;&#23545;LLM&#36827;&#34892;&#31934;&#32454;&#35843;&#33410;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#24341;&#36215;&#39640;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#21487;&#33021;&#23545;&#22810;&#20219;&#21153;&#27169;&#22411;&#36896;&#25104;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;KCTS&#65288;&#30693;&#35782;&#32422;&#26463;&#26641;&#25628;&#32034;&#65289;&#30340;&#30693;&#35782;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#30693;&#35782;&#20998;&#31867;&#22120;&#24471;&#20998;&#21644;MCTS&#65288;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65289;&#26469;&#25351;&#23548;&#20923;&#32467;&#30340;LM&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#29983;&#25104;&#19982;&#21442;&#32771;&#30693;&#35782;&#23545;&#40784;&#30340;&#25991;&#26412;&#12290;&#20026;&#20102;&#23558;&#24207;&#21015;&#32423;&#30693;&#35782;&#20998;&#31867;&#22120;&#36866;&#24212;&#20196;&#29260;&#32423;&#25351;&#23548;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#32423;&#24187;&#35273;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;RIPA&#65288;&#22870;&#21169;&#25296;&#28857;&#36817;&#20284;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable human-level natural language generation capabilities. However, their potential to generate misinformation, often called the hallucination problem, poses a significant risk to their deployment. A common approach to address this issue is to retrieve relevant knowledge and fine-tune the LLM with the knowledge in its input. Unfortunately, this method incurs high training costs and may cause catastrophic forgetting for multi-tasking models. To overcome these limitations, we propose a knowledge-constrained decoding method called KCTS (Knowledge-Constrained Tree Search), which guides a frozen LM to generate text aligned with the reference knowledge at each decoding step using a knowledge classifier score and MCTS (Monte-Carlo Tree Search). To adapt the sequence-level knowledge classifier to token-level guidance, we also propose a novel token-level hallucination detection method called RIPA (Reward Inflection Point Approximation). Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#26102;&#38388;&#27573;&#30005;&#20215;&#35745;&#21010;&#65292;&#36890;&#36807;&#20248;&#21270;&#35843;&#24230;&#30005;&#21160;&#36710;&#20805;&#30005;&#36807;&#31243;&#26469;&#38477;&#20302;&#29992;&#25143;&#23478;&#24237;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.09040</link><description>&lt;p&gt;
&#32771;&#34385;&#32456;&#31471;&#29992;&#25143;&#28789;&#27963;&#24615;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30005;&#21160;&#36710;&#20805;&#30005;&#20248;&#21270;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimal Scheduling of Electric Vehicle Charging with Deep Reinforcement Learning considering End Users Flexibility. (arXiv:2310.09040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#26102;&#38388;&#27573;&#30005;&#20215;&#35745;&#21010;&#65292;&#36890;&#36807;&#20248;&#21270;&#35843;&#24230;&#30005;&#21160;&#36710;&#20805;&#30005;&#36807;&#31243;&#26469;&#38477;&#20302;&#29992;&#25143;&#23478;&#24237;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#65288;&#29305;&#21035;&#26159;&#30005;&#21160;&#27773;&#36710;&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#39044;&#35745;&#22312;&#26410;&#26469;&#21313;&#24180;&#20869;&#23558;&#22823;&#24133;&#22686;&#21152;&#23545;&#29616;&#26377;&#30005;&#21147;&#20998;&#37197;&#32593;&#32476;&#30340;&#21387;&#21147;&#65292;&#22686;&#21152;&#20102;&#23545;&#26356;&#39640;&#31995;&#32479;&#21487;&#38752;&#24615;&#21644;&#28789;&#27963;&#24615;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#32593;&#32476;&#25237;&#36164;&#24182;&#22686;&#21152;&#23545;&#20998;&#37197;&#32593;&#32476;&#30340;&#21487;&#25511;&#24615;&#65292;&#32593;&#32476;&#36816;&#33829;&#21830;&#24320;&#23637;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#65292;&#20197;&#40723;&#21169;&#32456;&#31471;&#29992;&#25143;&#22312;&#22238;&#25253;&#32463;&#27982;&#25110;&#20854;&#20182;&#21033;&#30410;&#30340;&#21069;&#25552;&#19979;&#36716;&#31227;&#20854;&#29992;&#30005;&#28040;&#32791;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#22312;&#20303;&#23429;&#36127;&#33655;&#35843;&#24230;&#24212;&#29992;&#39046;&#22495;&#22788;&#20110;&#30740;&#31350;&#21069;&#27839;&#65292;&#20027;&#35201;&#30001;&#20110;&#20854;&#39640;&#31934;&#24230;&#24615;&#12289;&#39640;&#35745;&#31639;&#36895;&#24230;&#21644;&#36739;&#20302;&#23545;&#20110;&#27491;&#22312;&#24320;&#21457;&#30340;&#27169;&#22411;&#30340;&#29289;&#29702;&#29305;&#24615;&#30340;&#20381;&#36182;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22312;&#26102;&#38388;&#27573;&#30005;&#20215;&#35745;&#21010;&#19979;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30830;&#23450;&#29992;&#25143;&#23478;&#24237;&#30005;&#21160;&#36710;&#20943;&#23569;&#25104;&#26412;&#30340;&#20805;&#30005;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of decentralized energy resources and especially Electric Vehicles (EV), that are expected to increase sharply over the next decade, will put further stress on existing power distribution networks, increasing the need for higher system reliability and flexibility. In an attempt to avoid unnecessary network investments and to increase the controllability over distribution networks, network operators develop demand response (DR) programs that incentivize end users to shift their consumption in return for financial or other benefits. Artificial intelligence (AI) methods are in the research forefront for residential load scheduling applications, mainly due to their high accuracy, high computational speed and lower dependence on the physical characteristics of the models under development. The aim of this work is to identify households' EV cost-reducing charging policy under a Time-of-Use tariff scheme, with the use of Deep Reinforcement Learning, and more specifically Deep
&lt;/p&gt;</description></item><item><title>MINDE&#26159;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#20989;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#25991;&#29486;&#20013;&#30340;&#20027;&#35201;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09031</link><description>&lt;p&gt;
MINDE: &#20114;&#20449;&#24687;&#31070;&#32463;&#25193;&#25955;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
MINDE: Mutual Information Neural Diffusion Estimation. (arXiv:2310.09031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09031
&lt;/p&gt;
&lt;p&gt;
MINDE&#26159;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#20989;&#25968;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#25991;&#29486;&#20013;&#30340;&#20027;&#35201;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;Girsanov&#23450;&#29702;&#30340;&#21407;&#21019;&#35299;&#37322;&#65292;&#20801;&#35768;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#20272;&#35745;&#20004;&#20010;&#23494;&#24230;&#20989;&#25968;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#65292;&#35813;&#20272;&#35745;&#26159;&#23427;&#20204;&#24471;&#20998;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#22815;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#30340;&#29109;&#12290;&#20511;&#21161;&#36825;&#26679;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#27979;&#37327;MI&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#26041;&#21521;&#23637;&#24320;&#65306;&#19968;&#20010;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#36807;&#31243;&#65292;&#21478;&#19968;&#20010;&#20351;&#29992;&#32852;&#21512;&#25193;&#25955;&#36807;&#31243;&#65292;&#21487;&#20197;&#21516;&#26102;&#23545;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26469;&#33258;&#20110;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#21508;&#31181;&#21464;&#20307;&#36827;&#34892;&#24443;&#24213;&#30340;&#23454;&#39564;&#21327;&#35758;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#25991;&#29486;&#20013;&#30340;&#20027;&#35201;&#26367;&#20195;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20102;MI&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
In this work we present a new method for the estimation of Mutual Information (MI) between random variables. Our approach is based on an original interpretation of the Girsanov theorem, which allows us to use score-based diffusion models to estimate the Kullback Leibler divergence between two densities as a difference between their score functions. As a by-product, our method also enables the estimation of the entropy of random variables. Armed with such building blocks, we present a general recipe to measure MI, which unfolds in two directions: one uses conditional diffusion process, whereas the other uses joint diffusion processes that allow simultaneous modelling of two random variables. Our results, which derive from a thorough experimental protocol over all the variants of our approach, indicate that our method is more accurate than the main alternatives from the literature, especially for challenging distributions. Furthermore, our methods pass MI self-consistency tests, includin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#23376;&#31354;&#38388;&#36866;&#24212;&#20808;&#39564;&#31639;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#21021;&#22987;&#21270;&#21442;&#25968;&#21644;&#21442;&#25968;&#23376;&#31354;&#38388;&#65292;&#21487;&#20197;&#22522;&#20110;&#20219;&#21153;&#20998;&#24067;&#20915;&#23450;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35843;&#25972;&#21738;&#20123;&#25805;&#20316;&#23376;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#24182;&#38477;&#20302;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2310.09028</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#23376;&#31354;&#38388;&#36866;&#24212;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Subspace Adaptation Prior for Few-Shot Learning. (arXiv:2310.09028v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09028
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#23376;&#31354;&#38388;&#36866;&#24212;&#20808;&#39564;&#31639;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#23398;&#20064;&#21021;&#22987;&#21270;&#21442;&#25968;&#21644;&#21442;&#25968;&#23376;&#31354;&#38388;&#65292;&#21487;&#20197;&#22522;&#20110;&#20219;&#21153;&#20998;&#24067;&#20915;&#23450;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35843;&#25972;&#21738;&#20123;&#25805;&#20316;&#23376;&#38598;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#24182;&#38477;&#20302;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#22522;&#20110;&#30340;&#20803;&#23398;&#20064;&#25216;&#26415;&#26088;&#22312;&#20174;&#19968;&#31995;&#21015;&#35757;&#32451;&#20219;&#21153;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#20415;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#36866;&#24212;&#21487;&#35757;&#32451;&#23618;&#30340;&#25152;&#26377;&#21442;&#25968;&#12290;&#36825;&#24573;&#30053;&#20102;&#23545;&#20110;&#32473;&#23450;&#20219;&#21153;&#20998;&#24067;&#26469;&#35828;&#21487;&#33021;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#19988;&#21487;&#33021;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#65292;&#20854;&#20013;&#24517;&#39035;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23376;&#31354;&#38388;&#36866;&#24212;&#20808;&#39564;(SAP)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#21516;&#26102;&#23398;&#20064;&#33391;&#22909;&#30340;&#21021;&#22987;&#21270;&#21442;&#25968;(&#20808;&#39564;&#30693;&#35782;)&#21644;&#21442;&#25968;&#23376;&#31354;&#38388;&#65292;&#20197;&#25805;&#20316;&#23376;&#38598;&#30340;&#24418;&#24335;&#34920;&#31034;&#24212;&#35813;&#26159;&#21487;&#36866;&#24212;&#30340;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;SAP&#21487;&#20197;&#26681;&#25454;&#28508;&#22312;&#30340;&#20219;&#21153;&#20998;&#24067;&#23398;&#20064;&#24212;&#35813;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35843;&#25972;&#30340;&#25805;&#20316;&#23376;&#38598;&#65292;&#21516;&#26102;&#38477;&#20302;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based meta-learning techniques aim to distill useful prior knowledge from a set of training tasks such that new tasks can be learned more efficiently with gradient descent. While these methods have achieved successes in various scenarios, they commonly adapt all parameters of trainable layers when learning new tasks. This neglects potentially more efficient learning strategies for a given task distribution and may be susceptible to overfitting, especially in few-shot learning where tasks must be learned from a limited number of examples. To address these issues, we propose Subspace Adaptation Prior (SAP), a novel gradient-based meta-learning algorithm that jointly learns good initialization parameters (prior knowledge) and layer-wise parameter subspaces in the form of operation subsets that should be adaptable. In this way, SAP can learn which operation subsets to adjust with gradient descent based on the underlying task distribution, simultaneously decreasing the risk of over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#32534;&#30721;&#30340;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550; (REFML) &#29992;&#20110;&#23569;&#26679;&#26412;&#25925;&#38556;&#35786;&#26029;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#25554;&#20540;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.09002</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#31034;&#32534;&#30721;&#30340;&#32852;&#37030;&#20803;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Federated Meta-Learning for Few-Shot Fault Diagnosis with Representation Encoding. (arXiv:2310.09002v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#32534;&#30721;&#30340;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550; (REFML) &#29992;&#20110;&#23569;&#26679;&#26412;&#25925;&#38556;&#35786;&#26029;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#25554;&#20540;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28982;&#32780;&#30001;&#20110;&#20998;&#24067;&#22312;&#19981;&#21516;&#23454;&#20307;&#20013;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#38590;&#33719;&#21462;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#20445;&#38556;&#25968;&#25454;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#65292;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471; collaboratively &#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#21644;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#20250;&#38477;&#20302;&#20840;&#23616;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#34920;&#31034;&#32534;&#30721;&#30340;&#32852;&#37030;&#20803;&#23398;&#20064; (REFML) &#26694;&#26550;&#29992;&#20110;&#23569;&#26679;&#26412;&#25925;&#38556;&#35786;&#26029;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#32534;&#30721;&#21644;&#20803;&#23398;&#20064;&#30340;&#26032;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#23427;&#20805;&#20998;&#21033;&#29992;&#20102;&#35757;&#32451;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#65292;&#23558;&#20854;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#23545;&#26410;&#35265;&#24037;&#20917;&#25110;&#35774;&#22791;&#31867;&#22411;&#30340;&#36234;&#22495;&#27867;&#21270;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25554;&#20540;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#23616;&#37096;&#27169;&#22411;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#26368;&#20248;&#32452;&#21512;&#65292;&#20316;&#20026;&#23616;&#37096;&#35757;&#32451;&#30340;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based fault diagnosis (FD) approaches require a large amount of training data, which are difficult to obtain since they are located across different entities. Federated learning (FL) enables multiple clients to collaboratively train a shared model with data privacy guaranteed. However, the domain discrepancy and data scarcity problems among clients deteriorate the performance of the global FL model. To tackle these issues, we propose a novel framework called representation encoding-based federated meta-learning (REFML) for few-shot FD. First, a novel training strategy based on representation encoding and meta-learning is developed. It harnesses the inherent heterogeneity among training clients, effectively transforming it into an advantage for out-of-distribution generalization on unseen working conditions or equipment types. Additionally, an adaptive interpolation method that calculates the optimal combination of local and global models as the initialization of local tra
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#32447;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#25511;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22235;&#20010;&#24615;&#33021;&#20803;&#24230;&#37327;&#65306;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#30340;&#39057;&#29575;&#12289;&#36825;&#20123;&#19979;&#38477;&#30340;&#24133;&#24230;&#12289;&#24674;&#22797;&#29575;&#21644;&#24615;&#33021;&#30340;&#27874;&#21160;&#12290;</title><link>http://arxiv.org/abs/2310.09000</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#34913;&#37327;&#36807;&#31243;&#32467;&#26524;&#39044;&#27979;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Measuring the Stability of Process Outcome Predictions in Online Settings. (arXiv:2310.09000v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#22312;&#32447;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#25511;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22235;&#20010;&#24615;&#33021;&#20803;&#24230;&#37327;&#65306;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#30340;&#39057;&#29575;&#12289;&#36825;&#20123;&#19979;&#38477;&#30340;&#24133;&#24230;&#12289;&#24674;&#22797;&#29575;&#21644;&#24615;&#33021;&#30340;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#25511;&#26088;&#22312;&#20351;&#29992;&#21382;&#21490;&#20107;&#20214;&#25968;&#25454;&#39044;&#27979;&#36807;&#31243;&#23454;&#20363;&#30340;&#26410;&#26469;&#36827;&#23637;&#12290;&#38543;&#30528;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#25511;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#21450;&#26102;&#24178;&#39044;&#65292;&#35780;&#20272;&#24213;&#23618;&#27169;&#22411;&#30340;&#24615;&#33021;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20854;&#38543;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36825;&#22312;&#39640;&#39118;&#38505;&#30340;&#21830;&#19994;&#22330;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#38169;&#35823;&#30340;&#39044;&#27979;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36890;&#24120;&#20351;&#29992;&#21333;&#20010;&#27719;&#24635;&#20540;&#25110;&#26102;&#38388;&#24207;&#21015;&#21487;&#35270;&#21270;&#26469;&#35780;&#20272;&#39044;&#27979;&#27169;&#22411;&#65292;&#36825;&#20351;&#24471;&#35780;&#20272;&#20854;&#24615;&#33021;&#29305;&#21035;&#26159;&#38543;&#26102;&#38388;&#30340;&#31283;&#23450;&#24615;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#32447;&#39044;&#27979;&#24615;&#36807;&#31243;&#30417;&#25511;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#22235;&#20010;&#24615;&#33021;&#20803;&#24230;&#37327;&#65306;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#30340;&#39057;&#29575;&#65292;&#36825;&#20123;&#19979;&#38477;&#30340;&#24133;&#24230;&#65292;&#24674;&#22797;&#29575;&#21644;&#24615;&#33021;&#30340;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive Process Monitoring aims to forecast the future progress of process instances using historical event data. As predictive process monitoring is increasingly applied in online settings to enable timely interventions, evaluating the performance of the underlying models becomes crucial for ensuring their consistency and reliability over time. This is especially important in high risk business scenarios where incorrect predictions may have severe consequences. However, predictive models are currently usually evaluated using a single, aggregated value or a time-series visualization, which makes it challenging to assess their performance and, specifically, their stability over time. This paper proposes an evaluation framework for assessing the stability of models for online predictive process monitoring. The framework introduces four performance meta-measures: the frequency of significant performance drops, the magnitude of such drops, the recovery rate, and the volatility of perfor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#31215;&#26497;&#25903;&#25345;&#37325;&#26032;&#35268;&#21010;&#20915;&#31574;&#26469;&#20943;&#23569;&#33322;&#29677;&#24310;&#35823;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#21382;&#21490;&#37325;&#26032;&#35268;&#21010;&#25968;&#25454;&#21644;&#22825;&#27668;&#25968;&#25454;&#39044;&#27979;&#26410;&#26469;&#20960;&#22825;&#26159;&#21542;&#20250;&#21457;&#24067;&#37325;&#26032;&#35268;&#21010;&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#24433;&#21709;&#33322;&#32447;&#30340;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2310.08988</link><description>&lt;p&gt;
&#36335;&#32447;&#37325;&#26032;&#35268;&#21010;&#39044;&#27979;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Reroute Prediction Service. (arXiv:2310.08988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08988
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#31215;&#26497;&#25903;&#25345;&#37325;&#26032;&#35268;&#21010;&#20915;&#31574;&#26469;&#20943;&#23569;&#33322;&#29677;&#24310;&#35823;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#21382;&#21490;&#37325;&#26032;&#35268;&#21010;&#25968;&#25454;&#21644;&#22825;&#27668;&#25968;&#25454;&#39044;&#27979;&#26410;&#26469;&#20960;&#22825;&#26159;&#21542;&#20250;&#21457;&#24067;&#37325;&#26032;&#35268;&#21010;&#24314;&#35758;&#65292;&#20197;&#20943;&#23569;&#24433;&#21709;&#33322;&#32447;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#22312;2019&#24180;&#65292;&#32654;&#22269;&#22269;&#23478;&#33322;&#31354;&#31354;&#38388;&#31995;&#32479;&#30340;&#24310;&#35823;&#25104;&#26412;&#23601;&#20272;&#35745;&#20026;330&#20159;&#32654;&#20803;&#65292;&#36825;&#26159;&#36807;&#21435;&#20960;&#24180;&#22686;&#38271;&#36235;&#21183;&#30340;&#23792;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#24040;&#22823;&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#31215;&#26497;&#25903;&#25345;&#37325;&#26032;&#35268;&#21010;&#20915;&#31574;&#26469;&#20943;&#23569;&#24310;&#35823;&#12290;&#35813;&#31995;&#32479;&#22312;&#26410;&#26469;&#20960;&#22825;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#65292;&#39044;&#27979;&#26576;&#20010;&#31354;&#20013;&#31649;&#21046;&#21306;&#22495;&#25110;&#26576;&#20010;&#25351;&#23450;&#30340;&#21672;&#35810;&#26631;&#35782;&#26159;&#21542;&#20250;&#21457;&#24067;&#37325;&#26032;&#35268;&#21010;&#24314;&#35758;&#65292;&#20174;&#32780;&#21487;&#33021;&#24433;&#21709;&#30456;&#20851;&#36335;&#32447;&#12290;&#20026;&#20102;&#25552;&#20379;&#36825;&#20123;&#39044;&#27979;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#20174;FAA&#25552;&#20379;&#30340;&#31995;&#32479;&#33539;&#22260;&#20449;&#24687;&#31649;&#29702;&#65288;SWIM&#65289;&#25968;&#25454;&#26381;&#21153;&#21644;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#39044;&#27979;&#20013;&#24515;&#65288;NCEP&#65289;&#25552;&#20379;&#30340;&#22825;&#27668;&#25968;&#25454;&#25910;&#38598;&#30340;&#21382;&#21490;&#37325;&#26032;&#35268;&#21010;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#37327;&#24222;&#22823;&#65292;&#21253;&#21547;&#35768;&#22810;&#20197;&#39640;&#36895;&#29575;&#27969;&#24335;&#20256;&#36755;&#30340;&#19981;&#30456;&#20851;&#21644;&#22122;&#22768;&#25968;&#25454;&#12290;&#31995;&#32479;&#25345;&#32493;&#22788;&#29702;&#36827;&#20837;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cost of delays was estimated as 33 billion US dollars only in 2019 for the US National Airspace System, a peak value following a growth trend in past years. Aiming to address this huge inefficiency, we designed and developed a novel Data Analytics and Machine Learning system, which aims at reducing delays by proactively supporting re-routing decisions.  Given a time interval up to a few days in the future, the system predicts if a reroute advisory for a certain Air Route Traffic Control Center or for a certain advisory identifier will be issued, which may impact the pertinent routes. To deliver such predictions, the system uses historical reroute data, collected from the System Wide Information Management (SWIM) data services provided by the FAA, and weather data, provided by the US National Centers for Environmental Prediction (NCEP). The data is huge in volume, and has many items streamed at high velocity, uncorrelated and noisy. The system continuously processes the incoming raw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21338;&#24328;&#35770;&#24179;&#34913;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;PAGE&#65292;&#36890;&#36807;&#23558;&#32852;&#37030;&#23398;&#20064;&#35270;&#20026;&#23458;&#25143;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#21512;&#20316;&#31454;&#20105;&#21338;&#24328;&#65292;&#24182;&#32467;&#21512;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#23547;&#25214;&#24179;&#34913;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.08961</link><description>&lt;p&gt;
PAGE: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24179;&#34913;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
PAGE: Equilibrate Personalization and Generalization in Federated Learning. (arXiv:2310.08961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21338;&#24328;&#35770;&#24179;&#34913;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;PAGE&#65292;&#36890;&#36807;&#23558;&#32852;&#37030;&#23398;&#20064;&#35270;&#20026;&#23458;&#25143;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#21512;&#20316;&#31454;&#20105;&#21338;&#24328;&#65292;&#24182;&#32467;&#21512;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#23547;&#25214;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#27491;&#22312;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#26381;&#21153;&#30340;&#20027;&#35201;&#25512;&#21160;&#21147;&#65292;&#20854;&#20013;&#23458;&#25143;&#65288;&#23458;&#25143;&#31471;&#65289;&#22312;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;&#26381;&#21153;&#22120;&#65289;&#30340;&#21327;&#35843;&#19979;&#20849;&#21516;&#20174;&#20849;&#20139;&#30340;&#26412;&#22320;&#26356;&#26032;&#20013;&#21463;&#30410;&#12290;&#32771;&#34385;&#21040;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#36127;&#38754;&#25928;&#24212;&#65292;&#30740;&#31350;&#20154;&#21592;&#20998;&#21035;&#35843;&#26597;&#20102;&#23458;&#25143;&#30340;&#24403;&#21069;&#38656;&#27714;&#21644;&#26381;&#21153;&#22120;&#30340;&#26410;&#26469;&#38656;&#27714;&#65292;&#21363;&#26412;&#22320;&#27169;&#22411;&#20010;&#24615;&#21270;&#21644;&#20840;&#23616;&#27169;&#22411;&#27867;&#21270;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#21462;&#20195;&#21478;&#19968;&#20010;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#30475;&#20284;&#31454;&#20105;&#30340;&#30446;&#26631;&#37117;&#21516;&#26679;&#37325;&#35201;&#65292;&#32780;&#19981;&#26159;&#40657;&#30333;&#38382;&#39064;&#65292;&#24212;&#35813;&#21516;&#26102;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;&#31532;&#19968;&#20010;&#24179;&#34913;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;PAGE&#12290;&#20026;&#20102;&#25506;&#32034;&#22343;&#34913;&#65292;PAGE&#23558;&#21338;&#24328;&#36827;&#19968;&#27493;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#31616;&#21270;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is becoming a major driving force behind machine learning as a service, where customers (clients) collaboratively benefit from shared local updates under the orchestration of the service provider (server). Representing clients' current demands and the server's future demand, local model personalization and global model generalization are separately investigated, as the ill-effects of data heterogeneity enforce the community to focus on one over the other. However, these two seemingly competing goals are of equal importance rather than black and white issues, and should be achieved simultaneously. In this paper, we propose the first algorithm to balance personalization and generalization on top of game theory, dubbed PAGE, which reshapes FL as a co-opetition game between clients and the server. To explore the equilibrium, PAGE further formulates the game as Markov decision processes, and leverages the reinforcement learning algorithm, which simplifies the solving
&lt;/p&gt;</description></item><item><title>CAMELL&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#38656;&#19987;&#23478;&#26631;&#27880;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#12289;&#33258;&#30417;&#30563;&#21644;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#26469;&#35299;&#20915;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.08944</link><description>&lt;p&gt;
CAMELL&#65306;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#39640;&#25928;&#33258;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#19982;&#26631;&#31614;&#39564;&#35777;&#33719;&#21462;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CAMELL: Confidence-based Acquisition Model for Efficient Self-supervised Active Learning with Label Validation. (arXiv:2310.08944v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08944
&lt;/p&gt;
&lt;p&gt;
CAMELL&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#38656;&#19987;&#23478;&#26631;&#27880;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#12289;&#33258;&#30417;&#30563;&#21644;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#26469;&#35299;&#20915;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#21463;&#22823;&#35268;&#27169;&#19988;&#31934;&#30830;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#65292;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#21463;&#21040;&#38459;&#30861;&#12290;&#26631;&#27880;&#36136;&#37327;&#38543;&#30528;&#20174;&#19987;&#23478;&#26631;&#27880;&#21521;&#20247;&#21253;&#26631;&#27880;&#30340;&#36716;&#21464;&#32780;&#36880;&#28176;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMELL&#65288;Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#27744;&#21270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;CAMELL&#20855;&#26377;&#19977;&#20010;&#26680;&#24515;&#29305;&#28857;&#65306;(1)&#20165;&#35201;&#27714;&#19987;&#23478;&#26631;&#27880;&#25152;&#36873;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;(2)&#20026;&#20854;&#20313;&#24207;&#21015;&#25552;&#20379;&#33258;&#30417;&#30563;&#65292;(3)&#37319;&#29992;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#65292;&#38450;&#27490;&#38169;&#35823;&#26631;&#31614;&#27745;&#26579;&#25968;&#25454;&#38598;&#24182;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#24207;&#21015;&#20219;&#21153;&#20013;&#23545;CAMELL&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#29305;&#21035;&#24378;&#35843;&#23545;&#35805;&#20449;&#24565;&#36319;&#36394;&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labelling. To address these challenges, we present \textbf{CAMELL} (Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation), a pool-based active learning framework tailored for sequential multi-output problems. CAMELL possesses three core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, (2) it facilitates self-supervision for the remainder of the sequence, and (3) it employs a label validation mechanism to prevent erroneous labels from contaminating the dataset and harming model performance. We evaluate CAMELL on sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#36718;&#21453;&#39304;-&#20462;&#35746;&#26426;&#21046;&#21644;&#23376;&#20219;&#21153;&#37325;&#26631;&#35760;&#65292;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#25506;&#32034;&#24182;&#25552;&#39640;&#20854;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08922</link><description>&lt;p&gt;
LLaMA Rider&#65306;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#24320;&#25918;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
LLaMA Rider: Spurring Large Language Models to Explore the Open World. (arXiv:2310.08922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#36718;&#21453;&#39304;-&#20462;&#35746;&#26426;&#21046;&#21644;&#23376;&#20219;&#21153;&#37325;&#26631;&#35760;&#65292;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#25506;&#32034;&#24182;&#25552;&#39640;&#20854;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22810;&#39033;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29615;&#22659;&#20013;&#24110;&#21161;&#20915;&#31574;&#21644;&#35268;&#21010;&#65292;&#24182;&#23581;&#35797;&#23558;LLMs&#30340;&#30693;&#35782;&#19982;&#19990;&#30028;&#26465;&#20214;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#25345;&#32493;&#33719;&#21462;&#29615;&#22659;&#30693;&#35782;&#24182;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#36866;&#24212;&#30340;&#33021;&#21147;&#20173;&#28982;&#19981;&#30830;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25512;&#21160;LLMs&#25506;&#32034;&#24320;&#25918;&#19990;&#30028;&#65292;&#25910;&#38598;&#32463;&#39564;&#65292;&#24182;&#23398;&#20064;&#25552;&#39640;&#20854;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#21033;&#29992;&#22810;&#36718;&#21453;&#39304;-&#20462;&#35746;&#26426;&#21046;&#65292;&#40723;&#21169;LLMs&#26681;&#25454;&#29615;&#22659;&#30340;&#21453;&#39304;&#20449;&#24687;&#20027;&#21160;&#36873;&#25321;&#36866;&#24403;&#30340;&#20462;&#35746;&#25805;&#20316;&#65292;&#20419;&#36827;&#25506;&#32034;&#24182;&#22686;&#24378;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25972;&#21512;&#20102;&#23376;&#20219;&#21153;&#37325;&#26631;&#35760;&#65292;&#20197;&#24110;&#21161;LLMs&#22312;&#23376;&#20219;&#21153;&#35268;&#21010;&#20013;&#20445;&#25345;&#19968;&#33268;&#24615;&#65292;&#24182;&#24110;&#21161;&#27169;&#22411;&#23398;&#20064;&#20219;&#21153;&#20043;&#38388;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#22522;&#20110;&#25152;&#33719;&#24471;&#30340;&#25506;&#32034;&#32463;&#39564;&#30340;&#35757;&#32451;&#23436;&#25104;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, various studies have leveraged Large Language Models (LLMs) to help decision-making and planning in environments, and try to align the LLMs' knowledge with the world conditions. Nonetheless, the capacity of LLMs to continuously acquire environmental knowledge and adapt in an open world remains uncertain. In this paper, we propose an approach to spur LLMs to explore the open world, gather experiences, and learn to improve their task-solving capabilities. In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment. This facilitates exploration and enhances the model's performance. Besides, we integrate sub-task relabeling to assist LLMs in maintaining consistency in sub-task planning and help the model learn the combinatorial nature between tasks, enabling it to complete a wider range of tasks through training based on the acquired exploration experi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#25991;&#26412;&#27700;&#21360;&#26041;&#27861;Easymark&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#21547;&#20041;&#30340;&#24773;&#20917;&#19979;&#27880;&#20837;&#27700;&#21360;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#24230;&#21487;&#20449;&#22320;&#26816;&#27979;&#20986;&#25991;&#26412;&#26159;&#21542;&#30001;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#31995;&#32479;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;BLEU&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.08920</link><description>&lt;p&gt;
&#32670;&#20154;&#31616;&#21333;&#30340;&#25991;&#23383;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Embarrassingly Simple Text Watermarks. (arXiv:2310.08920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08920
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#25991;&#26412;&#27700;&#21360;&#26041;&#27861;Easymark&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#21547;&#20041;&#30340;&#24773;&#20917;&#19979;&#27880;&#20837;&#27700;&#21360;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#24230;&#21487;&#20449;&#22320;&#26816;&#27979;&#20986;&#25991;&#26412;&#26159;&#21542;&#30001;&#37319;&#29992;&#35813;&#26041;&#27861;&#30340;&#31995;&#32479;&#29983;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Easymark&#65292;&#36825;&#26159;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#27700;&#21360;&#26041;&#27861;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#25991;&#26412;&#27700;&#21360;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;LLM&#21487;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#26080;&#27861;&#21306;&#20998;&#30340;&#25991;&#26412;&#65292;&#36825;&#23545;&#25991;&#26412;&#30340;&#21487;&#20449;&#24230;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#12290;Easymark&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;Easymark&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#25991;&#26412;&#21547;&#20041;&#30340;&#24773;&#20917;&#19979;&#27880;&#20837;&#27700;&#21360;&#65292;&#32780;&#39564;&#35777;&#22120;&#21487;&#20197;&#39640;&#24230;&#21487;&#20449;&#22320;&#26816;&#27979;&#20986;&#25991;&#26412;&#26159;&#21542;&#30001;&#37319;&#29992;Easymark&#30340;&#31995;&#32479;&#29983;&#25104;&#12290;Easymark&#38750;&#24120;&#23481;&#26131;&#23454;&#29616;&#65292;&#21482;&#38656;&#35201;&#20960;&#34892;&#20195;&#30721;&#12290;Easymark&#19981;&#38656;&#35201;&#35775;&#38382;LLMs&#65292;&#22240;&#27492;&#24403;LLM&#25552;&#20379;&#32773;&#19981;&#25552;&#20379;&#24102;&#27700;&#21360;&#30340;LLM&#26102;&#65292;&#21487;&#20197;&#22312;&#29992;&#25143;&#31471;&#23454;&#26045;&#12290;&#23613;&#31649;&#23427;&#24456;&#31616;&#21333;&#65292;&#20294;&#23427;&#33021;&#22815;&#23454;&#29616;&#27604;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#27700;&#21360;&#26041;&#27861;&#26356;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;BLEU&#20998;&#25968;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23436;&#32654;&#27700;&#21360;&#30340;&#19981;&#21487;&#33021;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Easymark, a family of embarrassingly simple yet effective watermarks. Text watermarking is becoming increasingly important with the advent of Large Language Models (LLM). LLMs can generate texts that cannot be distinguished from human-written texts. This is a serious problem for the credibility of the text. Easymark is a simple yet effective solution to this problem. Easymark can inject a watermark without changing the meaning of the text at all while a validator can detect if a text was generated from a system that adopted Easymark or not with high credibility. Easymark is extremely easy to implement so that it only requires a few lines of code. Easymark does not require access to LLMs, so it can be implemented on the user-side when the LLM providers do not offer watermarked LLMs. In spite of its simplicity, it achieves higher detection accuracy and BLEU scores than the state-of-the-art text watermarking methods. We also prove the impossibility theorem of perfect watermarki
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20998;&#21106;&#25628;&#32034;&#21512;&#24182;&#30340;&#31639;&#27861;&#22312;&#25628;&#32034;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#26435;&#37325;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.08917</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Relation-aware Ensemble Learning for Knowledge Graph Embedding. (arXiv:2310.08917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#20998;&#21106;&#25628;&#32034;&#21512;&#24182;&#30340;&#31639;&#27861;&#22312;&#25628;&#32034;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#26435;&#37325;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#25506;&#32034;&#19981;&#21516;&#26041;&#24335;&#30340;&#35821;&#20041;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#26469;&#23398;&#20064;&#19968;&#20010;&#38598;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#25506;&#32034;&#36825;&#20123;&#35821;&#20041;&#20250;&#23548;&#33268;&#27604;&#19968;&#33324;&#38598;&#25104;&#26041;&#27861;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#21106;&#25628;&#32034;&#21512;&#24182;&#30340;&#31639;&#27861;RelEns-DSC&#65292;&#23427;&#29420;&#31435;&#22320;&#25628;&#32034;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#30340;&#26435;&#37325;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#19982;&#19968;&#33324;&#38598;&#25104;&#26041;&#27861;&#30456;&#21516;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#24615;&#33021;&#26356;&#22909;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#39640;&#25928;&#25628;&#32034;&#20851;&#31995;&#24863;&#30693;&#38598;&#25104;&#26435;&#37325;&#21644;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#20844;&#24320;&#22312;https://github.com/LARS-research/RelEns&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) embedding is a fundamental task in natural language processing, and various methods have been proposed to explore semantic patterns in distinctive ways. In this paper, we propose to learn an ensemble by leveraging existing methods in a relation-aware manner. However, exploring these semantics using relation-aware ensemble leads to a much larger search space than general ensemble methods. To address this issue, we propose a divide-search-combine algorithm RelEns-DSC that searches the relation-wise ensemble weights independently. This algorithm has the same computation cost as general ensemble methods but with much better performance. Experimental results on benchmark datasets demonstrate the effectiveness of the proposed method in efficiently searching relation-aware ensemble weights and achieving state-of-the-art embedding performance. The code is public at https://github.com/LARS-research/RelEns.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26631;&#37327;&#21270;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#23398;&#20064;&#20013;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#23454;&#29616;&#36328;&#20219;&#21153;/&#39046;&#22495;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#20998;&#26512;&#22810;&#39046;&#22495;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#21160;&#24577;&#24615;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#26631;&#37327;&#21270;&#26041;&#27861;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2310.08910</link><description>&lt;p&gt;
&#22312;&#35268;&#27169;&#21270;&#30340;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#23398;&#20064;&#20013;&#30340;&#26631;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalarization for Multi-Task and Multi-Domain Learning at Scale. (arXiv:2310.08910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08910
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26631;&#37327;&#21270;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#23398;&#20064;&#20013;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#23454;&#29616;&#36328;&#20219;&#21153;/&#39046;&#22495;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#20998;&#26512;&#22810;&#39046;&#22495;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#21160;&#24577;&#24615;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#26631;&#37327;&#21270;&#26041;&#27861;&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#36755;&#20837;&#39046;&#22495;&#21644;/&#25110;&#36755;&#20986;&#20219;&#21153;&#19978;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#21487;&#20197;&#23558;&#22810;&#20010;&#20449;&#24687;&#28304;&#21387;&#32553;&#20026;&#32479;&#19968;&#30340;&#20027;&#24178;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#12290;&#23427;&#36824;&#21487;&#20197;&#23454;&#29616;&#20219;&#21153;/&#39046;&#22495;&#20043;&#38388;&#30340;&#27491;&#21521;&#30693;&#35782;&#36716;&#31227;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25968;&#25454;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#36825;&#26679;&#30340;&#32593;&#32476;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#19981;&#21516;&#20219;&#21153;&#25110;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#24322;&#65306;&#23613;&#31649;&#22810;&#24180;&#26469;&#25552;&#20986;&#20102;&#20960;&#20010;&#20551;&#35774;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32479;&#19968;&#30340;&#26631;&#37327;&#21270;&#35757;&#32451;&#65292;&#21363;&#31616;&#21333;&#22320;&#26368;&#23567;&#21270;&#20219;&#21153;&#25439;&#22833;&#30340;&#24179;&#22343;&#20540;&#65292;&#19982;&#26356;&#26114;&#36149;&#30340;&#26368;&#26032;&#20248;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#12290;&#36825;&#24341;&#21457;&#20102;&#25105;&#20204;&#23545;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#29702;&#35299;&#31243;&#24230;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#32479;&#19968;&#20998;&#26512;&#22810;&#39046;&#22495;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#26631;&#37327;&#21270;&#22312;&#21508;&#31181;&#20219;&#21153;/&#39046;&#22495;&#32452;&#21512;&#20013;&#30340;&#21160;&#24577;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a single model on multiple input domains and/or output tasks allows for compressing information from multiple sources into a unified backbone hence improves model efficiency. It also enables potential positive knowledge transfer across tasks/domains, leading to improved accuracy and data-efficient training. However, optimizing such networks is a challenge, in particular due to discrepancies between the different tasks or domains: Despite several hypotheses and solutions proposed over the years, recent work has shown that uniform scalarization training, i.e., simply minimizing the average of the task losses, yields on-par performance with more costly SotA optimization methods. This raises the issue of how well we understand the training dynamics of multi-task and multi-domain networks. In this work, we first devise a large-scale unified analysis of multi-domain and multi-task learning to better understand the dynamics of scalarization across varied task/domain combinations and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25112;&#30053;&#22320;&#25913;&#21464;&#32593;&#32476;&#22270;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#38450;&#27490;&#33410;&#28857;&#34987;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#35782;&#21035;&#20986;&#26469;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08909</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#20316;&#20026;&#21453;&#20107;&#23454;&#22270;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Community Membership Hiding as Counterfactual Graph Search via Deep Reinforcement Learning. (arXiv:2310.08909v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#25112;&#30053;&#22320;&#25913;&#21464;&#32593;&#32476;&#22270;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#38450;&#27490;&#33410;&#28857;&#34987;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#35782;&#21035;&#20986;&#26469;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#26816;&#27979;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21457;&#29616;&#24444;&#27492;&#32039;&#23494;&#32852;&#31995;&#30340;&#29992;&#25143;&#32676;&#20307;&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#20182;&#20204;&#20849;&#20139;&#20849;&#21516;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21151;&#33021;&#24448;&#24448;&#20250;&#20197;&#21487;&#33021;&#26292;&#38706;&#20010;&#20154;&#38544;&#31169;&#20026;&#20195;&#20215;&#65292;&#26080;&#24847;&#20013;&#36879;&#38706;&#20182;&#20204;&#30340;&#21697;&#21619;&#25110;&#20559;&#22909;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#29992;&#25143;&#21487;&#33021;&#24076;&#26395;&#20445;&#25252;&#20182;&#20204;&#30340;&#21311;&#21517;&#24615;&#65292;&#24182;&#20986;&#20110;&#21508;&#31181;&#21407;&#22240;&#36873;&#25321;&#36864;&#20986;&#31038;&#21306;&#26816;&#27979;&#65292;&#20363;&#22914;&#19982;&#25919;&#27835;&#25110;&#23447;&#25945;&#32452;&#32455;&#30340;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31038;&#21306;&#25104;&#21592;&#38544;&#34255;&#30340;&#25361;&#25112;&#65292;&#23427;&#28041;&#21450;&#25112;&#30053;&#24615;&#22320;&#25913;&#21464;&#32593;&#32476;&#22270;&#30340;&#32467;&#26500;&#23646;&#24615;&#65292;&#20197;&#38450;&#27490;&#19968;&#20010;&#25110;&#22810;&#20010;&#33410;&#28857;&#34987;&#32473;&#23450;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#35782;&#21035;&#20986;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#21046;&#23450;&#19968;&#20010;&#21463;&#38480;&#30340;&#21453;&#20107;&#23454;&#22270;&#30446;&#26631;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#26469;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#33410;&#28857;&#21644;&#31038;&#21306;&#27450;&#39575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community detection techniques are useful tools for social media platforms to discover tightly connected groups of users who share common interests. However, this functionality often comes at the expense of potentially exposing individuals to privacy breaches by inadvertently revealing their tastes or preferences. Therefore, some users may wish to safeguard their anonymity and opt out of community detection for various reasons, such as affiliation with political or religious organizations.  In this study, we address the challenge of community membership hiding, which involves strategically altering the structural properties of a network graph to prevent one or more nodes from being identified by a given community detection algorithm. We tackle this problem by formulating it as a constrained counterfactual graph objective, and we solve it via deep reinforcement learning. We validate the effectiveness of our method through two distinct tasks: node and community deception. Extensive exper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21367;&#31215;&#26680;&#25163;&#24037;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#24038;&#23460;&#39640;&#34880;&#21387;&#30149;&#21464;&#30340;&#35782;&#21035;&#12290;&#36890;&#36807;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#22788;&#29702;&#20013;&#65292;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#29305;&#24449;&#22270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#25104;&#20687;&#35774;&#22791;&#21644;&#21327;&#35758;&#19979;&#30340;&#19968;&#33268;&#29305;&#24449;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2310.08897</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#21367;&#31215;&#26680;&#25163;&#24037;&#29305;&#24449;&#30340;&#29305;&#33394;&#34701;&#21512;&#65306;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#24038;&#23460;&#39640;&#34880;&#21387;&#30149;&#21464;&#34920;&#22411;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Self supervised convolutional kernel based handcrafted feature harmonization: Enhanced left ventricle hypertension disease phenotyping on echocardiography. (arXiv:2310.08897v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#21367;&#31215;&#26680;&#25163;&#24037;&#29305;&#24449;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#36229;&#22768;&#24515;&#21160;&#22270;&#24038;&#23460;&#39640;&#34880;&#21387;&#30149;&#21464;&#30340;&#35782;&#21035;&#12290;&#36890;&#36807;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#39044;&#22788;&#29702;&#20013;&#65292;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#29305;&#24449;&#22270;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#21516;&#25104;&#20687;&#35774;&#22791;&#21644;&#21327;&#35758;&#19979;&#30340;&#19968;&#33268;&#29305;&#24449;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#29305;&#24449;&#23398;&#26159;&#19968;&#31181;&#36890;&#36807;&#22270;&#20687;&#25552;&#21462;&#23450;&#37327;&#25163;&#24037;&#29305;&#24449;&#26469;&#39044;&#27979;&#30142;&#30149;&#30340;&#21307;&#23398;&#25104;&#20687;&#25216;&#26415;&#12290;&#22312;&#36825;&#20123;&#29305;&#24449;&#20013;&#36827;&#34892;&#34701;&#21512;&#65292;&#21487;&#20197;&#30830;&#20445;&#22312;&#19981;&#21516;&#30340;&#25104;&#20687;&#35774;&#22791;&#21644;&#21327;&#35758;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#34701;&#21512;&#30340;&#26041;&#27861;&#21253;&#25324;&#26631;&#20934;&#21270;&#25104;&#20687;&#21327;&#35758;&#12289;&#32479;&#35745;&#35843;&#25972;&#21644;&#35780;&#20272;&#29305;&#24449;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#36229;&#22768;&#24515;&#21160;&#22270;&#21487;&#20197;&#35786;&#26029;&#24515;&#32908;&#30142;&#30149;&#65292;&#22914;&#24038;&#23460;&#32933;&#21402;(LVH)&#21644;&#39640;&#34880;&#21387;&#24515;&#33039;&#30149;(HHD)&#65292;&#20294;&#19981;&#21516;&#30340;&#25104;&#20687;&#35774;&#32622;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29305;&#24449;&#34701;&#21512;&#25216;&#26415;&#23545;&#20110;&#22312;&#30142;&#30149;&#35786;&#26029;&#20013;&#24212;&#29992;&#25163;&#24037;&#29305;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;(SSl)&#36890;&#36807;&#38480;&#21046;&#30340;&#25968;&#25454;&#38598;&#22686;&#24378;&#25968;&#25454;&#29702;&#35299;&#65292;&#24182;&#36866;&#24212;&#22810;&#26679;&#30340;&#25968;&#25454;&#35774;&#32622;&#12290;ConvNeXt-V2&#23558;&#21367;&#31215;&#23618;&#38598;&#25104;&#21040;SSL&#20013;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;SSL&#20013;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#65292;&#23558;&#23427;&#20204;&#29992;&#20316;&#39044;&#22788;&#29702;&#65292;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#29305;&#24449;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiomics, a medical imaging technique, extracts quantitative handcrafted features from images to predict diseases. Harmonization in those features ensures consistent feature extraction across various imaging devices and protocols. Methods for harmonization include standardized imaging protocols, statistical adjustments, and evaluating feature robustness. Myocardial diseases such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD) are diagnosed via echocardiography, but variable imaging settings pose challenges. Harmonization techniques are crucial for applying handcrafted features in disease diagnosis in such scenario. Self-supervised learning (SSL) enhances data understanding within limited datasets and adapts to diverse data settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying superior performance in various tasks. This study focuses on convolutional filters within SSL, using them as preprocessing to convert images into feature maps for h
&lt;/p&gt;</description></item><item><title>EHI&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#23618;&#27425;&#32034;&#24341;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#23494;&#38598;&#26816;&#32034;&#12290;&#23427;&#21516;&#26102;&#23398;&#20064;&#23884;&#20837;&#21644;ANNS&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#36335;&#24452;&#23884;&#20837;&#26469;&#25429;&#33719;&#32034;&#24341;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#20248;&#21270;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08891</link><description>&lt;p&gt;
EHI: &#39640;&#25928;&#23494;&#38598;&#26816;&#32034;&#30340;&#23618;&#27425;&#32034;&#24341;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EHI: End-to-end Learning of Hierarchical Index for Efficient Dense Retrieval. (arXiv:2310.08891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08891
&lt;/p&gt;
&lt;p&gt;
EHI&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#23618;&#27425;&#32034;&#24341;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#23494;&#38598;&#26816;&#32034;&#12290;&#23427;&#21516;&#26102;&#23398;&#20064;&#23884;&#20837;&#21644;ANNS&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#36335;&#24452;&#23884;&#20837;&#26469;&#25429;&#33719;&#32034;&#24341;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#20248;&#21270;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#23884;&#20837;&#24335;&#26816;&#32034;&#29616;&#24050;&#25104;&#20026;&#35821;&#20041;&#25628;&#32034;&#21644;&#25490;&#21517;&#38382;&#39064;&#30340;&#34892;&#19994;&#26631;&#20934;&#65292;&#22914;&#33719;&#21462;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#32593;&#32476;&#25991;&#26723;&#12290;&#36825;&#20123;&#25216;&#26415;&#20351;&#29992;&#20102;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#65306;(a)&#23545;&#27604;&#23398;&#20064;&#26469;&#35757;&#32451;&#21452;&#32534;&#30721;&#22120;&#20197;&#23884;&#20837;&#26597;&#35810;&#21644;&#25991;&#26723;&#65292;&#20197;&#21450;(b)&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;(ANNS)&#20197;&#26597;&#25214;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20284;&#25991;&#26723;&#12290;&#36825;&#20004;&#20010;&#38454;&#27573;&#26159;&#19981;&#30456;&#20132;&#30340;&#65307;&#23398;&#24471;&#30340;&#23884;&#20837;&#21487;&#33021;&#19981;&#36866;&#21512;ANNS&#26041;&#27861;&#65292;&#21453;&#20043;&#20134;&#28982;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31471;&#21040;&#31471;&#23618;&#27425;&#32034;&#24341;(EHI)&#30340;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#23398;&#20064;&#23884;&#20837;&#21644;ANNS&#32467;&#26500;&#20197;&#20248;&#21270;&#26816;&#32034;&#24615;&#33021;&#12290;EHI&#20351;&#29992;&#26631;&#20934;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#23884;&#20837;&#26597;&#35810;&#21644;&#25991;&#26723;&#65292;&#21516;&#26102;&#23398;&#20064;&#19968;&#20010;&#20498;&#25490;&#25991;&#20214;&#32034;&#24341;(IVF)&#39118;&#26684;&#30340;&#26641;&#29366;&#32467;&#26500;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;ANNS&#12290;&#20026;&#20102;&#30830;&#20445;&#31163;&#25955;&#22522;&#20110;&#26641;&#30340;ANNS&#32467;&#26500;&#30340;&#31283;&#23450;&#21644;&#39640;&#25928;&#23398;&#20064;&#65292;EHI&#24341;&#20837;&#20102;&#23494;&#38598;&#36335;&#24452;&#23884;&#20837;&#30340;&#27010;&#24565;&#65292;&#29992;&#26469;&#25429;&#33719;&#32034;&#24341;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense embedding-based retrieval is now the industry standard for semantic search and ranking problems, like obtaining relevant web documents for a given query. Such techniques use a two-stage process: (a) contrastive learning to train a dual encoder to embed both the query and documents and (b) approximate nearest neighbor search (ANNS) for finding similar documents for a given query. These two stages are disjoint; the learned embeddings might be ill-suited for the ANNS method and vice-versa, leading to suboptimal performance. In this work, we propose End-to-end Hierarchical Indexing -- EHI -- that jointly learns both the embeddings and the ANNS structure to optimize retrieval performance. EHI uses a standard dual encoder model for embedding queries and documents while learning an inverted file index (IVF) style tree structure for efficient ANNS. To ensure stable and efficient learning of discrete tree-based ANNS structure, EHI introduces the notion of dense path embedding that capture
&lt;/p&gt;</description></item><item><title>METRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#65292;&#26088;&#22312;&#20351;&#20854;&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#12290;&#36825;&#20010;&#30446;&#26631;&#35299;&#20915;&#20102;&#32431;&#25506;&#32034;&#26041;&#27861;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20013;&#30340;&#22256;&#38590;&#20197;&#21450;&#20114;&#20449;&#24687;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#20013;&#32570;&#20047;&#28608;&#21169;&#32780;&#26080;&#27861;&#25506;&#32034;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08887</link><description>&lt;p&gt;
METRA:&#20855;&#26377;&#24230;&#37327;&#24863;&#30693;&#25277;&#35937;&#30340;&#21487;&#25193;&#23637;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
METRA: Scalable Unsupervised RL with Metric-Aware Abstraction. (arXiv:2310.08887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08887
&lt;/p&gt;
&lt;p&gt;
METRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#65292;&#26088;&#22312;&#20351;&#20854;&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#12290;&#36825;&#20010;&#30446;&#26631;&#35299;&#20915;&#20102;&#32431;&#25506;&#32034;&#26041;&#27861;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#29615;&#22659;&#20013;&#30340;&#22256;&#38590;&#20197;&#21450;&#20114;&#20449;&#24687;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#20013;&#32570;&#20047;&#28608;&#21169;&#32780;&#26080;&#27861;&#25506;&#32034;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#12290;&#21516;&#26679;&#65292;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26377;&#26395;&#21457;&#29616;&#21508;&#31181;&#28508;&#22312;&#26377;&#29992;&#30340;&#34892;&#20026;&#65292;&#21487;&#20197;&#21152;&#36895;&#23398;&#20064;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20043;&#21069;&#30340;&#23581;&#35797;&#65292;&#20351;&#26080;&#30417;&#30563;RL&#30495;&#27491;&#21487;&#25193;&#23637;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#25361;&#25112;&#65306;&#22312;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#65292;&#32431;&#25506;&#32034;&#26041;&#27861;&#21487;&#33021;&#20250;&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#35206;&#30422;&#27599;&#20010;&#21487;&#33021;&#30340;&#36716;&#25442;&#26159;&#19981;&#21487;&#34892;&#30340;&#65307;&#32780;&#20114;&#20449;&#24687;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#28608;&#21169;&#32780;&#23436;&#20840;&#26080;&#27861;&#25506;&#32034;&#29615;&#22659;&#12290;&#20026;&#20102;&#20351;&#26080;&#30417;&#30563;RL&#22312;&#22797;&#26434;&#30340;&#39640;&#32500;&#29615;&#22659;&#20013;&#21487;&#25193;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;RL&#30446;&#26631;&#65292;&#31216;&#20026;&#24230;&#37327;&#24863;&#30693;&#25277;&#35937;&#65288;METRA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision. Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array of downstream tasks. Previous unsupervised RL approaches have mainly focused on pure exploration and mutual information skill learning. However, despite the previous attempts, making unsupervised RL truly scalable still remains a major open challenge: pure exploration approaches might struggle in complex environments with large state spaces, where covering every possible transition is infeasible, and mutual information skill learning approaches might completely fail to explore the environment due to the lack of incentives. To make unsupervised RL scalable to complex, high-dimensional environments, we propose a novel unsupervised RL objective, which we call Metric-Aware Abstraction (METRA). Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;60 GHz FMCW&#38647;&#36798;&#30340;&#36731;&#37327;&#32423;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#20116;&#20010;&#29305;&#24449;&#21644;&#31934;&#31616;&#30340;&#22788;&#29702;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#39640;&#25928;&#35782;&#21035;&#25163;&#21183;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#20302;&#30340;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#21151;&#32791;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.08876</link><description>&lt;p&gt;
&#36793;&#32536;FMCW&#38647;&#36798;&#30340;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Gesture Recognition for FMCW Radar on the Edge. (arXiv:2310.08876v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;60 GHz FMCW&#38647;&#36798;&#30340;&#36731;&#37327;&#32423;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#20116;&#20010;&#29305;&#24449;&#21644;&#31934;&#31616;&#30340;&#22788;&#29702;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#39640;&#25928;&#35782;&#21035;&#25163;&#21183;&#65292;&#21516;&#26102;&#20855;&#26377;&#36739;&#20302;&#30340;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#21151;&#32791;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;60 GHz&#35843;&#39057;&#36830;&#32493;&#27874;(FMCW)&#38647;&#36798;&#30340;&#36731;&#37327;&#32423;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25163;&#21183;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#20116;&#20010;&#29305;&#24449;&#26377;&#25928;&#22320;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#31616;&#30340;&#38647;&#36798;&#22788;&#29702;&#31639;&#27861;&#26469;&#25552;&#21462;&#36825;&#20123;&#29305;&#24449;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#32321;&#37325;&#30340;&#20108;&#32500;&#22788;&#29702;&#65292;&#21363;&#36317;&#31163;-&#22810;&#26222;&#21202;&#25104;&#20687;&#65292;&#24182;&#25913;&#20026;&#36827;&#34892;&#26089;&#26399;&#30446;&#26631;&#26816;&#27979;-&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#31995;&#32479;&#31227;&#26893;&#21040;&#20855;&#26377;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#21151;&#32791;&#20005;&#26684;&#38480;&#21046;&#30340;&#23436;&#20840;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#12290;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#30340;&#26550;&#26500;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#20849;&#21516;&#26816;&#27979;&#21644;&#20998;&#31867;&#20116;&#31181;&#19981;&#21516;&#30340;&#25163;&#21183;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#25105;&#20204;&#30340;&#20445;&#30041;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20197;98.4%&#30340;F1&#20998;&#25968;&#35782;&#21035;&#25163;&#21183;&#65292;&#23427;&#22312;&#19968;&#20010;Arm Cortex-M4&#24494;&#25511;&#21046;&#22120;&#19978;&#36816;&#34892;&#65292;&#38656;&#35201;&#19981;&#21040;280 kB&#30340;&#38378;&#23384;&#23384;&#20648;&#22120;&#65292;120 kB&#30340;RAM&#65292;&#24182;&#28040;&#32791;75 mW&#30340;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a lightweight gesture recognition system based on 60 GHz frequency modulated continuous wave (FMCW) radar. We show that gestures can be characterized efficiently by a set of five features, and propose a slim radar processing algorithm to extract these features. In contrast to previous approaches, we avoid heavy 2D processing, i.e. range-Doppler imaging, and perform instead an early target detection - this allows us to port the system to fully embedded platforms with tight constraints on memory, compute and power consumption. A recurrent neural network (RNN) based architecture exploits these features to jointly detect and classify five different gestures. The proposed system recognizes gestures with an F1 score of 98.4% on our hold-out test dataset, it runs on an Arm Cortex-M4 microcontroller requiring less than 280 kB of flash memory, 120 kB of RAM, and consuming 75 mW of power.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#22788;&#29702;&#30913;&#30424;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#23618;&#12289;&#31639;&#27861;&#23618;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#31867;&#30340;&#25361;&#25112;&#21450;&#24212;&#23545;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.08867</link><description>&lt;p&gt;
&#22788;&#29702;&#30913;&#30424;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Methods for Handling Disk Data Imbalance. (arXiv:2310.08867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#22788;&#29702;&#30913;&#30424;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#23618;&#12289;&#31639;&#27861;&#23618;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#31867;&#30340;&#25361;&#25112;&#21450;&#24212;&#23545;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#38382;&#39064;&#20013;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#65292;&#25968;&#25454;&#35774;&#35745;&#20026;&#20934;&#30830;&#24615;&#65292;&#32780;&#25968;&#25454;&#31867;&#21035;&#19981;&#24179;&#34913;&#21487;&#33021;&#23548;&#33268;&#37096;&#20998;&#31867;&#21035;&#30340;&#38169;&#35823;&#20998;&#31867;&#25104;&#26412;&#36739;&#39640;&#12290;Backblaze&#25968;&#25454;&#38598;&#26159;&#19982;&#30828;&#30424;&#30456;&#20851;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#30340;&#22833;&#36133;&#25968;&#25454;&#37327;&#36739;&#23569;&#65292;&#20581;&#24247;&#25968;&#25454;&#37327;&#36739;&#22823;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#20998;&#31867;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#35752;&#35770;&#20998;&#20026;&#25968;&#25454;&#23618;&#26041;&#27861;&#12289;&#31639;&#27861;&#23618;&#26041;&#27861;&#21644;&#28151;&#21512;&#26041;&#27861;&#19977;&#20010;&#20027;&#35201;&#26041;&#38754;&#12290;&#23545;&#20110;&#27599;&#31181;&#26041;&#27861;&#31867;&#22411;&#65292;&#25105;&#20204;&#24635;&#32467;&#21644;&#20998;&#26512;&#20102;&#29616;&#26377;&#38382;&#39064;&#12289;&#31639;&#27861;&#24605;&#24819;&#12289;&#20248;&#21183;&#21644;&#32570;&#28857;&#12290;&#27492;&#22806;&#65292;&#35752;&#35770;&#20102;&#19981;&#24179;&#34913;&#25968;&#25454;&#20998;&#31867;&#30340;&#25361;&#25112;&#20197;&#21450;&#24212;&#23545;&#31574;&#30053;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#26041;&#20415;&#22320;&#36873;&#25321;&#36866;&#24403;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance exists in many classification problems, and since the data is designed for accuracy, imbalance in data classes can lead to classification challenges with a few classes having higher misclassification costs. The Backblaze dataset, a widely used dataset related to hard discs, has a small amount of failure data and a large amount of health data, which exhibits a serious class imbalance. This paper provides a comprehensive overview of research in the field of imbalanced data classification. The discussion is organized into three main aspects: data-level methods, algorithmic-level methods, and hybrid methods. For each type of method, we summarize and analyze the existing problems, algorithmic ideas, strengths, and weaknesses. Additionally, the challenges of unbalanced data classification are discussed, along with strategies to address them. It is convenient for researchers to choose the appropriate method according to their needs.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#26469;&#35780;&#20272;&#21464;&#21387;&#22120;&#22312;&#22788;&#29702;&#19981;&#21516;&#38590;&#24230;&#31034;&#20363;&#30340;&#38382;&#39064;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#33258;&#36866;&#24212;&#21644;&#27169;&#22359;&#21270;&#35745;&#31639;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#27867;&#21270;&#21040;&#26356;&#39640;&#25968;&#37327;&#30340;&#35745;&#31639;&#26102;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20844;&#24179;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2310.08866</link><description>&lt;p&gt;
&#24378;&#36866;&#24212;&#24615;&#21644;&#27169;&#22359;&#21270;&#29992;&#20110;&#26377;&#25928;&#22320;&#22312;&#20219;&#21153;&#22797;&#26434;&#24615;&#19978;&#36827;&#34892;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptivity and Modularity for Efficient Generalization Over Task Complexity. (arXiv:2310.08866v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08866
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#26469;&#35780;&#20272;&#21464;&#21387;&#22120;&#22312;&#22788;&#29702;&#19981;&#21516;&#38590;&#24230;&#31034;&#20363;&#30340;&#38382;&#39064;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#33258;&#36866;&#24212;&#21644;&#27169;&#22359;&#21270;&#35745;&#31639;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22312;&#27867;&#21270;&#21040;&#26356;&#39640;&#25968;&#37327;&#30340;&#35745;&#31639;&#26102;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20844;&#24179;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26159;&#21542;&#33021;&#22815;&#22312;&#38656;&#35201;&#22788;&#29702;&#19981;&#21516;&#38590;&#24230;&#31034;&#20363;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#27867;&#21270;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#23545;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#27867;&#21270;&#65292;&#24182;&#25552;&#20986;&#30340;&#32467;&#26524;&#34920;&#26126;&#26631;&#20934;&#30340;&#21464;&#21387;&#22120;&#22312;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20123;&#20219;&#21153;&#26159;&#30001;Zhang&#31561;&#20154;&#22312;2021&#24180;&#25552;&#20986;&#30340;&#25351;&#38024;&#20540;&#26816;&#32034;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#20013;&#33258;&#36866;&#24212;&#21644;&#27169;&#22359;&#21270;&#35745;&#31639;&#26426;&#21046;&#30340;&#20351;&#29992;&#22914;&#20309;&#20419;&#36827;&#23398;&#20064;&#38656;&#35201;&#22312;&#39034;&#24207;&#35745;&#31639;&#27493;&#39588;&#25968;&#37327;&#65288;&#21363;&#35745;&#31639;&#22270;&#30340;&#28145;&#24230;&#65289;&#19978;&#36827;&#34892;&#27867;&#21270;&#30340;&#20219;&#21153;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;&#31216;&#20026;Hyper-UT&#65292;&#23427;&#32467;&#21512;&#20102;&#26469;&#33258;&#36229;&#32593;&#32476;&#30340;&#21160;&#24577;&#20989;&#25968;&#29983;&#25104;&#21644;&#26469;&#33258;&#36890;&#29992;&#21464;&#21387;&#22120;&#30340;&#33258;&#36866;&#24212;&#28145;&#24230;&#12290;&#35813;&#27169;&#22411;&#22312;&#27867;&#21270;&#21040;&#26356;&#39640;&#25968;&#37327;&#30340;&#35745;&#31639;&#26102;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20844;&#24179;&#30340;&#35745;&#31639;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can transformers generalize efficiently on problems that require dealing with examples with different levels of difficulty? We introduce a new task tailored to assess generalization over different complexities and present results that indicate that standard transformers face challenges in solving these tasks. These tasks are variations of pointer value retrieval previously introduced by Zhang et al. (2021). We investigate how the use of a mechanism for adaptive and modular computation in transformers facilitates the learning of tasks that demand generalization over the number of sequential computation steps (i.e., the depth of the computation graph). Based on our observations, we propose a transformer-based architecture called Hyper-UT, which combines dynamic function generation from hyper networks with adaptive depth from Universal Transformers. This model demonstrates higher accuracy and a fairer allocation of computational resources when generalizing to higher numbers of computation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27010;&#24565;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#65288;&#20998;&#23376;&#65292;&#24615;&#36136;&#27979;&#37327;&#65289;&#23545;&#30340;&#19978;&#19979;&#25991;&#20013;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65292;&#24182;&#22312;&#26032;&#30340;&#24615;&#36136;&#19978;&#24555;&#36895;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#23567;&#25903;&#25345;&#38598;&#22823;&#23567;&#19978;&#36229;&#36807;&#20102;&#26368;&#36817;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22823;&#25903;&#25345;&#38598;&#22823;&#23567;&#19978;&#19982;&#26368;&#20339;&#26041;&#27861;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2310.08863</link><description>&lt;p&gt;
&#29992;&#20110;&#23569;&#26679;&#26412;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning for Few-Shot Molecular Property Prediction. (arXiv:2310.08863v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27010;&#24565;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#65292;&#36890;&#36807;&#23398;&#20064;&#20174;&#65288;&#20998;&#23376;&#65292;&#24615;&#36136;&#27979;&#37327;&#65289;&#23545;&#30340;&#19978;&#19979;&#25991;&#20013;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65292;&#24182;&#22312;&#26032;&#30340;&#24615;&#36136;&#19978;&#24555;&#36895;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#23567;&#25903;&#25345;&#38598;&#22823;&#23567;&#19978;&#36229;&#36807;&#20102;&#26368;&#36817;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22823;&#25903;&#25345;&#38598;&#22823;&#23567;&#19978;&#19982;&#26368;&#20339;&#26041;&#27861;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#22240;&#20854;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20165;&#36866;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#65292;&#19981;&#36866;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27010;&#24565;&#36816;&#29992;&#21040;&#23569;&#26679;&#26412;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#65288;&#20998;&#23376;&#65292;&#24615;&#36136;&#27979;&#37327;&#65289;&#23545;&#30340;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#65292;&#24182;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#24615;&#36136;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#22312;FS-Mol&#21644;BACE&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#23567;&#25903;&#25345;&#38598;&#22823;&#23567;&#19978;&#36229;&#36807;&#20102;&#26368;&#36817;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22823;&#25903;&#25345;&#38598;&#22823;&#23567;&#19978;&#19982;&#26368;&#20339;&#26041;&#27861;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning has become an important approach for few-shot learning in Large Language Models because of its ability to rapidly adapt to new tasks without fine-tuning model parameters. However, it is restricted to applications in natural language and inapplicable to other domains. In this paper, we adapt the concepts underpinning in-context learning to develop a new algorithm for few-shot molecular property prediction. Our approach learns to predict molecular properties from a context of (molecule, property measurement) pairs and rapidly adapts to new properties without fine-tuning. On the FS-Mol and BACE molecular property prediction benchmarks, we find this method surpasses the performance of recent meta-learning algorithms at small support sizes and is competitive with the best methods at large support sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;Adam-family&#26041;&#27861;&#22312;&#35757;&#32451;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#31163;&#26435;&#37325;&#34928;&#20943;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#35768;&#22810;&#24050;&#30693;&#30340;Adam-family&#26041;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.08858</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#31163;&#26435;&#37325;&#34928;&#20943;&#30340;Adam-family&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Adam-family Methods with Decoupled Weight Decay in Deep Learning. (arXiv:2310.08858v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;Adam-family&#26041;&#27861;&#22312;&#35757;&#32451;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#31163;&#26435;&#37325;&#34928;&#20943;&#30340;&#26032;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#35768;&#22810;&#24050;&#30693;&#30340;Adam-family&#26041;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;Adam-family&#26041;&#27861;&#22312;&#26368;&#23567;&#21270;&#20108;&#27425;&#27491;&#21017;&#21270;&#38750;&#20809;&#28369;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#20855;&#26377;&#26435;&#37325;&#34928;&#20943;&#30340;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#12290;&#21463;&#21040;AdamW&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#31163;&#26435;&#37325;&#34928;&#20943;&#30340;Adam-family&#26041;&#27861;&#30340;&#26032;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#65292;&#38543;&#26426;&#23376;&#26799;&#24230;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#20998;&#21035;&#29420;&#31435;&#20110;&#26435;&#37325;&#34928;&#20943;&#39033;&#36827;&#34892;&#26356;&#26032;&#12290;&#22312;&#21512;&#29702;&#30340;&#20551;&#35774;&#19979;&#65292;&#24182;&#19988;&#22312;&#26356;&#26032;&#20027;&#35201;&#20248;&#21270;&#21464;&#37327;&#26102;&#37319;&#29992;&#38750;&#36882;&#20943;&#27493;&#38271;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#21547;&#20102;&#35768;&#22810;&#20247;&#25152;&#21608;&#30693;&#30340;Adam-family&#26041;&#27861;&#65292;&#20174;&#32780;&#20026;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#38750;&#20809;&#28369;&#31070;&#32463;&#32593;&#32476;&#26102;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#28176;&#36817;&#36817;&#20284;&#20102;&#19968;&#31867;&#27425;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the convergence properties of a wide class of Adam-family methods for minimizing quadratically regularized nonsmooth nonconvex optimization problems, especially in the context of training nonsmooth neural networks with weight decay. Motivated by the AdamW method, we propose a novel framework for Adam-family methods with decoupled weight decay. Within our framework, the estimators for the first-order and second-order moments of stochastic subgradients are updated independently of the weight decay term. Under mild assumptions and with non-diminishing stepsizes for updating the primary optimization variables, we establish the convergence properties of our proposed framework. In addition, we show that our proposed framework encompasses a wide variety of well-known Adam-family methods, hence offering convergence guarantees for these methods in the training of nonsmooth neural networks. More importantly, we show that our proposed framework asymptotically approxi
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24402;&#19968;&#21270;&#32479;&#35745;&#23384;&#22312;&#36817;&#26399;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;BN&#30340;&#24179;&#34913;&#31574;&#30053;AdaB$^2$N&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08855</link><description>&lt;p&gt;
&#20811;&#26381;&#36830;&#32493;&#23398;&#20064;&#20013;&#24402;&#19968;&#21270;&#32479;&#35745;&#30340;&#36817;&#26399;&#20559;&#24046;&#65306;&#24179;&#34913;&#21644;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation. (arXiv:2310.08855v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08855
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#24402;&#19968;&#21270;&#32479;&#35745;&#23384;&#22312;&#36817;&#26399;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;BN&#30340;&#24179;&#34913;&#31574;&#30053;AdaB$^2$N&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#28041;&#21450;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#24182;&#36866;&#24403;&#24179;&#34913;&#23427;&#20204;&#30340;&#30693;&#35782;&#12290;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#23545;&#26087;&#20219;&#21153;&#30340;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#22312;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#20013;&#20811;&#26381;&#26087;&#20219;&#21153;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#24402;&#19968;&#21270;&#23618;&#25552;&#20379;&#20102;&#19968;&#31181;&#20363;&#22806;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#26799;&#24230;&#21644;&#24403;&#21069;&#35266;&#23519;&#21040;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#32479;&#35745;&#20449;&#24687;&#36827;&#34892;&#30456;&#20114;&#20381;&#36182;&#30340;&#26356;&#26032;&#65292;&#36825;&#38656;&#35201;&#19987;&#38376;&#30340;&#31574;&#30053;&#26469;&#20943;&#36731;&#36817;&#26399;&#20559;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#26368;&#27969;&#34892;&#30340;&#25209;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#24182;&#23545;&#20854;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#27425;&#20248;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#23637;&#31034;&#20102;BN&#32479;&#35745;&#30340;&#24179;&#34913;&#21644;&#36866;&#24212;&#20043;&#38388;&#30340;&#22256;&#22659;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#12290;&#38024;&#23545;&#36825;&#20123;&#29305;&#23450;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;BN&#30340;&#24179;&#34913;&#31574;&#30053;&#65288;AdaB$^2$N&#65289;&#65292;&#23427;&#36866;&#24403;&#22320;&#23558;&#36125;&#21494;&#26031;&#31574;&#30053;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#22686;&#37327;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning entails learning a sequence of tasks and balancing their knowledge appropriately. With limited access to old training samples, much of the current work in deep neural networks has focused on overcoming catastrophic forgetting of old tasks in gradient-based optimization. However, the normalization layers provide an exception, as they are updated interdependently by the gradient and statistics of currently observed training samples, which require specialized strategies to mitigate recency bias. In this work, we focus on the most popular Batch Normalization (BN) and provide an in-depth theoretical analysis of its sub-optimality in continual learning. Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization. Targeting on these particular challenges, we propose Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to 
&lt;/p&gt;</description></item><item><title>Rank-DETR&#26159;&#19968;&#31181;&#39640;&#36136;&#37327;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25490;&#21517;&#23548;&#21521;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;DETR&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.08854</link><description>&lt;p&gt;
&#39640;&#36136;&#37327;&#30446;&#26631;&#26816;&#27979;&#30340;Rank-DETR&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rank-DETR for High Quality Object Detection. (arXiv:2310.08854v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08854
&lt;/p&gt;
&lt;p&gt;
Rank-DETR&#26159;&#19968;&#31181;&#39640;&#36136;&#37327;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25490;&#21517;&#23548;&#21521;&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;DETR&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26816;&#27979;&#21464;&#25442;&#22120;&#65288;DETR&#65289;&#20351;&#29992;&#19968;&#32452;&#23545;&#35937;&#26597;&#35810;&#26469;&#39044;&#27979;&#36793;&#30028;&#26694;&#21015;&#34920;&#65292;&#36890;&#36807;&#23558;&#20854;&#20998;&#31867;&#32622;&#20449;&#24230;&#24471;&#20998;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#36873;&#25321;&#25490;&#21517;&#38752;&#21069;&#30340;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#32473;&#23450;&#36755;&#20837;&#22270;&#20687;&#30340;&#26368;&#32456;&#26816;&#27979;&#32467;&#26524;&#12290;&#24615;&#33021;&#21331;&#36234;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#38656;&#35201;&#23545;&#36793;&#30028;&#26694;&#39044;&#27979;&#36827;&#34892;&#20934;&#30830;&#30340;&#25490;&#24207;&#12290;&#23545;&#20110;&#22522;&#20110;DETR&#30340;&#26816;&#27979;&#22120;&#65292;&#25490;&#21517;&#38752;&#21069;&#30340;&#36793;&#30028;&#26694;&#30001;&#20110;&#20998;&#31867;&#24471;&#20998;&#19982;&#23450;&#20301;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#19981;&#23545;&#40784;&#32780;&#23548;&#33268;&#23450;&#20301;&#36136;&#37327;&#36739;&#24046;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#39640;&#36136;&#37327;&#26816;&#27979;&#22120;&#30340;&#26500;&#24314;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31995;&#21015;&#38754;&#21521;&#25490;&#21517;&#30340;&#35774;&#35745;&#65292;&#20849;&#21516;&#31216;&#20026;Rank-DETR&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;DETR&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;i&#65289;&#19968;&#20010;&#38754;&#21521;&#25490;&#21517;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#21487;&#20197;&#20419;&#36827;&#27491;&#38754;&#39044;&#27979;&#24182;&#25233;&#21046;&#36127;&#38754;&#39044;&#27979;&#65292;&#20197;&#30830;&#20445;&#26356;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#19968;&#20010;&#38754;&#21521;&#25490;&#21517;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#21305;&#37197;&#25104;&#26412;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern detection transformers (DETRs) use a set of object queries to predict a list of bounding boxes, sort them by their classification confidence scores, and select the top-ranked predictions as the final detection results for the given input image. A highly performant object detector requires accurate ranking for the bounding box predictions. For DETR-based detectors, the top-ranked bounding boxes suffer from less accurate localization quality due to the misalignment between classification scores and localization accuracy, thus impeding the construction of high-quality detectors. In this work, we introduce a simple and highly performant DETR-based object detector by proposing a series of rank-oriented designs, combinedly called Rank-DETR. Our key contributions include: (i) a rank-oriented architecture design that can prompt positive predictions and suppress the negative ones to ensure lower false positive rates, as well as (ii) a rank-oriented loss function and matching cost design 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SLOTS&#30340;&#21322;&#30417;&#30563;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#25509;&#25910;&#21322;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#22312;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#24494;&#35843;&#20013;&#32508;&#21512;&#21033;&#29992;&#23545;&#27604;&#25439;&#22833;&#21644;&#20998;&#31867;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.08848</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#31471;&#21040;&#31471;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised End-To-End Contrastive Learning For Time Series Classification. (arXiv:2310.08848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SLOTS&#30340;&#21322;&#30417;&#30563;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#25509;&#25910;&#21322;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#22312;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#24494;&#35843;&#20013;&#32508;&#21512;&#21033;&#29992;&#23545;&#27604;&#25439;&#22833;&#21644;&#20998;&#31867;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26159;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#20998;&#26512;&#31561;&#21508;&#31181;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22312;&#20351;&#29992;&#26377;&#38480;&#26631;&#31614;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#25928;&#34920;&#31034;&#26041;&#38754;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#29616;&#26377;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20013;&#26222;&#36941;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29420;&#31435;&#30340;&#38454;&#27573;&#65306;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#28982;&#21518;&#22312;&#23567;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#23545;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#20363;&#22914;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#23545;&#27604;&#25439;&#22833;&#19981;&#33021;&#30452;&#25509;&#24433;&#21709;&#19979;&#28216;&#24494;&#35843;&#20998;&#31867;&#22120;&#65292;&#20197;&#21450;&#32570;&#20047;&#21033;&#29992;&#30001;&#26377;&#20215;&#20540;&#30340;&#30495;&#23454;&#26631;&#31614;&#24341;&#23548;&#30340;&#20998;&#31867;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SLOTS&#65288;Semi-supervised Learning fOr Time clasSification&#65289;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;SLOTS&#25509;&#25910;&#21322;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#22823;&#37327;&#26080;&#26631;&#31614;&#26679;&#26412;&#21644;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series classification is a critical task in various domains, such as finance, healthcare, and sensor data analysis. Unsupervised contrastive learning has garnered significant interest in learning effective representations from time series data with limited labels. The prevalent approach in existing contrastive learning methods consists of two separate stages: pre-training the encoder on unlabeled datasets and fine-tuning the well-trained model on a small-scale labeled dataset. However, such two-stage approaches suffer from several shortcomings, such as the inability of unsupervised pre-training contrastive loss to directly affect downstream fine-tuning classifiers, and the lack of exploiting the classification loss which is guided by valuable ground truth. In this paper, we propose an end-to-end model called SLOTS (Semi-supervised Learning fOr Time clasSification). SLOTS receives semi-labeled datasets, comprising a large number of unlabeled samples and a small proportion of labele
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#35760;&#24518;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#20250;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#32508;&#21512;&#24615;&#22320;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.08847</link><description>&lt;p&gt;
&#20851;&#20110;&#33258;&#28982;&#12289;&#40065;&#26834;&#21644;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#20013;&#30340;&#36807;&#24230;&#35760;&#24518;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Over-Memorization During Natural, Robust and Catastrophic Overfitting. (arXiv:2310.08847v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#35760;&#24518;&#38382;&#39064;&#65292;&#21457;&#29616;&#20854;&#20250;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#32508;&#21512;&#24615;&#22320;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#25311;&#21512;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#26080;&#35770;&#26159;&#22312;&#33258;&#28982;&#35757;&#32451;&#36824;&#26159;&#23545;&#25239;&#24615;&#35757;&#32451;&#20013;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#38590;&#20197;&#19968;&#33268;&#22320;&#35299;&#20915;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#65292;&#36890;&#24120;&#35774;&#35745;&#20102;&#38024;&#23545;&#33258;&#28982;&#27169;&#24335;&#25110;&#23545;&#25239;&#27169;&#24335;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#20165;&#20851;&#27880;&#33258;&#28982;&#27169;&#24335;&#65292;&#21435;&#25506;&#32034;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DNN&#20013;&#30340;&#35760;&#24518;&#25928;&#24212;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#31181;&#31216;&#20026;&#36807;&#24230;&#35760;&#24518;&#30340;&#20849;&#21516;&#34892;&#20026;&#65292;&#36825;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#34892;&#20026;&#34920;&#29616;&#20026;DNN&#31361;&#28982;&#23545;&#26576;&#20123;&#35757;&#32451;&#27169;&#24335;&#20135;&#29983;&#39640;&#32622;&#20449;&#24230;&#30340;&#39044;&#27979;&#65292;&#24182;&#23545;&#20854;&#20445;&#25345;&#25345;&#20037;&#35760;&#24518;&#12290;&#27492;&#22806;&#65292;&#24403;DNN&#36807;&#24230;&#35760;&#24518;&#19968;&#31181;&#23545;&#25239;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#21516;&#26102;&#23637;&#29616;&#20986;&#23545;&#24212;&#33258;&#28982;&#27169;&#24335;&#30340;&#39640;&#32622;&#20449;&#24230;&#39044;&#27979;&#12290;&#36825;&#20123;&#21457;&#29616;&#28608;&#21169;&#25105;&#20204;&#32508;&#21512;&#24615;&#22320;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#36807;&#25311;&#21512;&#65292;&#38459;&#30861;&#36807;&#24230;&#35760;&#24518;&#34892;&#20026;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overfitting negatively impacts the generalization ability of deep neural networks (DNNs) in both natural and adversarial training. Existing methods struggle to consistently address different types of overfitting, typically designing strategies that focus separately on either natural or adversarial patterns. In this work, we adopt a unified perspective by solely focusing on natural patterns to explore different types of overfitting. Specifically, we examine the memorization effect in DNNs and reveal a shared behaviour termed over-memorization, which impairs their generalization capacity. This behaviour manifests as DNNs suddenly becoming high-confidence in predicting certain training patterns and retaining a persistent memory for them. Furthermore, when DNNs over-memorize an adversarial pattern, they tend to simultaneously exhibit high-confidence prediction for the corresponding natural pattern. These findings motivate us to holistically mitigate different types of overfitting by hinder
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35266;&#23519;&#26144;&#23556;&#21644;&#34892;&#20026;&#20811;&#38534;&#36827;&#34892;&#23569;&#26679;&#26412;&#31574;&#30053;&#36716;&#31227;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#26144;&#23556;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#35266;&#23519;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#26144;&#23556;&#26469;&#20811;&#38534;&#28304;&#20219;&#21153;&#30340;&#25104;&#21151;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2310.08836</link><description>&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#26144;&#23556;&#21644;&#34892;&#20026;&#20811;&#38534;&#36827;&#34892;&#23569;&#26679;&#26412;&#31574;&#30053;&#36716;&#31227;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Few-Shot Policy Transfer through Observation Mapping and Behavior Cloning. (arXiv:2310.08836v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35266;&#23519;&#26144;&#23556;&#21644;&#34892;&#20026;&#20811;&#38534;&#36827;&#34892;&#23569;&#26679;&#26412;&#31574;&#30053;&#36716;&#31227;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#26144;&#23556;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#35266;&#23519;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#26144;&#23556;&#26469;&#20811;&#38534;&#28304;&#20219;&#21153;&#30340;&#25104;&#21151;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#20132;&#20114;&#25104;&#26412;&#65292;&#35768;&#22810;&#20219;&#21153;&#20173;&#28982;&#38590;&#20197;&#35299;&#20915;&#12290;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#23558;&#22312;&#28304;&#39046;&#22495;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#30446;&#26631;&#39046;&#22495;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;Sim2Real&#36801;&#31227;&#26377;&#21161;&#20110;&#23558;&#22312;&#27169;&#25311;&#26426;&#22120;&#20154;&#39046;&#22495;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#29289;&#29702;&#30446;&#26631;&#39046;&#22495;&#12290;&#30693;&#35782;&#36801;&#31227;&#20943;&#23569;&#20102;&#22312;&#20132;&#20114;&#25104;&#26412;&#39640;&#30340;&#29289;&#29702;&#19990;&#30028;&#20013;&#35757;&#32451;&#20219;&#21153;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#20004;&#20010;&#39046;&#22495;&#30340;&#20219;&#21153;&#32467;&#26500;&#21644;&#29289;&#29702;&#23646;&#24615;&#23436;&#20840;&#23545;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35266;&#23519;&#26144;&#23556;&#21644;&#34892;&#20026;&#20811;&#38534;&#36827;&#34892;&#23569;&#26679;&#26412;&#31574;&#30053;&#36716;&#31227;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#26469;&#26144;&#23556;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#35266;&#23519;&#65292;&#24182;&#19988;&#21518;&#32493;&#20351;&#29992;&#36825;&#20010;&#23398;&#21040;&#30340;&#26144;&#23556;&#26469;&#20811;&#38534;&#28304;&#20219;&#21153;&#30340;&#25104;&#21151;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in Reinforcement Learning for robotics applications, many tasks remain prohibitively difficult to solve because of the expensive interaction cost. Transfer learning helps reduce the training time in the target domain by transferring knowledge learned in a source domain. Sim2Real transfer helps transfer knowledge from a simulated robotic domain to a physical target domain. Knowledge transfer reduces the time required to train a task in the physical world, where the cost of interactions is high. However, most existing approaches assume exact correspondence in the task structure and the physical properties of the two domains. This work proposes a framework for Few-Shot Policy Transfer between two domains through Observation Mapping and Behavior Cloning. We use Generative Adversarial Networks (GANs) along with a cycle-consistency loss to map the observations between the source and target domains and later use this learned mapping to clone the successful source task 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#23545;&#20110;&#22343;&#21248;&#25910;&#25947;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$&#30340;&#20248;&#21270;&#31574;&#30053;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.08833</link><description>&lt;p&gt;
&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimal Sample Complexity for Average Reward Markov Decision Processes. (arXiv:2310.08833v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#23545;&#20110;&#22343;&#21248;&#25910;&#25947;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$&#30340;&#20248;&#21270;&#31574;&#30053;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;&#20102;&#19982;&#22343;&#21248;&#25910;&#25947;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30456;&#20851;&#30340;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#30340;&#31574;&#30053;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#29616;&#26377;&#30340;&#25991;&#29486;&#25552;&#20379;&#20102;&#19968;&#20010;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#65292;$ \widetilde O(|S||A|t_{\text{mix}}^2 \epsilon^{-2})$&#65292;&#21644;&#19968;&#20010;&#19979;&#30028;&#65292;$\Omega(|S||A|t_{\text{mix}} \epsilon^{-2})$&#12290;&#22312;&#36825;&#20123;&#34920;&#36798;&#24335;&#20013;&#65292;$|S|$&#21644;$|A|$&#20998;&#21035;&#34920;&#31034;&#29366;&#24577;&#31354;&#38388;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#21183;&#65292;$t_{\text{mix}}$&#20316;&#20026;&#24635;&#21464;&#24322;&#28151;&#21512;&#26102;&#38388;&#30340;&#32479;&#19968;&#19978;&#38480;&#65292;$\epsilon$&#34920;&#31034;&#35823;&#24046;&#23481;&#24525;&#24230;&#12290;&#22240;&#27492;&#65292;$t_{\text{mix}}$&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26174;&#30528;&#30340;&#24046;&#36317;&#38656;&#35201;&#22635;&#34917;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24314;&#31435;&#19968;&#20010;&#20248;&#21270;&#31574;&#30053;&#30340;&#20272;&#35745;&#22120;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$&#65292;&#26377;&#25928;&#22320;&#36798;&#21040;&#20102;&#25991;&#29486;&#20013;&#30340;&#19979;&#30028;&#12290;&#36825;&#26159;&#36890;&#36807;&#32467;&#21512;&#31639;&#27861;&#24605;&#24819;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We settle the sample complexity of policy learning for the maximization of the long run average reward associated with a uniformly ergodic Markov decision process (MDP), assuming a generative model. In this context, the existing literature provides a sample complexity upper bound of $\widetilde O(|S||A|t_{\text{mix}}^2 \epsilon^{-2})$ and a lower bound of $\Omega(|S||A|t_{\text{mix}} \epsilon^{-2})$. In these expressions, $|S|$ and $|A|$ denote the cardinalities of the state and action spaces respectively, $t_{\text{mix}}$ serves as a uniform upper limit for the total variation mixing times, and $\epsilon$ signifies the error tolerance. Therefore, a notable gap of $t_{\text{mix}}$ still remains to be bridged. Our primary contribution is to establish an estimator for the optimal policy of average reward MDPs with a sample complexity of $\widetilde O(|S||A|t_{\text{mix}}\epsilon^{-2})$, effectively reaching the lower bound in the literature. This is achieved by combining algorithmic idea
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Distance-rank&#24863;&#30693;&#39034;&#24207;&#22870;&#21169;&#23398;&#20064;&#65288;DRASRL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#36712;&#36857;&#25490;&#21517;&#27169;&#31946;&#21644;&#22870;&#21169;&#27169;&#31946;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08823</link><description>&lt;p&gt;
Distance-rank&#24863;&#30693;&#39034;&#24207;&#22870;&#21169;&#23398;&#20064;&#29992;&#20110;&#20855;&#26377;&#27425;&#20248;&#28436;&#31034;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distance-rank Aware Sequential Reward Learning for Inverse Reinforcement Learning with Sub-optimal Demonstrations. (arXiv:2310.08823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08823
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Distance-rank&#24863;&#30693;&#39034;&#24207;&#22870;&#21169;&#23398;&#20064;&#65288;DRASRL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#36712;&#36857;&#25490;&#21517;&#27169;&#31946;&#21644;&#22870;&#21169;&#27169;&#31946;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26088;&#22312;&#22522;&#20110;&#25910;&#38598;&#21040;&#30340;&#19987;&#23478;&#28436;&#31034;&#26126;&#30830;&#25512;&#26029;&#20986;&#28508;&#22312;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#32771;&#34385;&#21040;&#33719;&#21462;&#19987;&#23478;&#28436;&#31034;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#65292;&#24403;&#21069;IRL&#25216;&#26415;&#30340;&#37325;&#28857;&#26159;&#20351;&#29992;&#20174;&#27425;&#20248;&#28436;&#31034;&#20013;&#23548;&#20986;&#30340;&#22870;&#21169;&#20989;&#25968;&#23398;&#20064;&#19968;&#20010;&#20248;&#20110;&#28436;&#31034;&#32773;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;IRL&#31639;&#27861;&#20027;&#35201;&#35299;&#20915;&#20102;&#22312;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#26102;&#30340;&#36712;&#36857;&#25490;&#21517;&#27169;&#31946;&#30340;&#25361;&#25112;&#65292;&#21364;&#24573;&#35270;&#20102;&#22312;&#36827;&#19968;&#27493;&#28040;&#38500;&#22870;&#21169;&#27169;&#31946;&#24615;&#26041;&#38754;&#65292;&#32771;&#34385;&#36712;&#36857;&#20043;&#38388;&#30340;&#22238;&#25253;&#24046;&#24322;&#31243;&#24230;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#21333;&#20010;&#36716;&#25442;&#30340;&#22870;&#21169;&#21463;&#21040;&#36712;&#36857;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Distance-rank&#24863;&#30693;&#39034;&#24207;&#22870;&#21169;&#23398;&#20064;&#65288;DRASRL&#65289;&#26694;&#26550;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;DRASRL&#21516;&#26102;&#32771;&#34385;&#20102;&#36712;&#36857;&#30340;&#25490;&#21517;&#21644;&#22238;&#25253;&#20043;&#38388;&#30340;&#24046;&#24322;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse reinforcement learning (IRL) aims to explicitly infer an underlying reward function based on collected expert demonstrations. Considering that obtaining expert demonstrations can be costly, the focus of current IRL techniques is on learning a better-than-demonstrator policy using a reward function derived from sub-optimal demonstrations. However, existing IRL algorithms primarily tackle the challenge of trajectory ranking ambiguity when learning the reward function. They overlook the crucial role of considering the degree of difference between trajectories in terms of their returns, which is essential for further removing reward ambiguity. Additionally, it is important to note that the reward of a single transition is heavily influenced by the context information within the trajectory. To address these issues, we introduce the Distance-rank Aware Sequential Reward Learning (DRASRL) framework. Unlike existing approaches, DRASRL takes into account both the ranking of trajectories
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#37327;&#34920;&#31572;&#39064;&#36807;&#31243;&#20013;&#30340;&#21453;&#24212;&#26102;&#38388;&#19982;&#22833;&#30496;&#20005;&#37325;&#31243;&#24230;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#21453;&#24212;&#26102;&#38388;&#25968;&#25454;&#39044;&#27979;&#21442;&#19982;&#32773;&#26159;&#21542;&#23384;&#22312;&#22833;&#30496;&#12290;</title><link>http://arxiv.org/abs/2310.08817</link><description>&lt;p&gt;
&#25506;&#32034;&#37327;&#34920;&#31572;&#39064;&#36807;&#31243;&#20013;&#21453;&#24212;&#26102;&#38388;&#24207;&#21015;&#19982;&#22833;&#30496;&#20005;&#37325;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring the relationship between response time sequence in scale answering process and severity of insomnia: a machine learning approach. (arXiv:2310.08817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08817
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#37327;&#34920;&#31572;&#39064;&#36807;&#31243;&#20013;&#30340;&#21453;&#24212;&#26102;&#38388;&#19982;&#22833;&#30496;&#20005;&#37325;&#31243;&#24230;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#21453;&#24212;&#26102;&#38388;&#25968;&#25454;&#39044;&#27979;&#21442;&#19982;&#32773;&#26159;&#21542;&#23384;&#22312;&#22833;&#30496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#22833;&#30496;&#19982;&#21453;&#24212;&#26102;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#21453;&#24212;&#26102;&#38388;&#25968;&#25454;&#24320;&#21457;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#21442;&#19982;&#32773;&#26159;&#21542;&#23384;&#22312;&#22833;&#30496;&#12290;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#20174;2729&#21517;&#21442;&#19982;&#32773;&#37027;&#37324;&#25910;&#38598;&#21040;&#37327;&#34920;&#27979;&#35797;&#21644;&#21453;&#24212;&#26102;&#38388;&#25968;&#25454;&#12290;&#30740;&#31350;&#21457;&#29616;&#22833;&#30496;&#30151;&#29366;&#30340;&#21442;&#19982;&#32773;&#19982;&#38750;&#22833;&#30496;&#30151;&#29366;&#30340;&#21442;&#19982;&#32773;&#22312;&#24635;&#21453;&#24212;&#26102;&#38388;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65288;p &lt;0.001&#65289;&#12290;&#22312;&#20010;&#20307;&#38382;&#39064;&#27700;&#24179;&#19978;&#35266;&#23519;&#21040;&#29305;&#23450;&#22833;&#30496;&#26041;&#38754;&#30340;&#20005;&#37325;&#31243;&#24230;&#21644;&#21453;&#24212;&#26102;&#38388;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35813;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22522;&#20110;&#21453;&#24212;&#26102;&#38388;&#25968;&#25454;&#39044;&#27979;&#22833;&#30496;&#30151;&#29366;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#65288;0.743&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objectives: The study aims to investigate the relationship between insomnia and response time. Additionally, it aims to develop a machine learning model to predict the presence of insomnia in participants using response time data. Methods: A mobile application was designed to administer scale tests and collect response time data from 2729 participants. The relationship between symptom severity and response time was explored, and a machine learning model was developed to predict the presence of insomnia. Results: The result revealed a statistically significant difference (p&lt;.001) in the total response time between participants with or without insomnia symptoms. A correlation was observed between the severity of specific insomnia aspects and response times at the individual questions level. The machine learning model demonstrated a high predictive accuracy of 0.743 in predicting insomnia symptoms based on response time data. Conclusions: These findings highlight the potential utility of 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;VMD-GARCH-LSTM&#27169;&#22411;&#26469;&#25429;&#25417;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;&#27874;&#21160;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#21508;&#20010;&#23376;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08812</link><description>&lt;p&gt;
&#20351;&#29992;VMD-GARCH-LSTM&#27169;&#22411;&#30340;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Nonlinear Method for time series forecasting using VMD-GARCH-LSTM model. (arXiv:2310.08812v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;VMD-GARCH-LSTM&#27169;&#22411;&#26469;&#25429;&#25417;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#29305;&#24449;&#21644;&#27874;&#21160;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#21508;&#20010;&#23376;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27169;&#24577;&#20998;&#35299;&#30340;&#26041;&#27861;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25429;&#25417;&#23616;&#37096;&#29305;&#24449;&#24182;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#20869;&#22312;&#27169;&#24577;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21253;&#21547;&#37325;&#35201;&#20449;&#24687;&#30340;&#26263;&#21547;&#27874;&#21160;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#24403;&#21069;&#12289;&#24555;&#36895;&#28436;&#21464;&#21644;&#27874;&#21160;&#24615;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#35299;-&#38598;&#25104;&#33539;&#24335;&#65292;&#21363;VMD-LSTM-GARCH&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21464;&#20998;&#27169;&#24577;&#20998;&#35299;&#31639;&#27861;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#20026;K&#20010;&#23376;&#27169;&#24577;&#12290;&#38543;&#21518;&#65292;GARCH&#27169;&#22411;&#20174;&#36825;&#20123;&#23376;&#27169;&#24577;&#20013;&#25552;&#21462;&#27874;&#21160;&#24615;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;LSTM&#30340;&#36755;&#20837;&#12290;&#27599;&#20010;&#23376;&#27169;&#24577;&#30340;&#25968;&#20540;&#21644;&#27874;&#21160;&#24615;&#20449;&#24687;&#34987;&#29992;&#26469;&#35757;&#32451;&#19968;&#20010;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#39044;&#27979;&#23376;&#27169;&#24577;&#65292;&#28982;&#21518;&#25105;&#20204;&#23558;&#25152;&#26377;&#23376;&#27169;&#24577;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting represents a significant and challenging task across various fields. Recently, methods based on mode decomposition have dominated the forecasting of complex time series because of the advantages of capturing local characteristics and extracting intrinsic modes from data. Unfortunately, most models fail to capture the implied volatilities that contain significant information. To enhance the forecasting of current, rapidly evolving, and volatile time series, we propose a novel decomposition-ensemble paradigm, the VMD-LSTM-GARCH model. The Variational Mode Decomposition algorithm is employed to decompose the time series into K sub-modes. Subsequently, the GARCH model extracts the volatility information from these sub-modes, which serve as the input for the LSTM. The numerical and volatility information of each sub-mode is utilized to train a Long Short-Term Memory network. This network predicts the sub-mode, and then we aggregate the predictions from all sub-modes 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDMT&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#22122;&#22768;&#21644;&#24369;&#36523;&#20221;&#26144;&#23556;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#21160;&#24577;&#37051;&#23621;&#25513;&#33180;&#26426;&#21046;(ADNM)&#20197;&#21450;&#38598;&#25104;Transformer&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2310.08800</link><description>&lt;p&gt;
DDMT: &#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#21435;&#22122;&#25193;&#25955;&#25513;&#33180;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DDMT: Denoising Diffusion Mask Transformer Models for Multivariate Time Series Anomaly Detection. (arXiv:2310.08800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DDMT&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#22122;&#22768;&#21644;&#24369;&#36523;&#20221;&#26144;&#23556;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#21160;&#24577;&#37051;&#23621;&#25513;&#33180;&#26426;&#21046;(ADNM)&#20197;&#21450;&#38598;&#25104;Transformer&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#24050;&#32463;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22312;&#27450;&#35784;&#26816;&#27979;&#12289;&#25925;&#38556;&#35786;&#26029;&#21644;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#31561;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#37325;&#24314;&#30340;&#27169;&#22411;&#22312;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#35268;&#27169;&#21644;&#32500;&#24230;&#30340;&#24555;&#36895;&#22686;&#21152;&#65292;&#26102;&#38388;&#24207;&#21015;&#37325;&#24314;&#36807;&#31243;&#20013;&#30340;&#22122;&#22768;&#21644;&#24369;&#36523;&#20221;&#26144;&#23556;&#38382;&#39064;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#21160;&#24577;&#37051;&#23621;&#25513;&#33180;&#26426;&#21046;(ADNM)&#65292;&#23558;&#20854;&#19982;Transformer&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#21517;&#20026;Denoising Diffusion Mask Transformer (DDMT) &#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;ADNM&#27169;&#22359;&#29992;&#20110;&#20943;&#36731;&#25968;&#25454;&#37325;&#24314;&#36807;&#31243;&#20013;&#36755;&#20837;&#21644;&#36755;&#20986;&#29305;&#24449;&#20043;&#38388;&#30340;&#20449;&#24687;&#27844;&#28431;&#38382;&#39064;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#24369;&#36523;&#20221;&#26144;&#23556;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in multivariate time series has emerged as a crucial challenge in time series research, with significant research implications in various fields such as fraud detection, fault diagnosis, and system state estimation. Reconstruction-based models have shown promising potential in recent years for detecting anomalies in time series data. However, due to the rapid increase in data scale and dimensionality, the issues of noise and Weak Identity Mapping (WIM) during time series reconstruction have become increasingly pronounced. To address this, we introduce a novel Adaptive Dynamic Neighbor Mask (ADNM) mechanism and integrate it with the Transformer and Denoising Diffusion Model, creating a new framework for multivariate time series anomaly detection, named Denoising Diffusion Mask Transformer (DDMT). The ADNM module is introduced to mitigate information leakage between input and output features during data reconstruction, thereby alleviating the problem of WIM during recon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#27979;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#12290;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.08795</link><description>&lt;p&gt;
&#36890;&#36807;&#36861;&#36394;&#20559;&#35265;&#24433;&#21709;&#26469;&#20943;&#36731;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Bias for Question Answering Models by Tracking Bias Influence. (arXiv:2310.08795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#27979;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#12290;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#27169;&#22411;&#23384;&#22312;&#21051;&#26495;&#21360;&#35937;&#65292;&#32780;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#23588;&#20854;&#26377;&#23475;&#65292;&#22240;&#20026;&#36755;&#20986;&#30340;&#31572;&#26696;&#21487;&#33021;&#30452;&#25509;&#34987;&#26368;&#32456;&#29992;&#25143;&#20351;&#29992;&#12290;&#24050;&#32463;&#26377;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;QA&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20294;&#26159;&#23545;&#20110;QA&#27169;&#22411;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#20173;&#22788;&#20110;&#25506;&#32034;&#38454;&#27573;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#22522;&#20110;&#19968;&#20010;&#30452;&#35273;&#65292;&#21363;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#20174;&#19968;&#20010;&#26377;&#20559;&#35265;&#30340;&#20363;&#23376;&#20013;&#23398;&#21040;&#20102;&#19996;&#35199;&#65292;&#23427;&#21487;&#33021;&#26356;&#23481;&#26131;&#20986;&#29616;&#20559;&#35265;&#65292;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#26469;&#34913;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;&#22914;&#26524;&#21463;&#21040;&#24433;&#21709;&#30340;&#23454;&#20363;&#26356;&#20559;&#35265;&#65292;&#25105;&#20204;&#35748;&#20026;&#26597;&#35810;&#23454;&#20363;&#26159;&#26377;&#20559;&#35265;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#27979;&#21040;&#30340;&#20559;&#35265;&#31243;&#24230;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#38500;&#20102;&#21407;&#26469;&#30340;QA&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#20840;&#38754;&#32780;&#25935;&#24863;&#30340;&#26041;&#24335;&#37327;&#21270;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20943;&#36731;QA&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of various NLP tasks have been shown to exhibit stereotypes, and the bias in the question answering (QA) models is especially harmful as the output answers might be directly consumed by the end users. There have been datasets to evaluate bias in QA models, while bias mitigation technique for the QA models is still under-explored. In this work, we propose BMBI, an approach to mitigate the bias of multiple-choice QA models. Based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. If the influenced instance is more biased, we derive that the query instance is biased. We then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task. We further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. We show that our method could be applie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26102;&#38388;&#21644;&#22825;&#27668;&#20449;&#24687;&#39044;&#27979;&#30701;&#26399;&#31995;&#32479;&#24635;&#36127;&#33655;&#65292;&#21457;&#29616;&#26368;&#22823;&#31243;&#24230;&#21033;&#29992;&#21508;&#31181;&#30456;&#20851;&#29305;&#24449;&#19981;&#19968;&#23450;&#33021;&#36798;&#21040;&#26368;&#20339;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08793</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;ERCOT&#36127;&#33655;&#39044;&#27979;&#20013;&#30340;&#22825;&#27668;&#21644;&#26102;&#38388;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Weather and Time Features in Machine Learning-aided ERCOT Load Forecasting. (arXiv:2310.08793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26102;&#38388;&#21644;&#22825;&#27668;&#20449;&#24687;&#39044;&#27979;&#30701;&#26399;&#31995;&#32479;&#24635;&#36127;&#33655;&#65292;&#21457;&#29616;&#26368;&#22823;&#31243;&#24230;&#21033;&#29992;&#21508;&#31181;&#30456;&#20851;&#29305;&#24449;&#19981;&#19968;&#23450;&#33021;&#36798;&#21040;&#26368;&#20339;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#36127;&#33655;&#39044;&#27979;&#23545;&#20110;&#30005;&#21147;&#31995;&#32479;&#30340;&#39640;&#25928;&#21487;&#38752;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#30005;&#21147;&#28040;&#32791;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#21463;&#22825;&#27668;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#22825;&#27668;&#20449;&#24687;&#26159;&#24433;&#21709;&#30005;&#21147;&#20351;&#29992;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#20010;&#20154;&#30005;&#22120;&#21644;&#24037;&#19994;&#35774;&#22791;&#20063;&#20197;&#20854;&#26102;&#38388;&#27169;&#24335;&#26174;&#33879;&#36129;&#29486;&#20110;&#30005;&#21147;&#38656;&#27714;&#65292;&#22240;&#27492;&#26102;&#38388;&#20063;&#26159;&#36127;&#33655;&#39044;&#27979;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#19968;&#20010;&#26377;&#29992;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20854;&#20013;&#23558;&#21508;&#31181;&#26102;&#38388;&#21644;&#22825;&#27668;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#29992;&#20110;&#39044;&#27979;&#30701;&#26399;&#31995;&#32479;&#24635;&#36127;&#33655;&#12290;&#36824;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;&#21644;&#27604;&#36739;&#19981;&#21516;&#22825;&#27668;&#22240;&#32032;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#23454;&#38469;&#36127;&#33655;&#21644;&#21382;&#21490;&#22825;&#27668;&#25968;&#25454;&#65292;&#35757;&#32451;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#26377;&#36259;&#30340;&#26159;&#35266;&#23519;&#21040;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#33021;&#19982;&#36127;&#33655;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#19981;&#22826;&#21487;&#33021;&#36798;&#21040;&#26368;&#20339;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate load forecasting is critical for efficient and reliable operations of the electric power system. A large part of electricity consumption is affected by weather conditions, making weather information an important determinant of electricity usage. Personal appliances and industry equipment also contribute significantly to electricity demand with temporal patterns, making time a useful factor to consider in load forecasting. This work develops several machine learning (ML) models that take various time and weather information as part of the input features to predict the short-term system-wide total load. Ablation studies were also performed to investigate and compare the impacts of different weather factors on the prediction accuracy. Actual load and historical weather data for the same region were processed and then used to train the ML models. It is interesting to observe that using all available features, each of which may be correlated to the load, is unlikely to achieve the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20998;&#24067;&#24335;&#38598;&#25104;&#23398;&#20064;&#25552;&#20986;&#20102;&#28608;&#21169;&#26426;&#21046;&#35774;&#35745;&#12290;&#36890;&#36807;&#25351;&#23450;&#23398;&#20064;&#32773;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#22870;&#21169;&#37329;&#39069;&#65292;&#35299;&#20915;&#20102;&#33258;&#21033;&#23398;&#20064;&#32773;&#19981;&#24895;&#21442;&#19982;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25286;&#35299;&#38598;&#25104;&#20934;&#30830;&#24615;&#20026;&#22810;&#26679;&#24615;-&#31934;&#30830;&#24615;&#26435;&#34913;&#65292;&#25351;&#23548;&#20102;&#26426;&#21046;&#35774;&#35745;&#12290;&#36890;&#36807;&#26032;&#30340;&#24555;&#36895;&#27714;&#35299;&#31639;&#27861;&#65292;&#20934;&#30830;&#20272;&#35745;&#20102;&#27599;&#20010;&#23398;&#20064;&#32773;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.08792</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#38598;&#25104;&#23398;&#20064;&#30340;&#28608;&#21169;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Incentive Mechanism Design for Distributed Ensemble Learning. (arXiv:2310.08792v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20998;&#24067;&#24335;&#38598;&#25104;&#23398;&#20064;&#25552;&#20986;&#20102;&#28608;&#21169;&#26426;&#21046;&#35774;&#35745;&#12290;&#36890;&#36807;&#25351;&#23450;&#23398;&#20064;&#32773;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#22870;&#21169;&#37329;&#39069;&#65292;&#35299;&#20915;&#20102;&#33258;&#21033;&#23398;&#20064;&#32773;&#19981;&#24895;&#21442;&#19982;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25286;&#35299;&#38598;&#25104;&#20934;&#30830;&#24615;&#20026;&#22810;&#26679;&#24615;-&#31934;&#30830;&#24615;&#26435;&#34913;&#65292;&#25351;&#23548;&#20102;&#26426;&#21046;&#35774;&#35745;&#12290;&#36890;&#36807;&#26032;&#30340;&#24555;&#36895;&#27714;&#35299;&#31639;&#27861;&#65292;&#20934;&#30830;&#20272;&#35745;&#20102;&#27599;&#20010;&#23398;&#20064;&#32773;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#38598;&#25104;&#23398;&#20064;&#65288;DEL&#65289;&#28041;&#21450;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#32773;&#20013;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#28982;&#21518;&#32467;&#21512;&#23427;&#20204;&#30340;&#39044;&#27979;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#30456;&#20851;&#30740;&#31350;&#38598;&#20013;&#22312;DEL&#31639;&#27861;&#35774;&#35745;&#21644;&#20248;&#21270;&#19978;&#65292;&#20294;&#24573;&#35270;&#20102;&#28608;&#21169;&#36825;&#19968;&#37325;&#35201;&#38382;&#39064;&#65292;&#27809;&#26377;&#28608;&#21169;&#26426;&#21046;&#65292;&#33258;&#21033;&#23398;&#20064;&#32773;&#21487;&#33021;&#19981;&#24895;&#21442;&#19982;DEL&#12290;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;DEL&#28608;&#21169;&#26426;&#21046;&#35774;&#35745;&#30340;&#39318;&#20010;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26426;&#21046;&#35268;&#23450;&#20102;&#20855;&#26377;&#24322;&#26500;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#30340;&#23398;&#20064;&#32773;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#22870;&#21169;&#37329;&#39069;&#12290;&#19968;&#20010;&#35774;&#35745;&#25361;&#25112;&#26159;&#20934;&#30830;&#29702;&#35299;&#23398;&#20064;&#32773;&#30340;&#22810;&#26679;&#24615;&#65288;&#21363;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#65289;&#22914;&#20309;&#24433;&#21709;&#38598;&#25104;&#20934;&#30830;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#38598;&#25104;&#20934;&#30830;&#24615;&#25286;&#35299;&#20026;&#22810;&#26679;&#24615;-&#31934;&#30830;&#24615;&#26435;&#34913;&#65292;&#20197;&#25351;&#23548;&#26426;&#21046;&#35774;&#35745;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#26426;&#21046;&#35774;&#35745;&#28041;&#21450;&#35299;&#20915;&#20855;&#26377;&#22823;&#25628;&#32034;&#31354;&#38388;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24555;&#36895;&#27714;&#35299;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#21644;&#36890;&#20449;&#25104;&#26412;&#26469;&#20934;&#30830;&#20272;&#35745;&#27599;&#20010;&#23398;&#20064;&#32773;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed ensemble learning (DEL) involves training multiple models at distributed learners, and then combining their predictions to improve performance. Existing related studies focus on DEL algorithm design and optimization but ignore the important issue of incentives, without which self-interested learners may be unwilling to participate in DEL. We aim to fill this gap by presenting a first study on the incentive mechanism design for DEL. Our proposed mechanism specifies both the amount of training data and reward for learners with heterogeneous computation and communication costs. One design challenge is to have an accurate understanding regarding how learners' diversity (in terms of training data) affects the ensemble accuracy. To this end, we decompose the ensemble accuracy into a diversity-precision tradeoff to guide the mechanism design. Another challenge is that the mechanism design involves solving a mixed-integer program with a large search space. To this end, we propose a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#30740;&#31350;&#20102;&#26631;&#31614;&#22122;&#22768;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#28216;&#25103;&#27169;&#22411;&#26469;&#29702;&#35299;&#23458;&#25143;&#31471;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22343;&#34913;&#32467;&#26524;&#30340;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24230;&#20302;&#20110;&#31038;&#20250;&#26368;&#20248;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#26469;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2310.08790</link><description>&lt;p&gt;
&#36136;&#37327;&#24863;&#30693;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Price of Stability in Quality-Aware Federated Learning. (arXiv:2310.08790v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65292;&#30740;&#31350;&#20102;&#26631;&#31614;&#22122;&#22768;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#28216;&#25103;&#27169;&#22411;&#26469;&#29702;&#35299;&#23458;&#25143;&#31471;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22343;&#34913;&#32467;&#26524;&#30340;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24230;&#20302;&#20110;&#31038;&#20250;&#26368;&#20248;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#26469;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#26696;&#65292;&#21487;&#20197;&#20351;&#23458;&#25143;&#31471;&#22312;&#19981;&#20132;&#25442;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#26631;&#31614;&#22122;&#22768;&#30340;&#23384;&#22312;&#20250;&#20005;&#37325;&#24433;&#21709;&#32852;&#37030;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#19968;&#20123;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#20851;&#27880;&#20102;&#29992;&#20110;&#26631;&#31614;&#21435;&#22122;&#30340;&#31639;&#27861;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24573;&#35270;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#21363;&#30001;&#20110;&#33258;&#21033;&#24615;&#21644;&#23545;&#32852;&#37030;&#23398;&#20064;&#24615;&#33021;&#30340;&#24322;&#36136;&#20272;&#20540;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#19981;&#20250;&#24212;&#29992;&#26114;&#36149;&#30340;&#26631;&#31614;&#21435;&#22122;&#31574;&#30053;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23558;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20114;&#21160;&#24314;&#27169;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#31614;&#21435;&#22122;&#21338;&#24328;&#65292;&#24182;&#30830;&#23450;&#20854;&#22343;&#34913;&#29366;&#24577;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#31283;&#23450;&#25104;&#26412;&#65292;&#35813;&#25104;&#26412;&#29992;&#26469;&#34913;&#37327;&#22343;&#34913;&#32467;&#26524;&#19982;&#31038;&#20250;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#31995;&#32479;&#24615;&#33021;&#24046;&#24322;&#65288;&#20363;&#22914;&#65292;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24230;&#12289;&#31038;&#20250;&#31119;&#21033;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22343;&#34913;&#32467;&#26524;&#30340;&#20840;&#23616;&#27169;&#22411;&#20934;&#30830;&#24230;&#22987;&#32456;&#20302;&#20110;&#31038;&#20250;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#31639;&#27861;&#26469;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning scheme that enables clients to train a shared global model without exchanging local data. The presence of label noise can severely degrade the FL performance, and some existing studies have focused on algorithm design for label denoising. However, they ignored the important issue that clients may not apply costly label denoising strategies due to them being self-interested and having heterogeneous valuations on the FL performance. To fill this gap, we model the clients' interactions as a novel label denoising game and characterize its equilibrium. We also analyze the price of stability, which quantifies the difference in the system performance (e.g., global model accuracy, social welfare) between the equilibrium outcome and the socially optimal solution. We prove that the equilibrium outcome always leads to a lower global model accuracy than the socially optimal solution does. We further design an efficient algorithm to compute 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#36825;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.08782</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#39537;&#21160;&#29983;&#20135;&#21147;&#65306;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Selectivity Drives Productivity: Efficient Dataset Pruning for Enhanced Transfer Learning. (arXiv:2310.08782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#25968;&#25454;&#38598;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#25968;&#25454;&#38598;&#20462;&#21098;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#35266;&#28857;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#36825;&#20004;&#31181;&#26032;&#30340;&#25968;&#25454;&#38598;&#20462;&#21098;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20294;&#21516;&#26102;&#20063;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#22522;&#30784;&#35774;&#26045;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#38598;&#20462;&#21098;&#65288;DP&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20986;&#29616;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#21024;&#38500;&#20887;&#20313;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35299;&#20915;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;DP&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#19979;&#28216;&#30446;&#26631;&#20219;&#21153;&#20013;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#21644;&#23436;&#25972;&#24494;&#35843;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20462;&#21098;&#28304;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36801;&#31227;&#23398;&#20064;&#30340;DP&#38382;&#39064;&#20173;&#28982;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;DP&#21644;&#36801;&#31227;&#23398;&#20064;&#35270;&#20026;&#29420;&#31435;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#65292;&#23558;DP&#19982;&#36801;&#31227;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;DP&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;DP&#26041;&#27861;&#65292;&#21363;&#26631;&#31614;&#26144;&#23556;&#21644;&#29305;&#24449;&#26144;&#23556;&#65292;&#29992;&#20110;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive data is often considered essential for deep learning applications, but it also incurs significant computational and infrastructural costs. Therefore, dataset pruning (DP) has emerged as an effective way to improve data efficiency by identifying and removing redundant training samples without sacrificing performance. In this work, we aim to address the problem of DP for transfer learning, i.e., how to prune a source dataset for improved pretraining efficiency and lossless finetuning accuracy on downstream target tasks. To our best knowledge, the problem of DP for transfer learning remains open, as previous studies have primarily addressed DP and transfer learning as separate problems. By contrast, we establish a unified viewpoint to integrate DP with transfer learning and find that existing DP methods are not suitable for the transfer learning paradigm. We then propose two new DP methods, label mapping and feature mapping, for supervised and self-supervised pretraining settings 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#19968;&#20010;&#39044;&#27979;&#20154;&#21592;&#25110;&#23478;&#24237;&#26159;&#21542;&#20250;&#22312;&#25509;&#19979;&#26469;&#30340;&#20004;&#24180;&#20869;&#25644;&#36801;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#20197;&#21450;&#20844;&#24320;&#30340;&#35757;&#32451;&#25968;&#25454;&#36793;&#38469;&#20998;&#24067;&#26469;&#25512;&#26029;&#30446;&#26631;&#20010;&#20307;&#30340;&#25935;&#24863;&#23646;&#24615;&#20540;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#26367;&#20195;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08775</link><description>&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27844;&#28431;&#65306;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
When Machine Learning Models Leak: An Exploration of Synthetic Training Data. (arXiv:2310.08775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#19968;&#20010;&#39044;&#27979;&#20154;&#21592;&#25110;&#23478;&#24237;&#26159;&#21542;&#20250;&#22312;&#25509;&#19979;&#26469;&#30340;&#20004;&#24180;&#20869;&#25644;&#36801;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#27169;&#22411;&#30340;&#39044;&#27979;&#20197;&#21450;&#20844;&#24320;&#30340;&#35757;&#32451;&#25968;&#25454;&#36793;&#38469;&#20998;&#24067;&#26469;&#25512;&#26029;&#30446;&#26631;&#20010;&#20307;&#30340;&#25935;&#24863;&#23646;&#24615;&#20540;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#29992;&#21512;&#25104;&#25968;&#25454;&#26367;&#20195;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#23545;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#19968;&#20010;&#20154;&#25110;&#23478;&#24237;&#22312;&#25509;&#19979;&#26469;&#30340;&#20004;&#24180;&#20869;&#26159;&#21542;&#20250;&#25644;&#36801;&#65292;&#21363;&#36801;&#31227;&#20542;&#21521;&#20998;&#31867;&#22120;&#12290;&#25915;&#20987;&#20551;&#35774;&#25915;&#20987;&#32773;&#21487;&#20197;&#26597;&#35810;&#27169;&#22411;&#20197;&#33719;&#21462;&#39044;&#27979;&#65292;&#24182;&#19988;&#27169;&#22411;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#25968;&#25454;&#30340;&#36793;&#38469;&#20998;&#24067;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;&#25915;&#20987;&#36824;&#20551;&#35774;&#25915;&#20987;&#32773;&#24050;&#32463;&#33719;&#21462;&#20102;&#19968;&#23450;&#25968;&#37327;&#30446;&#26631;&#20010;&#20307;&#30340;&#38750;&#25935;&#24863;&#23646;&#24615;&#20540;&#12290;&#25915;&#20987;&#30340;&#30446;&#26631;&#26159;&#25512;&#26029;&#36825;&#20123;&#30446;&#26631;&#20010;&#20307;&#30340;&#25935;&#24863;&#23646;&#24615;&#20540;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#29992;&#21512;&#25104;&#25968;&#25454;&#26367;&#20195;&#21407;&#22987;&#25968;&#25454;&#23545;&#25915;&#20987;&#32773;&#25104;&#21151;&#25512;&#26029;&#25935;&#24863;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate an attack on a machine learning model that predicts whether a person or household will relocate in the next two years, i.e., a propensity-to-move classifier. The attack assumes that the attacker can query the model to obtain predictions and that the marginal distribution of the data on which the model was trained is publicly available. The attack also assumes that the attacker has obtained the values of non-sensitive attributes for a certain number of target individuals. The objective of the attack is to infer the values of sensitive attributes for these target individuals. We explore how replacing the original data with synthetic data when training the model impacts how successfully the attacker can infer sensitive attributes.\footnote{Original paper published at PSD 2022. The paper was subsequently updated.}
&lt;/p&gt;</description></item><item><title>PhyloGFN&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#22797;&#26434;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#33021;&#22815;&#20135;&#29983;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#36827;&#21270;&#20551;&#35774;&#65292;&#24182;&#22312;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08774</link><description>&lt;p&gt;
PhyloGFN: &#22522;&#20110;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
PhyloGFN: Phylogenetic inference with generative flow networks. (arXiv:2310.08774v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08774
&lt;/p&gt;
&lt;p&gt;
PhyloGFN&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27969;&#32593;&#32476;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#26679;&#22797;&#26434;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#33021;&#22815;&#20135;&#29983;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#36827;&#21270;&#20551;&#35774;&#65292;&#24182;&#22312;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#21457;&#32946;&#23398;&#26159;&#35745;&#31639;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#30740;&#31350;&#29983;&#29289;&#23454;&#20307;&#20043;&#38388;&#30340;&#36827;&#21270;&#20851;&#31995;&#12290;&#23613;&#31649;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#21644;&#20247;&#22810;&#24212;&#29992;&#65292;&#20294;&#20174;&#24207;&#21015;&#25968;&#25454;&#25512;&#26029;&#31995;&#32479;&#21457;&#32946;&#26641;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#26641;&#31354;&#38388;&#30340;&#39640;&#22797;&#26434;&#24615;&#23545;&#24403;&#21069;&#30340;&#32452;&#21512;&#21644;&#27010;&#29575;&#25216;&#26415;&#26500;&#25104;&#20102;&#37325;&#35201;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#31995;&#32479;&#21457;&#32946;&#23398;&#20013;&#30340;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#22522;&#20110;&#26368;&#31616;&#21407;&#21017;&#30340;&#21644;&#36125;&#21494;&#26031;&#30340;&#31995;&#32479;&#21457;&#32946;&#25512;&#26029;&#12290;&#30001;&#20110;GFlowNets&#36866;&#29992;&#20110;&#37319;&#26679;&#22797;&#26434;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#23427;&#20204;&#26159;&#25506;&#32034;&#21644;&#37319;&#26679;&#26641;&#25299;&#25169;&#21644;&#36827;&#21270;&#36317;&#31163;&#30340;&#22810;&#27169;&#24577;&#21518;&#39564;&#20998;&#24067;&#30340;&#33258;&#28982;&#36873;&#25321;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25674;&#36824;&#21518;&#39564;&#37319;&#26679;&#22120;PhyloGFN&#22312;&#30495;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#36827;&#21270;&#20551;&#35774;&#12290;PhyloGFN&#22312;&#36793;&#32536;&#20284;&#28982;&#20272;&#35745;&#26041;&#38754;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the high complexity of tree space poses a significant obstacle for the current combinatorial and probabilistic techniques. In this paper, we adopt the framework of generative flow networks (GFlowNets) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. Because GFlowNets are well-suited for sampling complex combinatorial structures, they are a natural choice for exploring and sampling from the multimodal posterior distribution over tree topologies and evolutionary distances. We demonstrate that our amortized posterior sampler, PhyloGFN, produces diverse and high-quality evolutionary hypotheses on real benchmark datasets. PhyloGFN is competitive with prior works in marginal likelihood estimat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;2D&#26680;&#29123;&#26009;&#24494;&#35266;&#32467;&#26500;&#22270;&#20687;&#65292;&#36890;&#36807;&#35757;&#32451;&#20855;&#26377;&#22810;&#23610;&#24230;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#20013;&#23376;&#26448;&#26009;&#20013;&#35010;&#21464;&#27668;&#30340;&#30636;&#26102;&#37322;&#25918;&#36890;&#37327;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.08767</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#23610;&#24230;&#23494;&#38598;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#21450;Inception&#22359;&#27169;&#22411;&#23545;&#20013;&#23376;&#26448;&#26009;&#20013;&#30340;&#35010;&#21464;&#27668;&#37322;&#25918;&#36827;&#34892;&#20013;&#23610;&#24230;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Fission Gas Release at the Mesoscale using Multiscale DenseNet Regression with Attention Mechanism and Inception Blocks. (arXiv:2310.08767v1 [cond-mat.mes-hall])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08767
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;2D&#26680;&#29123;&#26009;&#24494;&#35266;&#32467;&#26500;&#22270;&#20687;&#65292;&#36890;&#36807;&#35757;&#32451;&#20855;&#26377;&#22810;&#23610;&#24230;&#22238;&#24402;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;&#20013;&#23376;&#26448;&#26009;&#20013;&#35010;&#21464;&#27668;&#30340;&#30636;&#26102;&#37322;&#25918;&#36890;&#37327;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#23610;&#24230;&#26680;&#29123;&#26009;&#20013;&#30340;&#35010;&#21464;&#27668;&#37322;&#25918;&#65288;FGR&#65289;&#30340;&#27169;&#25311;&#26159;&#29702;&#35299;&#24494;&#35266;&#32467;&#26500;&#28436;&#21464;&#23545;FGR&#30340;&#24433;&#21709;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#30340;&#12289;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20174;2D&#26680;&#29123;&#26009;&#24494;&#35266;&#32467;&#26500;&#22270;&#20687;&#20013;&#39044;&#27979;&#30636;&#26102;FGR&#36890;&#37327;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;&#22235;&#20010;&#20855;&#26377;&#22810;&#23610;&#24230;&#22238;&#24402;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#65292;&#36825;&#20123;&#32593;&#32476;&#20351;&#29992;&#19968;&#20010;&#28151;&#21512;&#30456;&#22330;/&#32858;&#31867;&#21160;&#21147;&#23398;&#27169;&#22411;&#29983;&#25104;&#30340;&#27169;&#25311;FGR&#25968;&#25454;&#12290;&#25152;&#26377;&#22235;&#20010;&#32593;&#32476;&#37117;&#20855;&#26377;&#24456;&#39640;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;$R^{2}$&#20540;&#22343;&#22312;98%&#20197;&#19978;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;&#32593;&#32476;&#32467;&#21512;&#20102;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;CBAM&#65289;&#21644;InceptionNet&#26426;&#21046;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65288;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#20026;4.4%&#65289;&#12289;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#23545;&#38750;&#24120;&#20302;&#30340;&#30636;&#26102;FGR&#36890;&#37327;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mesoscale simulations of fission gas release (FGR) in nuclear fuel provide a powerful tool for understanding how microstructure evolution impacts FGR, but they are computationally intensive. In this study, we present an alternate, data-driven approach, using deep learning to predict instantaneous FGR flux from 2D nuclear fuel microstructure images. Four convolutional neural network (CNN) architectures with multiscale regression are trained and evaluated on simulated FGR data generated using a hybrid phase field/cluster dynamics model. All four networks show high predictive power, with $R^{2}$ values above 98%. The best performing network combine a Convolutional Block Attention Module (CBAM) and InceptionNet mechanisms to provide superior accuracy (mean absolute percentage error of 4.4%), training stability, and robustness on very low instantaneous FGR flux values.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#30340;&#20284;&#28982;&#24615;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#19982;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#27979;&#37327;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#30456;&#19968;&#33268;&#65292;&#20174;&#32780;&#25552;&#39640;&#25688;&#35201;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.08764</link><description>&lt;p&gt;
&#26657;&#20934;&#20351;&#24471;&#25688;&#35201;&#27169;&#22411;&#22312;&#19968;&#33268;&#24615;&#26041;&#38754;&#26356;&#20026;&#20934;&#30830;
&lt;/p&gt;
&lt;p&gt;
Calibrating Likelihoods towards Consistency in Summarization Models. (arXiv:2310.08764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08764
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#30340;&#20284;&#28982;&#24615;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#19982;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#27979;&#37327;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#30456;&#19968;&#33268;&#65292;&#20174;&#32780;&#25552;&#39640;&#25688;&#35201;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25277;&#35937;&#21270;&#25991;&#26412;&#25688;&#35201;&#21462;&#24471;&#20102;&#19968;&#20123;&#26032;&#30340;&#36827;&#23637;&#65292;&#20294;&#30446;&#21069;&#30340;&#25688;&#35201;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#29983;&#25104;&#20107;&#23454;&#19981;&#19968;&#33268;&#30340;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#36825;&#20943;&#24369;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#36896;&#25104;&#36825;&#31181;&#34892;&#20026;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#30001;&#20110;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#30446;&#26631;&#35757;&#32451;&#30340;&#25688;&#35201;&#27169;&#22411;&#22312;&#32473;&#23450;&#19978;&#19979;&#25991;&#26102;&#36171;&#20104;&#21487;&#33021;&#24207;&#21015;&#39640;&#27010;&#29575;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#19981;&#33021;&#20934;&#30830;&#22320;&#26681;&#25454;&#20854;&#19968;&#33268;&#24615;&#25490;&#21517;&#24207;&#21015;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26657;&#20934;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#30340;&#20284;&#28982;&#24615;&#65292;&#20351;&#20854;&#26356;&#22909;&#22320;&#19982;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#27979;&#37327;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#30456;&#19968;&#33268;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#21644;&#33258;&#21160;&#25351;&#26631;&#34920;&#26126;&#65292;&#32463;&#36807;&#26657;&#20934;&#30340;&#27169;&#22411;&#29983;&#25104;&#26356;&#19968;&#33268;&#19988;&#26356;&#39640;&#36136;&#37327;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#36820;&#22238;&#30340;&#27010;&#29575;&#19982;NLI&#24471;&#20998;&#26356;&#20026;&#23545;&#40784;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#25688;&#35201;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent advances in abstractive text summarization, current summarization models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. We argue that the main reason for such behavior is that the summarization models trained with maximum likelihood objective assign high probability to plausible sequences given the context, but they often do not accurately rank sequences by their consistency. In this work, we solve this problem by calibrating the likelihood of model generated sequences to better align with a consistency metric measured by natural language inference (NLI) models. The human evaluation study and automatic metrics show that the calibrated models generate more consistent and higher-quality summaries. We also show that the models trained using our method return probabilities that are better aligned with the NLI scores, which significantly increase reliability of summarization models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;EEG&#20998;&#31867;&#20013;&#20351;&#29992;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20943;&#23569;&#20102;&#22312;&#26410;&#35265;&#27979;&#35797;&#23545;&#35937;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#35774;&#35745;&#27491;&#21017;&#21270;&#24809;&#32602;&#21644;&#21457;&#25955;&#20272;&#35745;&#31639;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#24378;&#21046;&#25191;&#34892;&#20102;&#32479;&#35745;&#20851;&#31995;&#65292;&#24182;&#22312;&#22823;&#37327;&#35745;&#31639;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08762</link><description>&lt;p&gt;
EEG&#20998;&#31867;&#20013;&#37319;&#29992;&#21457;&#25955;&#20272;&#35745;&#31283;&#23450;&#20027;&#20307;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Subject Transfer in EEG Classification with Divergence Estimation. (arXiv:2310.08762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;EEG&#20998;&#31867;&#20013;&#20351;&#29992;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20943;&#23569;&#20102;&#22312;&#26410;&#35265;&#27979;&#35797;&#23545;&#35937;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#35774;&#35745;&#27491;&#21017;&#21270;&#24809;&#32602;&#21644;&#21457;&#25955;&#20272;&#35745;&#31639;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#24378;&#21046;&#25191;&#34892;&#20102;&#32479;&#35745;&#20851;&#31995;&#65292;&#24182;&#22312;&#22823;&#37327;&#35745;&#31639;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;encephalogram(EEG)&#25968;&#25454;&#30340;&#20998;&#31867;&#27169;&#22411;&#22312;&#26410;&#35265;&#27979;&#35797;&#23545;&#35937;&#19978;&#30340;&#34920;&#29616;&#22823;&#24133;&#19979;&#38477;&#12290;&#25105;&#20204;&#36890;&#36807;&#26032;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20943;&#23569;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#22270;&#27169;&#22411;&#26469;&#25551;&#36848;EEG&#20998;&#31867;&#20219;&#21153;&#12290;&#20174;&#27599;&#20010;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#29702;&#24819;&#30340;&#35757;&#32451;&#22330;&#26223;&#19979;&#65288;&#20855;&#26377;&#26080;&#38480;&#25968;&#25454;&#21644;&#20840;&#23616;&#26368;&#20248;&#27169;&#22411;&#65289;&#65292;&#24212;&#35813;&#25104;&#31435;&#20294;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#30340;&#32479;&#35745;&#20851;&#31995;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#27491;&#21017;&#21270;&#24809;&#32602;&#26469;&#24378;&#21046;&#25191;&#34892;&#36825;&#20123;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36866;&#29992;&#20316;&#20026;&#20195;&#29702;&#25968;&#37327;&#30340;&#21512;&#36866;&#30340;&#21457;&#25955;&#65288;&#22914;&#20114;&#20449;&#24687;&#21644;Wasserstein-1&#65289;&#65292;&#21487;&#20197;&#29992;&#26469;&#27979;&#37327;&#32479;&#35745;&#29420;&#31435;&#21644;&#30456;&#20851;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#20108;&#27425;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39640;&#25928;&#20272;&#35745;&#36825;&#20123;&#25968;&#37327;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#22522;&#20934;EEG&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#35745;&#31639;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification models for electroencephalogram (EEG) data show a large decrease in performance when evaluated on unseen test sub jects. We reduce this performance decrease using new regularization techniques during model training. We propose several graphical models to describe an EEG classification task. From each model, we identify statistical relationships that should hold true in an idealized training scenario (with infinite data and a globally-optimal model) but that may not hold in practice. We design regularization penalties to enforce these relationships in two stages. First, we identify suitable proxy quantities (divergences such as Mutual Information and Wasserstein-1) that can be used to measure statistical independence and dependence relationships. Second, we provide algorithms to efficiently estimate these quantities during training using secondary neural network models. We conduct extensive computational experiments using a large benchmark EEG dataset, comparing our propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#38382;&#39064;&#22238;&#31572;&#36827;&#34892;&#20102;&#33539;&#22260;&#22238;&#39038;&#12290;&#19982;&#20854;&#20182;&#21307;&#23398;QA&#20219;&#21153;&#19981;&#21516;&#65292;EHR QA&#36890;&#36807;&#20174;&#24739;&#32773;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33719;&#21462;&#31572;&#26696;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#29616;&#26377;&#30340;EHR QA&#20316;&#21697;&#25552;&#20379;&#20102;&#26041;&#27861;&#35770;&#22238;&#39038;&#12290;</title><link>http://arxiv.org/abs/2310.08759</link><description>&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#38382;&#39064;&#22238;&#31572;&#65306;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#30340;&#33539;&#22260;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Question Answering for Electronic Health Records: A Scoping Review of datasets and models. (arXiv:2310.08759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#38382;&#39064;&#22238;&#31572;&#36827;&#34892;&#20102;&#33539;&#22260;&#22238;&#39038;&#12290;&#19982;&#20854;&#20182;&#21307;&#23398;QA&#20219;&#21153;&#19981;&#21516;&#65292;EHR QA&#36890;&#36807;&#20174;&#24739;&#32773;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33719;&#21462;&#31572;&#26696;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#29616;&#26377;&#30340;EHR QA&#20316;&#21697;&#25552;&#20379;&#20102;&#26041;&#27861;&#35770;&#22238;&#39038;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#24739;&#32773;&#30456;&#20851;&#30340;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#21644;&#24739;&#32773;&#12290;&#23427;&#20204;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#20570;&#20915;&#31574;&#65292;&#24182;&#20351;&#24739;&#32773;&#26356;&#22909;&#22320;&#20102;&#35299;&#20182;&#20204;&#30340;&#30149;&#21382;&#12290;&#22823;&#37327;&#30340;&#24739;&#32773;&#25968;&#25454;&#23384;&#20648;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#65292;&#20351;&#24471;EHR QA&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;EHR QA&#20013;&#65292;&#31572;&#26696;&#26159;&#20174;&#24739;&#32773;&#30340;&#21307;&#30103;&#35760;&#24405;&#20013;&#33719;&#24471;&#30340;&#12290;&#30001;&#20110;&#25968;&#25454;&#26684;&#24335;&#21644;&#27169;&#24335;&#30340;&#24046;&#24322;&#65292;&#36825;&#19982;&#20854;&#20182;&#20351;&#29992;&#21307;&#23398;&#32593;&#31449;&#25110;&#31185;&#23398;&#35770;&#25991;&#26816;&#32034;&#31572;&#26696;&#30340;&#21307;&#23398;QA&#20219;&#21153;&#26377;&#24456;&#22823;&#30340;&#19981;&#21516;&#65292;&#36825;&#20351;&#24471;&#30740;&#31350;EHR&#38382;&#39064;&#22238;&#31572;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#29616;&#26377;&#20851;&#20110;EHR QA&#30340;&#20316;&#21697;&#36827;&#34892;&#26041;&#27861;&#35770;&#22238;&#39038;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;Google Scholar&#12289;ACL Anthology&#12289;ACM Digital Library&#21644;PubMed&#22312;&#20869;&#30340;&#22235;&#20010;&#25968;&#23383;&#36164;&#28304;&#20013;&#25628;&#32034;&#20102;&#20174;2005&#24180;1&#26376;1&#26085;&#21040;2023&#24180;9&#26376;30&#26085;&#30340;&#25991;&#31456;&#65292;&#20197;&#25910;&#38598;&#26377;&#20851;EHR QA&#30340;&#30456;&#20851;&#20986;&#29256;&#29289;&#12290;&#20849;&#21457;&#29616;&#20102;4111&#31687;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question Answering (QA) systems on patient-related data can assist both clinicians and patients. They can, for example, assist clinicians in decision-making and enable patients to have a better understanding of their medical history. Significant amounts of patient data are stored in Electronic Health Records (EHRs), making EHR QA an important research area. In EHR QA, the answer is obtained from the medical record of the patient. Because of the differences in data format and modality, this differs greatly from other medical QA tasks that employ medical websites or scientific papers to retrieve answers, making it critical to research EHR question answering. This study aimed to provide a methodological review of existing works on QA over EHRs. We searched for articles from January 1st, 2005 to September 30th, 2023 in four digital sources including Google Scholar, ACL Anthology, ACM Digital Library, and PubMed to collect relevant publications on EHR QA. 4111 papers were identified for our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22522;&#20110;&#38271;&#26399;&#32467;&#26500;&#21270;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#33258;&#21160;&#26816;&#27979;&#21644;&#39044;&#27979;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21033;&#29992;&#33521;&#22269;&#29983;&#29289;&#24211;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#29702;&#35786;&#26029;&#12289;&#22788;&#26041;&#21644;&#36807;&#31243;&#35760;&#24405;&#26469;&#36827;&#34892;&#39044;&#27979;&#21644;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.08757</link><description>&lt;p&gt;
&#20351;&#29992;&#38271;&#26399;&#32467;&#26500;&#21270;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26816;&#27979;&#21644;&#39044;&#27979;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Detection and prediction of clopidogrel treatment failures using longitudinal structured electronic health records. (arXiv:2310.08757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22522;&#20110;&#38271;&#26399;&#32467;&#26500;&#21270;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#33258;&#21160;&#26816;&#27979;&#21644;&#39044;&#27979;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21033;&#29992;&#33521;&#22269;&#29983;&#29289;&#24211;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#29702;&#35786;&#26029;&#12289;&#22788;&#26041;&#21644;&#36807;&#31243;&#35760;&#24405;&#26469;&#36827;&#34892;&#39044;&#27979;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#38271;&#26399;&#32467;&#26500;&#21270;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#33258;&#21160;&#26816;&#27979;&#21644;&#39044;&#27979;&#27695;&#21537;&#26684;&#38647;&#27835;&#30103;&#22833;&#36133;&#12290;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#19982;&#32467;&#26500;&#21270;EHR&#36827;&#34892;&#31867;&#27604;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#26500;&#24314;&#27835;&#30103;&#22833;&#36133;&#26816;&#27979;&#21644;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#20174;&#33521;&#22269;&#29983;&#29289;&#24211;&#65288;UK Biobank&#65289;&#20013;&#29983;&#25104;&#20102;&#19968;&#20010;&#26381;&#29992;&#27695;&#21537;&#26684;&#38647;&#22788;&#26041;&#30340;&#24739;&#32773;&#38431;&#21015;&#65292;&#24182;&#26631;&#27880;&#20102;&#36825;&#20123;&#24739;&#32773;&#22312;&#31532;&#19968;&#27425;&#27695;&#21537;&#26684;&#38647;&#22788;&#26041;&#21518;&#19968;&#24180;&#20869;&#26159;&#21542;&#21457;&#29983;&#20102;&#27835;&#30103;&#22833;&#36133;&#20107;&#20214;&#65307;&#22312;502,527&#21517;&#24739;&#32773;&#20013;&#65292;&#21457;&#29616;&#20102;1,824&#20363;&#27835;&#30103;&#22833;&#36133;&#30149;&#20363;&#21644;6,859&#20363;&#23545;&#29031;&#30149;&#20363;&#12290;&#25105;&#20204;&#25353;&#24739;&#32773;&#25972;&#29702;&#20102;&#35786;&#26029;&#12289;&#22788;&#26041;&#21644;&#36807;&#31243;&#35760;&#24405;&#65292;&#24182;&#25353;&#29031;&#30456;&#21516;&#26085;&#26399;&#32452;&#32455;&#25104;&#35775;&#38382;&#35760;&#24405;&#65292;&#20197;&#26500;&#24314;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20998;&#21035;&#29992;&#20110;&#26816;&#27979;&#21644;&#39044;&#27979;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose machine learning algorithms to automatically detect and predict clopidogrel treatment failure using longitudinal structured electronic health records (EHR). By drawing analogies between natural language and structured EHR, we introduce various machine learning algorithms used in natural language processing (NLP) applications to build models for treatment failure detection and prediction. In this regard, we generated a cohort of patients with clopidogrel prescriptions from UK Biobank and annotated if the patients had treatment failure events within one year of the first clopidogrel prescription; out of 502,527 patients, 1,824 patients were identified as treatment failure cases, and 6,859 patients were considered as control cases. From the dataset, we gathered diagnoses, prescriptions, and procedure records together per patient and organized them into visits with the same date to build models. The models were built for two different tasks, i.e., detection and prediction, and t
&lt;/p&gt;</description></item><item><title>&#22312;LLM&#35757;&#32451;&#20013;&#65292;&#20998;&#35789;&#22120;&#30340;&#36873;&#25321;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#25104;&#26412;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#24120;&#35265;&#30340;&#20998;&#35789;&#22120;&#35780;&#20272;&#25351;&#26631;&#19981;&#19968;&#23450;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08754</link><description>&lt;p&gt;
LLM&#35757;&#32451;&#20013;&#30340;&#20998;&#35789;&#36873;&#25321;&#65306;&#24494;&#19981;&#36275;&#36947;&#36824;&#26159;&#33267;&#20851;&#37325;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Tokenizer Choice For LLM Training: Negligible or Crucial?. (arXiv:2310.08754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08754
&lt;/p&gt;
&lt;p&gt;
&#22312;LLM&#35757;&#32451;&#20013;&#65292;&#20998;&#35789;&#22120;&#30340;&#36873;&#25321;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#25104;&#26412;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#65292;&#24120;&#35265;&#30340;&#20998;&#35789;&#22120;&#35780;&#20272;&#25351;&#26631;&#19981;&#19968;&#23450;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;LLM&#30340;&#25104;&#21151;&#20027;&#35201;&#26159;&#30001;&#20110;&#31574;&#21010;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#25193;&#23637;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#36827;&#27493;&#65292;&#32780;&#20998;&#35789;&#22120;&#30340;&#24433;&#21709;&#21017;&#26159;&#19968;&#20010;&#30450;&#28857;&#12290;&#36890;&#36807;&#23545;24&#20010;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23545;&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#31639;&#27861;&#21644;&#21442;&#25968;&#36827;&#34892;&#22823;&#33539;&#22260;&#23454;&#39564;&#65292;&#25105;&#20204;&#23545;&#20998;&#35789;&#22120;&#36873;&#25321;&#23545;LLM&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20998;&#35789;&#22120;&#36873;&#25321;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#26377;&#30528;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24120;&#35265;&#30340;&#20998;&#35789;&#22120;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;&#20016;&#23500;&#24230;&#21644;&#24179;&#31561;&#24615;&#65289;&#24182;&#19981;&#24635;&#26159;&#23545;&#27169;&#22411;&#30340;&#21518;&#32493;&#24615;&#33021;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#25351;&#26631;&#25104;&#20026;&#23545;&#20998;&#35789;&#22120;&#35780;&#20272;&#30340;&#21487;&#30097;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#38024;&#23545;&#20116;&#31181;&#26368;&#24120;&#35265;&#30340;&#27431;&#27954;&#35821;&#35328;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#38656;&#35201;&#35789;&#27719;&#34920;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of LLMs has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance, training and inference costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable choice for tokenizer evaluation. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#36866;&#24212;&#20027;&#21160;&#23398;&#20064;&#26410;&#30693;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#32422;&#26463;&#22330;&#26223;&#26102;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.08751</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#36866;&#24212;&#20027;&#21160;&#23398;&#20064;&#26410;&#30693;&#32422;&#26463;&#30340;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Constrained Bayesian Optimization with Adaptive Active Learning of Unknown Constraints. (arXiv:2310.08751v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#33258;&#36866;&#24212;&#20027;&#21160;&#23398;&#20064;&#26410;&#30693;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#32422;&#26463;&#22330;&#26223;&#26102;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22914;&#31185;&#23398;&#23454;&#39564;&#35774;&#35745;&#12289;&#21307;&#23398;&#27835;&#30103;&#35774;&#35745;&#21644;&#24037;&#19994;&#36807;&#31243;&#20248;&#21270;&#31561;&#39046;&#22495;&#65292;&#20248;&#21270;&#30446;&#26631;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#24773;&#20917;&#26159;&#24120;&#35265;&#30340;&#65292;&#20854;&#20013;&#30446;&#26631;&#21644;&#32422;&#26463;&#37117;&#26159;&#40657;&#30418;&#20989;&#25968;&#12290;&#22788;&#29702;&#36825;&#20123;&#22797;&#26434;&#22330;&#26223;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#12290;&#22312;&#29702;&#35770;&#34892;&#20026;&#26041;&#38754;&#65292;BO&#22312;&#26080;&#32422;&#26463;&#35774;&#32622;&#19979;&#30456;&#23545;&#36739;&#20026;&#29702;&#35299;&#65292;&#20854;&#21407;&#21017;&#24050;&#32463;&#34987;&#24191;&#27867;&#25506;&#32034;&#21644;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CBO&#65289;&#26469;&#35828;&#65292;&#29616;&#26377;&#26694;&#26550;&#24448;&#24448;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#36817;&#20284;&#26041;&#27861;&#65292;&#27809;&#26377;&#21516;&#26679;&#31243;&#24230;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#32422;&#26463;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#29702;&#35770;&#21644;&#23454;&#38469;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#21644;&#32422;&#26463;&#21487;&#20197;&#29420;&#31435;&#35780;&#20272;&#19988;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#35748;&#35782;&#21040;&#30446;&#26631;&#21644;&#32422;&#26463;&#37117;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#39640;&#32622;&#20449;&#24230;&#30340;&#20852;&#36259;&#21306;&#22495;&#65292;
&lt;/p&gt;
&lt;p&gt;
Optimizing objectives under constraints, where both the objectives and constraints are black box functions, is a common scenario in real-world applications such as scientific experimental design, design of medical therapies, and industrial process optimization. One popular approach to handling these complex scenarios is Bayesian Optimization (BO). In terms of theoretical behavior, BO is relatively well understood in the unconstrained setting, where its principles have been well explored and validated. However, when it comes to constrained Bayesian optimization (CBO), the existing framework often relies on heuristics or approximations without the same level of theoretical guarantees.  In this paper, we delve into the theoretical and practical aspects of constrained Bayesian optimization, where the objective and constraints can be independently evaluated and are subject to noise. By recognizing that both the objective and constraints can help identify high-confidence regions of interest 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Search-Adaptor&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#21046;&#21270;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#21644;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20462;&#25913;&#25991;&#26412;&#23884;&#20837;&#65292;Search-Adaptor&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.08750</link><description>&lt;p&gt;
Search-Adaptor: &#29992;&#20110;&#20449;&#24687;&#26816;&#32034;&#30340;&#25991;&#26412;&#23884;&#20837;&#20010;&#24615;&#21270;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Search-Adaptor: Text Embedding Customization for Information Retrieval. (arXiv:2310.08750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Search-Adaptor&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#21046;&#21270;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#21644;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#20462;&#25913;&#25991;&#26412;&#23884;&#20837;&#65292;Search-Adaptor&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#31283;&#23450;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30340;&#25991;&#26412;&#23884;&#20837;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#21644;&#25628;&#32034;&#30340;&#28508;&#21147;&#12290;&#38500;&#20102;&#19968;&#30452;&#20197;&#26469;&#24120;&#35268;&#20351;&#29992;&#30340;&#38646;&#26679;&#26412;&#35774;&#32622;&#22806;&#65292;&#21033;&#29992;&#30456;&#20851;&#26597;&#35810;-&#35821;&#26009;&#24211;&#37197;&#23545;&#25968;&#25454;&#30340;&#20449;&#24687;&#33021;&#21147;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;Search-Adaptor&#65292;&#20197;&#20415;&#20197;&#39640;&#25928;&#19988;&#31283;&#20581;&#30340;&#26041;&#24335;&#23450;&#21046;&#21270;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#12290;Search-Adaptor&#21487;&#20197;&#20462;&#25913;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#21407;&#22987;&#25991;&#26412;&#23884;&#20837;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#21253;&#25324;&#21482;&#33021;&#36890;&#36807;API&#35775;&#38382;&#30340;&#27169;&#22411;&#12290;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#33521;&#25991;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Search-Adaptor&#30340;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;--&#20363;&#22914;&#65292;&#22312;13&#20010;BEIR&#25968;&#25454;&#38598;&#19978;&#65292;nDCG@10&#30456;&#23545;&#20110;Google Embedding APIs&#24179;&#22343;&#25552;&#39640;&#20102;5.2%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embeddings extracted by pre-trained Large Language Models (LLMs) have significant potential to improve information retrieval and search. Beyond the zero-shot setup in which they are being conventionally used, being able to take advantage of the information from the relevant query-corpus paired data has the power to further boost the LLM capabilities. In this paper, we propose a novel method, Search-Adaptor, for customizing LLMs for information retrieval in an efficient and robust way. Search-Adaptor modifies the original text embedding generated by pre-trained LLMs, and can be integrated with any LLM, including those only available via APIs. On multiple real-world English and multilingual retrieval datasets, we show consistent and significant performance benefits for Search-Adaptor -- e.g., more than 5.2% improvements over the Google Embedding APIs in nDCG@10 averaged over 13 BEIR datasets.
&lt;/p&gt;</description></item><item><title>&#36827;&#21270;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#32467;&#21512;&#20026;&#20248;&#21270;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26426;&#20250;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#25552;&#20379;&#23545;&#25628;&#32034;&#31354;&#38388;&#21644;&#31181;&#32676;&#21160;&#24577;&#30340;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.08748</link><description>&lt;p&gt;
&#36827;&#21270;&#21160;&#24577;&#20248;&#21270;&#19982;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Dynamic Optimization and Machine Learning. (arXiv:2310.08748v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08748
&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#32467;&#21512;&#20026;&#20248;&#21270;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26426;&#20250;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#36827;&#21270;&#35745;&#31639;&#31639;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#25552;&#20379;&#23545;&#25628;&#32034;&#31354;&#38388;&#21644;&#31181;&#32676;&#21160;&#24577;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;(EC)&#20316;&#20026;&#19968;&#31181;&#21463;&#33258;&#28982;&#28176;&#36827;&#21457;&#23637;&#26426;&#21046;&#21551;&#21457;&#30340;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;EC&#26041;&#27861;&#24120;&#24120;&#38754;&#20020;&#20572;&#28382;&#12289;&#22810;&#26679;&#24615;&#25439;&#22833;&#12289;&#35745;&#31639;&#22797;&#26434;&#24615;&#12289;&#31181;&#32676;&#21021;&#22987;&#21270;&#21644;&#36807;&#26089;&#25910;&#25947;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#30740;&#31350;&#20154;&#21592;&#23558;&#23398;&#20064;&#31639;&#27861;&#19982;&#36827;&#21270;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#38598;&#25104;&#21033;&#29992;&#20102;EC&#31639;&#27861;&#22312;&#36845;&#20195;&#25628;&#32034;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#23545;&#25628;&#32034;&#31354;&#38388;&#21644;&#31181;&#32676;&#21160;&#24577;&#30340;&#27934;&#23519;&#12290;&#31867;&#20284;&#22320;&#65292;&#36827;&#21270;&#31639;&#27861;&#19982;&#26426;&#22120;&#23398;&#20064;(ML)&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#30456;&#20114;&#30340;&#65292;&#22240;&#20026;EC&#26041;&#27861;&#20026;&#20248;&#21270;&#22122;&#22768;&#12289;&#19981;&#20934;&#30830;&#21644;&#21160;&#24577;&#30446;&#26631;&#20989;&#25968;&#25152;&#25551;&#36848;&#30340;&#22797;&#26434;ML&#20219;&#21153;&#25552;&#20379;&#20102;&#26497;&#22909;&#30340;&#26426;&#20250;&#12290;&#36825;&#20123;&#28151;&#21512;&#25216;&#26415;&#34987;&#31216;&#20026;&#36827;&#21270;&#26426;&#22120;&#23398;&#20064;(EML)&#65292;&#24050;&#22312;ML&#36807;&#31243;&#30340;&#21508;&#20010;&#38454;&#27573;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Computation (EC) has emerged as a powerful field of Artificial Intelligence, inspired by nature's mechanisms of gradual development. However, EC approaches often face challenges such as stagnation, diversity loss, computational complexity, population initialization, and premature convergence. To overcome these limitations, researchers have integrated learning algorithms with evolutionary techniques. This integration harnesses the valuable data generated by EC algorithms during iterative searches, providing insights into the search space and population dynamics. Similarly, the relationship between evolutionary algorithms and Machine Learning (ML) is reciprocal, as EC methods offer exceptional opportunities for optimizing complex ML tasks characterized by noisy, inaccurate, and dynamic objective functions. These hybrid techniques, known as Evolutionary Machine Learning (EML), have been applied at various stages of the ML process. EC techniques play a vital role in tasks such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#20013;&#25552;&#39640;MARL&#31283;&#20581;&#24615;&#30340;&#24191;&#20041;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#25216;&#26415;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.08746</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#20013;&#65292;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#25552;&#39640;MARL&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness to Multi-Modal Environment Uncertainty in MARL using Curriculum Learning. (arXiv:2310.08746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#20013;&#25552;&#39640;MARL&#31283;&#20581;&#24615;&#30340;&#24191;&#20041;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#35838;&#31243;&#23398;&#20064;&#25216;&#26415;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#25361;&#25112;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#20174;&#20223;&#30495;&#29615;&#22659;&#26080;&#32541;&#36807;&#28193;&#21040;&#29616;&#23454;&#19990;&#30028;&#38656;&#35201;&#23427;&#23545;&#21508;&#31181;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#22312;&#21333;&#20010;&#29615;&#22659;&#21464;&#37327;&#65288;&#21363;&#34892;&#21160;&#12289;&#29366;&#24577;&#25110;&#22870;&#21169;&#65289;&#30340;&#19981;&#30830;&#23450;&#24615;&#19979;&#25214;&#21040;&#32435;&#20160;&#22343;&#34913;&#25110;&#26368;&#20248;&#31574;&#30053;&#12290;&#36825;&#26159;&#22240;&#20026;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#26412;&#36523;&#38750;&#24120;&#22797;&#26434;&#21644;&#38750;&#24179;&#31283;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#21516;&#26102;&#21457;&#29983;&#22312;&#22810;&#20010;&#29615;&#22659;&#21464;&#37327;&#20013;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#22312;MARL&#20013;&#38024;&#23545;&#22810;&#27169;&#24577;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#30340;&#24191;&#20041;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#25216;&#26415;&#30340;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#30340;&#36890;&#29992;&#31283;&#20581;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#21516;&#26102;&#22788;&#29702;&#20004;&#31181;&#19981;&#21516;&#30340;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#21512;&#20316;&#21644;&#31454;&#20105;&#30340;MARL&#29615;&#22659;&#20013;&#21576;&#29616;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) plays a pivotal role in tackling real-world challenges. However, the seamless transition of trained policies from simulations to real-world requires it to be robust to various environmental uncertainties. Existing works focus on finding Nash Equilibrium or the optimal policy under uncertainty in one environment variable (i.e. action, state or reward). This is because a multi-agent system itself is highly complex and unstationary. However, in real-world situation uncertainty can occur in multiple environment variables simultaneously. This work is the first to formulate the generalised problem of robustness to multi-modal environment uncertainty in MARL. To this end, we propose a general robust training approach for multi-modal uncertainty based on curriculum learning techniques. We handle two distinct environmental uncertainty simultaneously and present extensive results across both cooperative and competitive MARL environments, demonstrating th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#30005;&#36335;&#32452;&#20214;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#22797;&#29992;&#24182;&#20135;&#29983;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20026;&#26356;&#39640;&#32423;&#30340;&#27169;&#22411;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.08744</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#20219;&#21153;&#30340;&#30005;&#36335;&#32452;&#20214;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
Circuit Component Reuse Across Tasks in Transformer Language Models. (arXiv:2310.08744v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08744
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#30005;&#36335;&#32452;&#20214;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#22797;&#29992;&#24182;&#20135;&#29983;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20026;&#26356;&#39640;&#32423;&#30340;&#27169;&#22411;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#30005;&#36335;&#20998;&#26512;&#21487;&#20197;&#25104;&#21151;&#22320;&#36870;&#21521;&#24037;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25209;&#35780;&#26159;&#27599;&#20010;&#30005;&#36335;&#37117;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#20998;&#26512;&#19981;&#33021;&#20026;&#26356;&#39640;&#32423;&#30340;&#29702;&#35299;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#35777;&#25454;&#34920;&#26126;&#27934;&#23519;&#21147;&#65288;&#20851;&#20110;&#29305;&#23450;&#22836;&#37096;&#30340;&#20302;&#32423;&#21457;&#29616;&#21644;&#20851;&#20110;&#19968;&#33324;&#31639;&#27861;&#30340;&#39640;&#32423;&#21457;&#29616;&#65289;&#30830;&#23454;&#21487;&#20197;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Wang&#31561;&#20154;&#65288;2022&#65289;&#22312;&#38388;&#25509;&#23486;&#35821;&#35782;&#21035;&#20219;&#21153;&#65288;IOI&#65289;&#20013;&#21457;&#29616;&#30340;&#30005;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20010;&#30005;&#36335;&#22312;&#26356;&#22823;&#30340;GPT2&#27169;&#22411;&#19978;&#30340;&#37325;&#29616;&#65292;&#20197;&#21450;&#22312;&#30475;&#20284;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#22823;&#37096;&#20998;&#34987;&#22797;&#29992;&#26469;&#35299;&#20915;&#38382;&#39064;&#65306;&#24425;&#33394;&#29289;&#20307;&#65288;Ippolito&#21644;Callison-Burch&#65292;2023&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#20004;&#20010;&#20219;&#21153;&#24213;&#23618;&#30340;&#36807;&#31243;&#22312;&#21151;&#33021;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#30005;&#36335;&#20013;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#20043;&#38388;&#26377;&#22823;&#32422;78&#65285;&#30340;&#37325;&#21472;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#24178;&#39044;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito &amp; Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#39564;&#35777;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#21355;&#26143;&#19981;&#31283;&#23450;&#24615;&#39044;&#27979;&#22120;&#65292;&#21487;&#20197;&#20174;&#21069;&#21015;&#33146;&#30284;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;MSI&#29366;&#24577;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#20110;&#20813;&#30123;&#27835;&#30103;&#30340;&#24739;&#32773;&#12290;</title><link>http://arxiv.org/abs/2310.08743</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#21355;&#26143;&#19981;&#31283;&#23450;&#24615;&#39044;&#27979;&#22120;&#30340;&#24320;&#21457;&#21644;&#39564;&#35777;&#65292;&#26469;&#33258;&#20110;&#21069;&#21015;&#33146;&#30284;&#20840;&#20999;&#29255;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Development and Validation of a Deep Learning-Based Microsatellite Instability Predictor from Prostate Cancer Whole-Slide Images. (arXiv:2310.08743v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#21644;&#39564;&#35777;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#21355;&#26143;&#19981;&#31283;&#23450;&#24615;&#39044;&#27979;&#22120;&#65292;&#21487;&#20197;&#20174;&#21069;&#21015;&#33146;&#30284;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;MSI&#29366;&#24577;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#20110;&#20813;&#30123;&#27835;&#30103;&#30340;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#21355;&#26143;&#19981;&#31283;&#23450;&#24615;&#39640;&#65288;MSI-H&#65289;&#26159;&#20813;&#30123;&#26816;&#26597;&#28857;&#25233;&#21046;&#21058;&#30103;&#27861;&#30340;&#19968;&#31181;&#32959;&#30244;&#19981;&#21487;&#30693;&#26631;&#24535;&#29289;&#12290;&#28982;&#32780;&#65292;&#22312;&#21069;&#21015;&#33146;&#30284;&#20013;&#65292;&#30001;&#20110;&#20302;&#24739;&#30149;&#29575;&#21644;&#26816;&#27979;&#25104;&#26412;&#36739;&#39640;&#65292;MSI&#29366;&#24577;&#36890;&#24120;&#19981;&#36827;&#34892;&#24120;&#35268;&#27979;&#35797;&#12290;&#22240;&#27492;&#65292;&#20174;&#34880;&#28082;&#26579;&#33394;&#21058;&#38747;&#34013;&#21644;&#20234;&#32418;&#65288;H&amp;E&#65289;&#26579;&#33394;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#20013;&#39044;&#27979;MSI&#29366;&#24577;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#20110;&#30830;&#35748;&#27979;&#35797;&#24182;&#26377;&#36164;&#26684;&#25509;&#21463;&#20813;&#30123;&#27835;&#30103;&#30340;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#12290;&#20998;&#26512;&#20102;&#36830;&#32493;&#36716;&#35786;&#33267;&#25105;&#20204;&#26426;&#26500;&#30340;&#21069;&#21015;&#33146;&#30284;&#24739;&#32773;&#30340;&#21435;&#21311;&#21517;&#35760;&#24405;&#20013;&#30340;&#21069;&#21015;&#33146;&#27963;&#26816;&#21644;&#25163;&#26415;&#20999;&#38500;&#26631;&#26412;&#12290;&#36890;&#36807;&#19979;&#19968;&#20195;&#27979;&#24207;&#30830;&#23450;&#20102;&#20182;&#20204;&#30340;MSI&#29366;&#24577;&#12290;&#22312;&#25130;&#27490;&#26085;&#26399;&#20043;&#21069;&#30340;&#24739;&#32773;&#20998;&#20026;&#31639;&#27861;&#24320;&#21457;&#38598;&#65288;n=4015, MSI-H 1.8%&#65289;&#21644;&#37197;&#23545;&#39564;&#35777;&#38598;&#65288;n=173, MSI-H 19.7%&#65289;&#65292;&#27599;&#20010;&#26679;&#26412;&#30001;&#20004;&#20010;&#36830;&#32493;&#30340;&#20999;&#29255;&#32452;&#25104;&#65292;&#19968;&#20010;&#20869;&#37096;&#26579;&#33394;&#21644;&#25195;&#25551;&#65292;&#21478;&#19968;&#20010;&#22312;&#22806;&#37096;&#26576;&#20010;&#22320;&#28857;&#12290;&#25130;&#27490;&#26085;&#26399;&#20043;&#21518;&#30340;&#24739;&#32773;&#24418;&#25104;&#20102;&#26102;&#38388;&#39564;&#35777;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Microsatellite instability-high (MSI-H) is a tumor agnostic biomarker for immune checkpoint inhibitor therapy. However, MSI status is not routinely tested in prostate cancer, in part due to low prevalence and assay cost. As such, prediction of MSI status from hematoxylin and eosin (H&amp;E) stained whole-slide images (WSIs) could identify prostate cancer patients most likely to benefit from confirmatory testing and becoming eligible for immunotherapy. Prostate biopsies and surgical resections from de-identified records of consecutive prostate cancer patients referred to our institution were analyzed. Their MSI status was determined by next generation sequencing. Patients before a cutoff date were split into an algorithm development set (n=4015, MSI-H 1.8%) and a paired validation set (n=173, MSI-H 19.7%) that consisted of two serial sections from each sample, one stained and scanned internally and the other at an external site. Patients after the cutoff date formed the temporal validation 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#25193;&#23637;&#21040;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#21033;&#29992;&#36873;&#25321;&#24615;&#21098;&#25509;&#21644;&#22522;&#22240;&#22797;&#21046;&#20135;&#29983;&#30340;&#24207;&#21015;&#20043;&#38388;&#30340;&#21151;&#33021;&#30456;&#20284;&#24615;&#65292;&#23398;&#20064;&#21040;&#24191;&#20041;RNA&#21516;&#20301;&#32032;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;RNA&#21322;&#34928;&#26399;&#21644;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#39044;&#27979;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#22312;&#20302;&#25968;&#25454;&#26465;&#20214;&#19979;&#30382;&#23572;&#36874;&#30456;&#20851;&#24615;&#22686;&#21152;&#20102;&#22810;&#36798;&#20004;&#20493;&#12290;</title><link>http://arxiv.org/abs/2310.08738</link><description>&lt;p&gt;
&#29992;RNA&#23545;&#27604;&#23398;&#20064;&#25913;&#36827;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Splicing Up Your Predictions with RNA Contrastive Learning. (arXiv:2310.08738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#25193;&#23637;&#21040;&#22522;&#22240;&#32452;&#25968;&#25454;&#65292;&#21033;&#29992;&#36873;&#25321;&#24615;&#21098;&#25509;&#21644;&#22522;&#22240;&#22797;&#21046;&#20135;&#29983;&#30340;&#24207;&#21015;&#20043;&#38388;&#30340;&#21151;&#33021;&#30456;&#20284;&#24615;&#65292;&#23398;&#20064;&#21040;&#24191;&#20041;RNA&#21516;&#20301;&#32032;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;RNA&#21322;&#34928;&#26399;&#21644;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#39044;&#27979;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#22312;&#20302;&#25968;&#25454;&#26465;&#20214;&#19979;&#30382;&#23572;&#36874;&#30456;&#20851;&#24615;&#22686;&#21152;&#20102;&#22810;&#36798;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22522;&#22240;&#32452;&#25968;&#25454;&#36805;&#36895;&#31215;&#32047;&#65292;&#25105;&#20204;&#23545;RNA&#35843;&#25511;&#20195;&#30721;&#30340;&#29702;&#35299;&#23578;&#19981;&#23436;&#25972;&#12290;&#36817;&#26399;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#20102;&#23398;&#20064;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#35268;&#21017;&#65288;&#20363;&#22914;&#35821;&#35328;&#20013;&#30340;&#21477;&#23376;&#32467;&#26500;&#65289;&#30340;&#33021;&#21147;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#36873;&#25321;&#24615;&#21098;&#25509;&#21644;&#22522;&#22240;&#22797;&#21046;&#20135;&#29983;&#30340;&#24207;&#21015;&#20043;&#38388;&#30340;&#21151;&#33021;&#30456;&#20284;&#24615;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#25193;&#23637;&#21040;&#22522;&#22240;&#32452;&#25968;&#25454;&#19978;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#21644;&#23545;&#27604;&#30446;&#26631;&#20351;&#24471;&#23398;&#20064;&#21040;&#24191;&#20041;RNA&#21516;&#20301;&#32032;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;RNA&#21322;&#34928;&#26399;&#21644;&#24179;&#22343;&#26680;&#31958;&#20307;&#36127;&#36733;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#22312;&#20004;&#39033;&#20219;&#21153;&#30340;&#32447;&#24615;&#25506;&#32034;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#20302;&#25968;&#25454;&#26465;&#20214;&#19979;&#30382;&#23572;&#36874;&#30456;&#20851;&#24615;&#22686;&#21152;&#20102;&#22810;&#36798;&#20004;&#20493;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23545;&#23398;&#20064;&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#25506;&#32034;&#25581;&#31034;&#20102;&#25105;&#20204;&#23545;&#27604;&#30446;&#26631;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the face of rapidly accumulating genomic data, our understanding of the RNA regulatory code remains incomplete. Recent self-supervised methods in other domains have demonstrated the ability to learn rules underlying the data-generating process such as sentence structure in language. Inspired by this, we extend contrastive learning techniques to genomic data by utilizing functional similarities between sequences generated through alternative splicing and gene duplication. Our novel dataset and contrastive objective enable the learning of generalized RNA isoform representations. We validate their utility on downstream tasks such as RNA half-life and mean ribosome load prediction. Our pre-training strategy yields competitive results using linear probing on both tasks, along with up to a two-fold increase in Pearson correlation in low-data conditions. Importantly, our exploration of the learned latent space reveals that our contrastive objective yields semantically meaningful representa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26694;&#26550;&#65292;&#20026;&#25104;&#26412;&#25935;&#24863;&#30340;&#31283;&#20581;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26041;&#26696;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#23376;&#32452;&#35774;&#35745;&#20102;&#32454;&#31890;&#24230;&#35748;&#35777;&#21322;&#24452;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08732</link><description>&lt;p&gt;
&#21487;&#35777;&#20445;&#20581;&#24247;&#21830;&#21153;&#23383;&#20307;&#23609;&#21253;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;(&#35793;&#27880;)&#27700;&#12290;
&lt;/p&gt;
&lt;p&gt;
Provably Robust Cost-Sensitive Learning via Randomized Smoothing. (arXiv:2310.08732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26694;&#26550;&#65292;&#20026;&#25104;&#26412;&#25935;&#24863;&#30340;&#31283;&#20581;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26041;&#26696;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#23376;&#32452;&#35774;&#35745;&#20102;&#32454;&#31890;&#24230;&#35748;&#35777;&#21322;&#24452;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#20110;&#22312;&#25104;&#26412;&#25935;&#24863;&#30340;&#24773;&#26223;&#19979;&#23398;&#20064;&#23545;&#25239;&#24615;&#31283;&#20581;&#20998;&#31867;&#22120;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#21464;&#25442;&#30340;&#28508;&#22312;&#21361;&#23475;&#34987;&#32534;&#30721;&#22312;&#19968;&#20010;&#20108;&#36827;&#21046;&#25104;&#26412;&#30697;&#38453;&#20013;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#26080;&#27861;&#35777;&#26126;&#31283;&#20581;&#24615;&#65292;&#35201;&#20040;&#23384;&#22312;&#22266;&#26377;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#24179;&#28369;&#65292;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#31283;&#20581;&#24615;&#35748;&#35777;&#26694;&#26550;&#65292;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35777;&#26126;&#25104;&#26412;&#25935;&#24863;&#30340;&#31283;&#20581;&#24615;&#12290;&#24314;&#31435;&#22312;&#19968;&#31181;&#25104;&#26412;&#25935;&#24863;&#35748;&#35777;&#21322;&#24452;&#30340;&#27010;&#24565;&#20043;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35843;&#25972;&#26631;&#20934;&#30340;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#27969;&#31243;&#65292;&#20026;&#20219;&#20309;&#25104;&#26412;&#30697;&#38453;&#20135;&#29983;&#20005;&#26684;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#23376;&#32452;&#35774;&#35745;&#30340;&#32454;&#31890;&#24230;&#35748;&#35777;&#21322;&#24452;&#20248;&#21270;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#38024;&#23545;&#25104;&#26412;&#25935;&#24863;&#31283;&#20581;&#24615;&#20248;&#21270;&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#22312;&#22270;&#20687;&#22522;&#20934;&#27979;&#35797;&#21644;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on learning adversarially robust classifiers under a cost-sensitive scenario, where the potential harm of different classwise adversarial transformations is encoded in a binary cost matrix. Existing methods are either empirical that cannot certify robustness or suffer from inherent scalability issues. In this work, we study whether randomized smoothing, a more scalable robustness certification framework, can be leveraged to certify cost-sensitive robustness. Built upon a notion of cost-sensitive certified radius, we show how to adapt the standard randomized smoothing certification pipeline to produce tight robustness guarantees for any cost matrix. In addition, with fine-grained certified radius optimization schemes specifically designed for different data subgroups, we propose an algorithm to train smoothed classifiers that are optimized for cost-sensitive robustness. Extensive experiments on image benchmarks and a real-world medical dataset demonstrate the superiority of our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#65292;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#12290;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.08731</link><description>&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#23454;&#29616;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Simple Way to Incorporate Novelty Detection in World Models. (arXiv:2310.08731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#65292;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#12290;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#19990;&#30028;&#26426;&#21046;&#25110;&#23646;&#24615;&#21457;&#29983;&#31361;&#28982;&#21464;&#21270;&#26102;&#65292;&#20195;&#29702;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#21487;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#35270;&#35273;&#23646;&#24615;&#25110;&#29366;&#24577;&#36716;&#25442;&#30340;&#31361;&#21464;&#31216;&#20026;&#8220;&#26032;&#39062;&#24615;&#8221;&#12290;&#22312;&#29983;&#25104;&#30340;&#19990;&#30028;&#27169;&#22411;&#26694;&#26550;&#20013;&#23454;&#26045;&#26032;&#39062;&#24615;&#26816;&#27979;&#26159;&#20445;&#25252;&#37096;&#32626;&#26102;&#20195;&#29702;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19982;&#24207;&#21015;&#20915;&#31574;&#30456;&#20851;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26412;&#20307;&#35770;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#20195;&#29702;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#36716;&#25442;&#20998;&#24067;&#20013;&#26816;&#27979;&#26032;&#39062;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) using world models has found significant recent successes. However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline. We refer to the sudden change in visual properties or state transitions as {\em novelties}. Implementing novelty detection within generated world model frameworks is a crucial task for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents, by utilizing the misalignment of the world model's hallucinated states and the true observed states as an anomaly score. We first provide an ontology of novelty detection relevant to sequential decision making, then we provide effective approaches to detecting novelties in a distribution of transitions learned by an agent in a world model. Finally, we show the advantage of our work in a novel environment compared to traditional ma
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#36136;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#22270;&#30340;&#24322;&#36136;&#24615;&#20197;&#21450;&#31867;&#21035;&#19981;&#24179;&#34913;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Fast Im-GBK&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#19981;&#24179;&#34913;&#22270;&#19978;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08725</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#36136;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#19981;&#24179;&#34913;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Heterophily-Based Graph Neural Network for Imbalanced Classification. (arXiv:2310.08725v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08725
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#36136;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#32771;&#34385;&#22270;&#30340;&#24322;&#36136;&#24615;&#20197;&#21450;&#31867;&#21035;&#19981;&#24179;&#34913;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Fast Im-GBK&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#19981;&#24179;&#34913;&#22270;&#19978;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#22312;&#35299;&#20915;&#22270;&#30456;&#20851;&#38382;&#39064;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;GNN&#20551;&#35774;&#25968;&#25454;&#22312;&#21508;&#20010;&#31867;&#21035;&#20043;&#38388;&#22343;&#21248;&#20998;&#24067;&#65292;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#26576;&#20123;&#31867;&#21035;&#24448;&#24448;&#20005;&#37325;&#19981;&#24179;&#34913;&#12290;&#36825;&#23548;&#33268;&#26631;&#20934;GNN&#22312;&#19981;&#24179;&#34913;&#22270;&#19978;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#36890;&#36807;&#32771;&#34385;&#22270;&#30340;&#24322;&#36136;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#22270;&#19978;&#30340;&#19981;&#24179;&#34913;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#19982;&#22270;&#30340;&#24322;&#36136;&#24615;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#23569;&#25968;&#31867;&#19981;&#20165;&#34920;&#29616;&#20986;&#26679;&#26412;&#31232;&#32570;&#65292;&#36824;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#21516;&#36136;&#24615;&#27700;&#24179;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#38169;&#35823;&#20449;&#24687;&#22312;&#30456;&#37051;&#33410;&#28857;&#20043;&#38388;&#30340;&#20256;&#25773;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Fast Im-GBK&#65292;&#23427;&#23558;&#19981;&#24179;&#34913;&#20998;&#31867;&#31574;&#30053;&#19982;&#24322;&#36136;&#24615;&#24863;&#30693;&#30340;GNN&#26377;&#25928;&#22320;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have shown promise in addressing graph-related problems, including node classification. However, conventional GNNs assume an even distribution of data across classes, which is often not the case in real-world scenarios, where certain classes are severely underrepresented. This leads to suboptimal performance of standard GNNs on imbalanced graphs. In this paper, we introduce a unique approach that tackles imbalanced classification on graphs by considering graph heterophily. We investigate the intricate relationship between class imbalance and graph heterophily, revealing that minority classes not only exhibit a scarcity of samples but also manifest lower levels of homophily, facilitating the propagation of erroneous information among neighboring nodes. Drawing upon this insight, we propose an efficient method, called Fast Im-GBK, which integrates an imbalance classification strategy with heterophily-aware GNNs to effectively address the class imbalance probl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#21487;&#35266;&#27979;&#37327;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#23637;&#24320;&#30340;&#24494;&#20998;&#25130;&#38754;&#21253;&#21547;&#20102;&#20851;&#20110;&#24863;&#20852;&#36259;&#21442;&#25968;&#30340;&#26368;&#22810;&#20449;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26500;&#36896;&#26041;&#27861;&#36827;&#34892;&#24456;&#22909;&#30340;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.08717</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20026;&#27979;&#37327;&#35774;&#35745;&#21487;&#35266;&#27979;&#37327;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Designing Observables for Measurements with Deep Learning. (arXiv:2310.08717v1 [physics.data-an])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08717
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#21487;&#35266;&#27979;&#37327;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#23637;&#24320;&#30340;&#24494;&#20998;&#25130;&#38754;&#21253;&#21547;&#20102;&#20851;&#20110;&#24863;&#20852;&#36259;&#21442;&#25968;&#30340;&#26368;&#22810;&#20449;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26500;&#36896;&#26041;&#27861;&#36827;&#34892;&#24456;&#22909;&#30340;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31890;&#23376;&#29289;&#29702;&#21644;&#26680;&#29289;&#29702;&#30340;&#35768;&#22810;&#20998;&#26512;&#20013;&#65292;&#20351;&#29992;&#27169;&#25311;&#26469;&#25512;&#26029;&#24213;&#23618;&#29289;&#29702;&#27169;&#22411;&#30340;&#22522;&#26412;&#12289;&#26377;&#25928;&#25110;&#29616;&#35937;&#23398;&#21442;&#25968;&#12290;&#24403;&#20351;&#29992;&#23637;&#24320;&#30340;&#25130;&#38754;&#36827;&#34892;&#25512;&#26029;&#26102;&#65292;&#21487;&#35266;&#27979;&#37327;&#26159;&#36890;&#36807;&#29289;&#29702;&#30452;&#35273;&#21644;&#32463;&#39564;&#27861;&#21017;&#36827;&#34892;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#35774;&#35745;&#26368;&#20248;&#30340;&#21487;&#35266;&#27979;&#37327;&#12290;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#23637;&#24320;&#30340;&#24494;&#20998;&#25130;&#38754;&#21253;&#21547;&#20102;&#20851;&#20110;&#24863;&#20852;&#36259;&#21442;&#25968;&#30340;&#26368;&#22810;&#20449;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26500;&#36896;&#26041;&#27861;&#36827;&#34892;&#24456;&#22909;&#30340;&#27979;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#29289;&#29702;&#27169;&#22411;&#26469;&#28436;&#31034;&#36825;&#20010;&#24819;&#27861;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#29992;&#20110;&#28145;&#24230;&#26080;&#24377;&#24615;&#25955;&#23556;&#20013;&#30340;&#21253;&#21547;&#24615;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many analyses in particle and nuclear physics use simulations to infer fundamental, effective, or phenomenological parameters of the underlying physics models. When the inference is performed with unfolded cross sections, the observables are designed using physics intuition and heuristics. We propose to design optimal observables with machine learning. Unfolded, differential cross sections in a neural network output contain the most information about parameters of interest and can be well-measured by construction. We demonstrate this idea using two physics models for inclusive measurements in deep inelastic scattering.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39044;&#27979;&#22810;&#20010;&#36873;&#25321;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#32771;&#34385;&#39038;&#23458;&#21644;&#39033;&#30446;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#22914;&#21487;&#36873;&#39033;&#30340;&#33539;&#22260;&#21644;&#23450;&#21046;&#38656;&#27714;&#65289;&#65292;&#35299;&#20915;&#20102;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#22312;&#39038;&#23458;&#36873;&#25321;&#22810;&#20010;&#39033;&#30446;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.08716</link><description>&lt;p&gt;
Transformer Choice Net: &#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39044;&#27979;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Transformer Choice Net: A Transformer Neural Network for Choice Prediction. (arXiv:2310.08716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#39044;&#27979;&#22810;&#20010;&#36873;&#25321;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#32771;&#34385;&#39038;&#23458;&#21644;&#39033;&#30446;&#30340;&#29305;&#24449;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#22914;&#21487;&#36873;&#39033;&#30340;&#33539;&#22260;&#21644;&#23450;&#21046;&#38656;&#27714;&#65289;&#65292;&#35299;&#20915;&#20102;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#22312;&#39038;&#23458;&#36873;&#25321;&#22810;&#20010;&#39033;&#30446;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24066;&#22330;&#33829;&#38144;&#12289;&#32463;&#27982;&#23398;&#21644;&#36816;&#31609;&#23398;&#20013;&#65292;&#31163;&#25955;&#36873;&#25321;&#27169;&#22411;&#65292;&#22914;&#22810;&#39033;&#24335;&#36923;&#36753;&#27169;&#22411;&#12289;Probit&#27169;&#22411;&#25110;&#28151;&#21512;&#36923;&#36753;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#65306;&#32473;&#23450;&#19968;&#32452;&#21487;&#36873;&#39033;&#65292;&#39038;&#23458;&#34987;&#27169;&#25311;&#20026;&#36873;&#25321;&#20854;&#20013;&#20043;&#19968;&#20197;&#26368;&#22823;&#21270;&#65288;&#28508;&#22312;&#30340;&#65289;&#25928;&#29992;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#25512;&#24191;&#21040;&#39038;&#23458;&#36873;&#25321;&#22810;&#20010;&#39033;&#30446;&#30340;&#24773;&#20917;&#65288;&#22914;&#30005;&#23376;&#21830;&#21153;&#36141;&#29289;&#65289;&#21364;&#23384;&#22312;&#38382;&#39064;&#12290;&#34429;&#28982;&#21487;&#20197;&#24314;&#31435;&#21512;&#29702;&#30340;&#39038;&#23458;&#34892;&#20026;&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#39033;&#30446;&#23376;&#38598;&#30340;&#32452;&#21512;&#29190;&#28856;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#20272;&#35745;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#21161;&#20110;&#39044;&#27979;&#22810;&#20010;&#36873;&#25321;&#30340;Transformer&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21363;Transformer Choice Net&#12290;&#20107;&#23454;&#35777;&#26126;&#65292;Transformer&#32593;&#32476;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#29305;&#21035;&#36866;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#20165;&#32771;&#34385;&#20102;&#39038;&#23458;&#21644;&#39033;&#30446;&#30340;&#29305;&#24449;&#65292;&#36824;&#32771;&#34385;&#20102;&#19978;&#19979;&#25991;&#65292;&#21363;&#21487;&#36873;&#39033;&#30340;&#33539;&#22260;&#20197;&#21450;&#23450;&#21046;&#21270;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discrete-choice models, such as Multinomial Logit, Probit, or Mixed-Logit, are widely used in Marketing, Economics, and Operations Research: given a set of alternatives, the customer is modeled as choosing one of the alternatives to maximize a (latent) utility function. However, extending such models to situations where the customer chooses more than one item (such as in e-commerce shopping) has proven problematic. While one can construct reasonable models of the customer's behavior, estimating such models becomes very challenging because of the combinatorial explosion in the number of possible subsets of items. In this paper we develop a transformer neural network architecture, the Transformer Choice Net, that is suitable for predicting multiple choices. Transformer networks turn out to be especially suitable for this task as they take into account not only the features of the customer and the items but also the context, which in this case could be the assortment as well as the custom
&lt;/p&gt;</description></item><item><title>Waymax&#26159;&#19968;&#31181;&#21152;&#36895;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#33258;&#20027;&#39550;&#39542;&#30740;&#31350;&#12290;&#23427;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#25968;&#25454;&#26469;&#21019;&#24314;&#30495;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#22330;&#26223;&#65292;&#24182;&#25903;&#25345;&#22823;&#35268;&#27169;&#12289;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#12290;&#36890;&#36807;&#23398;&#20064;&#21644;&#30828;&#32534;&#30721;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;Waymax&#21487;&#20197;&#25552;&#20379;&#36924;&#30495;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2310.08710</link><description>&lt;p&gt;
Waymax: &#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#33258;&#20027;&#39550;&#39542;&#30740;&#31350;&#30340;&#21152;&#36895;&#12289;&#25968;&#25454;&#39537;&#21160;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research. (arXiv:2310.08710v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08710
&lt;/p&gt;
&lt;p&gt;
Waymax&#26159;&#19968;&#31181;&#21152;&#36895;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#33258;&#20027;&#39550;&#39542;&#30740;&#31350;&#12290;&#23427;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#25968;&#25454;&#26469;&#21019;&#24314;&#30495;&#23454;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#22330;&#26223;&#65292;&#24182;&#25903;&#25345;&#22823;&#35268;&#27169;&#12289;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#12290;&#36890;&#36807;&#23398;&#20064;&#21644;&#30828;&#32534;&#30721;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;Waymax&#21487;&#20197;&#25552;&#20379;&#36924;&#30495;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#26159;&#20197;&#23433;&#20840;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#24320;&#21457;&#21644;&#35780;&#20272;&#33258;&#20027;&#36710;&#35268;&#21010;&#36719;&#20214;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#30340;&#27169;&#25311;&#38656;&#35201;&#31934;&#30830;&#24314;&#27169;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#34892;&#20026;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Waymax&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#19979;&#29992;&#20110;&#33258;&#20027;&#39550;&#39542;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#25311;&#22120;&#65292;&#19987;&#20026;&#22823;&#35268;&#27169;&#27169;&#25311;&#21644;&#27979;&#35797;&#35774;&#35745;&#12290;Waymax&#20351;&#29992;&#20844;&#24320;&#21457;&#24067;&#30340;&#30495;&#23454;&#19990;&#30028;&#39550;&#39542;&#25968;&#25454;&#65288;&#20363;&#22914;Waymo&#24320;&#25918;&#36816;&#21160;&#25968;&#25454;&#38598;&#65289;&#26469;&#21021;&#22987;&#21270;&#25110;&#22238;&#25918;&#22810;&#26679;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#27169;&#25311;&#22330;&#26223;&#12290;&#23427;&#23436;&#20840;&#36816;&#34892;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;&#22914;TPUs/GPUs&#65289;&#19978;&#65292;&#24182;&#25903;&#25345;&#22270;&#20869;&#27169;&#25311;&#20197;&#36827;&#34892;&#35757;&#32451;&#65292;&#36866;&#29992;&#20110;&#29616;&#20195;&#22823;&#35268;&#27169;&#12289;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#12290;&#20026;&#20102;&#25903;&#25345;&#22312;&#32447;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;Waymax&#21253;&#21547;&#20102;&#20960;&#20010;&#23398;&#20064;&#30340;&#21644;&#30828;&#32534;&#30721;&#30340;&#34892;&#20026;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#30495;&#23454;&#30340;&#20132;&#20114;&#12290;&#20026;&#20102;&#34917;&#20805;Waymax&#65292;&#25105;&#20204;&#36824;&#23545;&#19968;&#22871;&#27969;&#34892;&#30340;&#20223;&#30495;&#31639;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation is an essential tool to develop and benchmark autonomous vehicle planning software in a safe and cost-effective manner. However, realistic simulation requires accurate modeling of nuanced and complex multi-agent interactive behaviors. To address these challenges, we introduce Waymax, a new data-driven simulator for autonomous driving in multi-agent scenes, designed for large-scale simulation and testing. Waymax uses publicly-released, real-world driving data (e.g., the Waymo Open Motion Dataset) to initialize or play back a diverse set of multi-agent simulated scenarios. It runs entirely on hardware accelerators such as TPUs/GPUs and supports in-graph simulation for training, making it suitable for modern large-scale, distributed machine learning workflows. To support online training and evaluation, Waymax includes several learned and hard-coded behavior models that allow for realistic interaction within simulation. To supplement Waymax, we benchmark a suite of popular imita
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#25216;&#26415;&#65292;&#20197;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26597;&#35810;&#21644;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26102;&#38388;&#20174;&#40657;&#30418;&#23454;&#29616;&#30340;ReLU-based&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#20986;&#25152;&#26377;&#23454;&#20540;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.08708</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#23494;&#30721;&#20998;&#26512;&#25552;&#21462;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Polynomial Time Cryptanalytic Extraction of Neural Network Models. (arXiv:2310.08708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#25216;&#26415;&#65292;&#20197;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26597;&#35810;&#21644;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26102;&#38388;&#20174;&#40657;&#30418;&#23454;&#29616;&#30340;ReLU-based&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#20986;&#25152;&#26377;&#23454;&#20540;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#19978;&#33457;&#36153;&#20102;&#25968;&#21313;&#20159;&#32654;&#20803;&#21644;&#26080;&#25968;GPU&#23567;&#26102;&#65292;&#22240;&#27492;&#65292;&#30830;&#23450;&#22312;&#32473;&#23450;&#23545;&#20854;&#40657;&#30418;&#23454;&#29616;&#30340;&#35775;&#38382;&#26435;&#38480;&#26102;&#65292;&#25552;&#21462;&#27492;&#31867;&#31070;&#32463;&#32593;&#32476;&#30340;&#25152;&#26377;&#21442;&#25968;&#30340;&#38590;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36807;&#21435;30&#24180;&#20013;&#65292;&#30740;&#31350;&#20102;&#35768;&#22810;&#29256;&#26412;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#23545;ReLU-based&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29616;&#26377;&#26368;&#20339;&#25915;&#20987;&#26159;&#30001;Carlini&#12289;Jagielski&#21644;Mironov&#22312;2020&#24180;&#30340;&#21152;&#23494;&#30740;&#35752;&#20250;&#19978;&#25552;&#20986;&#30340;&#12290;&#36825;&#31867;&#20284;&#20110;&#23545;&#21152;&#23494;&#31995;&#32479;&#30340;&#24046;&#20998;&#36873;&#25321;&#26126;&#25991;&#25915;&#20987;&#65292;&#20854;&#40657;&#30418;&#23454;&#29616;&#20013;&#23884;&#20837;&#20102;&#19968;&#20010;&#23494;&#38053;&#65292;&#24182;&#19988;&#38656;&#35201;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#20294;&#25351;&#25968;&#25968;&#37327;&#30340;&#26102;&#38388;&#65288;&#20316;&#20026;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#20989;&#25968;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#20960;&#31181;&#26032;&#25216;&#26415;&#26469;&#25913;&#36827;&#36825;&#31181;&#25915;&#20987;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26597;&#35810;&#21644;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26102;&#38388;&#25552;&#21462;&#20855;&#26377;&#20219;&#24847;&#39640;&#31934;&#24230;&#30340;ReLU-based DNN&#30340;&#25152;&#26377;&#23454;&#20540;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Billions of dollars and countless GPU hours are currently spent on training Deep Neural Networks (DNNs) for a variety of tasks. Thus, it is essential to determine the difficulty of extracting all the parameters of such neural networks when given access to their black-box implementations. Many versions of this problem have been studied over the last 30 years, and the best current attack on ReLU-based deep neural networks was presented at Crypto 2020 by Carlini, Jagielski, and Mironov. It resembles a differential chosen plaintext attack on a cryptosystem, which has a secret key embedded in its black-box implementation and requires a polynomial number of queries but an exponential amount of time (as a function of the number of neurons). In this paper, we improve this attack by developing several new techniques that enable us to extract with arbitrarily high precision all the real-valued parameters of a ReLU-based DNN using a polynomial number of queries and a polynomial amount of time. We
&lt;/p&gt;</description></item><item><title>&#22312;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ELDEN&#65292;&#19968;&#31181;&#26032;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#40723;&#21169;&#26377;&#25928;&#22320;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.08702</link><description>&lt;p&gt;
ELDEN: &#22522;&#20110;&#26412;&#22320;&#20381;&#36182;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
ELDEN: Exploration via Local Dependencies. (arXiv:2310.08702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08702
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ELDEN&#65292;&#19968;&#31181;&#26032;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#36890;&#36807;&#20351;&#29992;&#20195;&#29702;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#40723;&#21169;&#26377;&#25928;&#22320;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#31232;&#30095;&#22870;&#21169;&#30340;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#19968;&#30452;&#38754;&#20020;&#30528;&#22256;&#38590;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#38656;&#35201;&#26377;&#25928;&#22320;&#25506;&#32034;&#29366;&#24577;&#31354;&#38388;&#65292;&#30452;&#21040;&#25214;&#21040;&#22870;&#21169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#31038;&#21306;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#22312;&#22870;&#21169;&#26469;&#22686;&#24378;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20869;&#22312;&#22870;&#21169;&#26159;&#19968;&#31181;&#40723;&#21169;&#20195;&#29702;&#35775;&#38382;&#26377;&#36259;&#29366;&#24577;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#23450;&#20041;&#20855;&#26377;&#22240;&#24335;&#29366;&#24577;&#31354;&#38388;&#21644;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#29615;&#22659;&#20013;&#30340;&#26377;&#36259;&#29366;&#24577;&#65292;&#20854;&#20013;&#20195;&#29702;&#30340;&#21160;&#20316;&#21487;&#33021;&#20250;&#25913;&#21464;&#19968;&#20010;&#23454;&#20307;&#30340;&#20540;&#65292;&#36827;&#32780;&#21487;&#33021;&#24433;&#21709;&#21478;&#19968;&#20010;&#23454;&#20307;&#30340;&#20540;&#12290;&#25105;&#20204;&#30340;&#35266;&#28857;&#26159;&#65292;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#65292;&#25506;&#32034;&#26377;&#36259;&#29366;&#24577;&#30340;&#20851;&#38190;&#26159;&#20195;&#29702;&#19981;&#30830;&#23450;&#23454;&#20307;&#65288;&#22914;&#20195;&#29702;&#25110;&#29289;&#20307;&#65289;&#26159;&#21542;&#30456;&#20114;&#24433;&#21709;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22914;&#20309;&#30456;&#20114;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"ELDEN: &#22522;&#20110;&#26412;&#22320;&#20381;&#36182;&#30340;&#25506;&#32034;"&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#20419;&#36827;&#20102;&#23545;&#26032;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tasks with large state space and sparse rewards present a longstanding challenge to reinforcement learning. In these tasks, an agent needs to explore the state space efficiently until it finds a reward. To deal with this problem, the community has proposed to augment the reward function with intrinsic reward, a bonus signal that encourages the agent to visit interesting states. In this work, we propose a new way of defining interesting states for environments with factored state spaces and complex chained dependencies, where an agent's actions may change the value of one entity that, in order, may affect the value of another entity. Our insight is that, in these environments, interesting states for exploration are states where the agent is uncertain whether (as opposed to how) entities such as the agent or objects have some influence on each other. We present ELDEN, Exploration via Local DepENdencies, a novel intrinsic reward that encourages the discovery of new interactions between en
&lt;/p&gt;</description></item><item><title>&#26680;-&#24377;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;KAE&#65289;&#26159;&#19968;&#31181;&#22312;&#20998;&#23376;&#35774;&#35745;&#39046;&#22495;&#20855;&#26377;&#22686;&#24378;&#24615;&#33021;&#30340;&#33258;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#12290;KAE&#23454;&#29616;&#20102;&#26377;&#25928;&#29983;&#25104;&#21644;&#20934;&#30830;&#37325;&#26500;&#30340;&#25361;&#25112;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#22810;&#26679;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;KAE&#36824;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#26465;&#20214;&#29983;&#25104;&#20998;&#23376;&#65292;&#24182;&#22312;&#20998;&#23376;&#23545;&#25509;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08685</link><description>&lt;p&gt;
&#20998;&#23376;&#35774;&#35745;&#30340;&#26680;-&#24377;&#24615;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Kernel-Elastic Autoencoder for Molecular Design. (arXiv:2310.08685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08685
&lt;/p&gt;
&lt;p&gt;
&#26680;-&#24377;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;KAE&#65289;&#26159;&#19968;&#31181;&#22312;&#20998;&#23376;&#35774;&#35745;&#39046;&#22495;&#20855;&#26377;&#22686;&#24378;&#24615;&#33021;&#30340;&#33258;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#12290;KAE&#23454;&#29616;&#20102;&#26377;&#25928;&#29983;&#25104;&#21644;&#20934;&#30830;&#37325;&#26500;&#30340;&#25361;&#25112;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#22810;&#26679;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;KAE&#36824;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#26465;&#20214;&#29983;&#25104;&#20998;&#23376;&#65292;&#24182;&#22312;&#20998;&#23376;&#23545;&#25509;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;-&#24377;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;KAE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#33258;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#20998;&#23376;&#35774;&#35745;&#24615;&#33021;&#12290;KAE&#22522;&#20110;&#20004;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24314;&#27169;&#65306;&#20462;&#25913;&#30340;&#26368;&#22823;&#22343;&#21248;&#20301;&#31227;&#21644;&#21152;&#26435;&#37325;&#26500;&#12290;KAE&#35299;&#20915;&#20102;&#21516;&#26102;&#23454;&#29616;&#26377;&#25928;&#29983;&#25104;&#21644;&#20934;&#30830;&#37325;&#26500;&#30340;&#38271;&#26399;&#25361;&#25112;&#12290;KAE&#22312;&#20998;&#23376;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#29420;&#31435;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#37325;&#26500;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#12290;KAE&#23454;&#29616;&#20102;&#26465;&#20214;&#29983;&#25104;&#65292;&#24182;&#20801;&#35768;&#22522;&#20110;&#27874;&#26463;&#25628;&#32034;&#36827;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#22312;&#21463;&#38480;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;KAE&#21487;&#20197;&#26681;&#25454;&#20998;&#23376;&#23545;&#25509;&#24212;&#29992;&#20013;&#30340;&#26377;&#21033;&#32467;&#21512;&#20146;&#21644;&#21147;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;AutoDock Vina&#21644;Glide&#24471;&#20998;&#36827;&#34892;&#30830;&#35748;&#65292;&#32988;&#36807;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#29616;&#26377;&#20505;&#36873;&#20998;&#23376;&#12290;&#38500;&#20102;&#20998;&#23376;&#35774;&#35745;&#65292;&#25105;&#20204;&#39044;&#26399;KAE&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Kernel-Elastic Autoencoder (KAE), a self-supervised generative model based on the transformer architecture with enhanced performance for molecular design. KAE is formulated based on two novel loss functions: modified maximum mean discrepancy and weighted reconstruction. KAE addresses the long-standing challenge of achieving valid generation and accurate reconstruction at the same time. KAE achieves remarkable diversity in molecule generation while maintaining near-perfect reconstructions on the independent testing dataset, surpassing previous molecule-generating models. KAE enables conditional generation and allows for decoding based on beam search resulting in state-of-the-art performance in constrained optimizations. Furthermore, KAE can generate molecules conditional to favorable binding affinities in docking applications as confirmed by AutoDock Vina and Glide scores, outperforming all existing candidates from the training dataset. Beyond molecular design, we antic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#25552;&#39640;Atari&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24615;&#33021;&#30340;&#26041;&#27861;</title><link>http://arxiv.org/abs/2310.08683</link><description>&lt;p&gt;
Atari&#24378;&#21270;&#23398;&#20064;&#30340;&#34394;&#25311;&#22686;&#24378;&#29616;&#23454;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Virtual Augmented Reality for Atari Reinforcement Learning. (arXiv:2310.08683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#25552;&#39640;Atari&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24615;&#33021;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#28216;&#25103;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#23588;&#20854;&#26159;&#35895;&#27468;DeepMind&#30340;AlphaGo&#20987;&#36133;&#20102;&#20154;&#31867;&#22260;&#26827;&#20896;&#20891;&#26607;&#27905;&#12290;&#36825;&#19968;&#32988;&#21033;&#20063;&#24471;&#30410;&#20110;Atari&#23398;&#20064;&#29615;&#22659;(ALE)&#65292;ALE&#22312;RL&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20419;&#36827;&#20102;AlphaGo&#21644;&#20854;&#20182;RL&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#24403;&#21069;&#30340;Atari&#35270;&#39057;&#28216;&#25103;RL&#30740;&#31350;&#20013;&#65292;RL&#26234;&#33021;&#20307;&#23545;&#29615;&#22659;&#30340;&#24863;&#30693;&#22522;&#20110;Atari&#35270;&#39057;&#28216;&#25103;&#23631;&#24149;&#30340;&#21407;&#22987;&#20687;&#32032;&#25968;&#25454;&#65292;&#19988;&#22270;&#20687;&#39044;&#22788;&#29702;&#24456;&#23569;&#12290;&#30456;&#21453;&#65292;&#26368;&#20808;&#36827;&#30340;ML&#30740;&#31350;&#22312;&#22686;&#24378;&#22270;&#20687;&#24863;&#30693;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#20854;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;&#20363;&#23376;&#26159;Meta Research&#30340;&#8220;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#20998;&#21106;&#27169;&#22411;(SAM)&#65292;&#23427;&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#27809;&#26377;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20998;&#21106;&#22270;&#20687;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#26412;&#25991;&#25506;&#35752;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#27861;&#35770;&#38382;&#39064;&#65306;&#20687;SAM&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#33021;&#21542;&#25552;&#39640;RL&#26234;&#33021;&#20307;&#22312;&#29609;Atari&#35270;&#39057;&#28216;&#25103;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has achieved significant milestones in the gaming domain, most notably Google DeepMind's AlphaGo defeating human Go champion Ken Jie. This victory was also made possible through the Atari Learning Environment (ALE): The ALE has been foundational in RL research, facilitating significant RL algorithm developments such as AlphaGo and others. In current Atari video game RL research, RL agents' perceptions of its environment is based on raw pixel data from the Atari video game screen with minimal image preprocessing. Contrarily, cutting-edge ML research, external to the Atari video game RL research domain, is focusing on enhancing image perception. A notable example is Meta Research's "Segment Anything Model" (SAM), a foundation model capable of segmenting images without prior training (zero-shot). This paper addresses a novel methodical question: Can state-of-the-art image segmentation models such as SAM improve the performance of RL agents playing Atari video g
&lt;/p&gt;</description></item><item><title>GDL-DS&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20855;&#26377;&#20998;&#24067;&#36716;&#25442;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#30340;&#20449;&#24687;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2310.08677</link><description>&lt;p&gt;
GDL-DS: &#20998;&#24067;&#36716;&#25442;&#19979;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GDL-DS: A Benchmark for Geometric Deep Learning under Distribution Shifts. (arXiv:2310.08677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08677
&lt;/p&gt;
&lt;p&gt;
GDL-DS&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20855;&#26377;&#20998;&#24067;&#36716;&#25442;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#23427;&#21253;&#25324;&#22810;&#20010;&#31185;&#23398;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#30340;&#20449;&#24687;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;(GDL)&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#20854;&#25797;&#38271;&#23545;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#30340;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#20854;&#22312;&#22788;&#29702;&#20998;&#24067;&#36716;&#25442;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#35768;&#22810;&#30456;&#20851;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GDL-DS&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;GDL&#27169;&#22411;&#22312;&#20855;&#26377;&#20998;&#24067;&#36716;&#25442;&#30340;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#31890;&#23376;&#29289;&#29702;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#21040;&#29983;&#29289;&#21270;&#23398;&#30340;&#19981;&#21516;&#31185;&#23398;&#39046;&#22495;&#65292;&#24182;&#21253;&#25324;&#21508;&#31181;&#20998;&#24067;&#36716;&#25442;&#65292;&#21253;&#25324;&#26465;&#20214;&#12289;&#21327;&#21464;&#21644;&#27010;&#24565;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26469;&#33258;&#36229;&#20986;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#30340;&#20449;&#24687;&#35775;&#38382;&#30340;&#19977;&#20010;&#32423;&#21035;&#65292;&#21253;&#25324;&#27809;&#26377;&#36229;&#20986;&#20998;&#24067;&#30340;&#20449;&#24687;&#12289;&#21482;&#26377;&#24102;&#26631;&#31614;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#21644;&#24102;&#26377;&#23569;&#25968;&#26631;&#31614;&#30340;&#36229;&#20986;&#20998;&#24067;&#29305;&#24449;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#28041;&#21450;30&#20010;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#35780;&#20272;3&#31181;&#20449;&#24687;&#35775;&#38382;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometric deep learning (GDL) has gained significant attention in various scientific fields, chiefly for its proficiency in modeling data with intricate geometric structures. Yet, very few works have delved into its capability of tackling the distribution shift problem, a prevalent challenge in many relevant applications. To bridge this gap, we propose GDL-DS, a comprehensive benchmark designed for evaluating the performance of GDL models in scenarios with distribution shifts. Our evaluation datasets cover diverse scientific domains from particle physics and materials science to biochemistry, and encapsulate a broad spectrum of distribution shifts including conditional, covariate, and concept shifts. Furthermore, we study three levels of information access from the out-of-distribution (OOD) testing data, including no OOD information, only OOD features without labels, and OOD features with a few labels. Overall, our benchmark results in 30 different experiment settings, and evaluates 3 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#23398;&#29983;&#36827;&#34892;&#22240;&#26524;&#19982;&#39044;&#27979;&#24615;&#23450;&#20301;&#30340;&#27604;&#36739;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23398;&#29983;&#21161;&#23398;&#37329;&#32493;&#31614;&#39046;&#22495;&#23454;&#39564;&#20013;&#40723;&#21169;&#23545;&#35937;&#36873;&#25321;&#30340;&#20215;&#20540;&#12290;&#36825;&#39033;&#22823;&#35268;&#27169;&#23454;&#22320;&#23454;&#39564;&#25581;&#31034;&#20102;&#23450;&#20301;&#24178;&#39044;&#23545;&#20110;&#19981;&#21516;&#23398;&#29983;&#30340;&#25928;&#26524;&#65292;&#20026;&#24178;&#39044;&#31574;&#30053;&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2310.08672</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#40723;&#21169;&#23545;&#35937;&#36873;&#25321;&#65306;&#22312;&#23398;&#29983;&#21161;&#23398;&#37329;&#32493;&#31614;&#39046;&#22495;&#23454;&#39564;&#20013;&#30340;&#22240;&#26524;&#19982;&#39044;&#27979;&#24615;&#30446;&#26631;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal. (arXiv:2310.08672v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08672
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#23398;&#29983;&#36827;&#34892;&#22240;&#26524;&#19982;&#39044;&#27979;&#24615;&#23450;&#20301;&#30340;&#27604;&#36739;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#23398;&#29983;&#21161;&#23398;&#37329;&#32493;&#31614;&#39046;&#22495;&#23454;&#39564;&#20013;&#40723;&#21169;&#23545;&#35937;&#36873;&#25321;&#30340;&#20215;&#20540;&#12290;&#36825;&#39033;&#22823;&#35268;&#27169;&#23454;&#22320;&#23454;&#39564;&#25581;&#31034;&#20102;&#23450;&#20301;&#24178;&#39044;&#23545;&#20110;&#19981;&#21516;&#23398;&#29983;&#30340;&#25928;&#26524;&#65292;&#20026;&#24178;&#39044;&#31574;&#30053;&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#22659;&#19979;&#65292;&#24178;&#39044;&#21487;&#33021;&#23545;&#26576;&#20123;&#20154;&#27604;&#20854;&#20182;&#20154;&#26356;&#26377;&#25928;&#65292;&#22240;&#27492;&#23450;&#20301;&#24178;&#39044;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#35268;&#27169;&#24222;&#22823;&#30340;&#23454;&#22320;&#23454;&#39564;&#65288;&#36229;&#36807;53,000&#21517;&#22823;&#23398;&#29983;&#65289;&#26469;&#20998;&#26512;&#22312;&#23398;&#29983;&#21161;&#23398;&#37329;&#32493;&#31614;&#21069;&#20351;&#29992;&#8220;&#40723;&#21169;&#8221;&#31574;&#30053;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23450;&#20301;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#20272;&#35745;&#24322;&#36136;&#22788;&#29702;&#25928;&#24212;&#30340;&#22240;&#26524;&#26862;&#26519;&#36827;&#34892;&#23450;&#20301;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#20986;&#30340;&#25317;&#26377;&#26368;&#39640;&#22788;&#29702;&#25928;&#24212;&#30340;&#23398;&#29983;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35780;&#20272;&#20004;&#31181;&#26367;&#20195;&#30340;&#23450;&#20301;&#31574;&#30053;&#65292;&#19968;&#31181;&#26159;&#38024;&#23545;&#22312;&#27809;&#26377;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#21040;&#20302;&#21161;&#23398;&#37329;&#32493;&#31614;&#27010;&#29575;&#30340;&#23398;&#29983;&#65292;&#21478;&#19968;&#31181;&#26159;&#38024;&#23545;&#39044;&#27979;&#21040;&#39640;&#27010;&#29575;&#30340;&#23398;&#29983;&#12290;&#39044;&#27979;&#30340;&#22522;&#32447;&#32467;&#26524;&#24182;&#19981;&#26159;&#23450;&#20301;&#30340;&#29702;&#24819;&#26631;&#20934;&#65292;&#32780;&#19988;&#22312;&#20808;&#39564;&#19978;&#20063;&#19981;&#28165;&#26970;&#26159;&#20248;&#20808;&#32771;&#34385;&#20302;&#12289;&#39640;&#36824;&#26159;&#20013;&#38388;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many settings, interventions may be more effective for some individuals than others, so that targeting interventions may be beneficial. We analyze the value of targeting in the context of a large-scale field experiment with over 53,000 college students, where the goal was to use "nudges" to encourage students to renew their financial-aid applications before a non-binding deadline. We begin with baseline approaches to targeting. First, we target based on a causal forest that estimates heterogeneous treatment effects and then assigns students to treatment according to those estimated to have the highest treatment effects. Next, we evaluate two alternative targeting policies, one targeting students with low predicted probability of renewing financial aid in the absence of the treatment, the other targeting those with high probability. The predicted baseline outcome is not the ideal criterion for targeting, nor is it a priori clear whether to prioritize low, high, or intermediate predic
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#36825;&#20123;&#31639;&#27861;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#37117;&#33021;&#25910;&#25947;&#21040;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#27169;&#22411;&#25552;&#21462;&#22122;&#22768;&#21644;&#26368;&#23567;&#35206;&#30422;&#25351;&#25968;&#65292;&#25552;&#20513;&#20102;&#26412;&#22320;&#27169;&#22411;&#36873;&#25321;&#21644;&#20840;&#23616;&#27169;&#22411;&#36873;&#25321;&#30340;&#32852;&#21512;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.08670</link><description>&lt;p&gt;
&#27599;&#20010;&#21442;&#25968;&#37117;&#24456;&#37325;&#35201;&#65306;&#30830;&#20445;&#20855;&#26377;&#21160;&#24577;&#24322;&#26500;&#27169;&#22411;&#32553;&#20943;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Every Parameter Matters: Ensuring the Convergence of Federated Learning with Dynamic Heterogeneous Models Reduction. (arXiv:2310.08670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#36825;&#20123;&#31639;&#27861;&#23545;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#37117;&#33021;&#25910;&#25947;&#21040;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#27169;&#22411;&#25552;&#21462;&#22122;&#22768;&#21644;&#26368;&#23567;&#35206;&#30422;&#25351;&#25968;&#65292;&#25552;&#20513;&#20102;&#26412;&#22320;&#27169;&#22411;&#36873;&#25321;&#21644;&#20840;&#23616;&#27169;&#22411;&#36873;&#25321;&#30340;&#32852;&#21512;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#30001;&#20110;&#24213;&#31471;&#35774;&#22791;&#23384;&#22312;&#36164;&#28304;&#29942;&#39048;&#65292;&#37027;&#20123;&#21487;&#33021;&#25552;&#20379;&#29420;&#29305;&#36129;&#29486;&#30340;&#20302;&#31471;&#35774;&#22791;&#34987;&#25490;&#38500;&#22312;&#35757;&#32451;&#22823;&#27169;&#22411;&#20043;&#22806;&#65292;&#36825;&#32473;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#38598;&#20013;&#22312;&#24322;&#26500;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#19978;&#65292;&#36890;&#36807;&#20174;&#20840;&#23616;&#27169;&#22411;&#20013;&#25552;&#21462;&#32553;&#23567;&#23610;&#23544;&#30340;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26412;&#22320;&#35774;&#22791;&#12290;&#23613;&#31649;&#23454;&#35777;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#35813;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#30340;&#19968;&#33324;&#29702;&#35770;&#20445;&#35777;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#20805;&#20998;&#26465;&#20214;&#19979;&#65292;&#23545;&#20110;IID&#21644;&#38750;IID&#25968;&#25454;&#65292;&#36825;&#20123;&#31639;&#27861;&#25910;&#25947;&#21040;&#26631;&#20934;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#20010;&#31283;&#23450;&#28857;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#24179;&#28369;&#25104;&#26412;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24433;&#21709;&#20854;&#25910;&#25947;&#24615;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#27169;&#22411;&#25552;&#21462;&#22122;&#22768;&#21644;&#26368;&#23567;&#35206;&#30422;&#25351;&#25968;&#65292;&#24182;&#20027;&#24352;&#20102;&#26412;&#22320;&#27169;&#22411;&#36873;&#25321;&#21644;&#20840;&#23616;&#27169;&#22411;&#36873;&#25321;&#30340;&#32852;&#21512;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-device Federated Learning (FL) faces significant challenges where low-end clients that could potentially make unique contributions are excluded from training large models due to their resource bottlenecks. Recent research efforts have focused on model-heterogeneous FL, by extracting reduced-size models from the global model and applying them to local clients accordingly. Despite the empirical success, general theoretical guarantees of convergence on this method remain an open question. In this paper, we present a unifying framework for heterogeneous FL algorithms with online model extraction and provide a general convergence analysis. In particular, we prove that under certain sufficient conditions and for both IID and non-IID data, these algorithms converge to a stationary point of standard FL for general smooth cost functions. Moreover, we illuminate two key factors impacting its convergence: model-extraction noise and minimum coverage index, advocating a joint design of local 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#35745;&#25968;&#20219;&#21153;&#20013;&#30340;&#31639;&#27861;&#25512;&#24191;&#65292;&#24182;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;Transformer&#30340;&#26550;&#26500;&#20915;&#31574;&#20250;&#38459;&#30861;&#20854;&#22312;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#28040;&#38500;&#38382;&#39064;&#25805;&#20316;&#65292;&#20462;&#25913;&#21518;&#30340;Transformer&#22312;&#35745;&#25968;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#31639;&#27861;&#25512;&#24191;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08661</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#36827;&#34892;&#35745;&#25968;&#21644;&#31639;&#27861;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Counting and Algorithmic Generalization with Transformers. (arXiv:2310.08661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08661
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#22312;&#35745;&#25968;&#20219;&#21153;&#20013;&#30340;&#31639;&#27861;&#25512;&#24191;&#65292;&#24182;&#35777;&#26126;&#20102;&#26631;&#20934;&#30340;Transformer&#30340;&#26550;&#26500;&#20915;&#31574;&#20250;&#38459;&#30861;&#20854;&#22312;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#28040;&#38500;&#38382;&#39064;&#25805;&#20316;&#65292;&#20462;&#25913;&#21518;&#30340;Transformer&#22312;&#35745;&#25968;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#31639;&#27861;&#25512;&#24191;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31639;&#27861;&#25512;&#24191;&#26159;&#25351;&#23398;&#20064;&#29983;&#25104;&#25968;&#25454;&#30340;&#24213;&#23618;&#31639;&#27861;&#65292;&#20197;&#19968;&#31181;&#23545;&#36229;&#20986;&#20998;&#24067;&#30340;&#26041;&#24335;&#36827;&#34892;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#36825;&#23545;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#36890;&#24120;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#35745;&#25968;&#26102;&#38656;&#35201;&#30340;&#31639;&#27861;&#25512;&#24191;&#65292;&#26080;&#35770;&#26159;&#38544;&#24335;&#36824;&#26159;&#26174;&#24335;&#12290;&#25105;&#20204;&#34920;&#26126;&#26631;&#20934;&#30340;Transformer&#26159;&#22522;&#20110;&#38459;&#30861;&#36825;&#31867;&#20219;&#21153;&#30340;&#26550;&#26500;&#20915;&#31574;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;&#23618;&#24402;&#19968;&#21270;&#21644;&#36890;&#36807;softmax&#24402;&#19968;&#21270;&#27880;&#24847;&#21147;&#26435;&#37325;&#30340;&#21518;&#26524;&#12290;&#36890;&#36807;&#28040;&#38500;&#36825;&#20123;&#38382;&#39064;&#25805;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;&#20462;&#25913;&#21518;&#30340;Transformer&#21487;&#20197;&#22312;&#35745;&#25968;&#26041;&#38754;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#31639;&#27861;&#25512;&#24191;&#24615;&#33021;&#65292;&#21516;&#26102;&#37319;&#29992;&#38750;&#24120;&#36731;&#37327;&#32423;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic generalization in machine learning refers to the ability to learn the underlying algorithm that generates data in a way that generalizes out-of-distribution. This is generally considered a difficult task for most machine learning algorithms. Here, we analyze algorithmic generalization when counting is required, either implicitly or explicitly. We show that standard Transformers are based on architectural decisions that hinder out-of-distribution performance for such tasks. In particular, we discuss the consequences of using layer normalization and of normalizing the attention weights via softmax. With ablation of the problematic operations, we demonstrate that a modified transformer can exhibit a good algorithmic generalization performance on counting while using a very lightweight architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#32422;&#26463;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#21644;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.08660</link><description>&lt;p&gt;
&#22312;&#19981;&#25506;&#32034;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32852;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;RL&#31574;&#30053;&#65306;&#19968;&#31181;&#25209;&#37327;&#32422;&#26463;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach. (arXiv:2310.08660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25209;&#37327;&#32422;&#26463;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#21442;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#21644;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32593;&#32476;&#21442;&#25968;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#36895;&#29575;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#23558;&#20854;&#26500;&#24314;&#20026;&#21151;&#29575;&#25511;&#21046;&#12289;&#27874;&#26463;&#25104;&#24418;&#21644;&#24178;&#25200;&#28040;&#38500;&#30340;&#32852;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#20010;&#22522;&#31449;&#19982;&#22810;&#20010;&#29992;&#25143;&#35774;&#22791;&#36890;&#20449;&#30340;&#24773;&#20917;&#12290;&#30001;&#20110;&#31351;&#20030;&#25628;&#32034;&#30340;&#25351;&#25968;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#26469;&#35299;&#20915;&#35813;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#29616;&#20195;&#36890;&#20449;&#31995;&#32479;&#20197;&#20854;&#38590;&#20197;&#20934;&#30830;&#24314;&#27169;&#30340;&#34892;&#20026;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;RL&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#20197;&#20415;&#20195;&#29702;&#33021;&#22815;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22833;&#36133;&#30340;&#39640;&#25104;&#26412;&#65292;&#23558;&#31639;&#27861;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#25506;&#32034;&#21644;&#23398;&#20064;&#26159;&#19981;&#26126;&#26234;&#30340;&#12290;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#22522;&#20110;RL&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;&#22914;&#22522;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#30340;&#25511;&#21046;&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#19968;&#31181;&#31163;&#31574;&#30053;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this project, we consider the problem of network parameter optimization for rate maximization. We frame this as a joint optimization problem of power control, beam forming, and interference cancellation. We consider the setting where multiple Base Stations (BSs) are communicating with multiple user equipments (UEs). Because of the exponential computational complexity of brute force search, we instead solve this non-convex optimization problem using deep reinforcement learning (RL) techniques. The modern communication systems are notorious for their difficulty in exactly modeling their behaviour. This limits us in using RL based algorithms as interaction with the environment is needed for the agent to explore and learn efficiently. Further, it is ill advised to deploy the algorithm in real world for exploration and learning because of the high cost of failure. In contrast to the previous RL-based solutions proposed, such as deep-Q network (DQN) based control, we propose taking an off
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08659</link><description>&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#24182;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;LoRA&#31934;&#35843;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21516;&#26102;&#24212;&#29992;&#37327;&#21270;&#21644;LoRA&#31934;&#35843;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#35266;&#23519;&#21040;&#23436;&#25972;&#31934;&#35843;&#21644;&#37327;&#21270;&#21152;LoRA&#31934;&#35843;&#26041;&#27861;&#20043;&#38388;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoftQ&#65288;LoRA-Fine-Tuning-aware Quantization&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#26469;&#36827;&#34892;LoRA&#31934;&#35843;&#12290;&#36825;&#31181;&#21021;&#22987;&#21270;&#20943;&#36731;&#20102;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
&lt;/p&gt;</description></item><item><title>SplitBeam&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#21106;&#35745;&#31639;&#65292;&#22312;Wi-Fi&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#27874;&#26463;&#25104;&#24418;&#12290;</title><link>http://arxiv.org/abs/2310.08656</link><description>&lt;p&gt;
SplitBeam: &#36890;&#36807;&#20998;&#21106;&#35745;&#31639;&#22312;Wi-Fi&#32593;&#32476;&#20013;&#23454;&#29616;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#27874;&#26463;&#25104;&#24418;
&lt;/p&gt;
&lt;p&gt;
SplitBeam: Effective and Efficient Beamforming in Wi-Fi Networks Through Split Computing. (arXiv:2310.08656v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08656
&lt;/p&gt;
&lt;p&gt;
SplitBeam&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#21106;&#35745;&#31639;&#65292;&#22312;Wi-Fi&#32593;&#32476;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#27874;&#26463;&#25104;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;IEEE 802.11&#65288;Wi-Fi&#65289;&#32593;&#32476;&#24191;&#27867;&#20381;&#36182;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;&#21534;&#21520;&#37327;&#12290;&#20026;&#20102;&#27491;&#30830;&#36827;&#34892;MIMO&#20256;&#36755;&#30340;&#27874;&#26463;&#25104;&#24418;&#65292;&#25509;&#20837;&#28857;&#38656;&#35201;&#39057;&#32321;&#22320;&#20174;&#27599;&#20010;&#36830;&#25509;&#30340;&#31449;&#28857;&#33719;&#21462;&#27874;&#26463;&#25104;&#24418;&#30697;&#38453;&#65288;BM&#65289;&#12290;&#28982;&#32780;&#65292;&#30697;&#38453;&#30340;&#22823;&#23567;&#38543;&#30528;&#22825;&#32447;&#21644;&#23376;&#36733;&#27874;&#30340;&#25968;&#37327;&#22686;&#21152;&#32780;&#22686;&#22823;&#65292;&#23548;&#33268;&#31354;&#38388;&#20013;&#30340;&#24320;&#38144;&#21644;&#31449;&#28857;&#30340;&#35745;&#31639;&#36127;&#36733;&#19981;&#26029;&#22686;&#21152;&#12290;&#20256;&#32479;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#36127;&#36733;&#36807;&#22823;&#65292;&#35201;&#20040;&#27874;&#26463;&#25104;&#24418;&#31934;&#24230;&#19981;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SplitBeam&#65292;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#20998;&#21106;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#65292;&#30452;&#25509;&#26681;&#25454;&#36755;&#20837;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30697;&#38453;&#36755;&#20986;BM&#12290;&#25105;&#20204;&#21046;&#23450;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#29942;&#39048;&#20248;&#21270;&#38382;&#39064;&#65288;BOP&#65289;&#65292;&#20197;&#20445;&#25345;&#35745;&#31639;&#12289;&#31354;&#38388;&#20013;&#30340;&#24320;&#38144;&#21644;&#35823;&#30721;&#29575;&#65288;BER&#65289;&#20302;&#20110;&#24212;&#29992;&#35201;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#25104;&#30340;Wi-Fi&#35774;&#22791;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#24615;CSI&#25910;&#38598;&#65292;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern IEEE 802.11 (Wi-Fi) networks extensively rely on multiple-input multiple-output (MIMO) to significantly improve throughput. To correctly beamform MIMO transmissions, the access point needs to frequently acquire a beamforming matrix (BM) from each connected station. However, the size of the matrix grows with the number of antennas and subcarriers, resulting in an increasing amount of airtime overhead and computational load at the station. Conventional approaches come with either excessive computational load or loss of beamforming precision. For this reason, we propose SplitBeam, a new framework where we train a split deep neural network (DNN) to directly output the BM given the channel state information (CSI) matrix as input. We formulate and solve a bottleneck optimization problem (BOP) to keep computation, airtime overhead, and bit error rate (BER) below application requirements. We perform extensive experimental CSI collection with off-the-shelf Wi-Fi devices in two distinct e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;BERT&#27169;&#22411;&#20998;&#26512;&#38463;&#23500;&#27735;&#27494;&#35013;&#20914;&#31361;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#36890;&#36807;&#23545;&#20107;&#20214;&#30340;&#25551;&#36848;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#20854;&#26159;&#21542;&#33268;&#21629;&#12290;&#27169;&#22411;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.08653</link><description>&lt;p&gt;
&#22312;&#38463;&#23500;&#27735;&#27494;&#35013;&#20914;&#31361;&#20013;&#36890;&#36807;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#27515;&#20129;&#20998;&#31867;&#30340;&#20998;&#26512;&#65306;&#19968;&#31181;BERT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing Textual Data for Fatality Classification in Afghanistan's Armed Conflicts: A BERT Approach. (arXiv:2310.08653v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08653
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;BERT&#27169;&#22411;&#20998;&#26512;&#38463;&#23500;&#27735;&#27494;&#35013;&#20914;&#31361;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#36890;&#36807;&#23545;&#20107;&#20214;&#30340;&#25551;&#36848;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#20854;&#26159;&#21542;&#33268;&#21629;&#12290;&#27169;&#22411;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23500;&#27735;&#22312;&#21382;&#21490;&#19978;&#32463;&#21382;&#20102;&#35768;&#22810;&#27494;&#35013;&#20914;&#31361;&#65292;&#23588;&#20854;&#26159;&#22312;&#36807;&#21435;&#30340;20&#24180;&#20013;&#65307;&#36825;&#20123;&#20107;&#20214;&#23545;&#20154;&#31867;&#29983;&#27963;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#21253;&#25324;&#20891;&#20107;&#20154;&#21592;&#21644;&#24179;&#27665;&#65292;&#21487;&#33021;&#23548;&#33268;&#27515;&#20129;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#26681;&#25454;&#30001;&#27494;&#35013;&#20914;&#31361;&#20301;&#32622;&#21644;&#20107;&#20214;&#25968;&#25454;&#39033;&#30446;&#65288;ACLED&#65289;&#25968;&#25454;&#38598;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#23545;&#38463;&#23500;&#27735;&#27494;&#35013;&#20914;&#31361;&#30340;&#32467;&#26524;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#20854;&#26159;&#21542;&#33268;&#21629;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;2021&#24180;8&#26376;&#33267;2023&#24180;3&#26376;&#22312;&#38463;&#23500;&#27735;&#21457;&#29983;&#30340;&#27494;&#35013;&#20914;&#31361;&#30340;&#35814;&#32454;&#25551;&#36848;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;BERT&#65288;&#21452;&#21521;&#32534;&#30721;&#22120;&#36716;&#25442;&#22120;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65289;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20808;&#36827;&#35821;&#35328;&#34920;&#31034;&#27169;&#22411;&#12290;&#20998;&#31867;&#22120;&#21033;&#29992;&#20107;&#20214;&#30340;&#21407;&#22987;&#25991;&#26412;&#25551;&#36848;&#26469;&#20272;&#35745;&#20107;&#20214;&#23548;&#33268;&#27515;&#20129;&#30340;&#21487;&#33021;&#24615;&#12290;&#35813;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Afghanistan has witnessed many armed conflicts throughout history, especially in the past 20 years; these events have had a significant impact on human lives, including military and civilians, with potential fatalities. In this research, we aim to leverage state-of-the-art machine learning techniques to classify the outcomes of Afghanistan armed conflicts to either fatal or non-fatal based on their textual descriptions provided by the Armed Conflict Location &amp; Event Data Project (ACLED) dataset. The dataset contains comprehensive descriptions of armed conflicts in Afghanistan that took place from August 2021 to March 2023. The proposed approach leverages the power of BERT (Bidirectional Encoder Representations from Transformers), a cutting-edge language representation model in natural language processing. The classifier utilizes the raw textual description of an event to estimate the likelihood of the event resulting in a fatality. The model achieved impressive performance on the test 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#23454;&#29616;&#30005;&#32593;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#38477;&#32500;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#23545;SCADA&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#26816;&#27979;&#20986;&#20854;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.08650</link><description>&lt;p&gt;
&#36890;&#36807;&#24352;&#37327;&#20998;&#35299;&#23454;&#29616;&#30005;&#32593;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Electrical Grid Anomaly Detection via Tensor Decomposition. (arXiv:2310.08650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#23454;&#29616;&#30005;&#32593;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#38477;&#32500;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#23545;SCADA&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#26816;&#27979;&#20986;&#20854;&#20013;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#25511;&#19982;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;&#65288;SCADA&#65289;&#32463;&#24120;&#20316;&#20026;&#30005;&#32593;&#20998;&#31449;&#30340;&#31070;&#32463;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#23454;&#29616;&#20102;&#23454;&#26102;&#30417;&#25511;&#12289;&#25968;&#25454;&#37319;&#38598;&#12289;&#35774;&#22791;&#25511;&#21046;&#65292;&#24182;&#30830;&#20445;&#20998;&#31449;&#21450;&#20854;&#36830;&#25509;&#35774;&#22791;&#30340;&#24179;&#31283;&#39640;&#25928;&#36816;&#34892;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#22522;&#20110;&#38477;&#32500;&#30340;&#26041;&#27861;&#65292;&#22914;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#65292;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;SCADA&#31995;&#32479;&#20013;&#30340;&#24322;&#24120;&#12290;&#34429;&#28982;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#24182;&#27809;&#26377;&#19987;&#38376;&#24212;&#29992;&#20110;SCADA&#65292;&#20294;&#23427;&#22312;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#26816;&#27979;&#24322;&#24120;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#26080;&#30417;&#30563;&#26041;&#27861;&#36890;&#36807;&#23545;&#27491;&#24120;&#25110;&#39044;&#26399;&#34892;&#20026;&#36827;&#34892;&#24314;&#27169;&#65292;&#35782;&#21035;&#20559;&#31163;&#39044;&#26399;&#34892;&#20026;&#30340;&#20107;&#20214;&#65292;&#20174;&#32780;&#26816;&#27979;&#26410;&#30693;&#31867;&#22411;&#30340;&#25915;&#20987;&#25110;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#23545;SCADA&#31995;&#32479;&#20013;&#33258;&#28982;&#23384;&#22312;&#30340;&#22797;&#26434;&#22810;&#32500;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#33021;&#22815;&#23545;SCADA&#31995;&#32479;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#24314;&#27169;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervisory Control and Data Acquisition (SCADA) systems often serve as the nervous system for substations within power grids. These systems facilitate real-time monitoring, data acquisition, control of equipment, and ensure smooth and efficient operation of the substation and its connected devices. Previous work has shown that dimensionality reduction-based approaches, such as Principal Component Analysis (PCA), can be used for accurate identification of anomalies in SCADA systems. While not specifically applied to SCADA, non-negative matrix factorization (NMF) has shown strong results at detecting anomalies in wireless sensor networks. These unsupervised approaches model the normal or expected behavior and detect the unseen types of attacks or anomalies by identifying the events that deviate from the expected behavior. These approaches; however, do not model the complex and multi-dimensional interactions that are naturally present in SCADA systems. Differently, non-negative tensor de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26102;&#38388;&#21521;&#37327;&#21270;&#30340;&#25968;&#20540;&#31215;&#20998;&#26041;&#27861;&#65292;&#29992;&#20110;&#31215;&#20998;&#21018;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#32452;&#65292;&#24182;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#35745;&#31639;&#21442;&#25968;&#26799;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#21644;&#36830;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#25209;&#27425;&#19978;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#35745;&#31639;&#24102;&#23485;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#29616;&#20195;GPU&#24182;&#23454;&#29616;&#36229;&#36807;100&#20493;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2310.08649</link><description>&lt;p&gt;
&#26102;&#38388;&#21521;&#37327;&#21270;&#25968;&#20540;&#31215;&#20998;&#29992;&#20110;ODE&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Time-vectorized numerical integration for systems of ODEs. (arXiv:2310.08649v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26102;&#38388;&#21521;&#37327;&#21270;&#30340;&#25968;&#20540;&#31215;&#20998;&#26041;&#27861;&#65292;&#29992;&#20110;&#31215;&#20998;&#21018;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#32452;&#65292;&#24182;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#35745;&#31639;&#21442;&#25968;&#26799;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#21644;&#36830;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#25209;&#27425;&#19978;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#35745;&#31639;&#24102;&#23485;&#65292;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#29616;&#20195;GPU&#24182;&#23454;&#29616;&#36229;&#36807;100&#20493;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#38382;&#39064;&#20013;&#65292;&#21018;&#24615;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#32452;&#21644;&#31232;&#30095;&#35757;&#32451;&#25968;&#25454;&#26159;&#24456;&#24120;&#35265;&#30340;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#29992;&#20110;&#31215;&#20998;&#21018;&#24615;&#24120;&#24494;&#20998;&#26041;&#31243;&#32452;&#30340;&#39640;&#25928;&#38544;&#24335;&#21521;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#35745;&#31639;&#21442;&#25968;&#26799;&#24230;&#12290;&#20027;&#35201;&#21019;&#26032;&#26159;&#22312;&#29420;&#31435;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#37327;&#21644;&#36830;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#25209;&#27425;&#25110;&#8220;&#22359;&#8221;&#19978;&#21521;&#37327;&#21270;&#38382;&#39064;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#21521;&#37327;&#21270;&#38544;&#24335;ODE&#31995;&#32479;&#30340;&#32452;&#35013;&#12290;&#21518;&#21521;Euler&#26041;&#27861;&#30340;&#32447;&#24615;&#21270;&#38544;&#24335;&#31995;&#32479;&#30340;&#22359;&#21452;&#23545;&#35282;&#32467;&#26500;&#20801;&#35768;&#36827;&#19968;&#27493;&#20351;&#29992;&#24182;&#34892;&#24490;&#29615;&#32422;&#21270;&#65288;PCR&#65289;&#36827;&#34892;&#21521;&#37327;&#21270;&#12290;&#22312;&#36755;&#20837;&#25968;&#25454;&#30340;&#20004;&#20010;&#36724;&#19978;&#36827;&#34892;&#21521;&#37327;&#21270;&#65292;&#20026;&#35745;&#31639;&#35774;&#22791;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#35745;&#31639;&#24102;&#23485;&#65292;&#20351;&#24471;&#21363;&#20351;&#26159;&#30456;&#23545;&#31232;&#30095;&#30340;&#38382;&#39064;&#20063;&#33021;&#20805;&#20998;&#21033;&#29992;&#29616;&#20195;GPU&#65292;&#24182;&#23454;&#29616;&#20102;&#36229;&#36807;100&#20493;&#30340;&#21152;&#36895;&#65292;&#19982;&#26631;&#20934;&#30340;&#39034;&#24207;&#26102;&#38388;&#31215;&#20998;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stiff systems of ordinary differential equations (ODEs) and sparse training data are common in scientific problems. This paper describes efficient, implicit, vectorized methods for integrating stiff systems of ordinary differential equations through time and calculating parameter gradients with the adjoint method. The main innovation is to vectorize the problem both over the number of independent times series and over a batch or "chunk" of sequential time steps, effectively vectorizing the assembly of the implicit system of ODEs. The block-bidiagonal structure of the linearized implicit system for the backward Euler method allows for further vectorization using parallel cyclic reduction (PCR). Vectorizing over both axes of the input data provides a higher bandwidth of calculations to the computing device, allowing even problems with comparatively sparse data to fully utilize modern GPUs and achieving speed ups of greater than 100x, compared to standard, sequential time integration. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#20102;3D&#25171;&#21360;&#22278;&#26609;&#20307;&#20013;&#30340;&#32570;&#38519;&#65292;&#32467;&#26524;&#21457;&#29616;MobileNetV2&#22312;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.08645</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#23545;3D&#25171;&#21360;&#22278;&#26609;&#29289;&#20307;&#30340;&#32570;&#38519;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Defect Analysis of 3D Printed Cylinder Object Using Transfer Learning Approaches. (arXiv:2310.08645v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#20102;3D&#25171;&#21360;&#22278;&#26609;&#20307;&#20013;&#30340;&#32570;&#38519;&#65292;&#32467;&#26524;&#21457;&#29616;MobileNetV2&#22312;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28155;&#21152;&#21046;&#36896;&#65288;AM&#65289;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#33322;&#31354;&#33322;&#22825;&#21644;&#27773;&#36710;&#31561;&#21508;&#20010;&#34892;&#19994;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26089;&#26399;&#35782;&#21035;AM&#36807;&#31243;&#20013;&#30340;&#32570;&#38519;&#21487;&#20197;&#38477;&#20302;&#29983;&#20135;&#25104;&#26412;&#24182;&#25552;&#39640;&#29983;&#20135;&#25928;&#29575;-&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#27169;&#22411;&#22312;3D&#25171;&#21360;&#22278;&#26609;&#20307;&#32570;&#38519;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20351;&#29992;&#21253;&#25324;VGG16&#12289;VGG19&#12289;ResNet50&#12289;ResNet101&#12289;InceptionResNetV2&#21644;MobileNetV2&#31561;&#27169;&#22411;&#23545;&#22278;&#26609;&#20307;&#30340;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1-score&#25351;&#26631;&#27604;&#36739;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#12290;&#22312;&#31532;&#19968;&#39033;&#30740;&#31350;&#20013;&#65292;VGG16&#12289;InceptionResNetV2&#21644;MobileNetV2&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;&#24471;&#20998;&#12290;&#30456;&#21453;&#65292;ResNet50&#30340;&#24615;&#33021;&#26368;&#24046;&#65292;&#24179;&#22343;F1-score&#20026;0.32&#12290;&#21516;&#26679;&#22320;&#65292;&#22312;&#31532;&#20108;&#39033;&#30740;&#31350;&#20013;&#65292;MobileNetV2&#27491;&#30830;&#20998;&#31867;&#20102;&#25152;&#26377;&#23454;&#20363;&#65292;&#32780;ResNet50&#21017;&#20986;&#29616;&#20102;&#26356;&#22810;&#30340;&#20551;&#38451;&#24615;&#21644;&#36739;&#23569;&#30340;&#30495;&#38451;&#24615;&#65292;&#23548;&#33268;F1-score&#20026;0.75&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive manufacturing (AM) is gaining attention across various industries like healthcare, aerospace, and automotive. However, identifying defects early in the AM process can reduce production costs and improve productivity - a key challenge. This study explored the effectiveness of machine learning (ML) approaches, specifically transfer learning (TL) models, for defect detection in 3D-printed cylinders. Images of cylinders were analyzed using models including VGG16, VGG19, ResNet50, ResNet101, InceptionResNetV2, and MobileNetV2. Performance was compared across two datasets using accuracy, precision, recall, and F1-score metrics. In the first study, VGG16, InceptionResNetV2, and MobileNetV2 achieved perfect scores. In contrast, ResNet50 had the lowest performance, with an average F1-score of 0.32. Similarly, in the second study, MobileNetV2 correctly classified all instances, while ResNet50 struggled with more false positives and fewer true positives, resulting in an F1-score of 0.75.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08644</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#24314;&#27169;&#20013;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25968;&#21313;&#24180;&#26469;&#33268;&#21147;&#20110;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#26102;&#38388;&#24207;&#21015;&#28436;&#21270;&#30340;&#29289;&#29702;-&#27010;&#24565; (PC) &#27169;&#22411;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064; (ML) &#30340;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20174;ML&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#29289;&#29702;&#29702;&#35299;&#30340;&#22256;&#38590;&#20351;&#24471;&#20854;&#22312;&#22686;&#24378;&#23545;&#31995;&#32479;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#24212;&#29992;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#29289;&#29702;&#24615;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120; (MCP) &#20316;&#20026;&#24357;&#21512;PC&#27169;&#22411;&#21644;ML&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;MCP&#21033;&#29992;PC&#27169;&#22411;&#21644;GRNNs&#32972;&#21518;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#20869;&#22312;&#21516;&#26500;&#24615;&#65292;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26126;&#30830;&#34920;&#31034;&#29289;&#29702;&#36807;&#31243;&#30340;&#36136;&#37327;&#20445;&#25345;&#24615;&#36136;&#65292;&#21516;&#26102;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#21644;&#29616;&#25104;&#30340;ML&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36825;&#31181;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#65288;&#21487;&#35299;&#37322;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;&#21517;&#20026;&#8220;&#31163;&#23130;&#39044;&#27979;&#25968;&#25454;&#38598;&#8221;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25104;&#21151;&#20934;&#30830;&#20998;&#31867;&#23130;&#23035;&#21644;&#31163;&#23130;&#20154;&#32676;&#65292;&#20855;&#26377;98.5%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.08620</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#31163;&#23130;&#65306;&#27934;&#23519;&#21644;LIME&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Divorce Prediction with Machine Learning: Insights and LIME Interpretability. (arXiv:2310.08620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08620
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#21517;&#20026;&#8220;&#31163;&#23130;&#39044;&#27979;&#25968;&#25454;&#38598;&#8221;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25104;&#21151;&#20934;&#30830;&#20998;&#31867;&#23130;&#23035;&#21644;&#31163;&#23130;&#20154;&#32676;&#65292;&#20855;&#26377;98.5%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#23130;&#26159;&#20687;&#32654;&#22269;&#36825;&#26679;&#30340;&#21457;&#36798;&#22269;&#23478;&#20013;&#26368;&#24120;&#35265;&#30340;&#31038;&#20250;&#38382;&#39064;&#20043;&#19968;&#12290;&#26368;&#36817;&#30340;&#23130;&#23035;&#20013;&#26377;&#36817;50&#65285;&#36716;&#21464;&#20026;&#38750;&#33258;&#24895;&#30340;&#31163;&#23130;&#25110;&#20998;&#23621;&#12290;&#23613;&#31649;&#20154;&#20204;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20250;&#21457;&#29983;&#21464;&#21270;&#65292;&#29978;&#33267;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#20294;&#20687;&#31163;&#23130;&#36825;&#26679;&#30340;&#20107;&#20214;&#24182;&#27809;&#26377;&#25171;&#26029;&#20010;&#20154;&#30340;&#26085;&#24120;&#27963;&#21160;&#65307;&#28982;&#32780;&#65292;&#31163;&#23130;&#23545;&#20010;&#20154;&#30340;&#24515;&#29702;&#20581;&#24247;&#21644;&#20010;&#20154;&#29983;&#27963;&#20135;&#29983;&#20102;&#20005;&#37325;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#30340;&#33539;&#22260;&#20869;&#65292;&#36890;&#36807;&#35780;&#20272;&#21517;&#20026;&#8220;&#31163;&#23130;&#39044;&#27979;&#25968;&#25454;&#38598;&#8221;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20845;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#36923;&#36753;&#22238;&#24402;&#65288;LR&#65289;&#12289;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#12289;K-&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#12289;&#20998;&#31867;&#22238;&#24402;&#26641;&#65288;CART&#65289;&#12289;&#39640;&#26031;&#26420;&#32032;&#36125;&#21494;&#26031;&#65288;NB&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#65289;&#20934;&#30830;&#20998;&#31867;&#23130;&#23035;&#21644;&#31163;&#23130;&#20154;&#32676;&#12290;&#21021;&#27493;&#30340;&#35745;&#31639;&#32467;&#26524;&#34920;&#26126;&#65292;SVM&#12289;KNN&#21644;LDA&#31561;&#31639;&#27861;&#21487;&#20197;&#20197;98.5&#30340;&#20934;&#30830;&#29575;&#25191;&#34892;&#36825;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Divorce is one of the most common social issues in developed countries like in the United States. Almost 50% of the recent marriages turn into an involuntary divorce or separation. While it is evident that people vary to a different extent, and even over time, an incident like Divorce does not interrupt the individual's daily activities; still, Divorce has a severe effect on the individual's mental health, and personal life. Within the scope of this research, the divorce prediction was carried out by evaluating a dataset named by the 'divorce predictor dataset' to correctly classify between married and Divorce people using six different machine learning algorithms- Logistic Regression (LR), Linear Discriminant Analysis (LDA), K-Nearest Neighbors (KNN), Classification and Regression Trees (CART), Gaussian Na\"ive Bayes (NB), and, Support Vector Machines (SVM). Preliminary computational results show that algorithms such as SVM, KNN, and LDA, can perform that task with an accuracy of 98.5
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#35299;&#20915;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#30830;&#20445;&#20102;SafeDPA&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08602</link><description>&lt;p&gt;
&#23433;&#20840;&#28145;&#24230;&#31574;&#30053;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Safe Deep Policy Adaptation. (arXiv:2310.08602v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08602
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#35299;&#20915;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#30830;&#20445;&#20102;SafeDPA&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#26159;&#20351;&#33258;&#20027;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#21160;&#24577;&#21644;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#24555;&#36895;&#36866;&#24212;&#12290;&#32463;&#20856;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#23433;&#20840;&#25511;&#21046;&#25552;&#20379;&#20102;&#31283;&#23450;&#24615;&#21644;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#20294;&#20165;&#38480;&#20110;&#29305;&#23450;&#30340;&#31995;&#32479;&#31867;&#21035;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31574;&#30053;&#36866;&#24212;&#25552;&#20379;&#20102;&#36890;&#29992;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#20294;&#21516;&#26102;&#20063;&#24102;&#26469;&#20102;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SafeDPA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;RL&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#31574;&#30053;&#36866;&#24212;&#21644;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;SafeDPA&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#32852;&#21512;&#23398;&#20064;&#33258;&#36866;&#24212;&#31574;&#30053;&#21644;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#39044;&#27979;&#29615;&#22659;&#37197;&#32622;&#65292;&#24182;&#20351;&#29992;&#23569;&#37327;&#30495;&#23454;&#25968;&#25454;&#23545;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;RL&#31574;&#30053;&#20043;&#19978;&#24341;&#20837;&#22522;&#20110;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#22120;&#65292;&#20197;&#30830;&#20445;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;SafeDPA&#30340;&#29702;&#35770;&#23433;&#20840;&#24615;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;SafeDPA&#23545;&#23398;&#20064;&#35823;&#24046;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A critical goal of autonomy and artificial intelligence is enabling autonomous robots to rapidly adapt in dynamic and uncertain environments. Classic adaptive control and safe control provide stability and safety guarantees but are limited to specific system classes. In contrast, policy adaptation based on reinforcement learning (RL) offers versatility and generalizability but presents safety and robustness challenges. We propose SafeDPA, a novel RL and control framework that simultaneously tackles the problems of policy adaptation and safe reinforcement learning. SafeDPA jointly learns adaptive policy and dynamics models in simulation, predicts environment configurations, and fine-tunes dynamics models with few-shot real-world data. A safety filter based on the Control Barrier Function (CBF) on top of the RL policy is introduced to ensure safety during real-world deployment. We provide theoretical safety guarantees of SafeDPA and show the robustness of SafeDPA against learning errors 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#26426;&#32452;&#21551;&#20572;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#39044;&#27979;&#24120;&#35268;&#26426;&#32452;&#30340;&#21551;&#20572;&#20915;&#31574;&#65292;&#31995;&#32479;&#36816;&#33829;&#21830;&#21487;&#20197;&#22312;&#27714;&#35299;&#22120;&#20013;&#20351;&#29992;&#39044;&#28909;&#21551;&#21160;&#24182;&#26174;&#33879;&#21152;&#36895;&#35745;&#31639;&#12290;&#23545;&#20110;&#39044;&#27979;&#65292;&#20351;&#29992;&#20102;&#36866;&#24403;&#27491;&#21017;&#21270;&#30340;&#26680;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#65292;&#33021;&#23558;&#35745;&#31639;&#26102;&#38388;&#20943;&#23569;1.7&#20493;&#12290;</title><link>http://arxiv.org/abs/2310.08601</link><description>&lt;p&gt;
&#24102;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#26426;&#32452;&#21551;&#20572;&#39044;&#27979;&#22120;&#65306;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Unit Commitment Predictor With a Performance Guarantee: A Support Vector Machine Classifier. (arXiv:2310.08601v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#26426;&#32452;&#21551;&#20572;&#39044;&#27979;&#22120;&#65292;&#36890;&#36807;&#23398;&#20064;&#21644;&#39044;&#27979;&#24120;&#35268;&#26426;&#32452;&#30340;&#21551;&#20572;&#20915;&#31574;&#65292;&#31995;&#32479;&#36816;&#33829;&#21830;&#21487;&#20197;&#22312;&#27714;&#35299;&#22120;&#20013;&#20351;&#29992;&#39044;&#28909;&#21551;&#21160;&#24182;&#26174;&#33879;&#21152;&#36895;&#35745;&#31639;&#12290;&#23545;&#20110;&#39044;&#27979;&#65292;&#20351;&#29992;&#20102;&#36866;&#24403;&#27491;&#21017;&#21270;&#30340;&#26680;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#65292;&#33021;&#23558;&#35745;&#31639;&#26102;&#38388;&#20943;&#23569;1.7&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#36816;&#33829;&#21830;&#36890;&#24120;&#38656;&#35201;&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;&#26426;&#32452;&#21551;&#20572;&#38382;&#39064;&#36827;&#34892;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23637;&#31034;&#20102;&#36890;&#36807;&#23398;&#20064;&#21644;&#39044;&#27979;&#24120;&#35268;&#26426;&#32452;&#30340;&#21551;&#20572;&#20915;&#31574;&#65292;&#31995;&#32479;&#36816;&#33829;&#21830;&#26377;&#21487;&#33021;&#22312;&#27714;&#35299;&#22120;&#20013;&#20351;&#29992;&#39044;&#28909;&#21551;&#21160;&#24182;&#26174;&#33879;&#21152;&#36895;&#35745;&#31639;&#12290;&#23545;&#20110;&#39044;&#27979;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#32447;&#24615;&#21644;&#26680;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#20998;&#31867;&#22120;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26679;&#26412;&#22806;&#24615;&#33021;&#20445;&#35777;&#65288;&#22914;&#26524;&#36866;&#24403;&#27491;&#21017;&#21270;&#65289;&#65292;&#36716;&#21270;&#20026;&#20998;&#24067;&#40065;&#26834;&#20998;&#31867;&#22120;&#12290;&#23545;&#20110;&#26426;&#32452;&#21551;&#20572;&#38382;&#39064;&#65292;&#25105;&#20204;&#27714;&#35299;&#20102;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#20108;&#38454;&#38181;&#38382;&#39064;&#12290;&#22522;&#20110;IEEE 6&#33410;&#28857;&#21644;118&#33410;&#28857;&#27979;&#35797;&#31995;&#32479;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36866;&#24403;&#27491;&#21017;&#21270;&#30340;&#26680;&#21270;&#25903;&#25345;&#21521;&#37327;&#26426;&#20248;&#20110;&#20854;&#20182;&#20998;&#31867;&#22120;&#65292;&#23558;&#35745;&#31639;&#26102;&#38388;&#20943;&#23569;&#20102;1.7&#20493;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#23384;&#22312;&#20005;&#26684;&#30340;&#35745;&#31639;&#38480;&#21046;&#65292;&#27809;&#26377;&#39044;&#28909;&#21551;&#21160;&#30340;&#26426;&#32452;&#21551;&#20572;&#38382;&#39064;&#19982;&#26368;&#20248;&#35299;&#30340;&#36317;&#31163;&#36739;&#36828;&#12290;
&lt;/p&gt;
&lt;p&gt;
The system operators usually need to solve large-scale unit commitment problems within limited time frame for computation. This paper provides a pragmatic solution, showing how by learning and predicting the on/off commitment decisions of conventional units, there is a potential for system operators to warm start their solver and speed up their computation significantly. For the prediction, we train linear and kernelized support vector machine classifiers, providing an out-of-sample performance guarantee if properly regularized, converting to distributionally robust classifiers. For the unit commitment problem, we solve a mixed-integer second-order cone problem. Our results based on the IEEE 6-bus and 118-bus test systems show that the kernelized SVM with proper regularization outperforms other classifiers, reducing the computational time by a factor of 1.7. In addition, if there is a tight computational limit, while the unit commitment problem without warm start is far away from the o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;TD3&#31639;&#27861;&#30340;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#65292;&#22312;CARLA&#27169;&#25311;&#24179;&#21488;&#20013;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;T&#22411;&#36335;&#21475;&#23548;&#33322;&#20013;&#31283;&#23450;&#25910;&#25947;&#21644;&#25913;&#36827;&#23433;&#20840;&#24615;&#33021;&#65292;&#24182;&#22312;&#34892;&#31243;&#24310;&#35823;&#12289;&#30896;&#25758;&#20943;&#23569;&#21644;&#24635;&#20307;&#25104;&#26412;&#31561;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.08595</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#20132;&#21449;&#36335;&#21475;&#23548;&#33322;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Autonomous Vehicle Intersection Navigation. (arXiv:2310.08595v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;TD3&#31639;&#27861;&#30340;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#65292;&#22312;CARLA&#27169;&#25311;&#24179;&#21488;&#20013;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;T&#22411;&#36335;&#21475;&#23548;&#33322;&#20013;&#31283;&#23450;&#25910;&#25947;&#21644;&#25913;&#36827;&#23433;&#20840;&#24615;&#33021;&#65292;&#24182;&#22312;&#34892;&#31243;&#24310;&#35823;&#12289;&#30896;&#25758;&#20943;&#23569;&#21644;&#24635;&#20307;&#25104;&#26412;&#31561;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#23494;&#38598;&#20132;&#36890;&#22330;&#26223;&#20013;&#65292;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#22312;&#22797;&#26434;T&#22411;&#36335;&#21475;&#23548;&#33322;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#23454;&#26102;&#22320;&#20351;AVs&#20570;&#20986;&#23433;&#20840;&#39640;&#25928;&#30340;&#20915;&#31574;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#21452;&#24310;&#36831;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;TD3&#65289;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20302;&#25104;&#26412;&#12289;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;T&#22411;&#36335;&#21475;&#19978;&#30340;&#39640;&#25928;&#23433;&#20840;&#23548;&#33322;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25105;&#20204;&#22312;CARLA&#27169;&#25311;&#24179;&#21488;&#19978;&#23545;&#25105;&#20204;&#30340;TD3&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#65292;&#35813;&#26041;&#27861;&#21576;&#29616;&#20986;&#31283;&#23450;&#30340;&#25910;&#25947;&#24615;&#21644;&#25913;&#36827;&#30340;&#23433;&#20840;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#20132;&#36890;&#23494;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;AV&#33021;&#22815;&#26377;&#25928;&#22320;&#23548;&#33322;T&#22411;&#36335;&#21475;&#65292;&#22312;&#34892;&#31243;&#24310;&#35823;&#12289;&#30896;&#25758;&#20943;&#23569;&#21644;&#24635;&#20307;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#23545;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#39046;&#22495;&#30340;&#24212;&#29992;&#36129;&#29486;&#20102;&#26032;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the challenges associated with navigating complex T-intersections in dense traffic scenarios for autonomous vehicles (AVs). Reinforcement learning algorithms have emerged as a promising approach to address these challenges by enabling AVs to make safe and efficient decisions in real-time. Here, we address the problem of efficiently and safely navigating T-intersections using a lower-cost, single-agent approach based on the Twin Delayed Deep Deterministic Policy Gradient (TD3) reinforcement learning algorithm. We show that our TD3-based method, when trained and tested in the CARLA simulation platform, demonstrates stable convergence and improved safety performance in various traffic densities. Our results reveal that the proposed approach enables the AV to effectively navigate T-intersections, outperforming previous methods in terms of travel delays, collision minimization, and overall cost. This study contributes to the growing body of knowledge on reinforceme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.08475</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#19982;&#32534;&#36753;&#21333;&#27169;&#24335;LLMs&#30456;&#27604;&#65292;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32534;&#36753;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#23457;&#26597;&#21644;&#24910;&#37325;&#32771;&#34385;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;MMEdit&#65292;&#29992;&#20110;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21253;&#25324;&#21508;&#31181;&#27169;&#22411;&#32534;&#36753;&#22522;&#32447;&#30340;&#32508;&#21512;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#30340;&#19981;&#21516;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#22522;&#32447;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#20294;&#25928;&#26524;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/EasyEdit&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CausalVul&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24615;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#25200;&#21160;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#27169;&#22411;&#19981;&#31283;&#23450;&#21644;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07958</link><description>&lt;p&gt;
&#36808;&#21521;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Causal Deep Learning for Vulnerability Detection. (arXiv:2310.07958v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28431;&#27934;&#26816;&#27979;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;CausalVul&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24615;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#25200;&#21160;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#27169;&#22411;&#19981;&#31283;&#23450;&#21644;&#27867;&#21270;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#28431;&#27934;&#26816;&#27979;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38459;&#30861;&#20854;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#29992;&#30340;&#37325;&#35201;&#25361;&#25112;&#26159;&#27169;&#22411;&#22312;&#25200;&#21160;&#19979;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24212;&#29992;&#21040;&#26410;&#35265;&#36807;&#30340;&#39033;&#30446;&#19978;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26159;&#22240;&#20026;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#38750;&#31283;&#23450;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#21464;&#37327;&#21517;&#65292;&#19982;&#26631;&#31614;&#20855;&#26377;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#24403;&#25200;&#21160;&#21644;OOD&#25968;&#25454;&#38598;&#19981;&#20877;&#20855;&#26377;&#30456;&#21516;&#30340;&#34394;&#20551;&#29305;&#24449;&#26102;&#65292;&#27169;&#22411;&#39044;&#27979;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#24615;&#24341;&#20837;&#20102;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#26816;&#27979;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;CausalVul&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#25200;&#21160;&#26469;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#29992;&#20110;&#36827;&#34892;&#39044;&#27979;&#30340;&#34394;&#20551;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20043;&#19978;&#24212;&#29992;&#20102;&#22240;&#26524;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;do-&#35745;&#31639;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this paper, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to sys
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.07805</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20301;&#38543;&#26426;&#26725;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26159;&#29992;&#20110;&#36830;&#32493;&#36755;&#20837;&#30340;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;DMs&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#65288;&#21363;&#20301;&#32622;&#31354;&#38388;&#65289;&#20013;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21453;&#28436;&#26469;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20301;&#31354;&#38388;&#21160;&#21147;&#23398;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#20854;&#20013;&#30456;&#20301;&#31354;&#38388;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#21253;&#25324;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#22686;&#24378;&#31354;&#38388;&#12290;&#21033;&#29992;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#30456;&#20301;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#27979;&#24230;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37319;&#26679;&#12290;&#19982;DMs&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#23601;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#12290;&#36825;&#31181;&#26089;&#26399;&#39044;&#27979;&#20026;&#36890;&#36807;&#27839;&#36712;&#36857;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23567;&#20989;&#25968;&#35780;&#20272;&#25968;&#37327;&#30340;&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#22885;&#36187;&#32599;-GPT&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#32447;&#24615;&#34920;&#31034;&#23545;&#31435;&#26827;&#23376;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#32447;&#24615;&#19990;&#30028;&#34920;&#31034;&#21644;&#22240;&#26524;&#20915;&#31574;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21450;&#20854;&#19982;&#23618;&#28145;&#24230;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07582</link><description>&lt;p&gt;
&#31616;&#21333;&#21464;&#21387;&#22120;&#20013;&#30340;&#32447;&#24615;&#28508;&#22312;&#19990;&#30028;&#27169;&#22411;: &#22885;&#36187;&#32599;-GPT&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT. (arXiv:2310.07582v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#22885;&#36187;&#32599;-GPT&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#32447;&#24615;&#34920;&#31034;&#23545;&#31435;&#26827;&#23376;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#32447;&#24615;&#19990;&#30028;&#34920;&#31034;&#21644;&#22240;&#26524;&#20915;&#31574;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21450;&#20854;&#19982;&#23618;&#28145;&#24230;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#22312;&#20915;&#31574;&#21644;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#23545;&#19990;&#30028;&#30340;&#30495;&#27491;&#29702;&#35299;&#19982;&#32431;&#31929;&#30340;&#38543;&#26426;&#27169;&#20223;&#30456;&#23545;&#30340;&#35752;&#35770;&#25345;&#32493;&#23384;&#22312;&#12290;&#26412;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#22312;&#22885;&#36187;&#32599;&#20013;&#35757;&#32451;&#30340;&#31616;&#21333;&#21464;&#21387;&#22120;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#20197;&#22686;&#24378;&#23545;&#22885;&#36187;&#32599;-GPT&#26032;&#20852;&#19990;&#30028;&#27169;&#22411;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22885;&#36187;&#32599;-GPT&#21253;&#21547;&#20102;&#23545;&#31435;&#26827;&#23376;&#30340;&#32447;&#24615;&#34920;&#31034;&#65292;&#36825;&#19968;&#22240;&#32032;&#22312;&#39537;&#21160;&#20854;&#20915;&#31574;&#36807;&#31243;&#20013;&#36215;&#21040;&#22240;&#26524;&#24615;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#38416;&#26126;&#20102;&#32447;&#24615;&#19990;&#30028;&#34920;&#31034;&#19982;&#22240;&#26524;&#20915;&#31574;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;&#23618;&#28145;&#24230;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#24050;&#32463;&#23558;&#20195;&#30721;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models exhibit significant capabilities in decision-making and logical deductions. Nonetheless, a continuing discourse persists regarding their genuine understanding of the world as opposed to mere stochastic mimicry. This paper meticulously examines a simple transformer trained for Othello, extending prior research to enhance comprehension of the emergent world model of Othello-GPT. The investigation reveals that Othello-GPT encapsulates a linear representation of opposing pieces, a factor that causally steers its decision-making process. This paper further elucidates the interplay between the linear world representation and causal decision-making, and their dependence on layer depth and model complexity. We have made the code public.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#23558;&#25968;&#25454;&#22810;&#26679;&#24615;&#32622;&#20110;&#25968;&#25454;&#36136;&#37327;&#20043;&#19978;&#30340;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.07321</link><description>&lt;p&gt;
&#20851;&#20110;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Cross-Domain Data on German Language Models. (arXiv:2310.07321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#24503;&#35821;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#23558;&#25968;&#25454;&#22810;&#26679;&#24615;&#32622;&#20110;&#25968;&#25454;&#36136;&#37327;&#20043;&#19978;&#30340;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35201;&#20040;&#22312;&#36890;&#29992;&#32593;&#32476;&#25235;&#21462;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#35201;&#20040;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#19978;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#31361;&#26174;&#20102;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#32771;&#23519;&#25968;&#25454;&#22810;&#26679;&#24615;&#39640;&#20110;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#39046;&#22495;&#25991;&#26412;&#30340;&#24503;&#35821;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#26088;&#22312;&#21253;&#21547;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21442;&#25968;&#33539;&#22260;&#20174;122M&#21040;750M&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20132;&#21449;&#39046;&#22495;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20351;&#29992;&#36136;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#20808;&#21069;&#26368;&#20808;&#36827;&#32467;&#26524;&#19978;&#25552;&#20986;&#20102;&#39640;&#36798;4.45%&#30340;&#25913;&#36827;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;https://huggingface.co/ikim-uk-essen&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, large language models have been either trained on general web crawls or domain-specific data. However, recent successes of generative large language models, have shed light on the benefits of cross-domain datasets. To examine the significance of prioritizing data diversity over quality, we present a German dataset comprising texts from five domains, along with another dataset aimed at containing high-quality data. Through training a series of models ranging between 122M and 750M parameters on both datasets, we conduct a comprehensive benchmark on multiple downstream tasks. Our findings demonstrate that the models trained on the cross-domain dataset outperform those trained on quality data alone, leading to improvements up to $4.45\%$ over the previous state-of-the-art. The models are available at https://huggingface.co/ikim-uk-essen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#20171;&#32461;&#26368;&#26032;&#30340;&#35270;&#35273;&#35745;&#31639;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#28085;&#30422;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#27010;&#24565;&#21644;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#24635;&#32467;&#20102;&#20010;&#20154;&#21270;&#12289;&#26465;&#20214;&#32422;&#26463;&#21644;&#21453;&#28436;&#31561;&#37325;&#35201;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.07204</link><description>&lt;p&gt;
&#35270;&#35273;&#35745;&#31639;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
State of the Art on Diffusion Models for Visual Computing. (arXiv:2310.07204v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07204
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#20171;&#32461;&#26368;&#26032;&#30340;&#35270;&#35273;&#35745;&#31639;&#39046;&#22495;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#28085;&#30422;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#27010;&#24565;&#21644;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#24635;&#32467;&#20102;&#20010;&#20154;&#21270;&#12289;&#26465;&#20214;&#32422;&#26463;&#21644;&#21453;&#28436;&#31561;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#65292;&#35270;&#35273;&#35745;&#31639;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#23427;&#20026;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;3D&#22330;&#26223;&#30340;&#29983;&#25104;&#12289;&#32534;&#36753;&#21644;&#37325;&#24314;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#26159;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#39318;&#36873;&#26550;&#26500;&#12290;&#20165;&#22312;&#36807;&#21435;&#19968;&#24180;&#20013;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#24037;&#20855;&#21644;&#24212;&#29992;&#30340;&#25991;&#29486;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#24182;&#19988;&#28041;&#21450;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#30456;&#20851;&#35770;&#25991;&#27599;&#22825;&#37117;&#22312;arXiv&#19978;&#21457;&#34920;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#24555;&#36895;&#22686;&#38271;&#20351;&#24471;&#36319;&#19978;&#25152;&#26377;&#26368;&#26032;&#21457;&#23637;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#20221;&#26368;&#26032;&#25216;&#26415;&#25253;&#21578;&#65288;STAR&#65289;&#30340;&#30446;&#26631;&#26159;&#20171;&#32461;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#25968;&#23398;&#27010;&#24565;&#12289;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#29616;&#32454;&#33410;&#21644;&#35774;&#35745;&#36873;&#25321;&#65292;&#20197;&#21450;&#27010;&#36848;&#36825;&#20123;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#12289;&#26465;&#20214;&#32422;&#26463;&#12289;&#21453;&#28436;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Mor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#35757;&#32451;&#20998;&#24067;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#19981;&#21305;&#37197;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07171</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#20998;&#24067;&#22810;&#26679;&#21270;&#23454;&#29616;&#32852;&#37030;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Federated Generalization via Information-Theoretic Distribution Diversification. (arXiv:2310.07171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07171
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#20851;&#27880;&#35757;&#32451;&#20998;&#24067;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#19981;&#21305;&#37197;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#22312;&#26080;&#38656;&#30452;&#25509;&#25968;&#25454;&#20849;&#20139;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#21516;&#27169;&#22411;&#35757;&#32451;&#30340;&#33021;&#21147;&#32780;&#26085;&#30410;&#31361;&#20986;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#31471;&#20043;&#38388;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24040;&#22823;&#24046;&#24322;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25361;&#25112;&#65292;&#23545;FL&#30340;&#27867;&#21270;&#33021;&#21147;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#24403;&#24182;&#38750;&#25152;&#26377;&#23458;&#25143;&#31471;&#37117;&#21442;&#19982;&#35757;&#32451;&#36807;&#31243;&#26102;&#65292;&#24773;&#20917;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#36825;&#26159;&#30001;&#20110;&#19981;&#31283;&#23450;&#30340;&#32593;&#32476;&#36830;&#25509;&#25110;&#26377;&#38480;&#30340;&#35745;&#31639;&#33021;&#21147;&#32780;&#24120;&#35265;&#12290;&#36825;&#21487;&#33021;&#26497;&#22823;&#22320;&#22797;&#26434;&#21270;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#22823;&#37327;&#30740;&#31350;&#38598;&#20013;&#22312;&#28041;&#21450;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#30340;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#24046;&#36317;&#38382;&#39064;&#19978;&#65292;&#20294;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#20998;&#24067;&#21644;&#38750;&#21442;&#19982;&#23458;&#25143;&#31471;&#30340;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#21364;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic genera
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#22686;&#24378;&#30340;&#26102;&#24577;&#28857;&#36807;&#31243;&#65288;PromptTPP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#27969;&#24335;&#20107;&#20214;&#24207;&#21015;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.04993</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#22686;&#24378;&#30340;&#26102;&#24577;&#28857;&#36807;&#31243;&#29992;&#20110;&#27969;&#24335;&#20107;&#20214;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Prompt-augmented Temporal Point Process for Streaming Event Sequence. (arXiv:2310.04993v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04993
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#22686;&#24378;&#30340;&#26102;&#24577;&#28857;&#36807;&#31243;&#65288;PromptTPP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#27969;&#24335;&#20107;&#20214;&#24207;&#21015;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26102;&#24577;&#28857;&#36807;&#31243;&#65288;TPP&#65289;&#26159;&#24314;&#27169;&#36830;&#32493;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#65288;&#22914;&#32593;&#32476;&#29992;&#25143;&#27963;&#21160;&#21644;&#37329;&#34701;&#20132;&#26131;&#65289;&#30340;&#20027;&#35201;&#33539;&#20363;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20107;&#20214;&#25968;&#25454;&#36890;&#24120;&#20197;&#27969;&#24335;&#26041;&#24335;&#25509;&#25910;&#65292;&#27169;&#24335;&#30340;&#20998;&#24067;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#38544;&#31169;&#21644;&#20869;&#23384;&#38480;&#21046;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#20063;&#24456;&#24120;&#35265;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#36830;&#32493;&#30417;&#27979;TPP&#20197;&#23398;&#20064;&#27969;&#24335;&#20107;&#20214;&#24207;&#21015;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#37319;&#29992;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#29616;&#23454;&#32422;&#26463;&#19979;&#36830;&#32493;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#30456;&#24212;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;PromptTPP&#65292;&#36890;&#36807;&#23558;&#22522;&#26412;TPP&#19982;&#19968;&#20010;&#25552;&#31034;&#26426;&#21046;&#36827;&#34892;&#25972;&#21512;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Temporal Point Processes (TPPs) are the prevalent paradigm for modeling continuous-time event sequences, such as user activities on the web and financial transactions. In real-world applications, event data is typically received in a \emph{streaming} manner, where the distribution of patterns may shift over time. Additionally, \emph{privacy and memory constraints} are commonly observed in practical scenarios, further compounding the challenges. Therefore, the continuous monitoring of a TPP to learn the streaming event sequence is an important yet under-explored problem. Our work paper addresses this challenge by adopting Continual Learning (CL), which makes the model capable of continuously learning a sequence of tasks without catastrophic forgetting under realistic constraints. Correspondingly, we propose a simple yet effective framework, PromptTPP\footnote{Our code is available at {\small \url{ https://github.com/yanyanSann/PromptTPP}}}, by integrating the base TPP with a cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-GNN&#12290;&#23427;&#21033;&#29992;LLMs&#23545;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;GNN&#33021;&#22815;&#23545;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.04668</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMS&#65289;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Label-free Node Classification on Graphs with Large Language Models (LLMS). (arXiv:2310.04668v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#26080;&#26631;&#31614;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21363;LLM-GNN&#12290;&#23427;&#21033;&#29992;LLMs&#23545;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#27880;&#37322;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#24471;GNN&#33021;&#22815;&#23545;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#20805;&#20998;&#21457;&#25381;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;Graph Neural Networks&#65292;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#30830;&#20445;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#22312;&#25991;&#26412;&#23646;&#24615;&#22270;&#19978;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;&#26679;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#39640;&#25928;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#25512;&#29702;&#25104;&#26412;&#36739;&#39640;&#12290;&#37492;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26080;&#26631;&#31614;&#22270;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;LLM-GNN&#12290;&#23427;&#38598;&#25104;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLMs&#34987;&#29992;&#26469;&#27880;&#37322;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;LLMs&#30340;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;GNNs&#33021;&#22815;&#39044;&#27979;&#20854;&#20313;&#22823;&#37096;&#20998;&#33410;&#28857;&#12290;LLM-GNN&#30340;&#23454;&#29616;&#38754;&#20020;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#25105;&#20204;&#22914;&#20309;&#20027;&#21160;&#36873;&#25321;&#35201;&#30001;LLMs&#27880;&#37322;&#30340;&#33410;&#28857;&#65292;&#20174;&#32780;&#22686;&#24378;GNN&#30340;&#35757;&#32451;&#65311;&#25105;&#20204;&#22914;&#20309;&#21033;&#29992;LLMs&#26469;&#20248;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22788;&#29702;&#65311;
&lt;/p&gt;
&lt;p&gt;
In recent years, there have been remarkable advancements in node classification achieved by Graph Neural Networks (GNNs). However, they necessitate abundant high-quality labels to ensure promising performance. In contrast, Large Language Models (LLMs) exhibit impressive zero-shot proficiency on text-attributed graphs. Yet, they face challenges in efficiently processing structural data and suffer from high inference costs. In light of these observations, this work introduces a label-free node classification on graphs with LLMs pipeline, LLM-GNN. It amalgamates the strengths of both GNNs and LLMs while mitigating their limitations. Specifically, LLMs are leveraged to annotate a small portion of nodes and then GNNs are trained on LLMs' annotations to make predictions for the remaining large portion of nodes. The implementation of LLM-GNN faces a unique challenge: how can we actively select nodes for LLMs to annotate and consequently enhance the GNN training? How can we leverage LLMs to ob
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2310.03986</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#65292;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation. (arXiv:2310.03986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03986
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#21442;&#25968;&#39640;&#25928;&#36866;&#24212;&#31243;&#24207;&#65292;&#20197;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#29420;&#31435;&#30340;&#19987;&#38376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26088;&#22312;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#26469;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#19968;&#20123;&#30456;&#20851;&#30340;&#27169;&#24577;&#20013;&#35266;&#23519;&#21040;&#65292;&#22914;&#26524;&#22312;&#27979;&#35797;&#26102;&#38388;&#32570;&#23569;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#32570;&#22833;&#27169;&#24577;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#30340;&#31616;&#21333;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#24212;&#31243;&#24207;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20302;&#31209;&#36866;&#24212;&#21644;&#20013;&#38388;&#29305;&#24449;&#30340;&#35843;&#21046;&#26469;&#34917;&#20607;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#31181;&#36866;&#24212;&#21487;&#20197;&#37096;&#20998;&#24357;&#34917;&#30001;&#20110;&#32570;&#22833;&#27169;&#24577;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32988;&#36807;&#38024;&#23545;&#21487;&#29992;&#27169;&#24577;&#32452;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#29420;&#31435;&#30340;&#12289;&#19987;&#38376;&#30340;&#32593;&#32476;&#12290;&#25152;&#25552;&#20986;&#30340;&#36866;&#24212;&#25152;&#38656;&#30340;&#21442;&#25968;&#38750;&#24120;&#23569;&#65288;&#20363;&#22914;&#65292;&#23569;&#20110;&#65289;
&lt;/p&gt;
&lt;p&gt;
Multimodal learning seeks to utilize data from multiple sources to improve the overall performance of downstream tasks. It is desirable for redundancies in the data to make multimodal systems robust to missing or corrupted observations in some correlated modalities. However, we observe that the performance of several existing multimodal networks significantly deteriorates if one or multiple modalities are absent at test time. To enable robustness to missing modalities, we propose simple and parameter-efficient adaptation procedures for pretrained multimodal networks. In particular, we exploit low-rank adaptation and modulation of intermediate features to compensate for the missing modalities. We demonstrate that such adaptation can partially bridge performance drop due to missing modalities and outperform independent, dedicated networks trained for the available modality combinations in some cases. The proposed adaptation requires extremely small number of parameters (e.g., fewer than 
&lt;/p&gt;</description></item><item><title>SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.03684</link><description>&lt;p&gt;
SmoothLLM&#65306;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03684
&lt;/p&gt;
&lt;p&gt;
SmoothLLM&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#25552;&#31034;&#19978;&#38543;&#26426;&#25200;&#21160;&#24182;&#27719;&#24635;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21162;&#21147;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#65288;&#22914;GPT&#12289;Llama&#12289;Claude&#21644;PaLM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#65292;&#21363;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#27450;&#39575;&#65292;&#20197;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#28431;&#27934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SmoothLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20943;&#36731;LLM&#19978;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#23545;&#25239;&#24615;&#29983;&#25104;&#30340;&#25552;&#31034;&#23545;&#23383;&#31526;&#32423;&#21035;&#30340;&#25913;&#21464;&#24456;&#33030;&#24369;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#39318;&#20808;&#38543;&#26426;&#25200;&#21160;&#32473;&#23450;&#36755;&#20837;&#25552;&#31034;&#30340;&#22810;&#20010;&#21103;&#26412;&#65292;&#28982;&#21518;&#27719;&#24635;&#30456;&#24212;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;SmoothLLM&#23558;&#20247;&#22810;&#28909;&#38376;LLM&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#33267;&#19981;&#21040;&#19968;&#20010;&#30334;&#20998;&#28857;&#65292;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#20445;&#23432;&#24615;&#65292;&#24182;&#23545;&#25915;&#20987;&#32531;&#35299;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#20351;&#29992;&#30340;&#26597;&#35810;&#25968;&#37327;&#27604;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#19982;&#20219;&#20309;LLM&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.
&lt;/p&gt;</description></item><item><title>BioBridge&#26159;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#26725;&#25509;&#21333;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BioBridge&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#32988;&#36807;&#26368;&#20339;&#22522;&#32447;KG&#23884;&#20837;&#26041;&#27861;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03320</link><description>&lt;p&gt;
BioBridge: &#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#26725;&#25509;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph. (arXiv:2310.03320v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03320
&lt;/p&gt;
&lt;p&gt;
BioBridge&#26159;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#26725;&#25509;&#21333;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BioBridge&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#32988;&#36807;&#26368;&#20339;&#22522;&#32447;KG&#23884;&#20837;&#26041;&#27861;&#65292;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;(FMs)&#33021;&#22815;&#21033;&#29992;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;FMs&#20027;&#35201;&#20173;&#22788;&#20110;&#21333;&#27169;&#24577;&#29366;&#24577;&#65292;&#21363;&#29420;&#31435;&#35757;&#32451;&#24182;&#29992;&#20110;&#22788;&#29702;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#23567;&#20998;&#23376;&#32467;&#26500;&#25110;&#20020;&#24202;&#25968;&#25454;&#31561;&#21333;&#19968;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#29983;&#29289;&#21307;&#23398;FMs&#30340;&#36825;&#31181;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;&#26694;&#26550;BioBridge&#65292;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;(KG)&#26469;&#23398;&#20064;&#19981;&#38656;&#35201;&#24494;&#35843;&#20219;&#20309;&#24213;&#23618;&#21333;&#27169;&#24577;FMs&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#26725;&#25509;&#29420;&#31435;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;FMs&#20197;&#24314;&#31435;&#22810;&#27169;&#24577;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;BioBridge&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#21487;&#20197;&#20987;&#36133;&#26368;&#20339;&#22522;&#32447;KG&#23884;&#20837;&#26041;&#27861;&#65288;&#24179;&#22343;&#25552;&#39640;&#32422;76.3%&#65289;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;BioBridge&#34920;&#29616;&#20986;&#39046;&#22495;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#30340;&#27169;&#24577;&#25110;&#20851;&#31995;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs. Our empirical results demonstrate that BioBridge can beat the best baseline KG embedding methods (on average by around 76.3%) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#36890;&#36807;&#35748;&#30693;&#36335;&#24452;&#21644;&#31639;&#27861;&#26469;&#23454;&#29616;&#20840;&#38754;&#25512;&#29702;&#21644;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#36716;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#22312;&#21333;&#36710;&#20219;&#21153;&#21644;&#22797;&#26434;&#39550;&#39542;&#34892;&#20026;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36825;&#26159;&#22240;&#20026;&#20854;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03026</link><description>&lt;p&gt;
LanguageMPC&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#32773;
&lt;/p&gt;
&lt;p&gt;
LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving. (arXiv:2310.03026v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#36890;&#36807;&#35748;&#30693;&#36335;&#24452;&#21644;&#31639;&#27861;&#26469;&#23454;&#29616;&#20840;&#38754;&#25512;&#29702;&#21644;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#36716;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#33021;&#22815;&#22312;&#21333;&#36710;&#20219;&#21153;&#21644;&#22797;&#26434;&#39550;&#39542;&#34892;&#20026;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#36825;&#26159;&#22240;&#20026;&#20854;&#20855;&#26377;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22312;&#29702;&#35299;&#39640;&#32423;&#20449;&#24687;&#12289;&#25512;&#24191;&#32597;&#35265;&#20107;&#20214;&#21644;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#22797;&#26434;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#30340;&#20915;&#31574;&#32452;&#20214;&#65292;&#38656;&#35201;&#20154;&#31867;&#24120;&#35782;&#29702;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#35748;&#30693;&#36335;&#24452;&#65292;&#20351;LLMs&#33021;&#22815;&#36827;&#34892;&#20840;&#38754;&#25512;&#29702;&#65292;&#24182;&#24320;&#21457;&#20102;&#23558;LLM&#20915;&#31574;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;&#39550;&#39542;&#25351;&#20196;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;LLM&#20915;&#31574;&#36890;&#36807;&#24341;&#23548;&#21442;&#25968;&#30697;&#38453;&#36866;&#24212;&#19982;&#20302;&#32423;&#25511;&#21046;&#22120;&#26080;&#32541;&#38598;&#25104;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#21333;&#36710;&#20219;&#21153;&#20013;&#22987;&#32456;&#36229;&#36234;&#22522;&#32447;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#33021;&#22788;&#29702;&#22797;&#26434;&#30340;&#39550;&#39542;&#34892;&#20026;&#65292;&#29978;&#33267;&#22810;&#36710;&#21327;&#35843;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;LLMs&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;LLMs&#20316;&#20026;&#26377;&#25928;&#20915;&#31574;&#32773;&#30340;&#21021;&#27493;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-make
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#20307;&#25163;&#37096;&#20449;&#24687;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;H-InDex&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#22256;&#38590;&#30340;&#24039;&#22937;&#25805;&#32437;&#20219;&#21153;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;H-InDex&#26126;&#26174;&#20248;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.01404</link><description>&lt;p&gt;
H-InDex&#65306;&#22522;&#20110;&#25163;&#37096;&#20449;&#24687;&#30340;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#22312;&#24039;&#22937;&#25805;&#32437;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
H-InDex: Visual Reinforcement Learning with Hand-Informed Representations for Dexterous Manipulation. (arXiv:2310.01404v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01404
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#20307;&#25163;&#37096;&#20449;&#24687;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;H-InDex&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#22256;&#38590;&#30340;&#24039;&#22937;&#25805;&#32437;&#20219;&#21153;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;H-InDex&#26126;&#26174;&#20248;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#30340;&#25163;&#20855;&#26377;&#21331;&#36234;&#30340;&#28789;&#24039;&#24615;&#65292;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#28789;&#24863;&#26469;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#20307;$\textbf{H}$and$\textbf{-In}$formed&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#22256;&#38590;&#30340;$\textbf{Dex}$terous&#25805;&#32437;&#20219;&#21153;&#65288;$\textbf{H-InDex}$&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#20351;&#29992;3D&#20154;&#25163;&#23039;&#21183;&#20272;&#35745;&#36827;&#34892;&#39044;&#35757;&#32451;&#34920;&#31034;&#65292;&#65288;ii&#65289;&#20351;&#29992;&#33258;&#30417;&#30563;&#20851;&#38190;&#28857;&#26816;&#27979;&#36827;&#34892;&#31163;&#32447;&#33258;&#36866;&#24212;&#34920;&#31034;&#65292;&#21644;&#65288;iii&#65289;&#20351;&#29992;&#25351;&#25968;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;BatchNorm&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#21518;&#20004;&#20010;&#38454;&#27573;&#20165;&#20462;&#25913;&#39044;&#35757;&#32451;&#34920;&#31034;&#30340;&#24635;&#21442;&#25968;&#30340; $0.36\%$&#65292;&#30830;&#20445;&#20445;&#30041;&#20102;&#26469;&#33258;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;12&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24039;&#22937;&#25805;&#32437;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;H-InDex&#26126;&#26174;&#20248;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#29992;&#20110;&#36816;&#21160;&#25511;&#21046;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#12290; &#20195;&#30721;&#20301;&#20110;https://yanjieze.com/H
&lt;/p&gt;
&lt;p&gt;
Human hands possess remarkable dexterity and have long served as a source of inspiration for robotic manipulation. In this work, we propose a human $\textbf{H}$and$\textbf{-In}$formed visual representation learning framework to solve difficult $\textbf{Dex}$terous manipulation tasks ($\textbf{H-InDex}$) with reinforcement learning. Our framework consists of three stages: (i) pre-training representations with 3D human hand pose estimation, (ii) offline adapting representations with self-supervised keypoint detection, and (iii) reinforcement learning with exponential moving average BatchNorm. The last two stages only modify $0.36\%$ parameters of the pre-trained representation in total, ensuring the knowledge from pre-training is maintained to the full extent. We empirically study 12 challenging dexterous manipulation tasks and find that H-InDex largely surpasses strong baseline methods and the recent visual foundation models for motor control. Code is available at https://yanjieze.com/H
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#27169;&#20223;&#23398;&#20064;&#20013;&#31070;&#32463;&#20195;&#29702;&#30340;&#21160;&#24577;DAG&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#21521;&#26080;&#29615;&#22240;&#26524;&#22270;&#23637;&#29616;&#20854;&#25429;&#33719;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#21152;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00489</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#27169;&#20223;&#23398;&#20064;&#30340;&#21160;&#24577;DAG&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Dynamic DAG Discovery for Interpretable Imitation Learning. (arXiv:2310.00489v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#27169;&#20223;&#23398;&#20064;&#20013;&#31070;&#32463;&#20195;&#29702;&#30340;&#21160;&#24577;DAG&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#21521;&#26080;&#29615;&#22240;&#26524;&#22270;&#23637;&#29616;&#20854;&#25429;&#33719;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#21152;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#30340;&#31034;&#33539;&#26469;&#23398;&#20064;&#20195;&#29702;&#31574;&#30053;&#65292;&#22312;&#21307;&#30103;&#27835;&#30103;&#26041;&#26696;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#20195;&#29702;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22256;&#38590;&#20027;&#35201;&#26469;&#33258;&#20004;&#20010;&#26041;&#38754;&#65306;1&#65289;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#36890;&#24120;&#23454;&#29616;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65307;2&#65289;&#20195;&#29702;&#20915;&#31574;&#32972;&#21518;&#30340;&#28508;&#22312;&#22240;&#26524;&#26426;&#21046;&#21487;&#33021;&#38543;&#30528;&#36712;&#36857;&#32780;&#21464;&#21270;&#65292;&#32780;&#19981;&#26159;&#22312;&#25972;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#20445;&#25345;&#38745;&#24577;&#19981;&#21464;&#12290;&#20026;&#20102;&#22686;&#21152;&#31070;&#32463;&#20195;&#29702;&#30340;&#36879;&#26126;&#24230;&#21644;&#25552;&#20379;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#26377;&#21521;&#26080;&#29615;&#22240;&#26524;&#22270;&#30340;&#24418;&#24335;&#23637;&#31034;&#20854;&#25152;&#25429;&#33719;&#30340;&#30693;&#35782;&#65292;&#20854;&#20013;&#33410;&#28857;&#26159;&#21160;&#20316;&#21644;&#29366;&#24577;&#21464;&#37327;&#65292;&#36793;&#34920;&#31034;&#39044;&#27979;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#36825;&#20010;&#22240;&#26524;&#21457;&#29616;&#36807;&#31243;&#26159;&#20381;&#36182;&#29366;&#24577;&#30340;&#65292;&#20351;&#20854;&#33021;&#22815;&#23545;&#28508;&#22312;&#22240;&#26524;&#22270;&#20013;&#30340;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning, which learns agent policy by mimicking expert demonstration, has shown promising results in many applications such as medical treatment regimes and self-driving vehicles. However, it remains a difficult task to interpret control policies learned by the agent. Difficulties mainly come from two aspects: 1) agents in imitation learning are usually implemented as deep neural networks, which are black-box models and lack interpretability; 2) the latent causal mechanism behind agents' decisions may vary along the trajectory, rather than staying static throughout time steps. To increase transparency and offer better interpretability of the neural agent, we propose to expose its captured knowledge in the form of a directed acyclic causal graph, with nodes being action and state variables and edges denoting the causal relations behind predictions. Furthermore, we design this causal discovery process to be state-dependent, enabling it to model the dynamics in latent causal gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRI&#65292;&#29992;&#20110;&#22312;&#32447;CMDPs&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;CMDPs&#30340;&#26377;&#38480;&#38543;&#26426;&#24615;&#23646;&#24615;&#65292;&#33021;&#22815;&#20197;&#20302;&#36951;&#25022;&#24182;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.15395</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;CMDPs&#20013;&#65292;&#26080;&#27169;&#22411;&#12289;&#36951;&#25022;&#26368;&#20248;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Model-Free, Regret-Optimal Best Policy Identification in Online CMDPs. (arXiv:2309.15395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRI&#65292;&#29992;&#20110;&#22312;&#32447;CMDPs&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;CMDPs&#30340;&#26377;&#38480;&#38543;&#26426;&#24615;&#23646;&#24615;&#65292;&#33021;&#22815;&#20197;&#20302;&#36951;&#25022;&#24182;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32447;&#32422;&#26463;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#35782;&#21035;&#65288;BPI&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20855;&#26377;&#20302;&#36951;&#25022;&#24182;&#19988;&#20197;&#39640;&#27010;&#29575;&#35782;&#21035;&#26368;&#20248;&#31574;&#30053;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#24863;&#20852;&#36259;&#12290;&#29616;&#26377;&#30340;&#22312;&#32447;CMDPs&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#22312;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#36829;&#32422;&#26102;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#19988;&#21482;&#22312;&#20174;&#20197;&#21069;&#20351;&#29992;&#30340;&#31574;&#30053;&#20013;&#38543;&#26426;&#22343;&#21248;&#25277;&#26679;&#26102;&#25552;&#20379;&#24179;&#22343;&#24615;&#33021;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;PRUNING-REFINEMENT-IDENTIFICATION&#65288;PRI&#65289;&#65292;&#22522;&#20110;&#25105;&#20204;&#21457;&#29616;&#30340;CMDPs&#30340;&#19968;&#20010;&#22522;&#26412;&#32467;&#26500;&#24615;&#36136;&#65292;&#31216;&#20026;&#26377;&#38480;&#38543;&#26426;&#24615;&#12290;&#35813;&#23646;&#24615;&#34920;&#26126;&#23545;&#20110;&#20855;&#26377;N&#32422;&#26463;&#30340;CMDP&#65292;&#23384;&#22312;&#19968;&#20010;&#26368;&#20248;&#31574;&#30053;&#65292;&#20854;&#20013;&#33267;&#22810;&#26377;N&#20010;&#38543;&#26426;&#20915;&#31574;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#39318;&#20808;&#35782;&#21035;&#20986;&#22312;&#21738;&#20010;&#27493;&#39588;&#21644;&#21738;&#20010;&#29366;&#24577;&#38656;&#35201;&#36827;&#34892;&#38543;&#26426;&#20915;&#31574;&#65292;&#28982;&#21518;&#23545;&#36825;&#20123;&#20915;&#31574;&#30340;&#20998;&#24067;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the best policy identification (BPI) problem in online Constrained Markov Decision Processes (CMDPs). We are interested in algorithms that are model-free, have low regret, and identify an optimal policy with a high probability. Existing model-free algorithms for online CMDPs with sublinear regret and constraint violation do not provide any convergence guarantee to an optimal policy and provide only average performance guarantees when a policy is uniformly sampled at random from all previously used policies. In this paper, we develop a new algorithm, named Pruning-Refinement-Identification (PRI), based on a fundamental structural property of CMDPs we discover, called limited stochasticity. The property says for a CMDP with $N$ constraints, there exists an optimal policy with at most $N$ stochastic decisions.  The proposed algorithm first identifies at which step and in which state a stochastic decision has to be taken and then fine-tunes the distributions of these s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Guided Online Distillation (GOLD)&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#20013;&#25552;&#21462;&#19987;&#23478;&#31574;&#30053;&#26469;&#24341;&#23548;&#22312;&#32447;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#20445;&#23432;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20915;&#31574;&#36716;&#25442;&#22120;&#27169;&#22411;&#23545;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#36827;&#34892;&#26377;&#25928;&#30340;&#39640;&#23481;&#37327;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2309.09408</link><description>&lt;p&gt;
&#24341;&#23548;&#22312;&#32447;&#33976;&#39311;&#65306;&#36890;&#36807;&#31163;&#32447;&#28436;&#31034;&#26469;&#20419;&#36827;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Guided Online Distillation: Promoting Safe Reinforcement Learning by Offline Demonstration. (arXiv:2309.09408v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Guided Online Distillation (GOLD)&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#20013;&#25552;&#21462;&#19987;&#23478;&#31574;&#30053;&#26469;&#24341;&#23548;&#22312;&#32447;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#20445;&#23432;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20915;&#31574;&#36716;&#25442;&#22120;&#27169;&#22411;&#23545;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#36827;&#34892;&#26377;&#25928;&#30340;&#39640;&#23481;&#37327;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#25214;&#21040;&#28385;&#36275;&#25104;&#26412;&#32422;&#26463;&#30340;&#39640;&#22870;&#21169;&#31574;&#30053;&#12290;&#20174;&#38646;&#24320;&#22987;&#36827;&#34892;&#23398;&#20064;&#26102;&#65292;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#65292;&#36825;&#38459;&#30861;&#20102;&#25506;&#32034;&#24182;&#38480;&#21046;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#65292;&#21487;&#20197;&#33719;&#24471;&#22823;&#35268;&#27169;&#30340;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#12290;&#25105;&#20204;&#35748;&#20026;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#25552;&#21462;&#19987;&#23478;&#31574;&#30053;&#26469;&#24341;&#23548;&#22312;&#32447;&#25506;&#32034;&#26159;&#35299;&#20915;&#20445;&#23432;&#24615;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#23481;&#37327;&#27169;&#22411;&#65292;&#20363;&#22914;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#24456;&#23569;&#21253;&#21547;&#21361;&#38505;&#24773;&#20917;&#65288;&#20363;&#22914;&#30896;&#25758;&#65289;&#65292;&#36825;&#20351;&#24471;&#31574;&#30053;&#24456;&#38590;&#23398;&#20064;&#23433;&#20840;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#22823;&#35268;&#27169;&#31574;&#30053;&#32593;&#32476;&#26080;&#27861;&#28385;&#36275;&#20687;&#33258;&#21160;&#39550;&#39542;&#36825;&#26679;&#30340;&#30495;&#23454;&#20219;&#21153;&#25512;&#29702;&#26102;&#38388;&#30340;&#35745;&#31639;&#36895;&#24230;&#35201;&#27714;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#23548;&#22312;&#32447;&#33976;&#39311;&#65288;GOLD&#65289;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning (RL) aims to find a policy that achieves high rewards while satisfying cost constraints. When learning from scratch, safe RL agents tend to be overly conservative, which impedes exploration and restrains the overall performance. In many realistic tasks, e.g. autonomous driving, large-scale expert demonstration data are available. We argue that extracting expert policy from offline data to guide online exploration is a promising solution to mitigate the conserveness issue. Large-capacity models, e.g. decision transformers (DT), have been proven to be competent in offline policy learning. However, data collected in real-world scenarios rarely contain dangerous cases (e.g., collisions), which makes it prohibitive for the policies to learn safety concepts. Besides, these bulk policy networks cannot meet the computation speed requirements at inference time on real-world tasks such as autonomous driving. To this end, we propose Guided Online Distillation (GOLD), a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DiscoSCMs&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#26597;&#35810;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33268;&#24615;&#35268;&#21017;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#20998;&#26512;&#20010;&#24615;&#21270;&#28608;&#21169;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#26102;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#29420;&#31435;&#28508;&#22312;&#22122;&#22768;&#26465;&#20214;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;Layer 3&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.09323</link><description>&lt;p&gt;
&#29992;DiscoSCMs&#22238;&#31572;Layer 3&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Answering Layer 3 queries with DiscoSCMs. (arXiv:2309.09323v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DiscoSCMs&#65292;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#26597;&#35810;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#25193;&#23637;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21644;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#26469;&#35299;&#20915;&#19968;&#33268;&#24615;&#35268;&#21017;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#20998;&#26512;&#20010;&#24615;&#21270;&#28608;&#21169;&#22330;&#26223;&#20013;&#30340;&#28508;&#22312;&#32467;&#26524;&#26102;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#29420;&#31435;&#28508;&#22312;&#22122;&#22768;&#26465;&#20214;&#65292;&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;Layer 3&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#22240;&#26524;&#25512;&#26029;&#30740;&#31350;&#20013;&#65292;&#35299;&#20915;Pearl&#22240;&#26524;&#23618;&#27425;&#65288;PCH&#65289;&#19979;&#30340;&#20851;&#32852;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#30340;&#22240;&#26524;&#26597;&#35810;&#26159;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#33268;&#24615;&#35268;&#21017;&#24341;&#21457;&#30340;&#36864;&#21270;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20998;&#24067;&#19968;&#33268;&#24615;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;DiscoSCMs&#65289;&#65292;&#25193;&#23637;&#20102;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#21644;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#12290;&#20197;&#20010;&#24615;&#21270;&#28608;&#21169;&#22330;&#26223;&#20013;&#28508;&#22312;&#32467;&#26524;&#30340;&#30456;&#20851;&#27169;&#24335;$P(y_x, y'_{x'})$&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#23613;&#31649;&#21453;&#20107;&#23454;&#19981;&#20877;&#36864;&#21270;&#65292;&#20294;&#20173;&#26080;&#27861;&#30830;&#23450;&#12290;&#22240;&#27492;&#65292;&#23558;&#29420;&#31435;&#28508;&#22312;&#22122;&#22768;&#26465;&#20214;&#32435;&#20837;DiscoSCM&#12290;&#21457;&#29616;&#36890;&#36807;&#36866;&#24212;&#20998;&#24067;&#30340;&#23884;&#20837;&#24335;&#25512;&#26029;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#35299;&#20915;Layer 3&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing causal queries across the Pearl Causal Hierarchy (PCH) (i.e., associational, interventional and counterfactual), which is formalized as \Layer{} Valuations, is a central task in contemporary causal inference research. Counterfactual questions, in particular, pose a significant challenge as they often necessitate a complete knowledge of structural equations. This paper identifies \textbf{the degeneracy problem} caused by the consistency rule. To tackle this, the \textit{Distribution-consistency Structural Causal Models} (DiscoSCMs) is introduced, which extends both the structural causal models (SCM) and the potential outcome framework. The correlation pattern of potential outcomes in personalized incentive scenarios, described by $P(y_x, y'_{x'})$, is used as a case study for elucidation. Although counterfactuals are no longer degenerate, they remain indeterminable. As a result, the condition of independent potential noise is incorporated into DiscoSCM. It is found that by ad
&lt;/p&gt;</description></item><item><title>&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.03084</link><description>&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03084
&lt;/p&gt;
&lt;p&gt;
&#32431;&#33945;&#29305;&#21345;&#27931;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65288;PCFR&#65289;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#27010;&#24565;&#30340;&#26032;&#31639;&#27861;&#65292;&#33021;&#22815;&#19982;&#21508;&#31181;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#12290;PCFR&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#36951;&#25022;&#26368;&#23567;&#21270;&#65288;CFR&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#30446;&#21069;&#35299;&#20915;&#22823;&#35268;&#27169;&#19981;&#23436;&#20840;&#20449;&#24687;&#21338;&#24328;&#30340;&#26368;&#20339;&#31639;&#27861;&#12290;&#26412;&#25991;&#22312;CFR&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32431;CFR&#65288;PCFR&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;PCFR&#21487;&#20197;&#30475;&#20316;&#26159;CFR&#21644;&#34394;&#25311;&#28216;&#25103;&#65288;FP&#65289;&#30340;&#32467;&#21512;&#65292;&#32487;&#25215;&#20102;CFR&#30340;&#21453;&#20107;&#23454;&#36951;&#25022;&#65288;&#20540;&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#20351;&#29992;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#32780;&#19981;&#26159;&#36951;&#25022;&#21305;&#37197;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#35777;&#26126;&#20102;PCFR&#21487;&#20197;&#23454;&#29616;Blackwell&#21487;&#36798;&#24615;&#65292;&#20351;PCFR&#33021;&#22815;&#19982;&#21253;&#25324;&#33945;&#29305;&#21345;&#27931;CFR&#65288;MCCFR&#65289;&#22312;&#20869;&#30340;&#20219;&#20309;CFR&#21464;&#20307;&#30456;&#32467;&#21512;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#32431;MCCFR&#65288;PMCCFR&#65289;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;PMCCFR&#30340;&#25910;&#25947;&#36895;&#24230;&#33267;&#23569;&#27604;MCCFR&#24555;&#19977;&#20493;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PMCCFR&#19981;&#36890;&#36807;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#36335;&#24452;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21551;&#21160;&#31639;&#27861;&#65292;&#21463;&#21040;&#20102;&#20005;&#26684;&#34987;&#25903;&#37197;&#31574;&#30053;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Regret Minimization (CFR) and its variants are the best algorithms so far for solving large-scale incomplete information games. Building upon CFR, this paper proposes a new algorithm named Pure CFR (PCFR) for achieving better performance. PCFR can be seen as a combination of CFR and Fictitious Play (FP), inheriting the concept of counterfactual regret (value) from CFR, and using the best response strategy instead of the regret matching strategy for the next iteration. Our theoretical proof that PCFR can achieve Blackwell approachability enables PCFR's ability to combine with any CFR variant including Monte Carlo CFR (MCCFR). The resultant Pure MCCFR (PMCCFR) can significantly reduce time and space complexity. Particularly, the convergence speed of PMCCFR is at least three times more than that of MCCFR. In addition, since PMCCFR does not pass through the path of strictly dominated strategies, we developed a new warm-start algorithm inspired by the strictly dominated strat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#23558;&#26799;&#24230;&#31232;&#30095;&#24615;&#21644;&#28608;&#27963;&#31232;&#30095;&#24615;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20197;&#38544;&#34255;&#29305;&#24449;&#21644;&#21442;&#25968;&#32780;&#35328;&#65292;&#36825;&#22823;&#33268;&#31561;&#20110;&#23545;&#23398;&#20064;&#33391;&#22909;&#27169;&#22411;&#30340;&#26497;&#23567;&#20540;&#24179;&#22374;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.03004</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#22374;&#26497;&#23567;&#20540;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#35299;&#37322;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness. (arXiv:2309.03004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03004
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#23558;&#26799;&#24230;&#31232;&#30095;&#24615;&#21644;&#28608;&#27963;&#31232;&#30095;&#24615;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20197;&#38544;&#34255;&#29305;&#24449;&#21644;&#21442;&#25968;&#32780;&#35328;&#65292;&#36825;&#22823;&#33268;&#31561;&#20110;&#23545;&#23398;&#20064;&#33391;&#22909;&#27169;&#22411;&#30340;&#26497;&#23567;&#20540;&#24179;&#22374;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;MLP&#23618;&#20013;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#23454;&#35777;&#35266;&#23519;&#20026;&#22823;&#24133;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#30740;&#31350;&#23558;&#20854;&#24402;&#22240;&#20110;&#35757;&#32451;&#21160;&#21147;&#23398;&#65292;&#20294;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;&#20165;&#38480;&#20110;&#27973;&#23618;&#32593;&#32476;&#12289;&#23567;&#35757;&#32451;&#27493;&#38271;&#20197;&#21450;&#20462;&#25913;&#30340;&#35757;&#32451;&#65292;&#23613;&#31649;&#36825;&#31181;&#31232;&#30095;&#24615;&#24050;&#22312;&#36890;&#36807;vanilla&#21327;&#35758;&#36827;&#34892;&#22823;&#27493;&#39588;&#35757;&#32451;&#30340;&#28145;&#23618;&#27169;&#22411;&#20013;&#34987;&#21457;&#29616;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19977;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26799;&#24230;&#31232;&#30095;&#24615;&#30340;&#27010;&#24565;&#20316;&#20026;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28304;&#22836;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35299;&#37322;&#65292;&#35813;&#35299;&#37322;&#23558;&#26799;&#24230;&#31232;&#30095;&#24615;&#21644;&#28608;&#27963;&#31232;&#30095;&#24615;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20197;&#38544;&#34255;&#29305;&#24449;&#21644;&#21442;&#25968;&#32780;&#35328;&#65292;&#36825;&#22823;&#33268;&#31561;&#20110;&#23545;&#23398;&#20064;&#33391;&#22909;&#27169;&#22411;&#30340;&#26497;&#23567;&#20540;&#24179;&#22374;&#24615;&#12290;&#36825;&#20010;&#29702;&#35770;&#36866;&#29992;&#20110;&#32463;&#36807;LayerNorm&#26631;&#20934;&#35757;&#32451;&#30340;&#32431;MLP&#65292;&#24182;&#19988;&#22914;&#26524;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32473;&#26435;&#37325;&#28155;&#21152;&#22122;&#22768;&#65292;&#36824;&#36866;&#29992;&#20110;Transformers&#25110;&#20854;&#20182;&#26550;&#26500;&#12290;&#20026;&#20102;&#28040;&#38500;&#20854;&#20182;&#26469;&#28304;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent empirical observation of activation sparsity in MLP layers offers an opportunity to drastically reduce computation costs for free. Despite several works attributing it to training dynamics, the theoretical explanation of activation sparsity's emergence is restricted to shallow networks, small training steps well as modified training, even though the sparsity has been found in deep models trained by vanilla protocols for large steps. To fill the three gaps, we propose the notion of gradient sparsity as the source of activation sparsity and a theoretical explanation based on it that explains gradient sparsity and then activation sparsity as necessary steps to adversarial robustness w.r.t. hidden features and parameters, which is approximately the flatness of minima for well-learned models. The theory applies to standardly trained LayerNorm-ed pure MLPs, and further to Transformers or other architectures if noises are added to weights during training. To eliminate other sources o
&lt;/p&gt;</description></item><item><title>FRGNN&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#27979;&#35797;&#26102;&#38388;&#29305;&#24449;&#37325;&#26500;&#65292;&#20943;&#36731;&#20998;&#24067;&#20559;&#31227;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#65292;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#21407;&#22987;&#29305;&#24449;&#30340;&#20851;&#38190;&#20449;&#24687;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09259</link><description>&lt;p&gt;
FRGNN:&#36890;&#36807;&#27979;&#35797;&#26102;&#38388;&#29305;&#24449;&#37325;&#26500;&#20943;&#36731;&#20998;&#24067;&#20559;&#31227;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
FRGNN: Mitigating the Impact of Distribution Shift on Graph Neural Networks via Test-Time Feature Reconstruction. (arXiv:2308.09259v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09259
&lt;/p&gt;
&lt;p&gt;
FRGNN&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#27979;&#35797;&#26102;&#38388;&#29305;&#24449;&#37325;&#26500;&#65292;&#20943;&#36731;&#20998;&#24067;&#20559;&#31227;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24433;&#21709;&#65292;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#21407;&#22987;&#29305;&#24449;&#30340;&#20851;&#38190;&#20449;&#24687;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#19981;&#21512;&#36866;&#30340;&#26679;&#26412;&#36873;&#25321;&#21644;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20043;&#38388;&#32463;&#24120;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#12290;&#36825;&#31181;&#20559;&#31227;&#21487;&#33021;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#27979;&#35797;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;GNNs&#23545;&#20998;&#24067;&#20559;&#31227;&#30340;&#40065;&#26834;&#24615;&#25110;&#20943;&#23567;&#20559;&#31227;&#26412;&#36523;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#24403;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#32467;&#26500;&#21644;&#21442;&#25968;&#26102;&#65292;&#36825;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FR-GNN&#65292;&#19968;&#31181;&#29992;&#20110;GNNs&#36827;&#34892;&#29305;&#24449;&#37325;&#26500;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;FRGNN&#26500;&#24314;&#20102;&#19968;&#20010;&#20174;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;GNN&#30340;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#26144;&#23556;&#20851;&#31995;&#65292;&#20197;&#33719;&#24471;&#31867;&#21035;&#20195;&#34920;&#24615;&#23884;&#20837;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#23884;&#20837;&#26469;&#37325;&#26500;&#26631;&#35760;&#33410;&#28857;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#37325;&#26500;&#30340;&#29305;&#24449;&#34987;&#21512;&#24182;&#21040;GNNs&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#20013;&#65292;&#20197;&#24433;&#21709;&#27979;&#35797;&#26102;&#38388;&#26410;&#26631;&#35760;&#33410;&#28857;&#30340;&#39044;&#27979;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#37325;&#26500;&#30340;&#29305;&#24449;&#20445;&#30041;&#20102;&#21407;&#22987;&#29305;&#24449;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#20415;&#22312;&#27809;&#26377;&#21487;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;GNNs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to inappropriate sample selection and limited training data, a distribution shift often exists between the training and test sets. This shift can adversely affect the test performance of Graph Neural Networks (GNNs). Existing approaches mitigate this issue by either enhancing the robustness of GNNs to distribution shift or reducing the shift itself. However, both approaches necessitate retraining the model, which becomes unfeasible when the model structure and parameters are inaccessible. To address this challenge, we propose FR-GNN, a general framework for GNNs to conduct feature reconstruction. FRGNN constructs a mapping relationship between the output and input of a well-trained GNN to obtain class representative embeddings and then uses these embeddings to reconstruct the features of labeled nodes. These reconstructed features are then incorporated into the message passing mechanism of GNNs to influence the predictions of unlabeled nodes at test time. Notably, the reconstructed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#33258;&#36866;&#24212;&#21487;&#24494;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#23616;&#37096;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#22312;&#19981;&#21516;&#26412;&#22320;&#21306;&#22495;&#22788;&#29702;&#25968;&#25454;&#26102;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#23454;&#29616;&#26356;&#24555;&#30340;&#32479;&#35745;&#25910;&#25947;&#20197;&#21450;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07418</link><description>&lt;p&gt;
&#26412;&#22320;&#33258;&#36866;&#24212;&#21487;&#24494;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Locally Adaptive and Differentiable Regression. (arXiv:2308.07418v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#33258;&#36866;&#24212;&#21487;&#24494;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#23616;&#37096;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#22312;&#19981;&#21516;&#26412;&#22320;&#21306;&#22495;&#22788;&#29702;&#25968;&#25454;&#26102;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#23454;&#29616;&#26356;&#24555;&#30340;&#32479;&#35745;&#25910;&#25947;&#20197;&#21450;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#26862;&#26519;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#36229;&#21442;&#25968;&#21270;&#30340;&#26412;&#22320;&#33258;&#36866;&#24212;&#27169;&#22411;&#20013;&#65292;&#24120;&#35265;&#30340;&#36830;&#32493;&#24615;&#21644;&#21487;&#24494;&#24615;&#30446;&#26631;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#23545;&#24212;&#30340;&#26412;&#22320;&#21306;&#22495;&#20013;&#23545;&#23616;&#37096;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#26500;&#24314;&#20840;&#23616;&#36830;&#32493;&#21487;&#24494;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#23494;&#24230;&#25110;&#19981;&#21516;&#26412;&#22320;&#21306;&#22495;&#20013;&#30340;&#20989;&#25968;&#20540;&#23610;&#24230;&#30340;&#25968;&#25454;&#26102;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#25105;&#20204;&#22312;&#26412;&#22320;&#27169;&#22411;&#20013;&#28151;&#21512;&#20351;&#29992;&#26680;&#23725;&#21644;&#22810;&#39033;&#24335;&#22238;&#24402;&#39033;&#65292;&#24182;&#23545;&#23427;&#20204;&#36827;&#34892;&#36830;&#32493;&#25340;&#25509;&#26102;&#65292;&#22312;&#29702;&#35770;&#19978;&#23454;&#29616;&#26356;&#24555;&#30340;&#32479;&#35745;&#25910;&#25947;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#38469;&#29615;&#22659;&#20013;&#23454;&#29616;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-parameterized models like deep nets and random forests have become very popular in machine learning. However, the natural goals of continuity and differentiability, common in regression models, are now often ignored in modern overparametrized, locally-adaptive models. We propose a general framework to construct a global continuous and differentiable model based on a weighted average of locally learned models in corresponding local regions. This model is competitive in dealing with data with different densities or scales of function values in different local regions. We demonstrate that when we mix kernel ridge and polynomial regression terms in the local models, and stitch them together continuously, we achieve faster statistical convergence in theory and improved performance in various practical settings.
&lt;/p&gt;</description></item><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#26102;&#38388;&#34701;&#21512;&#32593;&#32476;&#32467;&#21512;&#27880;&#24847;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#38024;&#23545;&#24515;&#24459;&#22833;&#24120;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#26412;&#22320;&#26102;&#38388;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#26412;&#22320;-&#20840;&#23616;&#20449;&#24687;&#34701;&#21512;&#65292;&#20197;&#22788;&#29702;&#38271;&#24230;&#21464;&#21270;&#30340;&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02416</link><description>&lt;p&gt;
&#26412;&#22320;-&#20840;&#23616;&#26102;&#38388;&#34701;&#21512;&#32593;&#32476;&#32467;&#21512;&#27880;&#24847;&#26426;&#21046;&#29992;&#20110;&#22810;&#31867;&#21644;&#22810;&#31181;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Local-Global Temporal Fusion Network with an Attention Mechanism for Multiple and Multiclass Arrhythmia Classification. (arXiv:2308.02416v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26412;&#22320;-&#20840;&#23616;&#26102;&#38388;&#34701;&#21512;&#32593;&#32476;&#32467;&#21512;&#27880;&#24847;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#38024;&#23545;&#24515;&#24459;&#22833;&#24120;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#26412;&#22320;&#26102;&#38388;&#20449;&#24687;&#21644;&#20840;&#23616;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#26412;&#22320;-&#20840;&#23616;&#20449;&#24687;&#34701;&#21512;&#65292;&#20197;&#22788;&#29702;&#38271;&#24230;&#21464;&#21270;&#30340;&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#22312;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSSs&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25903;&#25345;&#24515;&#30005;&#22270;&#65288;ECGs&#65289;&#20013;&#24515;&#34880;&#31649;&#31185;&#21307;&#29983;&#26816;&#27979;&#21644;&#20998;&#31867;&#24515;&#24459;&#22833;&#24120;&#26102;&#25152;&#20570;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24515;&#24459;&#22833;&#24120;&#38271;&#24230;&#30340;&#21464;&#21270;&#65292;&#24418;&#25104;&#19968;&#20010;&#38024;&#23545;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#20219;&#21153;&#30340;CDSS&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#24515;&#24459;&#22833;&#24120;&#30340;&#21457;&#20316;&#26102;&#38388;&#26159;&#21464;&#21270;&#30340;&#65292;&#20294;&#20043;&#21069;&#24320;&#21457;&#30340;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#20123;&#26465;&#20214;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#65288;i&#65289;&#26412;&#22320;&#26102;&#38388;&#20449;&#24687;&#25552;&#21462;&#65292;&#65288;ii&#65289;&#20840;&#23616;&#27169;&#24335;&#25552;&#21462;&#65292;&#21644;&#65288;iii&#65289;&#24102;&#26377;&#27880;&#24847;&#21147;&#30340;&#26412;&#22320;-&#20840;&#23616;&#20449;&#24687;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#23545;&#26377;&#38480;&#36755;&#20837;&#38271;&#24230;&#30340;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#21644;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#36215;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#20316;&#20026;&#19968;&#20010;&#20107;&#20214;&#65292;&#20197;&#21450;&#22522;&#20110;MIT-BIH&#24515;&#24459;&#22833;&#24120;&#25968;&#25454;&#24211;&#65288;MITDB&#65289;&#21644;MIT-BIH&#25151;&#39076;&#25968;&#25454;&#24211;&#65288;AFDB&#65289;&#30340;&#24515;&#24459;&#22833;&#24120;&#25345;&#32493;&#26102;&#38388;&#26469;&#35780;&#20272;10&#31867;&#21644;4&#31867;&#34920;&#29616;&#12290;&#32467;&#26524;&#22312;&#32479;&#35745;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms (ECGs). However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local temporal information extraction, (ii) global pattern extraction, and (iii) local-global information fusion with attention to perform arrhythmia detection and classification with a constrained input length. The 10-class and 4-class performances of our approach were assessed by detecting the onset and offset of arrhythmia as an episode and the duration of arrhythmia based on the MIT-BIH arrhythmia database (MITDB) and MIT-BIH atrial fibrillation database (AFDB), respectively. The results were statistically superior to 
&lt;/p&gt;</description></item><item><title>AutoML4ETC&#26159;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#31070;&#32463;&#26550;&#26500;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#20854;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2308.02182</link><description>&lt;p&gt;
AutoML4ETC: &#33258;&#21160;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23454;&#29616;&#29616;&#23454;&#19990;&#30028;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification. (arXiv:2308.02182v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02182
&lt;/p&gt;
&lt;p&gt;
AutoML4ETC&#26159;&#19968;&#20010;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#31070;&#32463;&#26550;&#26500;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#20854;&#36890;&#36807;&#23450;&#20041;&#26032;&#39062;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#29615;&#22659;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21152;&#23494;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;DL&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#38543;&#26102;&#38388;&#19981;&#21487;&#36991;&#20813;&#22320;&#19979;&#38477;&#12290;&#20165;&#20165;&#23545;&#26032;&#25968;&#25454;&#38598;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#21482;&#33021;&#37096;&#20998;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25163;&#21160;&#35843;&#25972;&#27169;&#22411;&#26550;&#26500;&#20197;&#28385;&#36275;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#26399;&#26395;&#32791;&#26102;&#19988;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24037;&#20855;AutoML4ETC&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#39640;&#25928;&#19988;&#39640;&#24615;&#33021;&#30340;&#31070;&#32463;&#26550;&#26500;&#20197;&#36827;&#34892;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#24378;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#19987;&#38376;&#38024;&#23545;&#20351;&#29992;&#25968;&#25454;&#21253;&#22836;&#23383;&#33410;&#36827;&#34892;&#36817;&#23454;&#26102;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#25628;&#32034;&#31354;&#38388;&#19978;&#20351;&#29992;&#19981;&#21516;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AutoML4ETC&#29983;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#20844;&#20849;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been successfully applied to encrypted network traffic classification in experimental settings. However, in production use, it has been shown that a DL classifier's performance inevitably decays over time. Re-training the model on newer datasets has been shown to only partially improve its performance. Manually re-tuning the model architecture to meet the performance expectations on newer datasets is time-consuming and requires domain expertise. We propose AutoML4ETC, a novel tool to automatically design efficient and high-performing neural architectures for encrypted traffic classification. We define a novel, powerful search space tailored specifically for the near real-time classification of encrypted traffic using packet header bytes. We show that with different search strategies over our search space, AutoML4ETC generates neural architectures that outperform the state-of-the-art encrypted traffic classifiers on several datasets, including public benchmark dat
&lt;/p&gt;</description></item><item><title>PELICAN&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#38382;&#39064;&#20013;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;PELICAN&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#23545;&#31216;&#32676;&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#38477;&#20302;&#22797;&#26434;&#24615;&#12289;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#21644;&#25552;&#39640;&#24615;&#33021;&#30340;&#29305;&#28857;&#12290;&#23427;&#22312;&#26631;&#35760;&#21644;&#37325;&#26500;&#21160;&#37327;&#22686;&#24378;&#30340;&#39030;&#22840;&#20811;&#65292;&#24182;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#29305;&#21035;&#35782;&#21035;&#21644;&#27979;&#37327;W&#29627;&#33394;&#23376;&#65292;&#20197;&#21450;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#21943;&#27880;&#31561;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2307.16506</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#22312;&#31890;&#23376;&#29289;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;PELICAN
&lt;/p&gt;
&lt;p&gt;
Explainable Equivariant Neural Networks for Particle Physics: PELICAN. (arXiv:2307.16506v2 [hep-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16506
&lt;/p&gt;
&lt;p&gt;
PELICAN&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#38382;&#39064;&#20013;&#12290;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;PELICAN&#30340;&#20248;&#21183;&#22312;&#20110;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#23545;&#31216;&#32676;&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#38477;&#20302;&#22797;&#26434;&#24615;&#12289;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#21644;&#25552;&#39640;&#24615;&#33021;&#30340;&#29305;&#28857;&#12290;&#23427;&#22312;&#26631;&#35760;&#21644;&#37325;&#26500;&#21160;&#37327;&#22686;&#24378;&#30340;&#39030;&#22840;&#20811;&#65292;&#24182;&#22312;&#23494;&#38598;&#29615;&#22659;&#20013;&#29305;&#21035;&#35782;&#21035;&#21644;&#27979;&#37327;W&#29627;&#33394;&#23376;&#65292;&#20197;&#21450;&#35782;&#21035;&#19981;&#21516;&#31867;&#22411;&#30340;&#21943;&#27880;&#31561;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PELICAN&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32622;&#25442;&#31561;&#21464;&#19988;&#27931;&#20262;&#20857;&#19981;&#21464;&#25110;&#21327;&#21464;&#30340;&#32858;&#21512;&#32593;&#32476;&#65292;&#26088;&#22312;&#20811;&#26381;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#38382;&#39064;&#30340;&#24120;&#35265;&#38480;&#21046;&#12290;&#19982;&#35768;&#22810;&#20351;&#29992;&#38750;&#19987;&#29992;&#26550;&#26500;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;PELICAN&#37319;&#29992;&#22522;&#20110;&#23545;&#31216;&#32676;&#30340;&#26550;&#26500;&#65292;&#20307;&#29616;&#20102;&#22797;&#26434;&#24230;&#38477;&#20302;&#12289;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#21644;&#24615;&#33021;&#25552;&#21319;&#31561;&#20248;&#21183;&#65292;&#32780;&#38750;&#20197;&#24222;&#22823;&#30340;&#21442;&#25968;&#20026;&#20195;&#20215;&#12290;&#25105;&#20204;&#22312;&#26631;&#35760;&#65288;&#20998;&#31867;&#65289;&#21644;&#37325;&#26500;&#65288;&#22238;&#24402;&#65289;&#21160;&#37327;&#22686;&#24378;&#30340;&#39030;&#22840;&#20811;&#30340;&#32972;&#26223;&#19979;&#23545;PELICAN&#31639;&#27861;&#26550;&#26500;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#22312;&#27931;&#20262;&#20857;&#22686;&#24378;&#30340;&#39030;&#22840;&#20811;&#24378;&#23376;&#26411;&#24577;&#30340;&#23494;&#38598;&#29615;&#22659;&#20013;&#29305;&#21035;&#35782;&#21035;&#21644;&#27979;&#37327;W&#29627;&#33394;&#23376;&#30340;&#22256;&#38590;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#23558;PELICAN&#24212;&#29992;&#20110;&#35782;&#21035;&#22840;&#20811;-&#24341;&#21457;&#19982;&#33014;&#23376;-&#24341;&#21457;&#21943;&#27880;&#20197;&#21450;&#22810;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
PELICAN is a novel permutation equivariant and Lorentz invariant or covariant aggregator network designed to overcome common limitations found in architectures applied to particle physics problems. Compared to many approaches that use non-specialized architectures that neglect underlying physics principles and require very large numbers of parameters, PELICAN employs a fundamentally symmetry group-based architecture that demonstrates benefits in terms of reduced complexity, increased interpretability, and raw performance. We present a comprehensive study of the PELICAN algorithm architecture in the context of both tagging (classification) and reconstructing (regression) Lorentz-boosted top quarks, including the difficult task of specifically identifying and measuring the $W$-boson inside the dense environment of the Lorentz-boosted top-quark hadronic final state. We also extend the application of PELICAN to the tasks of identifying quark-initiated vs.~gluon-initiated jets, and a multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.13421</link><description>&lt;p&gt;
&#20851;&#20110;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#19977;&#31181;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#35299;&#37322;&#20102;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#19977;&#20010;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#20043;&#19968;&#26469;&#23398;&#20064;&#65292;&#20998;&#21035;&#31216;&#20026;&#36719;&#27880;&#24847;&#21147;&#12289;&#30828;&#27880;&#24847;&#21147;&#21644;&#28508;&#21464;&#37327;&#36793;&#38469;&#20284;&#28982;&#65288;LVML&#65289;&#27880;&#24847;&#21147;&#12290;&#36825;&#19977;&#31181;&#33539;&#24335;&#37117;&#26159;&#20026;&#20102;&#36798;&#21040;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#21363;&#25214;&#21040;&#20004;&#20010;&#27169;&#22411;&#65306;&#19968;&#20010;&#8220;&#28966;&#28857;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#8220;&#36873;&#25321;&#8221;&#36755;&#20837;&#20013;&#30340;&#27491;&#30830;&#8220;&#29255;&#27573;&#8221;&#65292;&#21644;&#19968;&#20010;&#8220;&#20998;&#31867;&#8221;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#36873;&#23450;&#30340;&#29255;&#27573;&#22788;&#29702;&#25104;&#30446;&#26631;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25152;&#36873;&#25321;&#30340;&#29255;&#27573;&#32858;&#21512;&#26041;&#24335;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#23548;&#33268;&#20102;&#19981;&#21516;&#30340;&#21160;&#24577;&#21644;&#26368;&#32456;&#32467;&#26524;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36825;&#20123;&#33539;&#24335;&#23398;&#20064;&#30340;&#27169;&#22411;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#35299;&#37322;&#20026;&#22312;&#28966;&#28857;&#27169;&#22411;&#22266;&#23450;&#26102;&#65292;&#20998;&#31867;&#27169;&#22411;&#22312;&#26799;&#24230;&#19979;&#38477;&#19979;&#30340;&#28436;&#21270;&#25152;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#35774;&#32622;&#20013;&#20998;&#26512;&#20102;&#36825;&#20123;&#33539;&#24335;&#65292;&#24182;&#25512;&#23548;&#20986;&#26799;&#24230;&#27969;&#19979;&#21442;&#25968;&#36712;&#36857;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#22312;&#36719;&#27880;&#24847;&#21147;&#25439;&#22833;&#19979;&#65292;&#28966;&#28857;&#27169;&#22411;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#24555;&#36895;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#20107;&#20214;&#26085;&#24535;&#36716;&#25442;&#20026;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;OCDiGCN&#26469;&#26816;&#27979;&#19968;&#32452;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#20013;&#30340;&#22270;&#32423;&#21035;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#23545;&#24322;&#24120;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00527</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#19982;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks based Log Anomaly Detection and Explanation. (arXiv:2307.00527v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00527
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#20107;&#20214;&#26085;&#24535;&#36716;&#25442;&#20026;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#65292;&#24182;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;OCDiGCN&#26469;&#26816;&#27979;&#19968;&#32452;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#20013;&#30340;&#22270;&#32423;&#21035;&#24322;&#24120;&#65292;&#24182;&#25552;&#20379;&#23545;&#24322;&#24120;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26085;&#24535;&#34987;&#24191;&#27867;&#29992;&#20110;&#35760;&#24405;&#39640;&#31185;&#25216;&#31995;&#32479;&#30340;&#29366;&#24577;&#65292;&#22240;&#27492;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#30417;&#25511;&#36825;&#20123;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#23558;&#26085;&#24535;&#20107;&#20214;&#35745;&#25968;&#30697;&#38453;&#25110;&#26085;&#24535;&#20107;&#20214;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#21033;&#29992;&#26085;&#24535;&#20107;&#20214;&#20043;&#38388;&#30340;&#23450;&#37327;&#21644;/&#25110;&#39034;&#24207;&#20851;&#31995;&#26469;&#26816;&#27979;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#20165;&#32771;&#34385;&#23450;&#37327;&#25110;&#39034;&#24207;&#20851;&#31995;&#21487;&#33021;&#23548;&#33268;&#26816;&#27979;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26080;&#30417;&#30563;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;Logs2Graphs&#65292;&#23427;&#39318;&#20808;&#23558;&#20107;&#20214;&#26085;&#24535;&#36716;&#25442;&#20026;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#65292;&#28982;&#21518;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;One-Class Digraph Inception Convolutional Networks&#65288;OCDiGCN&#65289;&#65292;&#29992;&#20110;&#22312;&#19968;&#32452;&#24102;&#23646;&#24615;&#12289;&#26377;&#21521;&#21644;&#21152;&#26435;&#30340;&#22270;&#20013;&#26816;&#27979;&#22270;&#32423;&#21035;&#30340;&#24322;&#24120;&#12290;&#36890;&#36807;&#23558;&#22270;&#34920;&#31034;&#19982;&#23646;&#24615;&#34920;&#31034;&#32806;&#21512;&#36215;&#26469;&#65292;&#22312;&#22270;&#32423;&#21035;&#19978;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#24322;&#24120;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event logs are widely used to record the status of high-tech systems, making log anomaly detection important for monitoring those systems. Most existing log anomaly detection methods take a log event count matrix or log event sequences as input, exploiting quantitative and/or sequential relationships between log events to detect anomalies. Unfortunately, only considering quantitative or sequential relationships may result in low detection accuracy. To alleviate this problem, we propose a graph-based method for unsupervised log anomaly detection, dubbed Logs2Graphs, which first converts event logs into attributed, directed, and weighted graphs, and then leverages graph neural networks to perform graph-level anomaly detection. Specifically, we introduce One-Class Digraph Inception Convolutional Networks, abbreviated as OCDiGCN, a novel graph neural network model for detecting graph-level anomalies in a collection of attributed, directed, and weighted graphs. By coupling the graph represe
&lt;/p&gt;</description></item><item><title>GPTWatermark&#26159;&#19968;&#31181;&#38024;&#23545;&#24615;&#27169;&#22411;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#22266;&#23450;&#20998;&#32452;&#35774;&#35745;&#21644;&#24378;&#22823;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#65292;&#25552;&#20379;&#20102;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#24615;&#26816;&#27979;&#21644;&#23433;&#20840;&#24615;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#25512;&#21160;&#20102;LLMs&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#36827;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.17439</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#30340;&#38024;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Provable Robust Watermarking for AI-Generated Text. (arXiv:2306.17439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17439
&lt;/p&gt;
&lt;p&gt;
GPTWatermark&#26159;&#19968;&#31181;&#38024;&#23545;&#24615;&#27169;&#22411;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#22266;&#23450;&#20998;&#32452;&#35774;&#35745;&#21644;&#24378;&#22823;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#65292;&#25552;&#20379;&#20102;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#40065;&#26834;&#24615;&#26816;&#27979;&#21644;&#23433;&#20840;&#24615;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#65292;&#25512;&#21160;&#20102;LLMs&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#25776;&#20889;&#30340;&#20869;&#23481;&#65292;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPTWatermark&#65292;&#19968;&#31181;&#24378;&#22823;&#19988;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#30830;&#23450;&#19968;&#27573;&#25991;&#26412;&#26159;&#21542;&#26469;&#33258;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#31574;&#30053;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#22266;&#23450;&#30340;&#20998;&#32452;&#35774;&#35745;&#65292;&#20197;&#22686;&#24378;&#23545;&#32534;&#36753;&#21644;&#25913;&#20889;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24102;&#27700;&#21360;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#36136;&#37327;&#12289;&#26816;&#27979;&#27491;&#30830;&#24615;&#21644;&#23545;&#25239;&#35268;&#36991;&#25915;&#20987;&#30340;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35777;&#26126;&#20445;&#35777;&#12290;&#22312;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#20248;&#36234;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#19982;&#29983;&#25104;&#36136;&#37327;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#30456;&#24403;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;LLMs&#30340;&#36127;&#36131;&#20219;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI-generated text increasingly resembles human-written content, the ability to detect machine-generated text becomes crucial. To address this challenge, we present GPTWatermark, a robust and high-quality solution designed to ascertain whether a piece of text originates from a specific model. Our approach extends existing watermarking strategies and employs a fixed group design to enhance robustness against editing and paraphrasing attacks. We show that our watermarked language model enjoys strong provable guarantees on generation quality, correctness in detection, and security against evasion attacks. Experimental results on various large language models (LLMs) and diverse datasets demonstrate that our method achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22788;&#29702;&#26410;&#30693;&#29615;&#22659;&#24182;&#32467;&#21512;&#20840;&#23616;&#22320;&#22270;&#21644;&#23616;&#37096;&#24863;&#30693;&#36755;&#20837;&#65292;&#21516;&#26102;&#32771;&#34385;&#38271;&#26399;&#36335;&#24452;&#35268;&#21010;&#21644;&#30701;&#26399;&#38556;&#30861;&#29289;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.16978</link><description>&lt;p&gt;
&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-end Reinforcement Learning for Online Coverage Path Planning in Unknown Environments. (arXiv:2306.16978v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22788;&#29702;&#26410;&#30693;&#29615;&#22659;&#24182;&#32467;&#21512;&#20840;&#23616;&#22320;&#22270;&#21644;&#23616;&#37096;&#24863;&#30693;&#36755;&#20837;&#65292;&#21516;&#26102;&#32771;&#34385;&#38271;&#26399;&#36335;&#24452;&#35268;&#21010;&#21644;&#30701;&#26399;&#38556;&#30861;&#29289;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#26159;&#23547;&#25214;&#35206;&#30422;&#32473;&#23450;&#23553;&#38381;&#21306;&#22495;&#25972;&#20010;&#33258;&#30001;&#31354;&#38388;&#30340;&#26368;&#30701;&#36335;&#24452;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#26426;&#22120;&#20154;&#21106;&#33609;&#21644;&#21560;&#23576;&#21040;&#22320;&#38647;&#28165;&#38500;&#21644;&#25628;&#25937;&#20219;&#21153;&#12290;&#34429;&#28982;&#31163;&#32447;&#26041;&#27861;&#21487;&#20197;&#20026;&#24050;&#30693;&#29615;&#22659;&#25214;&#21040;&#21487;&#35777;&#26126;&#23436;&#22791;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#26368;&#20248;&#30340;&#36335;&#24452;&#65292;&#20294;&#22312;&#22312;&#32447;&#22330;&#26223;&#19979;&#65292;&#29615;&#22659;&#20107;&#20808;&#26410;&#30693;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#38750;&#38745;&#24577;&#38556;&#30861;&#29289;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#20215;&#20540;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#26410;&#30693;&#29615;&#22659;&#30340;&#22312;&#32447;&#35206;&#30422;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#20840;&#23616;&#22320;&#22270;&#21644;&#23616;&#37096;&#24863;&#30693;&#36755;&#20837;&#26500;&#24314;&#35266;&#23519;&#31354;&#38388;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#35268;&#21010;&#38271;&#26399;&#36335;&#24452;&#65292;&#24182;&#21516;&#26102;&#23545;&#30701;&#26399;&#38556;&#30861;&#29289;&#36827;&#34892;&#34892;&#21160;&#12290;&#20026;&#20102;&#32771;&#34385;&#22823;&#35268;&#27169;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#23610;&#24230;&#22320;&#22270;&#36755;&#20837;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#20943;&#23569;&#36335;&#24452;&#20559;&#31163;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coverage path planning is the problem of finding the shortest path that covers the entire free space of a given confined area, with applications ranging from robotic lawn mowing and vacuum cleaning, to demining and search-and-rescue tasks. While offline methods can find provably complete, and in some cases optimal, paths for known environments, their value is limited in online scenarios where the environment is not known beforehand, especially in the presence of non-static obstacles. We propose an end-to-end reinforcement learning-based approach in continuous state and action space, for the online coverage path planning problem that can handle unknown environments. We construct the observation space from both global maps and local sensory inputs, allowing the agent to plan a long-term path, and simultaneously act on short-term obstacle detections. To account for large-scale environments, we propose to use a multi-scale map input representation. Furthermore, we propose a novel total var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIMF&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#35821;&#20041;&#24863;&#30693;&#30340;&#20132;&#20114;&#24335;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#35821;&#20041;&#30340;&#34892;&#20026;&#20307;&#36873;&#25321;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20840;&#23616;&#32534;&#30721;&#65292;&#33021;&#22815;&#25429;&#25417;&#31354;&#38388;&#20449;&#24687;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20248;&#36873;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.14941</link><description>&lt;p&gt;
SIMF: &#33258;&#21160;&#39550;&#39542;&#30340;&#35821;&#20041;&#24863;&#30693;&#20132;&#20114;&#24335;&#36816;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SIMF: Semantics-aware Interactive Motion Forecasting for Autonomous Driving. (arXiv:2306.14941v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIMF&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#35821;&#20041;&#24863;&#30693;&#30340;&#20132;&#20114;&#24335;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#35821;&#20041;&#30340;&#34892;&#20026;&#20307;&#36873;&#25321;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21462;&#20840;&#23616;&#32534;&#30721;&#65292;&#33021;&#22815;&#25429;&#25417;&#31354;&#38388;&#20449;&#24687;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20248;&#36873;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38656;&#35201;&#23545;&#21608;&#22260;&#22810;&#20010;&#34892;&#20026;&#20307;&#65288;&#34892;&#20154;&#21644;&#36710;&#36742;&#65289;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#65292;&#20197;&#20570;&#20986;&#26368;&#20248;&#23548;&#33322;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#34892;&#20026;&#20307;&#30340;&#20301;&#32622;&#21644;&#36895;&#24230;&#65292;&#24182;&#26410;&#33021;&#25429;&#25417;&#21040;&#22330;&#26223;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#19982;&#22330;&#26223;&#20013;&#34892;&#20026;&#20307;&#25968;&#37327;&#22686;&#21152;&#30456;&#20851;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#27431;&#27663;&#36317;&#31163;&#26469;&#21098;&#26525;&#36828;&#31163;&#30340;&#34892;&#20026;&#20307;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#26080;&#27861;&#36873;&#25321;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#24182;&#20934;&#30830;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SIMF&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#31354;&#38388;&#20449;&#24687;&#20197;&#21450;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20248;&#36873;&#30456;&#20851;&#30340;&#34892;&#20026;&#20307;&#36827;&#34892;&#36816;&#21160;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30340;&#34892;&#20026;&#20307;&#36873;&#25321;&#26041;&#27861;&#65292;&#23558;&#20854;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20256;&#36882;&#65292;&#20197;&#25552;&#21462;&#20840;&#23616;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous vehicles require motion forecasting of their surrounding multi-agents (pedestrians and vehicles) to make optimal decisions for navigation. The existing methods focus on techniques to utilize the positions and velocities of these agents and fail to capture semantic information from the scene. Moreover, to mitigate the increase in computational complexity associated with the number of agents in the scene, some works leverage Euclidean distance to prune far-away agents. However, distance-based metric alone is insufficient to select relevant agents and accurately perform their predictions. To resolve these issues, we propose Semantics-aware Interactive Motion Forecasting (SIMF) method to capture semantics along with spatial information, and optimally select relevant agents for motion prediction. Specifically, we achieve this by implementing a semantic-aware selection of relevant agents from the scene and passing them through an attention mechanism to extract global encodings. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;OpenDataVal&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#20061;&#31181;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#20272;&#20540;&#31639;&#27861;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#20010;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#26469;&#35780;&#20272;&#25968;&#25454;&#20215;&#20540;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.10577</link><description>&lt;p&gt;
OpenDataVal&#65306;&#19968;&#31181;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#30340;&#32479;&#19968;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
OpenDataVal: a Unified Benchmark for Data Valuation. (arXiv:2306.10577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;OpenDataVal&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#22810;&#31181;&#25968;&#25454;&#38598;&#21644;&#20061;&#31181;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#20272;&#20540;&#31639;&#27861;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#20010;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#26469;&#35780;&#20272;&#25968;&#25454;&#20215;&#20540;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#23545;&#20110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#19981;&#33391;&#20559;&#24046;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#20010;&#25968;&#25454;&#20272;&#20540;&#31639;&#27861;&#26469;&#37327;&#21270;&#25968;&#25454;&#36136;&#37327;&#65292;&#20294;&#36824;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#21644;&#26631;&#20934;&#21270;&#30340;&#25968;&#25454;&#20272;&#20540;&#22522;&#20934;&#27979;&#35797;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OpenDataVal&#65292;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#21644;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#33021;&#22815;&#24212;&#29992;&#21644;&#27604;&#36739;&#21508;&#31181;&#25968;&#25454;&#20272;&#20540;&#31639;&#27861;&#12290;OpenDataVal&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#29615;&#22659;&#65292;&#21253;&#25324;&#65288;i&#65289;&#21508;&#31181;&#22270;&#20687;&#65292;&#33258;&#28982;&#35821;&#35328;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#65288;ii&#65289;&#20061;&#31181;&#19981;&#21516;&#30340;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#20272;&#20540;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#21487;&#20197;&#23548;&#20837;&#20219;&#20309;scikit-learn&#27169;&#22411;&#30340;&#39044;&#27979;&#27169;&#22411;API&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#19979;&#28216;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#20540;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;OpenDataVal&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#65292;&#37327;&#21270;&#24182;&#27604;&#36739;&#19981;&#21516;&#25968;&#25454;&#20272;&#20540;&#31639;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessing the quality and impact of individual data points is critical for improving model performance and mitigating undesirable biases within the training dataset. Several data valuation algorithms have been proposed to quantify data quality, however, there lacks a systemic and standardized benchmarking system for data valuation. In this paper, we introduce OpenDataVal, an easy-to-use and unified benchmark framework that empowers researchers and practitioners to apply and compare various data valuation algorithms. OpenDataVal provides an integrated environment that includes (i) a diverse collection of image, natural language, and tabular datasets, (ii) implementations of nine different state-of-the-art data valuation algorithms, and (iii) a prediction model API that can import any models in scikit-learn. Furthermore, we propose four downstream machine learning tasks for evaluating the quality of data values. We perform benchmarking analysis using OpenDataVal, quantifying and comparin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#24863;&#30693;&#27169;&#22359;&#65292;&#31216;&#20026;&#35821;&#20041;&#20960;&#20309;&#34920;&#31034;&#65288;SGR&#65289;&#65292;&#35813;&#27169;&#22359;&#32467;&#21512;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21644;3D&#31354;&#38388;&#25512;&#29702;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.10474</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#36890;&#29992;&#35821;&#20041;&#20960;&#20309;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A Universal Semantic-Geometric Representation for Robotic Manipulation. (arXiv:2306.10474v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10474
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#24863;&#30693;&#27169;&#22359;&#65292;&#31216;&#20026;&#35821;&#20041;&#20960;&#20309;&#34920;&#31034;&#65288;SGR&#65289;&#65292;&#35813;&#27169;&#22359;&#32467;&#21512;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21644;3D&#31354;&#38388;&#25512;&#29702;&#30340;&#20248;&#21183;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#20013;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#22312;&#24863;&#30693;&#21644;&#19982;&#19990;&#30028;&#20114;&#21160;&#26102; heavily relies &#20256;&#24863;&#22120;&#65292;&#29305;&#21035;&#26159;RGB&#21644;&#28145;&#24230;&#30456;&#26426;&#12290;RGB&#30456;&#26426;&#35760;&#24405;&#20102;&#20855;&#26377;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#30340;2D&#22270;&#20687;&#65292;&#20294;&#32570;&#20047;&#31934;&#30830;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28145;&#24230;&#30456;&#26426;&#25552;&#20379;&#20102;&#20851;&#38190;&#30340;3D&#20960;&#20309;&#25968;&#25454;&#65292;&#20294;&#25429;&#25417;&#21040;&#30340;&#35821;&#20041;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25972;&#21512;&#20004;&#31181;&#27169;&#24577;&#23545;&#20110;&#23398;&#20064;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#25511;&#21046;&#30340;&#34920;&#31034;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20854;&#20013;&#19968;&#31181;&#27169;&#24577;&#19978;&#65292;&#24182;&#24573;&#30053;&#20102;&#32467;&#21512;&#20004;&#32773;&#30340;&#22909;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textbf{&#35821;&#20041;&#20960;&#20309;&#34920;&#31034;} (\textbf{SGR})$&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#20154;&#30340;&#36890;&#29992;&#24863;&#30693;&#27169;&#22359;&#65292;&#23427;&#21033;&#29992;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#25215;&#32487;&#20102;3D&#31354;&#38388;&#25512;&#29702;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SGR&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#25104;&#21151;&#23436;&#25104;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#20219;&#21153;&#65292;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots rely heavily on sensors, especially RGB and depth cameras, to perceive and interact with the world. RGB cameras record 2D images with rich semantic information while missing precise spatial information. On the other side, depth cameras offer critical 3D geometry data but capture limited semantics. Therefore, integrating both modalities is crucial for learning representations for robotic perception and control. However, current research predominantly focuses on only one of these modalities, neglecting the benefits of incorporating both. To this end, we present $\textbf{Semantic-Geometric Representation} (\textbf{SGR})$, a universal perception module for robotics that leverages the rich semantic information of large-scale pre-trained 2D models and inherits the merits of 3D spatial reasoning. Our experiments demonstrate that SGR empowers the agent to successfully complete a diverse range of simulated and real-world robotic manipulation tasks, outperforming state-of-the-art methods 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MVCIL&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#38543;&#26426;&#21270;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#25552;&#20986;&#27491;&#20132;&#34701;&#21512;&#23376;&#31354;&#38388;&#21644;&#36873;&#25321;&#24615;&#26435;&#37325;&#21512;&#24182;&#26469;&#35299;&#20915;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#26087;&#20449;&#24687;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;&#26377;&#25928;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.09675</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-View Class Incremental Learning. (arXiv:2306.09675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MVCIL&#65289;&#30340;&#26032;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#38543;&#26426;&#21270;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#25552;&#20986;&#27491;&#20132;&#34701;&#21512;&#23376;&#31354;&#38388;&#21644;&#36873;&#25321;&#24615;&#26435;&#37325;&#21512;&#24182;&#26469;&#35299;&#20915;&#22686;&#37327;&#23398;&#20064;&#20013;&#36951;&#24536;&#26087;&#20449;&#24687;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;&#26377;&#25928;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;MVL&#65289;&#22312;&#25972;&#21512;&#25968;&#25454;&#38598;&#30340;&#22810;&#20010;&#35270;&#35282;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20026;&#20102;&#20351;MVL&#26041;&#27861;&#22312;&#24320;&#25918;&#24335;&#29615;&#22659;&#20013;&#26356;&#23454;&#29992;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#31216;&#20026;&#22810;&#35270;&#35282;&#20998;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;MVCIL&#65289;&#65292;&#20854;&#20013;&#21333;&#20010;&#27169;&#22411;&#20174;&#36830;&#32493;&#30340;&#35270;&#22270;&#27969;&#20013;&#36880;&#27493;&#20998;&#31867;&#26032;&#31867;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#26089;&#26399;&#25968;&#25454;&#30340;&#35270;&#22270;&#12290;&#20294;&#26159;&#65292;MVCIL&#38754;&#20020;&#30528;&#32769;&#20449;&#24687;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#21270;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#20197;&#20445;&#35777;&#23427;&#20204;&#22312;&#24037;&#20316;&#29366;&#24577;&#19979;&#30340;&#20998;&#31163;&#35270;&#22270;&#26368;&#20248;&#65292;&#20854;&#20013;&#23646;&#20110;&#31867;&#30340;&#22810;&#20010;&#35270;&#22270;&#25353;&#39034;&#24207;&#21576;&#29616;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#36880;&#20010;&#38598;&#25104;&#21040;&#30001;&#25552;&#21462;&#30340;&#29305;&#24449;&#36328;&#36234;&#30340;&#27491;&#20132;&#34701;&#21512;&#23376;&#31354;&#38388;&#20013;&#65307;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#36873;&#25321;&#24615;&#26435;&#37325;&#21512;&#24182;&#65292;&#20197;&#20445;&#30041;&#26087;&#31867;&#30340;&#30693;&#35782;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view learning (MVL) has gained great success in integrating information from multiple perspectives of a dataset to improve downstream task performance. To make MVL methods more practical in an open-ended environment, this paper investigates a novel paradigm called multi-view class incremental learning (MVCIL), where a single model incrementally classifies new classes from a continual stream of views, requiring no access to earlier views of data. However, MVCIL is challenged by the catastrophic forgetting of old information and the interference with learning new concepts. To address this, we first develop a randomization-based representation learning technique serving for feature extraction to guarantee their separate view-optimal working states, during which multiple views belonging to a class are presented sequentially; Then, we integrate them one by one in the orthogonality fusion subspace spanned by the extracted features; Finally, we introduce selective weight consolidation f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21462;&#28040;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21518;&#22788;&#29702;&#23454;&#29616;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;Pareto&#36793;&#30028;&#21253;&#21547;&#20102;&#21487;&#35780;&#20272;&#30340;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07261</link><description>&lt;p&gt;
&#21462;&#28040;&#19971;&#24180;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unprocessing Seven Years of Algorithmic Fairness. (arXiv:2306.07261v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07261
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21462;&#28040;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#21518;&#22788;&#29702;&#23454;&#29616;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;Pareto&#36793;&#30028;&#21253;&#21547;&#20102;&#21487;&#35780;&#20272;&#30340;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19971;&#24180;&#21069;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20197;&#20351;&#27169;&#22411;&#22312;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#35823;&#24046;&#29575;&#30456;&#31561;&#12290;&#36825;&#39033;&#24037;&#20316;&#21551;&#21160;&#20102;&#25968;&#30334;&#31687;&#35770;&#25991;&#65292;&#22768;&#31216;&#33021;&#22815;&#25913;&#36827;&#21518;&#22788;&#29702;&#22522;&#32447;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20960;&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#25968;&#21315;&#20010;&#27169;&#22411;&#35780;&#20272;&#30340;&#23454;&#35777;&#35780;&#20272;&#26469;&#35780;&#20272;&#36825;&#20123;&#22768;&#26126;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21518;&#22788;&#29702;&#23454;&#29616;&#30340;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;Pareto&#36793;&#30028;&#21253;&#21547;&#25105;&#20204;&#21487;&#20197;&#35780;&#20272;&#30340;&#25152;&#26377;&#20854;&#20182;&#26041;&#27861;&#12290;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#20004;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#35770;&#38169;&#35823;&#65292;&#36825;&#20123;&#38169;&#35823;&#22256;&#25200;&#20102;&#20197;&#21069;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#19968;&#20010;&#19982;&#20351;&#29992;&#19981;&#21516;&#30340;&#26080;&#32422;&#26463;&#22522;&#30784;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#26377;&#20851;&#12290;&#21478;&#19968;&#20010;&#28041;&#21450;&#23454;&#29616;&#19981;&#21516;&#30340;&#32422;&#26463;&#25918;&#26494;&#27700;&#24179;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#24819;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21462;&#28040;&#22788;&#29702;&#65292;&#22823;&#33268;&#23545;&#24212;&#20110;&#21518;&#22788;&#29702;&#30340;&#21453;&#28436;&#12290;&#21462;&#28040;&#22788;&#29702;&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;&#20351;&#29992;&#19981;&#21516;&#22522;&#30784;&#27169;&#22411;&#21644;&#25918;&#26494;&#32423;&#21035;&#30340;&#26041;&#27861;&#12290;&#35299;&#35835;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation. Interpreting our findi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05880</link><description>&lt;p&gt;
&#22522;&#20110;Implicit Neural Representations&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#29992;&#20110;&#25554;&#20540;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26102;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;Implicit Neural Representations (INR)&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#25110;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#35843;&#21046;INR&#21442;&#25968;&#24182;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#35265;&#26679;&#26412;&#21644;&#26102;&#38388;&#31383;&#21475;&#31227;&#20301;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22788;&#29702;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#26041;&#38754;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;CVV-Pro&#65292;&#21487;&#20197;&#22788;&#29702;&#23545;&#25239;&#24615;&#30340;&#26102;&#21464;&#21644;&#38750;&#32447;&#24615;&#32422;&#26463;&#65292;&#21482;&#20381;&#36182;&#20110;&#23616;&#37096;&#31232;&#30095;&#32447;&#24615;&#36924;&#36817;&#65292;&#36798;&#21040;&#20102;$\sqrt{T}$&#36951;&#25022;&#29575;&#21644;$1/\sqrt{T}$&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.03655</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning under Adversarial Nonlinear Constraints. (arXiv:2306.03655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03655
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;CVV-Pro&#65292;&#21487;&#20197;&#22788;&#29702;&#23545;&#25239;&#24615;&#30340;&#26102;&#21464;&#21644;&#38750;&#32447;&#24615;&#32422;&#26463;&#65292;&#21482;&#20381;&#36182;&#20110;&#23616;&#37096;&#31232;&#30095;&#32447;&#24615;&#36924;&#36817;&#65292;&#36798;&#21040;&#20102;$\sqrt{T}$&#36951;&#25022;&#29575;&#21644;$1/\sqrt{T}$&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#22788;&#29702;&#36830;&#32493;&#30340;&#38750;&#31283;&#24577;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#22312;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#20013;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#23545;&#25239;&#24615;&#30340;&#26102;&#21464;&#21644;&#38750;&#32447;&#24615;&#32422;&#26463;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#36825;&#20010;&#21517;&#20026;Constraint Violation Velocity Projection (CVV-Pro)&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;$\sqrt{T}$&#30340;&#36951;&#25022;&#29575;&#65292;&#24182;&#20197;$1/\sqrt{T}$&#30340;&#36895;&#24230;&#25910;&#25947;&#20110;&#21487;&#34892;&#38598;&#65292;&#23613;&#31649;&#21487;&#34892;&#38598;&#32531;&#24930;&#22320;&#38543;&#26102;&#38388;&#21464;&#21270;&#19988;&#19981;&#20026;&#23398;&#20064;&#32773;&#25152;&#30693;&#12290;CVV-Pro&#20165;&#20381;&#36182;&#20110;&#21487;&#34892;&#38598;&#30340;&#23616;&#37096;&#31232;&#30095;&#32447;&#24615;&#36924;&#36817;&#65292;&#22240;&#27492;&#36991;&#20813;&#20102;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#20248;&#21270;&#25972;&#20010;&#38598;&#21512;&#65292;&#36825;&#19982;&#25237;&#24433;&#26799;&#24230;&#25110;Frank-Wolfe&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#25105;&#20204;&#36824;&#22312;&#20004;&#20010;&#29609;&#23478;&#28216;&#25103;&#20013;&#23545;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#20854;&#20013;&#29609;&#23478;&#21463;&#21040;&#20849;&#20139;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many applications, learning systems are required to process continuous non-stationary data streams. We study this problem in an online learning framework and propose an algorithm that can deal with adversarial time-varying and nonlinear constraints. As we show in our work, the algorithm called Constraint Violation Velocity Projection (CVV-Pro) achieves $\sqrt{T}$ regret and converges to the feasible set at a rate of $1/\sqrt{T}$, despite the fact that the feasible set is slowly time-varying and a priori unknown to the learner. CVV-Pro only relies on local sparse linear approximations of the feasible set and therefore avoids optimizing over the entire set at each iteration, which is in sharp contrast to projected gradients or Frank-Wolfe methods. We also empirically evaluate our algorithm on two-player games, where the players are subjected to a shared constraint.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.02865</link><description>&lt;p&gt;
&#25235;&#20303;&#24847;&#22806;&#25910;&#33719;&#65306;&#22312;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#20215;&#20540;(arXiv:2306.02865v2 [cs.LG]&#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39640;&#36136;&#37327;&#30340; Q &#20540;&#20989;&#25968;&#22312;&#35768;&#22810;&#29616;&#20195;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (RL) &#31639;&#27861;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#35299;&#20915;&#37319;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#21644;&#31163;&#32447;&#23398;&#20064;&#25152;&#23548;&#33268;&#30340;&#20540;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#19982;&#36825;&#31181;&#26222;&#36941;&#35266;&#28857;&#19981;&#21516;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040; Q &#20540;&#22312; RL &#35757;&#32451;&#36807;&#31243;&#30340;&#21518;&#26399;&#23454;&#38469;&#19978;&#34987;&#20302;&#20272;&#20102;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36125;&#23572;&#26364;&#26356;&#26032;&#20013;&#65292;&#24403;&#21069;&#31574;&#30053;&#20351;&#29992;&#27604;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#26356;&#20248;&#30340;&#21160;&#20316;&#26679;&#26412;&#24046;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20010;&#38271;&#26399;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#21487;&#33021;&#38459;&#30861;&#20102;&#31574;&#30053;&#23398;&#20064;&#65292;&#38477;&#20302;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#22312;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#30340;&#21516;&#26102;&#65292;&#32467;&#21512;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#21033;&#29992;&#21644;&#25506;&#32034; (BEE) &#25805;&#20316;&#31526;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21382;&#21490;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#21160;&#20316;&#21644;&#24403;&#21069;&#31574;&#30053;&#29983;&#25104;&#30340;&#21160;&#20316;&#26469;&#26356;&#26032; Q &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#36335;&#23398;&#20064;&#36755;&#20837;&#20302;&#32500;&#24230;&#34920;&#31034;&#21644;&#26368;&#23567;&#21270;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#22797;&#26434;&#24615;/&#19981;&#35268;&#21017;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25511;&#21046;&#20102;&#35268;&#24459;&#24615;&#65292;&#24182;&#21033;&#29992;&#29702;&#35770;&#24037;&#20855;&#35777;&#26126;&#20102;&#29942;&#39048;&#32467;&#26500;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2305.19008</link><description>&lt;p&gt;
&#23398;&#20064;&#29305;&#24449;&#20013;&#30340;&#29942;&#39048;&#32467;&#26500;&#65306;&#20302;&#32500;&#24230;&#19982;&#35268;&#24459;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Bottleneck Structure in Learned Features: Low-Dimension vs Regularity Tradeoff. (arXiv:2305.19008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#36335;&#23398;&#20064;&#36755;&#20837;&#20302;&#32500;&#24230;&#34920;&#31034;&#21644;&#26368;&#23567;&#21270;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#22797;&#26434;&#24615;/&#19981;&#35268;&#21017;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#25511;&#21046;&#20102;&#35268;&#24459;&#24615;&#65292;&#24182;&#21033;&#29992;&#29702;&#35770;&#24037;&#20855;&#35777;&#26126;&#20102;&#29942;&#39048;&#32467;&#26500;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#22823;&#28145;&#24230;$L$&#21644;$L_{2}$&#27491;&#21017;&#21270;&#30340;DNN&#20559;&#21521;&#20110;&#23398;&#20064;&#36755;&#20837;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#26368;&#23567;&#21270;&#23398;&#20064;&#20989;&#25968;$f$&#30340;&#31209;$R^{(0)}(f)$&#30340;&#27010;&#24565;&#65292;&#20854;&#34987;&#25512;&#27979;&#20026;&#29942;&#39048;&#31209;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;&#36825;&#20010;&#32467;&#26524;&#30340;&#26377;&#38480;&#28145;&#24230;&#20462;&#27491;&#65292;&#25581;&#31034;&#20102;&#19968;&#20010;&#24230;&#37327;$R^{(1)}$&#30340;&#35268;&#24459;&#24615;&#65292;&#23427;&#25511;&#21046;&#20102;&#38597;&#21487;&#27604;&#30697;&#38453;$\left|Jf(x)\right|_{+}$&#30340;&#20266;&#34892;&#21015;&#24335;&#24182;&#22312;&#32452;&#21512;&#21644;&#21152;&#27861;&#19979;&#26159;&#27425;&#21487;&#21152;&#30340;&#12290;&#36825;&#20351;&#24471;&#32593;&#32476;&#21487;&#20197;&#22312;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#21644;&#26368;&#23567;&#21270;&#29305;&#24449;&#26144;&#23556;&#20013;&#30340;&#22797;&#26434;&#24615;/&#19981;&#35268;&#21017;&#24615;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#20174;&#32780;&#23398;&#20064;&#8220;&#27491;&#30830;&#8221;&#30340;&#20869;&#37096;&#23610;&#23544;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22823;&#23398;&#20064;&#36895;&#29575;&#22914;&#20309;&#25511;&#21046;&#23398;&#20064;&#20989;&#25968;&#30340;&#35268;&#24459;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29702;&#35770;&#24037;&#20855;&#35777;&#26126;&#20102;&#29942;&#39048;&#32467;&#26500;&#22312;$L\to\infty$&#26102;&#22312;&#23398;&#20064;&#29305;&#24449;&#20013;&#30340;&#29468;&#24819;&#65306;&#23545;&#20110;&#22823;&#28145;&#24230;&#65292;&#20960;&#20046;&#25152;&#26377;&#30340;&#38544;&#34255;&#34920;&#31034;&#37117;&#38598;&#20013;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Previous work has shown that DNNs with large depth $L$ and $L_{2}$-regularization are biased towards learning low-dimensional representations of the inputs, which can be interpreted as minimizing a notion of rank $R^{(0)}(f)$ of the learned function $f$, conjectured to be the Bottleneck rank. We compute finite depth corrections to this result, revealing a measure $R^{(1)}$ of regularity which bounds the pseudo-determinant of the Jacobian $\left|Jf(x)\right|_{+}$ and is subadditive under composition and addition. This formalizes a balance between learning low-dimensional representations and minimizing complexity/irregularity in the feature maps, allowing the network to learn the `right' inner dimension. We also show how large learning rates also control the regularity of the learned function. Finally, we use these theoretical tools to prove the conjectured bottleneck structure in the learned features as $L\to\infty$: for large depths, almost all hidden representations concentrates aroun
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20215;&#20540;&#30446;&#26631;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#20540;&#23548;&#21521;&#30340;&#25968;&#25454;&#36807;&#28388;&#31639;&#27861;(VGDF)&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#31574;&#30053;&#36866;&#24212;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17625</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#31574;&#30053;&#36866;&#24212;&#24615;&#36890;&#36807;&#20215;&#20540;&#23548;&#21521;&#25968;&#25454;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Policy Adaptation via Value-Guided Data Filtering. (arXiv:2305.17625v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17625
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20215;&#20540;&#30446;&#26631;&#30340;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20215;&#20540;&#23548;&#21521;&#30340;&#25968;&#25454;&#36807;&#28388;&#31639;&#27861;(VGDF)&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#36827;&#34892;&#31574;&#30053;&#36866;&#24212;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#31574;&#30053;&#27867;&#21270;&#23384;&#22312;&#30528;&#21160;&#21147;&#23398;&#19981;&#21305;&#37197;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#26426;&#22120;&#20154;&#22312;&#27169;&#25311;&#22120;&#20013;&#23398;&#20064;&#31574;&#30053;&#65292;&#20294;&#24403;&#23427;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#26102;&#65292;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#38024;&#23545;&#21160;&#21147;&#23398;&#19981;&#21305;&#37197;&#30340;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#21160;&#21147;&#23398;&#36866;&#24212;&#38382;&#39064;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#35775;&#38382;&#36275;&#22815;&#30340;&#28304;&#22495;&#25968;&#25454;&#65292;&#32780;&#19982;&#30446;&#26631;&#22495;&#30340;&#22312;&#32447;&#20132;&#20114;&#26159;&#26377;&#38480;&#30340;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#35797;&#22270;&#20174;&#21160;&#21147;&#23398;&#24046;&#24322;&#30340;&#35282;&#24230;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#39046;&#22495;&#20043;&#38388;&#20215;&#20540;&#30340;&#19968;&#31181;&#26032;&#30340;&#27934;&#23519;&#65292;&#20174;&#20215;&#20540;&#24046;&#24322;&#30340;&#35282;&#24230;&#25506;&#32034;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20215;&#20540;&#30446;&#26631;&#23545;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#37197;&#23545;&#20540;&#30340;&#36317;&#31163;&#30340;Value-Guided Data Filtering (VGDF)&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#36873;&#25321;&#24615;&#22320;&#20849;&#20139;&#26469;&#33258;&#28304;&#22495;&#30340;&#36716;&#25442;&#12290;Empir&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalizing policies across different domains with dynamics mismatch poses a significant challenge in reinforcement learning. For example, a robot learns the policy in a simulator, but when it is deployed in the real world, the dynamics of the environment may be different. Given the source and target domain with dynamics mismatch, we consider the online dynamics adaptation problem, in which case the agent can access sufficient source domain data while online interactions with the target domain are limited. Existing research has attempted to solve the problem from the dynamics discrepancy perspective. In this work, we reveal the limitations of these methods and explore the problem from the value difference perspective via a novel insight on the value consistency across domains. Specifically, we present the Value-Guided Data Filtering (VGDF) algorithm, which selectively shares transitions from the source domain based on the proximity of paired value targets across the two domains. Empir
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#33021;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#22270;&#20687;&#36755;&#20986;&#65292;&#21516;&#26102;&#20063;&#33021;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2305.17216</link><description>&lt;p&gt;
&#29992;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Generating Images with Multimodal Language Models. (arXiv:2305.17216v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#33021;&#29983;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#30340;&#22270;&#20687;&#36755;&#20986;&#65292;&#21516;&#26102;&#20063;&#33021;&#36827;&#34892;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#20165;&#21253;&#21547;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34701;&#21512;&#65292;&#36890;&#36807;&#26144;&#23556;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65306;&#22270;&#20687;&#26816;&#32034;&#12289;&#26032;&#39062;&#22270;&#20687;&#29983;&#25104;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#22312;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#36827;&#34892;&#26465;&#20214;&#35843;&#33410;&#65292;&#29983;&#25104;&#36830;&#36143;&#22270;&#20687;&#65288;&#21644;&#25991;&#26412;&#65289;&#36755;&#20986;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26144;&#23556;&#32593;&#32476;&#65292;&#23558;LLM&#22522;&#20110;&#29616;&#25104;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#30340;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#20026;&#35270;&#35273;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#65292;&#21033;&#29992;LLM&#24378;&#22823;&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#29983;&#25104;&#35270;&#35273;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#19988;&#22797;&#26434;&#35821;&#35328;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#22522;&#20934;&#29983;&#25104;&#27169;&#22411;&#12290;&#38500;&#20102;&#26032;&#39062;&#22270;&#20687;&#29983;&#25104;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#33021;&#22815;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#26816;&#32034;&#22270;&#20687;&#65292;&#24182;&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#31616;&#21333;&#30340;&#32447;&#24615;&#26356;&#26032;&#26469;&#22788;&#29702;&#20851;&#31995;&#20219;&#21153;&#65292;&#24182;&#20197;&#20869;&#23481;&#26080;&#20851;&#30340;&#26041;&#24335;&#20419;&#36827;&#20851;&#31995;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2305.16130</link><description>&lt;p&gt;
&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Mechanism for Solving Relational Tasks in Transformer Language Models. (arXiv:2305.16130v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16130
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#31616;&#21333;&#30340;&#32447;&#24615;&#26356;&#26032;&#26469;&#22788;&#29702;&#20851;&#31995;&#20219;&#21153;&#65292;&#24182;&#20197;&#20869;&#23481;&#26080;&#20851;&#30340;&#26041;&#24335;&#20419;&#36827;&#20851;&#31995;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#26377;&#26102;&#20505;&#21033;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#35745;&#31639;&#26426;&#21046;&#26469;&#35299;&#20915;&#19968;&#23545;&#19968;&#30340;&#20851;&#31995;&#20219;&#21153;&#65288;&#20363;&#22914; capital_of(Poland)=Warsaw&#65289;&#12290;&#25105;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#65288;&#20174;124M&#21442;&#25968;&#21040;176B&#21442;&#25968;&#65289;&#65292;&#24182;&#21457;&#29616;&#23545;&#20110;&#22810;&#31181;&#20219;&#21153;&#65288;&#28041;&#21450;&#39318;&#37117;&#12289;&#22823;&#20889;&#21644;&#36807;&#21435;&#26102;&#24577;&#31561;&#65289;&#65292;&#26426;&#21046;&#30340;&#20851;&#38190;&#37096;&#20998;&#21487;&#20197;&#31616;&#21270;&#20026;&#21069;&#39304;&#65288;FFN&#65289;&#32593;&#32476;&#36890;&#24120;&#24212;&#29992;&#30340;&#31616;&#21333;&#32447;&#24615;&#26356;&#26032;&#12290;&#36825;&#20123;&#26356;&#26032;&#20063;&#20542;&#21521;&#20110;&#20197;&#20869;&#23481;&#26080;&#20851;&#30340;&#26041;&#24335;&#20419;&#36827;&#20851;&#31995;&#30340;&#36755;&#20986;&#65288;&#20363;&#22914;&#23545;&#32534;&#30721; Poland:Warsaw::China:Beijing&#65289;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#21487;&#39044;&#27979;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26174;&#31034;&#36825;&#20010;&#26426;&#21046;&#26159;&#29305;&#23450;&#20110;&#38656;&#35201;&#20174;&#39044;&#35757;&#32451;&#23384;&#20648;&#22120;&#20013;&#26816;&#32034;&#32780;&#19981;&#26159;&#20174;&#23616;&#37096;&#19978;&#19979;&#25991;&#26816;&#32034;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#35299;&#20915;&#20851;&#31995;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple computational mechanism to solve one-to-one relational tasks (e.g., capital_of(Poland)=Warsaw). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, upper-casing, and past-tensing) a key part of the mechanism reduces to a simple linear update typically applied by the feedforward (FFN) networks. These updates also tend to promote the output of the relation in a content-independent way (e.g., encoding Poland:Warsaw::China:Beijing), revealing a predictable pattern that these models take in solving these tasks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#21464;&#37327;&#22788;&#29702;&#24773;&#20917;&#19979;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25512;&#26029;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.15742</link><description>&lt;p&gt;
&#26102;&#38388;&#21464;&#21270;&#22788;&#29702;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Generative Models for Time-Varying Treatments. (arXiv:2305.15742v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#21464;&#37327;&#22788;&#29702;&#24773;&#20917;&#19979;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25512;&#26029;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#26159;&#27979;&#35797;&#26032;&#30103;&#27861;&#30340;&#24120;&#29992;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#25928;&#24212;&#20250;&#25513;&#30422;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#37325;&#35201;&#30340;&#20010;&#20307;&#29305;&#24449;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26102;&#38388;&#35774;&#32622;&#20013;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#22788;&#29702;&#26159;&#26102;&#24207;&#30340;&#21644;&#26102;&#21464;&#30340;&#65292;&#23545;&#21453;&#20107;&#23454;&#20998;&#24067;&#20135;&#29983;&#20102;&#38169;&#32508;&#22797;&#26434;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#25429;&#33719;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#20801;&#35768;&#23545;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;&#36825;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23588;&#20854;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#36890;&#36807;&#36793;&#38469;&#32467;&#26500;&#27169;&#22411;&#35880;&#24910;&#22320;&#35299;&#20915;&#20102;&#35266;&#23519;&#25968;&#25454;&#21644;&#30446;&#26631;&#21453;&#20107;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating average causal effects is a common practice to test new treatments. However, the average effect ''masks'' important individual characteristics in the counterfactual distribution, which may lead to safety, fairness, and ethical concerns. This issue is exacerbated in the temporal setting, where the treatment is sequential and time-varying, leading to an intricate influence on the counterfactual distribution. In this paper, we propose a novel conditional generative modeling approach to capture the whole counterfactual distribution, allowing efficient inference on certain statistics of the counterfactual distribution. This makes the proposed approach particularly suitable for healthcare and public policy making. Our generative modeling approach carefully tackles the distribution mismatch in the observed data and the targeted counterfactual distribution via a marginal structural model. Our method outperforms state-of-the-art baselines on both synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#19988;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#26657;&#27491;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14707</link><description>&lt;p&gt;
&#23398;&#29983;&#36229;&#36234;&#20102;&#22823;&#24072;&#65306;&#22522;&#20110;GPT-3&#30340;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#26041;&#27861;&#30340;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
The student becomes the master: Matching GPT3 on Scientific Factual Error Correction. (arXiv:2305.14707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#19988;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31185;&#23398;&#20107;&#23454;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#26657;&#27491;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21019;&#24314;&#38169;&#35823;&#26657;&#27491;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#26497;&#39640;&#65292;&#22823;&#22810;&#25968;&#20107;&#23454;&#20027;&#24352;&#26657;&#27491;&#26041;&#27861;&#20381;&#36182;&#20110;&#24378;&#22823;&#30340;&#39564;&#35777;&#27169;&#22411;&#26469;&#25351;&#23548;&#26657;&#27491;&#36807;&#31243;&#12290;&#36825;&#23548;&#33268;&#22312;&#31185;&#23398;&#20107;&#23454;&#26657;&#27491;&#31561;&#39046;&#22495;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#22909;&#30340;&#39564;&#35777;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#23384;&#22312;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#19981;&#20570;&#39046;&#22495;&#20551;&#35774;&#19988;&#19981;&#38656;&#35201;&#39564;&#35777;&#32773;&#30340;&#20027;&#24352;&#26657;&#27491;&#31995;&#32479;&#65292;&#20294;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021; - &#22312;SciFact&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;94&#65285;&#30340;&#20462;&#27491;&#20934;&#30830;&#24615;&#65292;&#22312;SciFact-Open&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;62.5&#65285;&#30340;&#20462;&#27491;&#20934;&#30830;&#24615;&#65292;&#20998;&#21035;&#27604;&#19979;&#19968;&#20010;&#26368;&#22909;&#30340;&#26041;&#27861;&#39640;&#20986;0.5&#65285;&#21644;1.50&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#20013;&#30340;&#25552;&#31034;&#21151;&#33021;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#21019;&#24314;&#19968;&#20010;&#20016;&#23500;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#23436;&#20840;&#30417;&#30563;&#30340;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20027;&#24352;&#24863;&#30693;&#30340;&#35299;&#30721;&#36807;&#31243;&#26469;&#25552;&#39640;&#32416;&#27491;&#20027;&#24352;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29992;&#20110;&#21019;&#24314;&#25968;&#25454;&#38598;&#30340;LLM&#30456;&#31454;&#20105;&#65292;&#35777;&#26126;&#20102;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#35757;&#32451;&#25552;&#39640;&#31185;&#23398;&#20027;&#24352;&#26657;&#27491;&#20219;&#21153;&#24615;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the prohibitively high cost of creating error correction datasets, most Factual Claim Correction methods rely on a powerful verification model to guide the correction process. This leads to a significant drop in performance in domains like Scientific Claim Correction, where good verification models do not always exist. In this work, we introduce a claim correction system that makes no domain assumptions and does not require a verifier but is able to outperform existing methods by an order of magnitude -- achieving 94% correction accuracy on the SciFact dataset, and 62.5% on the SciFact-Open dataset, compared to the next best methods 0.5% and 1.50% respectively. Our method leverages the power of prompting with LLMs during training to create a richly annotated dataset that can be used for fully supervised training and regularization. We additionally use a claim-aware decoding procedure to improve the quality of corrected claims. Our method is competitive with the very LLM that was
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Newton-Cotes&#20844;&#24335;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#36739;&#65292; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.14642</link><description>&lt;p&gt;
Newton-Cotes&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#35770;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Newton-Cotes Graph Neural Networks: On the Time Evolution of Dynamic Systems. (arXiv:2305.14642v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Newton-Cotes&#20844;&#24335;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#21160;&#24577;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#36739;&#65292; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#31995;&#32479;&#21160;&#24577;&#24615;&#26159;&#35768;&#22810;&#31185;&#23398;&#30740;&#31350;&#20013;&#26368;&#37325;&#35201;&#30340;&#20998;&#26512;&#26041;&#27861;&#20043;&#19968;&#12290;&#20351;&#29992;&#31995;&#32479;&#30340;&#21021;&#22987;&#29366;&#24577;&#20316;&#20026;&#36755;&#20837;&#65292;&#26368;&#36817;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26041;&#27861;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#36828;&#31163;&#21021;&#22987;&#29366;&#24577;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#22312;&#24314;&#27169;&#31995;&#32479;&#30340;&#22352;&#26631;&#21644;&#30456;&#20114;&#20316;&#29992;&#21147;&#26041;&#38754;&#26377;&#19981;&#21516;&#30340;&#35774;&#35745;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#23454;&#38469;&#19978;&#20849;&#20139;&#19968;&#31181;&#24120;&#25968;&#30340;&#31215;&#20998;&#23398;&#20064;&#33539;&#20363;&#65292; &#33021;&#22815;&#22312;&#21021;&#22987;&#21644;&#32456;&#31471;&#22352;&#26631;&#20043;&#38388;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#39044;&#27979;&#36895;&#24230;&#30340;&#31215;&#20998;&#12290;&#21463;&#27492;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#29992;Newton-Cotes&#20844;&#24335;&#20272;&#31639;&#30340;&#33509;&#24178;&#36895;&#24230;&#20272;&#35745;&#26469;&#39044;&#27979;&#31215;&#20998;&#65292;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290; &#22312;&#20960;&#20010;&#22522;&#20934;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#26041;&#27861;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#26377;&#30528;&#25345;&#32493;&#19988;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning system dynamics is one of the most important analytical approaches for many scientific studies. With the initial state of a system as input, the recent graph neural networks (GNNs)-based methods are capable of predicting the future state distant in time with high accuracy. Although these methods have diverse designs in modeling the coordinates and interacting forces of the system, we show that they actually share a common paradigm that learns the integration of the velocity over the interval between the initial and terminal coordinates. However, their integrand is constant w.r.t. time. Inspired by this observation, we propose a new approach to predict the integration based on several velocity estimations with Newton-Cotes formulas and prove its effectiveness theoretically. Extensive experiments on several benchmarks empirically demonstrate consistent and significant improvement compared with the state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#23558;&#27492;&#36816;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14600</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Learning Semantic Role Labeling from Compatible Label Sequences. (arXiv:2305.14600v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14600
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#23558;&#27492;&#36816;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#23398;&#20064;&#20174;&#19981;&#30456;&#20132;&#30340;&#20860;&#23481;&#26631;&#31614;&#24207;&#21015;&#20013;&#26631;&#27880;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#30456;&#20132;&#26631;&#31614;&#38598;&#20043;&#38388;&#30340;&#20860;&#23481;&#32467;&#26500;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#20855;&#20307;&#22320;&#65292;&#26631;&#35760;&#20855;&#26377;&#20004;&#20010;&#35282;&#33394;&#24207;&#21015;&#30340;&#21477;&#23376;&#65306;VerbNet&#21442;&#25968;&#21644;PropBank&#21442;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#36328;&#20219;&#21153;&#20132;&#20114;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#20173;&#28982;&#26159;&#20998;&#21035;&#35299;&#30721;&#30340;&#65292;&#23384;&#22312;&#29983;&#25104;&#32467;&#26500;&#19981;&#19968;&#33268;&#30340;&#26631;&#31614;&#24207;&#21015; (&#22312;&#20687;SEMLINK&#30340;&#35789;&#20856;&#20013;)&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35774;&#32622;&#65292;&#32852;&#21512;&#22788;&#29702;VerbNet&#21644;PropBank&#26631;&#31614;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#12290;&#36890;&#36807;&#36825;&#20010;&#35774;&#32622;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#24378;&#21046;&#25191;&#34892;SEMLINK&#32422;&#26463;&#19981;&#26029;&#25552;&#39640;&#24635;F1&#20540;&#12290;&#36890;&#36807;&#29305;&#27530;&#30340;&#36755;&#20837;&#26500;&#36896;&#65292;&#25105;&#20204;&#30340;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#20197;&#36229;&#36807;99%&#30340;&#20934;&#30830;&#24615;&#20174;PropBank&#21442;&#25968;&#20013;&#25512;&#26029;&#20986;VerbNet&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;co
&lt;/p&gt;
&lt;p&gt;
This paper addresses the question of how to efficiently learn from disjoint, compatible label sequences. We argue that the compatible structures between disjoint label sets help model learning and inference. We verify this hypothesis on the task of semantic role labeling (SRL), specifically, tagging a sentence with two role sequences: VerbNet arguments and PropBank arguments. Prior work has shown that cross-task interaction improves performance. However, the two tasks are still separately decoded, running the risk of generating structurally inconsistent label sequences (as per lexicons like SEMLINK). To eliminate this issue, we first propose a simple and effective setup that jointly handles VerbNet and PropBank labels as one sequence. With this setup, we show that enforcing SEMLINK constraints during decoding constantly improves the overall F1. With special input constructions, our joint model infers VerbNet arguments from PropBank arguments with over 99% accuracy. We also propose a co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20284;&#28982;&#27604;&#26041;&#27861;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#26080;&#38656;&#20351;&#29992;&#36882;&#24402;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#22312;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19978;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#27169;&#22411;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.08960</link><description>&lt;p&gt;
&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65306;&#28145;&#20837;&#25506;&#31350;&#20284;&#28982;&#27604;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Training Neural Networks without Backpropagation: A Deeper Dive into the Likelihood Ratio Method. (arXiv:2305.08960v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20284;&#28982;&#27604;&#26041;&#27861;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#26080;&#38656;&#20351;&#29992;&#36882;&#24402;&#26799;&#24230;&#35745;&#31639;&#65292;&#24182;&#22312;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19978;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#27169;&#22411;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#20256;&#25773;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#37325;&#35201;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#34920;&#26126;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20284;&#28982;&#27604;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26080;&#38656;&#36882;&#24402;&#26799;&#24230;&#35745;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#26377;&#25928;&#22320;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#26799;&#24230;&#20272;&#35745;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24471;&#21040;&#20102;&#25968;&#20540;&#32467;&#26524;&#12290;&#25152;&#26377;&#32467;&#26524;&#37117;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#65292;&#20284;&#28982;&#27604;&#26041;&#27861;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#26377;&#25928;&#22320;&#35757;&#32451;&#20102;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation (BP) is the most important gradient estimation method for training neural networks in deep learning. However, the literature shows that neural networks trained by BP are vulnerable to adversarial attacks. We develop the likelihood ratio (LR) method, a new gradient estimation method, for training a broad range of neural network architectures, including convolutional neural networks, recurrent neural networks, graph neural networks, and spiking neural networks, without recursive gradient computation. We propose three methods to efficiently reduce the variance of the gradient estimation in the neural network training process. Our experiments yield numerical results for training different neural networks on several datasets. All results demonstrate that the LR method is effective for training various neural networks and significantly improves the robustness of the neural networks under adversarial attacks relative to the BP method.
&lt;/p&gt;</description></item><item><title>DOCTOR&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;&#23427;&#21487;&#20197;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05738</link><description>&lt;p&gt;
DOCTOR&#65306;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors. (arXiv:2305.05738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05738
&lt;/p&gt;
&lt;p&gt;
DOCTOR&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;&#23427;&#21487;&#20197;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#36793;&#32536;&#35774;&#22791;&#20013;&#30340;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#65288;WMS&#65289;&#30340;&#36827;&#27493;&#20351;&#24471;&#26234;&#33021;&#21307;&#30103;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#25104;&#20026;&#21487;&#33021;&#12290;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#20026;&#27599;&#31181;&#30142;&#30149;&#21644;&#30456;&#24212;&#30340;WMS&#25968;&#25454;&#23450;&#21046;&#20010;&#21035;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26032;&#20219;&#21153;&#20998;&#31867;&#30340;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26816;&#27979;&#27599;&#20010;&#26032;&#30142;&#30149;&#65292;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#26500;&#24314;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;WMS&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;DOCTOR&#12290;&#23427;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#19968;&#31181;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;CL&#31639;&#27861;&#20351;&#24471;&#26694;&#26550;&#33021;&#22815;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12289;&#20998;&#31867;&#31867;&#21035;&#21644;&#30142;&#30149;&#26816;&#27979;&#20219;&#21153;&#12290;DOCTOR&#22312;&#20351;&#29992;&#26469;&#33258;&#23454;&#38469;WMS&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#22235;&#31181;&#24120;&#35265;&#30142;&#30149;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#65292;DOCTOR&#20063;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern advances in machine learning (ML) and wearable medical sensors (WMSs) in edge devices have enabled ML-driven disease detection for smart healthcare. Conventional ML-driven disease detection methods rely on customizing individual models for each disease and its corresponding WMS data. However, such methods lack adaptability to distribution shifts and new task classification classes. Also, they need to be rearchitected and retrained from scratch for each new disease. Moreover, installing multiple ML models in an edge device consumes excessive memory, drains the battery faster, and complicates the detection process. To address these challenges, we propose DOCTOR, a multi-disease detection continual learning (CL) framework based on WMSs. It employs a multi-headed deep neural network (DNN) and an exemplar-replay-style CL algorithm. The CL algorithm enables the framework to continually learn new missions where different data distributions, classification classes, and disease detection
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#20316;&#20026;&#24418;&#29366;&#34920;&#31034;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#25152;&#24471;&#65292;&#23545;&#26059;&#36716;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#26469;&#29983;&#25104;&#36924;&#30495;&#30340;&#27963;&#32454;&#32990;&#27169;&#22411;&#65292;&#20026;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#32454;&#32990;&#36319;&#36394;&#21644;&#20998;&#21106;&#26041;&#27861;&#25552;&#20379;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.08960</link><description>&lt;p&gt;
&#20351;&#29992;SO(3)-&#31561;&#21464;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#29983;&#25104;&#27963;&#32454;&#32990;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative modeling of living cells with SO(3)-equivariant implicit neural representations. (arXiv:2304.08960v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#20316;&#20026;&#24418;&#29366;&#34920;&#31034;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#25152;&#24471;&#65292;&#23545;&#26059;&#36716;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#26469;&#29983;&#25104;&#36924;&#30495;&#30340;&#27963;&#32454;&#32990;&#27169;&#22411;&#65292;&#20026;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#32454;&#32990;&#36319;&#36394;&#21644;&#20998;&#21106;&#26041;&#27861;&#25552;&#20379;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20013;&#22522;&#20110;&#25968;&#25454;&#30340;&#32454;&#32990;&#36319;&#36394;&#21644;&#20998;&#21106;&#26041;&#27861;&#38656;&#35201;&#22810;&#26679;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#24403;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#26102;&#65292;&#21487;&#20197;&#20351;&#29992;&#21512;&#25104;&#30340;&#35745;&#31639;&#26426;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#36825;&#20123;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#38656;&#35201;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#32454;&#32990;&#24418;&#29366;&#20197;&#21450;&#30456;&#24212;&#30340;&#26174;&#24494;&#38236;&#22270;&#20687;&#12290;&#20026;&#20102;&#21512;&#25104;&#36924;&#30495;&#30340;&#27963;&#32454;&#32990;&#24418;&#24577;&#65292;&#29983;&#25104;&#27169;&#22411;&#20351;&#29992;&#30340;&#24418;&#29366;&#34920;&#31034;&#24212;&#33021;&#22815;&#20934;&#30830;&#34920;&#31034;&#32454;&#33410;&#21644;&#25299;&#25169;&#21464;&#21270;&#65292;&#36825;&#22312;&#32454;&#32990;&#20013;&#24456;&#24120;&#35265;&#12290;&#36825;&#20123;&#35201;&#27714;&#24182;&#19981;&#36866;&#29992;&#20110;3D&#20307;&#32032;&#25513;&#27169;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#20998;&#36776;&#29575;&#38480;&#21046;&#65292;&#20063;&#19981;&#36866;&#29992;&#20110;&#22810;&#36793;&#24418;&#32593;&#26684;&#65292;&#22240;&#20026;&#26080;&#27861;&#26131;&#20110;&#27169;&#25311;&#32454;&#32990;&#22686;&#38271;&#21644;&#26377;&#19997;&#20998;&#35010;&#31561;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26377;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDFs&#65289;&#30340;&#27700;&#24179;&#38598;&#26469;&#34920;&#31034;&#27963;&#32454;&#32990;&#24418;&#29366;&#65292;&#36825;&#20123;&#27700;&#24179;&#38598;&#30001;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#24471;&#20986;&#65292;&#32780;&#19988;&#23545;&#26059;&#36716;&#20855;&#26377;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#27492;&#34920;&#31034;&#36716;&#25442;&#20026;&#32593;&#26684;&#21644;RGB&#22270;&#20687;&#20197;&#36827;&#34892;&#21487;&#35270;&#21270;&#21644;&#29992;&#20110;&#19979;&#28216;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven cell tracking and segmentation methods in biomedical imaging require diverse and information-rich training data. In cases where the number of training samples is limited, synthetic computer-generated data sets can be used to improve these methods. This requires the synthesis of cell shapes as well as corresponding microscopy images using generative models. To synthesize realistic living cell shapes, the shape representation used by the generative model should be able to accurately represent fine details and changes in topology, which are common in cells. These requirements are not met by 3D voxel masks, which are restricted in resolution, and polygon meshes, which do not easily model processes like cell growth and mitosis. In this work, we propose to represent living cell shapes as level sets of signed distance functions (SDFs) which are estimated by neural networks. We optimize a fully-connected neural network to provide an implicit representation of the SDF value at any p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27744;&#21270;&#31639;&#23376;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26631;&#20934;&#26469;&#36873;&#25321;&#25110;&#35774;&#35745;&#27744;&#21270;&#31639;&#23376;&#12290;</title><link>http://arxiv.org/abs/2304.01575</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#27744;&#21270;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The expressive power of pooling in Graph Neural Networks. (arXiv:2304.01575v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27744;&#21270;&#31639;&#23376;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#26631;&#20934;&#26469;&#36873;&#25321;&#25110;&#35774;&#35745;&#27744;&#21270;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#65292;&#20998;&#23618;&#27744;&#21270;&#31639;&#23376;&#36890;&#36807;&#21019;&#24314;&#22270;&#32467;&#26500;&#21644;&#20854;&#39030;&#28857;&#29305;&#24449;&#30340;&#26412;&#22320;&#25688;&#35201;&#26469;&#29983;&#25104;&#36755;&#20837;&#25968;&#25454;&#30340;&#26356;&#31895;&#31961;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#24050;&#32463;&#33268;&#21147;&#20110;&#30740;&#31350;GNN&#20013;&#28040;&#24687;&#20256;&#36882;&#65288;MP&#65289;&#23618;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#20851;&#20110;&#27744;&#21270;&#31639;&#23376;&#22914;&#20309;&#24433;&#21709;GNN&#34920;&#36798;&#33021;&#21147;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#26368;&#36817;&#22312;&#26377;&#25928;&#27744;&#21270;&#31639;&#23376;&#30340;&#35774;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26631;&#20934;&#26469;&#27604;&#36739;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#36275;&#22815;&#30340;&#26465;&#20214;&#20351;&#27744;&#21270;&#31639;&#23376;&#22312;&#20854;&#20043;&#21069;&#30340;MP&#23618;&#20013;&#23436;&#20840;&#20445;&#30041;&#34920;&#36798;&#33021;&#21147;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#36825;&#20123;&#26465;&#20214;&#20316;&#20026;&#36873;&#25321;&#29616;&#26377;&#27744;&#21270;&#31639;&#23376;&#25110;&#35774;&#35745;&#26032;&#30340;&#27744;&#21270;&#31639;&#23376;&#30340;&#36890;&#29992;&#21644;&#29702;&#35770;&#22522;&#30784;&#30340;&#26631;&#20934;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#20960;&#20010;&#29616;&#26377;&#30340;&#27744;&#21270;&#31639;&#23376;&#65292;&#24182;&#30830;&#23450;&#20102;&#37027;&#20123;&#19981;&#33021;&#28385;&#36275;&#34920;&#36798;&#24615;&#20551;&#35774;&#30340;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Graph Neural Networks (GNNs), hierarchical pooling operators generate a coarser representation of the input data by creating local summaries of the graph structure and its vertex features. Considerable attention has been devoted to studying the expressive power of message-passing (MP) layers in GNNs, while a study on how pooling operators affect the expressivity of a GNN is still lacking. Additionally, despite the recent advances in the design of effective pooling operators, there is not a principled criterion to compare them. Our work aims to fill this gap by providing sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically-grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we reviewed several existing pooling operators and identified those that fail to satisfy the expressiveness assumptions. Finally,
&lt;/p&gt;</description></item><item><title>Topograph&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#31890;&#23376;&#34928;&#21464;&#33258;&#28982;&#35268;&#24459;&#30340;&#25299;&#25169;&#32467;&#26500;&#37325;&#24314;&#26041;&#27861;&#65292;&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#36824;&#39044;&#27979;&#20102;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#65292;&#27604;&#26631;&#20934;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#65292;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2303.13937</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#37325;&#24314;&#31890;&#23376;&#29289;&#29702;&#36807;&#31243;&#30340;&#25299;&#25169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Topological Reconstruction of Particle Physics Processes using Graph Neural Networks. (arXiv:2303.13937v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13937
&lt;/p&gt;
&lt;p&gt;
Topograph&#26159;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#31890;&#23376;&#34928;&#21464;&#33258;&#28982;&#35268;&#24459;&#30340;&#25299;&#25169;&#32467;&#26500;&#37325;&#24314;&#26041;&#27861;&#65292;&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#36824;&#39044;&#27979;&#20102;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#65292;&#27604;&#26631;&#20934;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#65292;&#19982;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Topograph&#65292;&#23427;&#21033;&#29992;&#31890;&#23376;&#29289;&#29702;&#34928;&#21464;&#30340;&#26412;&#36136;&#21644;&#20449;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#65292;&#37325;&#24314;&#20102;&#21253;&#25324;&#20013;&#20171;&#31890;&#23376;&#22312;&#20869;&#30340;&#24213;&#23618;&#29289;&#29702;&#36807;&#31243;&#12290;Topograph&#19981;&#20165;&#35299;&#20915;&#20102;&#35266;&#27979;&#21040;&#30340;&#26411;&#24577;&#23545;&#35937;&#30340;&#32452;&#21512;&#25351;&#27966;&#38382;&#39064;&#65292;&#23558;&#23427;&#20204;&#19982;&#23427;&#20204;&#21407;&#26469;&#30340;&#27597;&#31890;&#23376;&#20851;&#32852;&#36215;&#26469;&#65292;&#32780;&#19988;&#30452;&#25509;&#39044;&#27979;&#20102;&#30828;&#25955;&#23556;&#36807;&#31243;&#20013;&#20013;&#38388;&#31890;&#23376;&#30340;&#24615;&#36136;&#21450;&#20854;&#21518;&#32493;&#34928;&#21464;&#12290;&#19982;&#26631;&#20934;&#30340;&#32452;&#21512;&#26041;&#27861;&#25110;&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#30340;&#22797;&#26434;&#24230;&#19982;&#37325;&#26500;&#23545;&#35937;&#30340;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#24212;&#29992;Topograph&#20110;&#20840;&#24378;&#23376;&#34928;&#21464;&#27169;&#24335;&#19979;&#30340;&#39030;&#22840;&#20811;&#23545;&#20135;&#29983;&#38382;&#39064;&#65292;&#30456;&#23545;&#26631;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new approach, the Topograph, which reconstructs underlying physics processes, including the intermediary particles, by leveraging underlying priors from the nature of particle physics decays and the flexibility of message passing graph neural networks. The Topograph not only solves the combinatoric assignment of observed final state objects, associating them to their original mother particles, but directly predicts the properties of intermediate particles in hard scatter processes and their subsequent decays. In comparison to standard combinatoric approaches or modern approaches using graph neural networks, which scale exponentially or quadratically, the complexity of Topographs scales linearly with the number of reconstructed objects.  We apply Topographs to top quark pair production in the all hadronic decay channel, where we outperform the standard approach and match the performance of the state-of-the-art machine learning technique.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09716</link><description>&lt;p&gt;
&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#25919;&#31574;&#36845;&#20195;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games. (arXiv:2303.09716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#20004;&#20010;&#38454;&#27573;: &#23398;&#20064;&#38454;&#27573;&#21644;&#35268;&#21010;&#38454;&#27573;&#12290;&#22312;&#26631;&#20934;MDPs&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#20215;&#20540;&#36845;&#20195;&#25110;&#31574;&#30053;&#36845;&#20195;&#26469;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#20294;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65292;&#20197;&#21069;&#30340;&#23581;&#35797;&#37117;&#26377;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#36845;&#20195;&#21464;&#20307;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in Hansen et al. (2013) that one has to solve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#24577;&#30340;&#26143;&#31995;&#24418;&#24577;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#22495;&#36866;&#24212;&#23454;&#29616;&#20302;&#32500;&#24230;&#34920;&#31034;&#65292;40&#32500;&#28508;&#22312;&#21464;&#37327;&#33021;&#22815;&#26377;&#25928;&#20877;&#29616;&#26143;&#31995;&#22270;&#20687;&#20013;&#30340;&#22823;&#22810;&#25968;&#24418;&#24577;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#32463;&#20856;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#35814;&#32454;&#30340;&#24418;&#24577;&#29305;&#24449;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26080;&#20559;&#22320;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#26143;&#31995;&#35843;&#26597;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.08627</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#22495;&#36866;&#24212;&#23454;&#29616;&#26080;&#20559;&#24577;&#24418;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
From Images to Features: Unbiased Morphology Classification via Variational Auto-Encoders and Domain Adaptation. (arXiv:2303.08627v1 [astro-ph.GA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#24577;&#30340;&#26143;&#31995;&#24418;&#24577;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#22495;&#36866;&#24212;&#23454;&#29616;&#20302;&#32500;&#24230;&#34920;&#31034;&#65292;40&#32500;&#28508;&#22312;&#21464;&#37327;&#33021;&#22815;&#26377;&#25928;&#20877;&#29616;&#26143;&#31995;&#22270;&#20687;&#20013;&#30340;&#22823;&#22810;&#25968;&#24418;&#24577;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#32463;&#20856;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#35814;&#32454;&#30340;&#24418;&#24577;&#29305;&#24449;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26080;&#20559;&#22320;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#30340;&#26143;&#31995;&#35843;&#26597;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#21644;&#22495;&#36866;&#24212;&#65288;DA&#65289;&#30340;&#32452;&#21512;&#26469;&#38477;&#20302;&#26143;&#31995;&#22270;&#20687;&#30340;&#32500;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;Galaxy-Zoo DECaLS&#39033;&#30446;&#20013;&#20855;&#26377;&#35814;&#32454;&#24418;&#24577;&#31867;&#22411;&#26631;&#31614;&#30340;&#20302;&#32418;&#31227;&#26143;&#31995;&#30340;&#26679;&#26412;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;40&#32500;&#28508;&#22312;&#21464;&#37327;&#33021;&#22815;&#26377;&#25928;&#22320;&#20877;&#29616;&#26143;&#31995;&#22270;&#20687;&#20013;&#30340;&#22823;&#22810;&#25968;&#24418;&#24577;&#29305;&#24449;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;40&#32500;&#28508;&#22312;&#21464;&#37327;&#19978;&#30340;&#32463;&#20856;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#20998;&#31867;&#22120;&#26469;&#23454;&#29616;&#35814;&#32454;&#30340;&#24418;&#24577;&#29305;&#24449;&#20998;&#31867;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#30452;&#25509;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#25928;&#26524;&#30456;&#20284;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20351;&#29992;DECaLS&#21644;BASS+MzLS&#37325;&#21472;&#21306;&#22495;&#20013;&#30340;&#26143;&#31995;&#26469;&#35843;&#25972;VAE&#32593;&#32476;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#26080;&#20559;&#22320;&#24212;&#29992;&#20110;&#36825;&#20004;&#20010;&#35843;&#26597;&#20013;&#30340;&#26143;&#31995;&#22270;&#20687;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;DA&#26399;&#38388;&#65292;&#22122;&#22768;&#24471;&#21040;&#20102;&#26377;&#25928;&#25233;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for the dimensionality reduction of galaxy images by leveraging a combination of variational auto-encoders (VAE) and domain adaptation (DA). We demonstrate the effectiveness of this approach using a sample of low redshift galaxies with detailed morphological type labels from the Galaxy-Zoo DECaLS project. We show that 40-dimensional latent variables can effectively reproduce most morphological features in galaxy images. To further validate the effectiveness of our approach, we utilised a classical random forest (RF) classifier on the 40-dimensional latent variables to make detailed morphology feature classifications. This approach performs similarly to a direct neural network application on galaxy images. We further enhance our model by tuning the VAE network via DA using galaxies in the overlapping footprint of DECaLS and BASS+MzLS, enabling the unbiased application of our model to galaxy images in both surveys. We observed that noise suppression during DA 
&lt;/p&gt;</description></item><item><title>KD-BIRL&#26159;&#19968;&#31181;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22797;&#26434;&#21644;&#26080;&#38480;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2303.06827</link><description>&lt;p&gt;
&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Kernel Density Bayesian Inverse Reinforcement Learning. (arXiv:2303.06827v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06827
&lt;/p&gt;
&lt;p&gt;
KD-BIRL&#26159;&#19968;&#31181;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#26469;&#23398;&#20064;&#20195;&#29702;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22797;&#26434;&#21644;&#26080;&#38480;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#34892;&#20026;&#26469;&#25512;&#26029;&#20854;&#22870;&#21169;&#20989;&#25968;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20294;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#28857;&#20272;&#35745;&#21487;&#33021;&#20250;&#35823;&#23548;&#65292;&#22240;&#20026;&#21487;&#33021;&#26377;&#22810;&#20010;&#20989;&#25968;&#33021;&#22815;&#24456;&#22909;&#22320;&#25551;&#36848;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#37319;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#27169;&#25311;&#20505;&#36873;&#22870;&#21169;&#20989;&#25968;&#30340;&#20998;&#24067;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#28857;&#20272;&#35745;&#30340;&#32570;&#28857;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;Q&#20540;&#20989;&#25968;&#20195;&#26367;&#20284;&#28982;&#20989;&#25968;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#21518;&#39564;&#35745;&#31639;&#37327;&#22823;&#65292;&#29702;&#35770;&#20445;&#35777;&#23569;&#65292;&#24182;&#19988;Q&#20540;&#20989;&#25968;&#36890;&#24120;&#23545;&#20284;&#28982;&#20989;&#25968;&#30340;&#36924;&#36817;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#23494;&#24230;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;KD-BIRL&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26465;&#20214;&#26680;&#23494;&#24230;&#20272;&#35745;&#30452;&#25509;&#36924;&#36817;&#20284;&#28982;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#22312;&#32463;&#36807;&#25913;&#36827;&#30340;&#22870;&#21169;&#20989;&#25968;&#21442;&#25968;&#21270;&#19979;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22797;&#26434;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse reinforcement learning~(IRL) is a powerful framework to infer an agent's reward function by observing its behavior, but IRL algorithms that learn point estimates of the reward function can be misleading because there may be several functions that describe an agent's behavior equally well. A Bayesian approach to IRL models a distribution over candidate reward functions, alleviating the shortcomings of learning a point estimate. However, several Bayesian IRL algorithms use a $Q$-value function in place of the likelihood function. The resulting posterior is computationally intensive to calculate, has few theoretical guarantees, and the $Q$-value function is often a poor approximation for the likelihood. We introduce kernel density Bayesian IRL (KD-BIRL), which uses conditional kernel density estimation to directly approximate the likelihood, providing an efficient framework that, with a modified reward function parameterization, is applicable to environments with complex and infin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DP-Fast MH&#31639;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20855;&#26377;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.06171</link><description>&lt;p&gt;
DP-Fast MH: &#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#31169;&#26377;&#12289;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;Metropolis-Hastings&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference. (arXiv:2303.06171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DP-Fast MH&#31639;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20855;&#26377;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new DP-Fast MH algorithm for large-scale Bayesian inference, which is accurate, fast, and privacy-preserving.
&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#25512;&#29702;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#23427;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#35786;&#26029;&#12289;&#33647;&#29289;&#35774;&#35745;&#21644;&#25919;&#31574;&#21046;&#23450;&#12290;&#22312;&#36825;&#20123;&#24120;&#35265;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#21487;&#33021;&#38750;&#24120;&#25935;&#24863;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25552;&#20379;&#20102;&#20855;&#26377;&#24378;&#22823;&#26368;&#22351;&#24773;&#20917;&#38544;&#31169;&#20445;&#35777;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#24182;&#24050;&#21457;&#23637;&#25104;&#20026;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20998;&#26512;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Metropolis-Hastings&#65288;MH&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#26368;&#22522;&#26412;&#30340;MCMC&#26041;&#27861;&#20043;&#19968;&#65292;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31169;&#26377;MCMC&#31639;&#27861;&#20026;&#20102;&#33719;&#24471;&#38544;&#31169;&#32780;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#31934;&#30830;&#19988;&#24555;&#36895;&#30340;DP MH&#31639;&#27861;&#65292;&#22823;&#22810;&#25968;&#36845;&#20195;&#20013;&#20165;&#20351;&#29992;&#19968;&#20010;&#23567;&#25209;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#38544;&#31169;&#12289;&#21487;&#25193;&#23637;&#24615;&#65288;&#21363;&#25209;&#37327;&#22823;&#23567;&#65289;&#21644;&#25928;&#29575;&#65288;&#21363;&#25910;&#25947;&#36895;&#24230;&#65289;&#20043;&#38388;&#30340;&#19977;&#37325;&#26435;&#34913;&#65292;&#20174;&#29702;&#35770;&#19978;&#35828;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference provides a principled framework for learning from complex data and reasoning under uncertainty. It has been widely applied in machine learning tasks such as medical diagnosis, drug design, and policymaking. In these common applications, the data can be highly sensitive. Differential privacy (DP) offers data analysis tools with powerful worst-case privacy guarantees and has been developed as the leading approach in privacy-preserving data analysis. In this paper, we study Metropolis-Hastings (MH), one of the most fundamental MCMC methods, for large-scale Bayesian inference under differential privacy. While most existing private MCMC algorithms sacrifice accuracy and efficiency to obtain privacy, we provide the first exact and fast DP MH algorithm, using only a minibatch of data in most iterations. We further reveal, for the first time, a three-way trade-off among privacy, scalability (i.e. the batch size), and efficiency (i.e. the convergence rate), theoretically char
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;Transformer&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19978;&#22788;&#29702;&#22810;&#27169;&#24577;&#21333;&#32454;&#32990;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#19979;&#28216;&#20219;&#21153;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;scMoFormer&#30340;&#26694;&#26550;</title><link>http://arxiv.org/abs/2303.00233</link><description>&lt;p&gt;
&#21333;&#32454;&#32990;&#22810;&#27169;&#24577;&#39044;&#27979;&#30340;Transformer&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Single-Cell Multimodal Prediction via Transformers. (arXiv:2303.00233v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;Transformer&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19978;&#22788;&#29702;&#22810;&#27169;&#24577;&#21333;&#32454;&#32990;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#19979;&#28216;&#20219;&#21153;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;scMoFormer&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22810;&#27169;&#24577;&#21333;&#32454;&#32990;&#25216;&#26415;&#30340;&#21457;&#23637;&#20351;&#24471;&#20174;&#21333;&#20010;&#32454;&#32990;&#20013;&#33719;&#21462;&#22810;&#20010;&#32452;&#23398;&#25968;&#25454;&#25104;&#20026;&#21487;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#32454;&#32990;&#29366;&#24577;&#21644;&#21160;&#24577;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#21333;&#32454;&#32990;&#25968;&#25454;&#30340;&#28608;&#22686;&#20063;&#24102;&#26469;&#20102;&#24314;&#27169;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#26041;&#27861;&#20391;&#37325;&#20110;&#26500;&#24314;&#38745;&#24577;&#20132;&#20114;&#22270;&#65292;&#24182;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#38745;&#24577;&#22270;&#21487;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#19979;&#28216;&#20219;&#21153;&#20449;&#24687;&#65307;&#32780;&#19988;&#65292;&#24403;&#28145;&#24230;&#22534;&#21472;GNN&#23618;&#26102;&#65292;GNNs&#20063;&#26377;&#19968;&#20123;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;Transformer&#27169;&#22411;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19978;&#22788;&#29702;&#22810;&#27169;&#24577;&#21333;&#32454;&#32990;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#19979;&#28216;&#20219;&#21153;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;scMoFormer&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#25972;&#21512;&#22806;&#37096;&#30340;d
&lt;/p&gt;
&lt;p&gt;
The recent development of multimodal single-cell technology has made the possibility of acquiring multiple omics data from individual cells, thereby enabling a deeper understanding of cellular states and dynamics. Nevertheless, the proliferation of multimodal single-cell data also introduces tremendous challenges in modeling the complex interactions among different modalities. The recently advanced methods focus on constructing static interaction graphs and applying graph neural networks (GNNs) to learn from multimodal data. However, such static graphs can be suboptimal as they do not take advantage of the downstream task information; meanwhile GNNs also have some inherent limitations when deeply stacking GNN layers. To tackle these issues, in this work, we investigate how to leverage transformers for multimodal single-cell data in an end-to-end manner while exploiting downstream task information. In particular, we propose a scMoFormer framework which can readily incorporate external d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#36136;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#19979;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#31867;&#21035;$F$&#30456;&#27604;&#31867;&#21035;$G$&#26356;&#8220;&#31616;&#21333;&#8221;&#26102;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#22312;$\textbf{y}$&#30340;&#20559;&#31227;&#36828;&#23567;&#20110;$\textbf{x}$&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;ERM&#30340;&#34892;&#20026;&#19982;&#27491;&#20132;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13934</link><description>&lt;p&gt;
&#24322;&#36136;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Statistical Learning under Heterogenous Distribution Shift. (arXiv:2302.13934v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24322;&#36136;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#22312;&#19981;&#21516;&#31867;&#21035;&#30340;&#22797;&#26434;&#24615;&#19979;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#31867;&#21035;$F$&#30456;&#27604;&#31867;&#21035;$G$&#26356;&#8220;&#31616;&#21333;&#8221;&#26102;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#20855;&#26377;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#23588;&#20854;&#22312;$\textbf{y}$&#30340;&#20559;&#31227;&#36828;&#23567;&#20110;$\textbf{x}$&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;ERM&#30340;&#34892;&#20026;&#19982;&#27491;&#20132;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#31867;&#20284;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#38543;&#26426;&#21464;&#37327;&#23545;$(\mathbf{x},\mathbf{y})$&#20013;&#39044;&#27979;&#30446;&#26631;$\mathbf{z}$, &#20854;&#20013;&#30495;&#23454;&#30340;&#39044;&#27979;&#22120;&#26159;&#21152;&#27861;&#30340;$\mathbb{E}[\mathbf{z} \mid \mathbf{x},\mathbf{y}] = f_\star(\mathbf{x}) +g_{\star}(\mathbf{y})$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32473;&#23450;&#35757;&#32451;&#20998;&#24067;&#19978;&#25311;&#21512;&#30340;&#20989;&#25968;$f+g$, $f \in F$&#21644;$g \in G$&#19978;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#22312;&#34920;&#29616;&#19978;&#30340;&#24046;&#24322;&#65292;&#20294;&#22312;&#27979;&#35797;&#20998;&#24067;&#19978;&#24471;&#21040;&#35780;&#20272;&#26102;&#20250;&#26174;&#31034;&#20986;&#21327;&#21464;&#37327;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#31867;&#21035;$F$&#27604;$G$&#26356;&#8220;&#31616;&#21333;&#8221;&#65288;&#20363;&#22914;&#65292;&#20197;&#24230;&#37327;&#29109;&#20026;&#34913;&#37327;&#26631;&#20934;&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#39044;&#27979;&#22120;&#23545;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#26356;&#24378;&#65292;&#20854;&#20013;$\textbf{y}$&#30340;&#20559;&#31227;&#35201;&#36828;&#23567;&#20110;$\textbf{x}$&#30340;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ERM&#30340;&#34892;&#20026;&#19982;&#27491;&#20132;&#26426;&#22120;&#23398;&#20064;$\textbf{ qualitatively similarly}$&#65306;ERM&#24674;&#22797;&#39044;&#27979;&#22120;&#20013;&#30340;$f$&#25104;&#20998;&#30340;&#36895;&#29575;&#20165;&#23545;&#20110;&#31867;&#21035;$G$&#30340;&#22797;&#26434;&#24615;&#20855;&#26377;&#36739;&#20302;&#38454;&#30340;&#20381;&#36182;&#24615;&#65292;&#35843;&#25972;&#21518;...
&lt;/p&gt;
&lt;p&gt;
This paper studies the prediction of a target $\mathbf{z}$ from a pair of random variables $(\mathbf{x},\mathbf{y})$, where the ground-truth predictor is additive $\mathbb{E}[\mathbf{z} \mid \mathbf{x},\mathbf{y}] = f_\star(\mathbf{x}) +g_{\star}(\mathbf{y})$. We study the performance of empirical risk minimization (ERM) over functions $f+g$, $f \in F$ and $g \in G$, fit on a given training distribution, but evaluated on a test distribution which exhibits covariate shift. We show that, when the class $F$ is "simpler" than $G$ (measured, e.g., in terms of its metric entropy), our predictor is more resilient to $\textbf{heterogenous covariate shifts}$ in which the shift in $\mathbf{x}$ is much greater than that in $\mathbf{y}$. Our analysis proceeds by demonstrating that ERM behaves $\textbf{qualitatively similarly to orthogonal machine learning}$: the rate at which ERM recovers the $f$-component of the predictor has only a lower-order dependence on the complexity of the class $G$, adjus
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#36807;&#31243;&#35889;&#23494;&#24230;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#20449;&#21495;&#20540;&#21644;&#36827;&#34892;&#20449;&#21495;&#25554;&#20540;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#26102;&#38388;-&#39030;&#28857;&#20449;&#21495;&#20272;&#35745;&#38382;&#39064;&#20013;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06887</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;-&#39030;&#28857;&#35889;&#23398;&#20064;&#22270;ARMA&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Graph ARMA Processes from Time-Vertex Spectra. (arXiv:2302.06887v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#36807;&#31243;&#35889;&#23494;&#24230;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#20449;&#21495;&#20540;&#21644;&#36827;&#34892;&#20449;&#21495;&#25554;&#20540;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#26102;&#38388;-&#39030;&#28857;&#20449;&#21495;&#20272;&#35745;&#38382;&#39064;&#20013;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#26102;&#38388;&#21464;&#21270;&#30340;&#22270;&#20449;&#21495;&#24314;&#27169;&#20026;&#31283;&#24577;&#26102;&#38388;-&#39030;&#28857;&#38543;&#26426;&#36807;&#31243;&#65292;&#21487;&#20197;&#36890;&#36807;&#26377;&#25928;&#22320;&#21033;&#29992;&#36807;&#31243;&#22312;&#19981;&#21516;&#22270;&#33410;&#28857;&#21644;&#26102;&#38388;&#30636;&#38388;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#27169;&#24335;&#26469;&#25512;&#26029;&#32570;&#22833;&#30340;&#20449;&#21495;&#20540;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#23398;&#20064;&#36807;&#31243;&#30340;&#19981;&#23436;&#25972;&#23454;&#29616;&#30340;&#32852;&#21512;&#26102;&#38388;-&#39030;&#28857;&#21151;&#29575;&#35889;&#23494;&#24230;&#26469;&#35745;&#31639;&#22270;&#33258;&#22238;&#24402;&#31227;&#21160;&#24179;&#22343;&#65288;&#22270;ARMA&#65289;&#36807;&#31243;&#65292;&#20197;&#29992;&#20110;&#20449;&#21495;&#25554;&#20540;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#39318;&#20808;&#36890;&#36807;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#23454;&#29616;&#31895;&#30053;&#20272;&#35745;&#36807;&#31243;&#30340;&#32852;&#21512;&#35889;&#65292;&#28982;&#21518;&#36890;&#36807;&#20984;&#26494;&#24347;&#23558;&#20854;&#25237;&#24433;&#21040;&#22270;ARMA&#36807;&#31243;&#30340;&#35889;&#27969;&#24418;&#19978;&#26469;&#25913;&#36827;&#36825;&#20010;&#20272;&#35745;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#20272;&#35745;&#26368;&#21021;&#32570;&#22833;&#30340;&#20449;&#21495;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;-&#39030;&#28857;&#20449;&#21495;&#20272;&#35745;&#38382;&#39064;&#20013;&#36798;&#21040;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modeling of time-varying graph signals as stationary time-vertex stochastic processes permits the inference of missing signal values by efficiently employing the correlation patterns of the process across different graph nodes and time instants. In this study, we propose an algorithm for computing graph autoregressive moving average (graph ARMA) processes based on learning the joint time-vertex power spectral density of the process from its incomplete realizations for the task of signal interpolation. Our solution relies on first roughly estimating the joint spectrum of the process from partially observed realizations and then refining this estimate by projecting it onto the spectrum manifold of the graph ARMA process through convex relaxations. The initially missing signal values are then estimated based on the learnt model. Experimental results show that the proposed approach achieves high accuracy in time-vertex signal estimation problems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#20301;&#32622;&#19982;&#24615;&#33021;&#30340;&#20851;&#32852;&#65292;&#21457;&#29616;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#21306;&#22495;&#65292;&#19988;&#36825;&#20123;&#21306;&#22495;&#20013;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32469;&#36807;&#36825;&#20123;&#21306;&#22495;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#26032;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.04863</link><description>&lt;p&gt;
&#30693;&#35782;&#26159;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#37325;&#31354;&#38388;&#30340;&#19968;&#20010;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Knowledge is a Region in Weight Space for Fine-tuned Language Models. (arXiv:2302.04863v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04863
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#20301;&#32622;&#19982;&#24615;&#33021;&#30340;&#20851;&#32852;&#65292;&#21457;&#29616;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#21306;&#22495;&#65292;&#19988;&#36825;&#20123;&#21306;&#22495;&#20013;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32469;&#36807;&#36825;&#20123;&#21306;&#22495;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#26032;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#19968;&#30452;&#19987;&#27880;&#20110;&#29702;&#35299;&#21333;&#20010;&#27169;&#22411;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#25110;&#27979;&#35797;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#20102;&#35299;&#29978;&#23569;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#27169;&#22411;&#30340;&#26435;&#37325;&#31354;&#38388;&#21644;&#28508;&#22312;&#30340;&#25439;&#22833;&#22320;&#24418;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#39640;&#24615;&#33021;&#32780;&#36827;&#34892;&#24494;&#35843;&#20248;&#21270;&#30340;&#27169;&#22411;&#23384;&#22312;&#20110;&#26435;&#37325;&#31354;&#38388;&#20013;&#23450;&#20041;&#26126;&#30830;&#30340;&#21306;&#22495;&#20013;&#65292;&#21453;&#20043;&#20134;&#28982;&#8212;&#8212;&#20219;&#20309;&#22312;&#36825;&#20123;&#21306;&#22495;&#20013;&#30340;&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#24418;&#25104;&#19968;&#20010;&#32039;&#23494;&#30340;&#32858;&#31867;&#65292;&#32780;&#22312;&#30456;&#21516;&#22522;&#30784;&#20219;&#21153;&#19979;&#20174;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#21017;&#24418;&#25104;&#19968;&#20010;&#36739;&#26494;&#25955;&#30340;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;&#32469;&#36807;&#27169;&#22411;&#20043;&#38388;&#30340;&#21306;&#22495;&#20250;&#29983;&#25104;&#24615;&#33021;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#26032;&#27169;&#22411;&#65292;&#29978;&#33267;&#22312;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on neural networks has focused on understanding a single model trained on a single dataset. However, relatively little is known about the relationships between different models, particularly those trained or tested on different datasets. We address this by studying how the weight space and the underlying loss landscape of different models are interconnected.  Specifically, we demonstrate that finetuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa -- that any model that resides anywhere in those regions also exhibits high performance. Notably, we show that language models that have been finetuned on the same dataset form a tight cluster in the weight space, while models finetuned on different datasets from the same underlying task form a looser cluster. Moreover, traversing around the region between the models leads to new models that perform comparably or even better than models obtained via finetuning, even on
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24212;&#29992;&#65288;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65289;&#20197;&#21450;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.04062</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning for Synthetic Data Generation: A Review. (arXiv:2302.04062v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04062
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#25506;&#35752;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24212;&#29992;&#65288;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65289;&#20197;&#21450;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#23384;&#22312;&#22810;&#31181;&#38382;&#39064;&#65292;&#22914;&#25968;&#25454;&#36136;&#37327;&#20302;&#65292;&#26377;&#38480;&#30340;&#25968;&#25454;&#28857;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27424;&#25311;&#21512;&#65292;&#30001;&#20110;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#30417;&#31649;&#38382;&#39064;&#38590;&#20197;&#35775;&#38382;&#25968;&#25454;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20197;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#26080;&#27861;&#20570;&#21040;&#30340;&#26041;&#24335;&#36827;&#34892;&#20849;&#20139;&#21644;&#20351;&#29992;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#29616;&#26377;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#35752;&#35770;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#24037;&#20316;&#65306;&#65288;i&#65289;&#24212;&#29992;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#21830;&#19994;&#65307;&#65288;ii&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65307;&#65288;iii&#65289;&#38544;&#31169;&#21644;&#20844;&#24179;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data plays a crucial role in machine learning. However, in real-world applications, there are several problems with data, e.g., data are of low quality; a limited number of data points lead to under-fitting of the machine learning model; it is hard to access the data due to privacy, safety and regulatory concerns. Synthetic data generation offers a promising new avenue, as it can be shared and used in ways that real-world data cannot. This paper systematically reviews the existing works that leverage machine learning models for synthetic data generation. Specifically, we discuss the synthetic data generation works from several perspectives: (i) applications, including computer vision, speech, natural language, healthcare, and business; (ii) machine learning methods, particularly neural network architectures and deep generative models; (iii) privacy and fairness issue. In addition, we identify the challenges and opportunities in this emerging field and suggest future research directions
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#22810;&#24052;&#33018;&#31070;&#32463;&#20803;&#30340;&#20998;&#21106;&#21644;&#35745;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#25805;&#20316;&#30340;&#20027;&#35266;&#24615;&#21644;&#26102;&#38388;&#28040;&#32791;&#65292;&#25552;&#20379;&#21487;&#38752;&#21644;&#26080;&#20559;&#35265;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2301.08141</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#24085;&#37329;&#26862;&#30149;&#20013;&#22810;&#24052;&#33018;&#31070;&#32463;&#20803;&#30340;&#20998;&#21106;&#21644;&#35745;&#37327;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning for Segmentation and Quantification of Dopamine Neurons in Parkinson's Disease. (arXiv:2301.08141v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08141
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#22810;&#24052;&#33018;&#31070;&#32463;&#20803;&#30340;&#20998;&#21106;&#21644;&#35745;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#20154;&#24037;&#25805;&#20316;&#30340;&#20027;&#35266;&#24615;&#21644;&#26102;&#38388;&#28040;&#32791;&#65292;&#25552;&#20379;&#21487;&#38752;&#21644;&#26080;&#20559;&#35265;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#26159;&#20154;&#31867;&#20013;&#31532;&#20108;&#24120;&#35265;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20854;&#29305;&#28857;&#26159;&#22823;&#33041;&#40657;&#36136;&#20013;&#22810;&#24052;&#33018;&#31070;&#32463;&#20803;&#36880;&#28176;&#20007;&#22833;&#12290;&#22312;&#35780;&#20272;&#24085;&#37329;&#26862;&#30149;&#21160;&#29289;&#27169;&#22411;&#30340;&#33647;&#29289;&#30103;&#25928;&#26102;&#65292;&#35745;&#25968;&#22823;&#33041;&#40657;&#36136;&#20013;&#30340;&#22810;&#24052;&#33018;&#31070;&#32463;&#20803;&#26159;&#26368;&#37325;&#35201;&#30340;&#25351;&#26631;&#20043;&#19968;&#12290;&#30446;&#21069;&#65292;&#36890;&#36807;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#26512;&#20154;&#24037;&#36827;&#34892;&#22810;&#24052;&#33018;&#31070;&#32463;&#20803;&#30340;&#20998;&#26512;&#21644;&#35745;&#37327;&#65292;&#36825;&#31181;&#26041;&#27861;&#32791;&#26102;&#36153;&#21147;&#19988;&#39640;&#24230;&#20027;&#35266;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#21487;&#38752;&#19988;&#26080;&#20559;&#35265;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#23545;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#22810;&#24052;&#33018;&#31070;&#32463;&#20803;&#36827;&#34892;&#35745;&#37327;&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#20013;&#26222;&#36941;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#20915;&#20110;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#36825;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#24212;&#29992;&#20013;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#25968;&#25454;&#26631;&#27880;&#30340;&#22256;&#38590;&#24182;&#23454;&#29616;&#23545;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#22810;&#24052;&#33018;&#31070;&#32463;&#20803;&#30340;&#20934;&#30830;&#20998;&#21106;&#21644;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's Disease (PD) is the second most common neurodegenerative disease in humans. PD is characterized by the gradual loss of dopaminergic neurons in the Substantia Nigra (SN). Counting the number of dopaminergic neurons in the SN is one of the most important indexes in evaluating drug efficacy in PD animal models. Currently, analyzing and quantifying dopaminergic neurons is conducted manually by experts through analysis of digital pathology images which is laborious, time-consuming, and highly subjective. As such, a reliable and unbiased automated system is demanded for the quantification of dopaminergic neurons in digital pathology images. Recent years have seen a surge in adopting deep learning solutions in medical image processing. However, developing high-performing deep learning models hinges on the availability of large-scale, high-quality annotated data, which can be expensive to acquire, especially in applications like digital pathology image analysis. To this end, we pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26368;&#20248;&#26102;&#38388;&#31283;&#23450;&#24615;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#37319;&#26679;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#19982;Actor-Critic&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#21457;&#23637;&#20986;&#20102;&#33258;&#36866;&#24212;&#26446;&#38597;&#26222;&#35834;&#22827;Actor-Critic&#65288;ALAC&#65289;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#21313;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#24341;&#23548;&#31995;&#32479;&#29983;&#25104;&#31283;&#23450;&#27169;&#24335;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2301.00521</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#26368;&#20248;&#26102;&#38388;&#31283;&#23450;&#24615;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Policy Optimization Method Towards Optimal-time Stability. (arXiv:2301.00521v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26368;&#20248;&#26102;&#38388;&#31283;&#23450;&#24615;&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#37319;&#26679;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#19982;Actor-Critic&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#21457;&#23637;&#20986;&#20102;&#33258;&#36866;&#24212;&#26446;&#38597;&#26222;&#35834;&#22827;Actor-Critic&#65288;ALAC&#65289;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#21313;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#24341;&#23548;&#31995;&#32479;&#29983;&#25104;&#31283;&#23450;&#27169;&#24335;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#22522;&#20110;&#37319;&#26679;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#26631;&#20934;&#24120;&#29992;&#20110;&#25351;&#23548;&#31574;&#30053;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26631;&#20934;&#20165;&#20445;&#35777;&#31995;&#32479;&#29366;&#24577;&#26080;&#38480;&#26102;&#38388;&#25910;&#25947;&#21040;&#19968;&#20010;&#24179;&#34913;&#28857;&#65292;&#20174;&#32780;&#23548;&#33268;&#31574;&#30053;&#30340;&#27425;&#20248;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22522;&#20110;&#37319;&#26679;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#31283;&#23450;&#24615;&#32435;&#20837;&#31574;&#30053;&#20248;&#21270;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#31995;&#32479;&#29366;&#24577;&#33021;&#22815;&#22312;&#26368;&#20248;&#26102;&#38388;&#20869;&#36798;&#21040;&#24179;&#34913;&#28857;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#65292;&#21363;"&#26368;&#20248;&#26102;&#38388;&#31283;&#23450;&#24615;"&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#20248;&#21270;&#26041;&#27861;&#25972;&#21512;&#21040;Actor-Critic&#26694;&#26550;&#20013;&#65292;&#20174;&#32780;&#24320;&#21457;&#20986;&#33258;&#36866;&#24212;&#26446;&#38597;&#26222;&#35834;&#22827;Actor-Critic&#65288;ALAC&#65289;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#21313;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#22312;&#24341;&#23548;&#31995;&#32479;&#29983;&#25104;&#31283;&#23450;&#27169;&#24335;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In current model-free reinforcement learning (RL) algorithms, stability criteria based on sampling methods are commonly utilized to guide policy optimization. However, these criteria only guarantee the infinite-time convergence of the system's state to an equilibrium point, which leads to sub-optimality of the policy. In this paper, we propose a policy optimization technique incorporating sampling-based Lyapunov stability. Our approach enables the system's state to reach an equilibrium point within an optimal time and maintain stability thereafter, referred to as "optimal-time stability". To achieve this, we integrate the optimization method into the Actor-Critic framework, resulting in the development of the Adaptive Lyapunov-based Actor-Critic (ALAC) algorithm. Through evaluations conducted on ten robotic tasks, our approach outperforms previous studies significantly, effectively guiding the system to generate stable patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20174;&#24341;&#23548;&#24335;&#28216;&#25103;&#23398;&#20064;&#65288;LfGP&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#20010;&#36741;&#21161;&#20219;&#21153;&#21644;&#20027;&#20219;&#21153;&#30340;&#19987;&#23478;&#28436;&#31034;&#65292;&#25552;&#21319;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;AIL&#22312;&#23398;&#20064;&#25805;&#20316;&#20219;&#21153;&#26102;&#21487;&#33021;&#38519;&#20837;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.00051</link><description>&lt;p&gt;
&#20174;&#24341;&#23548;&#24335;&#28216;&#25103;&#23398;&#20064;&#20013;&#25552;&#21319;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning from Guided Play: Improving Exploration for Adversarial Imitation Learning with Simple Auxiliary Tasks. (arXiv:2301.00051v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20174;&#24341;&#23548;&#24335;&#28216;&#25103;&#23398;&#20064;&#65288;LfGP&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#22810;&#20010;&#36741;&#21161;&#20219;&#21153;&#21644;&#20027;&#20219;&#21153;&#30340;&#19987;&#23478;&#28436;&#31034;&#65292;&#25552;&#21319;&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#20256;&#32479;AIL&#22312;&#23398;&#20064;&#25805;&#20316;&#20219;&#21153;&#26102;&#21487;&#33021;&#38519;&#20837;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#27169;&#20223;&#23398;&#20064;&#65288;AIL&#65289;&#24050;&#25104;&#20026;&#20943;&#23569;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#27969;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#38454;&#27573;&#65292;AIL&#38656;&#35201;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#12289;&#22825;&#30495;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#22914;&#26524;&#20351;&#29992;AIL&#23398;&#20064;&#30340;&#31574;&#30053;&#19982;&#19987;&#23478;&#20998;&#24067;&#36275;&#22815;&#21305;&#37197;&#20294;&#27809;&#26377;&#23436;&#20840;&#23398;&#20250;&#25152;&#38656;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#20250;&#34920;&#29616;&#20026;&#19968;&#20010;&#27425;&#20248;&#30340;&#23616;&#37096;&#26368;&#22823;&#20540;&#12290;&#36825;&#23545;&#20110;&#25805;&#20316;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#23588;&#20026;&#28798;&#38590;&#65292;&#22240;&#20026;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#20043;&#38388;&#30340;&#24046;&#21035;&#36890;&#24120;&#26159;&#24494;&#22937;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#24341;&#23548;&#28216;&#25103;&#20013;&#23398;&#20064;&#65288;LfGP&#65289;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#20010;&#25506;&#32034;&#24615;&#36741;&#21161;&#20219;&#21153;&#30340;&#19987;&#23478;&#28436;&#31034;&#65292;&#38500;&#20102;&#19968;&#20010;&#20027;&#20219;&#21153;&#12290;&#36825;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#28155;&#21152;&#24378;&#21046;&#20195;&#29702;&#20154;&#25506;&#32034;&#26631;&#20934;AIL&#21487;&#33021;&#23398;&#20250;&#24573;&#35270;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#29305;&#23450;&#30340;&#20844;&#24335;&#20801;&#35768;&#36741;&#21161;&#20219;&#21153;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial imitation learning (AIL) has become a popular alternative to supervised imitation learning that reduces the distribution shift suffered by the latter. However, AIL requires effective exploration during an online reinforcement learning phase. In this work, we show that the standard, naive approach to exploration can manifest as a suboptimal local maximum if a policy learned with AIL sufficiently matches the expert distribution without fully learning the desired task. This can be particularly catastrophic for manipulation tasks, where the difference between an expert and a non-expert state-action pair is often subtle. We present Learning from Guided Play (LfGP), a framework in which we leverage expert demonstrations of multiple exploratory, auxiliary tasks in addition to a main task. The addition of these auxiliary tasks forces the agent to explore states and actions that standard AIL may learn to ignore. Additionally, this particular formulation allows for the reusability of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.09849</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#24182;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#23454;&#29616;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#26500;&#24314;&#19979;&#28216;NLP&#27169;&#22411;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#24050;&#32463;&#21487;&#29992;&#65292;&#20294;&#20854;&#35757;&#32451;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#30693;&#35782;&#20135;&#26435;&#38382;&#39064;&#12290;&#36825;&#23601;&#36896;&#25104;&#20102;&#36328;&#27169;&#22411;&#34701;&#21512;&#30693;&#35782;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24314;&#31435;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#21512;&#24182;&#30340;&#38382;&#39064;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#21512;&#24182;&#27169;&#22411;&#65292;&#30001;&#26435;&#37325;&#24341;&#23548;&#65292;&#20197;&#26368;&#23567;&#21270;&#21512;&#24182;&#27169;&#22411;&#21644;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#24322;&#12290;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22914;Fisher&#21152;&#26435;&#24179;&#22343;&#25110;&#27169;&#22411;&#38598;&#25104;&#31561;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#22810;&#35821;&#35328;&#24494;&#35843;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#30456;&#26426;&#30340;&#35835;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29289;&#20307;&#37325;&#26032;&#23450;&#21521;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#23454;&#26102;&#12289;&#21160;&#24577;&#22320;&#37325;&#26032;&#23450;&#21521;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#20013;&#20301;&#25968;&#37325;&#26032;&#23450;&#21521;&#26102;&#38388;&#25509;&#36817;&#20110;&#19971;&#31186;&#12290;&#35813;&#25511;&#21046;&#22120;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#23545;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#26032;&#29289;&#20307;&#24418;&#29366;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2211.11744</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#24863;&#30693;&#23454;&#29616;&#25163;&#25345;&#28789;&#24039;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Visual Dexterity: In-hand Dexterous Manipulation from Depth. (arXiv:2211.11744v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11744
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#30456;&#26426;&#30340;&#35835;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29289;&#20307;&#37325;&#26032;&#23450;&#21521;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#23454;&#26102;&#12289;&#21160;&#24577;&#22320;&#37325;&#26032;&#23450;&#21521;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#20013;&#20301;&#25968;&#37325;&#26032;&#23450;&#21521;&#26102;&#38388;&#25509;&#36817;&#20110;&#19971;&#31186;&#12290;&#35813;&#25511;&#21046;&#22120;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#23545;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#26032;&#29289;&#20307;&#24418;&#29366;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#25345;&#29289;&#20307;&#30340;&#37325;&#26032;&#23450;&#21521;&#23545;&#20110;&#25191;&#34892;&#35768;&#22810;&#28789;&#24039;&#25805;&#20316;&#20219;&#21153;&#38750;&#24120;&#24517;&#35201;&#65292;&#20363;&#22914;&#22312;&#24403;&#21069;&#26426;&#22120;&#20154;&#26080;&#27861;&#35302;&#21450;&#30340;&#32467;&#26500;&#19981;&#22826;&#23436;&#21892;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#24037;&#20855;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#37325;&#26032;&#23450;&#21521;&#31995;&#32479;&#65292;&#20551;&#35774;&#20197;&#19979;&#24773;&#20917;&#20043;&#19968;&#25110;&#22810;&#31181;&#24773;&#20917;&#21516;&#26102;&#23384;&#22312;&#65306;&#20165;&#37325;&#26032;&#23450;&#21521;&#20855;&#26377;&#31616;&#21333;&#24418;&#29366;&#30340;&#29305;&#23450;&#29289;&#20307;&#12289;&#37325;&#26032;&#23450;&#21521;&#33539;&#22260;&#26377;&#38480;&#12289;&#24930;&#36895;&#25110;&#20934;&#38745;&#24577;&#25805;&#20316;&#12289;&#20165;&#27169;&#25311;&#32467;&#26524;&#12289;&#38656;&#35201;&#19987;&#29992;&#19988;&#26114;&#36149;&#30340;&#20256;&#24863;&#22120;&#22871;&#20214;&#20197;&#21450;&#20854;&#20182;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#37096;&#32626;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20570;&#36825;&#20123;&#20551;&#35774;&#30340;&#36890;&#29992;&#29289;&#20307;&#37325;&#26032;&#23450;&#21521;&#25511;&#21046;&#22120;&#12290;&#23427;&#20351;&#29992;&#26469;&#33258;&#21333;&#20010;&#26222;&#36890;&#28145;&#24230;&#25668;&#20687;&#26426;&#30340;&#35835;&#25968;&#65292;&#20197;&#23454;&#26102;&#26041;&#24335;&#36890;&#36807;&#20219;&#24847;&#26059;&#36716;&#21160;&#24577;&#37325;&#26032;&#23450;&#21521;&#22797;&#26434;&#19988;&#26032;&#39062;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#20013;&#20301;&#25968;&#37325;&#26032;&#23450;&#21521;&#26102;&#38388;&#25509;&#36817;&#20110;&#19971;&#31186;&#12290;&#35813;&#25511;&#21046;&#22120;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#26032;&#29289;&#20307;&#24418;&#29366;&#19978;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324; ...
&lt;/p&gt;
&lt;p&gt;
In-hand object reorientation is necessary for performing many dexterous manipulation tasks, such as tool use in less structured environments that remain beyond the reach of current robots. Prior works built reorientation systems assuming one or many of the following: reorienting only specific objects with simple shapes, limited range of reorientation, slow or quasistatic manipulation, simulation-only results, the need for specialized and costly sensor suites, and other constraints which make the system infeasible for real-world deployment. We present a general object reorientation controller that does not make these assumptions. It uses readings from a single commodity depth camera to dynamically reorient complex and new object shapes by any rotation in real-time, with the median reorientation time being close to seven seconds. The controller is trained using reinforcement learning in simulation and evaluated in the real world on new object shapes not used for training, including the m
&lt;/p&gt;</description></item><item><title>ViNL&#26159;&#36890;&#36807;&#35270;&#35273;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#22312;&#26410;&#30693;&#23460;&#20869;&#29615;&#22659;&#20013;&#23454;&#29616;&#26426;&#22120;&#20154;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#36816;&#21160;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#26080;&#27169;&#22411;&#30340;&#35270;&#35273;&#23548;&#33322;&#31574;&#30053;&#21644;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#65292;&#24182;&#33021;&#22815;&#36991;&#20813;&#36393;&#21040;&#23567;&#38556;&#30861;&#29289;&#12290;ViNL&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#19988;&#31283;&#23450;&#30340;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#36816;&#21160;&#65292;&#26080;&#38656;&#29615;&#22659;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2210.14791</link><description>&lt;p&gt;
ViNL&#65306;&#36890;&#36807;&#35270;&#35273;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#34892;&#36208;&#36991;&#20813;&#38556;&#30861;&#29289;
&lt;/p&gt;
&lt;p&gt;
ViNL: Visual Navigation and Locomotion Over Obstacles. (arXiv:2210.14791v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14791
&lt;/p&gt;
&lt;p&gt;
ViNL&#26159;&#36890;&#36807;&#35270;&#35273;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#22312;&#26410;&#30693;&#23460;&#20869;&#29615;&#22659;&#20013;&#23454;&#29616;&#26426;&#22120;&#20154;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#36816;&#21160;&#30340;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#26080;&#27169;&#22411;&#30340;&#35270;&#35273;&#23548;&#33322;&#31574;&#30053;&#21644;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#65292;&#24182;&#33021;&#22815;&#36991;&#20813;&#36393;&#21040;&#23567;&#38556;&#30861;&#29289;&#12290;ViNL&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#19988;&#31283;&#23450;&#30340;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#36816;&#21160;&#65292;&#26080;&#38656;&#29615;&#22659;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ViNL&#65288;Visual Navigation and Locomotion&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#22235;&#36275;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#36335;&#24452;&#19978;&#36328;&#36807;&#23567;&#38556;&#30861;&#29289;&#65288;&#20363;&#22914;&#38795;&#23376;&#12289;&#29609;&#20855;&#12289;&#30005;&#32518;&#65289;&#65292;&#31867;&#20284;&#20110;&#20154;&#21644;&#23456;&#29289;&#22312;&#34892;&#36208;&#26102;&#25260;&#36215;&#33050;&#27493;&#36229;&#36807;&#29289;&#20307;&#12290;ViNL&#21253;&#25324;&#65306;&#65288;1&#65289;&#19968;&#20010;&#35270;&#35273;&#23548;&#33322;&#31574;&#30053;&#65292;&#36755;&#20986;&#32447;&#24615;&#21644;&#35282;&#36895;&#24230;&#21629;&#20196;&#65292;&#25351;&#23548;&#26426;&#22120;&#20154;&#22312;&#38476;&#29983;&#30340;&#23460;&#20869;&#29615;&#22659;&#20013;&#36798;&#21040;&#30446;&#26631;&#22352;&#26631;&#65307;&#65288;2&#65289;&#19968;&#20010;&#35270;&#35273;&#36816;&#21160;&#31574;&#30053;&#65292;&#36890;&#36807;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#20851;&#33410;&#65292;&#36991;&#20813;&#36393;&#21040;&#38556;&#30861;&#29289;&#65292;&#24182;&#25353;&#29031;&#25552;&#20379;&#30340;&#36895;&#24230;&#21629;&#20196;&#31227;&#21160;&#12290;&#36825;&#20004;&#20010;&#31574;&#30053;&#37117;&#26159;&#23436;&#20840;&#8220;&#26080;&#27169;&#22411;&#8221;&#30340;&#65292;&#21363;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#20256;&#24863;&#22120;&#21040;&#34892;&#21160;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20004;&#32773;&#22312;&#20004;&#20010;&#23436;&#20840;&#19981;&#21516;&#30340;&#27169;&#25311;&#22120;&#20013;&#29420;&#31435;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#23548;&#33322;&#22120;&#30340;&#36895;&#24230;&#21629;&#20196;&#36755;&#20837;&#21040;&#23450;&#20301;&#22120;&#20013;&#21327;&#21516;&#37096;&#32626;&#65292;&#23436;&#20840;&#8220;&#38646;&#35757;&#32451;&#8221;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#29992;&#20110;&#35270;&#35273;&#23548;&#33322;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#27169;&#22411;&#20808;&#39564;&#21644;/&#25110;&#20154;&#24037;&#29305;&#24449;&#24037;&#31243;&#12290;ViNL&#19982;&#36825;&#20123;&#26041;&#27861;&#19981;&#21516;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#29615;&#22659;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26410;&#30693;&#30340;&#23460;&#20869;&#29615;&#22659;&#20013;&#23454;&#29616;&#39640;&#25928;&#19988;&#31283;&#23450;&#30340;&#23548;&#33322;&#21644;&#36275;&#29699;&#26415;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Visual Navigation and Locomotion over obstacles (ViNL), which enables a quadrupedal robot to navigate unseen apartments while stepping over small obstacles that lie in its path (e.g., shoes, toys, cables), similar to how humans and pets lift their feet over objects as they walk. ViNL consists of: (1) a visual navigation policy that outputs linear and angular velocity commands that guides the robot to a goal coordinate in unfamiliar indoor environments; and (2) a visual locomotion policy that controls the robot's joints to avoid stepping on obstacles while following provided velocity commands. Both the policies are entirely "model-free", i.e. sensors-to-actions neural networks trained end-to-end. The two are trained independently in two entirely different simulators and then seamlessly co-deployed by feeding the velocity commands from the navigator to the locomotor, entirely "zero-shot" (without any co-training). While prior works have developed learning methods for visual na
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#20840;&#26223;&#20998;&#21106;&#12290;&#20182;&#20204;&#23558;&#20840;&#26223;&#20998;&#21106;&#38382;&#39064;&#23450;&#20041;&#20026;&#31163;&#25955;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#20840;&#26223;&#25513;&#30721;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#24314;&#27169;&#35270;&#39057;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36319;&#36394;&#23545;&#35937;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#23478;&#26041;&#27861;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.06366</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#20840;&#26223;&#20998;&#21106;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalist Framework for Panoptic Segmentation of Images and Videos. (arXiv:2210.06366v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#20840;&#26223;&#20998;&#21106;&#12290;&#20182;&#20204;&#23558;&#20840;&#26223;&#20998;&#21106;&#38382;&#39064;&#23450;&#20041;&#20026;&#31163;&#25955;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#20840;&#26223;&#25513;&#30721;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#24314;&#27169;&#35270;&#39057;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36319;&#36394;&#23545;&#35937;&#23454;&#20363;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#23478;&#26041;&#27861;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#26223;&#20998;&#21106;&#20026;&#22270;&#20687;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#37197;&#35821;&#20041;&#21644;&#23454;&#20363;ID&#26631;&#31614;&#12290;&#30001;&#20110;&#23454;&#20363;ID&#30340;&#25490;&#21015;&#20063;&#26159;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#20219;&#21153;&#38656;&#35201;&#23398;&#20064;&#39640;&#32500;&#24230;&#30340;&#19968;&#23545;&#22810;&#26144;&#23556;&#12290;&#22240;&#27492;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#23450;&#21046;&#30340;&#26550;&#26500;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;&#20840;&#26223;&#20998;&#21106;&#38382;&#39064;&#23450;&#20041;&#20026;&#31163;&#25955;&#25968;&#25454;&#29983;&#25104;&#38382;&#39064;&#65292;&#19981;&#20381;&#36182;&#20219;&#21153;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#26469;&#24314;&#27169;&#20840;&#26223;&#25513;&#30721;&#65292;&#20855;&#26377;&#31616;&#21333;&#30340;&#26550;&#26500;&#21644;&#36890;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#36890;&#36807;&#23558;&#36807;&#21435;&#30340;&#39044;&#27979;&#20316;&#20026;&#26465;&#20214;&#20449;&#21495;&#28155;&#21152;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#27969;&#24335;&#35774;&#32622;&#20013;&#24314;&#27169;&#35270;&#39057;&#65292;&#24182;&#33258;&#21160;&#23398;&#20064;&#36319;&#36394;&#23545;&#35937;&#23454;&#20363;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#22312;&#31867;&#20284;&#30340;&#35774;&#32622;&#19979;&#33021;&#22815;&#19982;&#26368;&#20808;&#36827;&#30340;&#19987;&#23478;&#26041;&#27861;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Panoptic segmentation assigns semantic and instance ID labels to every pixel of an image. As permutations of instance IDs are also valid solutions, the task requires learning of high-dimensional one-to-many mapping. As a result, state-of-the-art approaches use customized architectures and task-specific loss functions. We formulate panoptic segmentation as a discrete data generation problem, without relying on inductive bias of the task. A diffusion model is proposed to model panoptic masks, with a simple architecture and generic loss function. By simply adding past predictions as a conditioning signal, our method is capable of modeling video (in a streaming setting) and thereby learns to track object instances automatically. With extensive experiments, we demonstrate that our simple approach can perform competitively to state-of-the-art specialist methods in similar settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#23545;&#25968;&#31934;&#24230;transformer&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#23545;&#25968;&#31934;&#24230;transformer&#37117;&#21487;&#20197;&#31561;&#25928;&#22320;&#34920;&#31034;&#20026;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#65292;&#25193;&#23637;&#20102;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2210.02671</link><description>&lt;p&gt;
&#34920;&#36798;&#23545;&#25968;&#31934;&#24230;&#21464;&#25442;&#22120;&#30340;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
A Logic for Expressing Log-Precision Transformers. (arXiv:2210.02671v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#23545;&#25968;&#31934;&#24230;transformer&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#23545;&#25968;&#31934;&#24230;transformer&#37117;&#21487;&#20197;&#31561;&#25928;&#22320;&#34920;&#31034;&#20026;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#65292;&#25193;&#23637;&#20102;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#25551;&#36848;&#23427;&#20204;&#21487;&#20197;&#22312;&#26576;&#20123;&#36755;&#20837;&#25991;&#26412;&#19978;&#35299;&#20915;&#30340;&#36923;&#36753;&#35268;&#21017;&#31867;&#22411;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#22312;&#38271;&#24230;&#20026;n&#30340;&#35821;&#22659;&#19978;&#35745;&#31639;&#21069;&#21521;&#20256;&#36882;&#30340;&#23545;&#25968;&#31934;&#24230;transformer&#65292;&#35777;&#26126;&#20102;&#20219;&#20309;&#23545;&#25968;&#31934;&#24230;transformer&#37117;&#21487;&#20197;&#31561;&#25928;&#22320;&#34920;&#31034;&#20026;&#19968;&#38454;&#36923;&#36753;&#21477;&#23376;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26631;&#20934;&#30340;&#20840;&#31216;&#21644;&#23384;&#22312;&#37327;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
One way to interpret the reasoning power of transformer-based language models is to describe the types of logical rules they can resolve over some input text. Recently, Chiang et al. (2023) showed that finite-precision transformers can be equivalently expressed in a generalization of first-order logic. However, finite-precision transformers are a weak transformer variant because, as we show, a single head can only attend to a constant number of tokens and, in particular, cannot represent uniform attention. Since attending broadly is a core capability for transformers, we ask whether a minimally more expressive model that can attend universally can also be characterized in logic. To this end, we analyze transformers whose forward pass is computed in $\log n$ precision on contexts of length $n$. We prove that any log-precision transformer can be equivalently expressed as a first-order logic sentence that, in addition to standard universal and existential quantifiers, may also contain maj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SpeedLimit&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#37327;&#21270;&#30340;Transformer&#27169;&#22411;&#20013;&#28155;&#21152;&#19978;&#38480;&#24310;&#36831;&#32422;&#26463;&#65292;&#20248;&#21270;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#65292;&#20026;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;Transformer&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12127</link><description>&lt;p&gt;
SpeedLimit&#65306;&#37327;&#21270;Transformer&#27169;&#22411;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
SpeedLimit: Neural Architecture Search for Quantized Transformer Models. (arXiv:2209.12127v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SpeedLimit&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#37327;&#21270;&#30340;Transformer&#27169;&#22411;&#20013;&#28155;&#21152;&#19978;&#38480;&#24310;&#36831;&#32422;&#26463;&#65292;&#20248;&#21270;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#65292;&#20026;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;Transformer&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#35832;&#22914;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24230;&#36825;&#26679;&#30340;&#24615;&#33021;&#25351;&#26631;&#19978;&#65292;&#20294;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#20005;&#26684;&#32771;&#34385;&#25512;&#29702;&#24310;&#36831;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SpeedLimit&#65292;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#25216;&#26415;&#65292;&#23427;&#22312;&#20445;&#25345;&#19978;&#38480;&#24310;&#36831;&#32422;&#26463;&#30340;&#21069;&#25552;&#19979;&#20248;&#21270;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#32467;&#21512;&#20102;8&#20301;&#25972;&#25968;&#37327;&#21270;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#24615;&#33021;&#21644;&#24310;&#36831;&#20043;&#38388;&#23547;&#27714;&#26368;&#20339;&#24179;&#34913;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#20026;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#29615;&#22659;&#20013;&#37096;&#32626;&#26368;&#20808;&#36827;&#30340;Transformer&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While research in the field of transformer models has primarily focused on enhancing performance metrics such as accuracy and perplexity, practical applications in industry often necessitate a rigorous consideration of inference latency constraints. Addressing this challenge, we introduce SpeedLimit, a novel Neural Architecture Search (NAS) technique that optimizes accuracy whilst adhering to an upper-bound latency constraint. Our method incorporates 8-bit integer quantization in the search process to outperform the current state-of-the-art technique. Our results underline the feasibility and efficacy of seeking an optimal balance between performance and latency, providing new avenues for deploying state-of-the-art transformer models in latency-sensitive environments.
&lt;/p&gt;</description></item><item><title>DataPerf&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#20027;&#23548;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#36890;&#36807;&#31454;&#20105;&#12289;&#21487;&#27604;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#20419;&#36827;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2207.10062</link><description>&lt;p&gt;
DataPerf&#65306;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DataPerf: Benchmarks for Data-Centric AI Development. (arXiv:2207.10062v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10062
&lt;/p&gt;
&lt;p&gt;
DataPerf&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#20027;&#23548;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#36890;&#36807;&#31454;&#20105;&#12289;&#21487;&#27604;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#20419;&#36827;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#19968;&#30452;&#27880;&#37325;&#27169;&#22411;&#32780;&#19981;&#26159;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#33879;&#21517;&#25968;&#25454;&#38598;&#34987;&#29992;&#20110;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#32780;&#24573;&#35270;&#20102;&#24213;&#23618;&#38382;&#39064;&#30340;&#24191;&#24230;&#12289;&#38590;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#24573;&#35270;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;&#23548;&#33268;&#20102;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#19981;&#20934;&#30830;&#24615;&#12289;&#20559;&#35265;&#21644;&#33030;&#24369;&#24615;&#65292;&#24182;&#19988;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#36798;&#21040;&#20102;&#39281;&#21644;&#29366;&#24577;&#65292;&#38459;&#30861;&#20102;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DataPerf&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#31038;&#21306;&#20027;&#23548;&#30340;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#21644;&#25968;&#25454;&#20013;&#24515;&#31639;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31454;&#20105;&#12289;&#21487;&#27604;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#20419;&#36827;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#21019;&#26032;&#12290;&#25105;&#20204;&#20351;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#33021;&#22815;&#36845;&#20195;&#25968;&#25454;&#38598;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#24320;&#25918;&#30340;&#22312;&#32447;&#24179;&#21488;&#65292;&#20197;&#25903;&#25345;&#36825;&#31181;&#36845;&#20195;&#24320;&#21457;&#12290;DataPerf&#30340;&#31532;&#19968;&#20010;&#36845;&#20195;&#21253;&#21547;&#20102;&#20116;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#35270;&#35273;&#12289;&#35821;&#38899;&#12289;&#33719;&#21462;&#31561;&#22810;&#31181;&#25968;&#25454;&#20013;&#24515;&#25216;&#26415;&#12289;&#20219;&#21153;&#21644;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning research has long focused on models rather than datasets, and prominent datasets are used for common ML tasks without regard to the breadth, difficulty, and faithfulness of the underlying problems. Neglecting the fundamental importance of data has given rise to inaccuracy, bias, and fragility in real-world applications, and research is hindered by saturation across existing dataset benchmarks. In response, we present DataPerf, a community-led benchmark suite for evaluating ML datasets and data-centric algorithms. We aim to foster innovation in data-centric AI through competition, comparability, and reproducibility. We enable the ML community to iterate on datasets, instead of just architectures, and we provide an open, online platform with multiple rounds of challenges to support this iterative development. The first iteration of DataPerf contains five benchmarks covering a wide spectrum of data-centric techniques, tasks, and modalities in vision, speech, acquisition, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#22270;&#24418;&#26631;&#20934;&#21644;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;CIP&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#21453;&#20107;&#23454;&#19981;&#21464;&#30340;&#39044;&#27979;&#22120;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20844;&#24179;&#24615;&#12289;&#24378;&#20581;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.09768</link><description>&lt;p&gt;
&#23398;&#20064;&#21453;&#20107;&#23454;&#19981;&#21464;&#30340;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Counterfactually Invariant Predictors. (arXiv:2207.09768v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09768
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#22270;&#24418;&#26631;&#20934;&#21644;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;CIP&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#21453;&#20107;&#23454;&#19981;&#21464;&#30340;&#39044;&#27979;&#22120;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20844;&#24179;&#24615;&#12289;&#24378;&#20581;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#19981;&#21464;&#24615;&#65288;CI&#65289;&#30340;&#27010;&#24565;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20844;&#24179;&#12289;&#24378;&#20581;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#39044;&#27979;&#22120;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#24418;&#26631;&#20934;&#65292;&#23427;&#20197;&#35266;&#27979;&#20998;&#24067;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#20316;&#20026;&#39044;&#27979;&#22120;&#21453;&#20107;&#23454;&#19981;&#21464;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#20026;&#20102;&#23398;&#20064;&#36825;&#26679;&#30340;&#39044;&#27979;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Counterfactually Invariant Prediction&#65288;CIP&#65289;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#22522;&#20110;Hilbert-Schmidt&#26465;&#20214;&#29420;&#31435;&#20934;&#21017;&#65288;HSCIC&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#26465;&#20214;&#20381;&#36182;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#22312;&#21253;&#25324;&#26631;&#37327;&#21644;&#22810;&#21464;&#37327;&#35774;&#32622;&#22312;&#20869;&#30340;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;CIP&#22312;&#24378;&#21046;&#21453;&#20107;&#23454;&#19981;&#21464;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Notions of counterfactual invariance (CI) have proven essential for predictors that are fair, robust, and generalizable in the real world. We propose graphical criteria that yield a sufficient condition for a predictor to be counterfactually invariant in terms of a conditional independence in the observational distribution. In order to learn such predictors, we propose a model-agnostic framework, called Counterfactually Invariant Prediction (CIP), building on the Hilbert-Schmidt Conditional Independence Criterion (HSCIC), a kernel-based conditional dependence measure. Our experimental results demonstrate the effectiveness of CIP in enforcing counterfactual invariance across various simulated and real-world datasets including scalar and multi-variate settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38544;&#24615;&#21442;&#25968;&#30340;&#36882;&#24402;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(HiP-RSSMs)&#65292;&#29992;&#20110;&#24314;&#27169;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#20294;&#21160;&#21147;&#23398;&#19981;&#23436;&#20840;&#30456;&#21516;&#30340;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#19968;&#31995;&#21015;&#30456;&#20851;&#21160;&#21147;&#31995;&#32479;&#21442;&#25968;&#21270;&#20026;&#20302;&#32500;&#28508;&#22312;&#22240;&#23376;&#65292;&#23454;&#29616;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#21644;&#25512;&#26029;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HiP-RSSMs&#22312;&#22810;&#20010;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#27604;RSSMs&#21644;&#20854;&#20182;&#22810;&#20219;&#21153;&#27169;&#22411;&#26356;&#20855;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2206.14697</link><description>&lt;p&gt;
&#38544;&#24615;&#21442;&#25968;&#30340;&#36882;&#24402;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#29992;&#20110;&#21464;&#21270;&#21160;&#24577;&#22330;&#26223;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hidden Parameter Recurrent State Space Models For Changing Dynamics Scenarios. (arXiv:2206.14697v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38544;&#24615;&#21442;&#25968;&#30340;&#36882;&#24402;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(HiP-RSSMs)&#65292;&#29992;&#20110;&#24314;&#27169;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#24120;&#35265;&#20294;&#21160;&#21147;&#23398;&#19981;&#23436;&#20840;&#30456;&#21516;&#30340;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23558;&#19968;&#31995;&#21015;&#30456;&#20851;&#21160;&#21147;&#31995;&#32479;&#21442;&#25968;&#21270;&#20026;&#20302;&#32500;&#28508;&#22312;&#22240;&#23376;&#65292;&#23454;&#29616;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#21644;&#25512;&#26029;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HiP-RSSMs&#22312;&#22810;&#20010;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#27604;RSSMs&#21644;&#20854;&#20182;&#22810;&#20219;&#21153;&#27169;&#22411;&#26356;&#20855;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(RSSMs)&#26159;&#19968;&#31181;&#39640;&#24230;&#34920;&#36798;&#24615;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#31995;&#32479;&#35782;&#21035;&#20013;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#21160;&#21147;&#23398;&#26159;&#22266;&#23450;&#19988;&#19981;&#21464;&#30340;&#65292;&#32780;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#24456;&#23569;&#35265;&#12290;&#35768;&#22810;&#25511;&#21046;&#24212;&#29992;&#36890;&#24120;&#23637;&#31034;&#20986;&#20855;&#26377;&#31867;&#20284;&#20294;&#38750;&#23436;&#20840;&#30456;&#21516;&#30340;&#21160;&#21147;&#23398;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#23558;&#20854;&#24314;&#27169;&#20026;&#28508;&#22312;&#21464;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#38544;&#24615;&#21442;&#25968;&#30340;&#36882;&#24402;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(HiP-RSSMs)&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#19968;&#31995;&#21015;&#30456;&#20851;&#21160;&#21147;&#31995;&#32479;&#21442;&#25968;&#21270;&#20026;&#20302;&#32500;&#28508;&#22312;&#22240;&#23376;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#23398;&#20064;&#21644;&#25512;&#26029;&#39640;&#26031;&#22270;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#31867;&#20284;&#21464;&#20998;&#25512;&#26029;&#30340;&#36817;&#20284;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HiP-RSSMs&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#26080;&#35770;&#26159;&#22312;&#30495;&#23454;&#31995;&#32479;&#36824;&#26159;&#27169;&#25311;&#20013;&#65292;&#37117;&#20248;&#20110;RSSMs&#21644;&#31454;&#20105;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent State-space models (RSSMs) are highly expressive models for learning patterns in time series data and system identification. However, these models assume that the dynamics are fixed and unchanging, which is rarely the case in real-world scenarios. Many control applications often exhibit tasks with similar but not identical dynamics which can be modeled as a latent variable. We introduce the Hidden Parameter Recurrent State Space Models (HiP-RSSMs), a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors. We present a simple and effective way of learning and performing inference over this Gaussian graphical model that avoids approximations like variational inference. We show that HiP-RSSMs outperforms RSSMs and competing multi-task models on several challenging robotic benchmarks both on real-world systems and simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24335;&#30446;&#26631;&#23884;&#20837;&#36827;&#34892;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#26631;&#27880;&#30340;3D&#29615;&#22659;&#20013;&#35757;&#32451;&#35821;&#20041;&#30446;&#26631;&#23548;&#33322;&#20195;&#29702;&#65292;&#23558;&#30446;&#26631;&#22270;&#29255;&#32534;&#30721;&#25104;&#22810;&#27169;&#24335;&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#25214;&#21040;&#29289;&#20307;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.12403</link><description>&lt;p&gt;
ZSON: &#20351;&#29992;&#22810;&#27169;&#24335;&#30446;&#26631;&#23884;&#20837;&#36827;&#34892;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings. (arXiv:2206.12403v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24335;&#30446;&#26631;&#23884;&#20837;&#36827;&#34892;&#38646;&#26679;&#26412;&#30446;&#26631;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26410;&#26631;&#27880;&#30340;3D&#29615;&#22659;&#20013;&#35757;&#32451;&#35821;&#20041;&#30446;&#26631;&#23548;&#33322;&#20195;&#29702;&#65292;&#23558;&#30446;&#26631;&#22270;&#29255;&#32534;&#30721;&#25104;&#22810;&#27169;&#24335;&#30340;&#35821;&#20041;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#25214;&#21040;&#29289;&#20307;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#24320;&#25918;&#19990;&#30028;&#30340;&#29289;&#20307;&#30446;&#26631;&#23548;&#33322;(ObjectNav) -- &#21363;&#35753;&#34394;&#25311;&#26426;&#22120;&#20154;(&#26234;&#33021;&#20307;)&#22312;&#26410;&#25506;&#32034;&#30340;&#29615;&#22659;&#20013;&#25214;&#21040;&#20219;&#20309;&#19968;&#20010;&#29289;&#20307;&#23454;&#20363;(&#20363;&#22914;&#65292;&#8220;&#25214;&#21040;&#19968;&#20010;&#27700;&#27133;&#8221;&#30340;&#20219;&#21153;)&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#26159;&#38646;&#26679;&#26412;&#30340; -- &#21363;&#19981;&#38656;&#35201;ObjectNav&#30340;&#22870;&#21169;&#25110;&#20219;&#20309;&#24418;&#24335;&#30340;&#31034;&#33539;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#30446;&#26631;&#23548;&#33322;(ImageNav)&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#21363;&#20351;&#20195;&#29702;&#25214;&#21040;&#19968;&#20010;&#22270;&#29255;(&#21363;&#30446;&#26631;&#22270;&#29255;)&#34987;&#25293;&#25668;&#30340;&#20301;&#32622;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23558;&#30446;&#26631;&#22270;&#29255;&#32534;&#30721;&#21040;&#19968;&#20010;&#22810;&#27169;&#24335;&#30340;&#35821;&#20041;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20197;&#23454;&#29616;&#22312;&#26410;&#26631;&#27880;&#30340;3D&#29615;&#22659;(&#20363;&#22914;HM3D)&#20013;&#35757;&#32451;&#35821;&#20041;&#30446;&#26631;&#23548;&#33322;(SemanticNav)&#20195;&#29702;&#30340;&#35268;&#27169;&#25193;&#23637;&#12290;&#35757;&#32451;&#21518;&#65292;&#21487;&#20197;&#25351;&#31034;SemanticNav&#20195;&#29702;&#26681;&#25454;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;(&#20363;&#22914;&#65292;&#8220;&#27700;&#27133;&#8221;&#65292;&#8220;&#28020;&#23460;&#27700;&#27133;&#8221;&#31561;)&#26469;&#26597;&#25214;&#29289;&#20307;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#30446;&#26631;&#26144;&#23556;&#21040;&#30456;&#21516;&#30340;&#22810;&#27169;&#24335;&#35821;&#20041;&#23884;&#20837;&#31354;&#38388;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#24320;&#25918;&#19990;&#30028;&#30340;ObjectNav&#12290;&#25105;&#20204;&#23545;&#20195;&#29702;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a scalable approach for learning open-world object-goal navigation (ObjectNav) -- the task of asking a virtual robot (agent) to find any instance of an object in an unexplored environment (e.g., "find a sink"). Our approach is entirely zero-shot -- i.e., it does not require ObjectNav rewards or demonstrations of any kind. Instead, we train on the image-goal navigation (ImageNav) task, in which agents find the location where a picture (i.e., goal image) was captured. Specifically, we encode goal images into a multimodal, semantic embedding space to enable training semantic-goal navigation (SemanticNav) agents at scale in unannotated 3D environments (e.g., HM3D). After training, SemanticNav agents can be instructed to find objects described in free-form natural language (e.g., "sink", "bathroom sink", etc.) by projecting language goals into the same multimodal, semantic embedding space. As a result, our approach enables open-world ObjectNav. We extensively evaluate our agents 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#30340;&#23384;&#26723;&#65292;&#38598;&#20013;&#25910;&#38598;&#20102;&#21508;&#31181;&#38382;&#39064;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#23545;&#38382;&#39064;&#30340;&#25551;&#36848;&#12289;&#26684;&#24335;&#21644;&#29305;&#24615;&#36827;&#34892;&#24635;&#32467;&#65292;&#20197;&#21450;&#21015;&#20030;&#20102;&#30456;&#20851;&#31639;&#27861;&#12290;&#35813;&#23384;&#26723;&#26088;&#22312;&#26041;&#20415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#19982;&#24050;&#26377;&#24037;&#20316;&#30340;&#27604;&#36739;&#65292;&#24182;&#27426;&#36814;&#25552;&#20132;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2202.03574</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#23384;&#26723;
&lt;/p&gt;
&lt;p&gt;
Structured Prediction Problem Archive. (arXiv:2202.03574v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03574
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#30340;&#23384;&#26723;&#65292;&#38598;&#20013;&#25910;&#38598;&#20102;&#21508;&#31181;&#38382;&#39064;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#23545;&#38382;&#39064;&#30340;&#25551;&#36848;&#12289;&#26684;&#24335;&#21644;&#29305;&#24615;&#36827;&#34892;&#24635;&#32467;&#65292;&#20197;&#21450;&#21015;&#20030;&#20102;&#30456;&#20851;&#31639;&#27861;&#12290;&#35813;&#23384;&#26723;&#26088;&#22312;&#26041;&#20415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#19982;&#24050;&#26377;&#24037;&#20316;&#30340;&#27604;&#36739;&#65292;&#24182;&#27426;&#36814;&#25552;&#20132;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#39044;&#27979;&#38382;&#39064;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#24037;&#20855;&#20043;&#19968;&#12290;&#20026;&#20102;&#20415;&#20110;&#31639;&#27861;&#24320;&#21457;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#22320;&#26041;&#25910;&#38598;&#20102;&#22823;&#37327;&#26131;&#20110;&#38405;&#35835;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#38382;&#39064;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#25968;&#25454;&#38598;&#30340;&#23384;&#26723;&#38142;&#25509;&#65292;&#38382;&#39064;&#25551;&#36848;&#21644;&#38382;&#39064;&#26684;&#24335;&#65292;&#20197;&#21450;&#38382;&#39064;&#29305;&#24615;&#30340;&#31616;&#35201;&#24635;&#32467;&#65292;&#21253;&#25324;&#22823;&#23567;&#65292;&#23454;&#20363;&#25968;&#37327;&#31561;&#12290;&#20026;&#20102;&#21442;&#32771;&#65292;&#25105;&#20204;&#36824;&#21015;&#20030;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#19968;&#20123;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#20013;&#22830;&#23384;&#20648;&#24211;&#33021;&#22815;&#26356;&#23481;&#26131;&#22320;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#19982;&#24050;&#26377;&#24037;&#20316;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#27426;&#36814;&#25552;&#20132;&#26377;&#36259;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#31639;&#27861;&#65292;&#20197;&#20415;&#23558;&#20854;&#32435;&#20837;&#25105;&#20204;&#30340;&#23384;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured prediction problems are one of the fundamental tools in machine learning. In order to facilitate algorithm development for their numerical solution, we collect in one place a large number of datasets in easy to read formats for a diverse set of problem classes. We provide archival links to datasets, description of the considered problems and problem formats, and a short summary of problem characteristics including size, number of instances etc. For reference we also give a non-exhaustive selection of algorithms proposed in the literature for their solution. We hope that this central repository will make benchmarking and comparison to established works easier. We welcome submission of interesting new datasets and algorithms for inclusion in our archive.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#20064;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#25913;&#21892;&#21271;&#26497;&#22320;&#21306;&#30340;&#36947;&#36335;&#32500;&#25252;&#12290;&#23454;&#20064;&#22242;&#38431;&#25104;&#21151;&#22320;&#24320;&#21457;&#20986;&#19968;&#20010;&#22825;&#27668;&#39044;&#25253;&#24212;&#29992;&#31243;&#24207;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#36947;&#36335;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2107.06755</link><description>&lt;p&gt;
DIT4BEARs&#26234;&#33021;&#36947;&#36335;&#23454;&#20064;&#65288;arXiv&#65306;2107.06755v2 [cs.LG]&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
DIT4BEARs Smart Roads Internship. (arXiv:2107.06755v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.06755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#20064;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#25913;&#21892;&#21271;&#26497;&#22320;&#21306;&#30340;&#36947;&#36335;&#32500;&#25252;&#12290;&#23454;&#20064;&#22242;&#38431;&#25104;&#21151;&#22320;&#24320;&#21457;&#20986;&#19968;&#20010;&#22825;&#27668;&#39044;&#25253;&#24212;&#29992;&#31243;&#24207;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#36947;&#36335;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23454;&#20064;&#26159;&#30001;&#25386;&#23041;&#21271;&#26497;&#22823;&#23398;&#65288;UiT&#65289;&#20026;&#25105;&#20204;&#22242;&#38431;&#25552;&#20379;&#30340;&#65292;&#25105;&#20204;&#26159;&#8220;&#26234;&#33021;&#36947;&#36335;-&#20908;&#23395;&#36947;&#36335;&#32500;&#25252;2021&#8221;&#40657;&#23458;&#39532;&#25289;&#26494;&#30340;&#33719;&#32988;&#32773;&#12290;&#23454;&#20064;&#20174;2021&#24180;5&#26376;3&#26085;&#24320;&#22987;&#65292;&#20110;2021&#24180;5&#26376;21&#26085;&#32467;&#26463;&#65292;&#27599;&#21608;&#36827;&#34892;&#20004;&#27425;&#20250;&#35758;&#12290;&#23613;&#31649;&#25105;&#20204;&#23454;&#20064;&#29983;&#26377;&#19981;&#21516;&#30340;&#22269;&#31821;&#21644;&#25945;&#32946;&#32972;&#26223;&#65292;&#20294;&#25105;&#20204;&#23613;&#21487;&#33021;&#22320;&#20316;&#20026;&#19968;&#20010;&#22242;&#38431;&#21512;&#20316;&#12290;&#26368;&#21560;&#24341;&#20154;&#30340;&#37096;&#20998;&#26159;&#25105;&#20204;&#24847;&#35782;&#21040;&#21271;&#26497;&#20154;&#27665;&#38754;&#20020;&#30340;&#20005;&#23803;&#26465;&#20214;&#65292;&#20174;&#25105;&#20204;&#30446;&#21069;&#23621;&#20303;&#30340;&#22320;&#26041;&#24456;&#38590;&#33719;&#24471;&#36825;&#26679;&#29420;&#29305;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#24320;&#21457;&#21644;&#23454;&#26045;&#20102;&#20960;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;&#29366;&#24577;&#65288;&#24178;&#29157;&#12289;&#28287;&#28070;&#12289;&#28526;&#28287;&#12289;&#32467;&#20912;&#12289;&#31215;&#38634;&#12289;&#27877;&#27870;&#65289;&#36827;&#34892;&#20998;&#31867;&#12290;&#26681;&#25454;&#26368;&#20339;&#27169;&#22411;&#65292;&#22825;&#27668;&#39044;&#25253;&#24212;&#29992;&#31243;&#24207;&#23558;&#32771;&#34385;Ta&#12289;Tsurf&#12289;Height&#12289;Speed&#12289;Water&#31561;&#22240;&#32032;&#26469;&#39044;&#27979;&#29366;&#24577;&#12290;&#20851;&#38190;&#37096;&#20998;&#26159;&#23450;&#20041;&#19968;&#20010;&#23433;&#20840;&#24230;&#37327;&#26631;&#20934;&#65292;&#35813;&#26631;&#20934;&#26159;&#22522;&#20110;&#25705;&#25830;&#21147;&#21644;&#20107;&#25925;&#29575;&#30340;&#20056;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research internship at UiT - The Arctic University of Norway was offered for our team being the winner of the 'Smart Roads - Winter Road Maintenance 2021' Hackathon. The internship commenced on 3 May 2021 and ended on 21 May 2021 with meetings happening twice each week. In spite of having different nationalities and educational backgrounds, we both interns tried to collaborate as a team as much as possible. The most alluring part was working on this project made us realize the critical conditions faced by the arctic people, where it was hard to gain such a unique experience from our residence. We developed and implemented several deep learning models to classify the states (dry, moist, wet, icy, snowy, slushy). Depending upon the best model, the weather forecast app will predict the state taking the Ta, Tsurf, Height, Speed, Water, etc. into consideration. The crucial part was to define a safety metric which is the product of the accident rates based on friction and the accident ra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#23457;&#20102;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;MDL&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#22797;&#26434;&#24230;&#19981;&#20165;&#21462;&#20915;&#20110;&#21442;&#25968;&#25968;&#37327;&#65292;&#36824;&#19982;&#35774;&#35745;&#30697;&#38453;&#25110;&#26680;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#21644;&#20449;&#22122;&#27604;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2006.10189</link><description>&lt;p&gt;
&#37325;&#23457;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Revisiting minimum description length complexity in overparameterized models. (arXiv:2006.10189v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#23457;&#20102;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#22797;&#26434;&#24230;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;MDL&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#22797;&#26434;&#24230;&#19981;&#20165;&#21462;&#20915;&#20110;&#21442;&#25968;&#25968;&#37327;&#65292;&#36824;&#19982;&#35774;&#35745;&#30697;&#38453;&#25110;&#26680;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#21644;&#20449;&#22122;&#27604;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24230;&#26159;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;&#27867;&#21270;&#24615;&#33021;&#30340;&#20449;&#24687;&#12290;&#22312;&#20302;&#32500;&#24230;&#24773;&#20917;&#19979;&#65292;&#21442;&#25968;&#25968;&#37327;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#26159;&#25104;&#21151;&#30340;&#65292;&#20294;&#22312;&#36229;&#21442;&#25968;&#27169;&#22411;&#20013;&#65292;&#24403;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#65292;&#20854;&#21512;&#29702;&#24615;&#19981;&#36275;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;Rissanen&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#65288;MDL&#65289;&#21407;&#29702;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#29992;&#20110;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#22522;&#20110;MDL&#30340;&#22797;&#26434;&#24230;&#65288;MDL-COMP&#65289;&#12290;MDL-COMP&#36890;&#36807;&#23545;&#19968;&#20010;&#33391;&#22909;&#30340;Ridge&#20272;&#35745;&#31867;&#25152;&#24341;&#36215;&#30340;&#32534;&#30721;&#32780;&#23450;&#20041;&#20986;&#26469;&#30340;&#26368;&#20248;&#24615;&#20934;&#21017;&#12290;&#25105;&#20204;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#26680;&#26041;&#27861;&#30340;MDL-COMP&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29702;&#35770;&#21051;&#30011;&#65292;&#24182;&#34920;&#26126;&#23427;&#19981;&#20165;&#26159;&#21442;&#25968;&#25968;&#37327;&#30340;&#20989;&#25968;&#65292;&#32780;&#26159;&#35774;&#35745;&#25110;&#26680;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#21644;&#20449;&#22122;&#27604;&#30340;&#20989;&#25968;&#12290;&#23545;&#20110;&#20855;&#26377;n&#20010;&#35266;&#27979;&#20540;&#65292;d&#20010;&#21442;&#25968;&#21644;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#39640;&#26031;&#39044;&#27979;&#22240;&#23376;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;MDL-COMP&#30340;&#23610;&#24230;&#26159;&#32447;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complexity is a fundamental concept underlying statistical learning theory that aims to inform generalization performance. Parameter count, while successful in low-dimensional settings, is not well-justified for overparameterized settings when the number of parameters is more than the number of training samples. We revisit complexity measures based on Rissanen's principle of minimum description length (MDL) and define a novel MDL-based complexity (MDL-COMP) that remains valid for overparameterized models. MDL-COMP is defined via an optimality criterion over the encodings induced by a good Ridge estimator class. We provide an extensive theoretical characterization of MDL-COMP for linear models and kernel methods and show that it is not just a function of parameter count, but rather a function of the singular values of the design or the kernel matrix and the signal-to-noise ratio. For a linear model with $n$ observations, $d$ parameters, and i.i.d. Gaussian predictors, MDL-COMP scales li
&lt;/p&gt;</description></item></channel></rss>