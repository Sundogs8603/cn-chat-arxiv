<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.01335</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#21487;&#20197;&#23558;&#20854;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#32454;&#35843;&#65288;SFT&#65289;&#21033;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#21147;&#37327;&#23545;&#20110;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#38656;&#35201;&#33719;&#21462;&#39069;&#22806;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#25104;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26032;&#30340;&#32454;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#32454;&#35843;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;SPIN&#30340;&#26680;&#24515;&#26159;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#20854;&#20013;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#23454;&#20363;&#23545;&#24328;&#26469;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#33258;&#24049;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20248;&#21270;&#33258;&#36523;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#33258;&#25105;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#26469;&#33258;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#22238;&#24212;&#26469;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20026;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20805;&#20998;&#21457;&#25496;&#20154;&#31867;&#26631;&#27880;&#31034;&#33539;&#25968;&#25454;&#22312;SFT&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#26159;&#21487;&#20197;&#36798;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#22270;&#29983;&#25104;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.01326</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#33258;&#22238;&#24402;&#25991;&#26412;&#21040;&#22270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Autoregressive Text-to-Graph Framework for Joint Entity and Relation Extraction. (arXiv:2401.01326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01326
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#36328;&#24230;&#30340;&#22270;&#29983;&#25104;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#23545;&#40784;&#12290;&#35780;&#20272;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#30340;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#20316;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#26469;&#35299;&#20915;&#12290;&#19982;&#20256;&#32479;&#30340;&#29983;&#25104;&#24335;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36328;&#24230;&#30340;&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#32447;&#24615;&#21270;&#30340;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#25991;&#26412;&#36328;&#24230;&#65292;&#36793;&#34920;&#31034;&#20851;&#31995;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#25351;&#21521;&#26426;&#21046;&#30340;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#21160;&#24577;&#35789;&#27719;&#34920;&#26469;&#34920;&#31034;&#36328;&#24230;&#21644;&#20851;&#31995;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36328;&#24230;&#34920;&#31034;&#25429;&#25417;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#32467;&#26500;&#29305;&#24449;&#21644;&#36793;&#30028;&#65292;&#21516;&#26102;&#36890;&#36807;&#25351;&#21521;&#26426;&#21046;&#23558;&#29983;&#25104;&#30340;&#36755;&#20986;&#19982;&#21407;&#22987;&#25991;&#26412;&#36827;&#34892;&#23545;&#40784;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/urchade/ATG&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method for joint entity and relation extraction from unstructured text by framing it as a conditional sequence generation problem. In contrast to conventional generative information extraction models that are left-to-right token-level generators, our approach is \textit{span-based}. It generates a linearized graph where nodes represent text spans and edges represent relation triplets. Our method employs a transformer encoder-decoder architecture with pointing mechanism on a dynamic vocabulary of spans and relation types. Our model can capture the structural characteristics and boundaries of entities and relations through span representations while simultaneously grounding the generated output in the original text thanks to the pointing mechanism. Evaluation on benchmark datasets validates the effectiveness of our approach, demonstrating competitive results. Code is available at https://github.com/urchade/ATG.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01325</link><description>&lt;p&gt;
&#33258;&#25193;&#23637;LLM:&#26080;&#38656;&#35843;&#25972;&#30340;LLM&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. (arXiv:2401.01325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Extend&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#26080;&#38656;&#35843;&#25972;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#22312;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26102;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#31934;&#35843;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#35757;&#32451;&#24207;&#21015;&#30340;&#26377;&#38480;&#38271;&#24230;&#21487;&#33021;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;LLMs&#26412;&#36523;&#20855;&#26377;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#33258;&#36523;&#25193;&#23637;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#20854;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Self-Extend&#26041;&#27861;&#26469;&#28608;&#21457;LLMs&#30340;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#28508;&#21147;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#26500;&#24314;&#21452;&#23618;&#27880;&#24847;&#20449;&#24687;&#65306;&#32676;&#32452;&#32423;&#21644;&#37051;&#23621;&#32423;&#12290;&#36825;&#20004;&#20010;&#32423;&#21035;&#36890;&#36807;&#21407;&#22987;&#27169;&#22411;&#30340;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#25552;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#12290;&#21482;&#38656;&#20462;&#25913;&#22235;&#34892;&#20195;&#30721;&#65292;&#25152;&#25552;&#26041;&#27861;&#23601;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#29616;&#26377;LLMs&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#31934;&#35843;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#26041;&#27861;&#21487;&#20197;+&#25688;&#35201;&#20943;&#25481;&#25991;&#31456;&#26368;&#21518;&#19968;&#21477;&#35441;
&lt;/p&gt;
&lt;p&gt;
This work elicits LLMs' inherent ability to handle long contexts without fine-tuning. The limited length of the training sequence during training may limit the application of Large Language Models (LLMs) on long input sequences for inference. In this work, we argue that existing LLMs themselves have inherent capabilities for handling long contexts. Based on this argument, we suggest extending LLMs' context window by themselves to fully utilize the inherent ability.We propose Self-Extend to stimulate LLMs' long context handling potential. The basic idea is to construct bi-level attention information: the group level and the neighbor level. The two levels are computed by the original model's self-attention, which means the proposed does not require any training. With only four lines of code modification, the proposed method can effortlessly extend existing LLMs' context window without any fine-tuning. We conduct comprehensive experiments and the results show that the proposed method can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#36816;&#29992;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#32602;&#20989;&#25968;&#27861;&#21644;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#27861;&#30340;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#22312;&#29609;&#20855;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35777;&#26126;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#22815;&#20135;&#29983;&#19981;&#38169;&#30340;&#36817;&#20284;&#35299;&#12290;&#22312;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#26159;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#26356;&#26032;&#35268;&#21017;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2401.01306</link><description>&lt;p&gt;
&#22312;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#23398;&#20064;&#19968;&#20123;&#29609;&#20855;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Learning solutions to some toy constrained optimization problems in infinite dimensional Hilbert spaces. (arXiv:2401.01306v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#36816;&#29992;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#32602;&#20989;&#25968;&#27861;&#21644;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#27861;&#30340;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#22312;&#29609;&#20855;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35777;&#26126;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#22815;&#20135;&#29983;&#19981;&#38169;&#30340;&#36817;&#20284;&#35299;&#12290;&#22312;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#26159;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#26356;&#26032;&#35268;&#21017;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20004;&#31181;&#27969;&#34892;&#30340;&#29702;&#35770;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#8212;&#8212;&#32602;&#20989;&#25968;&#27861;&#21644;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#19968;&#20123;&#28304;&#33258;&#21464;&#20998;&#27861;&#25110;&#29289;&#29702;&#23398;&#30340;&#29609;&#20855;&#38382;&#39064;&#19978;&#27979;&#35797;&#20102;&#36825;&#20123;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#22815;&#20135;&#29983;&#23545;&#27979;&#35797;&#38382;&#39064;&#30340;&#19981;&#38169;&#36817;&#20284;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#35823;&#24046;&#26041;&#38754;&#26159;&#21487;&#27604;&#36739;&#30340;&#12290;&#36890;&#36807;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#26356;&#26032;&#35268;&#21017;&#22312;&#35745;&#31639;&#19978;&#27604;&#27714;&#35299;&#32602;&#20989;&#25968;&#27861;&#20013;&#30340;&#23376;&#38382;&#39064;&#26356;&#31616;&#21333;&#30340;&#26222;&#36941;&#24773;&#20917;&#65292;&#25105;&#20204;&#22312;&#36755;&#20986;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#26159;&#19968;&#20010;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we present deep learning implementations of two popular theoretical constrained optimization algorithms in infinite dimensional Hilbert spaces, namely, the penalty and the augmented Lagrangian methods. We test these algorithms on some toy problems originating in either calculus of variations or physics. We demonstrate that both methods are able to produce decent approximations for the test problems and are comparable in terms of different errors. Leveraging the common occurrence of the Lagrange multiplier update rule being computationally less expensive than solving subproblems in the penalty method, we achieve significant speedups in cases when the output of the constraint function is itself a function.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36793;&#32536;&#25972;&#21512;&#21040;U-Net&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#28608;&#27963;&#22270;&#23545;&#33041;&#32959;&#30244;&#36827;&#34892;&#20998;&#21106;&#12290;&#20316;&#32773;&#36890;&#36807;&#25552;&#21462;&#36793;&#32536;&#24182;&#36827;&#34892;&#37325;&#24314;&#65292;&#24471;&#21040;&#20102;&#36793;&#32536;&#30340;&#30495;&#23454;&#24773;&#20917;&#65292;&#24182;&#19982;&#33041;&#32959;&#30244;&#19968;&#36215;&#20316;&#20026;&#30446;&#26631;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.01303</link><description>&lt;p&gt;
&#23558;&#36793;&#32536;&#25972;&#21512;&#21040;U-Net&#27169;&#22411;&#20013;&#65292;&#24182;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#28608;&#27963;&#22270;&#23545;&#33041;&#32959;&#30244;&#20998;&#21106;&#36827;&#34892;MRI&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Integrating Edges into U-Net Models with Explainable Activation Maps for Brain Tumor Segmentation using MR Images. (arXiv:2401.01303v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36793;&#32536;&#25972;&#21512;&#21040;U-Net&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#28608;&#27963;&#22270;&#23545;&#33041;&#32959;&#30244;&#36827;&#34892;&#20998;&#21106;&#12290;&#20316;&#32773;&#36890;&#36807;&#25552;&#21462;&#36793;&#32536;&#24182;&#36827;&#34892;&#37325;&#24314;&#65292;&#24471;&#21040;&#20102;&#36793;&#32536;&#30340;&#30495;&#23454;&#24773;&#20917;&#65292;&#24182;&#19982;&#33041;&#32959;&#30244;&#19968;&#36215;&#20316;&#20026;&#30446;&#26631;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30913;&#20849;&#25391;&#65288;MR&#65289;&#22270;&#20687;&#20013;&#25163;&#21160;&#21010;&#23450;&#32959;&#30244;&#21306;&#22495;&#32791;&#26102;&#12289;&#38656;&#35201;&#19987;&#23478;&#65292;&#24182;&#23481;&#26131;&#20986;&#29616;&#20154;&#20026;&#38169;&#35823;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#20026;&#33041;&#32959;&#30244;&#20998;&#21106;&#30340;&#39318;&#36873;&#26041;&#27861;&#12290;U-Net&#21450;&#20854;&#21464;&#31181;&#22312;&#21307;&#23398;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;U-Net&#21450;&#20854;&#21464;&#31181;&#20542;&#21521;&#20110;&#36807;&#20998;&#20998;&#21106;&#32959;&#30244;&#21306;&#22495;&#65292;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21010;&#20998;&#32959;&#30244;&#36793;&#32536;&#12290;&#23545;&#20110;&#20934;&#30830;&#35786;&#26029;&#12289;&#25163;&#26415;&#31934;&#24230;&#21644;&#27835;&#30103;&#35268;&#21010;&#65292;&#32959;&#30244;&#36793;&#32536;&#19982;&#32959;&#30244;&#21306;&#22495;&#21516;&#26679;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#26088;&#22312;&#21033;&#29992;&#31867;&#20284;&#23548;&#25968;&#30340;&#28388;&#27874;&#22120;&#20174;&#23454;&#38469;&#24773;&#20917;&#20013;&#25552;&#21462;&#36793;&#32536;&#65292;&#28982;&#21518;&#36827;&#34892;&#36793;&#32536;&#37325;&#24314;&#65292;&#20197;&#33719;&#24471;&#36793;&#32536;&#30340;&#30495;&#23454;&#24773;&#20917;&#65292;&#38500;&#20102;&#33041;&#32959;&#30244;&#30340;&#30495;&#23454;&#24773;&#20917;&#12290;&#21033;&#29992;&#36825;&#20004;&#20010;&#30495;&#23454;&#24773;&#20917;&#65292;&#20316;&#32773;&#30740;&#31350;&#20102;&#20960;&#31181;U-Net&#21450;&#20854;&#21464;&#31181;&#32467;&#26500;&#65292;&#26377;&#20123;&#20197;&#21450;&#27809;&#26377;&#32959;&#30244;&#36793;&#32536;&#30495;&#23454;&#24773;&#20917;&#20316;&#20026;&#30446;&#26631;&#19982;&#32959;&#30244;&#30495;&#23454;&#24773;&#20917;&#19968;&#36215;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manual delineation of tumor regions from magnetic resonance (MR) images is time-consuming, requires an expert, and is prone to human error. In recent years, deep learning models have been the go-to approach for the segmentation of brain tumors. U-Net and its' variants for semantic segmentation of medical images have achieved good results in the literature. However, U-Net and its' variants tend to over-segment tumor regions and may not accurately segment the tumor edges. The edges of the tumor are as important as the tumor regions for accurate diagnosis, surgical precision, and treatment planning. In the proposed work, the authors aim to extract edges from the ground truth using a derivative-like filter followed by edge reconstruction to obtain an edge ground truth in addition to the brain tumor ground truth. Utilizing both ground truths, the author studies several U-Net and its' variant architectures with and without tumor edges ground truth as a target along with the tumor ground trut
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#24555;&#36895;&#30340;&#31639;&#27861;&#65292;&#23558;&#38750;&#20809;&#28369;&#25439;&#22833;&#38382;&#39064;&#36716;&#21270;&#20026;&#24809;&#32602;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#19977;&#38454;&#27573;&#22122;&#22768;&#27880;&#20837;&#26469;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2401.01294</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#31232;&#30095;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#22238;&#24402;&#19982;&#24046;&#20998;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Efficient Sparse Least Absolute Deviation Regression with Differential Privacy. (arXiv:2401.01294v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#24555;&#36895;&#30340;&#31639;&#27861;&#65292;&#23558;&#38750;&#20809;&#28369;&#25439;&#22833;&#38382;&#39064;&#36716;&#21270;&#20026;&#24809;&#32602;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#19977;&#38454;&#27573;&#22122;&#22768;&#27880;&#20837;&#26469;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#26469;&#65292;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22240;&#20854;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#30340;&#37325;&#35201;&#24212;&#29992;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#65292;&#22823;&#22810;&#25968;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#35201;&#27714;&#23398;&#20064;&#30446;&#26631;&#26159;&#24378;&#20984;&#19988;Lipschitz&#24179;&#28369;&#30340;&#65292;&#36825;&#19981;&#33021;&#28085;&#30422;&#24191;&#27867;&#30340;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65288;&#20363;&#22914;&#65292;&#20998;&#20301;&#25968;/&#26368;&#23567;&#32477;&#23545;&#25439;&#22833;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#31232;&#30095;&#40065;&#26834;&#22238;&#24402;&#38382;&#39064;&#24320;&#21457;&#19968;&#31181;&#24555;&#36895;&#30340;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23398;&#20064;&#25439;&#22833;&#21253;&#25324;&#19968;&#20010;&#40065;&#26834;&#30340;&#26368;&#23567;&#32477;&#23545;&#25439;&#22833;&#21644;&#19968;&#20010;l1&#31232;&#30095;&#24809;&#32602;&#39033;&#12290;&#20026;&#20102;&#22312;&#32473;&#23450;&#30340;&#38544;&#31169;&#39044;&#31639;&#19979;&#24555;&#36895;&#35299;&#20915;&#38750;&#20809;&#28369;&#25439;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#40065;&#26834;&#21644;&#38544;&#31169;&#20445;&#25252;&#20272;&#35745;&#65288;FRAPPE&#65289;&#31639;&#27861;&#26469;&#36827;&#34892;&#26368;&#23567;&#32477;&#23545;&#20559;&#24046;&#22238;&#24402;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#23558;&#31232;&#30095;LAD&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#24809;&#32602;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#19977;&#38454;&#27573;&#22122;&#22768;&#27880;&#20837;&#26469;&#20445;&#35777;&#65288;&#949;&#12289;&#948;&#65289;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, privacy-preserving machine learning algorithms have attracted increasing attention because of their important applications in many scientific fields. However, in the literature, most privacy-preserving algorithms demand learning objectives to be strongly convex and Lipschitz smooth, which thus cannot cover a wide class of robust loss functions (e.g., quantile/least absolute loss). In this work, we aim to develop a fast privacy-preserving learning solution for a sparse robust regression problem. Our learning loss consists of a robust least absolute loss and an $\ell_1$ sparse penalty term. To fast solve the non-smooth loss under a given privacy budget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE) algorithm for least absolute deviation regression. Our algorithm achieves a fast estimation by reformulating the sparse LAD problem as a penalized least square estimation problem and adopts a three-stage noise injection to guarantee the $(\epsilon,\delta)
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#30340;&#35821;&#20041;&#31561;&#20215;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#20998;&#26512;&#24341;&#25806;&#19978;&#26816;&#27979;&#31561;&#20215;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01280</link><description>&lt;p&gt;
GEqO: &#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#30340;&#35821;&#20041;&#31561;&#20215;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GEqO: ML-Accelerated Semantic Equivalence Detection. (arXiv:2401.01280v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#21152;&#36895;&#30340;&#35821;&#20041;&#31561;&#20215;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;&#20998;&#26512;&#24341;&#25806;&#19978;&#26816;&#27979;&#31561;&#20215;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#20998;&#26512;&#24341;&#25806;&#24050;&#32463;&#25104;&#20026;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#20225;&#19994;&#25512;&#23548;&#19994;&#21153;&#27934;&#23519;&#21644;&#25512;&#21160;&#34892;&#21160;&#30340;&#26680;&#24515;&#20381;&#36182;&#12290;&#36825;&#20123;&#24341;&#25806;&#25903;&#25345;&#22823;&#37327;&#30340;&#20998;&#26512;&#20316;&#19994;&#65292;&#27599;&#22825;&#22788;&#29702;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#24037;&#20316;&#36127;&#36733;&#32463;&#24120;&#34987;&#22810;&#20010;&#20316;&#19994;&#30340;&#37325;&#21472;&#35745;&#31639;&#25152;&#28153;&#27809;&#12290;&#37325;&#22797;&#20351;&#29992;&#24120;&#35265;&#35745;&#31639;&#23545;&#20110;&#26377;&#25928;&#21033;&#29992;&#38598;&#32676;&#36164;&#28304;&#21644;&#20943;&#23569;&#20316;&#19994;&#25191;&#34892;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#26816;&#27979;&#24120;&#35265;&#35745;&#31639;&#26159;&#20943;&#23569;&#36825;&#31181;&#35745;&#31639;&#20887;&#20313;&#30340;&#31532;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#20998;&#26512;&#24341;&#25806;&#19978;&#26816;&#27979;&#31561;&#20215;&#20851;&#31995;&#38656;&#35201;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#23436;&#20840;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26368;&#22823;&#31243;&#24230;&#22320;&#37325;&#22797;&#20351;&#29992;&#35745;&#31639;&#65292;&#38656;&#35201;&#22312;&#35821;&#20041;&#23618;&#38754;&#19978;&#26816;&#27979;&#31561;&#20215;&#20851;&#31995;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#21477;&#27861;&#23618;&#38754;&#19978;&#65288;&#21363;&#33021;&#22815;&#26816;&#27979;&#22806;&#35266;&#19981;&#21516;&#30340;&#26597;&#35810;&#30340;&#35821;&#20041;&#31561;&#20215;&#24615;&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large scale analytics engines have become a core dependency for modern data-driven enterprises to derive business insights and drive actions. These engines support a large number of analytic jobs processing huge volumes of data on a daily basis, and workloads are often inundated with overlapping computations across multiple jobs. Reusing common computation is crucial for efficient cluster resource utilization and reducing job execution time. Detecting common computation is the first and key step for reducing this computational redundancy. However, detecting equivalence on large-scale analytics engines requires efficient and scalable solutions that are fully automated. In addition, to maximize computation reuse, equivalence needs to be detected at the semantic level instead of just the syntactic level (i.e., the ability to detect semantic equivalence of seemingly different-looking queries). Unfortunately, existing solutions fall short of satisfying these requirements.  In this paper, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;Gym-DSSAT&#27169;&#25311;&#22120;&#35757;&#32451;&#26234;&#33021;agent&#26469;&#25484;&#25569;&#26368;&#20339;&#27694;&#32933;&#31649;&#29702;&#31574;&#30053;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#24207;&#21015;&#35266;&#27979;&#24320;&#21457;&#26356;&#39640;&#25928;&#27694;&#32933;&#36755;&#20837;&#31574;&#30053;&#30340;&#20248;&#21183;&#65292;&#24182;&#25506;&#35752;&#20102;&#27668;&#20505;&#21464;&#24322;&#23545;&#20892;&#19994;&#31649;&#29702;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.01273</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#29615;&#22659;&#27668;&#20505;&#21464;&#24322;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#20892;&#19994;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning-based agricultural management in partially observable environments subject to climate variability. (arXiv:2401.01273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;Gym-DSSAT&#27169;&#25311;&#22120;&#35757;&#32451;&#26234;&#33021;agent&#26469;&#25484;&#25569;&#26368;&#20339;&#27694;&#32933;&#31649;&#29702;&#31574;&#30053;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#24207;&#21015;&#35266;&#27979;&#24320;&#21457;&#26356;&#39640;&#25928;&#27694;&#32933;&#36755;&#20837;&#31574;&#30053;&#30340;&#20248;&#21183;&#65292;&#24182;&#25506;&#35752;&#20102;&#27668;&#20505;&#21464;&#24322;&#23545;&#20892;&#19994;&#31649;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#31649;&#29702;&#22312;&#22609;&#36896;&#20316;&#29289;&#20135;&#37327;&#12289;&#32463;&#27982;&#21487;&#30408;&#21033;&#24615;&#21644;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#29305;&#21035;&#20851;&#27880;&#26045;&#32933;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26497;&#31471;&#22825;&#27668;&#26465;&#20214;&#65288;&#22914;&#28909;&#28010;&#21644;&#24178;&#26097;&#65289;&#26102;&#65292;&#20256;&#32479;&#25351;&#23548;&#26041;&#38024;&#30340;&#26377;&#25928;&#24615;&#20943;&#24369;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#19982;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;Gym-DSSAT&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26234;&#33021;agent&#26469;&#25484;&#25569;&#26368;&#20339;&#27694;&#32933;&#31649;&#29702;&#12290;&#36890;&#36807;&#22312;&#29233;&#33655;&#21326;&#24030;&#29577;&#31859;&#20892;&#20316;&#29289;&#19978;&#36827;&#34892;&#19968;&#31995;&#21015;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#27169;&#22411;&#21644;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#24207;&#21015;&#35266;&#27979;&#26469;&#24320;&#21457;&#26356;&#39640;&#25928;&#30340;&#27694;&#32933;&#36755;&#20837;&#31574;&#30053;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#27668;&#20505;&#30340;&#21464;&#24322;&#24615;&#23545;&#20892;&#19994;&#31649;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agricultural management, with a particular focus on fertilization strategies, holds a central role in shaping crop yield, economic profitability, and environmental sustainability. While conventional guidelines offer valuable insights, their efficacy diminishes when confronted with extreme weather conditions, such as heatwaves and droughts. In this study, we introduce an innovative framework that integrates Deep Reinforcement Learning (DRL) with Recurrent Neural Networks (RNNs). Leveraging the Gym-DSSAT simulator, we train an intelligent agent to master optimal nitrogen fertilization management. Through a series of simulation experiments conducted on corn crops in Iowa, we compare Partially Observable Markov Decision Process (POMDP) models with Markov Decision Process (MDP) models. Our research underscores the advantages of utilizing sequential observations in developing more efficient nitrogen input policies. Additionally, we explore the impact of climate variability, particularly duri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#22823;&#32500;&#24230;&#19979;&#30340;&#26680;&#33034;&#22238;&#24402;&#30340;&#34892;&#20026;&#20197;&#21450;&#20854;&#26368;&#20248;&#36873;&#25321;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#12290;&#32467;&#26524;&#21457;&#29616;&#22312;&#28385;&#36275;&#28304;&#26465;&#20214;$s&gt;0$&#26102;&#65292;KRR&#26159;&#26368;&#20248;&#30340;&#36873;&#25321;&#65307;&#32780;&#24403;$s&gt;1$&#26102;&#65292;KRR&#19981;&#26159;&#26368;&#20248;&#30340;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#36895;&#29575;&#26354;&#32447;&#22312;&#19981;&#21516;$\gamma$&#21644;$s$&#19979;&#30340;&#21608;&#26399;&#24615;&#21488;&#38454;&#34892;&#20026;&#21644;&#22810;&#27425;&#19979;&#38477;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.01270</link><description>&lt;p&gt;
&#22312;&#22823;&#32500;&#24230;&#19979;&#30340;&#28304;&#26465;&#20214;&#19979;&#30340;&#26680;&#33034;&#22238;&#24402;&#30340;&#26368;&#20248;&#29575;
&lt;/p&gt;
&lt;p&gt;
Optimal Rates of Kernel Ridge Regression under Source Condition in Large Dimensions. (arXiv:2401.01270v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#22823;&#32500;&#24230;&#19979;&#30340;&#26680;&#33034;&#22238;&#24402;&#30340;&#34892;&#20026;&#20197;&#21450;&#20854;&#26368;&#20248;&#36873;&#25321;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#12290;&#32467;&#26524;&#21457;&#29616;&#22312;&#28385;&#36275;&#28304;&#26465;&#20214;$s&gt;0$&#26102;&#65292;KRR&#26159;&#26368;&#20248;&#30340;&#36873;&#25321;&#65307;&#32780;&#24403;$s&gt;1$&#26102;&#65292;KRR&#19981;&#26159;&#26368;&#20248;&#30340;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#23637;&#31034;&#20102;&#36895;&#29575;&#26354;&#32447;&#22312;&#19981;&#21516;$\gamma$&#21644;$s$&#19979;&#30340;&#21608;&#26399;&#24615;&#21488;&#38454;&#34892;&#20026;&#21644;&#22810;&#27425;&#19979;&#38477;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#65288;&#22914;&#31070;&#32463;&#20999;&#21521;&#26680;&#29702;&#35770;&#65289;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#23545;&#26680;&#33034;&#22238;&#24402;&#65288;KRR&#65289;&#22312;&#22823;&#32500;&#24230;&#19979;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20854;&#20013;&#26679;&#26412;&#37327;$n \asymp d^{\gamma}$&#65292;&#20854;&#20013;$\gamma &gt; 0$&#12290;&#32473;&#23450;&#19982;&#29699;$\mathbb{S}^{d}$&#19978;&#23450;&#20041;&#30340;&#20869;&#31215;&#26680;&#30456;&#20851;&#30340;RKHS $\mathcal{H}$&#65292;&#25105;&#20204;&#20551;&#35774;&#30495;&#23454;&#20989;&#25968;$f_{\rho}^{*} \in [\mathcal{H}]^{s}$&#65292;&#21363;$\mathcal{H}$&#30340;&#25554;&#20540;&#31354;&#38388;&#65292;&#20854;&#28304;&#26465;&#20214;$s&gt;0$&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#26680;&#33034;&#22238;&#24402;&#22312;&#26368;&#20248;&#36873;&#25321;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;$\lambda$&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#31934;&#30830;&#38454;&#25968;&#65288;&#19978;&#30028;&#21644;&#19979;&#30028;&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#24403;$0&lt;s\le1$&#26102;&#65292;KRR&#26159;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#26368;&#20248;&#36873;&#25321;&#65307;&#24403;$s&gt;1$&#26102;&#65292;KRR&#19981;&#26159;&#26368;&#23567;&#21270;&#39118;&#38505;&#30340;&#26368;&#20248;&#36873;&#25321;&#65288;&#20063;&#31216;&#39281;&#21644;&#25928;&#24212;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35828;&#26126;&#20102;&#22312;$\gamma$&#30340;&#21464;&#21270;&#19979;&#65292;&#36895;&#29575;&#26354;&#32447;&#21576;&#29616;&#21608;&#26399;&#24615;&#30340;&#21488;&#38454;&#34892;&#20026;&#21644;&#22810;&#27425;&#19979;&#38477;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#26354;&#32447;&#22914;&#20309;&#38543;$s&gt;0$&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the studies of neural networks (e.g.,the neural tangent kernel theory), we perform a study on the large-dimensional behavior of kernel ridge regression (KRR) where the sample size $n \asymp d^{\gamma}$ for some $\gamma &gt; 0$. Given an RKHS $\mathcal{H}$ associated with an inner product kernel defined on the sphere $\mathbb{S}^{d}$, we suppose that the true function $f_{\rho}^{*} \in [\mathcal{H}]^{s}$, the interpolation space of $\mathcal{H}$ with source condition $s&gt;0$. We first determined the exact order (both upper and lower bound) of the generalization error of kernel ridge regression for the optimally chosen regularization parameter $\lambda$. We then further showed that when $0&lt;s\le1$, KRR is minimax optimal; and when $s&gt;1$, KRR is not minimax optimal (a.k.a. he saturation effect). Our results illustrate that the curves of rate varying along $\gamma$ exhibit the periodic plateau behavior and the multiple descent behavior and show how the curves evolve with $s&gt;0$. Inte
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$f$&#25955;&#24230;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#20351;&#29992;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#21462;&#22522;&#20110;&#21464;&#20998;&#34920;&#31034;&#30340;$f$&#25955;&#24230;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#36125;&#21494;&#26031;&#35270;&#35282;&#23558;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20116;&#20010;&#37319;&#29992;&#19981;&#21516;$f$&#25955;&#24230;&#30340;&#21518;&#39564;&#27010;&#29575;&#20272;&#35745;&#22120;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24179;&#31227;&#23545;&#25968;&#30340;$f$&#25955;&#24230;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#30340;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01268</link><description>&lt;p&gt;
&#22522;&#20110;$f$&#25955;&#24230;&#30340;&#20998;&#31867;&#65306;&#36229;&#36234;&#20132;&#21449;&#29109;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy. (arXiv:2401.01268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01268
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;$f$&#25955;&#24230;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#20351;&#29992;&#20132;&#21449;&#29109;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#21462;&#22522;&#20110;&#21464;&#20998;&#34920;&#31034;&#30340;$f$&#25955;&#24230;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#36125;&#21494;&#26031;&#35270;&#35282;&#23558;&#20998;&#31867;&#20219;&#21153;&#35270;&#20026;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20116;&#20010;&#37319;&#29992;&#19981;&#21516;$f$&#25955;&#24230;&#30340;&#21518;&#39564;&#27010;&#29575;&#20272;&#35745;&#22120;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24179;&#31227;&#23545;&#25968;&#30340;$f$&#25955;&#24230;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#36825;&#39033;&#30740;&#31350;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#20998;&#31867;&#20219;&#21153;&#34987;&#24418;&#24335;&#21270;&#20026;&#36890;&#36807;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#26469;&#35299;&#20915;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#30446;&#26631;&#20989;&#25968;&#35774;&#35745;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;$f$&#25955;&#24230;&#24230;&#37327;&#21487;&#20197;&#25512;&#24191;&#20998;&#31867;&#38382;&#39064;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#35270;&#35282;&#65292;&#24182;&#23558;&#20998;&#31867;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#26368;&#22823;&#21518;&#39564;&#27010;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#22522;&#20110;$f$&#25955;&#24230;&#21464;&#20998;&#34920;&#31034;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#20174;&#20013;&#25552;&#21462;&#20102;&#19968;&#31995;&#21015;&#20116;&#20010;&#21518;&#39564;&#27010;&#29575;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;$f$&#25955;&#24230;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;&#25913;&#36827;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25361;&#25112;&#30340;&#39537;&#21160;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#19968;&#20010;&#23545;&#24212;&#20110;&#19968;&#31181;&#26032;&#30340;&#34987;&#31216;&#20026;&#24179;&#31227;&#23545;&#25968; (SL) &#30340;$f$&#25955;&#24230;&#30340;&#26032;&#30446;&#26631;&#20989;&#25968;&#65288;&#21644;&#21518;&#39564;&#27010;&#29575;&#20272;&#35745;&#22120;&#65289;&#30340;&#20844;&#24335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25910;&#25947;&#24615;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, classification tasks are formalized as optimization problems solved via the minimization of the cross-entropy. However, recent advancements in the design of objective functions allow the $f$-divergence measure to generalize the formulation of the optimization problem for classification. With this goal in mind, we adopt a Bayesian perspective and formulate the classification task as a maximum a posteriori probability problem. We propose a class of objective functions based on the variational representation of the $f$-divergence, from which we extract a list of five posterior probability estimators leveraging well-known $f$-divergences. In addition, driven by the challenge of improving the state-of-the-art approach, we propose a bottom-up method that leads us to the formulation of a new objective function (and posterior probability estimator) corresponding to a novel $f$-divergence referred to as shifted log (SL). First, we theoretically prove the convergence property o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#21457;&#20844;&#24179;&#24615;&#35748;&#35777;&#26041;&#27861;&#12290;&#36890;&#36807;&#32508;&#36848;&#22823;&#37327;&#25991;&#29486;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20026;&#25805;&#20316;&#21270;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2401.01262</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20844;&#24179;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01262
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#21457;&#20844;&#24179;&#24615;&#35748;&#35777;&#26041;&#27861;&#12290;&#36890;&#36807;&#32508;&#36848;&#22823;&#37327;&#25991;&#29486;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20026;&#25805;&#20316;&#21270;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;NLP&#22312;&#25307;&#32856;&#31561;&#20844;&#24179;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#20013;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#20363;&#22914;&#20316;&#20026;&#19987;&#23478;&#31995;&#32479;&#25110;&#22522;&#20110;LLM&#30340;&#25945;&#32946;&#23548;&#24072;&#12290;&#30001;&#20110;NLP&#22522;&#20110;&#20154;&#31867;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#26377;&#23475;&#20559;&#35265;&#28183;&#20837;NLP&#31995;&#32479;&#65292;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#27495;&#35270;&#23569;&#25968;&#32676;&#20307;&#25110;&#24341;&#21457;&#27861;&#24459;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24320;&#23637;NLP&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#37319;&#29992;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#22823;&#37327;&#25991;&#29486;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#19982;&#35813;&#39046;&#22495;&#30340;&#22810;&#20301;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#30340;&#19987;&#23478;&#35775;&#35848;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25552;&#20986;&#20102;NLP&#30340;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#32454;&#21270;&#20026;18&#20010;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26631;&#20934;&#20026;&#23454;&#26045;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) plays an important role in our daily lives, particularly due to the enormous progress of Large Language Models (LLM). However, NLP has many fairness-critical use cases, e.g., as an expert system in recruitment or as an LLM-based tutor in education. Since NLP is based on human language, potentially harmful biases can diffuse into NLP systems and produce unfair results, discriminate against minorities or generate legal issues. Hence, it is important to develop a fairness certification for NLP approaches. We follow a qualitative research approach towards a fairness certification for NLP. In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area. We have systematically devised six fairness criteria for NLP, which can be further refined into 18 sub-categories. Our criteria offer a foundation for operationalizing and testing processes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.01259</link><description>&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#26159;&#21542;&#36981;&#24490;&#23616;&#37096;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Concept Bottleneck Models Obey Locality?. (arXiv:2401.01259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#22522;&#30784;&#23398;&#20064;&#36890;&#36807;&#35299;&#37322;&#20854;&#39044;&#27979;&#32467;&#26524;&#20351;&#29992;&#20154;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#25913;&#21892;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#31181;&#33539;&#24335;&#19979;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#29420;&#31435;&#20110;&#20854;&#20182;&#27010;&#24565;&#30340;&#32473;&#23450;&#27010;&#24565;&#30340;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#28872;&#26263;&#31034;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#22312;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26550;&#26500;&#20013;&#19981;&#33021;&#25104;&#31435;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#36825;&#20123;&#27010;&#24565;&#26082;&#22312;&#31354;&#38388;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#23436;&#20840;&#30001;&#22266;&#23450;&#23376;&#38598;&#30340;&#29305;&#24449;&#23450;&#20041;&#65289;&#21448;&#22312;&#35821;&#20041;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#20165;&#19982;&#39044;&#23450;&#20041;&#30340;&#22266;&#23450;&#23376;&#38598;&#30340;&#27010;&#24565;&#30456;&#20851;&#32852;&#65289;&#23450;&#20301;&#26102;&#65292;CBMs&#26159;&#21542;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#12290;&#20026;&#20102;&#29702;&#35299;&#23616;&#37096;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27010;&#24565;&#20043;&#22806;&#30340;&#29305;&#24449;&#21464;&#21270;&#23545;&#27010;&#24565;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept-based learning improves a deep learning model's interpretability by explaining its predictions via human-understandable concepts. Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts. Recent work, however, strongly suggests that this assumption may fail to hold in Concept Bottleneck Models (CBMs), a quintessential family of concept-based interpretable architectures. In this paper, we investigate whether CBMs correctly capture the degree of conditional independence across concepts when such concepts are localised both spatially, by having their values entirely defined by a fixed subset of features, and semantically, by having their values correlated with only a fixed subset of predefined concepts. To understand locality, we analyse how changes to features outside of a concept's spatial or semantic locality impact concept predictions. Our
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20849;&#21516;&#36827;&#21270;&#40654;&#26364;&#31354;&#38388;&#30340;&#23545;&#27604;&#39034;&#24207;&#20132;&#20114;&#32593;&#32476;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#21644;&#39033;&#30446;&#33410;&#28857;&#30340;&#21306;&#20998;&#38382;&#39064;&#65292;&#20197;&#21450;&#21160;&#24577;&#34920;&#31034;&#31354;&#38388;&#30340;&#28436;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01243</link><description>&lt;p&gt;
&#22522;&#20110;&#20849;&#21516;&#36827;&#21270;&#27969;&#24418;&#31354;&#38388;&#30340;&#23545;&#27604;&#39034;&#24207;&#20132;&#20114;&#32593;&#32476;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Sequential Interaction Network Learning on Co-Evolving Riemannian Spaces. (arXiv:2401.01243v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20849;&#21516;&#36827;&#21270;&#40654;&#26364;&#31354;&#38388;&#30340;&#23545;&#27604;&#39034;&#24207;&#20132;&#20114;&#32593;&#32476;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#21644;&#39033;&#30446;&#33410;&#28857;&#30340;&#21306;&#20998;&#38382;&#39064;&#65292;&#20197;&#21450;&#21160;&#24577;&#34920;&#31034;&#31354;&#38388;&#30340;&#28436;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#20132;&#20114;&#32593;&#32476;&#36890;&#24120;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#22914;&#25512;&#33616;&#31995;&#32479;&#12290;&#25512;&#27979;&#26410;&#26469;&#30340;&#20132;&#20114;&#34892;&#20026;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#32463;&#20856;&#30340;&#38646;&#26354;&#29575;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#31995;&#21015;&#37325;&#35201;&#30340;&#38382;&#39064;&#65306;&#22312;&#21452;&#20998;&#22270;&#24615;&#36136;&#20013;&#65292;&#23558;&#29992;&#25143;&#21644;&#39033;&#30446;&#33410;&#28857;&#25918;&#32622;&#22312;&#19968;&#20010;&#30456;&#21516;&#30340;&#31354;&#38388;&#20013;&#26159;&#21542;&#36866;&#24403;&#65292;&#26080;&#35270;&#23427;&#20204;&#20043;&#38388;&#30340;&#20869;&#22312;&#24046;&#24322;&#65311;&#22312;&#32593;&#32476;&#21160;&#24577;&#26041;&#38754;&#65292;&#19982;&#22266;&#23450;&#26354;&#29575;&#31354;&#38388;&#19981;&#21516;&#65292;&#24403;&#26032;&#30340;&#20132;&#20114;&#19981;&#26029;&#21040;&#26469;&#26102;&#65292;&#34920;&#31034;&#31354;&#38388;&#26159;&#21542;&#20250;&#21457;&#29983;&#28436;&#21270;&#65311;&#22312;&#23398;&#20064;&#33539;&#24335;&#26041;&#38754;&#65292;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#25670;&#33073;&#26114;&#36149;&#30340;&#26631;&#31614;&#20449;&#24687;&#33719;&#21462;&#65311;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#20849;&#21516;&#36827;&#21270;&#40654;&#26364;&#31354;&#38388;&#30340;&#23545;&#27604;&#39034;&#24207;&#20132;&#20114;&#32593;&#32476;&#23398;&#20064;&#27169;&#22411;CSINCERE&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#24341;&#20837;&#20102;&#19968;&#23545;&#20849;&#21516;&#36827;&#21270;&#30340;&#27969;&#24418;&#31354;&#38388;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sequential interaction network usually find itself in a variety of applications, e.g., recommender system. Herein, inferring future interaction is of fundamental importance, and previous efforts are mainly focused on the dynamics in the classic zero-curvature Euclidean space. Despite the promising results achieved by previous methods, a range of significant issues still largely remains open: On the bipartite nature, is it appropriate to place user and item nodes in one identical space regardless of their inherent difference? On the network dynamics, instead of a fixed curvature space, will the representation spaces evolve when new interactions arrive continuously? On the learning paradigm, can we get rid of the label information costly to acquire? To address the aforementioned issues, we propose a novel Contrastive model for Sequential Interaction Network learning on Co-Evolving RiEmannian spaces, CSINCERE. To the best of our knowledge, we are the first to introduce a couple of co-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20108;&#36827;&#21046;&#20107;&#20214;&#32534;&#30721;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20855;&#26377;&#25512;&#26029;&#26412;&#22320;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01242</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#26681;&#26641;&#20013;&#23545;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#30340;&#20108;&#36827;&#21046;&#20107;&#20214;&#36827;&#34892;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Encoding Binary Events from Continuous Time Series in Rooted Trees using Contrastive Learning. (arXiv:2401.01242v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20108;&#36827;&#21046;&#20107;&#20214;&#32534;&#30721;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20855;&#26377;&#25512;&#26029;&#26412;&#22320;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#22495;&#22522;&#30784;&#35774;&#26045;&#25152;&#26377;&#32773;&#36890;&#24120;&#19981;&#30693;&#36947;&#20182;&#20204;&#30340;&#23458;&#25143;&#22312;&#26412;&#22320;&#32593;&#32476;&#20013;&#26159;&#22914;&#20309;&#36830;&#25509;&#30340;&#65292;&#36825;&#20123;&#32593;&#32476;&#20197;&#26681;&#26641;&#32467;&#26500;&#32452;&#32455;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#20351;&#29992;&#26469;&#33258;&#26641;&#21494;&#65288;&#23458;&#25143;&#65289;&#30340;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25512;&#26029;&#26412;&#22320;&#32593;&#32476;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20108;&#36827;&#21046;&#20107;&#20214;&#32534;&#30721;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#20316;&#20026;&#21021;&#27493;&#32467;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#26377;&#20215;&#20540;&#30340;&#32534;&#30721;&#22120;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Broadband infrastructure owners do not always know how their customers are connected in the local networks, which are structured as rooted trees. A recent study is able to infer the topology of a local network using discrete time series data from the leaves of the tree (customers). In this study we propose a contrastive approach for learning a binary event encoder from continuous time series data. As a preliminary result, we show that our approach has some potential in learning a valuable encoder.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#28040;&#38500;&#32593;&#32476;&#65288;GENs&#65289;&#65292;&#20854;&#36890;&#36807;&#28040;&#38500;&#37051;&#22495;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#20887;&#20313;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#28145;&#23618;&#27425;&#19978;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;GENs&#21487;&#20197;&#22686;&#24378;&#33410;&#28857;&#23545;&#36828;&#36317;&#31163;&#37051;&#22495;&#30340;&#24863;&#30693;&#65292;&#24182;&#25193;&#23637;&#32593;&#32476;&#20256;&#25773;&#30340;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.01233</link><description>&lt;p&gt;
&#22270;&#28040;&#38500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Elimination Networks. (arXiv:2401.01233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#28040;&#38500;&#32593;&#32476;&#65288;GENs&#65289;&#65292;&#20854;&#36890;&#36807;&#28040;&#38500;&#37051;&#22495;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#20887;&#20313;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#28145;&#23618;&#27425;&#19978;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;GENs&#21487;&#20197;&#22686;&#24378;&#33410;&#28857;&#23545;&#36828;&#36317;&#31163;&#37051;&#22495;&#30340;&#24863;&#30693;&#65292;&#24182;&#25193;&#23637;&#32593;&#32476;&#20256;&#25773;&#30340;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20294;&#22312;&#28145;&#23618;&#27425;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#23558;&#36825;&#20010;&#38382;&#39064;&#24402;&#22240;&#20110;&#33410;&#28857;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#22312;&#22810;&#36718;&#20256;&#25773;&#20043;&#21518;&#65292;&#33410;&#28857;&#34920;&#31034;&#21464;&#24471;&#26080;&#27861;&#21306;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;GNN&#30340;&#37051;&#22495;&#20256;&#25773;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;GNN&#22312;&#28145;&#23618;&#27425;&#19978;&#24615;&#33021;&#19979;&#38477;&#30340;&#30495;&#27491;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#37051;&#22495;&#29305;&#24449;&#20256;&#25773;&#30340;&#26080;&#25928;&#24615;&#12290;&#36825;&#31181;&#20256;&#25773;&#22312;&#27599;&#19968;&#27493;&#20256;&#25773;&#20013;&#23548;&#33268;&#33410;&#28857;&#24403;&#21069;&#34920;&#31034;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#20351;&#24471;&#25429;&#25417;&#38271;&#36317;&#31163;&#33410;&#28857;&#20043;&#38388;&#30340;&#26377;&#20215;&#20540;&#20381;&#36182;&#20851;&#31995;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22270;&#28040;&#38500;&#32593;&#32476;&#65288;GENs&#65289;&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#29305;&#23450;&#30340;&#31639;&#27861;&#22312;&#37051;&#22495;&#20256;&#25773;&#36807;&#31243;&#20013;&#28040;&#38500;&#20887;&#20313;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GENs&#21487;&#20197;&#22686;&#24378;&#33410;&#28857;&#23545;&#36828;&#36317;&#31163;&#37051;&#22495;&#30340;&#24863;&#30693;&#65292;&#24182;&#25193;&#23637;&#32593;&#32476;&#20256;&#25773;&#30340;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are widely applied across various domains, yet they perform poorly in deep layers. Existing research typically attributes this problem to node over-smoothing, where node representations become indistinguishable after multiple rounds of propagation. In this paper, we delve into the neighborhood propagation mechanism of GNNs and discover that the real root cause of GNNs' performance degradation in deep layers lies in ineffective neighborhood feature propagation. This propagation leads to an exponential growth of a node's current representation at every propagation step, making it extremely challenging to capture valuable dependencies between long-distance nodes. To address this issue, we introduce Graph Elimination Networks (GENs), which employ a specific algorithm to eliminate redundancies during neighborhood propagation. We demonstrate that GENs can enhance nodes' perception of distant neighborhoods and extend the depth of network propagation. Extensive exp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#30340;&#40654;&#26364;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;-&#23545;&#27604;&#23398;&#20064;&#22312;&#40654;&#26364;&#27969;&#24418;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#25429;&#25417;&#22810;&#26679;&#26354;&#29575;&#27969;&#24418;&#20013;&#30340;&#20027;&#39064;&#35268;&#24459;&#12290;</title><link>http://arxiv.org/abs/2401.01232</link><description>&lt;p&gt;
&#20855;&#26377;&#29983;&#25104;-&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#40654;&#26364;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Motif-aware Riemannian Graph Neural Network with Generative-Contrastive Learning. (arXiv:2401.01232v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#39064;&#30340;&#40654;&#26364;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;-&#23545;&#27604;&#23398;&#20064;&#22312;&#40654;&#26364;&#27969;&#24418;&#20013;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#25429;&#25417;&#22810;&#26679;&#26354;&#29575;&#27969;&#24418;&#20013;&#30340;&#20027;&#39064;&#35268;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26159;&#20856;&#22411;&#30340;&#22797;&#26434;&#32467;&#26500;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#40654;&#26364;&#22270;&#34920;&#31034;&#23398;&#20064;&#20316;&#20026;&#27431;&#20960;&#37324;&#24471;&#26041;&#27861;&#30340;&#19968;&#31181;&#28608;&#21160;&#20154;&#24515;&#30340;&#26367;&#20195;&#26041;&#26696;&#20986;&#29616;&#20102;&#12290;&#28982;&#32780;&#65292;&#40654;&#26364;&#26041;&#27861;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65306;&#22823;&#22810;&#25968;&#26041;&#27861;&#21482;&#25552;&#20379;&#20102;&#19968;&#20010;&#26354;&#29575;&#65288;&#21322;&#24452;&#65289;&#65292;&#26080;&#35270;&#32467;&#26500;&#22797;&#26434;&#24615;&#65292;&#30001;&#20110;&#25351;&#25968;/&#23545;&#25968;&#26144;&#23556;&#32780;&#23548;&#33268;&#25968;&#20540;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#32570;&#20047;&#25429;&#25417;&#20027;&#39064;&#35268;&#24459;&#30340;&#33021;&#21147;&#12290;&#37492;&#20110;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#20027;&#39064;&#30340;&#40654;&#26364;&#22270;&#34920;&#31034;&#23398;&#20064;&#8221;&#30340;&#38382;&#39064;&#65292;&#23547;&#27714;&#19968;&#31181;&#25968;&#20540;&#31283;&#23450;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25429;&#25417;&#22810;&#26679;&#26354;&#29575;&#27969;&#24418;&#20013;&#30340;&#20027;&#39064;&#35268;&#24459;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#29983;&#25104;-&#23545;&#27604;&#23398;&#20064;&#30340;MotifRGC&#27169;&#22411;&#65292;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#22312;&#40654;&#26364;&#27969;&#24418;&#20013;&#36827;&#34892;minmax&#21338;&#24328;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#40654;&#26364;GCN&#65288;D-GCN&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#20135;&#21697;&#23618;&#26469;&#26500;&#24314;&#22810;&#26679;&#26354;&#29575;&#27969;&#24418;
&lt;/p&gt;
&lt;p&gt;
Graphs are typical non-Euclidean data of complex structures. In recent years, Riemannian graph representation learning has emerged as an exciting alternative to Euclidean ones. However, Riemannian methods are still in an early stage: most of them present a single curvature (radius) regardless of structural complexity, suffer from numerical instability due to the exponential/logarithmic map, and lack the ability to capture motif regularity. In light of the issues above, we propose the problem of \emph{Motif-aware Riemannian Graph Representation Learning}, seeking a numerically stable encoder to capture motif regularity in a diverse-curvature manifold without labels. To this end, we present a novel Motif-aware Riemannian model with Generative-Contrastive learning (MotifRGC), which conducts a minmax game in Riemannian manifold in a self-supervised manner. First, we propose a new type of Riemannian GCN (D-GCN), in which we construct a diverse-curvature manifold by a product layer with the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01218</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Position Debiasing for Large Language Models. (arXiv:2401.01218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#26041;&#27861;&#65288;ZOE&#65289;&#26469;&#38477;&#20302;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#12290;&#23454;&#39564;&#35777;&#23454;ZOE&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#25913;&#21892;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#24615;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#36866;&#24212;&#25968;&#25454;&#38598;&#20559;&#35265;&#21644;&#39044;&#27979;&#30340;&#25463;&#24452;&#65292;&#23548;&#33268;&#29983;&#25104;&#24615;&#33021;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#23481;&#26131;&#34920;&#29616;&#20986;&#20301;&#32622;&#20559;&#24046;&#65292;&#21363;&#21033;&#29992;&#20301;&#20110;&#24320;&#22836;&#25110;&#26411;&#23614;&#25110;&#36755;&#20837;&#20013;&#29305;&#23450;&#20301;&#32622;&#32447;&#32034;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#20943;&#36731;&#20301;&#32622;&#20559;&#24046;&#30340;&#24037;&#20316;&#38656;&#35201;&#22806;&#37096;&#20559;&#24046;&#30693;&#35782;&#25110;&#24102;&#27880;&#37322;&#30340;&#38750;&#20559;&#20506;&#26679;&#26412;&#65292;&#22312;&#23454;&#38469;&#20013;&#19981;&#22826;&#23454;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#20301;&#32622;&#21435;&#20559;&#65288;ZOE&#65289;&#26694;&#26550;&#23545;LLMs&#36827;&#34892;&#20301;&#32622;&#21435;&#20559;&#12290;ZOE&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLMs&#30340;&#26080;&#30417;&#30563;&#21709;&#24212;&#36827;&#34892;&#21435;&#20559;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#20219;&#20309;&#22806;&#37096;&#30693;&#35782;&#25110;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25552;&#39640;&#26080;&#30417;&#30563;&#21709;&#24212;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#20174;&#23545;&#40784;&#65288;MSA&#65289;&#27169;&#22359;&#26469;&#20462;&#21098;&#36825;&#20123;&#21709;&#24212;&#12290;&#23545;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#20116;&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ZOE&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning has been demonstrated to be an effective method to improve the domain performance of large language models (LLMs). However, LLMs might fit the dataset bias and shortcuts for prediction, leading to poor generation performance. Experimental result shows that LLMs are prone to exhibit position bias, i.e., leveraging information positioned at the beginning or end, or specific positional cues within the input. Existing works on mitigating position bias require external bias knowledge or annotated non-biased samples, which is unpractical in reality. In this work, we propose a zero-shot position debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages unsupervised responses from pre-trained LLMs for debiasing, thus without any external knowledge or datasets. To improve the quality of unsupervised responses, we propose a master-slave alignment (MSA) module to prune these responses. Experiments on eight datasets and five tasks show that ZOE consistently outperform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#32974;&#20799;&#29983;&#29289;&#27979;&#37327;&#65292;&#36890;&#36807;&#20174;&#27599;&#20010;&#36229;&#22768;&#25195;&#25551;&#24103;&#20013;&#25552;&#21462;&#29983;&#29289;&#27979;&#37327;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#20934;&#30830;&#20272;&#35745;&#21644;&#24322;&#24120;&#20540;&#25298;&#32477;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01201</link><description>&lt;p&gt;
&#20174;20&#21608;&#36229;&#22768;&#25195;&#25551;&#20013;&#20840;&#38754;&#20272;&#35745;&#32974;&#20799;&#29983;&#29289;&#27979;&#37327;&#30340;AI&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Whole-examination AI estimation of fetal biometrics from 20-week ultrasound scans. (arXiv:2401.01201v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#32974;&#20799;&#29983;&#29289;&#27979;&#37327;&#65292;&#36890;&#36807;&#20174;&#27599;&#20010;&#36229;&#22768;&#25195;&#25551;&#24103;&#20013;&#25552;&#21462;&#29983;&#29289;&#27979;&#37327;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#20934;&#30830;&#20272;&#35745;&#21644;&#24322;&#24120;&#20540;&#25298;&#32477;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#32974;&#20799;&#30072;&#24418;&#31579;&#26597;&#26041;&#27861;&#26159;&#22522;&#20110;&#20174;&#21333;&#20010;&#36873;&#25321;&#30340;&#36229;&#22768;&#22270;&#20687;&#20013;&#33719;&#24471;&#30340;&#29983;&#29289;&#27979;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#65292;&#36890;&#36807;&#32858;&#21512;&#25972;&#20010;&#25195;&#25551;&#20013;&#33258;&#21160;&#25552;&#21462;&#30340;&#29983;&#29289;&#27979;&#37327;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#27700;&#24179;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#19988;&#26080;&#38656;&#25805;&#20316;&#21592;&#24178;&#39044;&#12290;&#25105;&#20204;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#36229;&#22768;&#35270;&#39057;&#24405;&#20687;&#30340;&#27599;&#20010;&#24103;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#21518;&#65292;&#22312;&#26174;&#31034;&#36866;&#24403;&#35299;&#21078;&#32467;&#26500;&#30340;&#27599;&#20010;&#24103;&#20013;&#27979;&#37327;&#32974;&#20799;&#29983;&#29289;&#27979;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#20174;&#22823;&#37327;&#27979;&#37327;&#20540;&#20013;&#20272;&#35745;&#27599;&#20010;&#29983;&#29289;&#27979;&#37327;&#30340;&#30495;&#23454;&#20540;&#65292;&#24182;&#20197;&#27010;&#29575;&#26041;&#24335;&#25298;&#32477;&#24322;&#24120;&#20540;&#12290;&#25105;&#20204;&#23545;1457&#20010;20&#21608;&#36229;&#22768;&#25195;&#25551;&#30340;&#24405;&#20687;&#36827;&#34892;&#20102;&#22238;&#39038;&#24615;&#23454;&#39564;&#65292;&#22312;&#36825;&#20123;&#25195;&#25551;&#20013;&#20272;&#35745;&#20102;&#32974;&#20799;&#29983;&#29289;&#27979;&#37327;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#20272;&#35745;&#19982;&#36229;&#22768;&#21307;&#29983;&#22312;&#25195;&#25551;&#36807;&#31243;&#20013;&#30340;&#27979;&#37327;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20272;&#35745;&#32974;&#20799;&#29983;&#29289;&#27979;&#37327;&#26041;&#38754;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current approach to fetal anomaly screening is based on biometric measurements derived from individually selected ultrasound images. In this paper, we introduce a paradigm shift that attains human-level performance in biometric measurement by aggregating automatically extracted biometrics from every frame across an entire scan, with no need for operator intervention. We use a convolutional neural network to classify each frame of an ultrasound video recording. We then measure fetal biometrics in every frame where appropriate anatomy is visible. We use a Bayesian method to estimate the true value of each biometric from a large number of measurements and probabilistically reject outliers. We performed a retrospective experiment on 1457 recordings (comprising 48 million frames) of 20-week ultrasound scans, estimated fetal biometrics in those scans and compared our estimates to the measurements sonographers took during the scan. Our method achieves human-level performance in estimating
&lt;/p&gt;</description></item><item><title>JMA&#26159;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20960;&#20046;&#26368;&#20248;&#30340;&#23450;&#21521;&#23545;&#25239;&#26679;&#26412;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;Jacobian&#24341;&#36215;&#30340;&#39532;&#27663;&#36317;&#31163;&#65292;&#32771;&#34385;&#20102;&#23558;&#36755;&#20837;&#26679;&#26412;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#22312;&#32473;&#23450;&#26041;&#21521;&#19978;&#31227;&#21160;&#25152;&#38656;&#30340;&#25237;&#20837;&#12290;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#23545;&#25239;&#26679;&#26412;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.01199</link><description>&lt;p&gt;
JMA:&#19968;&#31181;&#24555;&#36895;&#29983;&#25104;&#20960;&#20046;&#26368;&#20248;&#23450;&#21521;&#23545;&#25239;&#26679;&#26412;&#30340;&#36890;&#29992;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial Example. (arXiv:2401.01199v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01199
&lt;/p&gt;
&lt;p&gt;
JMA&#26159;&#19968;&#31181;&#36890;&#29992;&#31639;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20960;&#20046;&#26368;&#20248;&#30340;&#23450;&#21521;&#23545;&#25239;&#26679;&#26412;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;Jacobian&#24341;&#36215;&#30340;&#39532;&#27663;&#36317;&#31163;&#65292;&#32771;&#34385;&#20102;&#23558;&#36755;&#20837;&#26679;&#26412;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#22312;&#32473;&#23450;&#26041;&#21521;&#19978;&#31227;&#21160;&#25152;&#38656;&#30340;&#25237;&#20837;&#12290;&#35813;&#31639;&#27861;&#22312;&#35299;&#20915;&#23545;&#25239;&#26679;&#26412;&#38382;&#39064;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#29992;&#20110;&#29983;&#25104;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#23450;&#21521;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#37117;&#26159;&#39640;&#24230;&#27425;&#20248;&#30340;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#22686;&#21152;&#30446;&#26631;&#31867;&#21035;&#30340;&#21487;&#33021;&#24615;&#65292;&#22240;&#27492;&#38544;&#21547;&#22320;&#19987;&#27880;&#20110;&#19968;&#28909;&#32534;&#30721;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#36890;&#29992;&#30340;&#12289;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#23450;&#21521;&#25915;&#20987;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26368;&#23567;&#21270;&#38597;&#21487;&#27604;&#24341;&#36215;&#30340;&#39532;&#27663;&#36317;&#31163;&#65288;JMA&#65289;&#39033;&#65292;&#32771;&#34385;&#23558;&#36755;&#20837;&#26679;&#26412;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#22312;&#32473;&#23450;&#26041;&#21521;&#19978;&#31227;&#21160;&#25152;&#38656;&#30340;&#25237;&#20837;&#65288;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#27779;&#23572;&#22827;&#20108;&#37325;&#24615;&#23450;&#29702;&#27714;&#35299;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#35299;&#38750;&#36127;&#26368;&#23567;&#20108;&#20056;&#65288;NNLS&#65289;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20026;Szegedy&#31561;&#20154;&#26368;&#21021;&#24341;&#20837;&#30340;&#23545;&#25239;&#26679;&#26412;&#38382;&#39064;&#30340;&#32447;&#24615;&#21270;&#29256;&#26412;&#25552;&#20379;&#20102;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#30340;&#25915;&#20987;&#30340;&#24191;&#27867;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the approaches proposed so far to craft targeted adversarial examples against Deep Learning classifiers are highly suboptimal and typically rely on increasing the likelihood of the target class, thus implicitly focusing on one-hot encoding settings. In this paper, we propose a more general, theoretically sound, targeted attack that resorts to the minimization of a Jacobian-induced MAhalanobis distance (JMA) term, taking into account the effort (in the input space) required to move the latent space representation of the input sample in a given direction. The minimization is solved by exploiting the Wolfe duality theorem, reducing the problem to the solution of a Non-Negative Least Square (NNLS) problem. The proposed algorithm provides an optimal solution to a linearized version of the adversarial example problem originally introduced by Szegedy et al. \cite{szegedy2013intriguing}. The experiments we carried out confirm the generality of the proposed attack which is proven to be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Deep-ELA&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#65292;&#23545;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;ELA&#29305;&#24449;&#23384;&#22312;&#30340;&#24378;&#30456;&#20851;&#24615;&#21644;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#23616;&#38480;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01192</link><description>&lt;p&gt;
Deep-ELA: &#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#36827;&#34892;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#30340;&#28145;&#24230;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Deep-ELA: Deep Exploratory Landscape Analysis with Self-Supervised Pretrained Transformers for Single- and Multi-Objective Continuous Optimization Problems. (arXiv:2401.01192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Deep-ELA&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#21464;&#25442;&#22120;&#65292;&#23545;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;ELA&#29305;&#24449;&#23384;&#22312;&#30340;&#24378;&#30456;&#20851;&#24615;&#21644;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#23616;&#38480;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;&#65288;ELA&#65289;&#29305;&#24449;&#30340;&#28508;&#21147;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#23545;&#29305;&#23450;&#30340;&#21333;&#30446;&#26631;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#25968;&#20540;&#34920;&#24449;&#12290;&#36825;&#20123;&#25968;&#20540;&#29305;&#24449;&#20026;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#36755;&#20837;&#65292;&#21253;&#25324;&#39640;&#32423;&#23646;&#24615;&#39044;&#27979;&#12289;&#33258;&#21160;&#31639;&#27861;&#36873;&#25321;&#21644;&#33258;&#21160;&#31639;&#27861;&#37197;&#32622;&#31561;&#12290;&#27809;&#26377;ELA&#29305;&#24449;&#65292;&#20998;&#26512;&#21644;&#29702;&#35299;&#21333;&#30446;&#26631;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#30340;&#29305;&#24615;&#23558;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#26080;&#21487;&#20105;&#35758;&#65292;ELA&#29305;&#24449;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#12290;&#20854;&#20013;&#21253;&#25324;&#65288;1.&#65289;&#22810;&#20010;&#29305;&#24449;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#65288;2.&#65289;&#20854;&#22312;&#22810;&#30446;&#26631;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#38750;&#24120;&#26377;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20197;&#28145;&#24230;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#20316;&#20026;ELA&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#36825;&#20123;&#30740;&#31350;&#20013;&#65292;&#20363;&#22914;&#65292;&#28857;&#20113;&#21464;&#25442;&#22120;&#34987;&#25552;&#20986;&#20316;&#20026;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many recent works, the potential of Exploratory Landscape Analysis (ELA) features to numerically characterize, in particular, single-objective continuous optimization problems has been demonstrated. These numerical features provide the input for all kinds of machine learning tasks on continuous optimization problems, ranging, i.a., from High-level Property Prediction to Automated Algorithm Selection and Automated Algorithm Configuration. Without ELA features, analyzing and understanding the characteristics of single-objective continuous optimization problems would be impossible.  Yet, despite their undisputed usefulness, ELA features suffer from several drawbacks. These include, in particular, (1.) a strong correlation between multiple features, as well as (2.) its very limited applicability to multi-objective continuous optimization problems. As a remedy, recent works proposed deep learning-based approaches as alternatives to ELA. In these works, e.g., point-cloud transformers were
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20923;&#32467;&#20027;&#24178;&#30340;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#25239;&#24178;&#25200;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20445;&#30041;&#20449;&#24687;&#30340;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#24182;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01179</link><description>&lt;p&gt;
&#20923;&#32467;&#20027;&#24178;&#65306;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#25239;&#24178;&#25200;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23545;&#27604;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training. (arXiv:2401.01179v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01179
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20923;&#32467;&#20027;&#24178;&#30340;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#25239;&#24178;&#25200;&#21307;&#23398;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20445;&#30041;&#20449;&#24687;&#30340;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#24182;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#21307;&#30103;&#24120;&#24120;&#22312;&#35786;&#26029;&#20013;&#21516;&#26102;&#20351;&#29992;&#25918;&#23556;&#22270;&#20687;&#21644;&#25991;&#23383;&#25253;&#21578;&#65292;&#40723;&#21169;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#35270;&#35273;-&#35821;&#35328;&#33258;&#30417;&#30563;&#23398;&#20064;(VL-SSL)&#20197;&#23398;&#20064;&#22810;&#21151;&#33021;&#30340;&#21307;&#23398;&#35270;&#35273;&#34920;&#24449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;VL-SSL&#26694;&#26550;&#26159;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#65292;&#36825;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#20002;&#22833;&#23884;&#20837;&#22312;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#30340;&#37325;&#35201;&#20808;&#39564;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26080;&#20027;&#24178;&#36866;&#37197;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#20445;&#25345;&#39044;&#35757;&#32451;&#22270;&#20687;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#20923;&#32467;&#29366;&#24577;&#65292;&#20445;&#30041;&#21307;&#23398;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#27169;&#22359;&#36827;&#34892;&#36328;&#27169;&#24577;&#23398;&#20064;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20445;&#30041;&#20449;&#24687;&#30340;&#21516;&#26102;&#65292;&#21487;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20943;&#23569;&#36229;&#36807;90%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#21482;&#20351;&#29992;1%&#30340;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#36866;&#37197;&#22120;&#30340;&#24615;&#33021;&#20248;&#20110;&#20960;&#31181;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern healthcare often utilises radiographic images alongside textual reports for diagnostics, encouraging the use of Vision-Language Self-Supervised Learning (VL-SSL) with large pre-trained models to learn versatile medical vision representations. However, most existing VL-SSL frameworks are trained end-to-end, which is computation-heavy and can lose vital prior information embedded in pre-trained encoders. To address both issues, we introduce the backbone-agnostic Adaptor framework, which preserves medical knowledge in pre-trained image and text encoders by keeping them frozen, and employs a lightweight Adaptor module for cross-modal learning. Experiments on medical image classification and segmentation tasks across three datasets reveal that our framework delivers competitive performance while cutting trainable parameters by over 90% compared to current pre-training approaches. Notably, when fine-tuned with just 1% of data, Adaptor outperforms several Transformer-based methods trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#20041;&#36890;&#20449;&#22312;&#31163;&#25955;&#35760;&#24518;&#36890;&#36947;&#19978;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#36890;&#36807;&#37319;&#29992;&#35821;&#20041;&#36895;&#29575;&#22833;&#30495;&#20989;&#25968;&#65288;SRDF&#65289;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#23567;&#21387;&#32553;&#29575;&#12289;&#35266;&#27979;&#22833;&#30495;&#12289;&#35821;&#20041;&#22833;&#30495;&#21644;&#36890;&#36947;&#23481;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23545;&#20110;&#26410;&#30693;&#35821;&#20041;&#28304;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20998;&#24067;&#12290;&#23545;&#20110;&#24050;&#30693;&#35821;&#20041;&#28304;&#20998;&#24067;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;SRDF&#12290;</title><link>http://arxiv.org/abs/2401.01176</link><description>&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#30340;&#22522;&#26412;&#38480;&#21046;: &#29992;&#20110;&#36895;&#29575;&#22833;&#30495;&#30340;&#31070;&#32463;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limitation of Semantic Communications: Neural Estimation for Rate-Distortion. (arXiv:2401.01176v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#20041;&#36890;&#20449;&#22312;&#31163;&#25955;&#35760;&#24518;&#36890;&#36947;&#19978;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#36890;&#36807;&#37319;&#29992;&#35821;&#20041;&#36895;&#29575;&#22833;&#30495;&#20989;&#25968;&#65288;SRDF&#65289;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#23567;&#21387;&#32553;&#29575;&#12289;&#35266;&#27979;&#22833;&#30495;&#12289;&#35821;&#20041;&#22833;&#30495;&#21644;&#36890;&#36947;&#23481;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23545;&#20110;&#26410;&#30693;&#35821;&#20041;&#28304;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20998;&#24067;&#12290;&#23545;&#20110;&#24050;&#30693;&#35821;&#20041;&#28304;&#20998;&#24067;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;SRDF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31163;&#25955;&#35760;&#24518;&#36890;&#36947;&#19978;&#35821;&#20041;&#36890;&#20449;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#25105;&#20204;&#32771;&#34385;&#21457;&#36865;&#19968;&#20010;&#21253;&#21547;&#35266;&#27979;&#29366;&#24577;&#21644;&#23545;&#24212;&#30340;&#35821;&#20041;&#29366;&#24577;&#30340;&#35821;&#20041;&#28304;&#30340;&#22330;&#26223;&#65292;&#25509;&#25910;&#31471;&#33021;&#22815;&#24674;&#22797;&#36825;&#20004;&#31181;&#29366;&#24577;&#12290;&#20026;&#20102;&#25512;&#23548;&#24615;&#33021;&#38480;&#21046;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#35821;&#20041;&#36895;&#29575;&#22833;&#30495;&#20989;&#25968;&#65288;SRDF&#65289;&#26469;&#30740;&#31350;&#26368;&#23567;&#21387;&#32553;&#29575;&#12289;&#35266;&#27979;&#22833;&#30495;&#12289;&#35821;&#20041;&#22833;&#30495;&#21644;&#36890;&#36947;&#23481;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23545;&#20110;&#26410;&#30693;&#35821;&#20041;&#28304;&#20998;&#24067;&#30340;&#24773;&#20917;&#65292;&#20165;&#26377;&#19968;&#32452;&#28304;&#26679;&#26412;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#32593;&#32476;&#23398;&#20064;&#35821;&#20041;&#28304;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#35821;&#20041;&#29366;&#24577;&#26159;&#35266;&#27979;&#30340;&#30830;&#23450;&#20989;&#25968;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;SRDF&#12290;&#23545;&#20110;&#23436;&#20840;&#24050;&#30693;&#30340;&#35821;&#20041;&#28304;&#20998;&#24067;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;Blahut-Arimoto&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the fundamental limit of semantic communications over the discrete memoryless channel. We consider the scenario to send a semantic source consisting of an observation state and its corresponding semantic state, both of which are recovered at the receiver. To derive the performance limitation, we adopt the semantic rate-distortion function (SRDF) to study the relationship among the minimum compression rate, observation distortion, semantic distortion, and channel capacity. For the case with unknown semantic source distribution, while only a set of the source samples is available, we propose a neural-network-based method by leveraging the generative networks to learn the semantic source distribution. Furthermore, for a special case where the semantic state is a deterministic function of the observation, we design a cascade neural network to estimate the SRDF. For the case with perfectly known semantic source distribution, we propose a general Blahut-Arimoto algorithm t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#35786;&#26029;&#24102;&#26377;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.01172</link><description>&lt;p&gt;
&#25391;&#21160;&#20449;&#21495;&#30340;&#20108;&#27425;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#29992;&#20110;&#35786;&#26029;&#36724;&#25215;&#25925;&#38556;
&lt;/p&gt;
&lt;p&gt;
Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing Bearing Faults. (arXiv:2401.01172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#35786;&#26029;&#24102;&#26377;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36724;&#25215;&#25925;&#38556;&#30340;&#35786;&#26029;&#23545;&#20110;&#38477;&#20302;&#32500;&#20462;&#25104;&#26412;&#21644;&#35774;&#22791;&#20572;&#26426;&#33267;&#20851;&#37325;&#35201;&#12290;&#36724;&#25215;&#25925;&#38556;&#26159;&#26426;&#22120;&#25391;&#21160;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#20998;&#26512;&#20854;&#20449;&#21495;&#24418;&#24577;&#21487;&#20197;&#25581;&#31034;&#20854;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#25511;&#21046;&#29615;&#22659;&#36827;&#34892;&#20248;&#21270;&#65292;&#24573;&#30053;&#20102;&#23454;&#38469;&#26465;&#20214;&#19979;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#36716;&#36895;&#21644;&#25391;&#21160;&#30340;&#38750;&#24179;&#31283;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#35786;&#26029;&#36724;&#25215;&#25925;&#38556;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#36724;&#25215;&#25925;&#38556;&#24341;&#36215;&#30340;&#25391;&#21160;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#38750;&#24179;&#31283;&#24615;&#19982;&#36724;&#25215;&#22266;&#26377;&#21644;&#25805;&#20316;&#21442;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#38416;&#36848;&#20102;&#20108;&#27425;&#26102;&#38388;&#39057;&#29575;&#20998;&#24067;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#39057;&#29575;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosis of bearing faults is paramount to reducing maintenance costs and operational breakdowns. Bearing faults are primary contributors to machine vibrations, and analyzing their signal morphology offers insights into their health status. Unfortunately, existing approaches are optimized for controlled environments, neglecting realistic conditions such as time-varying rotational speeds and the vibration's non-stationary nature. This paper presents a fusion of time-frequency analysis and deep learning techniques to diagnose bearing faults under time-varying speeds and varying noise levels. First, we formulate the bearing fault-induced vibrations and discuss the link between their non-stationarity and the bearing's inherent and operational parameters. We also elucidate quadratic time-frequency distributions and validate their effectiveness in resolving distinctive dynamic patterns associated with different bearing faults. Based on this, we design a time-frequency convolutional neural n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedQV&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#20108;&#27425;&#25237;&#31080;&#26426;&#21046;&#30340;&#26032;&#22411;&#32858;&#21512;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#23481;&#26131;&#21463;&#21040;&#27745;&#26579;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;FedQV&#26159;&#19968;&#20010;&#30495;&#23454;&#26426;&#21046;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01168</link><description>&lt;p&gt;
FedQV: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#20108;&#27425;&#25237;&#31080;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedQV: Leveraging Quadratic Voting in Federated Learning. (arXiv:2401.01168v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedQV&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#20108;&#27425;&#25237;&#31080;&#26426;&#21046;&#30340;&#26032;&#22411;&#32858;&#21512;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#23481;&#26131;&#21463;&#21040;&#27745;&#26579;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;FedQV&#26159;&#19968;&#20010;&#30495;&#23454;&#26426;&#21046;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#19981;&#21516;&#26041;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#36879;&#38706;&#21508;&#33258;&#30340;&#26412;&#22320;&#26631;&#31614;&#12290;FL&#30340;&#20851;&#38190;&#27493;&#39588;&#20043;&#19968;&#26159;&#23558;&#26412;&#22320;&#27169;&#22411;&#32858;&#21512;&#25104;&#20840;&#23616;&#27169;&#22411;&#65292;&#36825;&#19982;&#20844;&#20849;&#20915;&#31574;&#65292;&#23588;&#20854;&#26159;&#36873;&#20030;&#65292;&#26377;&#24456;&#22810;&#30456;&#20284;&#20043;&#22788;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;FL&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#26159;&#20854;&#26131;&#21463;&#27745;&#26579;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#29616;&#20195;&#32858;&#21512;&#35268;&#21017;&#20013;&#8220;&#19968;&#20154;&#19968;&#31080;&#8221;&#21407;&#21017;&#65288;&#21363;1p1v&#65289;&#30340;&#21518;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedQV&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20108;&#27425;&#25237;&#31080;&#26426;&#21046;&#30340;&#26032;&#22411;&#32858;&#21512;&#31639;&#27861;&#65292;&#35813;&#26426;&#21046;&#26368;&#36817;&#34987;&#25552;&#20986;&#20316;&#20026;1p1v&#36873;&#20030;&#30340;&#26356;&#22909;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;FedQV&#26159;&#19968;&#20010;&#30495;&#23454;&#26426;&#21046;&#65292;&#26681;&#25454;&#33258;&#24049;&#30340;&#30495;&#23454;&#20272;&#20540;&#36827;&#34892;&#25237;&#26631;&#26159;&#19968;&#20010;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#31574;&#30053;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#35777;&#20998;&#26512;...
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) permits different parties to collaboratively train a global model without disclosing their respective local labels. A crucial step of FL, that of aggregating local models to produce the global one, shares many similarities with public decision-making, and elections in particular. In that context, a major weakness of FL, namely its vulnerability to poisoning attacks, can be interpreted as a consequence of the one person one vote (henceforth 1p1v) principle underpinning most contemporary aggregation rules. In this paper, we propose FedQV, a novel aggregation algorithm built upon the quadratic voting scheme, recently proposed as a better alternative to 1p1v-based elections. Our theoretical analysis establishes that FedQV is a truthful mechanism in which bidding according to one's true valuation is a dominant strategy that achieves a convergence rate that matches those of state-of-the-art methods. Furthermore, our empirical analysis using multiple real-world dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#24494;&#20998;&#30340;SAR&#28210;&#26579;&#22120;&#36827;&#34892;SAR&#35270;&#35282;&#21453;&#28436;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#35282;&#24230;&#39044;&#27979;&#36807;&#31243;&#65292;&#24182;&#22312;&#23454;&#26102;&#29983;&#25104;&#20219;&#24847;&#35270;&#35282;&#30340;SAR&#22270;&#20687;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#25233;&#21046;&#20102;&#22797;&#26434;&#30340;&#32972;&#26223;&#24178;&#25200;&#65292;&#22686;&#24378;&#20102;&#23545;&#26102;&#38388;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01165</link><description>&lt;p&gt;
&#29992;&#21487;&#24494;&#20998;&#30340;SAR&#28210;&#26579;&#22120;&#36827;&#34892;SAR&#35270;&#35282;&#21453;&#28436;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for SAR View Angle Inversion with Differentiable SAR Renderer. (arXiv:2401.01165v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#24494;&#20998;&#30340;SAR&#28210;&#26579;&#22120;&#36827;&#34892;SAR&#35270;&#35282;&#21453;&#28436;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#30340;&#35282;&#24230;&#39044;&#27979;&#36807;&#31243;&#65292;&#24182;&#22312;&#23454;&#26102;&#29983;&#25104;&#20219;&#24847;&#35270;&#35282;&#30340;SAR&#22270;&#20687;&#30340;&#21516;&#26102;&#65292;&#26377;&#25928;&#25233;&#21046;&#20102;&#22797;&#26434;&#30340;&#32972;&#26223;&#24178;&#25200;&#65292;&#22686;&#24378;&#20102;&#23545;&#26102;&#38388;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#30913;&#21453;&#38382;&#39064;&#19968;&#30452;&#26159;&#19968;&#20010;&#30740;&#31350;&#28909;&#28857;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#32473;&#23450;&#19968;&#20010;&#30446;&#26631;&#27169;&#22411;&#65292;&#21453;&#36716;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#22270;&#20687;&#20013;&#30340;&#38647;&#36798;&#35270;&#35282;&#12290;&#28982;&#32780;&#65292;SAR&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#20197;&#21450;&#22797;&#26434;&#30340;&#32972;&#26223;&#24178;&#25200;&#21644;&#25104;&#20687;&#26426;&#21046;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#23884;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#21487;&#24494;&#20998;SAR&#28210;&#26579;&#22120;&#65288;DSR&#65289;&#30340;&#30005;&#30913;&#27169;&#25311;&#22120;&#65292;&#20197;&#20419;&#36827;&#20195;&#29702;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#27169;&#25311;&#20154;&#31867;&#30340;&#35282;&#24230;&#39044;&#27979;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DSR&#33021;&#22815;&#23454;&#26102;&#29983;&#25104;&#20219;&#24847;&#35270;&#35282;&#30340;SAR&#22270;&#20687;&#12290;&#24182;&#19988;&#65292;&#21033;&#29992;&#35270;&#35282;&#23545;&#24212;&#22270;&#20687;&#20043;&#38388;&#30340;&#39034;&#24207;&#21644;&#35821;&#20041;&#26041;&#38754;&#30340;&#24046;&#24322;&#26500;&#36896;&#20102;DRL&#20013;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#26377;&#25928;&#25233;&#21046;&#20102;&#22797;&#26434;&#30340;&#32972;&#26223;&#24178;&#25200;&#65292;&#22686;&#24378;&#20102;&#23545;&#26102;&#38388;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The electromagnetic inverse problem has long been a research hotspot. This study aims to reverse radar view angles in synthetic aperture radar (SAR) images given a target model. Nonetheless, the scarcity of SAR data, combined with the intricate background interference and imaging mechanisms, limit the applications of existing learning-based approaches. To address these challenges, we propose an interactive deep reinforcement learning (DRL) framework, where an electromagnetic simulator named differentiable SAR render (DSR) is embedded to facilitate the interaction between the agent and the environment, simulating a human-like process of angle prediction. Specifically, DSR generates SAR images at arbitrary view angles in real-time. And the differences in sequential and semantic aspects between the view angle-corresponding images are leveraged to construct the state space in DRL, which effectively suppress the complex background interference, enhance the sensitivity to temporal variations
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#36827;&#34892;MRI&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#65292;&#26080;&#38656;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#26356;&#21487;&#35299;&#37322;&#21644;&#31283;&#23450;&#30340;&#20998;&#21106;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.01160</link><description>&lt;p&gt;
&#26080;&#38656;&#35757;&#32451;&#30340;MRI&#31435;&#26041;&#25345;&#32493;&#21516;&#35843;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Train-Free Segmentation in MRI with Cubical Persistent Homology. (arXiv:2401.01160v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01160
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#36827;&#34892;MRI&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#65292;&#26080;&#38656;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#26356;&#21487;&#35299;&#37322;&#21644;&#31283;&#23450;&#30340;&#20998;&#21106;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;MRI&#25195;&#25551;&#20998;&#21106;&#26041;&#27861;&#65292;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#23427;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65292;&#39318;&#20808;&#36890;&#36807;&#33258;&#21160;&#38408;&#20540;&#30830;&#23450;&#35201;&#20998;&#21106;&#30340;&#25972;&#20010;&#23545;&#35937;&#65292;&#28982;&#21518;&#26816;&#27979;&#19968;&#20010;&#24050;&#30693;&#25299;&#25169;&#32467;&#26500;&#30340;&#29420;&#29305;&#23376;&#38598;&#65292;&#26368;&#21518;&#25512;&#23548;&#20986;&#20998;&#21106;&#30340;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#34429;&#28982;&#35843;&#29992;&#20102;TDA&#30340;&#32463;&#20856;&#24605;&#24819;&#65292;&#20294;&#36825;&#26679;&#30340;&#31639;&#27861;&#20174;&#26410;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20998;&#31163;&#25552;&#20986;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38500;&#20102;&#32771;&#34385;&#22270;&#20687;&#30340;&#21516;&#35843;&#24615;&#22806;&#65292;&#36824;&#32771;&#34385;&#20102;&#20195;&#34920;&#24615;&#21608;&#26399;&#30340;&#23450;&#20301;&#65292;&#36825;&#26159;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20284;&#20046;&#20174;&#26410;&#34987;&#21033;&#29992;&#36807;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#25552;&#20379;&#20102;&#26080;&#38656;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#21106;&#30340;&#33021;&#21147;&#12290;TDA&#36824;&#36890;&#36807;&#23558;&#25299;&#25169;&#29305;&#24449;&#26126;&#30830;&#26144;&#23556;&#21040;&#20998;&#21106;&#32452;&#20214;&#26469;&#25552;&#20379;&#26356;&#21487;&#35299;&#37322;&#21644;&#31283;&#23450;&#30340;&#20998;&#21106;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe a new general method for segmentation in MRI scans using Topological Data Analysis (TDA), offering several advantages over traditional machine learning approaches. It works in three steps, first identifying the whole object to segment via automatic thresholding, then detecting a distinctive subset whose topology is known in advance, and finally deducing the various components of the segmentation. Although convoking classical ideas of TDA, such an algorithm has never been proposed separately from deep learning methods. To achieve this, our approach takes into account, in addition to the homology of the image, the localization of representative cycles, a piece of information that seems never to have been exploited in this context. In particular, it offers the ability to perform segmentation without the need for large annotated data sets. TDA also provides a more interpretable and stable framework for segmentation by explicitly mapping topological features to segmentation comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CSI&#26080;&#20851;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#25554;&#20837;&#21644;&#21024;&#38500;&#36890;&#36947;&#19978;&#26816;&#27979;&#26631;&#35760;&#30721;&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#23436;&#32654;CSI&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#24182;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#23384;&#20648;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.01155</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25554;&#20837;&#21024;&#38500;&#36890;&#36947;&#19978;&#26631;&#35760;&#30721;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Detection for Marker Codes over Insertion and Deletion Channels. (arXiv:2401.01155v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CSI&#26080;&#20851;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#25554;&#20837;&#21644;&#21024;&#38500;&#36890;&#36947;&#19978;&#26816;&#27979;&#26631;&#35760;&#30721;&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#23436;&#32654;CSI&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#24182;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#23384;&#20648;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#35760;&#30721;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#32534;&#30721;&#26041;&#26696;&#65292;&#21487;&#20197;&#20445;&#25252;&#25968;&#25454;&#20813;&#21463;&#25554;&#20837;&#21644;&#21024;&#38500;&#25805;&#20316;&#30340;&#24433;&#21709;&#12290;&#23427;&#22312;&#26410;&#26469;&#30340;&#23384;&#20648;&#31995;&#32479;&#65292;&#22914;DNA&#23384;&#20648;&#21644;&#36187;&#36947;&#35760;&#24518;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#12290;&#22312;&#35299;&#30721;&#26631;&#35760;&#30721;&#26102;&#65292;&#38656;&#35201;&#23436;&#32654;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#65292;&#21363;&#25554;&#20837;&#21644;&#21024;&#38500;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#26469;&#26816;&#27979;&#25554;&#20837;&#21644;&#21024;&#38500;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#20505;&#24456;&#38590;&#33719;&#24471;&#23436;&#32654;&#30340;CSI&#25110;&#20934;&#30830;&#30340;&#20449;&#36947;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#19968;&#31181;&#19981;&#38656;&#35201;&#23436;&#32654;CSI&#30693;&#35782;&#30340;&#26631;&#35760;&#30721;&#26816;&#27979;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CSI&#26080;&#20851;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#26631;&#35760;&#30721;&#26816;&#27979;&#12290;&#31532;&#19968;&#31181;&#26159;&#19968;&#31181;&#27169;&#22411;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#28145;&#24230;&#23637;&#24320;&#20102;&#21407;&#22987;&#30340;&#36845;&#20195;&#26816;&#27979;&#31639;&#27861;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;CSI&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#21487;&#20197;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#24471;&#21040;&#12290;&#31532;&#20108;&#31181;&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#31471;&#23545;&#31471;&#31995;&#32479;&#65292;&#22522;&#20110;&#28145;&#24230;&#21452;&#21521;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Marker code is an effective coding scheme to protect data from insertions and deletions. It has potential applications in future storage systems, such as DNA storage and racetrack memory. When decoding marker codes, perfect channel state information (CSI), i.e., insertion and deletion probabilities, are required to detect insertion and deletion errors. Sometimes, the perfect CSI is not easy to obtain or the accurate channel model is unknown. Therefore, it is deserved to develop detecting algorithms for marker code without the knowledge of perfect CSI. In this paper, we propose two CSI-agnostic detecting algorithms for marker code based on deep learning. The first one is a model-driven deep learning method, which deep unfolds the original iterative detecting algorithm of marker code. In this method, CSI become weights in neural networks and these weights can be learned from training data. The second one is a data-driven method which is an end-to-end system based on the deep bidirectiona
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#33258;&#30001;&#21442;&#25968;&#35299;&#20915;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#20551;&#35774;&#20135;&#29983;&#20102;&#26032;&#30340;&#24191;&#20041;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.01148</link><description>&lt;p&gt;
&#26080;&#30028;&#25439;&#22833;&#30340;PAC-Bayes-Chernoff&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayes-Chernoff bounds for unbounded losses. (arXiv:2401.01148v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#33258;&#30001;&#21442;&#25968;&#35299;&#20915;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#20551;&#35774;&#20135;&#29983;&#20102;&#26032;&#30340;&#24191;&#20041;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#26080;&#30028;&#25439;&#22833;&#30340;&#39640;&#27010;&#29575;PAC-Bayes&#21442;&#32771;&#30028;&#38480;&#12290;&#36825;&#20010;&#32467;&#26524;&#21487;&#20197;&#29702;&#35299;&#20026;Chernoff&#30028;&#38480;&#30340;PAC-Bayes&#29256;&#26412;&#12290;&#35777;&#26126;&#25216;&#24039;&#20381;&#36182;&#20110;&#36890;&#36807;Cram&#233;r&#21464;&#25442;&#23545;&#25439;&#22833;&#36827;&#34892;&#32479;&#19968;&#36793;&#30028;&#30340;&#23614;&#37096;&#38543;&#26426;&#21464;&#37327;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#20027;&#35201;&#32467;&#26524;&#30340;&#20004;&#20010;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#35299;&#20915;&#20102;&#35768;&#22810;PAC-Bayes&#30028;&#38480;&#19978;&#30340;&#33258;&#30001;&#21442;&#25968;&#20248;&#21270;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#25439;&#22833;&#20989;&#25968;&#19978;&#36827;&#34892;&#28789;&#27963;&#30340;&#20551;&#35774;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#24191;&#20041;&#20102;&#20043;&#21069;&#30340;&#30028;&#38480;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#26469;&#33719;&#24471;&#31867;&#20284;Gibbs&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new high-probability PAC-Bayes oracle bound for unbounded losses. This result can be understood as a PAC-Bayes version of the Chernoff bound. The proof technique relies on uniformly bounding the tail of certain random variable based on the Cram\'er transform of the loss. We highlight two applications of our main result. First, we show that our bound solves the open problem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we show that our approach allows working with flexible assumptions on the loss function, resulting in novel bounds that generalize previous ones and can be minimized to obtain Gibbs-like posteriors.
&lt;/p&gt;</description></item><item><title>HAAQI-Net&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#29992;&#25143;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;BLSTM&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;BEATs&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#33021;&#22815;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#39044;&#27979;&#38899;&#20048;&#30340;HAAQI&#24471;&#20998;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.01145</link><description>&lt;p&gt;
HAAQI-Net: &#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HAAQI-Net: A non-intrusive neural music quality assessment model for hearing aids. (arXiv:2401.01145v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01145
&lt;/p&gt;
&lt;p&gt;
HAAQI-Net&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#29992;&#25143;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;BLSTM&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;BEATs&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#33021;&#22815;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#39044;&#27979;&#38899;&#20048;&#30340;HAAQI&#24471;&#20998;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HAAQI-Net&#65292;&#19968;&#31181;&#38024;&#23545;&#21161;&#21548;&#22120;&#29992;&#25143;&#23450;&#21046;&#30340;&#38750;&#20405;&#20837;&#24615;&#28145;&#24230;&#23398;&#20064;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#22914;Hearing Aid Audio Quality Index (HAAQI) &#19981;&#21516;&#65292;HAAQI-Net&#37319;&#29992;&#20102;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(BLSTM)&#12290;&#35813;&#27169;&#22411;&#20197;&#35780;&#20272;&#30340;&#38899;&#20048;&#26679;&#26412;&#21644;&#21548;&#21147;&#25439;&#22833;&#27169;&#24335;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#39044;&#27979;&#30340;HAAQI&#24471;&#20998;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#26469;&#33258;&#38899;&#39057;&#21464;&#25442;&#22120;(BEATs)&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#12290;&#36890;&#36807;&#23558;&#39044;&#27979;&#20998;&#25968;&#19982;&#30495;&#23454;&#20998;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;HAAQI-Net&#36798;&#21040;&#20102;0.9257&#30340;&#38271;&#26399;&#19968;&#33268;&#24615;&#30456;&#20851;(LCC)&#65292;0.9394&#30340;&#26031;&#30382;&#23572;&#26364;&#31561;&#32423;&#30456;&#20851;&#31995;&#25968;(SRCC)&#65292;&#21644;0.0080&#30340;&#22343;&#26041;&#35823;&#24046;(MSE)&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#39640;&#24615;&#33021;&#20276;&#38543;&#30528;&#25512;&#29702;&#26102;&#38388;&#30340;&#22823;&#24133;&#20943;&#23569;&#65306;&#20174;62.52&#31186;(HAAQI)&#20943;&#23569;&#21040;2.71&#31186;(HAAQI-Net)&#65292;&#20026;&#21161;&#21548;&#22120;&#29992;&#25143;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces HAAQI-Net, a non-intrusive deep learning model for music quality assessment tailored to hearing aid users. In contrast to traditional methods like the Hearing Aid Audio Quality Index (HAAQI), HAAQI-Net utilizes a Bidirectional Long Short-Term Memory (BLSTM) with attention. It takes an assessed music sample and a hearing loss pattern as input, generating a predicted HAAQI score. The model employs the pre-trained Bidirectional Encoder representation from Audio Transformers (BEATs) for acoustic feature extraction. Comparing predicted scores with ground truth, HAAQI-Net achieves a Longitudinal Concordance Correlation (LCC) of 0.9257, Spearman's Rank Correlation Coefficient (SRCC) of 0.9394, and Mean Squared Error (MSE) of 0.0080. Notably, this high performance comes with a substantial reduction in inference time: from 62.52 seconds (by HAAQI) to 2.71 seconds (by HAAQI-Net), serving as an efficient music quality assessment model for hearing aid users.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22312;&#32447;&#36873;&#25321;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;TreeSHAP&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#30340;&#19987;&#38376;&#21270;&#65292;&#20197;&#24212;&#23545;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#23454;&#38469;&#20915;&#31574;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01124</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#33258;&#36866;&#24212;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#36873;&#25321;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Adaptive Tree-based Model Selection for Time Series Forecasting. (arXiv:2401.01124v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01124
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22312;&#32447;&#36873;&#25321;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;TreeSHAP&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#30340;&#19987;&#38376;&#21270;&#65292;&#20197;&#24212;&#23545;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#22312;&#23454;&#38469;&#20915;&#31574;&#20013;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#30001;&#20110;&#20854;&#30456;&#23545;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#23427;&#20204;&#22312;&#38656;&#27714;&#21644;&#24191;&#27867;&#35748;&#21487;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27169;&#22411;&#37117;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22312;&#32447;&#39044;&#27979;&#35774;&#32622;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#35266;&#27979;&#19981;&#26029;&#22686;&#21152;&#65292;&#32780;&#23427;&#20204;&#25152;&#32472;&#21046;&#30340;&#20998;&#24067;&#21487;&#33021;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;TreeSHAP&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#32447;&#36873;&#25321;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#26469;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#20174;&#20219;&#24847;&#19968;&#32452;&#19981;&#21516;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20010;&#22522;&#20110;&#24615;&#33021;&#30340;&#25490;&#21517;&#65292;&#24182;&#20855;&#26377;&#19968;&#33268;&#30340;&#35774;&#35745;&#65292;&#20351;&#24471;TreeSHAP&#33021;&#22815;&#26681;&#25454;&#36755;&#20837;&#26102;&#38388;&#24207;&#21015;&#30340;&#19981;&#21516;&#21306;&#22495;&#26469;&#19987;&#38376;&#21270;&#22522;&#20110;&#26641;&#30340;&#39044;&#27979;&#22120;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#36866;&#24403;&#30340;&#27169;&#24335;...
&lt;/p&gt;
&lt;p&gt;
Tree-based models have been successfully applied to a wide variety of tasks, including time series forecasting. They are increasingly in demand and widely accepted because of their comparatively high level of interpretability. However, many of them suffer from the overfitting problem, which limits their application in real-world decision-making. This problem becomes even more severe in online-forecasting settings where time series observations are incrementally acquired, and the distributions from which they are drawn may keep changing over time. In this context, we propose a novel method for the online selection of tree-based models using the TreeSHAP explainability method in the task of time series forecasting. We start with an arbitrary set of different tree-based models. Then, we outline a performance-based ranking with a coherent design to make TreeSHAP able to specialize the tree-based forecasters across different regions in the input time series. In this framework, adequate mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;CVGAN&#27169;&#22411;&#29983;&#25104;&#28378;&#21160;&#36724;&#25215;&#25391;&#21160;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#21382;&#21490;&#25391;&#21160;&#25968;&#25454;&#21644;&#21097;&#20313;&#23551;&#21629;&#26465;&#20214;&#29983;&#25104;&#19968;&#32500;&#25391;&#21160;&#20449;&#21495;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#26469;&#25351;&#23548;&#20449;&#21495;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CVGAN&#27169;&#22411;&#22312;MMD&#21644;FID&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#39640;&#32423;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01119</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#22238;&#24402;&#32593;&#32476;&#36827;&#34892;&#28378;&#21160;&#36724;&#25215;&#20840;&#29983;&#21629;&#21608;&#26399;&#25968;&#25454;&#29983;&#25104;&#29992;&#20110;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Utilizing Autoregressive Networks for Full Lifecycle Data Generation of Rolling Bearings for RUL Prediction. (arXiv:2401.01119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;CVGAN&#27169;&#22411;&#29983;&#25104;&#28378;&#21160;&#36724;&#25215;&#25391;&#21160;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#21382;&#21490;&#25391;&#21160;&#25968;&#25454;&#21644;&#21097;&#20313;&#23551;&#21629;&#26465;&#20214;&#29983;&#25104;&#19968;&#32500;&#25391;&#21160;&#20449;&#21495;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#26469;&#25351;&#23548;&#20449;&#21495;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CVGAN&#27169;&#22411;&#22312;MMD&#21644;FID&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#39640;&#32423;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#29983;&#20135;&#20013;&#65292;&#28378;&#21160;&#36724;&#25215;&#23551;&#21629;&#30340;&#39044;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#39640;&#36136;&#37327;&#30340;&#28378;&#21160;&#36724;&#25215;&#20840;&#29983;&#21629;&#21608;&#26399;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#19968;&#30452;&#26159;&#31934;&#30830;&#39044;&#27979;&#30340;&#20027;&#35201;&#21046;&#32422;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;CVGAN&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#27700;&#24179;&#21644;&#22402;&#30452;&#26041;&#21521;&#19978;&#30340;&#19968;&#32500;&#25391;&#21160;&#20449;&#21495;&#30340;&#26032;&#26694;&#26550;&#65292;&#20854;&#21463;&#21382;&#21490;&#25391;&#21160;&#25968;&#25454;&#21644;&#21097;&#20313;&#23551;&#21629;&#30340;&#26465;&#20214;&#32422;&#26463;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#36845;&#20195;&#22320;&#21033;&#29992;&#20043;&#21069;&#29983;&#25104;&#30340;&#25391;&#21160;&#20449;&#24687;&#26469;&#25351;&#23548;&#24403;&#21069;&#20449;&#21495;&#30340;&#29983;&#25104;&#12290;&#36890;&#36807;&#22312;PHM 2012&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;CVGAN&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;CVGAN&#27169;&#22411;&#22312;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#27169;&#24335;&#19979;&#65292;&#22312;MMD&#21644;FID&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#35768;&#22810;&#39640;&#32423;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35757;&#32451;&#20351;&#29992;&#20102;&#30001;CVGAN&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#29983;&#21629;&#21608;&#26399;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prediction of rolling bearing lifespan is of significant importance in industrial production. However, the scarcity of high-quality, full lifecycle data has been a major constraint in achieving precise predictions. To address this challenge, this paper introduces the CVGAN model, a novel framework capable of generating one-dimensional vibration signals in both horizontal and vertical directions, conditioned on historical vibration data and remaining useful life. In addition, we propose an autoregressive generation method that can iteratively utilize previously generated vibration information to guide the generation of current signals. The effectiveness of the CVGAN model is validated through experiments conducted on the PHM 2012 dataset. Our findings demonstrate that the CVGAN model, in terms of both MMD and FID metrics, outperforms many advanced methods in both autoregressive and non-autoregressive generation modes. Notably, training using the full lifecycle data generated by the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22343;&#21248;&#22320;&#26631;&#25277;&#26679;&#21644;&#32422;&#26463;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20840;&#23616;&#32467;&#26500;&#22833;&#30495;&#21644;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01100</link><description>&lt;p&gt;
&#36890;&#36807;&#22343;&#21248;&#22320;&#26631;&#25277;&#26679;&#21644;&#32422;&#26463;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#23454;&#29616;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable manifold learning by uniform landmark sampling and constrained locally linear embedding. (arXiv:2401.01100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01100
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22343;&#21248;&#22320;&#26631;&#25277;&#26679;&#21644;&#32422;&#26463;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20840;&#23616;&#32467;&#26500;&#22833;&#30495;&#21644;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#26088;&#22312;&#25581;&#31034;&#39640;&#32500;&#31354;&#38388;&#20013;&#22797;&#26434;&#38750;&#32447;&#24615;&#27969;&#24418;&#20869;&#22312;&#30340;&#20302;&#32500;&#32467;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;&#27969;&#24418;&#20551;&#35774;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#26469;&#20419;&#36827;&#21487;&#35270;&#21270;&#12289;&#20998;&#31867;&#12289;&#32858;&#31867;&#21644;&#33719;&#24471;&#20851;&#38190;&#27934;&#23519;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#20840;&#23616;&#32467;&#26500;&#20013;&#30340;&#22823;&#37327;&#22833;&#30495;&#38382;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#23545;&#24213;&#23618;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#20063;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;(scML)&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#26377;&#25928;&#30340;&#26041;&#24335;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#12290;&#23427;&#36890;&#36807;&#23547;&#25214;&#19968;&#32452;&#22320;&#26631;&#26469;&#26500;&#24314;&#25972;&#20010;&#25968;&#25454;&#30340;&#20302;&#32500;&#39592;&#26550;&#65292;&#28982;&#21518;&#23558;&#38750;&#22320;&#26631;&#24341;&#20837;&#22320;&#26631;&#31354;&#38388;&#20013;
&lt;/p&gt;
&lt;p&gt;
As a pivotal approach in machine learning and data science, manifold learning aims to uncover the intrinsic low-dimensional structure within complex nonlinear manifolds in high-dimensional space. By exploiting the manifold hypothesis, various techniques for nonlinear dimension reduction have been developed to facilitate visualization, classification, clustering, and gaining key insights. Although existing manifold learning methods have achieved remarkable successes, they still suffer from extensive distortions incurred in the global structure, which hinders the understanding of underlying patterns. Scalability issues also limit their applicability for handling large-scale data. Here, we propose a scalable manifold learning (scML) method that can manipulate large-scale and high-dimensional data in an efficient manner. It starts by seeking a set of landmarks to construct the low-dimensional skeleton of the entire data and then incorporates the non-landmarks into the landmark space based 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32452;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21644;&#32452;&#36845;&#20195;&#24182;&#34892;&#35299;&#30721;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#65292;&#24182;&#25104;&#21151;&#25429;&#25417;&#25552;&#31034;&#35821;&#38899;&#30340;&#35828;&#35805;&#20154;&#39118;&#26684;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01099</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#26041;&#27861;&#65306;&#20351;&#29992;&#32452;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Parallel Audio Generation using Group Masked Language Modeling. (arXiv:2401.01099v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01099
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32452;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21644;&#32452;&#36845;&#20195;&#24182;&#34892;&#35299;&#30721;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#65292;&#24182;&#25104;&#21151;&#25429;&#25417;&#25552;&#31034;&#35821;&#38899;&#30340;&#35828;&#35805;&#20154;&#39118;&#26684;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#19988;&#39640;&#36136;&#37327;&#30340;&#32534;&#35299;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#12290;&#34429;&#28982;SoundStorm&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#65292;&#30456;&#27604;&#33258;&#22238;&#24402;&#27169;&#22411;&#21152;&#36895;&#20102;&#25512;&#29702;&#36895;&#24230;&#65292;&#20294;&#30001;&#20110;&#36845;&#20195;&#37319;&#26679;&#65292;&#23427;&#20173;&#28982;&#21463;&#21040;&#25512;&#29702;&#36895;&#24230;&#24930;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;G-MLM&#65289;&#21644;&#32452;&#36845;&#20195;&#24182;&#34892;&#35299;&#30721;&#65288;G-IPD&#65289;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#24182;&#34892;&#35821;&#38899;&#29983;&#25104;&#12290;&#35757;&#32451;&#21644;&#37319;&#26679;&#26041;&#26696;&#37117;&#33021;&#22815;&#36890;&#36807;&#26377;&#25928;&#24314;&#27169;&#32452;&#38388;&#26465;&#20214;&#20381;&#36182;&#26469;&#22312;&#23569;&#25968;&#36845;&#20195;&#27425;&#25968;&#20869;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#38899;&#39057;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#20197;&#25429;&#25417;&#25552;&#31034;&#35821;&#38899;&#30340;&#35828;&#35805;&#20154;&#39118;&#26684;&#24182;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a fast and high-quality codec language model for parallel audio generation. While SoundStorm, a state-of-the-art parallel audio generation model, accelerates inference speed compared to autoregressive models, it still suffers from slow inference due to iterative sampling. To resolve this problem, we propose Group-Masked Language Modeling~(G-MLM) and Group Iterative Parallel Decoding~(G-IPD) for efficient parallel audio generation. Both the training and sampling schemes enable the model to synthesize high-quality audio with a small number of iterations by effectively modeling the group-wise conditional dependencies. In addition, our model employs a cross-attention-based architecture to capture the speaker style of the prompt voice and improves computational efficiency. Experimental results demonstrate that our proposed model outperforms the baselines in prompt-based audio generation.
&lt;/p&gt;</description></item><item><title>Imperio&#26159;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#24341;&#23548;&#30340;&#21518;&#38376;&#25915;&#20987;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#23454;&#29616;&#20219;&#24847;&#27169;&#22411;&#30340;&#25511;&#21046;&#65292;&#25193;&#23637;&#20102;NLP&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01085</link><description>&lt;p&gt;
Imperio: &#20351;&#29992;&#35821;&#35328;&#24341;&#23548;&#30340;&#21518;&#38376;&#25915;&#20987;&#23454;&#29616;&#20219;&#24847;&#27169;&#22411;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control. (arXiv:2401.01085v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01085
&lt;/p&gt;
&lt;p&gt;
Imperio&#26159;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#24341;&#23548;&#30340;&#21518;&#38376;&#25915;&#20987;&#24037;&#20855;&#65292;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#23454;&#29616;&#20219;&#24847;&#27169;&#22411;&#30340;&#25511;&#21046;&#65292;&#25193;&#23637;&#20102;NLP&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;transformer&#26550;&#26500;&#30340;&#38761;&#21629;&#19979;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21463;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;NLP&#27169;&#22411;&#30340;&#36827;&#23637;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#20854;&#21518;&#38376;&#28431;&#27934;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#36825;&#20123;&#36827;&#23637;&#21487;&#33021;&#24341;&#20837;&#26032;&#30340;&#21518;&#38376;&#23041;&#32961;&#36824;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Imperio&#65292;&#23427;&#21033;&#29992;NLP&#27169;&#22411;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26469;&#20016;&#23500;&#21518;&#38376;&#25915;&#20987;&#12290;Imperio&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#25511;&#21046;&#20307;&#39564;&#65292;&#20351;&#23545;&#25163;&#36890;&#36807;&#35821;&#35328;&#24341;&#23548;&#30340;&#25351;&#20196;&#21487;&#20197;&#20219;&#24847;&#25511;&#21046;&#21463;&#23475;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#39537;&#21160;&#26465;&#20214;&#35302;&#21457;&#29983;&#25104;&#22120;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20197;&#25193;&#23637;&#20854;&#23545;&#21518;&#38376;&#25351;&#20196;&#35299;&#37322;&#21644;&#25191;&#34892;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#12289;&#20116;&#31181;&#25915;&#20987;&#21644;&#20061;&#31181;&#38450;&#24481;&#30340;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;Imperio&#30340;&#26377;&#25928;&#24615;&#12290;&#23427;&#21487;&#20197;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#20135;&#29983;&#19978;&#19979;&#25991;&#36866;&#24212;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#25511;&#21046;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revolutionized by the transformer architecture, natural language processing (NLP) has received unprecedented attention. While advancements in NLP models have led to extensive research into their backdoor vulnerabilities, the potential for these advancements to introduce new backdoor threats remains unexplored. This paper proposes Imperio, which harnesses the language understanding capabilities of NLP models to enrich backdoor attacks. Imperio provides a new model control experience. It empowers the adversary to control the victim model with arbitrary output through language-guided instructions. This is achieved using a language model to fuel a conditional trigger generator, with optimizations designed to extend its language understanding capabilities to backdoor instruction interpretation and execution. Our experiments across three datasets, five attacks, and nine defenses confirm Imperio's effectiveness. It can produce contextually adaptive triggers from text descriptions and control 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21464;&#20307;NPG-HM&#65292;&#37319;&#29992;Hessian&#36741;&#21161;&#21160;&#37327;&#25216;&#26415;&#36827;&#34892;&#26041;&#24046;&#20943;&#23567;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35299;&#20915;&#23376;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;NPG-HM&#22312;&#36890;&#29992;Fisher&#38750;&#36864;&#21270;&#31574;&#30053;&#21442;&#25968;&#21270;&#19979;&#21487;&#20197;&#23454;&#29616;&#20840;&#23616;&#26368;&#21518;&#36845;&#20195;&#30340;$\epsilon$-&#26368;&#20248;&#24615;&#65292;&#24182;&#19988;&#22312;Mujoco&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01084</link><description>&lt;p&gt;
&#20855;&#26377;Hessian&#36741;&#21161;&#21160;&#37327;&#26041;&#24046;&#20943;&#23567;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#20840;&#23616;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Global Convergence of Natural Policy Gradient with Hessian-aided Momentum Variance Reduction. (arXiv:2401.01084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21464;&#20307;NPG-HM&#65292;&#37319;&#29992;Hessian&#36741;&#21161;&#21160;&#37327;&#25216;&#26415;&#36827;&#34892;&#26041;&#24046;&#20943;&#23567;&#65292;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35299;&#20915;&#23376;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;NPG-HM&#22312;&#36890;&#29992;Fisher&#38750;&#36864;&#21270;&#31574;&#30053;&#21442;&#25968;&#21270;&#19979;&#21487;&#20197;&#23454;&#29616;&#20840;&#23616;&#26368;&#21518;&#36845;&#20195;&#30340;$\epsilon$-&#26368;&#20248;&#24615;&#65292;&#24182;&#19988;&#22312;Mujoco&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#21450;&#20854;&#21464;&#20307;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31574;&#30053;&#25628;&#32034;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;NPG&#21464;&#20307;&#65292;&#21629;&#21517;&#20026;NPG-HM&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;Hessian&#36741;&#21161;&#21160;&#37327;&#25216;&#26415;&#36827;&#34892;&#26041;&#24046;&#20943;&#23567;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#23376;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;NPG-HM&#21487;&#20197;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{O}(\epsilon^{-2})$&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20840;&#23616;&#26368;&#21518;&#36845;&#20195;&#30340;$\epsilon$-&#26368;&#20248;&#24615;&#65292;&#36825;&#26159;&#22312;&#36890;&#29992;&#30340;Fisher&#38750;&#36864;&#21270;&#31574;&#30053;&#21442;&#25968;&#21270;&#19979;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#24050;&#30693;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;&#25910;&#25947;&#20998;&#26512;&#24314;&#31435;&#22312;&#38024;&#23545;NPG&#30340;&#26494;&#24347;&#24369;&#26799;&#24230;&#20248;&#21183;&#24615;&#36136;&#20197;&#21450;&#22788;&#29702;&#23376;&#38382;&#39064;&#26102;&#30340;&#38169;&#35823;&#20998;&#35299;&#30340;&#20860;&#23481;&#20989;&#25968;&#36924;&#36817;&#26694;&#26550;&#19979;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;Mujoco&#29615;&#22659;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;NPG-HM&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#31574;&#30053;&#26041;&#27861;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural policy gradient (NPG) and its variants are widely-used policy search methods in reinforcement learning. Inspired by prior work, a new NPG variant coined NPG-HM is developed in this paper, which utilizes the Hessian-aided momentum technique for variance reduction, while the sub-problem is solved via the stochastic gradient descent method. It is shown that NPG-HM can achieve the global last iterate $\epsilon$-optimality with a sample complexity of $\mathcal{O}(\epsilon^{-2})$, which is the best known result for natural policy gradient type methods under the generic Fisher non-degenerate policy parameterizations. The convergence analysis is built upon a relaxed weak gradient dominance property tailored for NPG under the compatible function approximation framework, as well as a neat way to decompose the error when handling the sub-problem. Moreover, numerical experiments on Mujoco-based environments demonstrate the superior performance of NPG-HM over other state-of-the-art policy g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#39134;&#26426;&#30340;&#38477;&#33853;&#26102;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;&#36712;&#36857;&#22270;&#20687;&#21644;&#39069;&#22806;&#30340;&#36755;&#20837;&#20449;&#24687;&#65292;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.01083</link><description>&lt;p&gt;
&#22522;&#20110;&#36712;&#36857;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#39134;&#26426;&#38477;&#33853;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Aircraft Landing Time Prediction with Deep Learning on Trajectory Images. (arXiv:2401.01083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#39134;&#26426;&#30340;&#38477;&#33853;&#26102;&#38388;&#12290;&#36890;&#36807;&#20351;&#29992;&#36712;&#36857;&#22270;&#20687;&#21644;&#39069;&#22806;&#30340;&#36755;&#20837;&#20449;&#24687;&#65292;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39134;&#26426;&#38477;&#33853;&#26102;&#38388;&#65288;ALT&#65289;&#30340;&#39044;&#27979;&#23545;&#20110;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36305;&#36947;&#19978;&#30340;&#21040;&#36798;&#39134;&#26426;&#25490;&#24207;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#36827;&#20837;&#30740;&#31350;&#39046;&#22495;&#30340;&#39134;&#26426;&#30340;ALT&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20020;&#26102;&#25429;&#33719;&#31383;&#21475;&#20869;&#30340;&#25152;&#26377;&#31354;&#20013;&#21040;&#36798;&#39134;&#26426;&#30340;&#36712;&#36857;&#34987;&#29992;&#26469;&#29983;&#25104;&#19968;&#24352;&#22270;&#20687;&#65292;&#30446;&#26631;&#39134;&#26426;&#30340;&#36712;&#36857;&#34987;&#26631;&#35760;&#20026;&#32418;&#33394;&#65292;&#32972;&#26223;&#39134;&#26426;&#30340;&#36712;&#36857;&#34987;&#26631;&#35760;&#20026;&#34013;&#33394;&#12290;&#36712;&#36857;&#22270;&#20687;&#21253;&#25324;&#39134;&#26426;&#30340;&#20301;&#32622;&#12289;&#36895;&#24230;&#12289;&#33322;&#21521;&#12289;&#30456;&#23545;&#36317;&#31163;&#21644;&#21040;&#36798;&#20132;&#36890;&#27969;&#37327;&#31561;&#21508;&#31181;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;ALT&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#23454;&#26102;&#36305;&#36947;&#20351;&#29992;&#24773;&#20917;&#12289;&#36712;&#36857;&#25968;&#25454;&#20197;&#21450;&#39134;&#26426;&#31867;&#22411;&#21644;&#22825;&#27668;&#26465;&#20214;&#31561;&#22806;&#37096;&#20449;&#24687;&#20316;&#20026;&#39069;&#22806;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aircraft landing time (ALT) prediction is crucial for air traffic management, especially for arrival aircraft sequencing on the runway. In this study, a trajectory image-based deep learning method is proposed to predict ALTs for the aircraft entering the research airspace that covers the Terminal Maneuvering Area (TMA). Specifically, the trajectories of all airborne arrival aircraft within the temporal capture window are used to generate an image with the target aircraft trajectory labeled as red and all background aircraft trajectory labeled as blue. The trajectory images contain various information, including the aircraft position, speed, heading, relative distances, and arrival traffic flows. It enables us to use state-of-the-art deep convolution neural networks for ALT modeling. We also use real-time runway usage obtained from the trajectory data and the external information such as aircraft types and weather conditions as additional inputs. Moreover, a convolution neural network (
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#20102;&#22312;&#32447;&#31639;&#27861;&#12290;&#24403;&#27169;&#22411;&#21442;&#25968;&#26469;&#33258;&#26410;&#30693;&#30340;&#38750;&#24179;&#31283;&#20998;&#24067;&#19988;&#32473;&#23450;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#26102;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2401.01077</link><description>&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#65306;&#20855;&#26377;&#65288;&#21644;&#19981;&#20855;&#26377;&#65289;&#39044;&#27979;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constrained Online Two-stage Stochastic Optimization: Algorithm with (and without) Predictions. (arXiv:2401.01077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01077
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#34385;&#20102;&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#20102;&#22312;&#32447;&#31639;&#27861;&#12290;&#24403;&#27169;&#22411;&#21442;&#25968;&#26469;&#33258;&#26410;&#30693;&#30340;&#38750;&#24179;&#31283;&#20998;&#24067;&#19988;&#32473;&#23450;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#26102;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#26377;&#38480;&#26102;&#38388;&#27573;&#30340;T&#20010;&#26399;&#38388;&#20869;&#20855;&#26377;&#38271;&#26399;&#32422;&#26463;&#30340;&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#27599;&#20010;&#26399;&#38388;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#21462;&#31532;&#19968;&#38454;&#27573;&#30340;&#34892;&#21160;&#65292;&#28982;&#21518;&#35266;&#23519;&#27169;&#22411;&#21442;&#25968;&#30340;&#23454;&#29616;&#65292;&#24182;&#22312;&#21487;&#34892;&#38598;&#19978;&#37319;&#21462;&#31532;&#20108;&#38454;&#27573;&#30340;&#34892;&#21160;&#65292;&#21487;&#34892;&#38598;&#21516;&#26102;&#20381;&#36182;&#20110;&#31532;&#19968;&#38454;&#27573;&#20915;&#31574;&#21644;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#32047;&#31215;&#30446;&#26631;&#20540;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#30340;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#25105;&#20204;&#20174;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20013;&#24320;&#21457;&#20102;&#22312;&#32447;&#31639;&#27861;&#26469;&#35299;&#20915;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31639;&#27861;&#30340;&#36951;&#25022;&#36793;&#30028;&#21487;&#20197;&#36716;&#21270;&#20026;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#36793;&#30028;&#12290;&#22522;&#20110;&#27492;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;&#24403;&#27169;&#22411;&#21442;&#25968;&#26469;&#33258;&#26410;&#30693;&#30340;&#38750;&#24179;&#31283;&#20998;&#24067;&#19988;&#32473;&#23450;&#20102;&#23545;&#20998;&#24067;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#26102;&#65292;&#25105;&#20204;&#20174;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#36793;&#30028;&#20026;O&#65288;WT+&#8730;T&#65289;&#65292;&#20854;&#20013;WT&#20026;...
&lt;/p&gt;
&lt;p&gt;
We consider an online two-stage stochastic optimization with long-term constraints over a finite horizon of $T$ periods. At each period, we take the first-stage action, observe a model parameter realization and then take the second-stage action from a feasible set that depends both on the first-stage decision and the model parameter. We aim to minimize the cumulative objective value while guaranteeing that the long-term average second-stage decision belongs to a set. We develop online algorithms for the online two-stage problem from adversarial learning algorithms. Also, the regret bound of our algorithm can be reduced to the regret bound of embedded adversarial learning algorithms. Based on this framework, we obtain new results under various settings. When the model parameters are drawn from unknown non-stationary distributions and we are given machine-learned predictions of the distributions, we develop a new algorithm from our framework with a regret $O(W_T+\sqrt{T})$, where $W_T$ m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TLDNN&#30340;&#28151;&#21512;&#28145;&#24230;&#26694;&#26550;&#65292;&#23558;Transformer&#21644;LSTM&#30340;&#32467;&#26500;&#32467;&#21512;&#65292;&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#21644;&#25429;&#25417;&#26102;&#22495;&#20381;&#36182;&#24615;&#30340;&#22686;&#24378;&#65292;&#20026;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#24102;&#26469;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.01056</link><description>&lt;p&gt;
&#36890;&#36807;&#40065;&#26834;&#30340;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#22686;&#24378;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Enhancing Automatic Modulation Recognition through Robust Global Feature Extraction. (arXiv:2401.01056v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01056
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TLDNN&#30340;&#28151;&#21512;&#28145;&#24230;&#26694;&#26550;&#65292;&#23558;Transformer&#21644;LSTM&#30340;&#32467;&#26500;&#32467;&#21512;&#65292;&#36890;&#36807;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#21644;&#25429;&#25417;&#26102;&#22495;&#20381;&#36182;&#24615;&#30340;&#22686;&#24378;&#65292;&#20026;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#24102;&#26469;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#33258;&#21160;&#35843;&#21046;&#35782;&#21035;&#31574;&#30053;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#35843;&#21046;&#20449;&#21495;&#23637;&#31034;&#20102;&#38271;&#26399;&#30340;&#26102;&#22495;&#20381;&#36182;&#24615;&#65292;&#25552;&#21462;&#20840;&#23616;&#29305;&#24449;&#23545;&#20110;&#35782;&#21035;&#35843;&#21046;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#24037;&#19987;&#23478;&#20998;&#26512;&#26143;&#24231;&#22270;&#20013;&#30340;&#27169;&#24335;&#26469;&#20998;&#31867;&#35843;&#21046;&#26041;&#26696;&#12290;&#30001;&#20110;&#26377;&#38480;&#30340;&#24863;&#21463;&#37326;&#65292;&#20256;&#32479;&#30340;&#21367;&#31215;&#32593;&#32476;&#25797;&#38271;&#25552;&#21462;&#23616;&#37096;&#29305;&#24449;&#65292;&#20294;&#38590;&#20197;&#25429;&#25417;&#20840;&#23616;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;TLDNN&#30340;&#26032;&#22411;&#28151;&#21512;&#28145;&#24230;&#26694;&#26550;&#65292;&#23427;&#23558;Transformer&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#32467;&#26500;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#21033;&#29992;Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#27169;&#25311;&#20449;&#21495;&#24207;&#21015;&#20013;&#30340;&#20840;&#23616;&#20851;&#32852;&#65292;&#21516;&#26102;&#21033;&#29992;LSTM&#22686;&#24378;&#26102;&#22495;&#20381;&#36182;&#24615;&#30340;&#25429;&#25417;&#12290;&#20026;&#20102;&#20943;&#36731;&#23556;&#39057;&#25351;&#32441;&#29305;&#24449;&#21644;&#20449;&#36947;&#29305;&#24615;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Modulation Recognition (AMR) plays a crucial role in wireless communication systems. Deep learning AMR strategies have achieved tremendous success in recent years. Modulated signals exhibit long temporal dependencies, and extracting global features is crucial in identifying modulation schemes. Traditionally, human experts analyze patterns in constellation diagrams to classify modulation schemes. Classical convolutional-based networks, due to their limited receptive fields, excel at extracting local features but struggle to capture global relationships. To address this limitation, we introduce a novel hybrid deep framework named TLDNN, which incorporates the architectures of the transformer and long short-term memory (LSTM). We utilize the self-attention mechanism of the transformer to model the global correlations in signal sequences while employing LSTM to enhance the capture of temporal dependencies. To mitigate the impact like RF fingerprint features and channel characteri
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#35770;&#25991;&#65292;&#20171;&#32461;&#20102;&#22312;&#21160;&#24577;&#22810;&#20219;&#21153;&#22330;&#26223;&#19979;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#24377;&#24615;&#22240;&#23376;&#65292;&#21487;&#20197;&#35299;&#20915;&#26799;&#24230;&#24046;&#24322;&#21644;&#36127;&#36801;&#31227;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01054</link><description>&lt;p&gt;
&#24377;&#24615;&#22810;&#26799;&#24230;&#19979;&#38477;&#29992;&#20110;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Elastic Multi-Gradient Descent for Parallel Continual Learning. (arXiv:2401.01054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01054
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#35770;&#25991;&#65292;&#20171;&#32461;&#20102;&#22312;&#21160;&#24577;&#22810;&#20219;&#21153;&#22330;&#26223;&#19979;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#24377;&#24615;&#22240;&#23376;&#65292;&#21487;&#20197;&#35299;&#20915;&#26799;&#24230;&#24046;&#24322;&#21644;&#36127;&#36801;&#31227;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#30446;&#26631;&#26159;&#20174;&#26032;&#30340;&#25968;&#25454;&#27969;&#20013;&#25345;&#32493;&#23398;&#20064;&#24182;&#23436;&#25104;&#30456;&#24212;&#30340;&#20219;&#21153;&#12290;&#36807;&#21435;&#30740;&#31350;&#30340;CL&#20551;&#35774;&#25968;&#25454;&#25353;&#20219;&#21153;&#30340;&#39034;&#24207;&#32473;&#20986;&#65292;&#22240;&#27492;&#23646;&#20110;&#20018;&#34892;&#36830;&#32493;&#23398;&#20064;&#65288;SCL&#65289;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;&#22810;&#20219;&#21153;&#22330;&#26223;&#19979;&#30340;&#26032;&#20852;&#33539;&#24335;&#8212;&#8212;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#65288;PCL&#65289;&#65292;&#20854;&#20013;&#22312;&#19981;&#21516;&#26102;&#38388;&#28857;&#36935;&#21040;&#20102;&#22810;&#26679;&#30340;&#20219;&#21153;&#12290;PCL&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#35757;&#32451;&#25968;&#37327;&#19981;&#30830;&#23450;&#19988;&#23398;&#20064;&#36827;&#24230;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#23548;&#33268;&#24456;&#38590;&#20445;&#35777;&#25152;&#26377;&#36935;&#21040;&#30340;&#20219;&#21153;&#37117;&#33021;&#24471;&#21040;&#26377;&#25928;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#20250;&#35758;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#27979;&#37327;&#21644;&#20943;&#23567;&#26799;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#28982;&#32780;&#65292;&#27599;&#27425;&#27169;&#22411;&#26356;&#26032;&#20173;&#28982;&#21487;&#33021;&#23384;&#22312;&#36127;&#36801;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#21160;&#24577;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24377;&#24615;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Continual Learning (CL) is to continuously learn from new data streams and accomplish the corresponding tasks. Previously studied CL assumes that data are given in sequence nose-to-tail for different tasks, thus indeed belonging to Serial Continual Learning (SCL). This paper studies the novel paradigm of Parallel Continual Learning (PCL) in dynamic multi-task scenarios, where a diverse set of tasks is encountered at different time points. PCL presents challenges due to the training of an unspecified number of tasks with varying learning progress, leading to the difficulty of guaranteeing effective model updates for all encountered tasks. In our previous conference work, we focused on measuring and reducing the discrepancy among gradients in a multi-objective optimization problem, which, however, may still contain negative transfers in every model update. To address this issue, in the dynamic multi-objective optimization problem, we introduce task-specific elastic factors to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Bayesian&#22810;&#35270;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#22270;&#24212;&#29992;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#22411;&#36317;&#31163;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.01048</link><description>&lt;p&gt;
PAC-Bayesian&#22810;&#35270;&#22270;&#23398;&#20064;&#39046;&#22495;&#33258;&#36866;&#24212;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Domain Adaptation Bounds for Multi-view learning. (arXiv:2401.01048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Bayesian&#22810;&#35270;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#30028;&#38480;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#35270;&#22270;&#24212;&#29992;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#22411;&#36317;&#31163;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#35270;&#22270;&#23398;&#20064;&#35774;&#32622;&#20013;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20851;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26032;&#32467;&#26524;&#12290;&#22312;&#20197;&#24448;&#30340;&#30740;&#31350;&#20013;&#65292;&#24456;&#23569;&#20851;&#27880;&#23558;&#22810;&#20010;&#35270;&#22270;&#32435;&#20837;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Pac-Bayesian&#29702;&#35770;&#30340;&#27867;&#21270;&#30028;&#38480;&#20998;&#26512;&#65292;&#20197;&#25972;&#21512;&#30446;&#21069;&#20998;&#24320;&#22788;&#29702;&#30340;&#20004;&#31181;&#33539;&#24335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26681;&#25454;Germain&#31561;&#20154;&#30340;&#20197;&#24448;&#30740;&#31350;&#65292;&#23558;&#20854;&#25552;&#20986;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#29992;&#20110;&#22810;&#35270;&#22270;&#23398;&#20064;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#35270;&#22270;&#39046;&#22495;&#33258;&#36866;&#24212;&#35774;&#32622;&#30340;&#26032;&#22411;&#36317;&#31163;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#29992;&#20110;&#20272;&#35745;&#24341;&#20837;&#30340;&#24046;&#24322;&#30340;Pac-Bayesian&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#19981;&#21516;&#30340;&#26032;&#30028;&#38480;&#19982;&#20197;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a series of new results for domain adaptation in the multi-view learning setting. The incorporation of multiple views in the domain adaptation was paid little attention in the previous studies. In this way, we propose an analysis of generalization bounds with Pac-Bayesian theory to consolidate the two paradigms, which are currently treated separately. Firstly, building on previous work by Germain et al., we adapt the distance between distribution proposed by Germain et al. for domain adaptation with the concept of multi-view learning. Thus, we introduce a novel distance that is tailored for the multi-view domain adaptation setting. Then, we give Pac-Bayesian bounds for estimating the introduced divergence. Finally, we compare the different new bounds with the previous studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;Tensor PCA&#27169;&#22411;&#20013;&#30340;&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#38480;&#21046;&#65292;&#24182;&#24314;&#31435;&#20102;&#20851;&#20110;&#25910;&#25947;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#21644;&#31639;&#27861;&#38408;&#20540;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20572;&#27490;&#20934;&#21017;&#26469;&#33719;&#24471;&#39640;&#24230;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.01047</link><description>&lt;p&gt;
Tensor PCA&#30340;&#21151;&#29575;&#36845;&#20195;&#30340;&#23574;&#38160;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sharp Analysis of Power Iteration for Tensor PCA. (arXiv:2401.01047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;Tensor PCA&#27169;&#22411;&#20013;&#30340;&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#38480;&#21046;&#65292;&#24182;&#24314;&#31435;&#20102;&#20851;&#20110;&#25910;&#25947;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#21644;&#31639;&#27861;&#38408;&#20540;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20572;&#27490;&#20934;&#21017;&#26469;&#33719;&#24471;&#39640;&#24230;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;Richard&#21644;Montanari&#65288;2014&#65289;&#24341;&#20837;&#30340;Tensor PCA&#27169;&#22411;&#30340;&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#12290;&#20043;&#21069;&#30740;&#31350;Tensor&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#30340;&#24037;&#20316;&#35201;&#20040;&#20165;&#38480;&#20110;&#22266;&#23450;&#27425;&#25968;&#30340;&#36845;&#20195;&#65292;&#35201;&#20040;&#38656;&#35201;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#19982;&#25968;&#25454;&#26080;&#20851;&#30340;&#21021;&#22987;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#23545;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;Tensor&#21151;&#29575;&#36845;&#20195;&#30340;&#21160;&#24577;&#36827;&#34892;&#20102;&#22810;&#39033;&#24335;&#25968;&#37327;&#32423;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20449;&#22122;&#27604;&#33539;&#22260;&#19979;&#65292;&#21151;&#29575;&#36845;&#20195;&#25910;&#25947;&#21040;&#31181;&#26893;&#20449;&#21495;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#23454;&#38469;&#30340;&#31639;&#27861;&#38408;&#20540;&#27604;&#25991;&#29486;&#20013;&#29468;&#27979;&#30340;&#35201;&#23567;&#19968;&#20010;polylog(n)&#30340;&#22240;&#23376;&#65292;&#20854;&#20013;n&#26159;&#29615;&#22659;&#32500;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21151;&#29575;&#36845;&#20195;&#20572;&#27490;&#20934;&#21017;&#65292;&#21487;&#20197;&#20445;&#35777;&#36755;&#20986;&#19982;&#30495;&#23454;&#20449;&#21495;&#39640;&#24230;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the power iteration algorithm for the tensor PCA model introduced in Richard and Montanari (2014). Previous work studying the properties of tensor power iteration is either limited to a constant number of iterations, or requires a non-trivial data-independent initialization. In this paper, we move beyond these limitations and analyze the dynamics of randomly initialized tensor power iteration up to polynomially many steps. Our contributions are threefold: First, we establish sharp bounds on the number of iterations required for power method to converge to the planted signal, for a broad range of the signal-to-noise ratios. Second, our analysis reveals that the actual algorithmic threshold for power iteration is smaller than the one conjectured in literature by a polylog(n) factor, where n is the ambient dimension. Finally, we propose a simple and effective stopping criterion for power iteration, which provably outputs a solution that is highly correlated with the true si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#35805;&#20013;&#30340;&#33258;&#26432;&#24847;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#23558;&#27492;&#27169;&#22411;&#38598;&#25104;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#25903;&#25345;&#31995;&#32479;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.01023</link><description>&lt;p&gt;
CautionSuicide: &#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#35805;&#20013;&#30340;&#33258;&#26432;&#24847;&#21521;
&lt;/p&gt;
&lt;p&gt;
CautionSuicide: A Deep Learning Based Approach for Detecting Suicidal Ideation in Real Time Chatbot Conversation. (arXiv:2401.01023v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#35805;&#20013;&#30340;&#33258;&#26432;&#24847;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#23558;&#27492;&#27169;&#22411;&#38598;&#25104;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#25903;&#25345;&#31995;&#32479;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26432;&#34987;&#35748;&#20026;&#26159;&#29616;&#20195;&#31038;&#20250;&#26368;&#20005;&#37325;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#33258;&#26432;&#36896;&#25104;&#20102;&#23545;&#22269;&#23478;&#12289;&#31038;&#21306;&#21644;&#23478;&#24237;&#30340;&#24754;&#21095;&#24615;&#24433;&#21709;&#12290;&#26377;&#35768;&#22810;&#22240;&#32032;&#23548;&#33268;&#33258;&#26432;&#24847;&#21521;&#12290;&#26089;&#26399;&#21457;&#29616;&#33258;&#26432;&#24847;&#21521;&#21487;&#20197;&#36890;&#36807;&#20026;&#21463;&#23475;&#32773;&#25552;&#20379;&#25152;&#38656;&#30340;&#19987;&#19994;&#25903;&#25345;&#26469;&#38450;&#27490;&#33258;&#26432;&#21457;&#29983;&#65292;&#23588;&#20854;&#26159;&#24403;&#21463;&#23475;&#32773;&#27809;&#26377;&#24847;&#35782;&#21040;&#33258;&#26432;&#24847;&#21521;&#30340;&#21361;&#38505;&#26102;&#12290;&#38543;&#30528;&#31185;&#25216;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#20154;&#20204;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#12289;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#25968;&#23383;&#24179;&#21488;&#22312;&#25968;&#23383;&#20869;&#23481;&#20013;&#20998;&#20139;&#21644;&#34920;&#36798;&#33258;&#24049;&#30340;&#24847;&#21521;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#31616;&#21333;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#25968;&#23383;&#20869;&#23481;&#20013;&#30340;&#33258;&#26432;&#24847;&#21521;&#65292;&#20027;&#35201;&#20851;&#27880;&#32842;&#22825;&#26426;&#22120;&#20154;&#20316;&#20026;&#20027;&#35201;&#25968;&#25454;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#25152;&#25552;&#20986;&#30340;&#33258;&#26432;&#26816;&#27979;&#38598;&#25104;&#21040;&#22522;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25903;&#25345;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Suicide is recognized as one of the most serious concerns in the modern society. Suicide causes tragedy that affects countries, communities, and families. There are many factors that lead to suicidal ideations. Early detection of suicidal ideations can help to prevent suicide occurrence by providing the victim with the required professional support, especially when the victim does not recognize the danger of having suicidal ideations. As technology usage has increased, people share and express their ideations digitally via social media, chatbots, and other digital platforms. In this paper, we proposed a novel, simple deep learning-based model to detect suicidal ideations in digital content, mainly focusing on chatbots as the primary data source. In addition, we provide a framework that employs the proposed suicide detection integration with a chatbot-based support system.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#30340;&#21019;&#26032;&#31867;&#21035;&#30456;&#20851;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#20840;&#38754;&#30340;&#31867;&#21035;&#30456;&#20851;&#23398;&#20064;&#26694;&#26550;&#65292; strategically harnessing interclass relationships within the OOD pipeline&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;OOD&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01021</link><description>&lt;p&gt;
&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#30340;&#31867;&#21035;&#30456;&#20851;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class Relevance Learning For Out-of-distribution Detection. (arXiv:2401.01021v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#30340;&#21019;&#26032;&#31867;&#21035;&#30456;&#20851;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#20840;&#38754;&#30340;&#31867;&#21035;&#30456;&#20851;&#23398;&#20064;&#26694;&#26550;&#65292; strategically harnessing interclass relationships within the OOD pipeline&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;OOD&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#37096;&#32626;&#27169;&#22411;&#26102;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26816;&#27979;&#26410;&#22312;&#20998;&#31867;&#22120;&#35757;&#32451;&#20013;&#32435;&#20837;&#30340;&#38476;&#29983;&#31867;&#21035;&#26102;&#20250;&#20986;&#29616;&#38382;&#39064;&#65292;&#36825;&#26159;&#23433;&#20840;&#21644;&#26377;&#25928;&#30340;&#29616;&#23454;&#19990;&#30028;&#27169;&#22411;&#37096;&#32626;&#20013;&#30340;&#19968;&#22823;&#38556;&#30861;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#25216;&#26415;&#65292;&#22914;max logits&#65292;&#26088;&#22312;&#21033;&#29992;logits&#36827;&#34892;OOD&#35782;&#21035;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#24573;&#35270;&#20102;&#26377;&#25928;&#26816;&#27979;&#30340;&#22797;&#26434;&#31867;&#21035;&#38388;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;OOD&#26816;&#27979;&#37327;&#36523;&#23450;&#21046;&#30340;&#21019;&#26032;&#31867;&#21035;&#30456;&#20851;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#31867;&#21035;&#30456;&#20851;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;OOD&#31649;&#36947;&#20869;&#25112;&#30053;&#22320;&#21033;&#29992;&#31867;&#38388;&#20851;&#31995;&#12290;&#36825;&#20010;&#26694;&#26550;&#26174;&#33879;&#22686;&#24378;&#20102;OOD&#26816;&#27979;&#33021;&#21147;&#12290;&#22312;&#21253;&#25324;&#36890;&#29992;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#65288;Near OOD&#21644;Far OOD&#65289;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image classification plays a pivotal role across diverse applications, yet challenges persist when models are deployed in real-world scenarios. Notably, these models falter in detecting unfamiliar classes that were not incorporated during classifier training, a formidable hurdle for safe and effective real-world model deployment, commonly known as out-of-distribution (OOD) detection. While existing techniques, like max logits, aim to leverage logits for OOD identification, they often disregard the intricate interclass relationships that underlie effective detection. This paper presents an innovative class relevance learning method tailored for OOD detection. Our method establishes a comprehensive class relevance learning framework, strategically harnessing interclass relationships within the OOD pipeline. This framework significantly augments OOD detection capabilities. Extensive experimentation on diverse datasets, encompassing generic image classification datasets (Near OOD and Far O
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#39640;&#20102;Transformer&#22312;PPG&#20449;&#21495;&#20266;&#36857;&#26816;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#21147;&#65292;&#24182;&#21457;&#29616;&#23545;&#27604;&#23398;&#20064;&#26159;&#26368;&#31283;&#23450;&#19988;&#34920;&#29616;&#26368;&#20248;&#30340;SSL&#25216;&#26415;&#12290;&#36827;&#19968;&#27493;&#20248;&#21270;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23545;&#20110;&#23545;&#27604;SSL&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.01013</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;PPG&#20449;&#21495;&#20266;&#36857;&#26816;&#27979;&#20013;&#25552;&#39640;Transformer&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning. (arXiv:2401.01013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#39640;&#20102;Transformer&#22312;PPG&#20449;&#21495;&#20266;&#36857;&#26816;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#21147;&#65292;&#24182;&#21457;&#29616;&#23545;&#27604;&#23398;&#20064;&#26159;&#26368;&#31283;&#23450;&#19988;&#34920;&#29616;&#26368;&#20248;&#30340;SSL&#25216;&#26415;&#12290;&#36827;&#19968;&#27493;&#20248;&#21270;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23545;&#20110;&#23545;&#27604;SSL&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22307;&#26480;&#26031;&#22374;&#21307;&#23398;&#38498;&#30340;&#20799;&#31185;&#37325;&#30151;&#30417;&#25252;&#23460;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#22914;&#21322;&#30417;&#30563;&#26631;&#31614;&#20256;&#25773;&#21644;K&#26368;&#36817;&#37051;&#65292;&#22312;&#20174;PPG&#20449;&#21495;&#20013;&#26816;&#27979;&#21040;&#20266;&#36857;&#26041;&#38754;&#27604;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#29305;&#24449;&#65292;&#28982;&#21518;&#23545;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35299;&#20915;&#23545;&#20016;&#23500;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20302;&#25928;&#21033;&#29992;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SSL&#33021;&#22815;&#26174;&#33879;&#22686;&#24378;Transformer&#27169;&#22411;&#23398;&#20064;&#34920;&#31034;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20854;&#22312;&#20266;&#36857;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#21508;&#31181;SSL&#25216;&#26415;&#20013;&#65292;&#21253;&#25324;&#25513;&#30721;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#26080;&#26631;&#31614;&#33258;&#33976;&#39311;&#65288;DINO&#65289;-&#23545;&#27604;&#23398;&#20064;&#22312;&#23567;&#22411;PPG&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#31283;&#23450;&#12289;&#24615;&#33021;&#26368;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20248;&#21270;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#23545;&#27604;SSL&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#21040;...&#30340;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU) has revealed that traditional machine learning methods, such as semi-supervised label propagation and K-nearest neighbors, outperform Transformer-based models in artifact detection from PPG signals, mainly when data is limited. This study addresses the underutilization of abundant unlabeled data by employing self-supervised learning (SSL) to extract latent features from these data, followed by fine-tuning on labeled data. Our experiments demonstrate that SSL significantly enhances the Transformer model's ability to learn representations, improving its robustness in artifact classification tasks. Among various SSL techniques, including masking, contrastive learning, and DINO (self-distillation with no labels)-contrastive learning exhibited the most stable and superior performance in small PPG datasets. Further, we delve into optimizing contrastive loss functions, which are crucial for contrastive SSL. Inspired b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UCAD&#30340;&#26080;&#30417;&#30563;&#36830;&#32493;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25552;&#31034;&#20026;UAD&#22686;&#21152;&#20102;&#36830;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;UCAD&#36890;&#36807;&#21033;&#29992;&#20851;&#38190;&#25552;&#31034;&#30693;&#35782;&#24211;&#21644;&#32467;&#26500;&#23545;&#27604;&#23398;&#20064;&#26469;&#25351;&#23548;&#24322;&#24120;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#36951;&#24536;&#21644;&#35745;&#31639;&#36127;&#25285;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01010</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#36830;&#32493;&#24322;&#24120;&#26816;&#27979;&#19982;&#23545;&#27604;&#23398;&#20064;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Continual Anomaly Detection with Contrastively-learned Prompt. (arXiv:2401.01010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UCAD&#30340;&#26080;&#30417;&#30563;&#36830;&#32493;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25552;&#31034;&#20026;UAD&#22686;&#21152;&#20102;&#36830;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;UCAD&#36890;&#36807;&#21033;&#29992;&#20851;&#38190;&#25552;&#31034;&#30693;&#35782;&#24211;&#21644;&#32467;&#26500;&#23545;&#27604;&#23398;&#20064;&#26469;&#25351;&#23548;&#24322;&#24120;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#36951;&#24536;&#21644;&#35745;&#31639;&#36127;&#25285;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#21046;&#36896;&#20013;&#65292;&#26080;&#30417;&#30563;&#22686;&#37327;&#35757;&#32451;&#30340;&#24322;&#24120;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#19981;&#21487;&#39044;&#27979;&#30340;&#32570;&#38519;&#20351;&#24471;&#33719;&#21462;&#36275;&#22815;&#30340;&#26631;&#35760;&#25968;&#25454;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#30340;&#27880;&#37322;&#65292;&#32780;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#21463;&#38480;&#20110;&#32570;&#20047;&#30417;&#30563;&#12290;&#30446;&#21069;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26159;&#38024;&#23545;&#19981;&#21516;&#31867;&#21035;&#20381;&#27425;&#35757;&#32451;&#29420;&#31435;&#30340;&#27169;&#22411;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#35745;&#31639;&#36127;&#25285;&#27785;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#36830;&#32493;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;UCAD&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25552;&#31034;&#20026;UAD&#25552;&#20379;&#20102;&#36830;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#25552;&#20986;&#30340;UCAD&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31616;&#27905;&#30340;&#20851;&#38190;&#25552;&#31034;&#30693;&#35782;&#24211;&#35774;&#35745;&#20102;&#36830;&#32493;&#25552;&#31034;&#27169;&#22359;(CPM)&#65292;&#36890;&#36807;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#8220;&#27491;&#24120;&#8221;&#30693;&#35782;&#25351;&#23548;&#20219;&#21153;&#26080;&#20851;&#30340;&#8220;&#24322;&#24120;&#8221;&#27169;&#22411;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#27604;&#23398;&#20064;(SCL)&#65292;&#24182;&#37319;&#29992;Segment Anything Model (SAM)&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Anomaly Detection (UAD) with incremental training is crucial in industrial manufacturing, as unpredictable defects make obtaining sufficient labeled data infeasible. However, continual learning methods primarily rely on supervised annotations, while the application in UAD is limited due to the absence of supervision. Current UAD methods train separate models for different classes sequentially, leading to catastrophic forgetting and a heavy computational burden. To address this issue, we introduce a novel Unsupervised Continual Anomaly Detection framework called UCAD, which equips the UAD with continual learning capability through contrastively-learned prompts. In the proposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a concise key-prompt-knowledge memory bank to guide task-invariant `anomaly' model predictions using task-specific `normal' knowledge. Moreover, Structure-based Contrastive Learning (SCL) is designed with the Segment Anything Model (SAM) 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25506;&#32034;&#20102;&#39044;&#27979;&#21270;&#21512;&#29289;&#27963;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;100&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#32452;&#21512;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#26368;&#32456;&#26681;&#25454;&#19968;&#32452;&#35780;&#20272;&#26631;&#20934;&#36873;&#25321;&#20102;&#26368;&#20339;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.01004</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#21270;&#21512;&#29289;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predicting the activity of chemical compounds based on machine learning approaches. (arXiv:2401.01004v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25506;&#32034;&#20102;&#39044;&#27979;&#21270;&#21512;&#29289;&#27963;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;100&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#32452;&#21512;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#26368;&#32456;&#26681;&#25454;&#19968;&#32452;&#35780;&#20272;&#26631;&#20934;&#36873;&#25321;&#20102;&#26368;&#20339;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#35299;&#20915;&#29305;&#23450;&#25361;&#25112;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21270;&#23398;&#20449;&#24687;&#23398;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#21363;&#25552;&#20379;&#36866;&#24403;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#23613;&#21487;&#33021;&#20934;&#30830;&#22320;&#39044;&#27979;&#21270;&#21512;&#29289;&#30340;&#27963;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#22312;100&#31181;&#19981;&#21516;&#30340;&#29616;&#26377;&#25216;&#26415;&#32452;&#21512;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26159;&#26681;&#25454;&#19968;&#32452;&#26631;&#20934;&#36873;&#25321;&#30340;&#65292;&#21253;&#25324;G-means&#12289;F1-score&#21644;AUC&#24230;&#37327;&#12290;&#32467;&#26524;&#22312;&#19968;&#20010;&#21253;&#21547;&#32422;10,000&#20010;&#21270;&#21512;&#29289;&#30340;PubChem&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36825;&#20123;&#21270;&#21512;&#29289;&#26681;&#25454;&#20854;&#27963;&#24615;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring methods and techniques of machine learning (ML) to address specific challenges in various fields is essential. In this work, we tackle a problem in the domain of Cheminformatics; that is, providing a suitable solution to aid in predicting the activity of a chemical compound to the best extent possible. To address the problem at hand, this study conducts experiments on 100 different combinations of existing techniques. These solutions are then selected based on a set of criteria that includes the G-means, F1-score, and AUC metrics. The results have been tested on a dataset of about 10,000 chemical compounds from PubChem that have been classified according to their activity
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#33041;&#33034;&#28082;&#29983;&#29289;&#26631;&#24535;&#29289;&#27700;&#24179;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#19981;&#21516;&#38454;&#27573;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28096;&#31881;&#26679;&#34507;&#30333;&#946;1-42&#12289;T-&#964;&#21644;P-&#964;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#23545;&#26089;&#26399;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.00981</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#33041;&#33034;&#28082;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Classification of Alzheimer's Disease Stages Using Cerebrospinal Fluid Biomarkers Alone. (arXiv:2401.00981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#33041;&#33034;&#28082;&#29983;&#29289;&#26631;&#24535;&#29289;&#27700;&#24179;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#19981;&#21516;&#38454;&#27573;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#28096;&#31881;&#26679;&#34507;&#30333;&#946;1-42&#12289;T-&#964;&#21644;P-&#964;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#23545;&#26089;&#26399;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#22312;&#20020;&#24202;&#30151;&#29366;&#21457;&#29983;&#20043;&#21069;&#30340;&#28508;&#20239;&#26399;&#35782;&#21035;&#24739;&#32773;&#65292;&#26089;&#26399;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20960;&#39033;&#30740;&#31350;&#34920;&#26126;&#33041;&#33034;&#28082;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#28096;&#31881;&#26679;&#34507;&#30333;&#946;1-42&#12289;T-&#964;&#21644;P-&#964;&#65292;&#23545;&#20110;&#26089;&#26399;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#38454;&#27573;&#20855;&#26377;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26681;&#25454;&#33041;&#33034;&#28082;&#29983;&#29289;&#26631;&#24535;&#29289;&#27700;&#24179;&#21333;&#29420;&#20998;&#31867;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#19981;&#21516;&#38454;&#27573;&#12290;&#20998;&#26512;&#20102;&#22269;&#23478;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21327;&#35843;&#20013;&#24515;&#25968;&#25454;&#24211;&#20013;&#30340;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#24182;&#26681;&#25454;&#31616;&#26126;&#31934;&#31070;&#29366;&#24577;&#35780;&#20998;&#21644;&#20020;&#24202;&#30196;&#21574;&#35780;&#23450;&#23558;&#24739;&#32773;&#32454;&#20998;&#12290;&#36827;&#34892;&#20102;&#32479;&#35745;&#21644;&#30456;&#20851;&#24615;&#20998;&#26512;&#20197;&#30830;&#23450;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#38454;&#27573;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;K&#26368;&#36817;&#37051;&#12289;&#38598;&#25104;&#25552;&#21319;&#26641;&#12289;&#38598;&#25104;&#34955;&#35013;&#26641;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#31561;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of Alzheimer's disease is a challenge because the existing methodologies do not identify the patients in their preclinical stage, which can last up to a decade prior to the onset of clinical symptoms. Several research studies demonstrate the potential of cerebrospinal fluid biomarkers, amyloid beta 1-42, T-tau, and P-tau, in early diagnosis of Alzheimer's disease stages. In this work, we used machine learning models to classify different stages of Alzheimer's disease based on the cerebrospinal fluid biomarker levels alone. An electronic health record of patients from the National Alzheimer's Coordinating Centre database was analyzed and the patients were subdivided based on mini-mental state scores and clinical dementia ratings. Statistical and correlation analyses were performed to identify significant differences between the Alzheimer's stages. Afterward, machine learning classifiers including K-Nearest Neighbors, Ensemble Boosted Tree, Ensemble Bagged Tree, Support V
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#35757;&#32451;&#20013;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#22312;&#19981;&#21516;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#35757;&#32451;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#26102;&#65292;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BN&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#20248;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.00974</link><description>&lt;p&gt;
&#22312;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#20013;&#65292;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#30340;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models. (arXiv:2401.00974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#35757;&#32451;&#20013;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35843;&#26597;&#20102;&#22312;&#19981;&#21516;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#25104;&#35757;&#32451;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#26102;&#65292;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BN&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#20248;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#27969;&#31243;&#26159;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#20294;&#20855;&#26377;&#37325;&#35201;&#23454;&#38469;&#24847;&#20041;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#31867;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#23545;&#20110;&#21512;&#25104;&#35757;&#32451;&#20219;&#21153;&#20013;&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#65292;&#32473;&#23450;&#29305;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#31867;&#21035;&#21644;&#24615;&#33021;&#24230;&#37327;&#65292;&#25552;&#20379;&#30340;&#35265;&#35299;&#26377;&#38480;&#12290;&#26412;&#25991;&#38024;&#23545;&#35757;&#32451;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#23548;&#21521;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#35843;&#26597;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#32422;&#26463;&#19981;&#21516;&#26102;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25903;&#25345;&#20197;&#19979;&#35266;&#28857;&#65306;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#32422;&#26463;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BN&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#22343;&#33021;&#24456;&#22909;&#23436;&#25104;&#21512;&#25104;&#35757;&#32451;&#20219;&#21153;&#65292;&#20294;&#22312;&#21512;&#25104;&#35757;&#32451;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#26102;&#65292;BN-based&#29983;&#25104;&#27169;&#22411;&#27604;NN-based&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Devising procedures for downstream task-oriented generative model selections is an unresolved problem of practical importance. Existing studies focused on the utility of a single family of generative models. They provided limited insights on how synthetic data practitioners select the best family generative models for synthetic training tasks given a specific combination of machine learning model class and performance metric. In this paper, we approach the downstream task-oriented generative model selections problem in the case of training fraud detection models and investigate the best practice given different combinations of model interpretability and model performance constraints. Our investigation supports that, while both Neural Network(NN)-based and Bayesian Network(BN)-based generative models are both good to complete synthetic training task under loose model interpretability constrain, the BN-based generative models is better than NN-based when synthetic training fraud detectio
&lt;/p&gt;</description></item><item><title>&#26412;&#39033;&#30446;&#26088;&#22312;&#24320;&#21457;&#20445;&#25252;&#38544;&#31169;&#30340;fNIRS&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#25506;&#32034;&#38598;&#20013;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20445;&#38556;&#22810;&#23458;&#25143;&#20043;&#38388;&#20849;&#20139;&#27169;&#22411;&#30340;&#21516;&#26102;&#20445;&#25252;&#31169;&#26377;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.00973</link><description>&lt;p&gt;
Facebook&#38544;&#31169;&#20445;&#25252;fNIRS&#25968;&#25454;&#30340;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Facebook Report on Privacy of fNIRS data. (arXiv:2401.00973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#26088;&#22312;&#24320;&#21457;&#20445;&#25252;&#38544;&#31169;&#30340;fNIRS&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#25216;&#26415;&#65292;&#24182;&#25506;&#32034;&#38598;&#20013;&#24335;&#21644;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20445;&#38556;&#22810;&#23458;&#25143;&#20043;&#38388;&#20849;&#20139;&#27169;&#22411;&#30340;&#21516;&#26102;&#20445;&#25252;&#31169;&#26377;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;fNIRS&#25968;&#25454;&#24320;&#21457;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#25216;&#26415;&#12290;&#35813;&#39033;&#30446;&#23558;&#22312;&#38598;&#20013;&#24335;&#29615;&#22659;&#20013;&#26500;&#24314;&#19968;&#20010;&#26412;&#22320;&#27169;&#22411;&#65292;&#21516;&#26102;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21644;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;&#23427;&#36824;&#23558;&#25506;&#32034;&#21327;&#21516;&#32852;&#37030;&#23398;&#20064;&#65292;&#20197;&#22312;&#22810;&#20010;&#23458;&#25143;&#20043;&#38388;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#65292;&#32780;&#19981;&#20849;&#20139;&#26412;&#22320;&#30340;fNIRS&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#38450;&#27490;&#27492;&#31867;&#23458;&#25143;&#31169;&#26377;&#25968;&#25454;&#30340;&#26080;&#24847;&#27844;&#28431;&#65292;&#25105;&#20204;&#36824;&#23558;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary goal of this project is to develop privacy-preserving machine learning model training techniques for fNIRS data. This project will build a local model in a centralized setting with both differential privacy (DP) and certified robustness. It will also explore collaborative federated learning to train a shared model between multiple clients without sharing local fNIRS datasets. To prevent unintentional private information leakage of such clients' private datasets, we will also implement DP in the federated learning setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#20116;&#31181;&#39044;&#22788;&#29702;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#26465;&#20214;&#27010;&#29575;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;CPAR&#65289;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25913;&#21892;&#21512;&#25104;&#20449;&#29992;&#21345;&#20132;&#26131;&#25968;&#25454;&#30340;&#20445;&#30495;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#36827;&#19968;&#27493;&#36890;&#36807;&#23450;&#21046;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#39564;&#35777;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#20026;&#37329;&#34701;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#21644;&#23454;&#36341;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.00965</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#25552;&#39640;&#21512;&#25104;&#20449;&#29992;&#21345;&#20132;&#26131;&#26102;&#38388;&#24207;&#21015;&#30340;&#20445;&#30495;&#24230;&#21644;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improve Fidelity and Utility of Synthetic Credit Card Transaction Time Series from Data-centric Perspective. (arXiv:2401.00965v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#20116;&#31181;&#39044;&#22788;&#29702;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;&#26465;&#20214;&#27010;&#29575;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;CPAR&#65289;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#25913;&#21892;&#21512;&#25104;&#20449;&#29992;&#21345;&#20132;&#26131;&#25968;&#25454;&#30340;&#20445;&#30495;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#36827;&#19968;&#27493;&#36890;&#36807;&#23450;&#21046;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#39564;&#35777;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#20026;&#37329;&#34701;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#21644;&#23454;&#36341;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25506;&#32034;&#21512;&#25104;&#34920;&#26684;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#39034;&#24207;&#19978;&#19979;&#25991;&#20013;&#65292;&#22914;&#20449;&#29992;&#21345;&#20132;&#26131;&#25968;&#25454;&#65292;&#23384;&#22312;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#37325;&#28857;&#26159;&#23454;&#29616;&#23545;&#23454;&#38469;&#25968;&#25454;&#30340;&#39640;&#20445;&#30495;&#24230;&#21644;&#23545;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#26368;&#20339;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20116;&#31181;&#39044;&#22788;&#29702;&#26041;&#26696;&#26469;&#22686;&#24378;&#26465;&#20214;&#27010;&#29575;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;CPAR&#65289;&#30340;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#21512;&#25104;&#25968;&#25454;&#20445;&#30495;&#24230;&#21644;&#23454;&#29992;&#24615;&#30340;&#28176;&#36827;&#24615;&#25913;&#36827;&#12290;&#22312;&#36798;&#21040;&#28385;&#24847;&#30340;&#20445;&#30495;&#24230;&#27700;&#24179;&#21518;&#65292;&#25105;&#20204;&#36716;&#21521;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23450;&#21046;&#30340;&#27450;&#35784;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#37329;&#34701;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#20174;&#23454;&#38469;&#25968;&#25454;&#36807;&#28193;&#21040;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#21644;&#23454;&#36341;&#25351;&#23548;&#65292;&#24182;&#38416;&#26126;&#20102;&#21512;&#25104;&#20449;&#29992;&#21345;&#20132;&#26131;&#26102;&#38388;&#24207;&#21015;&#30340;&#26356;&#24191;&#27867;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring generative model training for synthetic tabular data, specifically in sequential contexts such as credit card transaction data, presents significant challenges. This paper addresses these challenges, focusing on attaining both high fidelity to actual data and optimal utility for machine learning tasks. We introduce five pre-processing schemas to enhance the training of the Conditional Probabilistic Auto-Regressive Model (CPAR), demonstrating incremental improvements in the synthetic data's fidelity and utility. Upon achieving satisfactory fidelity levels, our attention shifts to training fraud detection models tailored for time-series data, evaluating the utility of the synthetic data. Our findings offer valuable insights and practical guidelines for synthetic data practitioners in the finance sector, transitioning from real to synthetic datasets for training purposes, and illuminating broader methodologies for synthesizing credit card transaction time series.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24212;&#29992;&#22522;&#20110;&#22270;&#20687;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20110;WiFi CSI&#65292;&#26088;&#22312;&#35299;&#20915;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36328;&#22330;&#26223;&#21644;&#36328;&#31995;&#32479;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#32447;&#24615;&#35270;&#32447;&#65288;LOS&#65289;&#21644;&#38750;&#32447;&#24615;&#35270;&#32447;&#65288;NLOS&#65289;&#31359;&#22681;&#22330;&#26223;&#20043;&#38388;&#20197;&#21450;&#19981;&#21516;&#22825;&#32447;&#31995;&#32479;&#20043;&#38388;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;EfficientNetV2&#26550;&#26500;&#30340;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#24182;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.00964</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;WiFi CSI&#22522;&#20110;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human Activity Recognition. (arXiv:2401.00964v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24212;&#29992;&#22522;&#20110;&#22270;&#20687;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20110;WiFi CSI&#65292;&#26088;&#22312;&#35299;&#20915;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36328;&#22330;&#26223;&#21644;&#36328;&#31995;&#32479;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#32447;&#24615;&#35270;&#32447;&#65288;LOS&#65289;&#21644;&#38750;&#32447;&#24615;&#35270;&#32447;&#65288;NLOS&#65289;&#31359;&#22681;&#22330;&#26223;&#20043;&#38388;&#20197;&#21450;&#19981;&#21516;&#22825;&#32447;&#31995;&#32479;&#20043;&#38388;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;EfficientNetV2&#26550;&#26500;&#30340;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#24182;&#36827;&#34892;&#28040;&#34701;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#33021;&#22815;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#23454;&#29616;&#26080;&#25509;&#35302;&#21644;&#35270;&#35273;&#20445;&#25252;&#38544;&#31169;&#30340;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#26465;&#20214;&#21644;&#24863;&#30693;&#30828;&#20214;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#24046;&#26159;&#36825;&#20010;&#39046;&#22495;&#20013;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#23558;&#24120;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#24212;&#29992;&#20110;WiFi CSI&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#23545;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#22312;&#36328;&#22330;&#26223;&#21644;&#36328;&#31995;&#32479;&#35774;&#32622;&#20013;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;&#35270;&#32447;&#65288;LOS&#65289;&#21644;&#38750;&#32447;&#24615;&#35270;&#32447;&#65288;NLOS&#65289;&#31359;&#22681;&#22330;&#26223;&#20043;&#38388;&#30340;&#27867;&#21270;&#65292;&#20197;&#21450;&#19981;&#21516;&#22825;&#32447;&#31995;&#32479;&#20043;&#38388;&#30340;&#27867;&#21270;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#32463;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#20844;&#24320;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#20307;&#27963;&#21160;CSI&#24133;&#24230;&#35889;&#22270;&#30340;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#36827;&#34892;&#20102;&#19968;&#20010;&#28040;&#34701;&#30740;&#31350;&#65292;&#22522;&#20110;EfficientNetV2&#26550;&#26500;&#26500;&#24314;&#20102;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recognition of human activities based on WiFi Channel State Information (CSI) enables contactless and visual privacy-preserving sensing in indoor environments. However, poor model generalization, due to varying environmental conditions and sensing hardware, is a well-known problem in this space. To address this issue, in this work, data augmentation techniques commonly used in image-based learning are applied to WiFi CSI to investigate their effects on model generalization performance in cross-scenario and cross-system settings. In particular, we focus on the generalization between line-of-sight (LOS) and non-line-of-sight (NLOS) through-wall scenarios, as well as on the generalization between different antenna systems, which remains under-explored. We collect and make publicly available a dataset of CSI amplitude spectrograms of human activities. Utilizing this data, an ablation study is conducted in which activity recognition models based on the EfficientNetV2 architecture are tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#39044;&#27979;&#12290;&#35813;&#31639;&#27861;&#32771;&#34385;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#21253;&#21547;&#20102;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#38543;&#26426;&#32593;&#26684;&#25628;&#32034;&#21644;&#36138;&#23146;&#25628;&#32034;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.00961</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Automated Model Selection for Tabular Data. (arXiv:2401.00961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#27169;&#22411;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#39044;&#27979;&#12290;&#35813;&#31639;&#27861;&#32771;&#34385;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#21253;&#21547;&#20102;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#38543;&#26426;&#32593;&#26684;&#25628;&#32034;&#21644;&#36138;&#23146;&#25628;&#32034;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#21253;&#21547;&#29420;&#29305;&#19988;&#31163;&#25955;&#30340;&#29305;&#24449;&#65292;&#24182;&#19988;&#36825;&#20123;&#29305;&#24449;&#23545;&#30446;&#26631;&#30340;&#37325;&#35201;&#24615;&#21508;&#19981;&#30456;&#21516;&#12290;&#21333;&#20010;&#29305;&#24449;&#30340;&#32452;&#21512;&#21487;&#33021;&#27604;&#31616;&#21333;&#30340;&#21333;&#20010;&#29305;&#24449;&#36129;&#29486;&#26356;&#20855;&#39044;&#27979;&#24615;&#21644;&#24847;&#20041;&#12290;R&#30340;&#28151;&#21512;&#25928;&#24212;&#32447;&#24615;&#27169;&#22411;&#24211;&#20801;&#35768;&#29992;&#25143;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#25552;&#20379;&#36825;&#31181;&#20132;&#20114;&#24335;&#29305;&#24449;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#26377;&#35768;&#22810;&#29305;&#24449;&#21644;&#21487;&#33021;&#30340;&#20132;&#20114;&#36873;&#25321;&#65292;&#27169;&#22411;&#36873;&#25321;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#36739;&#23567;&#65292;&#33258;&#21160;&#21270;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20013;&#30340;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#65292;&#24182;&#21516;&#26102;&#32771;&#34385;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#31181;&#19981;&#21516;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65306;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#38543;&#26426;&#32593;&#26684;&#25628;&#32034;&#21644;&#36138;&#23146;&#25628;&#32034;&#26041;&#27861;&#12290;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#39564;&#27010;&#29575;&#26469;&#24341;&#23548;&#25628;&#32034;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#29305;&#24449;&#32452;&#21512;&#12290;&#36138;&#23146;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#22320;&#28155;&#21152;&#29305;&#24449;&#26500;&#24314;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured data in the form of tabular datasets contain features that are distinct and discrete, with varying individual and relative importances to the target. Combinations of one or more features may be more predictive and meaningful than simple individual feature contributions. R's mixed effect linear models library allows users to provide such interactive feature combinations in the model design. However, given many features and possible interactions to select from, model selection becomes an exponentially difficult task. We aim to automate the model selection process for predictions on tabular datasets incorporating feature interactions while keeping computational costs small. The framework includes two distinct approaches for feature selection: a Priority-based Random Grid Search and a Greedy Search method. The Priority-based approach efficiently explores feature combinations using prior probabilities to guide the search. The Greedy method builds the solution iteratively by addin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#26368;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#30340;&#32467;&#21512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32463;&#20856;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;SSM&#30340;SNN&#33021;&#22815;&#36229;&#36234;Transformers&#24182;&#19988;&#22312;&#39034;&#24207;&#22270;&#20687;&#20998;&#31867;&#20013;&#20197;&#26356;&#23569;&#30340;&#21442;&#25968;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;SNN&#12290;</title><link>http://arxiv.org/abs/2401.00955</link><description>&lt;p&gt;
&#22312;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#38271;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Learning Long Sequences in Spiking Neural Networks. (arXiv:2401.00955v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00955
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#26368;&#26032;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#30340;&#32467;&#21512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32463;&#20856;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;SSM&#30340;SNN&#33021;&#22815;&#36229;&#36234;Transformers&#24182;&#19988;&#22312;&#39034;&#24207;&#22270;&#20687;&#20998;&#31867;&#20013;&#20197;&#26356;&#23569;&#30340;&#21442;&#25968;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;SNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#21463;&#21040;&#22823;&#33041;&#30340;&#21551;&#21457;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36951;&#30041;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#38480;&#21046;&#20197;&#21450;&#35757;&#32451;&#26102;&#20351;&#29992;&#38750;&#21487;&#24494;&#30340;&#20108;&#20540;&#33033;&#20914;&#28608;&#27963;&#20989;&#25968;&#65292;SNN&#22312;&#29616;&#20195;&#24207;&#21015;&#20219;&#21153;&#20013;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#31454;&#20105;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#12290;&#26368;&#36817;&#65292;&#23545;&#20110;Transformers&#30340;&#39640;&#25928;&#26367;&#20195;&#26041;&#26696;&#30340;&#20852;&#36259;&#30340;&#22797;&#33487;&#65292;&#20351;&#24471;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#30340;&#26368;&#26032;&#36882;&#24402;&#26550;&#26500;&#24471;&#20197;&#20986;&#29616;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#26368;&#26032;&#30340;SSM&#19982;SNN&#30456;&#32467;&#21512;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#30340;&#20132;&#21449;&#28857;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;SSM&#30340;SNN&#22312;&#19968;&#20010;&#32463;&#20856;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#22522;&#20934;&#20219;&#21153;&#20013;&#33021;&#22815;&#36229;&#36234;Transformers&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;SSM&#30340;SNN&#22312;&#39034;&#24207;&#22270;&#20687;&#20998;&#31867;&#19978;&#21487;&#20197;&#20197;&#26356;&#23569;&#30340;&#21442;&#25968;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;SNN&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) take inspiration from the brain to enable energy-efficient computations. Since the advent of Transformers, SNNs have struggled to compete with artificial networks on modern sequential tasks, as they inherit limitations from recurrent neural networks (RNNs), with the added challenge of training with non-differentiable binary spiking activations. However, a recent renewed interest in efficient alternatives to Transformers has given rise to state-of-the-art recurrent architectures named state space models (SSMs). This work systematically investigates, for the first time, the intersection of state-of-the-art SSMs with SNNs for long-range sequence modelling. Results suggest that SSM-based SNNs can outperform the Transformer on all tasks of a well-established long-range sequence modelling benchmark. It is also shown that SSM-based SNNs can outperform current state-of-the-art SNNs with fewer parameters on sequential image classification. Finally, a novel feature
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26368;&#20248;&#20256;&#36865;&#20013;&#20351;&#29992;&#24418;&#24335;&#20026;$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$&#30340;&#36153;&#29992;&#20989;&#25968;&#26102;&#30340;&#38646;&#21644;&#38750;&#36127;MTW&#24352;&#37327;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;MTW&#24352;&#37327;&#22312;&#38646;&#21521;&#37327;&#19978;&#20026;&#38646;&#30340;&#26465;&#20214;&#20197;&#21450;&#30456;&#24212;&#30340;&#32447;&#24615;ODE&#30340;&#31616;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#36870;&#20989;&#25968;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#20197;&#21450;&#19968;&#20123;&#20855;&#20307;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.00953</link><description>&lt;p&gt;
&#25317;&#26377;&#38646;&#21644;&#38750;&#36127;MTW&#24352;&#37327;&#30340;&#36153;&#29992;&#26063;&#22312;&#26368;&#20248;&#20256;&#36865;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Families of costs with zero and nonnegative MTW tensor in optimal transport. (arXiv:2401.00953v1 [math.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00953
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26368;&#20248;&#20256;&#36865;&#20013;&#20351;&#29992;&#24418;&#24335;&#20026;$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$&#30340;&#36153;&#29992;&#20989;&#25968;&#26102;&#30340;&#38646;&#21644;&#38750;&#36127;MTW&#24352;&#37327;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;MTW&#24352;&#37327;&#22312;&#38646;&#21521;&#37327;&#19978;&#20026;&#38646;&#30340;&#26465;&#20214;&#20197;&#21450;&#30456;&#24212;&#30340;&#32447;&#24615;ODE&#30340;&#31616;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#36870;&#20989;&#25968;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#20197;&#21450;&#19968;&#20123;&#20855;&#20307;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35745;&#31639;&#20102;&#22312;$\mathbb{R}^n$&#19978;&#20855;&#26377;&#24418;&#24335;$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$&#30340;&#36153;&#29992;&#20989;&#25968;&#30340;&#26368;&#20248;&#20256;&#36865;&#38382;&#39064;&#30340;MTW&#24352;&#37327;&#65288;&#25110;&#20132;&#21449;&#26354;&#29575;&#65289;&#12290;&#20854;&#20013;&#65292;$\mathsf{u}$&#26159;&#19968;&#20010;&#20855;&#26377;&#36870;&#20989;&#25968;$\mathsf{s}$&#30340;&#26631;&#37327;&#20989;&#25968;&#65292;$x^{\ft}y$&#26159;&#23646;&#20110;$\mathbb{R}^n$&#24320;&#23376;&#38598;&#30340;&#21521;&#37327;$x&#65292;y$&#30340;&#38750;&#36864;&#21270;&#21452;&#32447;&#24615;&#37197;&#23545;&#12290;MTW&#24352;&#37327;&#22312;Kim-McCann&#24230;&#37327;&#19979;&#23545;&#20110;&#38646;&#21521;&#37327;&#30340;&#26465;&#20214;&#26159;&#19968;&#20010;&#22235;&#38454;&#38750;&#32447;&#24615;ODE&#65292;&#21487;&#20197;&#34987;&#31616;&#21270;&#20026;&#20855;&#26377;&#24120;&#25968;&#31995;&#25968;$P$&#21644;$S$&#30340;&#24418;&#24335;&#20026;$\mathsf{s}^{(2)} - S\mathsf{s}^{(1)} + P\mathsf{s} = 0$&#30340;&#32447;&#24615;ODE&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#36870;&#20989;&#25968;&#21253;&#25324;Lambert&#21644;&#24191;&#20041;&#21453;&#21452;&#26354;/&#19977;&#35282;&#20989;&#25968;&#12290;&#24179;&#26041;&#27431;&#27663;&#24230;&#37327;&#21644;$\log$&#22411;&#36153;&#29992;&#26159;&#36825;&#20123;&#35299;&#30340;&#23454;&#20363;&#12290;&#36825;&#20010;&#23478;&#26063;&#30340;&#26368;&#20248;&#26144;&#23556;&#20063;&#26159;&#26174;&#24335;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We compute explicitly the MTW tensor (or cross curvature) for the optimal transport problem on $\mathbb{R}^n$ with a cost function of form $\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$, where $\mathsf{u}$ is a scalar function with inverse $\mathsf{s}$, $x^{\ft}y$ is a nondegenerate bilinear pairing of vectors $x, y$ belonging to an open subset of $\mathbb{R}^n$. The condition that the MTW-tensor vanishes on null vectors under the Kim-McCann metric is a fourth-order nonlinear ODE, which could be reduced to a linear ODE of the form $\mathsf{s}^{(2)} - S\mathsf{s}^{(1)} + P\mathsf{s} = 0$ with constant coefficients $P$ and $S$. The resulting inverse functions include {\it Lambert} and {\it generalized inverse hyperbolic\slash trigonometric} functions. The square Euclidean metric and $\log$-type costs are equivalent to instances of these solutions. The optimal map for the family is also explicit. For cost functions of a similar form on a hyperboloid model of the hyperbolic space and u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;6G&#23376;&#32593;&#32476;&#20013;&#36827;&#34892;&#23376;&#39057;&#24102;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23376;&#39057;&#24102;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#19982;&#38598;&#20013;&#24335;&#36138;&#23146;&#30528;&#33394;&#23376;&#39057;&#24102;&#20998;&#37197;&#26041;&#27861;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#36739;&#23567;&#30340;&#20449;&#20196;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.00950</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;6G&#23376;&#32593;&#32476;&#30340;&#23376;&#39057;&#24102;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Graph-based Learning Method for Sub-band Allocation in 6G Subnetworks. (arXiv:2401.00950v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;6G&#23376;&#32593;&#32476;&#20013;&#36827;&#34892;&#23376;&#39057;&#24102;&#20998;&#37197;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23376;&#39057;&#24102;&#20998;&#37197;&#65292;&#23454;&#29616;&#20102;&#19982;&#38598;&#20013;&#24335;&#36138;&#23146;&#30528;&#33394;&#23376;&#39057;&#24102;&#20998;&#37197;&#26041;&#27861;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#36739;&#23567;&#30340;&#20449;&#20196;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#36827;&#34892;&#39057;&#29575;&#23376;&#24102;&#20998;&#37197;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#24037;&#21378;&#29615;&#22659;&#20013;&#23494;&#38598;&#37096;&#32626;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#20123;&#23376;&#32593;&#32476;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#23376;&#39057;&#24102;&#65292;&#24517;&#39035;&#34987;&#20248;&#21270;&#22320;&#20998;&#37197;&#20197;&#21327;&#35843;&#23376;&#32593;&#32476;&#38388;&#30340;&#24178;&#25200;&#12290;&#25105;&#20204;&#23558;&#23376;&#32593;&#32476;&#37096;&#32626;&#24314;&#27169;&#20026;&#19968;&#20010;&#20914;&#31361;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#22270;&#30528;&#33394;&#21551;&#21457;&#21644;Potts&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20248;&#21270;&#23376;&#39057;&#24102;&#20998;&#37197;&#12290;&#25968;&#20540;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36739;&#20302;&#30340;&#35745;&#31639;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;&#38598;&#20013;&#24335;&#36138;&#23146;&#30528;&#33394;&#23376;&#39057;&#24102;&#20998;&#37197;&#21551;&#21457;&#24335;&#26041;&#27861;&#25509;&#36817;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#19982;&#38656;&#35201;&#25152;&#26377;&#20114;&#30456;&#24178;&#25200;&#30340;&#20449;&#36947;&#20449;&#24687;&#30340;&#36845;&#20195;&#20248;&#21270;&#21551;&#21457;&#24335;&#30456;&#27604;&#65292;&#23427;&#20135;&#29983;&#26356;&#23569;&#30340;&#20449;&#20196;&#24320;&#38144;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#35813;&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#32593;&#32476;&#35774;&#32622;&#20855;&#26377;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an unsupervised approach for frequency sub-band allocation in wireless networks using graph-based learning. We consider a dense deployment of subnetworks in the factory environment with a limited number of sub-bands which must be optimally allocated to coordinate inter-subnetwork interference. We model the subnetwork deployment as a conflict graph and propose an unsupervised learning approach inspired by the graph colouring heuristic and the Potts model to optimize the sub-band allocation using graph neural networks. The numerical evaluation shows that the proposed method achieves close performance to the centralized greedy colouring sub-band allocation heuristic with lower computational time complexity. In addition, it incurs reduced signalling overhead compared to iterative optimization heuristics that require all the mutual interfering channel information. We further demonstrate that the method is robust to different network settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#28151;&#27788;&#31995;&#32479;&#20013;&#36827;&#34892;&#25968;&#25454;&#21516;&#21270;&#30340;&#26032;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23436;&#25972;&#25110;&#37096;&#20998;&#35266;&#27979;&#30340;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#29366;&#24577;&#26657;&#27491;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35266;&#27979;&#21644;&#39044;&#27979;&#29366;&#24577;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.00916</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#28151;&#27788;&#31995;&#32479;&#20013;&#36827;&#34892;&#25968;&#25454;&#21516;&#21270;
&lt;/p&gt;
&lt;p&gt;
Data Assimilation in Chaotic Systems Using Deep Reinforcement Learning. (arXiv:2401.00916v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#28151;&#27788;&#31995;&#32479;&#20013;&#36827;&#34892;&#25968;&#25454;&#21516;&#21270;&#30340;&#26032;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23436;&#25972;&#25110;&#37096;&#20998;&#35266;&#27979;&#30340;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#29366;&#24577;&#26657;&#27491;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#35266;&#27979;&#21644;&#39044;&#27979;&#29366;&#24577;&#20043;&#38388;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21516;&#21270;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20174;&#27668;&#20505;&#39044;&#27979;&#21644;&#22825;&#27668;&#39044;&#25253;&#21040;&#33258;&#20027;&#36710;&#36742;&#30340;&#36712;&#36857;&#35268;&#21010;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;EnKF&#65289;&#65292;&#23427;&#20381;&#36182;&#20110;&#32447;&#24615;&#26356;&#26032;&#26469;&#26368;&#23567;&#21270;&#39044;&#27979;&#29366;&#24577;&#38598;&#21512;&#30340;&#26041;&#24046;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30475;&#21040;&#20102;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#20027;&#35201;&#26159;&#22312;&#26377;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#20869;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26410;&#32463;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#30340;&#36866;&#24212;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#21516;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#20351;&#29992;&#23436;&#25972;&#25110;&#37096;&#20998;&#35266;&#27979;&#30340;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#29366;&#24577;&#26657;&#27491;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#22312;&#28151;&#27788;&#30340;Lorenz '63&#31995;&#32479;&#19978;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#23558;&#35266;&#27979;&#21644;&#30456;&#24212;&#30340;&#39044;&#27979;&#29366;&#24577;&#20043;&#38388;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#26657;&#27491;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data assimilation (DA) plays a pivotal role in diverse applications, ranging from climate predictions and weather forecasts to trajectory planning for autonomous vehicles. A prime example is the widely used ensemble Kalman filter (EnKF), which relies on linear updates to minimize variance among the ensemble of forecast states. Recent advancements have seen the emergence of deep learning approaches in this domain, primarily within a supervised learning framework. However, the adaptability of such models to untrained scenarios remains a challenge. In this study, we introduce a novel DA strategy that utilizes reinforcement learning (RL) to apply state corrections using full or partial observations of the state variables. Our investigation focuses on demonstrating this approach to the chaotic Lorenz '63 system, where the agent's objective is to minimize the root-mean-squared error between the observations and corresponding forecast states. Consequently, the agent develops a correction stra
&lt;/p&gt;</description></item><item><title>&#36816;&#21160;&#20998;&#21106;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#22788;&#29702;&#25668;&#20687;&#26426;&#33258;&#25105;&#36816;&#21160;&#12289;&#40060;&#30524;&#38236;&#22836;&#24452;&#21521;&#30072;&#21464;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#25928;&#26524;&#36739;&#24046;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;WoodScape&#40060;&#30524;&#36816;&#21160;&#20998;&#21106;&#25361;&#25112;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#40060;&#30524;&#36816;&#21160;&#20998;&#21106;&#30340;&#27604;&#36187;&#20043;&#19968;&#65292;&#26088;&#22312;&#25506;&#32034;&#21644;&#35780;&#20272;&#20854;&#28508;&#21147;&#21644;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.00910</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;WoodScape&#36816;&#21160;&#20998;&#21106;--CVPR 2023 OmniCV&#30740;&#35752;&#20250;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV Workshop Challenge. (arXiv:2401.00910v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00910
&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#20998;&#21106;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#22788;&#29702;&#25668;&#20687;&#26426;&#33258;&#25105;&#36816;&#21160;&#12289;&#40060;&#30524;&#38236;&#22836;&#24452;&#21521;&#30072;&#21464;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#26041;&#38754;&#25928;&#26524;&#36739;&#24046;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;WoodScape&#40060;&#30524;&#36816;&#21160;&#20998;&#21106;&#25361;&#25112;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#65292;&#36825;&#26159;&#39318;&#20010;&#19987;&#27880;&#20110;&#40060;&#30524;&#36816;&#21160;&#20998;&#21106;&#30340;&#27604;&#36187;&#20043;&#19968;&#65292;&#26088;&#22312;&#25506;&#32034;&#21644;&#35780;&#20272;&#20854;&#28508;&#21147;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#20998;&#21106;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#22797;&#26434;&#20294;&#19981;&#21487;&#25110;&#32570;&#30340;&#20219;&#21153;&#12290;&#25668;&#20687;&#26426;&#30340;&#33258;&#25105;&#36816;&#21160;&#12289;&#40060;&#30524;&#38236;&#22836;&#30340;&#24452;&#21521;&#30072;&#21464;&#20197;&#21450;&#23545;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#38656;&#27714;&#24341;&#20837;&#20102;&#25361;&#25112;&#65292;&#20351;&#24471;&#20256;&#32479;&#21644;&#26631;&#20934;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26041;&#27861;&#25928;&#26524;&#36739;&#24046;&#12290;&#30456;&#24212;&#30340;&#32321;&#29712;&#30340;&#25968;&#25454;&#26631;&#27880;&#12289;&#22810;&#26679;&#21270;&#21644;&#32597;&#35265;&#24773;&#20917;&#30340;&#34920;&#24449;&#20197;&#21450;&#24191;&#27867;&#30340;&#25968;&#25454;&#37319;&#38598;&#38656;&#27714;&#24378;&#35843;&#20102;&#29992;&#20110;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;Parallel Domain&#24320;&#21457;&#30340;PD-WoodScape&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;WoodScape&#40060;&#30524;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WoodScape&#40060;&#30524;&#36816;&#21160;&#20998;&#21106;&#25361;&#25112;&#65292;&#20316;&#20026;CVPR 2023&#20840;&#26223;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;OmniCV&#65289;&#30740;&#35752;&#20250;&#30340;&#19968;&#37096;&#20998;&#20030;&#34892;&#12290;&#20316;&#20026;&#39318;&#20010;&#19987;&#27880;&#20110;&#40060;&#30524;&#36816;&#21160;&#20998;&#21106;&#30340;&#27604;&#36187;&#20043;&#19968;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#21644;&#35780;&#20272;&#20854;&#28508;&#21147;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motion segmentation is a complex yet indispensable task in autonomous driving. The challenges introduced by the ego-motion of the cameras, radial distortion in fisheye lenses, and the need for temporal consistency make the task more complicated, rendering traditional and standard Convolutional Neural Network (CNN) approaches less effective. The consequent laborious data labeling, representation of diverse and uncommon scenarios, and extensive data capture requirements underscore the imperative of synthetic data for improving machine learning model performance. To this end, we employ the PD-WoodScape synthetic dataset developed by Parallel Domain, alongside the WoodScape fisheye dataset. Thus, we present the WoodScape fisheye motion segmentation challenge for autonomous driving, held as part of the CVPR 2023 Workshop on Omnidirectional Computer Vision (OmniCV). As one of the first competitions focused on fisheye motion segmentation, we aim to explore and evaluate the potential and impac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;Score Distillation&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26694;&#26550;&#23384;&#22312;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#21464;&#20998;&#30446;&#26631;&#20013;&#21152;&#20837;&#29109;&#39033;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#35299;&#20915;&#20102;Janus&#20266;&#20687;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00909</link><description>&lt;p&gt;
Score Distillation&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#35299;&#20915;&#27169;&#24335;&#23849;&#28291;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Taming Mode Collapse in Score Distillation for Text-to-3D Generation. (arXiv:2401.00909v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;Score Distillation&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26694;&#26550;&#23384;&#22312;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#21464;&#20998;&#30446;&#26631;&#20013;&#21152;&#20837;&#29109;&#39033;&#26469;&#25913;&#36827;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#35299;&#20915;&#20102;Janus&#20266;&#20687;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Score Distillation&#22312;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#26222;&#36941;&#23384;&#22312;&#35270;&#22270;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#8220;Janus&#8221;&#20266;&#20687;&#65292;&#21363;&#29983;&#25104;&#30340;&#23545;&#35937;&#20266;&#36896;&#20102;&#22810;&#20010;&#21069;&#35270;&#22270;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#32463;&#39564;&#26377;&#25928;&#30340;&#26041;&#27861;&#36890;&#36807;&#21435;&#20559;&#32622;&#25110;&#32773;&#24341;&#23548;&#24037;&#31243;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#30340;&#26356;&#20005;&#26684;&#30340;&#35299;&#37322;&#21644;&#35299;&#20915;&#26041;&#27861;&#20173;&#28982;&#24456;&#38590;&#25214;&#21040;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;Score Distillation&#30340;&#25991;&#26412;&#21040;3D&#29983;&#25104;&#26694;&#26550;&#38519;&#20837;&#20102;&#22312;&#27599;&#20010;&#35270;&#22270;&#19978;&#29420;&#31435;&#26368;&#22823;&#20284;&#28982;&#27714;&#35299;&#30340;&#26368;&#22823;&#20284;&#28982;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#65292;&#23454;&#36341;&#20013;&#34920;&#29616;&#20026;Janus&#20266;&#20687;&#12290;&#20026;&#20102;&#36991;&#20813;&#27169;&#24335;&#23849;&#28291;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#30456;&#24212;&#21464;&#20998;&#30446;&#26631;&#20013;&#37325;&#26032;&#24341;&#20837;&#29109;&#39033;&#65292;&#23545;&#28210;&#26579;&#22270;&#20687;&#30340;&#20998;&#24067;&#36827;&#34892;&#25913;&#36827;&#12290;&#26368;&#22823;&#21270;&#29109;&#40723;&#21169;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#22312;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#20855;&#26377;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of score distillation in text-to-3D generation, such techniques notoriously suffer from view inconsistency issues, also known as "Janus" artifact, where the generated objects fake each view with multiple front faces. Although empirically effective methods have approached this problem via score debiasing or prompt engineering, a more rigorous perspective to explain and tackle this problem remains elusive. In this paper, we reveal that the existing score distillation-based text-to-3D generation frameworks degenerate to maximal likelihood seeking on each view independently and thus suffer from the mode collapse problem, manifesting as the Janus artifact in practice. To tame mode collapse, we improve score distillation by re-establishing in entropy term in the corresponding variational objective, which is applied to the distribution of rendered images. Maximizing the entropy encourages diversity among different views in generated 3D assets, thereby mitiga
&lt;/p&gt;</description></item><item><title>LaFFi&#26159;&#19968;&#31181;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#26631;&#27880;&#32773;&#23558;&#20250;&#32473;&#20986;&#30340;&#21453;&#39304;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.00907</link><description>&lt;p&gt;
LaFFi: &#21033;&#29992;&#28151;&#21512;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models. (arXiv:2401.00907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00907
&lt;/p&gt;
&lt;p&gt;
LaFFi&#26159;&#19968;&#31181;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#26631;&#27880;&#32773;&#23558;&#20250;&#32473;&#20986;&#30340;&#21453;&#39304;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24494;&#35843;&#21487;&#20197;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36866;&#24212;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#12290;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;LLM&#34987;&#35757;&#32451;&#25104;&#20135;&#29983;&#26399;&#26395;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;SFT&#35757;&#32451;&#30340;LLM&#22312;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#38382;&#31572;&#65289;&#20013;&#26377;&#26102;&#20250;&#20986;&#29616;&#31616;&#21333;&#38169;&#35823;&#21644;&#24187;&#35273;&#12290;&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;SFT&#24456;&#38590;&#23398;&#20064;&#21040;&#38382;&#39064;&#21644;&#26399;&#26395;&#31572;&#26696;&#20043;&#38388;&#30340;&#33391;&#22909;&#26144;&#23556;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#24494;&#35843;LLM&#65288;LaFFi&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;LaFFi&#35201;&#27714;LLM&#30452;&#25509;&#39044;&#27979;&#26631;&#27880;&#32773;&#23558;&#20250;&#32473;&#20986;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#26679;&#30340;&#21453;&#24605;&#35201;&#27714;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22312;&#39046;&#22495;&#20869;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#22312;SFT LLM&#39046;&#22495;&#20013;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#39069;&#22806;&#30340;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#21487;&#20197;&#34987;&#26367;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models (LLMs) adapts a trained model to specific downstream tasks, significantly improving task-specific performance. Supervised Fine-Tuning (SFT) is a common approach, where an LLM is trained to produce desired answers. However, LLMs trained with SFT sometimes make simple mistakes and result in hallucinations on reasoning tasks such as question-answering. Without external feedback, it is difficult for SFT to learn a good mapping between the question and the desired answer, especially with a small dataset. This paper introduces an alternative to SFT called Natural Language Feedback for Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they will receive from an annotator. We find that requiring such reflection can significantly improve the accuracy in in-domain question-answering tasks, providing a promising direction for the application of natural language feedback in the realm of SFT LLMs. Additional ablation studies show that the portion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;MIMIC-IV&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#19968;&#20010;XGBoost&#20108;&#20998;&#31867;&#27169;&#22411;&#22312;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20303;&#38498;&#26102;&#38388;&#26102;&#30340;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#65292;&#32467;&#26524;&#21457;&#29616;&#20102;&#25968;&#25454;&#38598;&#22312;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#19978;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#23569;&#20559;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.00902</link><description>&lt;p&gt;
&#23545;MIMIC-IV&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#31639;&#27861;&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;&#65306;&#24212;&#29992;&#20110;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Fairness of the MIMIC-IV Dataset and a Baseline Algorithm: Application to the ICU Length of Stay Prediction. (arXiv:2401.00902v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;MIMIC-IV&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#19968;&#20010;XGBoost&#20108;&#20998;&#31867;&#27169;&#22411;&#22312;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20303;&#38498;&#26102;&#38388;&#26102;&#30340;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#65292;&#32467;&#26524;&#21457;&#29616;&#20102;&#25968;&#25454;&#38598;&#22312;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#19978;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#20943;&#23569;&#20559;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;MIMIC-IV&#25968;&#25454;&#38598;&#65292;&#26816;&#26597;&#20102;&#19968;&#20010;XGBoost&#20108;&#20998;&#31867;&#27169;&#22411;&#22312;&#39044;&#27979;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#20303;&#38498;&#26102;&#38388;&#26102;&#30340;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;&#12290;&#24378;&#35843;&#20102;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#22312;&#22788;&#29702;&#37325;&#30151;&#24739;&#32773;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#30740;&#31350;&#35299;&#20915;&#20102;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#23481;&#37327;&#32039;&#24352;&#30340;&#38382;&#39064;&#12290;&#23427;&#24378;&#35843;&#20102;&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;&#23545;&#36164;&#28304;&#20998;&#37197;&#30340;&#37325;&#35201;&#24615;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#22312;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#19978;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#37319;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#23613;&#31649;XGBoost&#27169;&#22411;&#25972;&#20307;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#31181;&#26063;&#21644;&#20445;&#38505;&#23646;&#24615;&#26041;&#38754;&#30340;&#19981;&#24179;&#31561;&#21453;&#26144;&#20986;&#38656;&#35201;&#23450;&#21046;&#35780;&#20272;&#21644;&#25345;&#32493;&#30417;&#27979;&#12290;&#26412;&#25991;&#26368;&#21518;&#25552;&#20986;&#20102;&#20851;&#20110;&#20943;&#23569;&#20559;&#35265;&#30340;&#27880;&#37325;&#20844;&#24179;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24314;&#35758;&#65292;&#24182;&#24378;&#35843;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#21644;&#25968;&#25454;&#31185;&#23398;&#23478;&#20043;&#38388;&#30340;&#21512;&#20316;&#21162;&#21147;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper uses the MIMIC-IV dataset to examine the fairness and bias in an XGBoost binary classification model predicting the Intensive Care Unit (ICU) length of stay (LOS). Highlighting the critical role of the ICU in managing critically ill patients, the study addresses the growing strain on ICU capacity. It emphasizes the significance of LOS prediction for resource allocation. The research reveals class imbalances in the dataset across demographic attributes and employs data preprocessing and feature extraction. While the XGBoost model performs well overall, disparities across race and insurance attributes reflect the need for tailored assessments and continuous monitoring. The paper concludes with recommendations for fairness-aware machine learning techniques for mitigating biases and the need for collaborative efforts among healthcare professionals and data scientists.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#31283;&#23450;&#30340;&#22810;&#33033;&#20914;&#32467;&#26500;&#65288;MPS&#65289;&#20316;&#20026;&#25240;&#23556;&#40120;&#22238;&#22768;&#23450;&#20301;&#28857;&#20987;&#30340;&#26816;&#27979;&#25351;&#26631;&#65292;&#33021;&#22815;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#28857;&#20987;&#23384;&#22312;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#39640;&#22122;&#22768;&#21644;&#20302;&#20449;&#22122;&#27604;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.00900</link><description>&lt;p&gt;
&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#26816;&#27979;&#25240;&#23556;&#40120;&#22238;&#22768;&#23450;&#20301;&#28857;&#20987;&#30340;&#23384;&#22312;
&lt;/p&gt;
&lt;p&gt;
Detecting the presence of sperm whales echolocation clicks in noisy environments. (arXiv:2401.00900v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00900
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#31283;&#23450;&#30340;&#22810;&#33033;&#20914;&#32467;&#26500;&#65288;MPS&#65289;&#20316;&#20026;&#25240;&#23556;&#40120;&#22238;&#22768;&#23450;&#20301;&#28857;&#20987;&#30340;&#26816;&#27979;&#25351;&#26631;&#65292;&#33021;&#22815;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#28857;&#20987;&#23384;&#22312;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#39640;&#22122;&#22768;&#21644;&#20302;&#20449;&#22122;&#27604;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25240;&#23556;&#40120;&#65288;Physeter macrocephalus&#65289;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#20914;&#20987;&#24615;&#12289;&#31867;&#20284;&#28857;&#20987;&#30340;&#22768;&#38899;&#65288;&#31216;&#20026;&#22238;&#22768;&#23450;&#20301;&#28857;&#20987;&#65289;&#22312;&#27700;&#19979;&#23548;&#33322;&#12290;&#36825;&#20123;&#28857;&#20987;&#20855;&#26377;&#22810;&#33033;&#20914;&#32467;&#26500;&#65288;MPS&#65289;&#65292;&#21487;&#20316;&#20026;&#19968;&#20010;&#29420;&#29305;&#30340;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;MPS&#30340;&#31283;&#23450;&#24615;&#20316;&#20026;&#26816;&#27979;&#25351;&#26631;&#65292;&#35782;&#21035;&#21644;&#20998;&#31867;&#22024;&#26434;&#29615;&#22659;&#20013;&#30340;&#28857;&#20987;&#23384;&#22312;&#12290;&#20026;&#20102;&#21306;&#20998;&#22122;&#22768;&#30636;&#21464;&#21644;&#22788;&#29702;&#22810;&#21482;&#25240;&#23556;&#40120;&#21516;&#26102;&#21457;&#20986;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32858;&#31867;MPS&#27979;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#26469;&#31227;&#38500;&#19981;&#31526;&#21512;&#38388;&#20987;&#38388;&#38548;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#39057;&#35889;&#38480;&#21046;&#30340;&#28508;&#22312;&#28857;&#20987;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#39640;&#22122;&#22768;&#30636;&#21464;&#21644;&#20302;&#20449;&#22122;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#25968;&#25454;&#38598;&#26469;&#26816;&#39564;&#25105;&#20204;&#30340;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#65306;&#21253;&#21547;&#32463;&#36807;&#25163;&#24037;&#39564;&#35777;&#30340;&#29615;&#22659;&#22122;&#22768;&#30340;&#22320;&#20013;&#28023;&#19971;&#20010;&#26376;&#30340;&#24405;&#38899;&#65307;&#26469;&#33258;&#22810;&#31859;&#23612;&#21152;&#23707;&#30340;&#32463;&#36807;&#25163;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20960;&#22825;&#30340;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sperm whales (Physeter macrocephalus) navigate underwater with a series of impulsive, click-like sounds known as echolocation clicks. These clicks are characterized by a multipulse structure (MPS) that serves as a distinctive pattern. In this work, we use the stability of the MPS as a detection metric for recognizing and classifying the presence of clicks in noisy environments. To distinguish between noise transients and to handle simultaneous emissions from multiple sperm whales, our approach clusters a time series of MPS measures while removing potential clicks that do not fulfil the limits of inter-click interval, duration and spectrum. As a result, our approach can handle high noise transients and low signal-to-noise ratio. The performance of our detection approach is examined using three datasets: seven months of recordings from the Mediterranean Sea containing manually verified ambient noise; several days of manually labelled data collected from the Dominica Island containing app
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#20840;&#23616;&#20027;&#23548;&#27169;&#24577;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#36328;&#27169;&#24577;&#28183;&#36879;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#30340;&#27169;&#24577;&#19981;&#24179;&#34913;&#21644;&#30693;&#35782;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00894</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#28183;&#36879;&#23454;&#29616;&#24179;&#34913;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Balanced Multi-modal Federated Learning via Cross-Modal Infiltration. (arXiv:2401.00894v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00894
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#20840;&#23616;&#20027;&#23548;&#27169;&#24577;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#36328;&#27169;&#24577;&#28183;&#36879;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#30340;&#27169;&#24577;&#19981;&#24179;&#34913;&#21644;&#30693;&#35782;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#38544;&#31169;&#20445;&#25252;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#22522;&#30784;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#32780;&#19981;&#26292;&#38706;&#23458;&#25143;&#31471;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;&#24403;&#21069;&#30340;&#32852;&#37030;&#23398;&#20064;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#23545;&#20110;&#20998;&#24067;&#24335;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#30693;&#35782;&#21033;&#29992;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#38024;&#23545;&#36755;&#20837;&#31471;&#30340;&#32479;&#35745;&#25110;&#27169;&#24577;&#24322;&#36136;&#24615;&#65292;&#28982;&#32780;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23578;&#26410;&#35299;&#20915;"&#27169;&#24577;&#19981;&#24179;&#34913;"&#30340;&#26681;&#26412;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#21033;&#29992;&#19981;&#36275;&#21644;&#24322;&#36136;&#30693;&#35782;&#32858;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#27169;&#24577;&#28183;&#36879;&#32852;&#37030;&#23398;&#20064;&#65288;FedCMI&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#20840;&#23616;&#20027;&#23548;&#27169;&#24577;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#65292;&#26377;&#25928;&#32531;&#35299;&#27169;&#24577;&#19981;&#24179;&#34913;&#21644;&#30693;&#35782;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#20165;&#20165;&#27169;&#20223;&#20027;&#23548;&#27169;&#24577;&#34892;&#20026;&#23548;&#33268;&#24369;&#27169;&#24577;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36873;&#25321;&#24615;&#22320;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) underpins advancements in privacy-preserving distributed computing by collaboratively training neural networks without exposing clients' raw data. Current FL paradigms primarily focus on uni-modal data, while exploiting the knowledge from distributed multimodal data remains largely unexplored. Existing multimodal FL (MFL) solutions are mainly designed for statistical or modality heterogeneity from the input side, however, have yet to solve the fundamental issue,"modality imbalance", in distributed conditions, which can lead to inadequate information exploitation and heterogeneous knowledge aggregation on different modalities.In this paper, we propose a novel Cross-Modal Infiltration Federated Learning (FedCMI) framework that effectively alleviates modality imbalance and knowledge heterogeneity via knowledge transfer from the global dominant modality. To avoid the loss of information in the weak modality due to merely imitating the behavior of dominant modality, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20648;&#23618;&#35745;&#31639;&#22312;&#21560;&#24341;&#23376;&#37325;&#24314;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#39537;&#21160;&#24335;&#20648;&#23618;&#30340;&#26368;&#22823;&#26465;&#20214;Lyapunov&#25351;&#25968;&#38656;&#35201;&#27604;&#30495;&#23454;&#31995;&#32479;&#30340;&#26368;&#23567;Lyapunov&#25351;&#25968;&#26356;&#23567;&#65292;&#20648;&#23618;&#30340;&#35889;&#21322;&#24452;&#23545;&#21560;&#24341;&#23376;&#37325;&#24314;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.00885</link><description>&lt;p&gt;
&#20351;&#29992;&#20648;&#23618;&#35745;&#31639;&#36827;&#34892;&#21560;&#24341;&#23376;&#37325;&#24314;&#65306;&#20648;&#23618;&#30340;&#26465;&#20214;Lyapunov&#25351;&#25968;&#23545;&#24544;&#23454;&#21560;&#24341;&#23376;&#37325;&#24314;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Attractor reconstruction with reservoir computers: The effect of the reservoir's conditional Lyapunov exponents on faithful attractor reconstruction. (arXiv:2401.00885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00885
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20648;&#23618;&#35745;&#31639;&#22312;&#21560;&#24341;&#23376;&#37325;&#24314;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#39537;&#21160;&#24335;&#20648;&#23618;&#30340;&#26368;&#22823;&#26465;&#20214;Lyapunov&#25351;&#25968;&#38656;&#35201;&#27604;&#30495;&#23454;&#31995;&#32479;&#30340;&#26368;&#23567;Lyapunov&#25351;&#25968;&#26356;&#23567;&#65292;&#20648;&#23618;&#30340;&#35889;&#21322;&#24452;&#23545;&#21560;&#24341;&#23376;&#37325;&#24314;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20648;&#23618;&#35745;&#31639;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#22797;&#21046;&#21160;&#21147;&#31995;&#32479;&#30340;&#28151;&#27788;&#21560;&#24341;&#23376;&#65292;&#21253;&#25324;&#20998;&#24418;&#32500;&#24230;&#21644;&#25972;&#20010;Lyapunov&#35889;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#23558;&#39537;&#21160;&#24335;&#20648;&#23618;&#35745;&#31639;&#30340;&#24191;&#20041;&#21516;&#27493;&#21160;&#21147;&#23398;&#19982;&#33258;&#20027;&#24335;&#20648;&#23618;&#35745;&#31639;&#22312;&#21560;&#24341;&#23376;&#37325;&#24314;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20026;&#20102;&#25104;&#21151;&#36827;&#34892;&#21560;&#24341;&#23376;&#37325;&#24314;&#21644;Lyapunov&#25351;&#25968;&#20272;&#35745;&#65292;&#39537;&#21160;&#24335;&#20648;&#23618;&#30340;&#26368;&#22823;&#26465;&#20214;Lyapunov&#25351;&#25968;&#24517;&#39035;&#26174;&#33879;&#23567;&#20110;&#30495;&#23454;&#31995;&#32479;&#30340;&#26368;&#23567;&#65288;&#26368;&#36127;&#65289;Lyapunov&#25351;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20648;&#23618;&#30340;&#26368;&#22823;&#26465;&#20214;Lyapunov&#25351;&#25968;&#24378;&#28872;&#20381;&#36182;&#20110;&#20648;&#23618;&#37051;&#25509;&#30697;&#38453;&#30340;&#35889;&#21322;&#24452;&#65292;&#22240;&#27492;&#65292;&#23545;&#20110;&#21560;&#24341;&#23376;&#37325;&#24314;&#21644;Lyapunov&#25351;&#25968;&#20272;&#35745;&#65292;&#35889;&#21322;&#24452;&#36739;&#23567;&#30340;&#20648;&#23618;&#35745;&#31639;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir computing is a machine learning technique which has been shown to be able to replicate the chaotic attractor, including the fractal dimension and the entire Lyapunov spectrum, of the dynamical system on which it is trained. We quantitatively relate the generalized synchronization dynamics of a driven reservoir computer during the training stage to the performance of the autonomous reservoir computer at the attractor reconstruction task. We show that, for successful attractor reconstruction and Lyapunov exponent estimation, the largest conditional Lyapunov exponent of the driven reservoir must be significantly smaller (more negative) than the smallest (most negative) Lyapunov exponent of the true system. We find that the maximal conditional Lyapunov exponent of the reservoir depends strongly on the spectral radius of the reservoir adjacency matrix, and therefore, for attractor reconstruction and Lyapunov exponent estimation, small spectral radius reservoir computers perform be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#24320;&#21457;&#20986;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#30333;&#34880;&#30149;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#31934;&#30830;&#24230;&#21644;F1-score&#25351;&#26631;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.00883</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#33258;&#21160;&#21270;&#30333;&#34880;&#30149;&#35786;&#26029;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Automating Leukemia Diagnosis with Autoencoders: A Comparative Study. (arXiv:2401.00883v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#24320;&#21457;&#20986;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#30333;&#34880;&#30149;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#31934;&#30830;&#24230;&#21644;F1-score&#25351;&#26631;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30333;&#34880;&#30149;&#26159;&#23041;&#32961;&#20154;&#31867;&#29983;&#21629;&#30340;&#26368;&#24120;&#35265;&#21644;&#33268;&#21629;&#30340;&#30284;&#30151;&#20043;&#19968;&#12290;&#26469;&#33258;&#24739;&#32773;&#20851;&#38190;&#21442;&#25968;&#30340;&#21307;&#30103;&#25968;&#25454;&#20013;&#38544;&#34255;&#30528;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#35838;&#39064;&#19978;&#65292;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#29992;&#26469;&#25552;&#21462;&#36825;&#20123;&#20449;&#24687;&#12290;&#26412;&#25991;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#24320;&#21457;&#20102;&#26377;&#20215;&#20540;&#30340;&#29305;&#24449;&#65292;&#20197;&#24110;&#21161;&#25552;&#39640;&#30333;&#34880;&#30149;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23581;&#35797;&#25214;&#21040;&#26368;&#20339;&#30340;&#28608;&#27963;&#20989;&#25968;&#21644;&#20248;&#21270;&#22120;&#26469;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#20351;&#29992;&#65292;&#24182;&#35774;&#35745;&#20102;&#26368;&#20339;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#19982;&#35813;&#39046;&#22495;&#30340;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#21644;F1-score&#25351;&#26631;&#19978;&#27604;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21319;&#20102;&#36229;&#36807;11%&#12290;
&lt;/p&gt;
&lt;p&gt;
Leukemia is one of the most common and death-threatening types of cancer that threaten human life. Medical data from some of the patient's critical parameters contain valuable information hidden among these data. On this subject, deep learning can be used to extract this information. In this paper, AutoEncoders have been used to develop valuable features to help the precision of leukemia diagnosis. It has been attempted to get the best activation function and optimizer to use in AutoEncoder and designed the best architecture for this neural network. The proposed architecture is compared with this area's classical machine learning models. Our proposed method performs better than other machine learning in precision and f1-score metrics by more than 11%.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bargrain&#30340;&#24179;&#34913;&#33041;&#22270;&#32467;&#26500;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#27169;&#25311;&#32463;&#36807;&#28388;&#27874;&#30340;&#30456;&#20851;&#30697;&#38453;&#21644;&#26368;&#20248;&#26679;&#26412;&#22270;&#26469;&#25913;&#36827;&#33041;&#30142;&#30149;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20165;&#20381;&#36182;&#21333;&#19968;&#31867;&#22411;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.00876</link><description>&lt;p&gt;
&#24179;&#34913;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#29992;&#20110;&#33041;&#30142;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Balanced Graph Structure Information for Brain Disease Detection. (arXiv:2401.00876v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00876
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bargrain&#30340;&#24179;&#34913;&#33041;&#22270;&#32467;&#26500;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#27169;&#25311;&#32463;&#36807;&#28388;&#27874;&#30340;&#30456;&#20851;&#30697;&#38453;&#21644;&#26368;&#20248;&#26679;&#26412;&#22270;&#26469;&#25913;&#36827;&#33041;&#30142;&#30149;&#26816;&#27979;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20165;&#20381;&#36182;&#21333;&#19968;&#31867;&#22411;&#32467;&#26500;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#33041;&#21306;&#38388;&#30340;&#36830;&#25509;&#23545;&#20110;&#26816;&#27979;&#33258;&#38381;&#30151;&#25110;&#31934;&#31070;&#20998;&#35010;&#31561;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26469;&#21033;&#29992;&#33041;&#20013;&#30340;&#22270;&#32467;&#26500;&#65292;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;ROI&#30340;&#34880;&#27687;&#27700;&#24179;&#20381;&#36182;&#24615;(BOLD)&#20449;&#21495;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#29983;&#25104;&#22270;&#32467;&#26500;&#12290;&#20854;&#20182;&#26041;&#27861;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#20351;&#29992;&#35757;&#32451;&#26679;&#26412;&#26469;&#23398;&#20064;&#26368;&#20248;&#30340;&#22270;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#29420;&#31435;&#23454;&#26045;&#36825;&#20123;&#26041;&#27861;&#20250;&#23548;&#33268;&#30456;&#20851;&#24615;&#22270;&#20013;&#30340;&#22122;&#38899;&#25968;&#25454;&#38382;&#39064;&#20197;&#21450;&#26368;&#20248;&#22270;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bargrain(&#24179;&#34913;&#30340;&#33041;&#22270;&#32467;&#26500;)&#65292;&#23427;&#27169;&#25311;&#20102;&#20004;&#31181;&#22270;&#32467;&#26500;&#65306;&#32463;&#36807;&#28388;&#27874;&#30340;&#30456;&#20851;&#30697;&#38453;&#21644;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;(GCNs)&#29983;&#25104;&#30340;&#26368;&#20248;&#26679;&#26412;&#22270;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#20004;&#31181;&#22270;&#30340;&#20248;&#28857;&#65292;&#24182;&#35299;&#20915;&#20165;&#20381;&#36182;&#21333;&#19968;&#31867;&#22411;&#32467;&#26500;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing connections between brain regions of interest (ROI) is vital to detect neurological disorders such as autism or schizophrenia. Recent advancements employ graph neural networks (GNNs) to utilize graph structures in brains, improving detection performances. Current methods use correlation measures between ROI's blood-oxygen-level-dependent (BOLD) signals to generate the graph structure. Other methods use the training samples to learn the optimal graph structure through end-to-end learning. However, implementing those methods independently leads to some issues with noisy data for the correlation graphs and overfitting problems for the optimal graph. In this work, we proposed Bargrain (balanced graph structure for brains), which models two graph structures: filtered correlation matrix and optimal sample graph using graph convolution networks (GCNs). This approach aims to get advantages from both graphs and address the limitations of only relying on a single type of structure. Bas
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#32479;&#19968;&#33258;&#30417;&#30563;&#32858;&#31867;&#21644;&#33021;&#37327;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#21270;&#30340;&#25512;&#23548;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#38752;&#22320;&#24809;&#32602;&#22833;&#36133;&#27169;&#24335;&#30340;&#19979;&#30028;&#12290;&#36825;&#20010;&#19979;&#30028;&#20351;&#24471;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#39592;&#26550;&#26550;&#26500;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2401.00873</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#32479;&#19968;&#33258;&#30417;&#30563;&#32858;&#31867;&#21644;&#33021;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Unification of Self-Supervised Clustering and Energy-Based Models. (arXiv:2401.00873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00873
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#32479;&#19968;&#33258;&#30417;&#30563;&#32858;&#31867;&#21644;&#33021;&#37327;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#21270;&#30340;&#25512;&#23548;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#38752;&#22320;&#24809;&#32602;&#22833;&#36133;&#27169;&#24335;&#30340;&#19979;&#30028;&#12290;&#36825;&#20010;&#19979;&#30028;&#20351;&#24471;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#39592;&#26550;&#26550;&#26500;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#27969;&#34892;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#23545;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#36125;&#21494;&#26031;&#20998;&#26512;&#65292;&#38416;&#26126;&#20102;&#27599;&#20010;&#31867;&#21035;&#20013;&#28508;&#22312;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22522;&#26412;&#21407;&#29702;&#20986;&#21457;&#25512;&#23548;&#36825;&#20123;&#27169;&#22411;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#20998;&#26512;&#36824;&#34920;&#26126;&#20102;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#33258;&#28982;&#25972;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#33021;&#37327;&#27169;&#22411;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#36825;&#20010;&#27010;&#24565;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#19979;&#30028;&#65292;&#32463;&#35777;&#26126;&#33021;&#21487;&#38752;&#22320;&#24809;&#32602;&#26368;&#37325;&#35201;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26032;&#25552;&#20986;&#30340;&#19979;&#30028;&#20351;&#24471;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#39592;&#24178;&#26550;&#26500;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#35832;&#22914;&#20572;&#27490;&#26799;&#24230;&#12289;&#21160;&#37327;&#32534;&#30721;&#22120;&#25110;&#19987;&#38376;&#30340;&#32858;&#31867;&#31561;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is a popular and powerful method for utilizing large amounts of unlabeled data, for which a wide variety of training objectives have been proposed in the literature. In this study, we perform a Bayesian analysis of state-of-the-art self-supervised learning objectives, elucidating the underlying probabilistic graphical models in each class and presenting a standardized methodology for their derivation from first principles. The analysis also indicates a natural means of integrating self-supervised learning with likelihood-based generative models. We instantiate this concept within the realm of cluster-based self-supervised learning and energy models, introducing a novel lower bound which is proven to reliably penalize the most important failure modes. Furthermore, this newly proposed lower bound enables the training of a standard backbone architecture without the necessity for asymmetric elements such as stop gradients, momentum encoders, or specialized clusteri
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#32593;&#32476;&#21487;&#20197;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#65292;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65292;&#22312;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#25552;&#21462;&#29305;&#24449;&#27010;&#29575;&#12289;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#24322;&#24120;&#30340;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2401.00867</link><description>&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#22312;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tensor Networks for Explainable Machine Learning in Cybersecurity. (arXiv:2401.00867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00867
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#21487;&#20197;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#65292;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65292;&#22312;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#25552;&#21462;&#29305;&#24449;&#27010;&#29575;&#12289;&#29109;&#21644;&#20114;&#20449;&#24687;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#24322;&#24120;&#30340;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#24352;&#37327;&#32593;&#32476;&#22914;&#20309;&#24110;&#21161;&#21457;&#23637;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#29366;&#24577;&#65288;MPS&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#20013;&#30340;&#23545;&#25163;&#29983;&#25104;&#30340;&#23041;&#32961;&#24773;&#25253;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#65292;MPS&#22312;&#24615;&#33021;&#26041;&#38754;&#21487;&#20197;&#19982;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#22320;&#20419;&#36827;&#20102;&#29305;&#24449;&#27010;&#29575;&#12289;&#20911;&#183;&#35834;&#20234;&#26364;&#29109;&#21644;&#20114;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#20026;&#24322;&#24120;&#20998;&#31867;&#25552;&#20379;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#21465;&#36848;&#65292;&#24182;&#20419;&#36827;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27700;&#24179;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#22522;&#26412;&#21407;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we show how tensor networks help in developing explainability of machine learning algorithms. Specifically, we develop an unsupervised clustering algorithm based on Matrix Product States (MPS) and apply it in the context of a real use-case of adversary-generated threat intelligence. Our investigation proves that MPS rival traditional deep learning models such as autoencoders and GANs in terms of performance, while providing much richer model interpretability. Our approach naturally facilitates the extraction of feature-wise probabilities, Von Neumann Entropy, and mutual information, offering a compelling narrative for classification of anomalies and fostering an unprecedented level of transparency and interpretability, something fundamental to understand the rationale behind artificial intelligence decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#23431;&#23449;&#30340;&#32852;&#21512;&#22810;&#35270;&#35282;&#21512;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#32500;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26080;&#32447;&#20869;&#23481;&#20256;&#36882;&#65292;&#28385;&#36275;&#20803;&#23431;&#23449;&#20013;&#20005;&#26684;&#30340;&#26381;&#21153;&#36136;&#37327;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.00859</link><description>&lt;p&gt;
&#20026;&#20803;&#23431;&#23449;&#36827;&#34892;&#32852;&#21512;&#22810;&#35270;&#35282;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-View Synthesizing for Metaverse. (arXiv:2401.00859v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#23431;&#23449;&#30340;&#32852;&#21512;&#22810;&#35270;&#35282;&#21512;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#32500;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26080;&#32447;&#20869;&#23481;&#20256;&#36882;&#65292;&#28385;&#36275;&#20803;&#23431;&#23449;&#20013;&#20005;&#26684;&#30340;&#26381;&#21153;&#36136;&#37327;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23431;&#23449;&#39044;&#35745;&#33021;&#25552;&#20379;&#27785;&#28024;&#24335;&#30340;&#23089;&#20048;&#12289;&#25945;&#32946;&#21644;&#21830;&#19994;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#30340;&#20256;&#36755;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#27492;&#38656;&#35201;&#24341;&#20837;&#28385;&#36275;&#20005;&#26684;&#30340;&#26381;&#21153;&#36136;&#37327;&#35201;&#27714;&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#36793;&#32536;&#26234;&#33021;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#35282;&#21512;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#20803;&#23431;&#23449;&#20013;&#30340;&#26080;&#32447;&#20869;&#23481;&#20256;&#36882;&#25552;&#20379;&#39640;&#25928;&#30340;&#35745;&#31639;&#12289;&#23384;&#20648;&#21644;&#36890;&#20449;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#32500;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#21333;&#35270;&#35282;&#22270;&#20687;&#38598;&#21512;&#12290;&#36825;&#20123;&#21333;&#35270;&#35282;&#22270;&#20687;&#34987;&#20256;&#36755;&#32473;&#19968;&#32452;&#20855;&#26377;&#37325;&#21472;&#35270;&#37326;&#30340;&#29992;&#25143;&#65292;&#30456;&#27604;&#20256;&#36755;&#29926;&#29255;&#25110;&#25972;&#20010;&#19977;&#32500;&#27169;&#22411;&#65292;&#21487;&#20197;&#36991;&#20813;&#22823;&#37327;&#30340;&#20869;&#23481;&#20256;&#36755;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20445;&#35777;&#39640;&#25928;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#36890;&#36807;&#34920;&#24449;&#22402;&#30452;&#26041;&#21521;&#19978;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The metaverse is expected to provide immersive entertainment, education, and business applications. However, virtual reality (VR) transmission over wireless networks is data- and computation-intensive, making it critical to introduce novel solutions that meet stringent quality-of-service requirements. With recent advances in edge intelligence and deep learning, we have developed a novel multi-view synthesizing framework that can efficiently provide computation, storage, and communication resources for wireless content delivery in the metaverse. We propose a three-dimensional (3D)-aware generative model that uses collections of single-view images. These single-view images are transmitted to a group of users with overlapping fields of view, which avoids massive content transmission compared to transmitting tiles or whole 3D models. We then present a federated learning approach to guarantee an efficient learning process. The training performance can be improved by characterizing the verti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25490;&#25918;&#25253;&#21578;&#25104;&#29087;&#24230;&#27169;&#22411;(ERMM)&#65292;&#36890;&#36807;&#20351;&#29992;&#24615;&#33021;&#25351;&#26631;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25903;&#25345;&#22478;&#24066;&#35299;&#20915;&#27668;&#20505;&#21464;&#21270;&#21644;&#20840;&#29699;&#21464;&#26262;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.00857</link><description>&lt;p&gt;
&#25490;&#25918;&#25253;&#21578;&#25104;&#29087;&#24230;&#27169;&#22411;&#65306;&#36890;&#36807;&#24615;&#33021;&#25351;&#26631;&#21644;&#20154;&#24037;&#26234;&#33021;&#25903;&#25345;&#22478;&#24066;&#21033;&#29992;&#25490;&#25918;&#30456;&#20851;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Emissions Reporting Maturity Model: supporting cities to leverage emissions-related processes through performance indicators and artificial intelligence. (arXiv:2401.00857v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25490;&#25918;&#25253;&#21578;&#25104;&#29087;&#24230;&#27169;&#22411;(ERMM)&#65292;&#36890;&#36807;&#20351;&#29992;&#24615;&#33021;&#25351;&#26631;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25903;&#25345;&#22478;&#24066;&#35299;&#20915;&#27668;&#20505;&#21464;&#21270;&#21644;&#20840;&#29699;&#21464;&#26262;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#8220;&#29615;&#22659;&#19982;&#21457;&#23637;&#20250;&#35758;&#8221;(Eco-92)&#20197;&#26469;&#65292;&#27668;&#20505;&#21464;&#21270;&#21644;&#20840;&#29699;&#21464;&#26262;&#19968;&#30452;&#26159;&#20840;&#29699;&#28909;&#38376;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20943;&#23569;&#28201;&#23460;&#27668;&#20307;&#26041;&#38754;&#36827;&#23637;&#29978;&#24494;&#12290;&#25490;&#25918;&#38382;&#39064;&#21644;&#25361;&#25112;&#22797;&#26434;&#65292;&#38656;&#35201;&#38598;&#20013;&#32780;&#20840;&#38754;&#30340;&#21162;&#21147;&#26469;&#35299;&#20915;&#12290;&#25490;&#25918;&#25253;&#21578;&#26159;&#28201;&#23460;&#27668;&#20307;&#20943;&#25490;&#25919;&#31574;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#27492;&#26159;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20004;&#26041;&#38754;&#65306;(&#19968;)&#25552;&#20986;&#19968;&#20010;&#25490;&#25918;&#25253;&#21578;&#35780;&#20272;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#25490;&#25918;&#25253;&#21578;&#30340;&#25972;&#20307;&#36136;&#37327;&#65307;(&#20108;)&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25903;&#25345;&#25913;&#21892;&#25490;&#25918;&#25253;&#21578;&#30340;&#20513;&#35758;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#25490;&#25918;&#25253;&#21578;&#25104;&#29087;&#24230;&#27169;&#22411;(ERMM)&#65292;&#29992;&#20110;&#26816;&#26597;&#12289;&#32858;&#31867;&#21644;&#20998;&#26512;&#25490;&#25918;&#25253;&#21578;&#20513;&#35758;&#30340;&#25968;&#25454;&#65292;&#24182;&#24110;&#21161;&#22478;&#24066;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#20840;&#29699;&#21464;&#26262;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change and global warming have been trending topics worldwide since the Eco-92 conference. However, little progress has been made in reducing greenhouse gases (GHGs). The problems and challenges related to emissions are complex and require a concerted and comprehensive effort to address them. Emissions reporting is a critical component of GHG reduction policy and is therefore the focus of this work. The main goal of this work is two-fold: (i) to propose an emission reporting evaluation model to leverage emissions reporting overall quality and (ii) to use artificial intelligence (AI) to support the initiatives that improve emissions reporting. Thus, this work presents an Emissions Reporting Maturity Model (ERMM) for examining, clustering, and analysing data from emissions reporting initiatives to help the cities to deal with climate change and global warming challenges. The Performance Indicator Development Process (PIDP) proposed in this work provides ways to leverage the quali
&lt;/p&gt;</description></item><item><title>&#22312;&#28145;&#24230;&#21704;&#23494;&#39039;&#22238;&#24402;&#20013;&#65292;&#23454;&#29616;&#21327;&#26041;&#24046;&#21644;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#32423;&#32852;&#22238;&#24402;&#26694;&#26550;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#21327;&#21464;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#24182;&#20135;&#29983;&#21327;&#21464;&#29305;&#24449;&#21644;&#22522;&#32447;&#39044;&#27979;&#65292;&#36741;&#21161;&#31532;&#20108;&#38454;&#27573;&#23398;&#20064;&#21327;&#26041;&#24046;&#12290;&#21516;&#26102;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#38750;&#32447;&#24615;&#22270;&#24418;Transformer&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21704;&#23494;&#39039;&#39044;&#27979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.00744</link><description>&lt;p&gt;
&#22312;&#26230;&#20307;&#26448;&#26009;&#30740;&#31350;&#20013;&#65292;&#23558;&#21327;&#26041;&#24046;&#21644;&#34920;&#36798;&#33021;&#21147;&#34701;&#21512;&#20026;&#28145;&#24230;&#21704;&#23494;&#39039;&#22238;&#24402;&#65306;&#19968;&#31181;&#28151;&#21512;&#32423;&#32852;&#22238;&#24402;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework. (arXiv:2401.00744v2 [physics.comp-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00744
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#21704;&#23494;&#39039;&#22238;&#24402;&#20013;&#65292;&#23454;&#29616;&#21327;&#26041;&#24046;&#21644;&#32593;&#32476;&#34920;&#36798;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#32423;&#32852;&#22238;&#24402;&#26694;&#26550;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#21327;&#21464;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#24182;&#20135;&#29983;&#21327;&#21464;&#29305;&#24449;&#21644;&#22522;&#32447;&#39044;&#27979;&#65292;&#36741;&#21161;&#31532;&#20108;&#38454;&#27573;&#23398;&#20064;&#21327;&#26041;&#24046;&#12290;&#21516;&#26102;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#38750;&#32447;&#24615;&#22270;&#24418;Transformer&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21704;&#23494;&#39039;&#39044;&#27979;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#30740;&#31350;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#21704;&#23494;&#39039;&#22238;&#24402;&#37327;&#23376;&#31995;&#32479;&#38656;&#35201;&#28385;&#36275;&#21327;&#26041;&#24046;&#23450;&#24459;&#65292;&#20854;&#20013;&#23454;&#29616;SO(3)&#31561;&#21464;&#24615;&#32780;&#19981;&#25439;&#22833;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#26159;&#19968;&#20010;&#38590;&#20197;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#38750;&#32447;&#24615;&#26144;&#23556;&#30340;&#29702;&#35770;&#31561;&#21464;&#24615;&#20445;&#35777;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#21327;&#26041;&#24046;-&#34920;&#36798;&#33021;&#21147;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#20998;&#20026;&#20004;&#20010;&#32423;&#32852;&#22238;&#24402;&#38454;&#27573;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#19968;&#20010;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#21327;&#21464;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#19977;&#32500;&#21407;&#23376;&#31995;&#32479;&#30340;&#23545;&#31216;&#24615;&#65292;&#20135;&#29983;&#29702;&#35770;&#19978;&#30340;&#21327;&#21464;&#29305;&#24449;&#21644;&#22522;&#32447;&#21704;&#23494;&#39039;&#39044;&#27979;&#65292;&#24110;&#21161;&#31532;&#20108;&#38454;&#27573;&#23398;&#20064;&#21327;&#21464;&#24615;&#12290;&#21516;&#26102;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#38750;&#32447;&#24615;&#19977;&#32500;&#22270;&#24418;Transformer&#32593;&#32476;&#26469;&#36827;&#34892;&#19977;&#32500;&#21407;&#23376;&#31995;&#32479;&#30340;&#32467;&#26500;&#24314;&#27169;&#65292;&#23558;&#31532;&#19968;&#38454;&#27573;&#30340;&#36755;&#20986;&#31934;&#32454;&#21270;&#20026;&#20855;&#26377;&#26356;&#22909;&#34920;&#36798;&#33021;&#21147;&#30340;&#21704;&#23494;&#39039;&#39044;&#27979;&#12290;&#36890;&#36807;&#29702;&#35770;&#19978;&#30340;&#21327;&#21464;&#24615;&#21644;&#26356;&#22909;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#23545;&#21704;&#23494;&#39039;&#22238;&#24402;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning for Hamiltonian regression of quantum systems in material research necessitates satisfying the covariance laws, among which achieving SO(3)-equivariance without sacrificing the expressiveness of networks remains an elusive challenge due to the restriction to non-linear mappings on guaranteeing theoretical equivariance. To alleviate the covariance-expressiveness dilemma, we propose a hybrid framework with two cascaded regression stages. The first stage, with a theoretically-guaranteed covariant neural network modeling symmetry properties of 3D atom systems, yields theoretically covariant features and baseline Hamiltonian predictions, assisting the second stage in learning covariance. Meanwhile, the second stage, powered by a non-linear 3D graph Transformer network we propose for structural modeling of 3D atomic systems, refines the first stage's output as a fine-grained prediction of Hamiltonians with better expressiveness capability. The combination of a theoretically cov
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#20854;&#24378;&#22823;&#30340;&#22270;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#20132;&#36890;&#39046;&#22495;&#34920;&#29616;&#20986;&#20248;&#31168;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20132;&#36890;&#39044;&#27979;&#26041;&#38754;&#65292;&#20854;&#20182;&#39046;&#22495;&#20173;&#38656;&#26356;&#22810;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2401.00713</link><description>&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Graph Neural Networks in Intelligent Transportation Systems. (arXiv:2401.00713v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#20854;&#24378;&#22823;&#30340;&#22270;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#20132;&#36890;&#39046;&#22495;&#34920;&#29616;&#20986;&#20248;&#31168;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20132;&#36890;&#39044;&#27979;&#26041;&#38754;&#65292;&#20854;&#20182;&#39046;&#22495;&#20173;&#38656;&#26356;&#22810;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#23545;&#20110;&#25913;&#21892;&#20132;&#36890;&#25317;&#22581;&#12289;&#20943;&#23569;&#20132;&#36890;&#20107;&#25925;&#12289;&#20248;&#21270;&#22478;&#24066;&#35268;&#21010;&#31561;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20132;&#36890;&#32593;&#32476;&#30340;&#22797;&#26434;&#24615;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#26041;&#27861;&#21464;&#24471;&#19981;&#21463;&#37325;&#35270;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#19988;&#34987;&#35748;&#20026;&#26159;&#35768;&#22810;&#39046;&#22495;&#20013;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#20316;&#20026;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#19982;&#22270;&#30456;&#20851;&#30340;&#38382;&#39064;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#33258;2019&#24180;&#20197;&#26469;&#24050;&#32463;&#22312;ITS&#39046;&#22495;&#20013;&#23853;&#38706;&#22836;&#35282;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#23398;&#32773;&#20851;&#27880;GNN&#22312;&#20132;&#36890;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35813;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#20173;&#38598;&#20013;&#22312;&#20132;&#36890;&#39044;&#27979;&#26041;&#38754;&#65292;&#32780;&#20854;&#20182;ITS&#39046;&#22495;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#22478;&#24066;&#35268;&#21010;&#65292;&#20173;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent Transportation System (ITS) is vital in improving traffic congestion, reducing traffic accidents, optimizing urban planning, etc. However, due to the complexity of the traffic network, traditional machine learning and statistical methods are relegated to the background. With the advent of the artificial intelligence era, many deep learning frameworks have made remarkable progress in various fields and are now considered effective methods in many areas. As a deep learning method, Graph Neural Networks (GNNs) have emerged as a highly competitive method in the ITS field since 2019 due to their strong ability to model graph-related problems. As a result, more and more scholars pay attention to the applications of GNNs in transportation domains, which have shown excellent performance. However, most of the research in this area is still concentrated on traffic forecasting, while other ITS domains, such as autonomous vehicles and urban planning, still require more attention. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#30693;&#35782;&#22788;&#29702;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#29123;&#28903;&#31185;&#23398;&#20013;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;RAG&#26694;&#26550;&#65292;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#29123;&#28903;&#30740;&#31350;&#25968;&#25454;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#35745;&#31639;&#21644;&#32463;&#27982;&#24320;&#38144;&#65292;&#21516;&#26102;&#20248;&#21270;&#25968;&#25454;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.00544</link><description>&lt;p&gt;
&#28779;&#29123;&#31185;&#23398;&#20013;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#21487;&#38752;&#30693;&#35782;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Reliable Knowledge Processing Framework for Combustion Science using Foundation Models. (arXiv:2401.00544v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#38752;&#30340;&#30693;&#35782;&#22788;&#29702;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#29123;&#28903;&#31185;&#23398;&#20013;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;RAG&#26694;&#26550;&#65292;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#29123;&#28903;&#30740;&#31350;&#25968;&#25454;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#35745;&#31639;&#21644;&#32463;&#27982;&#24320;&#38144;&#65292;&#21516;&#26102;&#20248;&#21270;&#25968;&#25454;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#31185;&#23398;&#25968;&#25454;&#34701;&#21512;&#20013;&#65292;&#20197;&#29123;&#28903;&#31185;&#23398;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#36890;&#36807;&#25972;&#21512;&#22522;&#30784;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#26679;&#21270;&#29123;&#28903;&#30740;&#31350;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#23454;&#39564;&#30740;&#31350;&#12289;&#27169;&#25311;&#21644;&#25991;&#29486;&#31561;&#26041;&#38754;&#12290;&#29123;&#28903;&#30740;&#31350;&#30340;&#22810;&#26041;&#38754;&#24615;&#24378;&#35843;&#20102;&#30693;&#35782;&#22788;&#29702;&#22312;&#20174;&#20016;&#23500;&#30340;&#12289;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#26469;&#28304;&#20013;&#23548;&#33322;&#21644;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#25152;&#24320;&#21457;&#30340;&#26041;&#27861;&#22312;&#20248;&#21270;&#25968;&#25454;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20102;&#35745;&#31639;&#21644;&#32463;&#27982;&#24320;&#38144;&#12290;&#23427;&#21253;&#25324;&#25552;&#31034;&#24037;&#31243;&#21644;&#31163;&#32447;&#24320;&#28304;LLMs&#65292;&#20026;&#29992;&#25143;&#36873;&#25321;&#22522;&#30784;&#27169;&#22411;&#25552;&#20379;&#20102;&#33258;&#20027;&#24615;&#12290;&#26412;&#30740;&#31350;&#23545;&#25991;&#26412;&#20998;&#21106;&#31574;&#30053;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#36827;&#34892;&#20102;LLMs&#20043;&#38388;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#24182;&#25506;&#32034;&#20102;&#21508;&#31181;&#20248;&#21270;&#30340;&#25552;&#31034;&#26041;&#24335;&#65292;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research explores the integration of large language models (LLMs) into scientific data assimilation, focusing on combustion science as a case study. Leveraging foundational models integrated with Retrieval-Augmented Generation (RAG) framework, the study introduces an approach to process diverse combustion research data, spanning experimental studies, simulations, and literature. The multifaceted nature of combustion research emphasizes the critical role of knowledge processing in navigating and extracting valuable information from a vast and diverse pool of sources. The developed approach minimizes computational and economic expenses while optimizing data privacy and accuracy. It incorporates prompt engineering and offline open-source LLMs, offering user autonomy in selecting base models. The study provides a thorough examination of text segmentation strategies, conducts comparative studies between LLMs, and explores various optimized prompts to demonstrate the effectiveness of th
&lt;/p&gt;</description></item><item><title>AVRE&#26159;&#19968;&#31181;&#29992;&#20110;&#24418;&#24335;&#39564;&#35777;NextG&#36890;&#20449;&#21327;&#35758;&#30340;&#33258;&#21160;&#24314;&#27169;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#21327;&#35758;&#25551;&#36848;&#20026;&#20381;&#36182;&#22270;&#21644;&#24418;&#24335;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24314;&#31435;&#21487;&#37327;&#21270;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#22797;&#26434;&#36890;&#20449;&#21327;&#35758;&#30340;&#39564;&#35777;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.17353</link><description>&lt;p&gt;
&#36808;&#21521;NextG&#21327;&#35758;&#24418;&#24335;&#39564;&#35777;&#30340;&#33258;&#21160;&#24314;&#27169;&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Auto-Modeling of Formal Verification for NextG Protocols: A Multimodal cross- and self-attention Large Language Model Approach. (arXiv:2312.17353v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17353
&lt;/p&gt;
&lt;p&gt;
AVRE&#26159;&#19968;&#31181;&#29992;&#20110;&#24418;&#24335;&#39564;&#35777;NextG&#36890;&#20449;&#21327;&#35758;&#30340;&#33258;&#21160;&#24314;&#27169;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#21327;&#35758;&#25551;&#36848;&#20026;&#20381;&#36182;&#22270;&#21644;&#24418;&#24335;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24314;&#31435;&#21487;&#37327;&#21270;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#22797;&#26434;&#36890;&#20449;&#21327;&#35758;&#30340;&#39564;&#35777;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Auto-modeling of Formal Verification with Real-world Prompting for 5G and NextG protocols&#65288;AVRE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#35774;&#35745;&#29992;&#20110;&#24418;&#24335;&#39564;&#35777;Next Generation&#65288;NextG&#65289;&#36890;&#20449;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#32593;&#32476;&#21327;&#35758;&#35774;&#35745;&#21644;&#39564;&#35777;&#20013;&#26085;&#30410;&#22797;&#26434;&#21644;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;AVRE&#23558;&#21327;&#35758;&#25551;&#36848;&#36716;&#21270;&#20026;&#20381;&#36182;&#22270;&#21644;&#24418;&#24335;&#27169;&#22411;&#65292;&#39640;&#25928;&#22320;&#35299;&#20915;&#20102;&#27495;&#20041;&#38382;&#39064;&#24182;&#25429;&#25417;&#35774;&#35745;&#24847;&#22270;&#12290;&#31995;&#32479;&#36890;&#36807;&#20132;&#21449;&#21644;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;&#21464;&#21387;&#22120;&#27169;&#22411;&#19982;LLM&#38598;&#25104;&#65292;&#33258;&#20027;&#24314;&#31435;&#21487;&#37327;&#21270;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#32463;&#36807;HyFuzz&#23454;&#39564;&#24179;&#21488;&#30340;&#36845;&#20195;&#21453;&#39304;&#65292;AVRE&#26174;&#33879;&#25552;&#39640;&#20102;&#22797;&#26434;&#36890;&#20449;&#21327;&#35758;&#27491;&#24335;&#39564;&#35777;&#30340;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20026;&#39564;&#35777;&#22797;&#26434;&#30340;&#36890;&#20449;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;CAL&#30340;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;LL&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Auto-modeling of Formal Verification with Real-world Prompting for 5G and NextG protocols (AVRE), a novel system designed for the formal verification of Next Generation (NextG) communication protocols, addressing the increasing complexity and scalability challenges in network protocol design and verification. Utilizing Large Language Models (LLMs), AVRE transforms protocol descriptions into dependency graphs and formal models, efficiently resolving ambiguities and capturing design intent. The system integrates a transformer model with LLMs to autonomously establish quantifiable dependency relationships through cross- and self-attention mechanisms. Enhanced by iterative feedback from the HyFuzz experimental platform, AVRE significantly advances the accuracy and relevance of formal verification in complex communication protocols, offering a groundbreaking approach to validating sophisticated communication systems. We compare CAL's performance with state-of-the-art L
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#22521;&#20859;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#26412;&#22320;&#21644;&#36328;&#39046;&#22495;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#23545;&#26410;&#30693;&#20998;&#24067;&#39046;&#22495;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20808;&#39564;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20998;&#31163;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17300</link><description>&lt;p&gt;
&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36890;&#36807;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#25913;&#21892;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Intrusion Detection with Domain-Invariant Representation Learning in Latent Space. (arXiv:2312.17300v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#22521;&#20859;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#26412;&#22320;&#21644;&#36328;&#39046;&#22495;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#23545;&#26410;&#30693;&#20998;&#24067;&#39046;&#22495;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20808;&#39564;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20998;&#31163;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#32858;&#28966;&#20110;&#21033;&#29992;&#26469;&#33258;&#20855;&#26377;&#20016;&#23500;&#35757;&#32451;&#25968;&#25454;&#21644;&#26631;&#31614;&#30340;&#22810;&#20010;&#30456;&#20851;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#23545;&#26410;&#30693;&#20998;&#24067;&#65288;IN&#65289;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#39046;&#22495;&#30340;&#25512;&#29702;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20174;&#36328;&#36234;&#22810;&#20010;&#39046;&#22495;&#30340;&#29305;&#24449;&#20013;&#22521;&#20859;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;&#21253;&#25324;&#26412;&#22320;&#21644;&#36328;&#39046;&#22495;&#65292;&#20197;&#22686;&#24378;&#23545;IN&#21644;OOD&#39046;&#22495;&#30340;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#26368;&#23567;&#21270;&#20808;&#39564;&#19982;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20998;&#31163;&#28508;&#22312;&#31354;&#38388;&#65292;&#26377;&#25928;&#28040;&#38500;&#34394;&#20551;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#32508;&#21512;&#32780;&#35328;&#65292;&#32852;&#21512;&#20248;&#21270;&#23558;&#20419;&#36827;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#20998;&#31867;&#25351;&#26631;&#35780;&#20272;&#27169;&#22411;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#33021;&#65292;&#23545;&#27604;&#20102;&#29616;&#20195;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization focuses on leveraging knowledge from multiple related domains with ample training data and labels to enhance inference on unseen in-distribution (IN) and out-of-distribution (OOD) domains. In our study, we introduce a two-phase representation learning technique using multi-task learning. This approach aims to cultivate a latent space from features spanning multiple domains, encompassing both native and cross-domains, to amplify generalization to IN and OOD territories. Additionally, we attempt to disentangle the latent space by minimizing the mutual information between the prior and latent space, effectively de-correlating spurious feature correlations. Collectively, the joint optimization will facilitate domain-invariant feature learning. We assess the model's efficacy across multiple cybersecurity datasets, using standard classification metrics on both unseen IN and OOD sets, and juxtapose the results with contemporary domain generalization methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20445;&#25345;&#36523;&#20221;&#30340;&#12289;&#30643;&#23380;&#23610;&#23544;&#21464;&#21270;&#30340;&#34425;&#33180;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21512;&#25104;&#19981;&#21516;&#30643;&#23380;&#23610;&#23544;&#30340;&#34425;&#33180;&#22270;&#20687;&#65292;&#20195;&#34920;&#19981;&#23384;&#22312;&#30340;&#36523;&#20221;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#25345;&#36523;&#20221;&#30340;&#21516;&#26102;&#36827;&#34892;&#38750;&#32447;&#24615;&#32441;&#29702;&#21464;&#24418;&#12290;</title><link>http://arxiv.org/abs/2312.12028</link><description>&lt;p&gt;
EyePreserve: &#20445;&#25345;&#36523;&#20221;&#30340;&#34425;&#33180;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
EyePreserve: Identity-Preserving Iris Synthesis. (arXiv:2312.12028v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20445;&#25345;&#36523;&#20221;&#30340;&#12289;&#30643;&#23380;&#23610;&#23544;&#21464;&#21270;&#30340;&#34425;&#33180;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21512;&#25104;&#19981;&#21516;&#30643;&#23380;&#23610;&#23544;&#30340;&#34425;&#33180;&#22270;&#20687;&#65292;&#20195;&#34920;&#19981;&#23384;&#22312;&#30340;&#36523;&#20221;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#25345;&#36523;&#20221;&#30340;&#21516;&#26102;&#36827;&#34892;&#38750;&#32447;&#24615;&#32441;&#29702;&#21464;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#30340;&#30643;&#23380;&#23610;&#23544;&#33539;&#22260;&#20869;&#20445;&#25345;&#36523;&#20221;&#30340;&#21516;&#36523;&#20221;&#29983;&#29289;&#29305;&#24449;&#34425;&#33180;&#22270;&#20687;&#30340;&#21512;&#25104;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#34425;&#33180;&#32908;&#32905;&#25910;&#32553;&#26426;&#21046;&#65292;&#38656;&#35201;&#23558;&#34425;&#33180;&#38750;&#32447;&#24615;&#32441;&#29702;&#21464;&#24418;&#27169;&#22411;&#23884;&#20837;&#21040;&#21512;&#25104;&#27969;&#31243;&#20013;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20445;&#25345;&#36523;&#20221;&#30340;&#12289;&#30643;&#23380;&#23610;&#23544;&#21464;&#21270;&#30340;&#34425;&#33180;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#19981;&#21516;&#30643;&#23380;&#23610;&#23544;&#30340;&#34425;&#33180;&#22270;&#20687;&#65292;&#20195;&#34920;&#19981;&#23384;&#22312;&#30340;&#36523;&#20221;&#65292;&#24182;&#33021;&#22815;&#22312;&#32473;&#23450;&#30446;&#26631;&#34425;&#33180;&#22270;&#20687;&#30340;&#20998;&#21106;&#25513;&#33180;&#19979;&#38750;&#32447;&#24615;&#22320;&#21464;&#24418;&#29616;&#26377;&#20027;&#20307;&#30340;&#34425;&#33180;&#22270;&#20687;&#32441;&#29702;&#12290;&#34425;&#33180;&#35782;&#21035;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#21464;&#24418;&#27169;&#22411;&#19981;&#20165;&#22312;&#25913;&#21464;&#30643;&#23380;&#23610;&#23544;&#26102;&#20445;&#25345;&#36523;&#20221;&#65292;&#32780;&#19988;&#22312;&#30643;&#23380;&#23610;&#23544;&#26377;&#26174;&#33879;&#24046;&#24322;&#30340;&#21516;&#36523;&#20221;&#34425;&#33180;&#26679;&#26412;&#20043;&#38388;&#25552;&#20379;&#26356;&#22909;&#30340;&#30456;&#20284;&#24230;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32447;&#24615;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesis of same-identity biometric iris images, both for existing and non-existing identities while preserving the identity across a wide range of pupil sizes, is complex due to intricate iris muscle constriction mechanism, requiring a precise model of iris non-linear texture deformations to be embedded into the synthesis pipeline. This paper presents the first method of fully data-driven, identity-preserving, pupil size-varying s ynthesis of iris images. This approach is capable of synthesizing images of irises with different pupil sizes representing non-existing identities as well as non-linearly deforming the texture of iris images of existing subjects given the segmentation mask of the target iris image. Iris recognition experiments suggest that the proposed deformation model not only preserves the identity when changing the pupil size but offers better similarity between same-identity iris samples with significant differences in pupil size, compared to state-of-the-art linear an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20869;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#26426;&#22120;&#20154;&#30340;&#21709;&#24212;&#26469;&#20272;&#35745;&#22806;&#37096;&#29366;&#24577;&#65292;&#36825;&#23545;&#20110;&#20581;&#22766;&#30340;&#36816;&#21160;&#25511;&#21046;&#38750;&#24120;&#37325;&#35201;&#12290;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#20248;&#21270;&#23884;&#20837;&#34920;&#31034;&#65292;&#20351;&#20854;&#25509;&#36817;&#26426;&#22120;&#20154;&#30340;&#21518;&#32487;&#29366;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#26426;&#22120;&#20154;&#30340;&#22266;&#26377;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2312.11460</link><description>&lt;p&gt;
&#28151;&#21512;&#20869;&#27169;&#65306;&#36890;&#36807;&#27169;&#25311;&#26426;&#22120;&#20154;&#21709;&#24212;&#23398;&#20064;&#25935;&#25463;&#33151;&#37096;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response. (arXiv:2312.11460v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#20869;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#26426;&#22120;&#20154;&#30340;&#21709;&#24212;&#26469;&#20272;&#35745;&#22806;&#37096;&#29366;&#24577;&#65292;&#36825;&#23545;&#20110;&#20581;&#22766;&#30340;&#36816;&#21160;&#25511;&#21046;&#38750;&#24120;&#37325;&#35201;&#12290;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#20248;&#21270;&#23884;&#20837;&#34920;&#31034;&#65292;&#20351;&#20854;&#25509;&#36817;&#26426;&#22120;&#20154;&#30340;&#21518;&#32487;&#29366;&#24577;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#26426;&#22120;&#20154;&#30340;&#22266;&#26377;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#22766;&#30340;&#36816;&#21160;&#25511;&#21046;&#20381;&#36182;&#20110;&#20934;&#30830;&#30340;&#29366;&#24577;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#20256;&#24863;&#22120;&#21482;&#33021;&#25552;&#20379;&#37096;&#20998;&#21644;&#22024;&#26434;&#30340;&#35266;&#27979;&#65292;&#20351;&#24471;&#20272;&#35745;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22806;&#37096;&#29366;&#24577;&#65292;&#22914;&#22320;&#24418;&#25705;&#25830;&#21644;&#39640;&#31243;&#22270;&#12290;&#21463;&#32463;&#20856;&#30340;&#20869;&#27169;&#25511;&#21046;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#22806;&#37096;&#29366;&#24577;&#35270;&#20026;&#24178;&#25200;&#65292;&#24182;&#24341;&#20837;&#28151;&#21512;&#20869;&#27169;&#65288;HIM&#65289;&#26469;&#26681;&#25454;&#26426;&#22120;&#20154;&#30340;&#21709;&#24212;&#26469;&#20272;&#35745;&#23427;&#20204;&#12290;&#25105;&#20204;&#23558;&#21709;&#24212;&#31216;&#20026;&#28151;&#21512;&#20869;&#23884;&#34920;&#31034;&#65292;&#23427;&#21253;&#21547;&#20102;&#26426;&#22120;&#20154;&#30340;&#26174;&#24335;&#36895;&#24230;&#21644;&#38544;&#24335;&#31283;&#23450;&#24615;&#34920;&#31034;&#65292;&#20998;&#21035;&#23545;&#24212;&#20110;&#36816;&#21160;&#20219;&#21153;&#30340;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#65306;&#26174;&#24335;&#36861;&#36394;&#36895;&#24230;&#21644;&#38544;&#24335;&#32500;&#25345;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#20248;&#21270;&#23884;&#20837;&#24335;&#34920;&#31034;&#65292;&#20351;&#20854;&#25509;&#36817;&#26426;&#22120;&#20154;&#30340;&#21518;&#32487;&#29366;&#24577;&#65292;&#20854;&#20013;&#33258;&#28982;&#23884;&#20837;&#20102;&#21709;&#24212;&#12290;HIM&#20855;&#26377;&#20960;&#20010;&#21560;&#24341;&#20154;&#30340;&#22909;&#22788;&#65306;&#23427;&#21482;&#38656;&#35201;&#26426;&#22120;&#20154;&#30340;&#22266;&#26377;&#24863;&#30693;&#65292;&#21363;&#20851;&#33410;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust locomotion control depends on accurate state estimations. However, the sensors of most legged robots can only provide partial and noisy observations, making the estimation particularly challenging, especially for external states like terrain frictions and elevation maps. Inspired by the classical Internal Model Control principle, we consider these external states as disturbances and introduce Hybrid Internal Model (HIM) to estimate them according to the response of the robot. The response, which we refer to as the hybrid internal embedding, contains the robot's explicit velocity and implicit stability representation, corresponding to two primary goals for locomotion tasks: explicitly tracking velocity and implicitly maintaining stability. We use contrastive learning to optimize the embedding to be close to the robot's successor state, in which the response is naturally embedded. HIM has several appealing benefits: It only needs the robot's proprioceptions, i.e., those from joint
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#19968;GPU&#19978;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2312.10144</link><description>&lt;p&gt;
&#21333;&#19968;GPU&#19978;&#30340;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#19968;GPU&#19978;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#20849;&#20139;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#30340;&#21333;&#19968;&#28508;&#22312;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#29616;&#26377;&#30340;&#22312;&#22823;&#37327;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#24212;&#35813;&#33021;&#22815;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#20174;&#21333;&#27169;&#24577;&#27169;&#22411;&#20013;&#21019;&#24314;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuseMix&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#22686;&#24378;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#20219;&#24847;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25805;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;FuseMix&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21644;&#38899;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#22312;Flickr30K&#30340;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#20013;&#27604;CLIP&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#32422;600&#20493;&#65292;&#32780;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of multimodal alignment is to learn a single latent space that is shared between multimodal inputs. The most powerful models in this space have been trained using massive datasets of paired inputs and large-scale computational resources, making them prohibitively expensive to train in many practical scenarios. We surmise that existing unimodal encoders pre-trained on large amounts of unimodal data should provide an effective bootstrap to create multimodal models from unimodal ones at much lower costs. We therefore propose FuseMix, a multimodal augmentation scheme that operates on the latent spaces of arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal alignment, we achieve competitive performance -- and in certain cases outperform state-of-the art methods -- in both image-text and audio-text retrieval, with orders of magnitude less compute and data: for example, we outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \! 600\times$ fewer GP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#25511;&#21046;&#22120;&#31867;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20854;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#24120;&#35265;&#25511;&#21046;&#22120;&#20013;&#30340;&#21487;&#39564;&#35777;&#24615;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;&#35813;&#25511;&#21046;&#22120;&#31867;&#20284;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#38382;&#39064;&#30340;&#20108;&#27425;&#35268;&#21010;&#27714;&#35299;&#22120;&#65292;&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#23646;&#24615;&#65292;&#24182;&#19988;&#22312;&#25511;&#21046;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#19982;&#20854;&#20182;&#25511;&#21046;&#22120;&#30456;&#23218;&#32654;&#12290;&#21516;&#26102;&#65292;&#35813;&#25511;&#21046;&#22120;&#30340;&#35745;&#31639;&#25928;&#29575;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2312.05332</link><description>&lt;p&gt;
&#28040;&#38500;&#24046;&#36317;&#65306;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#21487;&#39564;&#35777;&#27169;&#22411;&#26080;&#20851;&#20108;&#27425;&#35268;&#21010;&#25511;&#21046;&#22120;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming Controllers Inspired by Model Predictive Control. (arXiv:2312.05332v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#25511;&#21046;&#22120;&#31867;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20854;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#24120;&#35265;&#25511;&#21046;&#22120;&#20013;&#30340;&#21487;&#39564;&#35777;&#24615;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#38480;&#21046;&#12290;&#35813;&#25511;&#21046;&#22120;&#31867;&#20284;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#38382;&#39064;&#30340;&#20108;&#27425;&#35268;&#21010;&#27714;&#35299;&#22120;&#65292;&#20855;&#26377;&#21487;&#39564;&#35777;&#30340;&#23646;&#24615;&#65292;&#24182;&#19988;&#22312;&#25511;&#21046;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#19982;&#20854;&#20182;&#25511;&#21046;&#22120;&#30456;&#23218;&#32654;&#12290;&#21516;&#26102;&#65292;&#35813;&#25511;&#21046;&#22120;&#30340;&#35745;&#31639;&#25928;&#29575;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#25511;&#21046;&#22120;&#31867;&#65292;&#21463;&#21040;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#30340;&#21551;&#21457;&#12290;&#35813;&#25511;&#21046;&#22120;&#31867;&#20284;&#20110;&#32447;&#24615;MPC&#38382;&#39064;&#30340;&#20108;&#27425;&#35268;&#21010;&#65288;QP&#65289;&#27714;&#35299;&#22120;&#65292;&#20294;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#26159;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#20174;&#31995;&#32479;&#27169;&#22411;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#24120;&#35265;&#25511;&#21046;&#22120;&#20013;&#20351;&#29992;MLP&#25110;&#20854;&#20182;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;DRL&#30340;&#21487;&#39564;&#35777;&#24615;&#21644;&#24615;&#33021;&#20445;&#35777;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#25152;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#20855;&#26377;&#19982;MPC&#31867;&#20284;&#30340;&#25345;&#32493;&#21487;&#34892;&#24615;&#21644;&#28176;&#36817;&#31283;&#23450;&#24615;&#31561;&#21487;&#39564;&#35777;&#23646;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#22120;&#22312;&#25511;&#21046;&#24615;&#33021;&#19978;&#19982;MPC&#21644;MLP&#25511;&#21046;&#22120;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#23545;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#21644;&#22122;&#22768;&#20855;&#26377;&#26356;&#20248;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#22120;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26126;&#26174;&#20248;&#20110;MPC&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new class of parameterized controllers, drawing inspiration from Model Predictive Control (MPC). The controller resembles a Quadratic Programming (QP) solver of a linear MPC problem, with the parameters of the controller being trained via Deep Reinforcement Learning (DRL) rather than derived from system models. This approach addresses the limitations of common controllers with Multi-Layer Perceptron (MLP) or other general neural network architecture used in DRL, in terms of verifiability and performance guarantees, and the learned controllers possess verifiable properties like persistent feasibility and asymptotic stability akin to MPC. On the other hand, numerical examples illustrate that the proposed controller empirically matches MPC and MLP controllers in terms of control performance and has superior robustness against modeling uncertainty and noises. Furthermore, the proposed controller is significantly more computationally efficient compared to MPC a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25552;&#20986;&#20102;&#27700;&#21360;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20351;&#20854;&#27169;&#20223;&#20351;&#29992;&#35299;&#30721;&#27700;&#21360;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30452;&#25509;&#23398;&#20064;&#29983;&#25104;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#27700;&#21360;&#30340;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2312.04469</link><description>&lt;p&gt;
&#35770;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#21487;&#23398;&#20064;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Learnability of Watermarks for Language Models. (arXiv:2312.04469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25552;&#20986;&#20102;&#27700;&#21360;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20351;&#20854;&#27169;&#20223;&#20351;&#29992;&#35299;&#30721;&#27700;&#21360;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30452;&#25509;&#23398;&#20064;&#29983;&#25104;&#27700;&#21360;&#30340;&#33021;&#21147;&#65292;&#36825;&#23545;&#20110;&#27700;&#21360;&#30340;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#27700;&#21360;&#21487;&#20197;&#23454;&#29616;&#23545;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#30340;&#32479;&#35745;&#26816;&#27979;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#36127;&#36131;&#20219;&#37096;&#32626;&#20013;&#12290;&#29616;&#26377;&#30340;&#27700;&#21360;&#31574;&#30053;&#36890;&#36807;&#25913;&#21464;&#29616;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#26469;&#25805;&#20316;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#23398;&#20064;&#29983;&#25104;&#27700;&#21360;&#30340;&#33021;&#21147;&#23558;&#23545;&#27700;&#21360;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#39318;&#20808;&#65292;&#23398;&#20064;&#24471;&#21040;&#30340;&#27700;&#21360;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#33021;&#33258;&#28982;&#29983;&#25104;&#24102;&#27700;&#21360;&#25991;&#26412;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#20351;&#24320;&#25918;&#27169;&#22411;&#20063;&#33021;&#20174;&#27700;&#21360;&#20013;&#21463;&#30410;&#12290;&#20854;&#27425;&#65292;&#22914;&#26524;&#27700;&#21360;&#29992;&#20110;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#26469;&#28304;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#20266;&#36896;&#27700;&#21360;&#24182;&#29983;&#25104;&#26377;&#23475;&#30340;&#24102;&#27700;&#21360;&#25991;&#26412;&#26469;&#25439;&#23475;&#21463;&#23475;&#27169;&#22411;&#30340;&#22768;&#35465;&#12290;&#20026;&#20102;&#30740;&#31350;&#27700;&#21360;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27700;&#21360;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#20351;&#20854;&#34892;&#20026;&#31867;&#20284;&#20110;&#20351;&#29992;&#22522;&#20110;&#35299;&#30721;&#30340;&#27700;&#21360;&#30340;&#25945;&#24072;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermarking of language model outputs enables statistical detection of model-generated text, which has many applications in the responsible deployment of language models. Existing watermarking strategies operate by altering the decoder of an existing language model, and the ability for a language model to directly learn to generate the watermark would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, allowing for open models to benefit from watermarking. Second, if watermarking is used to determine the provenance of generated text, an adversary can hurt the reputation of a victim model by spoofing its watermark and generating damaging watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#26657;&#20934;&#20250;&#20808;&#22686;&#21152;&#32780;&#21518;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#20027;&#35201;&#20986;&#29616;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#31561;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#25552;&#31034;&#38656;&#35201;&#38024;&#23545;&#21487;&#38752;&#24615;&#22330;&#26223;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.04021</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26657;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Calibration of In-context Learning. (arXiv:2312.04021v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#26657;&#20934;&#20250;&#20808;&#22686;&#21152;&#32780;&#21518;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#20027;&#35201;&#20986;&#29616;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#31561;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#25552;&#31034;&#38656;&#35201;&#38024;&#23545;&#21487;&#38752;&#24615;&#22330;&#26223;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#29616;&#20195;LMs&#26657;&#20934;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;LMs&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#22312;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#26368;&#21021;&#20250;&#20986;&#29616;&#22686;&#21152;&#30340;&#26657;&#20934;&#35823;&#24046;&#65292;&#28982;&#21518;&#25165;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#26657;&#20934;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#24448;&#24448;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20197;&#25552;&#39640;&#21487;&#29992;&#24615;&#20026;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#22914;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#65292;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#36825;&#34920;&#26126;&#22312;&#26399;&#26395;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#22330;&#26223;&#20013;&#21487;&#33021;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.
&lt;/p&gt;</description></item><item><title>OpenVoice&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#35821;&#38899;&#20811;&#38534;&#26041;&#27861;&#65292;&#21487;&#20197;&#22797;&#21046;&#21442;&#32771;&#35762;&#35805;&#32773;&#30340;&#22768;&#38899;&#24182;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#29983;&#25104;&#35821;&#38899;&#12290;&#23427;&#20855;&#26377;&#28789;&#27963;&#30340;&#35821;&#38899;&#39118;&#26684;&#25511;&#21046;&#21644;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#35821;&#38899;&#20811;&#38534;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.01479</link><description>&lt;p&gt;
OpenVoice: &#22810;&#21151;&#33021;&#21363;&#26102;&#35821;&#38899;&#20811;&#38534;
&lt;/p&gt;
&lt;p&gt;
OpenVoice: Versatile Instant Voice Cloning. (arXiv:2312.01479v5 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01479
&lt;/p&gt;
&lt;p&gt;
OpenVoice&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#35821;&#38899;&#20811;&#38534;&#26041;&#27861;&#65292;&#21487;&#20197;&#22797;&#21046;&#21442;&#32771;&#35762;&#35805;&#32773;&#30340;&#22768;&#38899;&#24182;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#29983;&#25104;&#35821;&#38899;&#12290;&#23427;&#20855;&#26377;&#28789;&#27963;&#30340;&#35821;&#38899;&#39118;&#26684;&#25511;&#21046;&#21644;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#35821;&#38899;&#20811;&#38534;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;OpenVoice&#65292;&#19968;&#31181;&#22810;&#21151;&#33021;&#35821;&#38899;&#20811;&#38534;&#26041;&#27861;&#65292;&#21482;&#38656;&#19968;&#23567;&#27573;&#21442;&#32771;&#35762;&#35805;&#32773;&#30340;&#38899;&#39057;&#29255;&#27573;&#21363;&#21487;&#22797;&#21046;&#20182;&#20204;&#30340;&#22768;&#38899;&#24182;&#29983;&#25104;&#22810;&#31181;&#35821;&#35328;&#30340;&#35821;&#38899;&#12290;OpenVoice&#22312;&#20197;&#19979;&#39046;&#22495;&#20013;&#35299;&#20915;&#20102;&#19968;&#20123;&#24320;&#25918;&#24615;&#25361;&#25112;&#65306;1&#65289;&#28789;&#27963;&#30340;&#35821;&#38899;&#39118;&#26684;&#25511;&#21046;&#12290;OpenVoice&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#35821;&#38899;&#39118;&#26684;&#65292;&#21253;&#25324;&#24773;&#24863;&#12289;&#21475;&#38899;&#12289;&#33410;&#22863;&#12289;&#20572;&#39039;&#21644;&#35821;&#35843;&#65292;&#21516;&#26102;&#22797;&#21046;&#21442;&#32771;&#35762;&#35805;&#32773;&#30340;&#38899;&#33394;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22312;&#20811;&#38534;&#21518;&#28789;&#27963;&#25805;&#25511;&#35821;&#38899;&#39118;&#26684;&#12290;2&#65289;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#35821;&#38899;&#20811;&#38534;&#12290;OpenVoice&#21487;&#20197;&#22312;&#26410;&#21253;&#21547;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#38598;&#20013;&#30340;&#35821;&#35328;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#30340;&#36328;&#35821;&#35328;&#35821;&#38899;&#20811;&#38534;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#21253;&#21547;&#25152;&#26377;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22810;&#35762;&#35805;&#32773;&#22810;&#35821;&#35328;(MSML)&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce OpenVoice, a versatile voice cloning approach that requires only a short audio clip from the reference speaker to replicate their voice and generate speech in multiple languages. OpenVoice represents a significant advancement in addressing the following open challenges in the field: 1) Flexible Voice Style Control. OpenVoice enables granular control over voice styles, including emotion, accent, rhythm, pauses, and intonation, in addition to replicating the tone color of the reference speaker. The voice styles are not directly copied from and constrained by the style of the reference speaker. Previous approaches lacked the ability to flexibly manipulate voice styles after cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves zero-shot cross-lingual voice cloning for languages not included in the massive-speaker training set. Unlike previous approaches, which typically require extensive massive-speaker multi-lingual (MSML) dataset for all languages, OpenVoice
&lt;/p&gt;</description></item><item><title>SASSL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.01187</link><description>&lt;p&gt;
SASSL:&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#22686;&#24378;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer. (arXiv:2312.01187v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01187
&lt;/p&gt;
&lt;p&gt;
SASSL&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#32806;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#26469;&#20174;&#26080;&#26631;&#31614;&#22270;&#20687;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#34920;&#24449;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#22686;&#24378;&#27969;&#27700;&#32447;&#21253;&#25324;&#20102;&#21508;&#31181;&#21407;&#22987;&#30340;&#36716;&#25442;&#65292;&#20294;&#36890;&#24120;&#24573;&#30053;&#20102;&#33258;&#28982;&#22270;&#20687;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#22686;&#24378;&#26679;&#26412;&#21487;&#33021;&#26174;&#31034;&#20986;&#36864;&#21270;&#30340;&#35821;&#20041;&#20449;&#24687;&#21644;&#20302;&#39118;&#26684;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;&#33258;&#30417;&#30563;&#34920;&#24449;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SASSL&#30340;&#26032;&#22411;&#22686;&#24378;&#25216;&#26415;&#65292;&#23427;&#22522;&#20110;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#12290;&#35813;&#26041;&#27861;&#23558;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#21644;&#39118;&#26684;&#23646;&#24615;&#35299;&#32806;&#65292;&#24182;&#20165;&#23545;&#39118;&#26684;&#24212;&#29992;&#36716;&#25442;&#65292;&#20445;&#25345;&#20869;&#23481;&#65292;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#26356;&#22909;&#22320;&#20445;&#30041;&#23427;&#20204;&#30340;&#35821;&#20041;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#24191;&#20026;&#25509;&#21463;&#30340;MoCo v2&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;ImageNet&#19978;&#30340;top-1&#20998;&#31867;&#24615;&#33021;&#25552;&#21319;&#36229;&#36807;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning relies heavily on data augmentation to extract meaningful representations from unlabeled images. While existing state-of-the-art augmentation pipelines incorporate a wide range of primitive transformations, these often disregard natural image structure. Thus, augmented samples can exhibit degraded semantic information and low stylistic diversity, affecting downstream performance of self-supervised representations. To overcome this, we propose SASSL: Style Augmentations for Self Supervised Learning, a novel augmentation technique based on Neural Style Transfer. The method decouples semantic and stylistic attributes in images and applies transformations exclusively to the style while preserving content, generating diverse augmented samples that better retain their semantic properties. Experimental results show our technique achieves a top-1 classification performance improvement of more than 2% on ImageNet compared to the well-established MoCo v2. We also measure
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;DeepTreeGANv2&#65292;&#23427;&#26159;DeepTreeGAN&#30340;&#26174;&#33879;&#25193;&#23637;&#65292;&#33021;&#22815;&#20197;&#26641;&#29366;&#26041;&#24335;&#36845;&#20195;&#22320;&#32858;&#21512;&#28857;&#20113;&#65292;&#27169;&#25311;&#31890;&#23376;&#19982;&#25506;&#27979;&#22120;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#22312;&#30701;&#26102;&#38388;&#20869;&#29983;&#25104;&#22823;&#22411;&#28857;&#20113;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2312.00042</link><description>&lt;p&gt;
DeepTreeGANv2&#65306;&#36845;&#20195;&#27744;&#21270;&#30340;&#28857;&#20113;&#29983;&#25104;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DeepTreeGANv2: Iterative Pooling of Point Clouds. (arXiv:2312.00042v2 [physics.data-an] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00042
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;DeepTreeGANv2&#65292;&#23427;&#26159;DeepTreeGAN&#30340;&#26174;&#33879;&#25193;&#23637;&#65292;&#33021;&#22815;&#20197;&#26641;&#29366;&#26041;&#24335;&#36845;&#20195;&#22320;&#32858;&#21512;&#28857;&#20113;&#65292;&#27169;&#25311;&#31890;&#23376;&#19982;&#25506;&#27979;&#22120;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#22312;&#30701;&#26102;&#38388;&#20869;&#29983;&#25104;&#22823;&#22411;&#28857;&#20113;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#33021;&#29289;&#29702;&#23398;&#20013;&#65292;&#20026;&#20102;&#27169;&#25311;&#31890;&#23376;&#19982;&#25506;&#27979;&#22120;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#38656;&#35201;&#36827;&#34892;&#35814;&#32454;&#32780;&#32791;&#26102;&#30340;&#27169;&#25311;&#12290;&#20026;&#20102;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#32469;&#36807;&#36825;&#20123;&#27169;&#25311;&#65292;&#38656;&#35201;&#22312;&#30701;&#26102;&#38388;&#20869;&#29983;&#25104;&#22823;&#22411;&#28857;&#20113;&#65292;&#21516;&#26102;&#27491;&#30830;&#24314;&#27169;&#31890;&#23376;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#12290;&#31890;&#23376;&#38453;&#21015;&#22266;&#26377;&#22320;&#22522;&#20110;&#26641;&#29366;&#36807;&#31243;&#65292;&#22240;&#20026;&#27599;&#20010;&#31890;&#23376;&#26159;&#19978;&#19968;&#20195;&#31890;&#23376;&#30340;&#34928;&#21464;&#25110;&#25506;&#27979;&#22120;&#30456;&#20114;&#20316;&#29992;&#20135;&#29983;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepTreeGANv2&#65292;&#36825;&#26159;DeepTreeGAN&#30340;&#26174;&#33879;&#25193;&#23637;&#65292;&#20855;&#26377;&#19968;&#20010;&#35780;&#35770;&#32773;&#65292;&#33021;&#22815;&#20197;&#26641;&#29366;&#26041;&#24335;&#36845;&#20195;&#22320;&#32858;&#21512;&#36825;&#26679;&#30340;&#28857;&#20113;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#37325;&#29616;&#22797;&#26434;&#20998;&#24067;&#65292;&#24182;&#22312;&#20844;&#20849;&#30340;JetNet 150&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In High Energy Physics, detailed and time-consuming simulations are used for particle interactions with detectors. To bypass these simulations with a generative model, the generation of large point clouds in a short time is required, while the complex dependencies between the particles must be correctly modelled. Particle showers are inherently tree-based processes, as each particle is produced by the decay or detector interaction of a particle of the previous generation. In this work, we present a significant extension to DeepTreeGAN, featuring a critic, that is able to aggregate such point clouds iteratively in a tree-based manner. We show that this model can reproduce complex distributions, and we evaluate its performance on the public JetNet 150 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#30340;&#23454;&#26102;&#22312;&#32447;&#32929;&#31080;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;&#20102;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#25968;&#23383;&#32929;&#31080;&#25968;&#25454;&#21644;&#23450;&#24615;&#25991;&#26412;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#22810;&#20010;&#20844;&#21496;&#21644;&#36947;&#29756;&#26031;&#24037;&#19994;&#24179;&#22343;&#25351;&#25968;&#30340;&#25968;&#25454;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#25968;&#25454;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2311.15218</link><description>&lt;p&gt;
&#23454;&#26102;&#22312;&#32447;&#32929;&#31080;&#39044;&#27979;&#21033;&#29992;&#32508;&#21512;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and Qualitative Analysis. (arXiv:2311.15218v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#30340;&#23454;&#26102;&#22312;&#32447;&#32929;&#31080;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;&#20102;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#25968;&#23383;&#32929;&#31080;&#25968;&#25454;&#21644;&#23450;&#24615;&#25991;&#26412;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#22810;&#20010;&#20844;&#21496;&#21644;&#36947;&#29756;&#26031;&#24037;&#19994;&#24179;&#22343;&#25351;&#25968;&#30340;&#25968;&#25454;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#25968;&#25454;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#24212;&#29992;&#24050;&#32463;&#21464;&#24471;&#38750;&#24120;&#24120;&#35265;&#65292;&#23588;&#20854;&#22312;&#32929;&#31080;&#24066;&#22330;&#39044;&#27979;&#20013;&#26356;&#26159;&#22914;&#27492;&#12290;&#32929;&#31080;&#24066;&#22330;&#39640;&#24230;&#27874;&#21160;&#65292;&#20840;&#29699;&#27599;&#20998;&#38047;&#37117;&#20250;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#12290;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#25928;&#30340;&#26234;&#33021;&#20449;&#24687;&#21313;&#20998;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#25968;&#23383;&#32929;&#31080;&#25968;&#25454;&#19982;&#23450;&#24615;&#25991;&#26412;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#21487;&#33021;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21490;&#26080;&#21069;&#20363;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#20174;&#26032;&#38395;&#26723;&#26696;&#12289;&#30005;&#35270;&#26032;&#38395;&#23383;&#24149;&#12289;&#24191;&#25773;&#25991;&#26412;&#12289;&#25512;&#25991;&#12289;&#27599;&#26085;&#36130;&#32463;&#25253;&#32440;&#31561;&#22788;&#25910;&#38598;&#21040;&#20102;&#21253;&#25324;&#25216;&#26415;&#21644;&#22522;&#26412;&#25968;&#25454;&#20197;&#21450;&#24773;&#24863;&#25968;&#25454;&#12290;&#29992;&#20110;&#24773;&#24863;&#25552;&#21462;&#30340;&#25991;&#26412;&#25968;&#25454;&#24635;&#20849;&#36229;&#36807;140&#19975;&#26465;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;2018&#24180;1&#26376;&#21040;2022&#24180;12&#26376;&#20026;&#26399;&#19968;&#24180;&#30340;&#20843;&#23478;&#20195;&#34920;&#19981;&#21516;&#20135;&#19994;&#37096;&#38376;&#30340;&#20844;&#21496;&#30340;&#27599;&#26085;&#25968;&#25454;&#65292;&#20197;&#21450;&#36947;&#29756;&#26031;&#24037;&#19994;&#24179;&#22343;&#25351;&#25968;&#65288;DJIA&#65289;&#25972;&#20307;&#30340;&#25968;&#25454;&#12290;&#32508;&#21512;&#30340;&#22522;&#26412;&#25968;&#25454;&#21644;&#25216;&#26415;&#25968;&#25454;&#21487;&#30452;&#25509;&#29992;&#20110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of Machine learning to finance has become a familiar approach, even more so in stock market forecasting. The stock market is highly volatile, and huge amounts of data are generated every minute globally. The extraction of effective intelligence from this data is of critical importance. However, a collaboration of numerical stock data with qualitative text data can be a challenging task. In this work, we accomplish this by providing an unprecedented, publicly available dataset with technical and fundamental data and sentiment that we gathered from news archives, TV news captions, radio transcripts, tweets, daily financial newspapers, etc. The text data entries used for sentiment extraction total more than 1.4 Million. The dataset consists of daily entries from January 2018 to December 2022 for eight companies representing diverse industrial sectors and the Dow Jones Industrial Average (DJIA) as a whole. Holistic Fundamental and Technical data is provided training ready f
&lt;/p&gt;</description></item><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;</title><link>http://arxiv.org/abs/2310.10477</link><description>&lt;p&gt;
&#20174;&#25387;&#25240;&#20013;&#33719;&#24471;&#26234;&#24935;&#65306;&#36890;&#36807;&#38169;&#35823;&#20998;&#26512;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10477
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26292;&#38706;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#36755;&#20986;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24182;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#26082;&#24102;&#26469;&#20102;&#26426;&#36935;&#65292;&#20063;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24847;&#22806;&#29983;&#25104;&#26377;&#23475;&#21644;&#26377;&#27602;&#22238;&#24212;&#26041;&#38754;&#12290;&#20256;&#32479;&#30340;&#23545;&#40784;&#26041;&#27861;&#33268;&#21147;&#20110;&#24341;&#23548;LLMs&#26397;&#30528;&#26399;&#26395;&#30340;&#24615;&#33021;&#21457;&#23637;&#24182;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20869;&#23481;&#30340;&#20405;&#23475;&#65292;&#32780;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#20998;&#26512;&#30340;&#20840;&#26032;&#23545;&#40784;&#31574;&#30053;&#65292;&#36890;&#36807;&#26377;&#24847;&#26292;&#38706;LLMs&#30340;&#32570;&#38519;&#36755;&#20986;&#24182;&#36827;&#34892;&#28145;&#20837;&#35780;&#20272;&#65292;&#20197;&#23436;&#20840;&#29702;&#35299;&#20869;&#37096;&#21407;&#22240;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#26377;&#27602;&#22238;&#24212;&#21487;&#20197;&#36716;&#21270;&#20026;&#27169;&#22411;&#23545;&#40784;&#30340;&#25351;&#23548;&#35843;&#35856;&#35821;&#26009;&#65292;LLMs&#19981;&#20165;&#21487;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#22238;&#24212;&#65292;&#36824;&#21487;&#20197;&#35757;&#32451;&#20854;&#36827;&#34892;&#33258;&#25105;&#25209;&#35780;&#65292;&#21457;&#25381;&#20854;&#36776;&#21035;&#26377;&#27602;&#20869;&#23481;&#30340;&#20869;&#22312;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23433;&#20840;&#25351;&#20196;&#36981;&#24490;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#21516;&#26102;&#36824;&#20445;&#25345;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of large language models (LLMs) presents both opportunities and challenges, particularly concerning unintentional generation of harmful and toxic responses. While the traditional alignment methods strive to steer LLMs towards desired performance and shield them from malicious content, this study proposes a novel alignment strategy rooted in mistake analysis by exposing LLMs to flawed outputs purposefully and then conducting a thorough assessment to fully comprehend internal reasons via natural language analysis. Thus, toxic responses can be transformed into instruction tuning corpus for model alignment, and LLMs can not only be deterred from generating flawed responses but also trained to self-criticize, leveraging its innate ability to discriminate toxic content. Experimental results demonstrate that the proposed method outperforms conventional alignment techniques for safety instruction following, while maintaining superior efficiency.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#21046;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#40736;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#35757;&#32451;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#12289;&#25913;&#36827;&#32593;&#32476;&#26550;&#26500;&#21644;&#21442;&#25968;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;&#21475;&#26381;&#21644;&#38745;&#33033;&#32473;&#33647;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#39044;&#27979;&#26356;&#22810;&#32456;&#28857;&#21644;&#22788;&#29702;&#19981;&#21516;&#30340;&#21327;&#21464;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.09167</link><description>&lt;p&gt;
&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#21046;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#22823;&#40736;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Deep Neural Network -- Mechanistic Hybrid Model to Predict Pharmacokinetics in Rat. (arXiv:2310.09167v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09167
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#21046;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#22823;&#40736;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#12290;&#36890;&#36807;&#35757;&#32451;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#12289;&#25913;&#36827;&#32593;&#32476;&#26550;&#26500;&#21644;&#21442;&#25968;&#21270;&#26426;&#21046;&#27169;&#22411;&#65292;&#25104;&#21151;&#20943;&#23567;&#20102;&#21475;&#26381;&#21644;&#38745;&#33033;&#32473;&#33647;&#30340;&#35823;&#24046;&#65292;&#24182;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#39044;&#27979;&#26356;&#22810;&#32456;&#28857;&#21644;&#22788;&#29702;&#19981;&#21516;&#30340;&#21327;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23567;&#20998;&#23376;&#33647;&#29289;&#25110;&#20892;&#33647;&#30340;&#30740;&#21457;&#20013;&#65292;&#37325;&#35201;&#30340;&#19968;&#20010;&#26041;&#38754;&#23601;&#26159;&#23427;&#20204;&#22312;&#38745;&#33033;&#21644;&#21475;&#26381;&#32473;&#33647;&#21518;&#30340;&#20840;&#36523;&#21487;&#29992;&#24615;&#12290;&#20174;&#20505;&#36873;&#21270;&#21512;&#29289;&#30340;&#21270;&#23398;&#32467;&#26500;&#39044;&#27979;&#20840;&#36523;&#21487;&#29992;&#24615;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#35753;&#33647;&#29289;&#25110;&#20892;&#33647;&#30340;&#30740;&#21457;&#38598;&#20013;&#22312;&#20855;&#26377;&#33391;&#22909;&#21160;&#21147;&#23398;&#29305;&#24615;&#30340;&#21270;&#21512;&#29289;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21487;&#29992;&#24615;&#26159;&#20998;&#23376;&#24615;&#36136;&#12289;&#29983;&#29289;&#23398;&#21644;&#29983;&#29702;&#23398;&#20043;&#38388;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#31232;&#32570;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#20808;&#21069;&#24320;&#21457;&#30340;&#28151;&#21512;&#27169;&#22411;[34]&#12290;&#25105;&#20204;&#23558;&#24635;&#30340;&#21475;&#26381;&#26292;&#38706;&#30340;&#20013;&#20540;&#25240;&#21472;&#35823;&#24046;&#20174;2.85&#38477;&#20302;&#21040;2.35&#65292;&#23558;&#38745;&#33033;&#32473;&#33647;&#30340;&#35823;&#24046;&#20174;1.95&#38477;&#20302;&#21040;1.62&#12290;&#36825;&#26159;&#36890;&#36807;&#22312;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20197;&#21450;&#26426;&#21046;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#23454;&#29616;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#20854;&#20182;&#32456;&#28857;&#21644;&#22788;&#29702;&#19981;&#21516;&#30340;&#21327;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important aspect in the development of small molecules as drugs or agro-chemicals is their systemic availability after intravenous and oral administration.The prediction of the systemic availability from the chemical structure of a poten-tial candidate is highly desirable, as it allows to focus the drug or agrochemicaldevelopment on compounds with a favorable kinetic profile. However, such pre-dictions are challenging as the availability is the result of the complex interplaybetween molecular properties, biology and physiology and training data is rare.In this work we improve the hybrid model developed earlier [34]. We reducethe median fold change error for the total oral exposure from 2.85 to 2.35 andfor intravenous administration from 1.95 to 1.62. This is achieved by trainingon a larger data set, improving the neural network architecture as well as theparametrization of mechanistic model. Further, we extend our approach to predictadditional endpoints and to handle different covar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35760;&#24518;&#20581;&#36523;&#25151;&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#21033;&#29992;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26080;&#23613;&#20219;&#21153;&#23545;&#35760;&#24518;&#33021;&#21147;&#12289;&#22122;&#22768;&#25239;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Transformer-XL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.17207</link><description>&lt;p&gt;
&#35760;&#24518;&#20581;&#36523;&#25151;&#65306;&#23545;&#20869;&#23384;&#20026;&#22522;&#30784;&#30340;&#26234;&#33021;&#20307;&#22312;&#26080;&#23613;&#20219;&#21153;&#20013;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes. (arXiv:2309.17207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35760;&#24518;&#20581;&#36523;&#25151;&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#21033;&#29992;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#24182;&#36890;&#36807;&#26080;&#23613;&#20219;&#21153;&#23545;&#35760;&#24518;&#33021;&#21147;&#12289;&#22122;&#22768;&#25239;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;Transformer-XL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#20581;&#36523;&#25151;&#20171;&#32461;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#27979;&#35797;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#29305;&#21035;&#26159;&#23558;&#38376;&#24490;&#29615;&#21333;&#20803;(GRU)&#19982;Transformer-XL(TrXL)&#30456;&#27604;&#65292;&#23427;&#20204;&#23545;&#20110;&#35760;&#24518;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#12289;&#25239;&#22122;&#22768;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#37319;&#29992;&#20102;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#20108;&#32500;&#29615;&#22659;&#21644;&#31163;&#25955;&#25511;&#21046;&#65292;&#21363;Mortar Mayhem&#12289;Mystery Path&#21644;Searing Spotlights&#12290;&#36825;&#20123;&#26368;&#21021;&#26159;&#26377;&#38480;&#30340;&#29615;&#22659;&#34987;&#25512;&#24191;&#20026;&#26032;&#39062;&#30340;&#26080;&#23613;&#20219;&#21153;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#21160;&#35838;&#31243;&#65292;&#20174;&#36710;&#28216;&#25103;"I packed my bag"&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#36825;&#20123;&#26080;&#23613;&#20219;&#21153;&#19981;&#20165;&#26377;&#21161;&#20110;&#35780;&#20272;&#25928;&#29575;&#65292;&#32780;&#19988;&#26377;&#36259;&#22320;&#35780;&#20272;&#20102;&#35760;&#24518;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;&#29616;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#35760;&#24518;&#22522;&#20934;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#30001;TrXL&#21644;Proximal Policy Optimization&#39537;&#21160;&#30340;&#23454;&#29616;&#12290;&#26412;&#23454;&#29616;&#21033;&#29992;TrXL&#20316;&#20026;&#20197;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#20351;&#29992;&#30340;&#24773;&#33410;&#24615;&#35760;&#24518;&#12290;&#22312;&#26377;&#38480;&#29615;&#22659;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Memory Gym introduces a unique benchmark designed to test Deep Reinforcement Learning agents, specifically comparing Gated Recurrent Unit (GRU) against Transformer-XL (TrXL), on their ability to memorize long sequences, withstand noise, and generalize. It features partially observable 2D environments with discrete controls, namely Mortar Mayhem, Mystery Path, and Searing Spotlights. These originally finite environments are extrapolated to novel endless tasks that act as an automatic curriculum, drawing inspiration from the car game ``I packed my bag". These endless tasks are not only beneficial for evaluating efficiency but also intriguingly valuable for assessing the effectiveness of approaches in memory-based agents. Given the scarcity of publicly available memory baselines, we contribute an implementation driven by TrXL and Proximal Policy Optimization. This implementation leverages TrXL as episodic memory using a sliding window approach. In our experiments on the finite environment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.16741</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#36890;&#36807;&#28508;&#31354;&#38388;&#25237;&#24433;&#30340;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections. (arXiv:2309.16741v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20844;&#21496;&#36890;&#24120;&#22788;&#29702;&#21644;&#23384;&#20648;&#20135;&#29983;&#36830;&#32493;&#19988;&#39640;&#39057;&#30340;&#25968;&#21313;&#20159;&#26465;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20026;&#20102;&#25903;&#25345;&#39640;&#25928;&#30340;&#25968;&#25454;&#23384;&#20648;&#21644;&#26816;&#32034;&#65292;&#20986;&#29616;&#20102;&#19987;&#38376;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24211;&#21644;&#31995;&#32479;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25903;&#25345;&#36890;&#36807;&#31867;&#20284;&#20110;&#32422;&#26463;&#21270;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#30340;&#26684;&#24335;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32034;&#24341;&#21644;&#26597;&#35810;&#65292;&#20197;&#23454;&#29616;&#20687;&#8220;&#26376;&#24230;&#20215;&#26684;&#22238;&#25253;&#22823;&#20110;5%&#30340;&#32929;&#31080;&#8221;&#36825;&#26679;&#30340;&#26597;&#35810;&#65292;&#24182;&#20197;&#20005;&#26684;&#30340;&#26684;&#24335;&#34920;&#36798;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#26597;&#35810;&#19981;&#33021;&#25429;&#25417;&#21040;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#24448;&#24448;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#25110;&#35821;&#35328;&#65288;&#20363;&#22914;&#8220;&#22788;&#20110;&#20302;&#27874;&#21160;&#24615;&#29366;&#24577;&#30340;&#32929;&#31080;&#8221;&#65289;&#26356;&#22909;&#22320;&#25551;&#36848;&#12290;&#32780;&#19988;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#31354;&#38388;&#20013;&#36827;&#34892;&#25628;&#32034;&#25152;&#38656;&#30340;&#23384;&#20648;&#12289;&#35745;&#31639;&#26102;&#38388;&#21644;&#26816;&#32034;&#22797;&#26434;&#24230;&#24448;&#24448;&#26159;&#38750;&#24179;&#20961;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#28436;&#31034;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#32534;&#30721;&#22120;&#22312;&#20302;&#32500;&#28508;&#31354;&#38388;&#20013;&#23384;&#20648;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#28508;&#31354;&#38388;&#25237;&#24433;&#21487;&#20197;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial firms commonly process and store billions of time-series data, generated continuously and at a high frequency. To support efficient data storage and retrieval, specialized time-series databases and systems have emerged. These databases support indexing and querying of time-series by a constrained Structured Query Language(SQL)-like format to enable queries like "Stocks with monthly price returns greater than 5%", and expressed in rigid formats. However, such queries do not capture the intrinsic complexity of high dimensional time-series data, which can often be better described by images or language (e.g., "A stock in low volatility regime"). Moreover, the required storage, computational time, and retrieval complexity to search in the time-series space are often non-trivial. In this paper, we propose and demonstrate a framework to store multi-modal data for financial time-series in a lower-dimensional latent space using deep encoders, such that the latent space projections ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26377;&#25928;&#26816;&#27979;&#21644;&#27700;&#21360;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.15224</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Collaborative Watermarking for Adversarial Speech Synthesis. (arXiv:2309.15224v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35821;&#38899;&#21512;&#25104;&#30340;&#21327;&#21516;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#19982;&#29616;&#26377;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26377;&#25928;&#26816;&#27979;&#21644;&#27700;&#21360;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#38899;&#21512;&#25104;&#30340;&#36827;&#23637;&#20351;&#24471;&#25216;&#26415;&#19981;&#20165;&#25509;&#36817;&#20154;&#31867;&#30340;&#33258;&#28982;&#24230;&#65292;&#32780;&#19988;&#33021;&#22815;&#20197;&#23569;&#37327;&#25968;&#25454;&#36827;&#34892;&#21363;&#26102;&#35821;&#38899;&#20811;&#38534;&#65292;&#24182;&#19988;&#20511;&#21161;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#39640;&#24230;&#21487;&#35775;&#38382;&#24615;&#12290;&#24403;&#28982;&#65292;&#29983;&#25104;&#20869;&#23481;&#30340;&#28508;&#22312;&#27867;&#28389;&#24341;&#36215;&#20102;&#23545;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#21644;&#27700;&#21360;&#25216;&#26415;&#30340;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#21512;&#25104;&#35821;&#38899;&#26816;&#27979;&#30340;&#30740;&#31350;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#21644;&#27450;&#39575;&#23545;&#31574;&#25361;&#25112;&#65288;ASVspoof&#65289;&#19978;&#65292;&#35813;&#25361;&#25112;&#19987;&#27880;&#20110;&#34987;&#21160;&#23545;&#31574;&#12290;&#26412;&#25991;&#20174;&#21478;&#19968;&#35282;&#24230;&#20986;&#21457;&#65292;&#38024;&#23545;&#29983;&#25104;&#35821;&#38899;&#30340;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#22312;&#19981;&#24178;&#25200;&#20154;&#31867;&#21548;&#20247;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#36890;&#36807;&#21327;&#21516;&#26426;&#22120;&#26816;&#27979;&#21040;&#29983;&#25104;&#35821;&#38899;&#30340;&#27700;&#21360;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;ASVspoof 2021&#22522;&#32447;&#23545;&#31574;&#27169;&#22411;&#21512;&#20316;&#30340;HiFi-GAN&#31070;&#32463;&#22768;&#30721;&#22120;&#30340;&#21512;&#20316;&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in neural speech synthesis have brought us technology that is not only close to human naturalness, but is also capable of instant voice cloning with little data, and is highly accessible with pre-trained models available. Naturally, the potential flood of generated content raises the need for synthetic speech detection and watermarking. Recently, considerable research effort in synthetic speech detection has been related to the Automatic Speaker Verification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on passive countermeasures. This paper takes a complementary view to generated speech detection: a synthesis system should make an active effort to watermark the generated speech in a way that aids detection by another machine, but remains transparent to a human listener. We propose a collaborative training scheme for synthetic speech watermarking and show that a HiFi-GAN neural vocoder collaborating with the ASVspoof 2021 baseline countermeasure models consis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14496</link><description>&lt;p&gt;
Era Splitting.&#65288;arXiv:2309.14496v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Era Splitting. (arXiv:2309.14496v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#20915;&#31574;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#26102;&#20195;&#20449;&#24687;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20250;&#21576;&#29616;&#20986;&#25968;&#25454;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#34892;&#20026;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#33539;&#24335;&#30340;&#33539;&#22260;&#65292;&#35813;&#33539;&#24335;&#20551;&#35774;&#25968;&#25454;&#22312;&#26102;&#38388;&#21644;&#22320;&#28857;&#19978;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#12290;&#26032;&#20852;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#39046;&#22495;&#36890;&#36807;&#23558;&#29615;&#22659;&#25110;&#26102;&#20195;&#20449;&#24687;&#34701;&#20837;&#31639;&#27861;&#20013;&#65292;&#26469;&#24212;&#23545;&#36825;&#20010;&#29616;&#23454;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#32447;&#24615;&#27169;&#22411;&#21644;/&#25110;&#31070;&#32463;&#32593;&#32476;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#20998;&#35010;&#20934;&#21017;&#65292;&#20351;&#24471;&#26641;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#19982;&#27599;&#20010;&#25968;&#25454;&#28857;&#30456;&#20851;&#30340;&#26102;&#20195;&#20449;&#24687;&#65292;&#26469;&#25214;&#21040;&#22312;&#25968;&#25454;&#30340;&#25152;&#26377;&#19981;&#30456;&#20132;&#26102;&#20195;&#20013;&#37117;&#26159;&#26368;&#20248;&#30340;&#20999;&#20998;&#28857;&#65292;&#20174;&#32780;&#23558;&#36229;&#20998;&#24067;&#27867;&#21270;&#30740;&#31350;&#20013;&#30340;&#24605;&#24819;&#24212;&#29992;&#20110;&#20915;&#31574;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#26631;&#20934;&#21270;&#27969;&#24341;&#20837;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#28789;&#27963;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.09222</link><description>&lt;p&gt;
&#21452;&#37325;&#26631;&#20934;&#21270;&#27969;&#65306;&#28789;&#27963;&#30340;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Double Normalizing Flows: Flexible Bayesian Gaussian Process ODEs Learning. (arXiv:2309.09222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09222
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#26631;&#20934;&#21270;&#27969;&#24341;&#20837;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#28789;&#27963;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39640;&#26031;&#36807;&#31243;&#34987;&#29992;&#26469;&#24314;&#27169;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#30340;&#21521;&#37327;&#22330;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#36125;&#21494;&#26031;&#25512;&#26029;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31561;&#20219;&#21153;&#65292;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#22312;&#20855;&#26377;&#38750;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#32422;&#26463;&#20808;&#39564;&#21644;&#22343;&#20540;&#22330;&#21518;&#39564;&#21487;&#33021;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#20934;&#21270;&#27969;&#26469;&#37325;&#26032;&#21442;&#25968;&#21270;ODE&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#26356;&#28789;&#27963;&#12289;&#26356;&#34920;&#36798;&#24615;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26631;&#20934;&#21270;&#27969;&#30340;&#35299;&#26512;&#21487;&#35745;&#31639;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;GP ODE&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#29983;&#25104;&#19968;&#20010;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#12290;&#36890;&#36807;&#36825;&#20123;&#26631;&#20934;&#21270;&#27969;&#30340;&#21452;&#37325;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#20013;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Gaussian processes have been utilized to model the vector field of continuous dynamical systems. Bayesian inference for such models \cite{hegde2022variational} has been extensively studied and has been applied in tasks such as time series prediction, providing uncertain estimates. However, previous Gaussian Process Ordinary Differential Equation (ODE) models may underperform on datasets with non-Gaussian process priors, as their constrained priors and mean-field posteriors may lack flexibility. To address this limitation, we incorporate normalizing flows to reparameterize the vector field of ODEs, resulting in a more flexible and expressive prior distribution. Additionally, due to the analytically tractable probability density functions of normalizing flows, we apply them to the posterior inference of GP ODEs, generating a non-Gaussian posterior. Through these dual applications of normalizing flows, our model improves accuracy and uncertainty estimates for Bayesian Gaussian P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#36719;&#26448;&#26009;&#22312;&#22823;&#21464;&#24418;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#20855;&#26377;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#31283;&#20581;&#22320;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.15640</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#22797;&#26434;&#36229;&#24377;&#24615;&#22266;&#20307;&#30340;&#32452;&#20998;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Identifying Constitutive Parameters for Complex Hyperelastic Solids using Physics-Informed Neural Networks. (arXiv:2308.15640v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#36719;&#26448;&#26009;&#22312;&#22823;&#21464;&#24418;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#20855;&#26377;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#31283;&#20581;&#22320;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#31243;&#21644;&#29983;&#29289;&#26448;&#26009;&#20013;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#20855;&#26377;&#22797;&#26434;&#20960;&#20309;&#21644;&#26426;&#26800;&#34892;&#20026;&#30340;&#26448;&#26009;&#20013;&#65292;&#35782;&#21035;&#32452;&#20998;&#21442;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#20026;&#27492;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24403;&#21069;&#30340;&#26694;&#26550;&#36890;&#24120;&#20165;&#38480;&#20110;&#22522;&#26412;&#30340;&#32452;&#20998;&#23450;&#24459;&#65292;&#24182;&#22312;&#19982;&#23454;&#39564;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#36935;&#21040;&#23454;&#38469;&#32422;&#26463;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;PINN&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#36719;&#26448;&#26009;&#30340;&#26448;&#26009;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#22312;&#24179;&#38754;&#24212;&#21147;&#26465;&#20214;&#19979;&#21576;&#29616;&#22797;&#26434;&#32452;&#20998;&#34892;&#20026;&#30340;&#26448;&#26009;&#12290;&#35813;&#27169;&#22411;&#24378;&#35843;&#29992;&#22810;&#27169;&#24577;&#30340;&#26102;&#38388;&#30456;&#20851;&#23454;&#39564;&#25968;&#25454;&#35757;&#32451;PINN&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#22330;&#21464;&#24418;&#21644;&#21152;&#36733;&#21382;&#21490;&#65292;&#20197;&#30830;&#20445;&#31639;&#27861;&#22312;&#22024;&#26434;&#25968;&#25454;&#20013;&#20173;&#28982;&#31283;&#20581;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20934;&#30830;&#35782;&#21035;&#19981;&#21487;&#21387;&#32553;Arruda-Boyce&#27169;&#22411;&#30340;&#32452;&#20998;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying constitutive parameters in engineering and biological materials, particularly those with intricate geometries and mechanical behaviors, remains a longstanding challenge. The recent advent of Physics-Informed Neural Networks (PINNs) offers promising solutions, but current frameworks are often limited to basic constitutive laws and encounter practical constraints when combined with experimental data. In this paper, we introduce a new PINN-based framework designed to identify material parameters for soft materials, specifically those exhibiting complex constitutive behaviors, under large deformation in plane stress conditions. Distinctively, our model emphasizes training PINNs with multi-modal time-dependent experimental datasets consisting of full-field deformation and loading history, ensuring algorithm robustness even amidst noisy data. Our results reveal that our framework can accurately identify constitutive parameters of the incompressible Arruda-Boyce model for samples 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SyMOT-Flow&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26410;&#30693;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#31216;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#26469;&#35757;&#32451;&#21487;&#36870;&#36716;&#25442;&#65292;&#24182;&#32467;&#21512;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#20316;&#20026;&#27491;&#21017;&#21270;&#65292;&#23558;&#26410;&#30693;&#20998;&#24067;&#36716;&#25442;&#20026;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#26356;&#31283;&#23450;&#20934;&#30830;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.13815</link><description>&lt;p&gt;
SyMOT-Flow: &#23398;&#20064;&#20004;&#20010;&#20219;&#24847;&#20998;&#24067;&#20043;&#38388;&#30340;&#26368;&#20248;&#36755;&#36816;&#27969;&#21160;&#21450;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
SyMOT-Flow: Learning optimal transport flow for two arbitrary distributions with maximum mean discrepancy. (arXiv:2308.13815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SyMOT-Flow&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26410;&#30693;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#31216;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#26469;&#35757;&#32451;&#21487;&#36870;&#36716;&#25442;&#65292;&#24182;&#32467;&#21512;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#20316;&#20026;&#27491;&#21017;&#21270;&#65292;&#23558;&#26410;&#30693;&#20998;&#24067;&#36716;&#25442;&#20026;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#26356;&#31283;&#23450;&#20934;&#30830;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25214;&#21040;&#20004;&#20010;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#36716;&#25442;&#23545;&#20110;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#21644;&#25191;&#34892;&#23494;&#24230;&#20272;&#35745;&#12289;&#26679;&#26412;&#29983;&#25104;&#21644;&#32479;&#35745;&#25512;&#26029;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SyMOT-Flow&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20004;&#20010;&#26410;&#30693;&#20998;&#24067;&#26679;&#26412;&#20043;&#38388;&#30340;&#23545;&#31216;&#26368;&#22823;&#24179;&#22343;&#24046;&#24322;&#26469;&#35757;&#32451;&#19968;&#20010;&#21487;&#36870;&#36716;&#25442;&#65292;&#24182;&#32467;&#21512;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#20316;&#20026;&#27491;&#21017;&#21270;&#20197;&#33719;&#24471;&#19968;&#20010;&#30701;&#36317;&#31163;&#21644;&#21487;&#35299;&#37322;&#30340;&#36716;&#25442;&#12290;&#24471;&#21040;&#30340;&#36716;&#25442;&#21487;&#20197;&#23454;&#29616;&#26356;&#31283;&#23450;&#20934;&#30830;&#30340;&#26679;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24314;&#31435;&#20102;&#19968;&#20123;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#20302;&#32500;&#31034;&#20363;&#21644;&#39640;&#32500;&#29983;&#25104;&#26679;&#26412;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a transformation between two unknown probability distributions from samples is crucial for modeling complex data distributions and perform tasks such as density estimation, sample generation, and statistical inference. One powerful framework for such transformations is normalizing flow, which transforms an unknown distribution into a standard normal distribution using an invertible network. In this paper, we introduce a novel model called SyMOT-Flow that trains an invertible transformation by minimizing the symmetric maximum mean discrepancy between samples from two unknown distributions, and we incorporate an optimal transport cost as regularization to obtain a short-distance and interpretable transformation. The resulted transformation leads to more stable and accurate sample generation. We establish several theoretical results for the proposed model and demonstrate its effectiveness with low-dimensional illustrative examples as well as high-dimensional generative samples obt
&lt;/p&gt;</description></item><item><title>SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.04365</link><description>&lt;p&gt;
SLEM&#65306;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#36335;&#24452;&#24314;&#27169;&#21644;&#22240;&#26524;&#25512;&#26029;&#30340;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SLEM: Machine Learning for Path Modeling and Causal Inference with Super Learner Equation Modeling. (arXiv:2308.04365v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04365
&lt;/p&gt;
&lt;p&gt;
SLEM&#26159;&#19968;&#31181;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#31185;&#23398;&#30340;&#20851;&#38190;&#30446;&#26631;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#36890;&#36807;&#35266;&#23519;&#25968;&#25454;&#24471;&#20986;&#20851;&#20110;&#23545;&#20551;&#23450;&#24178;&#39044;&#30340;&#39044;&#27979;&#30340;&#26377;&#24847;&#20041;&#30340;&#32467;&#35770;&#12290;&#36335;&#24452;&#27169;&#22411;&#12289;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;(SEMs)&#20197;&#21450;&#26356;&#19968;&#33324;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#33021;&#22815;&#26126;&#30830;&#22320;&#25351;&#23450;&#20851;&#20110;&#29616;&#35937;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#20551;&#35774;&#12290;&#19982;DAGs&#19981;&#21516;&#65292;SEMs&#20551;&#35774;&#32447;&#24615;&#20851;&#31995;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20989;&#25968;&#38169;&#35823;&#35268;&#33539;&#65292;&#20174;&#32780;&#38459;&#30861;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#21487;&#38752;&#30340;&#25928;&#26524;&#22823;&#23567;&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36229;&#32423;&#23398;&#20064;&#32773;&#26041;&#31243;&#27169;&#22411;&#65288;SLEM&#65289;&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#26426;&#22120;&#23398;&#20064;&#36229;&#32423;&#23398;&#20064;&#32773;&#38598;&#25104;&#30340;&#36335;&#24452;&#24314;&#27169;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;SLEM&#33021;&#22815;&#25552;&#20379;&#19968;&#33268;&#19988;&#26080;&#20559;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#22312;&#19982;SEMs&#36827;&#34892;&#32447;&#24615;&#27169;&#22411;&#27604;&#36739;&#26102;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#38750;&#32447;&#24615;&#20851;&#31995;&#26102;&#20248;&#20110;SEMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference is a crucial goal of science, enabling researchers to arrive at meaningful conclusions regarding the predictions of hypothetical interventions using observational data. Path models, Structural Equation Models (SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to unambiguously specify assumptions regarding the causal structure underlying a phenomenon. Unlike DAGs, which make very few assumptions about the functional and parametric form, SEM assumes linearity. This can result in functional misspecification which prevents researchers from undertaking reliable effect size estimation. In contrast, we propose Super Learner Equation Modeling, a path modeling technique integrating machine learning Super Learner ensembles. We empirically demonstrate its ability to provide consistent and unbiased estimates of causal effects, its competitive performance for linear models when compared with SEM, and highlight its superiority over SEM when dealing with non
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#19968;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#24182;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#26032;&#30340;&#35823;&#24046;&#30028;&#12290;&#20854;&#26041;&#27861;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.16164</link><description>&lt;p&gt;
&#22312;RKHS&#20013;&#33258;&#36866;&#24212;&#23398;&#20064;&#23494;&#24230;&#27604;&#29575;
&lt;/p&gt;
&lt;p&gt;
Adaptive learning of density ratios in RKHS. (arXiv:2307.16164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#19968;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#24182;&#22312;&#26377;&#38480;&#26679;&#26412;&#24773;&#20917;&#19979;&#25512;&#23548;&#20986;&#26032;&#30340;&#35823;&#24046;&#30028;&#12290;&#20854;&#26041;&#27861;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#23494;&#24230;&#35266;&#27979;&#20013;&#20272;&#35745;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#30340;&#27604;&#29575;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#24212;&#29992;&#21253;&#25324;&#21452;&#26679;&#26412;&#26816;&#39564;&#12289;&#20998;&#27495;&#20272;&#35745;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#21327;&#21464;&#37327;&#36716;&#31227;&#36866;&#24212;&#12289;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#21644;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#22823;&#31867;&#23494;&#24230;&#27604;&#29575;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#20204;&#36890;&#36807;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#26368;&#23567;&#21270;&#30495;&#23454;&#23494;&#24230;&#27604;&#29575;&#19982;&#27169;&#22411;&#20043;&#38388;&#30340;&#27491;&#21017;Bregman&#36317;&#31163;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#26032;&#30340;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;Lepskii&#31867;&#22411;&#30340;&#21442;&#25968;&#36873;&#25321;&#21407;&#21017;&#65292;&#22312;&#19981;&#30693;&#36947;&#23494;&#24230;&#27604;&#29575;&#30340;&#27491;&#21017;&#24615;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#35823;&#24046;&#30028;&#12290;&#22312;&#20108;&#27425;&#25439;&#22833;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#29575;&#12290;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#20540;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the ratio of two probability densities from finitely many observations of the densities is a central problem in machine learning and statistics with applications in two-sample testing, divergence estimation, generative modeling, covariate shift adaptation, conditional density estimation, and novelty detection. In this work, we analyze a large class of density ratio estimation methods that minimize a regularized Bregman divergence between the true density ratio and a model in a reproducing kernel Hilbert space (RKHS). We derive new finite-sample error bounds, and we propose a Lepskii type parameter choice principle that minimizes the bounds without knowledge of the regularity of the density ratio. In the special case of quadratic loss, our method adaptively achieves a minimax optimal error rate. A numerical illustration is provided.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31283;&#20581;&#28857;&#20113;&#20998;&#31867;&#30340;&#39118;&#38505;&#20248;&#21270;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#21033;&#29992;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#28040;&#38500;&#39069;&#22806;&#30340;&#24322;&#24120;&#20540;&#24182;&#24674;&#22797;&#25968;&#25454;&#12290;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#20998;&#26512;&#30830;&#23450;&#27599;&#20010;&#28857;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#20248;&#21270;&#39640;&#39118;&#38505;&#28857;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.10875</link><description>&lt;p&gt;
&#38754;&#21521;&#31283;&#20581;&#28857;&#20113;&#20998;&#31867;&#30340;&#39118;&#38505;&#20248;&#21270;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Risk-optimized Outlier Removal for Robust Point Cloud Classification. (arXiv:2307.10875v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#31283;&#20581;&#28857;&#20113;&#20998;&#31867;&#30340;&#39118;&#38505;&#20248;&#21270;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#21033;&#29992;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#28040;&#38500;&#39069;&#22806;&#30340;&#24322;&#24120;&#20540;&#24182;&#24674;&#22797;&#25968;&#25454;&#12290;&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#20998;&#26512;&#30830;&#23450;&#27599;&#20010;&#28857;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#20248;&#21270;&#39640;&#39118;&#38505;&#28857;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#28145;&#24230;&#27169;&#22411;&#22312;&#23433;&#20840;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#20351;&#29992;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#26159;&#28857;&#20113;&#22122;&#22768;&#21487;&#33021;&#20250;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;(PointCVaR)&#65292;&#23427;&#21487;&#20197;&#20351;&#26631;&#20934;&#35757;&#32451;&#30340;&#27169;&#22411;&#28040;&#38500;&#39069;&#22806;&#30340;&#24322;&#24120;&#20540;&#24182;&#24674;&#22797;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#36827;&#34892;&#24402;&#22240;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#28857;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#28857;&#30340;&#39118;&#38505;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540; (CVaR) &#20316;&#20026;&#30446;&#26631;&#65292;&#20248;&#21270;&#39640;&#39118;&#38505;&#28857;&#30340;&#36807;&#28388;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#35266;&#23519;&#21040;&#28857;&#20113;&#22122;&#22768;&#28857;&#24448;&#24448;&#32858;&#38598;&#22312;&#39118;&#38505;&#20998;&#24067;&#30340;&#23614;&#37096;&#65292;&#39057;&#29575;&#20302;&#20294;&#39118;&#38505;&#27700;&#24179;&#39640;&#65292;&#20174;&#32780;&#23545;&#20998;&#31867;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24178;&#25200;&#12290;&#23613;&#31649;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21364;&#33021;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of point cloud deep models for safety-critical purposes has increased, but the reliability and security of these models can be compromised by intentional or naturally occurring point cloud noise. To combat this issue, we present a novel point cloud outlier removal method called PointCVaR, which empowers standard-trained models to eliminate additional outliers and restore the data. Our approach begins by conducting attribution analysis to determine the influence of each point on the model output, which we refer to as point risk. We then optimize the process of filtering high-risk points using Conditional Value at Risk (CVaR) as the objective. The rationale for this approach is based on the observation that noise points in point clouds tend to cluster in the tail of the risk distribution, with a low frequency but a high level of risk, resulting in significant interference with classification results. Despite requiring no additional training effort, our method produces exce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.17760</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;
&lt;/p&gt;
&lt;p&gt;
Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#65288;Kahneman&#65292;2011&#65289;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24515;&#29702;&#23398;&#23478;&#20204;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#26694;&#26550;&#30340;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#23454;&#36136;&#19978;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#26469;&#33719;&#24471;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#26041;&#38754;&#30340;&#28145;&#21051;&#35265;&#35299;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#21463;&#26410;&#30693;&#24178;&#25200;&#21644;&#38459;&#23612;&#24433;&#21709;&#26102;&#23398;&#20064;&#21040;&#20869;&#37096;&#21160;&#24577;&#35299;&#26512;&#39033;&#30340;&#20266;&#21704;&#23494;&#39039;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#65292;&#21363;&#20351;&#38590;&#20197;&#25214;&#21040;&#25200;&#21160;&#35299;&#26512;&#39033;&#65292;&#20063;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#21160;&#24577;&#65292;&#23545;&#20110;&#20854;&#20182;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#24773;&#20917;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22235;&#38454;&#23545;&#31216;&#31215;&#20998;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06920</link><description>&lt;p&gt;
&#20266;&#21704;&#23494;&#39039;&#31995;&#32479;&#36776;&#35782;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Hamiltonian system identification. (arXiv:2305.06920v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#21463;&#26410;&#30693;&#24178;&#25200;&#21644;&#38459;&#23612;&#24433;&#21709;&#26102;&#23398;&#20064;&#21040;&#20869;&#37096;&#21160;&#24577;&#35299;&#26512;&#39033;&#30340;&#20266;&#21704;&#23494;&#39039;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#65292;&#21363;&#20351;&#38590;&#20197;&#25214;&#21040;&#25200;&#21160;&#35299;&#26512;&#39033;&#65292;&#20063;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#21160;&#24577;&#65292;&#23545;&#20110;&#20854;&#20182;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#24773;&#20917;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22235;&#38454;&#23545;&#31216;&#31215;&#20998;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21482;&#26377;&#35266;&#27979;&#25968;&#25454;&#26102;&#65292;&#30830;&#23450;&#29289;&#29702;&#31995;&#32479;&#30340;&#22522;&#26412;&#21160;&#24577;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#21487;&#20197;&#24314;&#27169;&#20026;&#19968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#20551;&#35774;&#19968;&#23450;&#30340;&#20266;&#21704;&#23494;&#39039;&#24418;&#24335;&#65292;&#21363;&#20351;&#27169;&#22411;&#22312;&#31995;&#32479;&#21463;&#21040;&#26410;&#30693;&#38459;&#23612;&#21644;&#22806;&#25200;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#20063;&#33021;&#22815;&#23398;&#20064;&#21040;&#20869;&#37096;&#21160;&#24577;&#30340;&#35299;&#26512;&#39033;&#12290;&#22312;&#38590;&#20197;&#25214;&#21040;&#25200;&#21160;&#35299;&#26512;&#39033;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#36825;&#20123;&#39033;&#30340;&#28151;&#21512;&#27169;&#22411;&#20173;&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;&#31995;&#32479;&#30340;&#21160;&#24577;&#65292;&#23601;&#20687;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#19968;&#26679;&#12290;&#36825;&#20351;&#24471;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#20854;&#20182;&#31995;&#32479;&#36776;&#35782;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#20351;&#29992;&#22235;&#38454;&#23545;&#31216;&#31215;&#20998;&#26041;&#26696;&#65292;&#36991;&#20813;&#35757;&#32451;&#20013;&#30340;&#23454;&#38469;&#31215;&#20998;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#22122;&#22768;&#25968;&#25454;&#19978;&#22914;&#20309;&#25552;&#39640;&#24615;&#33021;&#30340;&#21508;&#31181;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying the underlying dynamics of physical systems can be challenging when only provided with observational data. In this work, we consider systems that can be modelled as first-order ordinary differential equations. By assuming a certain pseudo-Hamiltonian formulation, we are able to learn the analytic terms of internal dynamics even if the model is trained on data where the system is affected by unknown damping and external disturbances. In cases where it is difficult to find analytic terms for the disturbances, a hybrid model that uses a neural network to learn these can still accurately identify the dynamics of the system as if under ideal conditions. This makes the models applicable in situations where other system identification models fail. Furthermore, we propose to use a fourth-order symmetric integration scheme in the loss function and avoid actual integration in the training, and demonstrate on varied examples how this leads to increased performance on noisy data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;PCA&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#23548;&#20986;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#20197;&#35299;&#20915;&#20197;&#24448;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02803</link><description>&lt;p&gt;
&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#24352;&#37327;PCA
&lt;/p&gt;
&lt;p&gt;
Tensor PCA from basis in tensor space. (arXiv:2305.02803v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;PCA&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#23548;&#20986;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#20197;&#35299;&#20915;&#20197;&#24448;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;PCA&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20811;&#26381;&#20197;&#21069;&#36890;&#36807;&#36845;&#20195;&#27714;&#35299;&#20248;&#21270;&#38382;&#39064;&#26469;&#25552;&#21462;&#20302;&#32500;&#23376;&#31354;&#38388;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20174;&#23454;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#20013;&#23548;&#20986;&#24352;&#37327;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#65292;&#20174;&#32780;&#23558;&#22522;&#30784;&#30340;&#23548;&#20986;&#38382;&#39064;&#36716;&#21270;&#20026;&#29305;&#24449;&#20540;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#24773;&#20917;&#30340;&#23548;&#20986;&#65306;i&#65289;&#20174;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#20013;&#23548;&#20986;&#22522;&#30784;&#65307;ii&#65289;&#23548;&#20986;&#31209;&#20026;1&#30340;&#22522;&#30784;&#65307;iii&#65289;&#20174;&#23376;&#31354;&#38388;&#20013;&#23548;&#20986;&#22522;&#30784;&#12290;&#29305;&#21035;&#26159;&#65292;&#35777;&#26126;&#20102;&#23454;&#33258;&#20276;&#24352;&#37327;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#26041;&#31243;&#19982;&#26631;&#20934;&#30697;&#38453;&#29305;&#24449;&#20540;&#26041;&#31243;&#30340;&#31561;&#20215;&#24615;&#12290;&#38024;&#23545;&#25152;&#32771;&#34385;&#30340;&#19977;&#31181;&#24773;&#20917;&#65292;&#37319;&#29992;&#20102;&#23376;&#31354;&#38388;&#26041;&#27861;&#26469;&#23548;&#20986;&#24352;&#37327;PCA&#12290;&#22522;&#20110;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#25968;&#23398;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this paper is to present a mathematical framework for tensor PCA. The proposed approach is able to overcome the limitations of previous methods that extract a low dimensional subspace by iteratively solving an optimization problem. The core of the proposed approach is the derivation of a basis in tensor space from a real self-adjoint tensor operator, thus reducing the problem of deriving a basis to an eigenvalue problem. Three different cases have been studied to derive: i) a basis from a self-adjoint tensor operator; ii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence between eigenvalue equation for a real self-adjoint tensor operator and standard matrix eigenvalue equation has been proven. For all the three cases considered, a subspace approach has been adopted to derive a tensor PCA. Experiments on image datasets validate the proposed mathematical framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;(PHNN)&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#65292;PHNN&#34920;&#29616;&#26356;&#20026;&#20248;&#36234;&#65292;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#21435;&#38500;&#25110;&#25913;&#21464;&#22806;&#21147;&#24773;&#20917;&#24182;&#21487;&#20998;&#21035;&#24471;&#21040;&#19977;&#20010;&#19981;&#21516;&#29289;&#29702;&#35299;&#37322;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2304.14374</link><description>&lt;p&gt;
&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Hamiltonian neural networks for learning partial differential equations. (arXiv:2304.14374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;(PHNN)&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#65292;PHNN&#34920;&#29616;&#26356;&#20026;&#20248;&#36234;&#65292;&#27169;&#22411;&#21487;&#24212;&#29992;&#20110;&#21435;&#38500;&#25110;&#25913;&#21464;&#22806;&#21147;&#24773;&#20917;&#24182;&#21487;&#20998;&#21035;&#24471;&#21040;&#19977;&#20010;&#19981;&#21516;&#29289;&#29702;&#35299;&#37322;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#20266;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;(PHNN)&#26469;&#23398;&#20064;&#21487;&#20197;&#29992;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;&#26412;&#25991;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#20102;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#25152;&#24471;&#27169;&#22411;&#30001;&#39640;&#36798;&#19977;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#27169;&#25311;&#20195;&#34920;&#23432;&#24658;&#12289;&#32791;&#25955;&#21644;&#22806;&#21147;&#30340;&#39033;&#20197;&#21450;&#21487;&#20197;&#23398;&#20064;&#25110;&#20026;&#20808;&#21069;&#30693;&#35782;&#30340;&#31163;&#25955;&#21367;&#31215;&#31639;&#23376;&#26500;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;PHNN&#30456;&#27604;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#22522;&#32447;&#27169;&#22411;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PHNN&#27169;&#22411;&#30001;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#35299;&#37322;&#30340;&#37096;&#20998;&#32452;&#25104;&#65292;&#21487;&#20197;&#20998;&#21035;&#30740;&#31350;&#36825;&#20123;&#37096;&#20998;&#20197;&#33719;&#24471;&#23545;&#31995;&#32479;&#30340;&#27934;&#23519;&#65292;&#24182;&#19988;&#21363;&#20351;&#21435;&#38500;&#25110;&#25913;&#21464;&#22806;&#21147;&#65292;&#25152;&#23398;&#24471;&#30340;&#27169;&#22411;&#20173;&#28982;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Hamiltonian neural networks (PHNN) were recently introduced for learning dynamical systems that can be modelled by ordinary differential equations. In this paper, we extend the method to partial differential equations. The resulting model is comprised of up to three neural networks, modelling terms representing conservation, dissipation and external forces, and discrete convolution operators that can either be learned or be prior knowledge. We demonstrate numerically the superior performance of PHNN compared to a baseline model that models the full dynamics by a single neural network. Moreover, since the PHNN model consists of three parts with different physical interpretations, these can be studied separately to gain insight into the system, and the learned model is applicable also if external forces are removed or changed.
&lt;/p&gt;</description></item><item><title>&#21516;&#28304;&#24615;&#21407;&#21017;&#19981;&#19968;&#23450;&#26159;&#24433;&#21709;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#36234;&#24615;&#30340;&#21807;&#19968;&#21407;&#22240;&#65307;&#26412;&#25991;&#25552;&#20986;Contextual Stochastic Block Model for Homophily (CSBM-H)&#20197;&#28145;&#20837;&#30740;&#31350;&#21516;&#28304;&#24615;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.14274</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20309;&#26102;&#23545;&#33410;&#28857;&#20998;&#31867;&#26377;&#24110;&#21161;&#65306;&#30740;&#31350;&#21516;&#28304;&#24615;&#21407;&#21017;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
When Do Graph Neural Networks Help with Node Classification: Investigating the Homophily Principle on Node Distinguishability. (arXiv:2304.14274v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14274
&lt;/p&gt;
&lt;p&gt;
&#21516;&#28304;&#24615;&#21407;&#21017;&#19981;&#19968;&#23450;&#26159;&#24433;&#21709;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#36234;&#24615;&#30340;&#21807;&#19968;&#21407;&#22240;&#65307;&#26412;&#25991;&#25552;&#20986;Contextual Stochastic Block Model for Homophily (CSBM-H)&#20197;&#28145;&#20837;&#30740;&#31350;&#21516;&#28304;&#24615;&#23545;&#33410;&#28857;&#21487;&#21306;&#20998;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#28304;&#24615;&#21407;&#21017;&#25351;&#30456;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#26356;&#26377;&#21487;&#33021;&#36830;&#25509;&#22312;&#19968;&#36215;&#65292;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#65288;NC&#65289;&#20219;&#21153;&#19978;&#24615;&#33021;&#20248;&#36234;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#25552;&#20986;&#29702;&#35770;&#32467;&#26524;&#35748;&#20026;&#65292;&#21363;&#20351;&#21516;&#28304;&#24615;&#21407;&#21017;&#34987;&#25171;&#30772;&#65292;&#21482;&#35201;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#33410;&#28857;&#20998;&#20139;&#30456;&#20284;&#30340;&#37051;&#23621;&#27169;&#24335;&#65292;GNN&#30340;&#20248;&#21183;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#23545;&#21516;&#28304;&#24615;&#30340;&#26377;&#25928;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#35770;&#28857;&#20165;&#32771;&#34385;&#20102;&#21516;&#31867;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#24573;&#30053;&#20102;&#36328;&#31867;&#21035;&#30340;&#21487;&#21306;&#20998;&#24615;&#65292;&#36825;&#26159;&#30740;&#31350;&#21516;&#28304;&#24615;&#25928;&#24212;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20363;&#23376;&#35777;&#26126;&#20102;&#19978;&#36848;&#19981;&#36275;&#65292;&#24182;&#35748;&#20026;&#21487;&#21306;&#20998;&#24615;&#30340;&#29702;&#24819;&#24773;&#20917;&#26159;&#21516;&#31867;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#23567;&#20110;&#36328;&#31867;&#21035;&#33410;&#28857;&#30340;&#21487;&#21306;&#20998;&#24615;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#36825;&#20010;&#24819;&#27861;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#21516;&#28304;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Contextual Stochastic Block Model for Homophily (CSBM-H)&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Homophily principle, i.e. nodes with the same labels are more likely to be connected, was believed to be the main reason for the performance superiority of Graph Neural Networks (GNNs) over Neural Networks (NNs) on Node Classification (NC) tasks. Recently, people have developed theoretical results arguing that, even though the homophily principle is broken, the advantage of GNNs can still hold as long as nodes from the same class share similar neighborhood patterns, which questions the validity of homophily. However, this argument only considers intra-class Node Distinguishability (ND) and ignores inter-class ND, which is insufficient to study the effect of homophily. In this paper, we first demonstrate the aforementioned insufficiency with examples and argue that an ideal situation for ND is to have smaller intra-class ND than inter-class ND. To formulate this idea and have a better understanding of homophily, we propose Contextual Stochastic Block Model for Homophily (CSBM-H) and def
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#8212;&#8212;&#19978;&#19979;&#25991;&#22871;&#32034;&#65292;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#35299;&#20915;&#35299;&#37322;&#24615;&#21644;&#25311;&#21512;&#33021;&#21147;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#31232;&#30095;&#25311;&#21512;&#65292;&#24182;&#19988;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2302.00878</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#22871;&#32034;&#65306;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#23454;&#29616;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The contextual lasso: Sparse linear models via deep neural networks. (arXiv:2302.00878v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#8212;&#8212;&#19978;&#19979;&#25991;&#22871;&#32034;&#65292;&#21487;&#20197;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#35299;&#20915;&#35299;&#37322;&#24615;&#21644;&#25311;&#21512;&#33021;&#21147;&#30340;&#30683;&#30462;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#31232;&#30095;&#25311;&#21512;&#65292;&#24182;&#19988;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#26159;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#40644;&#37329;&#26631;&#20934;&#24037;&#20855;&#65292;&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#31232;&#30095;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#24378;&#22823;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#19978;&#19979;&#25991;&#22871;&#32034;&#26159;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#23427;&#23558;&#36755;&#20837;&#29305;&#24449;&#20998;&#25104;&#21487;&#35299;&#37322;&#29305;&#24449;&#21644;&#19978;&#19979;&#25991;&#29305;&#24449;&#20004;&#32452;&#65292;&#24182;&#23545;&#21487;&#35299;&#37322;&#29305;&#24449;&#36827;&#34892;&#31232;&#30095;&#25311;&#21512;&#65292;&#21516;&#26102;&#20854;&#31232;&#30095;&#27169;&#24335;&#21644;&#31995;&#25968;&#20250;&#38543;&#30528;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#21464;&#21270;&#32780;&#21457;&#29983;&#21464;&#21270;&#65292;&#36825;&#20010;&#36807;&#31243;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26080;&#38656;&#21442;&#25968;&#22320;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse linear models are a gold standard tool for interpretable machine learning, a field of emerging importance as predictive models permeate decision-making in many domains. Unfortunately, sparse linear models are far less flexible as functions of their input features than black-box models like deep neural networks. With this capability gap in mind, we study a not-uncommon situation where the input features dichotomize into two groups: explanatory features, which are candidates for inclusion as variables in an interpretable model, and contextual features, which select from the candidate variables and determine their effects. This dichotomy leads us to the contextual lasso, a new statistical estimator that fits a sparse linear model to the explanatory features such that the sparsity pattern and coefficients vary as a function of the contextual features. The fitting process learns this function nonparametrically via a deep neural network. To attain sparse coefficients, we train the net
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20248;&#28857;&#26159;&#36991;&#20813;&#20102;&#22312;&#25972;&#20010;&#21487;&#34892;&#38598;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#19988;&#21033;&#29992;&#36895;&#24230;&#26469;&#34920;&#36798;&#32422;&#26463;&#65292;&#20351;&#24471;&#31639;&#27861;&#22312;&#20915;&#31574;&#21464;&#37327;&#25968;&#37327;&#21644;&#32422;&#26463;&#25968;&#37327;&#19978;&#30340;&#22797;&#26434;&#24230;&#22686;&#38271;&#36866;&#24230;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.00316</link><description>&lt;p&gt;
&#21152;&#36895;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accelerated First-Order Optimization under Nonlinear Constraints. (arXiv:2302.00316v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00316
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20248;&#28857;&#26159;&#36991;&#20813;&#20102;&#22312;&#25972;&#20010;&#21487;&#34892;&#38598;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#19988;&#21033;&#29992;&#36895;&#24230;&#26469;&#34920;&#36798;&#32422;&#26463;&#65292;&#20351;&#24471;&#31639;&#27861;&#22312;&#20915;&#31574;&#21464;&#37327;&#25968;&#37327;&#21644;&#32422;&#26463;&#25968;&#37327;&#19978;&#30340;&#22797;&#26434;&#24230;&#22686;&#38271;&#36866;&#24230;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#32422;&#26463;&#20248;&#21270;&#21644;&#38750;&#20809;&#28369;&#21160;&#21147;&#31995;&#32479;&#20043;&#38388;&#30340;&#31867;&#27604;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#12290;&#19982;Frank-Wolfe&#25110;&#25237;&#24433;&#26799;&#24230;&#19981;&#21516;&#65292;&#36825;&#20123;&#31639;&#27861;&#36991;&#20813;&#20102;&#27599;&#27425;&#36845;&#20195;&#22312;&#25972;&#20010;&#21487;&#34892;&#38598;&#19978;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25512;&#23548;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20013;&#30340;&#20984;&#35774;&#32622;&#30340;&#21152;&#36895;&#29575;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#26159;&#20351;&#29992;&#36895;&#24230;&#32780;&#19981;&#26159;&#20301;&#32622;&#26469;&#34920;&#36798;&#32422;&#26463;&#65292;&#36825;&#33258;&#28982;&#22320;&#23548;&#33268;&#21487;&#34892;&#38598;&#30340;&#31232;&#30095;&#12289;&#23616;&#37096;&#21644;&#20984;&#36817;&#20284;&#65288;&#21363;&#20351;&#21487;&#34892;&#38598;&#26159;&#38750;&#20984;&#30340;&#65289;&#12290;&#22240;&#27492;&#65292;&#22797;&#26434;&#24230;&#22312;&#20915;&#31574;&#21464;&#37327;&#25968;&#37327;&#21644;&#32422;&#26463;&#25968;&#37327;&#19978;&#36866;&#24230;&#22686;&#38271;&#65292;&#20351;&#24471;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#21387;&#32553;&#24863;&#30693;&#21644;&#31232;&#30095;&#183;&#183;&#183;
&lt;/p&gt;
&lt;p&gt;
We exploit analogies between first-order algorithms for constrained optimization and non-smooth dynamical systems to design a new class of accelerated first-order algorithms for constrained optimization. Unlike Frank-Wolfe or projected gradients, these algorithms avoid optimization over the entire feasible set at each iteration. We prove convergence to stationary points even in a nonconvex setting and we derive accelerated rates for the convex setting both in continuous time, as well as in discrete time. An important property of these algorithms is that constraints are expressed in terms of velocities instead of positions, which naturally leads to sparse, local and convex approximations of the feasible set (even if the feasible set is nonconvex). Thus, the complexity tends to grow mildly in the number of decision variables and in the number of constraints, which makes the algorithms suitable for machine learning applications. We apply our algorithms to a compressed sensing and a sparse
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25903;&#26550;&#30340;&#22810;&#30446;&#26631;&#33647;&#29289;&#20505;&#36873;&#20248;&#21270;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#24341;&#20837;&#20102;&#19968;&#31181;&#20197;&#25903;&#26550;&#20026;&#37325;&#28857;&#30340;&#22522;&#20110;&#22270;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26694;&#26550;&#65288;ScaMARS&#65289;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#21644;&#22788;&#29702;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#24615;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#26368;&#20248;&#24615;&#36136;&#30340;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2301.07175</link><description>&lt;p&gt;
&#22522;&#20110;&#25903;&#26550;&#30340;&#22810;&#30446;&#26631;&#33647;&#29289;&#20505;&#36873;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scaffold-Based Multi-Objective Drug Candidate Optimization. (arXiv:2301.07175v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07175
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25903;&#26550;&#30340;&#22810;&#30446;&#26631;&#33647;&#29289;&#20505;&#36873;&#20248;&#21270;&#30340;&#21019;&#26032;&#28857;&#22312;&#20110;&#24341;&#20837;&#20102;&#19968;&#31181;&#20197;&#25903;&#26550;&#20026;&#37325;&#28857;&#30340;&#22522;&#20110;&#22270;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26694;&#26550;&#65288;ScaMARS&#65289;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#21644;&#22788;&#29702;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#24615;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#26368;&#20248;&#24615;&#36136;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27835;&#30103;&#35774;&#35745;&#20013;&#65292;&#24179;&#34913;&#21508;&#31181;&#29702;&#21270;&#24615;&#36136;&#23545;&#20110;&#20998;&#23376;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#65292;&#31867;&#20284;&#20110;&#22810;&#21442;&#25968;&#20248;&#21270;&#65288;MPO&#65289;&#35780;&#20272;&#22810;&#20010;&#21464;&#37327;&#20197;&#23454;&#29616;&#20027;&#35201;&#30446;&#26631;&#30340;&#26041;&#24335;&#12290;&#34429;&#28982;&#29616;&#22312;&#21487;&#20197;&#20351;&#29992;&#8220;\textit{in silico}&#8221;&#26041;&#27861;&#39044;&#27979;&#35768;&#22810;&#20998;&#23376;&#29305;&#24449;&#65292;&#20174;&#32780;&#24110;&#21161;&#26089;&#26399;&#33647;&#29289;&#24320;&#21457;&#65292;&#20294;&#39640;&#36890;&#37327;&#34394;&#25311;&#31579;&#36873;&#20135;&#29983;&#30340;&#22823;&#37327;&#25968;&#25454;&#25361;&#25112;&#20102;&#20256;&#32479;MPO&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20197;&#25903;&#26550;&#20026;&#37325;&#28857;&#30340;&#22522;&#20110;&#22270;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26694;&#26550;&#65288;ScaMARS&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#26368;&#20248;&#24615;&#36136;&#30340;&#20998;&#23376;&#12290;&#36825;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#33021;&#22815;&#33258;&#25105;&#35757;&#32451;&#65292;&#24182;&#22788;&#29702;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#24615;&#36136;&#65292;&#26681;&#25454;&#36215;&#22987;&#25903;&#26550;&#37319;&#26679;&#19981;&#21516;&#30340;&#21270;&#23398;&#31354;&#38388;&#12290;&#23545;&#22810;&#20010;&#24615;&#36136;&#30340;&#22522;&#20934;&#20998;&#26512;&#26174;&#31034;&#65292;ScaMARS&#20855;&#26377;84.6&#65285;&#30340;&#22810;&#26679;&#24615;&#20998;&#25968;&#65292;&#24182;&#19988;&#19982;&#26465;&#20214;&#27169;&#22411;&#30456;&#27604;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;99.5&#65285;&#12290;&#23558;&#26032;&#29305;&#24615;&#38598;&#25104;&#21040;MPO&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In therapeutic design, balancing various physiochemical properties is crucial for molecule development, similar to how Multiparameter Optimization (MPO) evaluates multiple variables to meet a primary goal. While many molecular features can now be predicted using \textit{in silico} methods, aiding early drug development, the vast data generated from high throughput virtual screening challenges the practicality of traditional MPO approaches. Addressing this, we introduce a scaffold focused graph-based Markov chain Monte Carlo framework (ScaMARS) built to generate molecules with optimal properties. This innovative framework is capable of self-training and handling a wider array of properties, sampling different chemical spaces according to the starting scaffold. The benchmark analysis on several properties shows that ScaMARS has a diversity score of 84.6\% and has a much higher success rate of 99.5\% compared to conditional models. The integration of new features into MPO significantly en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#39118;&#38505;&#20215;&#20540;&#30340;&#33258;&#20027;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#39640;&#32622;&#20449;&#24230;&#36793;&#30028;&#26469;&#30830;&#23450;&#26159;&#21542;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#28436;&#31034;&#12290;&#20316;&#32773;&#23450;&#20041;&#20102;&#20004;&#31181;&#20805;&#20998;&#24615;&#25351;&#26631;&#65292;&#24182;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15542</link><description>&lt;p&gt;
&#33258;&#20027;&#35780;&#20272;&#36890;&#36807;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#28436;&#31034;&#20805;&#20998;&#24615;
&lt;/p&gt;
&lt;p&gt;
Autonomous Assessment of Demonstration Sufficiency via Bayesian Inverse Reinforcement Learning. (arXiv:2211.15542v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#39118;&#38505;&#20215;&#20540;&#30340;&#33258;&#20027;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#39640;&#32622;&#20449;&#24230;&#36793;&#30028;&#26469;&#30830;&#23450;&#26159;&#21542;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#28436;&#31034;&#12290;&#20316;&#32773;&#23450;&#20041;&#20102;&#20004;&#31181;&#20805;&#20998;&#24615;&#25351;&#26631;&#65292;&#24182;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#30830;&#23450;&#28436;&#31034;&#20805;&#20998;&#24615;&#30340;&#38382;&#39064;&#65306;&#26426;&#22120;&#20154;&#22914;&#20309;&#33258;&#25105;&#35780;&#20272;&#23427;&#26159;&#21542;&#24050;&#32463;&#20174;&#19987;&#23478;&#37027;&#37324;&#33719;&#24471;&#36275;&#22815;&#30340;&#28436;&#31034;&#20197;&#30830;&#20445;&#25152;&#38656;&#30340;&#24615;&#33021;&#27700;&#24179;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#39118;&#38505;&#20215;&#20540;&#30340;&#26032;&#22411;&#33258;&#25105;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#24471;&#23398;&#20064;&#20174;&#28436;&#31034;&#20013;&#30340;&#26426;&#22120;&#20154;&#33021;&#22815;&#35745;&#31639;&#20854;&#24615;&#33021;&#30340;&#39640;&#32622;&#20449;&#24230;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#36793;&#30028;&#26469;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#28436;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#20805;&#20998;&#24615;&#30340;&#23450;&#20041;&#65306;&#65288;1&#65289;&#26631;&#20934;&#21270;&#26399;&#26395;&#20540;&#24046;&#24322;&#65292;&#29992;&#20110;&#34913;&#37327;&#30456;&#23545;&#20110;&#20154;&#31867;&#26410;&#35266;&#23519;&#21040;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#36951;&#25022;&#65292;&#20197;&#21450;&#65288;2&#65289;&#30456;&#23545;&#20110;&#22522;&#20934;&#31574;&#30053;&#30340;&#25913;&#36827;&#30334;&#20998;&#27604;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23545;&#36825;&#20004;&#20010;&#25351;&#26631;&#21046;&#23450;&#39640;&#32622;&#20449;&#24230;&#36793;&#30028;&#12290;&#25105;&#20204;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#39046;&#22495;&#30340;&#27169;&#25311;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35828;&#26126;&#20102;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#33258;&#20027;&#35780;&#20272;&#28436;&#31034;&#20805;&#20998;&#24615;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the problem of determining demonstration sufficiency: how can a robot self-assess whether it has received enough demonstrations from an expert to ensure a desired level of performance? To address this problem, we propose a novel self-assessment approach based on Bayesian inverse reinforcement learning and value-at-risk, enabling learning-from-demonstration ("LfD") robots to compute high-confidence bounds on their performance and use these bounds to determine when they have a sufficient number of demonstrations. We propose and evaluate two definitions of sufficiency: (1) normalized expected value difference, which measures regret with respect to the human's unobserved reward function, and (2) percent improvement over a baseline policy. We demonstrate how to formulate high-confidence bounds on both of these metrics. We evaluate our approach in simulation for both discrete and continuous state-space domains and illustrate the feasibility of developing a robotic system that can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25286;&#20998;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#22788;&#29702;&#30340;&#26696;&#20363;&#12290;&#36890;&#36807;&#26500;&#24314;tf.data&#26381;&#21153;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#25286;&#20998;&#65292;&#20197;&#25552;&#39640;&#21152;&#36895;&#22120;&#21644;&#20027;&#26426;&#36164;&#28304;&#30340;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.14826</link><description>&lt;p&gt;
tf.data&#26381;&#21153;&#65306;&#25286;&#20998;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#22788;&#29702;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
tf.data service: A Case for Disaggregating ML Input Data Processing. (arXiv:2210.14826v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25286;&#20998;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#22788;&#29702;&#30340;&#26696;&#20363;&#12290;&#36890;&#36807;&#26500;&#24314;tf.data&#26381;&#21153;&#65292;&#21487;&#20197;&#23454;&#29616;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#25286;&#20998;&#65292;&#20197;&#25552;&#39640;&#21152;&#36895;&#22120;&#21644;&#20027;&#26426;&#36164;&#28304;&#30340;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35745;&#31639;&#36890;&#24120;&#22312;&#26114;&#36149;&#30340;&#19987;&#29992;&#30828;&#20214;&#19978;&#25191;&#34892;&#65292;&#22914;GPU&#21644;TPU&#65292;&#23427;&#20204;&#25552;&#20379;&#39640;FLOP&#21644;&#27599;&#29926;&#24615;&#33021;&#12290;&#20026;&#20102;&#25104;&#26412;&#25928;&#30410;&#65292;&#24517;&#39035;&#20445;&#25345;&#36825;&#20123;&#21152;&#36895;&#22120;&#30340;&#39640;&#21033;&#29992;&#29575;&#12290;&#36825;&#38656;&#35201;&#20197;&#21152;&#36895;&#22120;&#21487;&#20197;&#25509;&#25910;&#21644;&#25191;&#34892;&#25968;&#25454;&#30340;&#36895;&#29575;&#39044;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#12290;&#20026;&#20102;&#36991;&#20813;&#25968;&#25454;&#20572;&#39039;&#65292;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#35745;&#31639;&#30340;&#21152;&#36895;&#22120;&#26680;&#24515;&#25152;&#38656;&#30340;&#20027;&#26426;CPU&#21644;RAM&#22312;&#19981;&#21516;&#30340;&#20316;&#19994;&#20013;&#26159;&#21487;&#21464;&#30340;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#22312;&#20855;&#26377;&#22266;&#23450;&#30828;&#20214;&#27604;&#20363;&#30340;ML&#21152;&#36895;&#22120;&#20027;&#26426;&#19978;&#22788;&#29702;&#36755;&#20837;&#25968;&#25454;&#30340;&#26041;&#27861;&#20250;&#23548;&#33268;&#21152;&#36895;&#22120;&#25110;&#20027;&#26426;CPU&#21644;RAM&#30340;&#20302;&#21033;&#29992;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#25286;&#20998;&#30340;ML&#25968;&#25454;&#22788;&#29702;&#31995;&#32479;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;tf.data service&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;TensorFlow&#30340;tf.data&#20043;&#19978;&#30340;&#24320;&#28304;&#25286;&#20998;&#36755;&#20837;&#25968;&#25454;&#22788;&#29702;&#26381;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#25968;&#25454;&#39044;&#22788;&#29702;&#25286;&#20998;&#30340;&#19977;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) computations commonly execute on expensive specialized hardware, such as GPUs and TPUs, which provide high FLOPs and performance-per-watt. For cost efficiency, it is essential to keep these accelerators highly utilized. This requires preprocessing input data at the rate at which the accelerators can ingest and perform ML computations on the data. To avoid data stalls, the host CPU and RAM required for input data processing per accelerator core used for ML computations varies across jobs. Hence, the traditional approach of processing input data on ML accelerator hosts with a fixed hardware ratio leads to either under-utilizing the accelerators or the host CPU and RAM. In this paper, we address these concerns by building a disaggregated ML data processing system.  We present tf.data service, an open-source disaggregated input data processing service built on top of tf.data in TensorFlow. We show that disaggregating data preprocessing has three key advantages for lar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#36827;&#34892;&#20102;&#36817;&#20284;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#30456;&#23545;&#20110;&#20256;&#32479;&#32447;&#24615;&#21464;&#25442;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#36890;&#36807;&#21019;&#36896;&#24615;&#30340;&#26500;&#36896;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#19988;&#25506;&#31350;&#20102;&#28145;&#23618;&#32593;&#32476;&#30340;&#20989;&#25968;&#36924;&#36817;&#36895;&#24230;&#12290;&#32447;&#24615;&#29305;&#24449;&#30340;&#22810;&#20998;&#36776;&#21367;&#31215;&#20998;&#35299;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2210.09041</link><description>&lt;p&gt;
&#20174;&#29305;&#24449;&#25552;&#21462;&#35282;&#24230;&#30340;CNN&#36817;&#20284;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Approximation analysis of CNNs from a feature extraction view. (arXiv:2210.09041v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#36827;&#34892;&#20102;&#36817;&#20284;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#30456;&#23545;&#20110;&#20256;&#32479;&#32447;&#24615;&#21464;&#25442;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#36890;&#36807;&#21019;&#36896;&#24615;&#30340;&#26500;&#36896;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#19988;&#25506;&#31350;&#20102;&#28145;&#23618;&#32593;&#32476;&#30340;&#20989;&#25968;&#36924;&#36817;&#36895;&#24230;&#12290;&#32447;&#24615;&#29305;&#24449;&#30340;&#22810;&#20998;&#36776;&#21367;&#31215;&#20998;&#35299;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#25104;&#21151;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#32593;&#32476;&#26550;&#26500;&#21644;&#32467;&#26500;&#30340;&#38480;&#21046;&#65292;&#23427;&#32570;&#20047;&#36275;&#22815;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#24230;&#22810;&#36890;&#36947;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#24314;&#31435;&#20102;&#19968;&#20123;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#30340;&#20998;&#26512;&#65292;&#36825;&#35777;&#26126;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20256;&#32479;&#32447;&#24615;&#21464;&#25442;&#65288;&#22914;&#20613;&#37324;&#21494;&#21464;&#25442;&#12289;&#23567;&#27874;&#21464;&#25442;&#12289;&#20887;&#20313;&#23383;&#20856;&#32534;&#30721;&#26041;&#27861;&#65289;&#19978;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#26500;&#36896;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22810;&#36890;&#36947;CNNs&#39640;&#25928;&#22320;&#36827;&#34892;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#38477;&#20302;&#36924;&#36817;&#39640;&#32500;&#20989;&#25968;&#25152;&#38656;&#30340;&#22522;&#26412;&#32500;&#24230;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20351;&#29992;&#36890;&#36947;&#23454;&#29616;&#30340;&#28145;&#23618;&#32593;&#32476;&#21644;&#20840;&#36830;&#25509;&#23618;&#30340;&#20989;&#25968;&#36924;&#36817;&#36895;&#29575;&#12290;&#23558;&#32447;&#24615;&#29305;&#24449;&#22240;&#23376;&#20998;&#35299;&#20026;&#22810;&#20998;&#36776;&#21367;&#31215;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning based on deep neural networks has been very successful in many practical applications, but it lacks enough theoretical understanding due to the network architectures and structures. In this paper we establish some analysis for linear feature extraction by a deep multi-channel convolutional neural networks (CNNs), which demonstrates the power of deep learning over traditional linear transformations, like Fourier, wavelets, redundant dictionary coding methods. Moreover, we give an exact construction presenting how linear features extraction can be conducted efficiently with multi-channel CNNs. It can be applied to lower the essential dimension for approximating a high dimensional function. Rates of function approximation by such deep networks implemented with channels and followed by fully-connected layers are investigated as well. Harmonic analysis for factorizing linear features into multi-resolution convolutions plays an essential role in our work. Nevertheless, a dedica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20869;&#23481;&#28508;&#21464;&#37327;&#20197;&#21450;&#21512;&#25104;&#32441;&#29702;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06950</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossy Image Compression with Conditional Diffusion Models. (arXiv:2209.06950v5 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#20869;&#23481;&#28508;&#21464;&#37327;&#20197;&#21450;&#21512;&#25104;&#32441;&#29702;&#21464;&#37327;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20248;&#21270;&#30340;&#26377;&#25439;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21464;&#25442;&#32534;&#30721;&#33539;&#24335;&#65292;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#20449;&#24687;&#29109;&#32534;&#30721;&#65292;&#28982;&#21518;&#20877;&#26144;&#23556;&#22238;&#25968;&#25454;&#31354;&#38388;&#36827;&#34892;&#37325;&#26500;&#12290;&#19982;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#31070;&#32463;&#21387;&#32553;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35299;&#30721;&#22120;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#8220;&#20869;&#23481;&#8221;&#28508;&#21464;&#37327;&#65292;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20250;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#24182;&#21033;&#29992;&#35813;&#21464;&#37327;&#23384;&#20648;&#22270;&#20687;&#20449;&#24687;&#12290;&#20915;&#23450;&#25193;&#25955;&#36807;&#31243;&#30340;&#21097;&#20313;&#8220;&#32441;&#29702;&#8221;&#21464;&#37327;&#20250;&#22312;&#35299;&#30721;&#26102;&#21512;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#20197;&#26681;&#25454;&#24863;&#30693;&#24230;&#37327;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#28041;&#21450;&#20102;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#36739;&#20110;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26041;&#27861;&#33021;&#22815;&#24471;&#21040;&#26356;&#22909;&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper outlines an end-to-end optimized lossy image compression framework using diffusion generative models. The approach relies on the transform coding paradigm, where an image is mapped into a latent space for entropy coding and, from there, mapped back to the data space for reconstruction. In contrast to VAE-based neural compression, where the (mean) decoder is a deterministic neural network, our decoder is a conditional diffusion model. Our approach thus introduces an additional "content" latent variable on which the reverse diffusion process is conditioned and uses this variable to store information about the image. The remaining "texture" variables characterizing the diffusion process are synthesized at decoding time. We show that the model's performance can be tuned toward perceptual metrics of interest. Our extensive experiments involving multiple datasets and image quality assessment metrics show that our approach yields stronger reported FID scores than the GAN-based mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;Bandits&#20013;&#30340;&#25490;&#21517;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;UCB&#21644;Thompson Sampling&#31867;&#22411;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#24182;&#23545;&#20301;&#32622;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20102;&#24314;&#27169;&#12290;&#30740;&#31350;&#32467;&#26524;&#22312;&#20301;&#32622;&#20381;&#36182;&#24615;&#21644;&#25490;&#21517;&#38382;&#39064;&#19982;&#22270;&#35770;&#30340;&#36830;&#25509;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2207.00109</link><description>&lt;p&gt;
&#24191;&#20041;&#32447;&#24615;Bandits&#20013;&#30340;&#25490;&#21517;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Ranking In Generalized Linear Bandits. (arXiv:2207.00109v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;Bandits&#20013;&#30340;&#25490;&#21517;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;UCB&#21644;Thompson Sampling&#31867;&#22411;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#24182;&#23545;&#20301;&#32622;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20102;&#24314;&#27169;&#12290;&#30740;&#31350;&#32467;&#26524;&#22312;&#20301;&#32622;&#20381;&#36182;&#24615;&#21644;&#25490;&#21517;&#38382;&#39064;&#19982;&#22270;&#35770;&#30340;&#36830;&#25509;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24191;&#20041;&#32447;&#24615;Bandits&#20013;&#30340;&#25490;&#21517;&#38382;&#39064;&#12290;&#22312;&#27599;&#20010;&#26102;&#21051;&#65292;&#23398;&#20064;&#20195;&#29702;&#36873;&#25321;&#19968;&#20010;&#26377;&#24207;&#30340;&#29289;&#21697;&#21015;&#34920;&#65292;&#24182;&#35266;&#23519;&#38543;&#26426;&#32467;&#26524;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#26174;&#31034;&#19968;&#20010;&#26377;&#24207;&#30340;&#26368;&#20855;&#21560;&#24341;&#21147;&#30340;&#29289;&#21697;&#21015;&#34920;&#24182;&#19981;&#24635;&#26159;&#26368;&#20248;&#30340;&#65292;&#22240;&#20026;&#20301;&#32622;&#21644;&#29289;&#21697;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#20363;&#23376;&#26159;&#24403;&#25152;&#26377;&#26368;&#20855;&#21560;&#24341;&#21147;&#30340;&#29289;&#21697;&#37117;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#26102;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#23545;&#26377;&#24207;&#21015;&#34920;&#20013;&#30340;&#20301;&#32622;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35774;&#35745;&#20102;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;UCB&#21644;Thompson Sampling&#31867;&#22411;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#20960;&#20010;&#26041;&#21521;&#19978;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#20301;&#32622;&#20381;&#36182;&#24615;&#65292;&#20854;&#20013;&#20301;&#32622;&#25240;&#25187;&#26159;&#19968;&#20010;&#29305;&#20363;&#65292;&#24182;&#23558;&#25490;&#21517;&#38382;&#39064;&#19982;&#22270;&#35770;&#30456;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the ranking problem in generalized linear bandits. At each time, the learning agent selects an ordered list of items and observes stochastic outcomes. In recommendation systems, displaying an ordered list of the most attractive items is not always optimal as both position and item dependencies result in a complex reward function. A very naive example is the lack of diversity when all the most attractive items are from the same category. We model the position and item dependencies in the ordered list and design UCB and Thompson Sampling type algorithms for this problem. Our work generalizes existing studies in several directions, including position dependencies where position discount is a particular case, and connecting the ranking problem to graph theory.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#21462;&#36135;&#21644;&#36865;&#36135;&#23545;&#36335;&#36793;&#20132;&#36890;&#30340;&#25317;&#22581;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#21644;&#20998;&#31163;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2206.02164</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#20943;&#36731;&#36335;&#36793;&#21462;&#36135;&#21644;&#36865;&#36135;&#30340;&#25317;&#22581;&#24433;&#21709;&#65306;&#19968;&#31181;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Estimating and Mitigating the Congestion Effect of Curbside Pick-ups and Drop-offs: A Causal Inference Approach. (arXiv:2206.02164v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#21462;&#36135;&#21644;&#36865;&#36135;&#23545;&#36335;&#36793;&#20132;&#36890;&#30340;&#25317;&#22581;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#21644;&#20998;&#31163;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36335;&#36793;&#31354;&#38388;&#26159;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#26368;&#32321;&#24537;&#30340;&#21306;&#22495;&#20043;&#19968;&#12290;&#29305;&#21035;&#26159;&#36817;&#24180;&#26469;&#65292;&#32593;&#32422;&#36710;&#21644;&#21830;&#19994;&#37197;&#36865;&#30340;&#24555;&#36895;&#22686;&#38271;&#23548;&#33268;&#20102;&#22823;&#37327;&#30340;&#21462;&#36135;&#21644;&#36865;&#36135;&#65288;PUDOs&#65289;&#65292;&#36825;&#20123;&#21344;&#25454;&#20102;&#20960;&#21313;&#24180;&#21069;&#35774;&#35745;&#24314;&#36896;&#30340;&#26377;&#38480;&#36335;&#36793;&#31354;&#38388;&#12290;&#36825;&#20123;PUDOs&#21487;&#33021;&#23548;&#33268;&#36335;&#36793;&#21033;&#29992;&#29575;&#30340;&#25317;&#22581;&#21644;&#24178;&#25200;&#20027;&#32447;&#20132;&#36890;&#27969;&#65292;&#26126;&#26174;&#24102;&#26469;&#26174;&#33879;&#30340;&#36127;&#38754;&#31038;&#20250;&#22806;&#37096;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#20005;&#26684;&#37327;&#21270;&#21644;&#20943;&#36731;PUDOs&#25317;&#22581;&#24433;&#21709;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#23588;&#20854;&#26159;&#32570;&#20047;&#25968;&#25454;&#25903;&#25345;&#21644;&#28151;&#28102;&#25928;&#24212;&#30340;&#21442;&#19982;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#26412;&#25991;&#37319;&#29992;&#20005;&#26684;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#35780;&#20272;PUDOs&#23545;&#19968;&#33324;&#21306;&#22495;&#32593;&#32476;&#30340;&#25317;&#22581;&#24433;&#21709;&#12290;&#24314;&#31435;&#20102;&#19968;&#20010;&#22240;&#26524;&#22270;&#26469;&#34920;&#31034;PUDOs&#21644;&#20132;&#36890;&#36895;&#24230;&#20043;&#38388;&#30340;&#26102;&#31354;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#21644;&#20998;&#31163;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;DSML&#65289;&#26041;&#27861;&#26469;&#37327;&#21270;PUDOs&#23545;&#20132;&#36890;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curb space is one of the busiest areas in urban road networks. Especially in recent years, the rapid increase of ride-hailing trips and commercial deliveries has induced massive pick-ups/drop-offs (PUDOs), which occupy the limited curb space that was designed and built decades ago. These PUDOs could jam curbside utilization and disturb the mainline traffic flow, evidently leading to significant negative societal externalities. However, there is a lack of an analytical framework that rigorously quantifies and mitigates the congestion effect of PUDOs in the system view, particularly with little data support and involvement of confounding effects. To bridge this research gap, this paper develops a rigorous causal inference approach to estimate the congestion effect of PUDOs on general regional networks. A causal graph is set to represent the spatio-temporal relationship between PUDOs and traffic speed, and a double and separated machine learning (DSML) method is proposed to quantify how P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#22240;&#26524;&#26426;&#21046;&#30340;&#20998;&#31163;&#34920;&#31034;&#65292;&#36890;&#36807;&#20272;&#35745;&#21407;&#22987;&#21644;&#26032;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;&#27010;&#29575;&#24046;&#24322;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#36924;&#36817;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#23398;&#20064;&#32773;&#23545;&#26032;&#20998;&#24067;&#30340;&#36866;&#24212;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#26356;&#21152;&#26679;&#26412;&#39640;&#25928;&#19988;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2201.01942</link><description>&lt;p&gt;
&#39640;&#25928;&#22320;&#35299;&#24320;&#22240;&#26524;&#34920;&#31034;&#20132;&#32455;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Efficiently Disentangle Causal Representations. (arXiv:2201.01942v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#22240;&#26524;&#26426;&#21046;&#30340;&#20998;&#31163;&#34920;&#31034;&#65292;&#36890;&#36807;&#20272;&#35745;&#21407;&#22987;&#21644;&#26032;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;&#27010;&#29575;&#24046;&#24322;&#65292;&#24182;&#21033;&#29992;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#36924;&#36817;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#23398;&#20064;&#32773;&#23545;&#26032;&#20998;&#24067;&#30340;&#36866;&#24212;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#26356;&#21152;&#26679;&#26412;&#39640;&#25928;&#19988;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#20998;&#24067;&#21644;&#26032;&#20998;&#24067;&#30340;&#26465;&#20214;&#27010;&#29575;&#20043;&#24046;&#30340;&#22240;&#26524;&#26426;&#21046;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26469;&#36924;&#36817;&#36825;&#31181;&#24046;&#24322;&#65292;&#20351;&#20854;&#36866;&#24212;&#26631;&#20934;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#24182;&#33021;&#22815;&#39640;&#25928;&#22320;&#35745;&#31639;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#23398;&#20064;&#32773;&#23545;&#26032;&#20998;&#24067;&#30340;&#36866;&#24212;&#36895;&#24230;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#21183;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#27604;&#20808;&#21069;&#26041;&#27861;&#26356;&#33410;&#32422;&#26679;&#26412;&#65292;&#36895;&#24230;&#26356;&#24555;&#65292;&#20998;&#21035;&#25552;&#21319;&#20102;1.9-11.0&#20493;&#21644;9.4-32.4&#20493;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312; \url{https://github.com/yuanpeng16/EDCR} &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient approach to learning disentangled representations with causal mechanisms based on the difference of conditional probabilities in original and new distributions. We approximate the difference with models' generalization abilities so that it fits in the standard machine learning framework and can be efficiently computed. In contrast to the state-of-the-art approach, which relies on the learner's adaptation speed to new distribution, the proposed approach only requires evaluating the model's generalization ability. We provide a theoretical explanation for the advantage of the proposed method, and our experiments show that the proposed technique is 1.9--11.0$\times$ more sample efficient and 9.4--32.4 times quicker than the previous method on various tasks. The source code is available at \url{https://github.com/yuanpeng16/EDCR}.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20849;&#21516;&#20272;&#35745;&#22810;&#20010;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#30340;&#36716;&#31227;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#25968;&#25454;&#27719;&#38598;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#37325;&#35201;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2112.10955</link><description>&lt;p&gt;
&#20849;&#21516;&#23398;&#20064;&#32447;&#24615;&#26102;&#19981;&#21464;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Linear Time-Invariant Dynamical Systems. (arXiv:2112.10955v6 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.10955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20849;&#21516;&#20272;&#35745;&#22810;&#20010;&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#30340;&#36716;&#31227;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#25968;&#25454;&#27719;&#38598;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#37325;&#35201;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#19981;&#21464;&#31995;&#32479;&#26159;&#31995;&#32479;&#35770;&#21644;&#24212;&#29992;&#20013;&#38750;&#24120;&#27969;&#34892;&#30340;&#27169;&#22411;&#12290;&#31995;&#32479;&#36776;&#35782;&#20013;&#19968;&#20010;&#26410;&#21463;&#21040;&#20805;&#20998;&#20851;&#27880;&#30340;&#22522;&#26412;&#38382;&#39064;&#26159;&#22914;&#20309;&#21033;&#29992;&#30456;&#20851;&#32447;&#24615;&#31995;&#32479;&#20043;&#38388;&#30340;&#20849;&#24615;&#26469;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#23427;&#20204;&#30340;&#36716;&#31227;&#30697;&#38453;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#21512;&#20272;&#35745;&#22810;&#20010;&#31995;&#32479;&#30340;&#36716;&#31227;&#30697;&#38453;&#30340;&#26041;&#27861;&#12290;&#20551;&#35774;&#36716;&#31227;&#30697;&#38453;&#26159;&#19968;&#20123;&#26410;&#30693;&#20849;&#20139;&#22522;&#30784;&#30697;&#38453;&#30340;&#26410;&#30693;&#32447;&#24615;&#20989;&#25968;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#23436;&#20840;&#21453;&#26144;&#32771;&#34385;&#30340;&#36712;&#36857;&#38271;&#24230;&#12289;&#32500;&#24230;&#21644;&#31995;&#32479;&#25968;&#37327;&#30340;&#26377;&#38480;&#26102;&#38388;&#20272;&#35745;&#35823;&#24046;&#29575;&#12290;&#25152;&#21576;&#29616;&#30340;&#32467;&#26524;&#30456;&#24403;&#26222;&#36941;&#65292;&#24182;&#26174;&#31034;&#20102;&#19982;&#21333;&#29420;&#23398;&#20064;&#27599;&#20010;&#31995;&#32479;&#30456;&#27604;&#65292;&#36890;&#36807;&#31995;&#32479;&#20043;&#38388;&#30340;&#25968;&#25454;&#27719;&#38598;&#21487;&#20197;&#33719;&#24471;&#30340;&#26174;&#33879;&#25910;&#30410;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#36824;&#26174;&#31034;&#20102;&#23545;&#27169;&#22411;&#38169;&#35823;&#35774;&#23450;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#24471;&#21040;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear time-invariant systems are very popular models in system theory and applications. A fundamental problem in system identification that remains rather unaddressed in extant literature is to leverage commonalities amongst related linear systems to estimate their transition matrices more accurately. To address this problem, the current paper investigates methods for jointly estimating the transition matrices of multiple systems. It is assumed that the transition matrices are unknown linear functions of some unknown shared basis matrices. We establish finite-time estimation error rates that fully reflect the roles of trajectory lengths, dimension, and number of systems under consideration. The presented results are fairly general and show the significant gains that can be achieved by pooling data across systems in comparison to learning each system individually. Further, they are shown to be robust against model misspecifications. To obtain the results, we develop novel techniques th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#35268;&#39044;&#27979;&#25216;&#26415;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#26426;&#22120;&#20154;/&#29615;&#22659;&#21160;&#21147;&#23398;&#27169;&#25311;&#22120;&#26469;&#35843;&#25972;&#39044;&#35686;&#31995;&#32479;&#65292;&#33021;&#22815;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#25968;&#25454;&#28857;&#35777;&#26126;&#36798;&#21040;&#21487;&#25509;&#21463;&#30340;&#35823;&#25253;&#29575;&#65292;&#24182;&#22312;&#39550;&#39542;&#21592;&#39044;&#35686;&#31995;&#32479;&#21644;&#26426;&#22120;&#20154;&#25235;&#21462;&#24212;&#29992;&#20013;&#23454;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2109.14082</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#35268;&#39044;&#27979;&#25552;&#20379;&#26679;&#26412;&#25928;&#29575;&#39640;&#30340;&#23433;&#20840;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Safety Assurances using Conformal Prediction. (arXiv:2109.14082v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.14082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#35268;&#39044;&#27979;&#25216;&#26415;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#26426;&#22120;&#20154;/&#29615;&#22659;&#21160;&#21147;&#23398;&#27169;&#25311;&#22120;&#26469;&#35843;&#25972;&#39044;&#35686;&#31995;&#32479;&#65292;&#33021;&#22815;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#25968;&#25454;&#28857;&#35777;&#26126;&#36798;&#21040;&#21487;&#25509;&#21463;&#30340;&#35823;&#25253;&#29575;&#65292;&#24182;&#22312;&#39550;&#39542;&#21592;&#39044;&#35686;&#31995;&#32479;&#21644;&#26426;&#22120;&#20154;&#25235;&#21462;&#24212;&#29992;&#20013;&#23454;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#33021;&#22815;&#26816;&#27979;&#19981;&#23433;&#20840;&#24773;&#20917;&#30340;&#33021;&#21147;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#39044;&#35686;&#31995;&#32479;&#21487;&#20197;&#22312;&#19981;&#37319;&#21462;&#32416;&#27491;&#25514;&#26045;&#30340;&#24773;&#20917;&#19979;&#25552;&#21069;&#25552;&#20379;&#35686;&#25253;&#65292;&#20197;&#39044;&#31034;&#21487;&#33021;&#20986;&#29616;&#30340;&#19981;&#23433;&#20840;&#24773;&#20917;&#12290;&#20026;&#20102;&#21487;&#38752;&#22320;&#25552;&#39640;&#23433;&#20840;&#24615;&#65292;&#36825;&#20123;&#39044;&#35686;&#31995;&#32479;&#24212;&#35813;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#35823;&#25253;&#29575;&#65292;&#21363;&#22312;&#19981;&#23433;&#20840;&#30340;&#24773;&#20917;&#19979;&#65292;&#20250;&#20986;&#29616;&#35686;&#25253;&#30340;&#27010;&#29575;&#23567;&#20110;&#1013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#32479;&#35745;&#25512;&#26029;&#25216;&#26415;&#65288;&#31216;&#20026;&#21512;&#35268;&#39044;&#27979;&#65289;&#19982;&#26426;&#22120;&#20154;/&#29615;&#22659;&#21160;&#21147;&#23398;&#27169;&#25311;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35843;&#25972;&#39044;&#35686;&#31995;&#32479;&#65292;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#25968;&#25454;&#28857;&#35777;&#26126;&#36798;&#21040;&#1013;&#30340;&#35823;&#25253;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#39550;&#39542;&#21592;&#39044;&#35686;&#31995;&#32479;&#21644;&#26426;&#22120;&#20154;&#25235;&#21462;&#24212;&#29992;&#65292;&#24182;&#22312;&#23454;&#35777;&#20013;&#35777;&#26126;&#20102;&#20445;&#35777;&#30340;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35266;&#23519;&#21040;&#36739;&#20302;&#30340;&#35823;&#25253;&#29575;&#65288;&#38451;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying machine learning models in high-stakes robotics applications, the ability to detect unsafe situations is crucial. Early warning systems can provide alerts when an unsafe situation is imminent (in the absence of corrective action). To reliably improve safety, these warning systems should have a provable false negative rate; i.e. of the situations that are unsafe, fewer than $\epsilon$ will occur without an alert. In this work, we present a framework that combines a statistical inference technique known as conformal prediction with a simulator of robot/environment dynamics, in order to tune warning systems to provably achieve an $\epsilon$ false negative rate using as few as $1/\epsilon$ data points. We apply our framework to a driver warning system and a robotic grasping application, and empirically demonstrate guaranteed false negative rate while also observing low false detection (positive) rate.
&lt;/p&gt;</description></item></channel></rss>