<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#8220;&#36234;&#29425;&#8221;&#65292;&#32780;&#21253;&#25324;&#36843;&#20351;&#27169;&#22411;&#23637;&#31034;&#21508;&#31181;&#24847;&#22806;&#34892;&#20026;&#65292;&#25915;&#20987;&#34920;&#38754;&#21644;&#30446;&#26631;&#24191;&#27867;&#12290;&#36825;&#20123;&#25915;&#20987;&#28304;&#20110;LLMs&#30340;&#39044;&#35757;&#32451;&#21644;&#24120;&#35265;&#35789;&#27719;&#20013;&#23384;&#22312;&#30340;&#8220;&#25925;&#38556;&#8221;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2402.14020</link><description>&lt;p&gt;
&#36843;&#20351;LLMs&#25191;&#34892;&#24182;&#25581;&#31034;&#65288;&#20960;&#20046;&#65289;&#20219;&#20309;&#20107;&#24773;
&lt;/p&gt;
&lt;p&gt;
Coercing LLMs to do and reveal (almost) anything
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#8220;&#36234;&#29425;&#8221;&#65292;&#32780;&#21253;&#25324;&#36843;&#20351;&#27169;&#22411;&#23637;&#31034;&#21508;&#31181;&#24847;&#22806;&#34892;&#20026;&#65292;&#25915;&#20987;&#34920;&#38754;&#21644;&#30446;&#26631;&#24191;&#27867;&#12290;&#36825;&#20123;&#25915;&#20987;&#28304;&#20110;LLMs&#30340;&#39044;&#35757;&#32451;&#21644;&#24120;&#35265;&#35789;&#27719;&#20013;&#23384;&#22312;&#30340;&#8220;&#25925;&#38556;&#8221;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#20197;&#8220;&#36234;&#29425;&#8221;&#35813;&#27169;&#22411;&#20197;&#21457;&#34920;&#26377;&#23475;&#35328;&#35770;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;LLMs&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#33539;&#22260;&#36828;&#19981;&#27490;&#20110;&#36234;&#29425;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#21487;&#33021;&#30340;&#25915;&#20987;&#38754;&#21644;&#25915;&#20987;&#30446;&#26631;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;&#26681;&#25454;&#19968;&#31995;&#21015;&#20855;&#20307;&#31034;&#20363;&#65292;&#25105;&#20204;&#35752;&#35770;&#12289;&#20998;&#31867;&#21644;&#31995;&#32479;&#21270;&#20102;&#19968;&#20123;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#36843;&#20351;LLMs&#23637;&#31034;&#21508;&#31181;&#24847;&#22806;&#34892;&#20026;&#65292;&#22914;&#35823;&#23548;&#12289;&#27169;&#22411;&#25511;&#21046;&#12289;&#25298;&#32477;&#26381;&#21153;&#25110;&#25968;&#25454;&#25552;&#21462;&#12290;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#20998;&#26512;&#36825;&#20123;&#25915;&#20987;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#35768;&#22810;&#26159;&#30001;&#20110;&#39044;&#35757;&#32451;LLMs&#20855;&#26377;&#32534;&#30721;&#33021;&#21147;&#30340;&#23454;&#36341;&#65292;&#20197;&#21450;&#24120;&#35265;LLMs&#35789;&#27719;&#20013;&#24212;&#21024;&#38500;&#30340;&#22855;&#24618;&#8220;&#25925;&#38556;&#8221;&#26631;&#35760;&#30340;&#25345;&#32493;&#23384;&#22312;&#25152;&#23548;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14020v1 Announce Type: cross  Abstract: It has recently been shown that adversarial attacks on large language models (LLMs) can "jailbreak" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange "glitch" tokens in common LLM vocabularies that should be removed for security reasons.
&lt;/p&gt;</description></item><item><title>D-Flow&#26694;&#26550;&#36890;&#36807;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#27969;&#24418;&#65292;&#20248;&#21270;&#28304;&#28857;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#21644;&#27969;&#21305;&#37197;&#27169;&#22411;&#20013;&#29983;&#25104;&#32467;&#26524;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.14017</link><description>&lt;p&gt;
D-Flow: &#36890;&#36807;&#27969;&#24418;&#36827;&#34892;&#21306;&#20998;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
D-Flow: Differentiating through Flows for Controlled Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14017
&lt;/p&gt;
&lt;p&gt;
D-Flow&#26694;&#26550;&#36890;&#36807;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#27969;&#24418;&#65292;&#20248;&#21270;&#28304;&#28857;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#21644;&#27969;&#21305;&#37197;&#27169;&#22411;&#20013;&#29983;&#25104;&#32467;&#26524;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#21516;&#26102;&#22312;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#24403;&#20170;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#21644;&#27969;&#21305;&#37197;&#65288;FM&#65289;&#27169;&#22411;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#65292;&#21487;&#20197;&#35299;&#20915;&#21453;&#38382;&#39064;&#12289;&#26465;&#20214;&#29983;&#25104;&#21644;&#19968;&#33324;&#25511;&#21046;&#29983;&#25104;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;D-Flow&#65292;&#19968;&#31181;&#31616;&#21333;&#30340;&#36890;&#36807;&#27969;&#24418;&#25511;&#21046;&#29983;&#25104;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#28304;&#65288;&#22122;&#22768;&#65289;&#28857;&#36827;&#34892;&#21306;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26469;&#28608;&#21457;&#36825;&#19968;&#26694;&#26550;&#65292;&#35813;&#35266;&#23519;&#25351;&#20986;&#65292;&#23545;&#20110;&#20351;&#29992;&#39640;&#26031;&#27010;&#29575;&#36335;&#24452;&#35757;&#32451;&#30340;&#25193;&#25955;/FM&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#21306;&#20998;&#20250;&#22312;&#25968;&#25454;&#27969;&#24418;&#19978;&#25237;&#24433;&#26799;&#24230;&#65292;&#23558;&#20808;&#39564;&#38544;&#24335;&#27880;&#20837;&#21040;&#20248;&#21270;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#22270;&#20687;&#21644;&#38899;&#39057;&#21453;&#38382;&#39064;&#20197;&#21450;&#26465;&#20214;&#20998;&#23376;&#29983;&#25104;&#22312;&#20869;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#25511;&#21046;&#29983;&#25104;&#38382;&#39064;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#22312;&#25152;&#26377;&#38382;&#39064;&#19978;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14017v1 Announce Type: new  Abstract: Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#8220;&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;&#8221;&#26469;&#35299;&#20915;&#21463;&#26410;&#30693;&#25805;&#32437;&#24433;&#21709;&#30340;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#33021;&#20165;&#30693;&#36947;&#19968;&#37096;&#20998;&#21463;&#24433;&#21709;&#26679;&#26412;&#12290;&#21457;&#29616;&#32416;&#27491;&#28040;&#38500;&#38382;&#39064;&#19982;&#20256;&#32479;&#20197;&#38544;&#31169;&#20026;&#23548;&#21521;&#30340;&#28040;&#38500;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.14015</link><description>&lt;p&gt;
&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Corrective Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#8220;&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;&#8221;&#26469;&#35299;&#20915;&#21463;&#26410;&#30693;&#25805;&#32437;&#24433;&#21709;&#30340;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#33021;&#20165;&#30693;&#36947;&#19968;&#37096;&#20998;&#21463;&#24433;&#21709;&#26679;&#26412;&#12290;&#21457;&#29616;&#32416;&#27491;&#28040;&#38500;&#38382;&#39064;&#19982;&#20256;&#32479;&#20197;&#38544;&#31169;&#20026;&#23548;&#21521;&#30340;&#28040;&#38500;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#38754;&#20020;&#25968;&#25454;&#23436;&#25972;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;&#20174;&#20114;&#32852;&#32593;&#20013;&#33719;&#21462;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#26524;&#27169;&#22411;&#24320;&#21457;&#32773;&#21457;&#29616;&#26576;&#20123;&#25968;&#25454;&#34987;&#31713;&#25913;&#25110;&#38169;&#35823;&#65292;&#20182;&#20204;&#21487;&#20197;&#37319;&#21462;&#20160;&#20040;&#25514;&#26045;&#12290;&#36825;&#20123;&#34987;&#31713;&#25913;&#30340;&#25968;&#25454;&#20250;&#23548;&#33268;&#19981;&#21033;&#24433;&#21709;&#65292;&#22914;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#26679;&#26412;&#30340;&#25915;&#20987;&#12289;&#31995;&#32479;&#24615;&#20559;&#35265;&#65292;&#20197;&#21450;&#22312;&#26576;&#20123;&#36755;&#20837;&#39046;&#22495;&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#12290;&#36890;&#24120;&#65292;&#24182;&#38750;&#25152;&#26377;&#34987;&#31713;&#25913;&#30340;&#35757;&#32451;&#26679;&#26412;&#37117;&#26159;&#24050;&#30693;&#30340;&#65292;&#32780;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#20195;&#34920;&#24615;&#30340;&#21463;&#24433;&#21709;&#25968;&#25454;&#34987;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14015v1 Announce Type: cross  Abstract: Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize "Corrective Machine Unlearning" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#26377;&#38480;&#30340;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#29992;&#25143;&#36873;&#25321;&#20914;&#21160;&#19982;&#38271;&#26399;&#22238;&#25253;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#65292;&#35774;&#35745;&#22312;&#32447;&#36172;&#21338;&#31639;&#27861;&#20197;&#36880;&#28176;&#20943;&#23569;&#21518;&#24724;&#12290;</title><link>https://arxiv.org/abs/2402.14013</link><description>&lt;p&gt;
&#19981;&#23545;&#40784;&#12289;&#23398;&#20064;&#21644;&#25490;&#21517;&#65306;&#21033;&#29992;&#29992;&#25143;&#26377;&#38480;&#30340;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Misalignment, Learning, and Ranking: Harnessing Users Limited Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14013
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#26377;&#38480;&#30340;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#29992;&#25143;&#36873;&#25321;&#20914;&#21160;&#19982;&#38271;&#26399;&#22238;&#25253;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#65292;&#35774;&#35745;&#22312;&#32447;&#36172;&#21338;&#31639;&#27861;&#20197;&#36880;&#28176;&#20943;&#23569;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20581;&#24247;&#21644;&#25945;&#32946;&#31185;&#25216;&#39046;&#22495;&#65292;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65306;&#29992;&#25143;&#36890;&#24120;&#20250;&#20914;&#21160;&#22320;&#36873;&#25321;&#65292;&#36825;&#19982;&#24179;&#21488;&#30340;&#38271;&#26399;&#22238;&#25253;&#30456;&#20914;&#31361;&#12290;&#36825;&#31181;&#19981;&#23545;&#40784;&#20351;&#24471;&#26377;&#25928;&#22320;&#23398;&#20064;&#25490;&#21517;&#39033;&#30446;&#21464;&#24471;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#38459;&#30861;&#20102;&#23545;&#20855;&#26377;&#26356;&#22823;&#38271;&#26399;&#22238;&#25253;&#30340;&#39033;&#30446;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#26377;&#38480;&#30340;&#27880;&#24847;&#21147;&#36328;&#36234;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#24179;&#21488;&#20250;&#38543;&#30528;&#26102;&#38388;&#21521;$T$&#20010;&#29992;&#25143;&#21576;&#29616;&#20855;&#26377;&#26410;&#30693;&#22238;&#25253;&#30340;&#39033;&#30446;&#30340;&#25490;&#21517;&#21015;&#34920;&#12290;&#27599;&#20010;&#29992;&#25143;&#36890;&#36807;&#39318;&#20808;&#32771;&#34385;&#36825;&#20123;&#25490;&#21517;&#39033;&#30446;&#30340;&#19968;&#20010;&#21069;&#32512;&#31383;&#21475;&#65292;&#28982;&#21518;&#36873;&#25321;&#35813;&#31383;&#21475;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#39033;&#30446;&#26469;&#36873;&#25321;&#19968;&#20010;&#39033;&#30446;&#65288;&#24179;&#21488;&#35266;&#23519;&#21040;&#20102;&#35813;&#39033;&#30446;&#30340;&#22238;&#25253;&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#36172;&#21338;&#31639;&#27861;&#30340;&#35774;&#35745;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38754;&#23545;&#20107;&#21518;&#26368;&#20248;&#22522;&#20934;&#26102;&#33021;&#22815;&#23454;&#29616;&#36880;&#28176;&#20943;&#23569;&#30340;&#21518;&#24724;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#23545;&#25239;&#24615;&#31383;&#21475;&#22823;&#23567;&#21644;&#38543;&#26426;iid&#22238;&#25253;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#28120;&#27760;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14013v1 Announce Type: new  Abstract: In digital health and EdTech, recommendation systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform's long-term payoffs. This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs. Our paper tackles this issue by utilizing users' limited attention spans. We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time. Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item). We study the design of online bandit algorithms that obtain vanishing regret against hindsight optimal benchmarks.   We first consider adversarial window sizes and stochastic iid payoffs. We design an active-elimination-based algor
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#19968;&#31867;&#24102;&#26377;&#38271;&#26399;&#32422;&#26463;&#30340;&#22312;&#32447;&#24230;&#37327;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#21487;&#25345;&#32493;&#33021;&#28304;&#21644;&#35745;&#31639;&#31995;&#32479;&#20013;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#24212;&#29992;&#20013;&#30340;&#26368;&#20248;&#31454;&#20105;&#31639;&#27861;&#21644;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.14012</link><description>&lt;p&gt;
&#22312;&#28385;&#36275;&#38271;&#26399;&#32422;&#26463;&#26465;&#20214;&#19979;&#36861;&#36880;&#20984;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Chasing Convex Functions with Long-term Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14012
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#19968;&#31867;&#24102;&#26377;&#38271;&#26399;&#32422;&#26463;&#30340;&#22312;&#32447;&#24230;&#37327;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#21487;&#25345;&#32493;&#33021;&#28304;&#21644;&#35745;&#31639;&#31995;&#32479;&#20013;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#24212;&#29992;&#20013;&#30340;&#26368;&#20248;&#31454;&#20105;&#31639;&#27861;&#21644;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#19968;&#31867;&#24102;&#26377;&#38271;&#26399;&#32422;&#26463;&#30340;&#22312;&#32447;&#24230;&#37327;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#22312;&#32447;&#29609;&#23478;&#22312;&#24230;&#37327;&#31354;&#38388;$(X,d)$&#20013;&#20570;&#20986;&#20915;&#31574;$\mathbf{x}_t$&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20182;&#20204;&#30340;&#21629;&#20013;&#25104;&#26412;$f_t(\mathbf{x}_t)$&#21644;&#30001;&#24230;&#37327;&#30830;&#23450;&#30340;&#20999;&#25442;&#25104;&#26412;&#12290;&#22312;&#26102;&#38388;&#36328;&#24230;$T$&#20869;&#65292;&#29609;&#23478;&#24517;&#39035;&#28385;&#36275;&#38271;&#26399;&#38656;&#27714;&#32422;&#26463;$\sum_{t} c(\mathbf{x}_t) \geq 1$&#65292;&#20854;&#20013;$c(\mathbf{x}_t)$&#34920;&#31034;&#26102;&#38388;$t$&#26102;&#28385;&#36275;&#30340;&#38656;&#27714;&#27604;&#20363;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#21487;&#25345;&#32493;&#33021;&#28304;&#21644;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#20855;&#20307;&#23454;&#20363;&#35774;&#35745;&#20102;&#26368;&#20248;&#30340;&#31454;&#20105;&#31639;&#27861;&#21644;&#23398;&#20064;&#22686;&#24378;&#31639;&#27861;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14012v1 Announce Type: cross  Abstract: We introduce and study a family of online metric problems with long-term constraints. In these problems, an online player makes decisions $\mathbf{x}_t$ in a metric space $(X,d)$ to simultaneously minimize their hitting cost $f_t(\mathbf{x}_t)$ and switching cost as determined by the metric. Over the time horizon $T$, the player must satisfy a long-term demand constraint $\sum_{t} c(\mathbf{x}_t) \geq 1$, where $c(\mathbf{x}_t)$ denotes the fraction of demand satisfied at time $t$. Such problems can find a wide array of applications to online resource allocation in sustainable energy and computing systems. We devise optimal competitive and learning-augmented algorithms for specific instantiations of these problems, and further show that our proposed algorithms perform well in numerical experiments.
&lt;/p&gt;</description></item><item><title>GINNs&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65292;&#37319;&#29992;&#26174;&#24335;&#22810;&#26679;&#24615;&#25439;&#22833;&#20197;&#21450;&#21487;&#24494;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#22797;&#26434;&#24615;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14009</link><description>&lt;p&gt;
&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Geometry-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14009
&lt;/p&gt;
&lt;p&gt;
GINNs&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65292;&#37319;&#29992;&#26174;&#24335;&#22810;&#26679;&#24615;&#25439;&#22833;&#20197;&#21450;&#21487;&#24494;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#22797;&#26434;&#24615;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINNs&#65289;&#30340;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#65288;i&#65289;&#22312;&#20960;&#20309;&#32422;&#26463;&#19979;&#23398;&#20064;&#65292;&#65288;ii&#65289;&#31070;&#32463;&#22330;&#20316;&#20026;&#21512;&#36866;&#30340;&#34920;&#31034;&#65292;&#65288;iii&#65289;&#29983;&#25104;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#27424;&#23450;&#31995;&#32479;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GINN&#30340;&#26500;&#24314;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#34987;&#32431;&#32422;&#26463;&#39537;&#21160;&#22320;&#35270;&#20026;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#22686;&#21152;&#20102;&#26174;&#24335;&#30340;&#22810;&#26679;&#24615;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#32422;&#26463;&#65292;&#29305;&#21035;&#26159;&#32452;&#20214;&#30340;&#36830;&#36890;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#33707;&#23572;&#26031;&#29702;&#35770;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#24494;&#25439;&#22833;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#26029;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#22330;&#26223;&#20013;&#65292;GINN&#23398;&#20064;&#33539;&#24335;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14009v1 Announce Type: new  Abstract: We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.
&lt;/p&gt;</description></item><item><title>&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23398;&#20064;&#36755;&#20986;&#23618;&#27979;&#35797;&#35823;&#24046;&#30340;&#20005;&#26684;&#28176;&#36817;&#29305;&#24615;&#65292;&#24182;&#23545;&#20351;&#29992;&#39640;&#26031;&#24425;&#34425;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#30340;&#38382;&#39064;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;</title><link>https://arxiv.org/abs/2402.13999</link><description>&lt;p&gt;
&#28145;&#24230;&#32467;&#26500;&#21270;&#65288;&#38543;&#26426;&#65289;&#29305;&#24449;&#23398;&#20064;&#30340;&#28176;&#36817;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Asymptotics of Learning with Deep Structured (Random) Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13999
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23398;&#20064;&#36755;&#20986;&#23618;&#27979;&#35797;&#35823;&#24046;&#30340;&#20005;&#26684;&#28176;&#36817;&#29305;&#24615;&#65292;&#24182;&#23545;&#20351;&#29992;&#39640;&#26031;&#24425;&#34425;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#30340;&#38382;&#39064;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#19968;&#22823;&#31867;&#29305;&#24449;&#26144;&#23556;&#65292;&#25105;&#20204;&#22312;&#36755;&#20837;&#32500;&#24230;&#12289;&#38544;&#34255;&#23618;&#23485;&#24230;&#21644;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#25104;&#27604;&#20363;&#22686;&#38271;&#30340;&#39640;&#32500;&#26497;&#38480;&#19979;&#65292;&#25552;&#20379;&#20102;&#19982;&#23398;&#20064;&#36755;&#20986;&#23618;&#30456;&#20851;&#30340;&#27979;&#35797;&#35823;&#24046;&#30340;&#20005;&#26684;&#28176;&#36817;&#29305;&#24615;&#21051;&#30011;&#12290;&#36825;&#19968;&#29305;&#24449;&#20197;&#29305;&#24449;&#30340;&#24635;&#20307;&#21327;&#26041;&#24046;&#20026;&#22522;&#30784;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#37096;&#20998;&#21463;&#21040;&#20351;&#29992;&#39640;&#26031;&#24425;&#34425;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#30340;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#21363;&#20855;&#26377;&#38543;&#26426;&#20294;&#32467;&#26500;&#21270;&#26435;&#37325;&#30340;&#28145;&#23618;&#38750;&#32447;&#24615;&#20840;&#36830;&#25509;&#32593;&#32476;&#65292;&#20854;&#25353;&#34892;&#30340;&#21327;&#26041;&#24046;&#36827;&#19968;&#27493;&#20801;&#35768;&#20381;&#36182;&#20110;&#20043;&#21069;&#23618;&#30340;&#26435;&#37325;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#20197;&#26435;&#37325;&#30697;&#38453;&#20026;&#22522;&#30784;&#30340;&#29305;&#24449;&#21327;&#26041;&#24046;&#30340;&#38381;&#21512;&#24418;&#24335;&#20844;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#33021;&#22815;&#25429;&#25417;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20855;&#26377;&#26377;&#38480;&#23485;&#24230;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13999v1 Announce Type: cross  Abstract: For a large class of feature maps we provide a tight asymptotic characterisation of the test error associated with learning the readout layer, in the high-dimensional limit where the input dimension, hidden layer widths, and number of training samples are proportionally large. This characterization is formulated in terms of the population covariance of the features. Our work is partially motivated by the problem of learning with Gaussian rainbow neural networks, namely deep non-linear fully-connected networks with random but structured weights, whose row-wise covariances are further allowed to depend on the weights of previous layers. For such networks we also derive a closed-form formula for the feature covariance in terms of the weight matrices. We further find that in some cases our results can capture feature maps learned by deep, finite-width neural networks trained under gradient descent.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;FedADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#20026;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#35774;&#35745;&#19968;&#20010;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#26412;&#22320;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#38656;&#35201;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#20943;&#36731;&#20102;&#28382;&#21518;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.13989</link><description>&lt;p&gt;
FedADMM-InSa: &#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;&#32852;&#37030;&#23398;&#20064;ADMM
&lt;/p&gt;
&lt;p&gt;
FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;FedADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#20026;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#35774;&#35745;&#19968;&#20010;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#65292;&#28040;&#38500;&#20102;&#35843;&#25972;&#26412;&#22320;&#35757;&#32451;&#20934;&#30830;&#24230;&#30340;&#38656;&#35201;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#24182;&#20943;&#36731;&#20102;&#28382;&#21518;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#23398;&#20064;&#21516;&#26102;&#20445;&#25345;&#38544;&#31169;&#12290;&#26377;&#25928;&#30340;FL&#31639;&#27861;&#30340;&#21457;&#23637;&#38754;&#20020;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#24322;&#26500;&#25968;&#25454;&#21644;&#31995;&#32479;&#12289;&#36890;&#20449;&#33021;&#21147;&#26377;&#38480;&#20197;&#21450;&#21463;&#38480;&#30340;&#26412;&#22320;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;FedADMM&#26041;&#27861;&#23545;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#24322;&#26500;&#24615;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#38887;&#24615;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36229;&#21442;&#25968;&#27809;&#26377;&#32463;&#36807;&#31934;&#24515;&#35843;&#25972;&#65292;&#23427;&#20204;&#20173;&#28982;&#20250;&#36973;&#21463;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#31934;&#30830;&#21644;&#33258;&#36866;&#24212;&#30340;FedADMM&#31639;&#27861;&#65292;&#21517;&#20026;FedADMM-InSa&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20026;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#35774;&#35745;&#20102;&#19968;&#20010;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#65292;&#20197;&#28040;&#38500;&#24517;&#39035;&#26681;&#25454;&#32463;&#39564;&#35774;&#32622;&#26412;&#22320;&#35757;&#32451;&#20934;&#30830;&#24615;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#19981;&#31934;&#30830;&#24615;&#26631;&#20934;&#21487;&#20197;&#30001;&#27599;&#20010;&#23458;&#25143;&#31471;&#29420;&#31435;&#22320;&#26681;&#25454;&#20854;&#29420;&#29305;&#26465;&#20214;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#38477;&#20302;&#26412;&#22320;&#35745;&#31639;&#25104;&#26412;&#24182;&#20943;&#36731;&#19981;&#33391;&#30340;&#28382;&#21518;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13989v1 Announce Type: new  Abstract: Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NoisyGNNs&#30340;&#26032;&#39062;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22122;&#22768;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#24314;&#31435;&#20102;&#22122;&#22768;&#27880;&#20837;&#19982;&#22686;&#24378;GNN&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#29702;&#35770;&#36830;&#25509;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#25110;&#21487;&#27604;&#30340;&#38450;&#24481;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13987</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#19988;&#30456;&#24403;&#26377;&#25928;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Yet Fairly Effective Defense for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NoisyGNNs&#30340;&#26032;&#39062;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22122;&#22768;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#26550;&#26500;&#65292;&#24314;&#31435;&#20102;&#22122;&#22768;&#27880;&#20837;&#19982;&#22686;&#24378;GNN&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#29702;&#35770;&#36830;&#25509;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#36234;&#25110;&#21487;&#27604;&#30340;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;GNN&#23545;&#23567;&#24178;&#25200;&#30340;&#33030;&#24369;&#24615;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#38024;&#23545;&#36825;&#31181;&#24178;&#25200; suffer from high time complexity and can negatively impact the model's performance on clean graphs&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102; NoisyGNNs&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#22122;&#22768;&#34701;&#20837;&#22522;&#30784;&#27169;&#22411;&#26550;&#26500;&#30340;&#26032;&#39062;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22122;&#22768;&#27880;&#20837;&#19982;&#22686;&#24378;GNN&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#29702;&#35770;&#36830;&#25509;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;GNNs&#65306;GCN&#21644;GIN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;NoisyGNN&#23454;&#29616;&#20102;&#20248;&#36234;&#25110;&#21487;&#27604;&#30340;&#38450;&#24481;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13987v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have emerged as the dominant approach for machine learning on graph-structured data. However, concerns have arisen regarding the vulnerability of GNNs to small adversarial perturbations. Existing defense methods against such perturbations suffer from high time complexity and can negatively impact the model's performance on clean graphs. To address these challenges, this paper introduces NoisyGNNs, a novel defense method that incorporates noise into the underlying model's architecture. We establish a theoretical connection between noise injection and the enhancement of GNN robustness, highlighting the effectiveness of our approach. We further conduct extensive empirical evaluations on the node classification task to validate our theoretical findings, focusing on two popular GNNs: the GCN and GIN. The results demonstrate that NoisyGNN achieves superior or comparable defense performance to existing methods while
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31283;&#23450;&#24615;&#24863;&#30693;Boltzmann&#20272;&#35745;&#22120;&#65288;StABlE&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#20256;&#32479;&#30417;&#30563;&#35757;&#32451;&#21644;&#21442;&#32771;&#31995;&#32479;&#21487;&#35266;&#23519;&#37327;&#65292;&#29992;&#20110;&#29983;&#25104;&#31283;&#23450;&#19988;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.13984</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#24494;Boltzmann&#20272;&#35745;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#30340;&#31283;&#23450;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13984
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31283;&#23450;&#24615;&#24863;&#30693;Boltzmann&#20272;&#35745;&#22120;&#65288;StABlE&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#20256;&#32479;&#30417;&#30563;&#35757;&#32451;&#21644;&#21442;&#32771;&#31995;&#32479;&#21487;&#35266;&#23519;&#37327;&#65292;&#29992;&#20110;&#29983;&#25104;&#31283;&#23450;&#19988;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#65288;NNIPs&#65289;&#26159;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#30340;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#19981;&#31283;&#23450;&#30340;&#27169;&#25311;&#65292;&#37319;&#26679;&#38750;&#29289;&#29702;&#29366;&#24577;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#22312;&#23545;&#27169;&#25311;&#38271;&#26102;&#38388;&#23610;&#24230;&#29616;&#35937;&#24314;&#27169;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#24615;&#24863;&#30693;Boltzmann&#20272;&#35745;&#22120;&#65288;StABlE&#65289;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27169;&#24335;&#35757;&#32451;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#20256;&#32479;&#30417;&#30563;&#35757;&#32451;&#21644;&#21442;&#32771;&#31995;&#32479;&#21487;&#35266;&#23519;&#37327;&#65292;&#20197;&#20135;&#29983;&#31283;&#23450;&#19988;&#20934;&#30830;&#30340;NNIPs&#12290;StABlE&#35757;&#32451;&#36890;&#36807;&#36845;&#20195;&#36816;&#34892;MD&#27169;&#25311;&#20197;&#23547;&#25214;&#19981;&#31283;&#23450;&#21306;&#22495;&#65292;&#24182;&#36890;&#36807;&#19982;&#21442;&#32771;&#21487;&#35266;&#23519;&#37327;&#30340;&#30417;&#30563;&#26469;&#32416;&#27491;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#12290;&#35813;&#35757;&#32451;&#36807;&#31243;&#30001;Boltzmann&#20272;&#35745;&#22120;&#25903;&#25345;&#65292;&#35813;&#20272;&#35745;&#22120;&#20801;&#35768;&#23545;&#31995;&#32479;&#21487;&#35266;&#23519;&#37327;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#26799;&#24230;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#65292;&#24182;&#33021;&#26816;&#27979;&#20840;&#23616;&#21644;&#23616;&#37096;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13984v1 Announce Type: new  Abstract: Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations. However, they can produce unstable simulations which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which combines conventional supervised training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs. StABlE Training iteratively runs MD simulations to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#27668;&#20505;&#31185;&#23398;&#24212;&#29992;&#20013;&#26550;&#26500;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#22810;&#26679;&#21270;&#30340;&#27668;&#20505;&#24773;&#26223;&#19979;&#21487;&#39044;&#27979;&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;MLP&#21644;&#28145;&#24230;&#38598;&#25104;&#21487;&#20197;&#23398;&#20064;AMOC&#30340;&#29289;&#29702;&#36807;&#31243;&#32780;&#38750;&#27169;&#25311;&#20854;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.13979</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#27668;&#20505;&#24212;&#29992;&#20013;&#30340;&#26550;&#26500;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Architecture Choice in Deep Learning for Climate Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#27668;&#20505;&#31185;&#23398;&#24212;&#29992;&#20013;&#26550;&#26500;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#22810;&#26679;&#21270;&#30340;&#27668;&#20505;&#24773;&#26223;&#19979;&#21487;&#39044;&#27979;&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;MLP&#21644;&#28145;&#24230;&#38598;&#25104;&#21487;&#20197;&#23398;&#20064;AMOC&#30340;&#29289;&#29702;&#36807;&#31243;&#32780;&#38750;&#27169;&#25311;&#20854;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#27668;&#20505;&#31185;&#23398;&#24212;&#29992;&#20013;&#26222;&#36941;&#20351;&#29992;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#26410;&#33021;&#35299;&#20915;&#30001;&#20154;&#20026;&#25913;&#21464;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#24341;&#36215;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#19988;&#19981;&#20250;&#23450;&#26399;&#37327;&#21270;&#25152;&#25552;&#20986;&#30340;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#23558;&#28201;&#26262;&#27700;&#36755;&#36865;&#21040;&#27431;&#27954;&#21644;&#32654;&#22269;&#19996;&#28023;&#23736;&#65292;&#23545;&#36825;&#20123;&#22320;&#21306;&#30340;&#27668;&#20505;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#26377;&#28508;&#22312;&#30340;&#31361;&#28982;&#23849;&#28291;&#39118;&#38505;&#12290;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#36890;&#36807;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#30340;&#20219;&#24847;&#26497;&#31471;&#27668;&#20505;&#22330;&#26223;&#65292;&#28982;&#21518;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#27668;&#20505;&#24773;&#26223;&#19979;&#65292;AMOC&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#39044;&#27979;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;MLP&#21644;&#28145;&#24230;&#38598;&#25104;&#21487;&#20197;&#23398;&#20064;AMOC&#30340;&#29289;&#29702;&#36807;&#31243;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#33258;&#30456;&#20851;&#27169;&#25311;&#20854;&#36827;&#23637;&#12290;&#36890;&#36807;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#21457;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13979v1 Announce Type: cross  Abstract: Machine Learning has become a pervasive tool in climate science applications. However, current models fail to address nonstationarity induced by anthropogenic alterations in greenhouse emissions and do not routinely quantify the uncertainty of proposed projections. In this paper, we model the Atlantic Meridional Overturning Circulation (AMOC) which is of major importance to climate in Europe and the US East Coast by transporting warm water to these regions, and has the potential for abrupt collapse. We can generate arbitrarily extreme climate scenarios through arbitrary time scales which we then predict using neural networks. Our analysis shows that the AMOC is predictable using neural networks under a diverse set of climate scenarios. Further experiments reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead of imitating its progression through autocorrelation. With quantified uncertainty, an intriguing pattern 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LTGNN&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13973</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#25512;&#33616;&#31995;&#32479;&#30340;&#32447;&#24615;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Linear-Time Graph Neural Networks for Scalable Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LTGNN&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#29190;&#28856;&#30340;&#26102;&#20195;&#65292;&#25512;&#33616;&#31995;&#32479;&#26159;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#25512;&#33616;&#31995;&#32479;&#30340;&#20851;&#38190;&#22312;&#20110;&#26681;&#25454;&#20808;&#21069;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20114;&#21160;&#26469;&#39044;&#27979;&#29992;&#25143;&#30340;&#26410;&#26469;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#39640;&#38454;&#36830;&#25509;&#24615;&#25429;&#25417;&#33021;&#21147;&#65292;&#20154;&#20204;&#23545;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#39044;&#27979;&#24615;&#33021;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30340;&#30697;&#38453;&#20998;&#35299;&#65288;MF&#65289;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26041;&#27861;&#30001;&#20110;&#20854;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#65292;&#20173;&#22312;&#23454;&#38469;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#23613;&#31649;&#23384;&#22312;GNN&#21152;&#36895;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;GNN-based&#25512;&#33616;&#31995;&#32479;&#33021;&#21542;&#20687;&#32463;&#20856;&#30340;MF&#21644;DNN&#26041;&#27861;&#19968;&#26679;&#39640;&#25928;&#25193;&#23637;&#20173;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LTGNN&#65289;&#26469;&#25193;&#23637;GN
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13973v1 Announce Type: cross  Abstract: In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users. The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions. Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems. Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages. Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods. In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GN
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#20855;&#26377;&#22810;&#26679;&#32972;&#26223;&#22122;&#38899;&#21644;&#22833;&#30495;&#30340;&#30495;&#23454;&#29615;&#22659;&#22330;&#26223;&#27169;&#25311;&#20013;&#30340;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#23454;&#29616;100%&#20934;&#30830;&#24615;&#30340;5&#31186;&#38899;&#39057;&#36755;&#20837;&#65292;&#31995;&#32479;&#21305;&#37197;&#36895;&#24230;&#21487;&#39044;&#27979;&#65292;&#24378;&#35843;&#20102;&#23454;&#38469;&#23454;&#26045;&#20013;&#30340;&#20851;&#38190;&#31354;&#38388;-&#36895;&#24230;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.13957</link><description>&lt;p&gt;
&#25552;&#39640;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#32972;&#26223;&#22122;&#38899;&#21644;&#22833;&#30495;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#20855;&#26377;&#22810;&#26679;&#32972;&#26223;&#22122;&#38899;&#21644;&#22833;&#30495;&#30340;&#30495;&#23454;&#29615;&#22659;&#22330;&#26223;&#27169;&#25311;&#20013;&#30340;&#24037;&#20316;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#23454;&#29616;100%&#20934;&#30830;&#24615;&#30340;5&#31186;&#38899;&#39057;&#36755;&#20837;&#65292;&#31995;&#32479;&#21305;&#37197;&#36895;&#24230;&#21487;&#39044;&#27979;&#65292;&#24378;&#35843;&#20102;&#23454;&#38469;&#23454;&#26045;&#20013;&#30340;&#20851;&#38190;&#31354;&#38388;-&#36895;&#24230;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#65292;&#22914; Shazam &#31561;&#20808;&#39537;&#25152;&#31034;&#65292;&#24050;&#32463;&#25913;&#21464;&#20102;&#25968;&#23383;&#38899;&#39057;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#20934;&#30830;&#24615;&#23384;&#22312;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#38598;&#25104;&#30340;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#31639;&#27861;&#20197;&#22686;&#24378;&#20934;&#30830;&#24615;&#12290;&#24314;&#31435;&#22312; Dejavu &#39033;&#30446;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#20855;&#26377;&#22810;&#26679;&#32972;&#26223;&#22122;&#38899;&#21644;&#22833;&#30495;&#30340;&#30495;&#23454;&#29615;&#22659;&#22330;&#26223;&#27169;&#25311;&#20013;&#30340;&#24037;&#20316;&#12290;&#20449;&#21495;&#22788;&#29702;&#65292;&#26159; Dejavu &#27169;&#22411;&#30340;&#26680;&#24515;&#65292;&#21253;&#25324;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#12289;&#39057;&#35889;&#22270;&#21644;&#23792;&#20540;&#25552;&#21462;&#12290;"&#26143;&#24231;"&#27010;&#24565;&#21644;&#25351;&#32441;&#21704;&#24076;&#20351;&#24471;&#27468;&#26354;&#29420;&#29305;&#26631;&#35782;&#25104;&#20026;&#21487;&#33021;&#12290;&#24615;&#33021;&#35780;&#20272;&#35777;&#23454;&#65292;&#22312;5&#31186;&#38899;&#39057;&#36755;&#20837;&#26102;&#21487;&#36798;&#21040;100%&#30340;&#20934;&#30830;&#24615;&#65292;&#31995;&#32479;&#23637;&#31034;&#20986;&#21487;&#39044;&#27979;&#30340;&#21305;&#37197;&#36895;&#24230;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#23384;&#20648;&#20998;&#26512;&#20984;&#26174;&#20102;&#23454;&#38469;&#23454;&#26045;&#20013;&#30340;&#20851;&#38190;&#31354;&#38388;-&#36895;&#24230;&#26435;&#34913;&#12290;&#26412;&#30740;&#31350;&#25512;&#21160;&#20102;&#38899;&#39057;&#25351;&#32441;&#35782;&#21035;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13957v1 Announce Type: cross  Abstract: Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition. However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability. This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built on the Dejavu Project's foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions. Signal processing, central to Dejavu's model, includes the Fast Fourier Transform, spectrograms, and peak extraction. The "constellation" concept and fingerprint hashing enable unique song identification. Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency. Storage analysis highlights the critical space-speed trade-off for practical implementation. This research advances audio fingerprinting's adaptab
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;AttackGNN&#65292;&#38024;&#23545;&#30828;&#20214;&#23433;&#20840;&#20013;&#20351;&#29992;&#30340;&#22522;&#20110;GNN&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#39318;&#27425;&#32418;&#38431;&#25915;&#20987;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#30005;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.13946</link><description>&lt;p&gt;
AttackGNN: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#30828;&#20214;&#23433;&#20840;&#20013;&#23545;GNN&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13946
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;AttackGNN&#65292;&#38024;&#23545;&#30828;&#20214;&#23433;&#20840;&#20013;&#20351;&#29992;&#30340;&#22522;&#20110;GNN&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#39318;&#27425;&#32418;&#38431;&#25915;&#20987;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35299;&#20915;&#19968;&#20123;&#20851;&#38190;&#30340;&#30828;&#20214;&#23433;&#20840;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20102;&#26497;&#22823;&#30340;&#28508;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26032;&#39062;&#25216;&#26415;&#65292;&#29992;&#20110;&#26816;&#27979;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#30423;&#29256;&#12289;&#26816;&#27979;&#30828;&#20214;&#29305;&#27931;&#20234;&#26408;&#39532;&#65288;HTs&#65289;&#21644;&#21453;&#21521;&#24037;&#31243;&#30005;&#36335;&#31561;&#38382;&#39064;&#12290;&#36825;&#20123;&#25216;&#26415;&#34920;&#29616;&#20986;&#33394;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AttackGNN&#65292;&#36825;&#26159;&#38024;&#23545;&#30828;&#20214;&#23433;&#20840;&#20013;&#22522;&#20110;GNN&#30340;&#25216;&#26415;&#30340;&#31532;&#19968;&#20010;&#32418;&#38431;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#65292;&#29992;&#20110;&#29983;&#25104;&#38024;&#23545;GNN&#25216;&#26415;&#30340;&#23545;&#25239;&#31034;&#20363;&#65292;&#21363;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13946v1 Announce Type: new  Abstract: Machine learning has shown great promise in addressing several critical hardware security problems. In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few. These techniques have demonstrated outstanding accuracy and have received much attention in the community. However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits.   In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security. To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques. We overcome three challenges related to effectiveness, scal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;PNNs&#65289;&#26469;&#24314;&#27169;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#24320;&#21457;&#27010;&#29575;&#36317;&#31163;&#24230;&#37327;&#26469;&#20248;&#21270;PNN&#26550;&#26500;&#65292;&#35777;&#23454;&#20102;PNNs&#22312;&#27169;&#25311;Aleatoric&#19981;&#30830;&#23450;&#24615;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13945</link><description>&lt;p&gt;
&#29992;&#20110;&#24314;&#27169;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;Aleatoric&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;PNNs&#65289;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty in Scientific Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;PNNs&#65289;&#26469;&#24314;&#27169;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#24320;&#21457;&#27010;&#29575;&#36317;&#31163;&#24230;&#37327;&#26469;&#20248;&#21270;PNN&#26550;&#26500;&#65292;&#35777;&#23454;&#20102;PNNs&#22312;&#27169;&#25311;Aleatoric&#19981;&#30830;&#23450;&#24615;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;PNNs&#65289;&#26469;&#24314;&#27169;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#19981;&#30830;&#23450;&#24615;&#26159;&#25351;&#31995;&#32479;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#20013;&#22266;&#26377;&#30340;&#21464;&#24322;&#24615;&#65292;&#36890;&#24120;&#34920;&#29616;&#20026;&#19981;&#22343;&#31561;&#30340;&#26041;&#24046;&#25110;&#24322;&#26041;&#24046;&#24615;&#12290;&#19981;&#21516;&#20110;&#20135;&#29983;&#30830;&#23450;&#24615;&#36755;&#20986;&#30340;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#65292;PNNs&#20026;&#30446;&#26631;&#21464;&#37327;&#29983;&#25104;&#27010;&#29575;&#20998;&#24067;&#65292;&#20801;&#35768;&#22312;&#22238;&#24402;&#22330;&#26223;&#20013;&#30830;&#23450;&#39044;&#27979;&#22343;&#20540;&#21644;&#21306;&#38388;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#24320;&#21457;&#27010;&#29575;&#36317;&#31163;&#24230;&#37327;&#26469;&#20248;&#21270;PNN&#26550;&#26500;&#65292;&#20197;&#21450;&#22312;&#21463;&#25511;&#25968;&#25454;&#38598;&#21644;&#28041;&#21450;&#32420;&#32500;&#22686;&#24378;&#22797;&#21512;&#26448;&#26009;&#30340;&#23454;&#38469;&#26448;&#26009;&#31185;&#23398;&#26696;&#20363;&#20013;&#37096;&#32626;PNNs&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#65292;PNNs&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#35777;&#26126;&#22312;&#36825;&#19968;&#30446;&#30340;&#19978;&#65292;&#23427;&#27604;&#36890;&#24120;&#37319;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26356;&#20026;&#21512;&#36866;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#31185;&#23398;&#29615;&#22659;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13945v1 Announce Type: cross  Abstract: This paper investigates the use of probabilistic neural networks (PNNs) to model aleatoric uncertainty, which refers to the inherent variability in the input-output relationships of a system, often characterized by unequal variance or heteroscedasticity. Unlike traditional neural networks that produce deterministic outputs, PNNs generate probability distributions for the target variable, allowing the determination of both predicted means and intervals in regression scenarios. Contributions of this paper include the development of a probabilistic distance metric to optimize PNN architecture, and the deployment of PNNs in controlled data sets as well as a practical material science case involving fiber-reinforced composites. The findings confirm that PNNs effectively model aleatoric uncertainty, proving to be more appropriate than the commonly employed Gaussian process regression for this purpose. Specifically, in a real-world scientific
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#25299;&#25169;&#30340;&#30028;&#38480;&#32039;&#32553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#35777;&#20070;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65292;&#25903;&#25345;&#28155;&#21152;&#21644;&#21024;&#38500;&#36793;&#12289;&#20840;&#23616;&#21644;&#23616;&#37096;&#39044;&#31639;&#30340;&#35774;&#32622;&#65292;&#20197;&#21450;&#25299;&#25169;&#25200;&#21160;&#21644;&#29305;&#24449;&#20462;&#25913;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20248;&#21270;&#32422;&#26463;&#21160;&#24577;&#35843;&#25972;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13937</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25299;&#25169;&#30340;&#30028;&#38480;&#32039;&#32553;&#39564;&#35777;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Verifying message-passing neural networks via topology-based bounds tightening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13937
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25299;&#25169;&#30340;&#30028;&#38480;&#32039;&#32553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#35777;&#20070;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39564;&#35777;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65292;&#25903;&#25345;&#28155;&#21152;&#21644;&#21024;&#38500;&#36793;&#12289;&#20840;&#23616;&#21644;&#23616;&#37096;&#39044;&#31639;&#30340;&#35774;&#32622;&#65292;&#20197;&#21450;&#25299;&#25169;&#25200;&#21160;&#21644;&#29305;&#24449;&#20462;&#25913;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20248;&#21270;&#32422;&#26463;&#21160;&#24577;&#35843;&#25972;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32463;&#24120;&#23481;&#26131;&#36973;&#21463;&#25915;&#20987;&#65292;&#25105;&#20204;&#38656;&#35201;&#30693;&#36947;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#23427;&#20204;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#35745;&#31639;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#20989;&#25968;&#20026;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#25552;&#20379;&#24378;&#22823;&#30340;&#35777;&#20070;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#20043;&#19978;&#65292;&#32534;&#30721;&#20102;&#22810;&#31181;&#23376;&#38382;&#39064;&#65292;&#20363;&#22914;&#20801;&#35768;&#28155;&#21152;&#21644;&#21024;&#38500;&#36793;&#65292;&#20840;&#23616;&#21644;&#23616;&#37096;&#39044;&#31639;&#65292;&#20197;&#21450;&#25299;&#25169;&#25200;&#21160;&#21644;&#29305;&#24449;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#22522;&#20110;&#25299;&#25169;&#30340;&#30028;&#38480;&#32039;&#32553;&#65292;&#21033;&#29992;&#22270;&#32467;&#26500;&#26469;&#25910;&#32039;&#30028;&#38480;&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#20351;&#29992;&#31215;&#26497;&#30340;&#30028;&#38480;&#32039;&#32553;&#26469;&#21160;&#24577;&#25913;&#21464;&#20248;&#21270;&#32422;&#26463;&#65292;&#21363;&#36890;&#36807;&#25910;&#32039;&#21464;&#37327;&#30028;&#38480;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20123;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#24320;&#28304;&#30340;&#20998;&#25903;&#23450;&#30028;&#27714;&#35299;&#22120;SCIP&#12290;&#25105;&#20204;&#22312;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13937v1 Announce Type: cross  Abstract: Since graph neural networks (GNNs) are often vulnerable to attack, we need to know when we can trust them. We develop a computationally effective approach towards providing robust certificates for message-passing neural networks (MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our work builds on mixed-integer optimization, it encodes a wide variety of subproblems, for example it admits (i) both adding and removing edges, (ii) both global and local budgets, and (iii) both topological perturbations and feature modifications. Our key technology, topology-based bounds tightening, uses graph structure to tighten bounds. We also experiment with aggressive bounds tightening to dynamically change the optimization constraints by tightening variable bounds. To demonstrate the effectiveness of these strategies, we implement an extension to the open-source branch-and-cut solver SCIP. We test on both node and graph classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#65288;&#20363;&#22914;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#65289;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#65292;&#20294;&#19981;&#21516;&#20110;&#26631;&#20934;Transformer&#12290;</title><link>https://arxiv.org/abs/2402.13934</link><description>&lt;p&gt;
&#30830;&#23454;&#39640;&#25928;&#30340;Transformer&#33021;&#22815;&#33410;&#32422;&#35745;&#31639;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Efficient Transformers Really Save Computation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#65288;&#20363;&#22914;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#65289;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#65292;&#20294;&#19981;&#21516;&#20110;&#26631;&#20934;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#24182;&#25317;&#26377;&#22823;&#37327;&#21442;&#25968;&#65292;&#25214;&#21040;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26631;&#20934;Transformer&#21464;&#24471;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#39640;&#25928;&#30340;Transformer&#21644;Transformer&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#23427;&#20204;&#36866;&#21512;&#26367;&#20195;&#26631;&#20934;Transformer&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#36825;&#20351;&#24471;&#24456;&#38590;&#30830;&#23450;&#20309;&#26102;&#20351;&#29992;&#29305;&#23450;&#27169;&#22411;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23427;&#20204;&#22312;Chain-of-Thought (CoT)&#25552;&#31034;&#20013;&#23637;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36981;&#24490;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#23427;&#20204;&#24314;&#27169;&#20026;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#36275;&#22815;&#34920;&#36798;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#19982;&#26631;&#20934;Transformer&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13934v1 Announce Type: cross  Abstract: As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28155;&#21152;&#26412;&#22320;&#25351;&#21335;&#31574;&#30053;&#21040;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#22122;&#22768;&#31574;&#30053;&#20999;&#25442;&#31243;&#24207;&#21644;&#36817;&#20284;&#31574;&#30053;&#35780;&#20272;&#26469;&#24341;&#23548;&#26412;&#22320;&#25351;&#21335;&#65292;&#25552;&#39640;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#23398;&#20064;&#21021;&#26399;&#12290;</title><link>https://arxiv.org/abs/2402.13930</link><description>&lt;p&gt;
&#20351;&#29992;&#26412;&#22320;&#25351;&#21335;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Enhancing Reinforcement Learning Agents with Local Guides
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28155;&#21152;&#26412;&#22320;&#25351;&#21335;&#31574;&#30053;&#21040;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#22122;&#22768;&#31574;&#30053;&#20999;&#25442;&#31243;&#24207;&#21644;&#36817;&#20284;&#31574;&#30053;&#35780;&#20272;&#26469;&#24341;&#23548;&#26412;&#22320;&#25351;&#21335;&#65292;&#25552;&#39640;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#23398;&#20064;&#21021;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#23558;&#26412;&#22320;&#25351;&#21335;&#31574;&#30053;&#25972;&#21512;&#36827;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35843;&#25972;&#29616;&#26377;&#31639;&#27861;&#20197;&#36866;&#24212;&#36825;&#31181;&#35774;&#32622;&#65292;&#28982;&#21518;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#31574;&#30053;&#20999;&#25442;&#31243;&#24207;&#30340;&#26032;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#36866;&#24403;&#30340;&#36817;&#20284;&#31574;&#30053;&#35780;&#20272;&#65288;APE&#65289;&#26041;&#26696;&#20043;&#19978;&#65292;&#36890;&#36807;&#31934;&#24515;&#24341;&#23548;&#26412;&#22320;&#25351;&#21335;&#26397;&#30528;&#26356;&#22909;&#30340;&#34892;&#20026;&#26041;&#21521;&#36827;&#34892;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#32463;&#20856;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#20854;&#20013;&#20195;&#29702;&#19981;&#33021;&#36827;&#20837;&#26576;&#20123;&#21306;&#22495;&#65292;&#20197;&#20813;&#35302;&#21457;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#22312;&#25152;&#26377;&#25552;&#20986;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#37117;&#34987;&#35777;&#26126;&#22312;&#25913;&#36827;&#20219;&#20309;&#22522;&#20110;APE&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#19978;&#26159;&#39640;&#25928;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#20854;&#23398;&#20064;&#30340;&#26368;&#21021;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13930v1 Announce Type: new  Abstract: This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent. For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure. This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions. We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences. In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#30340;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#30456;&#24212;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.13929</link><description>&lt;p&gt;
SDXL-Lightning: &#28176;&#36827;&#24335;&#23545;&#25239;&#24615;&#25193;&#25955;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
SDXL-Lightning: Progressive Adversarial Diffusion Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13929
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#30340;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#30456;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;SDXL&#30340;&#19968;&#27493;/&#20960;&#27493;1024&#20687;&#32032;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#36136;&#37327;&#21644;&#27169;&#24335;&#35206;&#30422;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#29702;&#35770;&#20998;&#26512;&#12289;&#21028;&#21035;&#22120;&#35774;&#35745;&#12289;&#27169;&#22411;&#20844;&#24335;&#21644;&#35757;&#32451;&#25216;&#24039;&#12290;&#25105;&#20204;&#20197;LoRA&#21644;&#23436;&#25972;UNet&#26435;&#37325;&#30340;&#24418;&#24335;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#33976;&#39311;SDXL-Lightning&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13929v1 Announce Type: cross  Abstract: We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#20113;&#36827;&#34892;&#20998;&#21106;&#65292;&#26088;&#22312;&#25552;&#39640;&#21355;&#26143;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24212;&#29992;&#20110;&#29615;&#22659;&#30417;&#27979;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.13918</link><description>&lt;p&gt;
BenchCloudVision: &#20113;&#26816;&#27979;&#21644;&#20998;&#21106;&#20013;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#22522;&#20934;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#20113;&#36827;&#34892;&#20998;&#21106;&#65292;&#26088;&#22312;&#25552;&#39640;&#21355;&#26143;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;&#24212;&#29992;&#20110;&#29615;&#22659;&#30417;&#27979;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37197;&#22791;&#20809;&#23398;&#20256;&#24863;&#22120;&#30340;&#21355;&#26143;&#25429;&#33719;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20026;&#21508;&#31181;&#29615;&#22659;&#29616;&#35937;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#36965;&#24863;&#20013;&#19968;&#20123;&#25361;&#25112;&#30340;&#30740;&#31350;&#28608;&#22686;&#65292;&#20174;&#19981;&#21516;&#26223;&#35266;&#20013;&#30340;&#27700;&#26816;&#27979;&#21040;&#23665;&#22320;&#21644;&#22320;&#24418;&#30340;&#20998;&#21106;&#12290;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#21355;&#26143;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;&#23588;&#20854;&#26159;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#37325;&#28857;&#25918;&#22312;&#24320;&#21457;&#20934;&#30830;&#30340;&#27700;&#20307;&#26816;&#27979;&#12289;&#38634;&#21644;&#20113;&#30340;&#26041;&#27861;&#19978;&#65292;&#36825;&#20123;&#23545;&#29615;&#22659;&#30417;&#27979;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#28798;&#23475;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#19987;&#27880;&#20110;&#20174;&#36965;&#24863;&#22270;&#20687;&#20013;&#20998;&#21106;&#20113;&#12290;&#30001;&#20110;&#20809;&#23398;&#20256;&#24863;&#22120;&#24212;&#29992;&#20013;&#20113;&#30340;&#23384;&#22312;&#65292;&#20934;&#30830;&#30340;&#36965;&#24863;&#25968;&#25454;&#20998;&#26512;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#20135;&#21697;&#65288;&#22914;&#24212;&#29992;&#21644;&#30740;&#31350;&#65289;&#30340;&#36136;&#37327;&#30452;&#25509;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13918v1 Announce Type: cross  Abstract: Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#39118;&#21147;&#21457;&#30005;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#36807;&#26657;&#27491;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#25552;&#21462;&#30340;48&#23567;&#26102;&#39044;&#27979;&#25968;&#25454;&#65292;&#20854;&#20013;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#23558;&#24179;&#22343;NRMSE&#38477;&#20302;&#33267;22%&#12290;</title><link>https://arxiv.org/abs/2402.13916</link><description>&lt;p&gt;
&#21033;&#29992;SCADA&#25968;&#25454;&#21644;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#39118;&#21147;&#21457;&#30005;&#39044;&#27979;&#30340;&#20559;&#24046;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
Bias correction of wind power forecasts with SCADA data and continuous learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#39118;&#21147;&#21457;&#30005;&#36827;&#34892;&#39044;&#27979;&#65292;&#36890;&#36807;&#26657;&#27491;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#25552;&#21462;&#30340;48&#23567;&#26102;&#39044;&#27979;&#25968;&#25454;&#65292;&#20854;&#20013;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#23558;&#24179;&#22343;NRMSE&#38477;&#20302;&#33267;22%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#33021;&#22312;&#21521;&#21487;&#20877;&#29983;&#33021;&#28304;&#36807;&#28193;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#28982;&#32780;&#65292;&#39118;&#21147;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#21464;&#21270;&#24615;&#21487;&#33021;&#20250;&#38480;&#21046;&#20854;&#20840;&#38754;&#28508;&#21147;&#21644;&#39118;&#30005;&#23481;&#37327;&#30340;&#24517;&#35201;&#22686;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#25361;&#25112;&#65292;&#39118;&#21147;&#39044;&#27979;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#31649;&#29702;&#12289;&#33021;&#28304;&#20132;&#26131;&#25110;&#32500;&#25252;&#35843;&#24230;&#31561;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#12289;&#35780;&#20272;&#24182;&#27604;&#36739;&#20102;&#22235;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39118;&#21147;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#20174;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#25552;&#21462;&#30340;48&#23567;&#26102;&#39044;&#27979;&#36827;&#34892;&#20102;&#26657;&#27491;&#21644;&#25913;&#36827;&#12290;&#27169;&#22411;&#22312;&#21253;&#21547;65&#21488;&#39118;&#21147;&#28065;&#36718;&#26426;&#30340;&#39118;&#30005;&#22330;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#24179;&#22343;&#20559;&#24046;&#25913;&#36827;&#65292;&#23558;&#24179;&#22343;NRMSE&#38477;&#20302;&#21040;22%&#65292;&#19982;&#26410;&#26657;&#27491;NWP&#20351;&#29992;&#30340;&#24378;&#20559;&#24046;&#22522;&#32447;&#27169;&#22411;&#30340;35%&#30340;NRMSE&#30456;&#27604;&#65292;&#20559;&#24046;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13916v1 Announce Type: new  Abstract: Wind energy plays a critical role in the transition towards renewable energy sources. However, the uncertainty and variability of wind can impede its full potential and the necessary growth of wind power capacity. To mitigate these challenges, wind power forecasting methods are employed for applications in power management, energy trading, or maintenance scheduling. In this work, we present, evaluate, and compare four machine learning-based wind power forecasting models. Our models correct and improve 48-hour forecasts extracted from a numerical weather prediction (NWP) model. The models are evaluated on datasets from a wind park comprising 65 wind turbines. The best improvement in forecasting error and mean bias was achieved by a convolutional neural network, reducing the average NRMSE down to 22%, coupled with a significant reduction in mean bias, compared to a NRMSE of 35% from the strongly biased baseline model using uncorrected NWP 
&lt;/p&gt;</description></item><item><title>XAI&#39046;&#22495;&#34987;&#21010;&#20998;&#20026;&#34013;&#33394;XAI&#21644;&#32418;&#33394;XAI&#20004;&#31181;&#35299;&#37322;&#25991;&#21270;&#65292;&#25351;&#20986;&#20102;&#32418;&#33394;XAI&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#30740;&#31350;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13914</link><description>&lt;p&gt;
&#19981;&#26159;&#20026;&#20102;&#36777;&#35299;&#32780;&#26159;&#20026;&#20102;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explain to Question not to Justify
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13914
&lt;/p&gt;
&lt;p&gt;
XAI&#39046;&#22495;&#34987;&#21010;&#20998;&#20026;&#34013;&#33394;XAI&#21644;&#32418;&#33394;XAI&#20004;&#31181;&#35299;&#37322;&#25991;&#21270;&#65292;&#25351;&#20986;&#20102;&#32418;&#33394;XAI&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#30740;&#31350;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#19968;&#20010;&#24180;&#36731;&#20294;&#38750;&#24120;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35813;&#39046;&#22495;&#30446;&#21069;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#19981;&#21516;&#21644;&#19981;&#20860;&#23481;&#30446;&#26631;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;XAI&#39046;&#22495;&#20869;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#21508;&#31181;&#32447;&#32034;&#20998;&#20026;&#20004;&#31181;&#20114;&#34917;&#30340;&#25991;&#21270;&#65292;&#21363;&#20154;&#31867;/&#20215;&#20540;&#21462;&#21521;&#35299;&#37322;&#65288;&#34013;&#33394;XAI&#65289;&#21644;&#27169;&#22411;/&#39564;&#35777;&#21462;&#21521;&#35299;&#37322;&#65288;&#32418;&#33394;XAI&#65289;&#12290;&#25105;&#20204;&#36824;&#35748;&#20026;&#65292;&#32418;&#33394;XAI&#39046;&#22495;&#30446;&#21069;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#38544;&#34255;&#30528;&#24040;&#22823;&#30340;&#26426;&#36935;&#21644;&#37325;&#35201;&#30740;&#31350;&#30340;&#28508;&#21147;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#26377;&#21069;&#36884;&#30340;&#25361;&#25112;&#26469;&#24635;&#32467;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13914v1 Announce Type: new  Abstract: Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#27169;&#22411;&#65292;&#23558;&#27010;&#24565;&#24615;&#27700;&#25991;&#27169;&#22411;&#30340;&#36807;&#31243;&#29702;&#35299;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#25928;&#29575;&#32467;&#21512;&#65292;&#22312;&#27700;&#25991;&#24314;&#27169;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.13911</link><description>&lt;p&gt;
&#36890;&#36807;&#29289;&#29702;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#27700;&#25991;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Replication Study: Enhancing Hydrological Modeling with Physics-Guided Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#27169;&#22411;&#65292;&#23558;&#27010;&#24565;&#24615;&#27700;&#25991;&#27169;&#22411;&#30340;&#36807;&#31243;&#29702;&#35299;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#25928;&#29575;&#32467;&#21512;&#65292;&#22312;&#27700;&#25991;&#24314;&#27169;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#27700;&#25991;&#24314;&#27169;&#26041;&#27861;&#23558;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#21508;&#33258;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;&#21018;&#24615;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#20135;&#29983;&#30340;&#21442;&#25968;&#20272;&#35745;&#19981;&#20934;&#30830;&#65292;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24573;&#30053;&#29289;&#29702;&#36807;&#31243;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#22312;&#32467;&#26524;&#39044;&#27979;&#26041;&#38754;&#24456;&#20934;&#30830;&#65292;&#20294;&#34701;&#21512;&#31185;&#23398;&#30693;&#35782;&#23545;&#20110;&#21487;&#38752;&#30340;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#29289;&#29702;&#20449;&#24687;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;PIML&#65289;&#27169;&#22411;&#65292;&#23558;&#27010;&#24565;&#24615;&#27700;&#25991;&#27169;&#22411;&#30340;&#36807;&#31243;&#29702;&#35299;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#25928;&#29575;&#30456;&#32467;&#21512;&#12290;&#35813;PIML&#27169;&#22411;&#24212;&#29992;&#20110;Anandapur&#20998;&#27700;&#23725;&#65292;&#22312;&#39044;&#27979;&#26376;&#22343;&#27969;&#37327;&#21644;&#23454;&#38469;&#33976;&#33150;&#33976;&#21457;&#26102;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#29420;&#31435;&#30340;&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#30830;&#20445;&#36755;&#20986;&#30340;&#29289;&#29702;&#19968;&#33268;&#24615;&#12290;&#26412;&#30740;&#31350;&#22797;&#21046;&#20102;Bhasme&#65292;P.&#65292;Vagadiya&#65292;J.&#21644;Bhatia&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13911v1 Announce Type: new  Abstract: Current hydrological modeling methods combine data-driven Machine Learning (ML) algorithms and traditional physics-based models to address their respective limitations incorrect parameter estimates from rigid physics-based models and the neglect of physical process constraints by ML algorithms. Despite the accuracy of ML in outcome prediction, the integration of scientific knowledge is crucial for reliable predictions. This study introduces a Physics Informed Machine Learning (PIML) model, which merges the process understanding of conceptual hydrological models with the predictive efficiency of ML algorithms. Applied to the Anandapur sub-catchment, the PIML model demonstrates superior performance in forecasting monthly streamflow and actual evapotranspiration over both standalone conceptual models and ML algorithms, ensuring physical consistency of the outputs. This study replicates the methodologies of Bhasme, P., Vagadiya, J., &amp; Bhatia
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#31283;&#23450;&#20102;&#38543;&#26426;&#38797;&#28857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#19981;&#26029;&#22686;&#38271;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#26080;&#30028;&#26799;&#24230;&#21644;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#24615;&#33021;&#20445;&#35777;</title><link>https://arxiv.org/abs/2402.13903</link><description>&lt;p&gt;
&#22788;&#29702;&#38543;&#26426;&#38797;&#28857;&#20248;&#21270;&#20013;&#30340;&#26080;&#30028;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Dealing with unbounded gradients in stochastic saddle-point optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13903
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#31283;&#23450;&#20102;&#38543;&#26426;&#38797;&#28857;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#19981;&#26029;&#22686;&#38271;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#26080;&#30028;&#26799;&#24230;&#21644;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#23547;&#25214;&#20984;&#20985;&#20989;&#25968;&#38797;&#28857;&#30340;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#36825;&#31867;&#26041;&#27861;&#38754;&#20020;&#30340;&#19968;&#20010;&#20030;&#19990;&#38395;&#21517;&#30340;&#25361;&#25112;&#26159;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#26799;&#24230;&#21487;&#33021;&#20250;&#20219;&#24847;&#22686;&#38271;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#21644;&#21457;&#25955;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#31283;&#23450;&#20102;&#36845;&#20195;&#24182;&#20135;&#29983;&#20102;&#26377;&#24847;&#20041;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#21363;&#20351;&#23450;&#20041;&#22495;&#21644;&#26799;&#24230;&#22122;&#22768;&#38543;&#36845;&#20195;&#30340;&#35268;&#27169;&#32447;&#24615;&#21464;&#21270;&#65288;&#22240;&#27492;&#21487;&#33021;&#26159;&#26080;&#30028;&#30340;&#65289;&#12290;&#38500;&#20102;&#25552;&#20379;&#19968;&#31995;&#21015;&#19968;&#33324;&#24615;&#32467;&#26524;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20855;&#20307;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#23548;&#33268;&#22312;&#19981;&#38656;&#35201;&#26377;&#20851;&#20559;&#32622;&#36328;&#24230;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#25214;&#21040;&#24179;&#22343;&#22870;&#21169;MDP&#20013;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13903v1 Announce Type: new  Abstract: We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions. A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence. In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded). Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#23545;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#39640;&#20102;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13901</link><description>&lt;p&gt;
&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#65306;&#26032;&#26041;&#27861;&#21644;&#25913;&#36827;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#23545;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#39640;&#20102;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#25216;&#26415;&#20986;&#29616;&#65292;&#23558;&#22122;&#22768;&#36716;&#21270;&#20026;&#25968;&#25454;&#12290;&#29702;&#35770;&#19978;&#20027;&#35201;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#20165;&#22312;&#25991;&#29486;&#20013;&#23545;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#33719;&#24471;&#12290;&#26412;&#25991;&#20026;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#24314;&#31435;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#23545;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#39318;&#20808;&#20026;&#20855;&#26377;&#26377;&#38480;&#20108;&#38454;&#30697;&#30340;&#24179;&#28369;&#21644;&#19968;&#33324;&#65288;&#21487;&#33021;&#38750;&#20809;&#28369;&#65289;&#20998;&#24067;&#24314;&#31435;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#21518;&#23558;&#32467;&#26524;&#19987;&#38376;&#24212;&#29992;&#20110;&#19968;&#20123;&#26377;&#26126;&#30830;&#21442;&#25968;&#20381;&#36182;&#20851;&#31995;&#30340;&#26377;&#36259;&#20998;&#24067;&#31867;&#21035;&#65292;&#21253;&#25324;&#20855;&#26377;Lipschitz&#20998;&#25968;&#12289;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#21644;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13901v1 Announce Type: new  Abstract: The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;</title><link>https://arxiv.org/abs/2402.13897</link><description>&lt;p&gt;
&#31185;&#23398;&#26816;&#26597;&#32773;&#20877;&#24230;&#21319;&#32423;&#65306;&#36879;&#26126;&#24230;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#21452;&#21521;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#28982;&#38754;&#20020;&#30528;&#22312;&#31185;&#23398;&#21644;&#24037;&#19994;&#30340;&#28023;&#37327;&#20449;&#24687;&#20013;&#30340;&#35832;&#22810;&#38480;&#21046;&#65292;&#27604;&#22914;&#35821;&#20041;&#20998;&#27495;&#21644;&#26816;&#32034;&#20013;&#30340;&#35789;&#27719;&#24046;&#36317;&#12289;&#35821;&#20041;&#25628;&#32034;&#20013;&#30340;&#20302;&#31934;&#24230;&#21644;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#25110;&#32773;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#30340;&#36825;&#20123;&#38556;&#30861;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#36890;&#36807;&#26597;&#35810;&#25193;&#23637;&#22686;&#24378;&#20102;&#22312;&#31232;&#30095;&#26816;&#32034;&#20013;&#30340;&#35821;&#35328;&#29702;&#35299;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#12290;&#31532;&#20108;&#20010;&#27169;&#22359;&#36890;&#36807;&#21482;&#20351;&#29992;&#38271;&#25991;&#26723;&#20013;&#20256;&#25773;&#30340;&#20449;&#24687;&#65292;&#20026;&#22797;&#26434;&#38382;&#39064;&#25552;&#20379;&#20840;&#38754;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#26469;&#21152;&#28145;&#32467;&#26524;&#65292;&#23454;&#29616;&#21452;&#21521;&#20132;&#20114;&#12290;&#22312;&#31649;&#36947;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21521;&#29992;&#25143;&#21576;&#29616;&#20013;&#38388;&#32467;&#26524;&#20197;&#20419;&#36827;&#23545;&#31995;&#32479;&#25512;&#29702;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#21452;&#21521;&#26041;&#27861;&#24102;&#26469;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13897v1 Announce Type: cross  Abstract: Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#35299;&#20915;&#20102;&#23494;&#24230;&#27604;&#20272;&#35745;&#20013;&#30340;&#39281;&#21644;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#25910;&#25947;&#65292;&#22312;&#23494;&#24230;&#27604;&#20272;&#35745;&#22522;&#20934;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#28145;&#24230;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#38598;&#25104;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.13891</link><description>&lt;p&gt;
&#20811;&#26381;&#36845;&#20195;&#27491;&#21017;&#21270;&#20013;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#39281;&#21644;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Overcoming Saturation in Density Ratio Estimation by Iterated Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13891
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#35299;&#20915;&#20102;&#23494;&#24230;&#27604;&#20272;&#35745;&#20013;&#30340;&#39281;&#21644;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#25910;&#25947;&#65292;&#22312;&#23494;&#24230;&#27604;&#20272;&#35745;&#22522;&#20934;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#28145;&#24230;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#38598;&#25104;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26377;&#38480;&#26679;&#26412;&#20013;&#20272;&#35745;&#20004;&#20010;&#27010;&#29575;&#23494;&#24230;&#30340;&#27604;&#29575;&#65292;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#22823;&#31867;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#26680;&#26041;&#27861;&#23384;&#22312;&#38169;&#35823;&#39281;&#21644;&#38382;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#31639;&#27861;&#22312;&#39640;&#24230;&#35268;&#21017;&#23398;&#20064;&#38382;&#39064;&#19978;&#23454;&#29616;&#24555;&#36895;&#38169;&#35823;&#25910;&#25947;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#39281;&#21644;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#23494;&#24230;&#27604;&#20272;&#35745;&#20013;&#20197;&#23454;&#29616;&#24555;&#36895;&#38169;&#35823;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23494;&#24230;&#27604;&#20272;&#35745;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#22823;&#35268;&#27169;&#35780;&#20272;&#28145;&#24230;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#38598;&#25104;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13891v1 Announce Type: new  Abstract: Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics. In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems. To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates. Our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models.
&lt;/p&gt;</description></item><item><title>&#20248;&#21270;&#30340;Transformer&#27169;&#22411;DistilBERT&#29992;&#20110;&#26816;&#27979;&#38035;&#40060;&#37038;&#20214;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#25216;&#26415;&#35299;&#20915;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.13871</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;&#38035;&#40060;&#37038;&#20214;&#26816;&#27979;&#27169;&#22411;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13871
&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#30340;Transformer&#27169;&#22411;DistilBERT&#29992;&#20110;&#26816;&#27979;&#38035;&#40060;&#37038;&#20214;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#25216;&#26415;&#35299;&#20915;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38035;&#40060;&#37038;&#20214;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#32593;&#32476;&#23041;&#32961;&#65292;&#35797;&#22270;&#36890;&#36807;&#21457;&#36865;&#34394;&#20551;&#37038;&#20214;&#26469;&#27450;&#39575;&#29992;&#25143;&#65292;&#24847;&#22270;&#26159;&#31363;&#21462;&#26426;&#23494;&#20449;&#24687;&#25110;&#36896;&#25104;&#36130;&#21153;&#25439;&#22833;&#12290;&#25915;&#20987;&#32773;&#24120;&#24120;&#20882;&#20805;&#21487;&#20449;&#23454;&#20307;&#65292;&#21033;&#29992;&#25216;&#26415;&#36827;&#27493;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#38035;&#40060;&#30340;&#26816;&#27979;&#21644;&#39044;&#38450;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#36827;&#34892;&#20102;&#22823;&#37327;&#23398;&#26415;&#30740;&#31350;&#65292;&#20294;&#38035;&#40060;&#37038;&#20214;&#26816;&#27979;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#19988;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25513;&#30422;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#25317;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#22815;&#25552;&#20379;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#22522;&#20110;Transformer&#30340;DistilBERT&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#38035;&#40060;&#37038;&#20214;&#12290;&#22312;&#26816;&#27979;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#32452;&#38035;&#40060;&#37038;&#20214;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#39044;&#22788;&#29702;&#25216;&#26415;&#26469;&#28165;&#29702;&#21644;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13871v1 Announce Type: cross  Abstract: Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#21019;&#26032;&#33258;&#21160;&#32534;&#30721;&#22120;&#32467;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#38750;&#21442;&#25968;&#31283;&#24577;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#21019;&#26032;&#24207;&#21015;&#65292;&#36866;&#29992;&#20110;&#29983;&#25104;&#24335;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.13870</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21450;&#22312;&#30005;&#32593;&#36816;&#33829;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Probabilistic Time Series Forecasting and Applications in Grid Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13870
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#21019;&#26032;&#33258;&#21160;&#32534;&#30721;&#22120;&#32467;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#38750;&#21442;&#25968;&#31283;&#24577;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#21019;&#26032;&#24207;&#21015;&#65292;&#36866;&#29992;&#20110;&#29983;&#25104;&#24335;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#26681;&#25454;&#36807;&#21435;&#26102;&#38388;&#24207;&#21015;&#35266;&#27979;&#20540;&#32473;&#20986;&#26465;&#20214;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#20135;&#29983;&#26410;&#26469;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#22522;&#20110;&#39118;&#38505;&#30340;&#20915;&#31574;&#21644;&#19981;&#30830;&#23450;&#24615;&#35268;&#21010;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#30005;&#32593;&#36816;&#33829;&#20013;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#30005;&#20215;&#39044;&#27979;&#12289;&#22522;&#20110;&#39118;&#38505;&#30340;&#32463;&#27982;&#35843;&#24230;&#21644;&#38543;&#26426;&#20248;&#21270;&#12290;&#21463;Wiener&#21644;Kallianpur&#30340;&#21019;&#26032;&#34920;&#36798;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#21019;&#26032;&#33258;&#21160;&#32534;&#30721;&#22120;&#32467;&#26500;&#21644;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#65292;&#20174;&#38750;&#21442;&#25968;&#31283;&#24577;&#26102;&#38388;&#24207;&#21015;&#20013;&#25552;&#21462;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#21019;&#26032;&#24207;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24369;&#21019;&#26032;&#24207;&#21015;&#26159;&#36125;&#21494;&#26031;&#20805;&#20998;&#30340;&#65292;&#36825;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#24369;&#21019;&#26032;&#33258;&#21160;&#32534;&#30721;&#22120;&#25104;&#20026;&#29983;&#25104;&#24335;&#27010;&#29575;&#39044;&#27979;&#30340;&#26631;&#20934;&#20307;&#31995;&#32467;&#26500;&#12290;&#35813;&#25216;&#26415;&#34987;&#24212;&#29992;&#20110;&#39044;&#27979;&#39640;&#24230;&#27874;&#21160;&#30340;&#23454;&#26102;&#30005;&#20215;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13870v1 Announce Type: new  Abstract: Generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations. Such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations. Inspired by Wiener and Kallianpur's innovation representation, we propose a weak innovation autoencoder architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series. We show that the weak innovation sequence is Bayesian sufficient, which makes the proposed weak innovation autoencoder a canonical architecture for generative probabilistic forecasting. The proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20197;&#32852;&#21512;&#26816;&#27979;&#21644;&#24674;&#22797;&#24418;&#24335;&#22788;&#29702;&#23556;&#30005;&#39057;&#29575;&#24178;&#25200;&#65288;RFI&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#26469;&#35782;&#21035;&#21644;&#24674;&#22797;&#21463;&#21040;RFI&#24433;&#21709;&#30340;&#21160;&#24577;&#35889;&#37096;&#20998;</title><link>https://arxiv.org/abs/2402.13867</link><description>&lt;p&gt;
RFI-DRUnet: &#24674;&#22797;&#34987;&#23556;&#30005;&#24178;&#25200;&#27745;&#26579;&#30340;&#21160;&#24577;&#35889; -- &#24212;&#29992;&#20110;&#33033;&#20914;&#26143;&#35266;&#27979;
&lt;/p&gt;
&lt;p&gt;
RFI-DRUnet: Restoring dynamic spectra corrupted by radio frequency interference -- Application to pulsar observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13867
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20197;&#32852;&#21512;&#26816;&#27979;&#21644;&#24674;&#22797;&#24418;&#24335;&#22788;&#29702;&#23556;&#30005;&#39057;&#29575;&#24178;&#25200;&#65288;RFI&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#26469;&#35782;&#21035;&#21644;&#24674;&#22797;&#21463;&#21040;RFI&#24433;&#21709;&#30340;&#21160;&#24577;&#35889;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#30005;&#39057;&#29575;&#24178;&#25200;&#65288;RFI&#65289;&#19968;&#30452;&#26159;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#20851;&#27880;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#23545;&#38656;&#35201;&#39640;&#26102;&#24207;&#31934;&#24230;&#21644;&#25968;&#25454;&#28789;&#25935;&#24230;&#30340;&#33033;&#20914;&#26143;&#35266;&#27979;&#32780;&#35328;&#12290;&#22312;&#22823;&#22810;&#25968;&#25991;&#29486;&#20013;&#65292;RFI&#25233;&#21046;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#26816;&#27979;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#21160;&#24577;&#35889;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;RFI&#36827;&#34892;&#23450;&#20301;&#12290;&#35813;&#31574;&#30053;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20449;&#24687;&#30340;&#28508;&#22312;&#20002;&#22833;&#65292;&#22240;&#20026;&#22312;&#38543;&#21518;&#30340;&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#20013;&#36890;&#24120;&#19981;&#32771;&#34385;&#34987;&#35782;&#21035;&#20026;&#21487;&#33021;&#21463;RFI&#27745;&#26579;&#30340;&#20449;&#21495;&#37096;&#20998;&#12290;&#30456;&#21453;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#23558;RFI&#25233;&#21046;&#20316;&#20026;&#19968;&#20010;&#32852;&#21512;&#26816;&#27979;&#21644;&#24674;&#22797;&#36807;&#31243;&#65292;&#20801;&#35768;&#23545;RFI&#24433;&#21709;&#30340;&#21160;&#24577;&#35889;&#37096;&#20998;&#36827;&#34892;&#35782;&#21035;&#21644;&#24674;&#22797;&#12290;&#25152;&#25552;&#20986;&#30340;&#30417;&#30563;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#32593;&#32476;&#65292;&#20854;&#26550;&#26500;&#32487;&#25215;&#20102;&#19968;&#20010;&#26368;&#36817;&#27969;&#34892;&#30340;&#22270;&#20687;&#21435;&#22122;&#32593;&#32476;&#36798;&#21040;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#20010;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13867v1 Announce Type: cross  Abstract: Radio frequency interference (RFI) have been an enduring concern in radio astronomy, particularly for the observations of pulsars which require high timing precision and data sensitivity. In most works of the literature, RFI mitigation has been formulated as a detection task that consists of localizing possible RFI in dynamic spectra. This strategy inevitably leads to a potential loss of information since parts of the signal identified as possibly RFI-corrupted are generally not considered in the subsequent data processing pipeline. Conversely, this work proposes to tackle RFI mitigation as a joint detection and restoration that allows parts of the dynamic spectrum affected by RFI to be not only identified but also recovered. The proposed supervised method relies on a deep convolutional network whose architecture inherits the performance reached by a recent yet popular image-denoising network. To train this network, a whole simulation 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#31639;&#26415;&#35268;&#21017;&#24182;&#20998;&#26512;&#32593;&#32476;&#36755;&#20986;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#33539;&#22260;&#20998;&#26512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#20013;&#31561;&#20540;&#38754;&#25552;&#21462;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13861</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#25913;&#36827;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30340;&#31561;&#20540;&#38754;&#25552;&#21462;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improving Efficiency of Iso-Surface Extraction on Implicit Neural Representations Using Uncertainty Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13861
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#31639;&#26415;&#35268;&#21017;&#24182;&#20998;&#26512;&#32593;&#32476;&#36755;&#20986;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#33539;&#22260;&#20998;&#26512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#20013;&#31561;&#20540;&#38754;&#25552;&#21462;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INRs&#65289;&#24191;&#27867;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#38477;&#32500;&#21644;&#21487;&#35270;&#21270;&#65292;&#36890;&#36807;&#24314;&#27169;&#23558;&#31354;&#38388;&#20301;&#32622;&#26144;&#23556;&#21040;&#25968;&#25454;&#20540;&#30340;&#20989;&#25968;&#12290;&#22312;&#27809;&#26377;&#20851;&#20110;&#20540;&#30340;&#31354;&#38388;&#20998;&#24067;&#30340;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34987;&#36843;&#20174;INRs&#20013;&#23494;&#38598;&#37319;&#26679;&#26469;&#25191;&#34892;&#35832;&#22914;&#31561;&#20540;&#38754;&#25552;&#21462;&#20043;&#31867;&#30340;&#21487;&#35270;&#21270;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#38750;&#24120;&#32791;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#30340;&#33539;&#22260;&#20998;&#26512;&#24050;&#32463;&#26174;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#31639;&#26415;&#35268;&#21017;&#23558;&#32593;&#32476;&#36755;&#20986;&#33539;&#22260;&#38480;&#21046;&#22312;&#31354;&#38388;&#21306;&#22495;&#20869;&#65292;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#22312;INRs&#19978;&#36827;&#34892;&#20960;&#20309;&#26597;&#35810;&#65288;&#22914;&#23556;&#32447;&#25237;&#23556;&#21644;&#23618;&#27425;&#32593;&#26684;&#25552;&#21462;&#65289;&#30340;&#25928;&#29575;&#65292;&#29992;&#20110;3D&#20960;&#20309;&#22270;&#24418;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#31185;&#23398;&#25968;&#25454;&#65292;&#20998;&#26512;&#36793;&#30028;&#24448;&#24448;&#36807;&#20110;&#20445;&#23432;&#12290;&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#31639;&#26415;&#35268;&#21017;&#24182;&#20998;&#26512;&#32593;&#32476;&#36755;&#20986;&#22312;&#31354;&#38388;&#21306;&#22495;&#20869;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#23545;&#33539;&#22260;&#20998;&#26512;&#30340;&#25913;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13861v1 Announce Type: cross  Abstract: Implicit Neural representations (INRs) are widely used for scientific data reduction and visualization by modeling the function that maps a spatial location to a data value. Without any prior knowledge about the spatial distribution of values, we are forced to sample densely from INRs to perform visualization tasks like iso-surface extraction which can be very computationally expensive. Recently, range analysis has shown promising results in improving the efficiency of geometric queries, such as ray casting and hierarchical mesh extraction, on INRs for 3D geometries by using arithmetic rules to bound the output range of the network within a spatial region. However, the analysis bounds are often too conservative for complex scientific data. In this paper, we present an improved technique for range analysis by revisiting the arithmetic rules and analyzing the probability distribution of the network output within a spatial region. We mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#23398;&#20064;&#22823;&#38388;&#36317;&#21322;&#31354;&#38388;&#38382;&#39064;&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#31639;&#27861;&#65292;&#22312;&#32500;&#24230;&#26080;&#20851;&#12289;&#26102;&#38388;&#22797;&#26434;&#24230;&#20248;&#21270;&#12289;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#31561;&#22810;&#20010;&#20851;&#38190;&#21442;&#25968;&#19978;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13857</link><description>&lt;p&gt;
&#21487;&#22797;&#21046;&#23398;&#20064;&#22823;&#38388;&#36317;&#21322;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Replicable Learning of Large-Margin Halfspaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#23398;&#20064;&#22823;&#38388;&#36317;&#21322;&#31354;&#38388;&#38382;&#39064;&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#31639;&#27861;&#65292;&#22312;&#32500;&#24230;&#26080;&#20851;&#12289;&#26102;&#38388;&#22797;&#26434;&#24230;&#20248;&#21270;&#12289;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#31561;&#22810;&#20010;&#20851;&#38190;&#21442;&#25968;&#19978;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#26469;&#35299;&#20915;&#23398;&#20064;&#22823;&#38388;&#36317;&#21322;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25913;&#36827;&#20102;Impagliazzo, Lei, Pitassi&#21644;Sorrell&#22312;STOC, 2022&#20013;&#25552;&#20379;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#36825;&#20010;&#20219;&#21153;&#30340;&#39318;&#20010;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;&#22810;&#39033;&#24335;&#65292;&#26159;&#27491;&#30830;&#30340;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#30456;&#20851;&#21442;&#25968;&#26041;&#38754;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#37117;&#20005;&#26684;&#27604;Impagliazzo&#31561;&#20154;&#22312;2022&#24180;&#23454;&#29616;&#30340;&#31639;&#27861;&#35201;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;&#22312;&#31934;&#24230;&#21442;&#25968;$\epsilon$&#26041;&#38754;&#20855;&#26377;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;SGD&#30340;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#22312;&#26576;&#20123;&#21442;&#25968;&#33539;&#22260;&#20869;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20248;&#20110;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13857v1 Announce Type: new  Abstract: We provide efficient replicable algorithms for the problem of learning large-margin halfspaces. Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first dimension-independent replicable algorithms for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al. [2022] with respect to all the relevant parameters. Moreover, our first algorithm has sample complexity that is optimal with respect to the accuracy parameter $\epsilon$. We also design an SGD-based replicable algorithm that, in some parameters' regimes, achieves better sample and time complexity than our first algorithm.   Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to o
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25928;&#29575;&#24182;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.13852</link><description>&lt;p&gt;
&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Neural Control System for Continuous Glucose Monitoring and Maintenance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13852
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25928;&#29575;&#24182;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#33889;&#33796;&#31958;&#27700;&#24179;&#31649;&#29702;&#23545;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#36991;&#20813;&#20005;&#37325;&#24182;&#21457;&#30151;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#21033;&#29992;&#24494;&#20998;&#39044;&#27979;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21463;&#21040;&#22797;&#26434;&#31070;&#32463;&#31574;&#30053;&#21644;&#21487;&#21306;&#20998;&#24314;&#27169;&#30340;&#25351;&#23548;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#12290;&#36825;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#26368;&#22823;&#21270;&#25928;&#29575;&#65292;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#21644;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#65292;&#22914;&#32463;&#39564;&#21457;&#29616;&#25152;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13852v1 Announce Type: cross  Abstract: Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#24615;LLM&#25512;&#26029;&#30340;&#21311;&#21517;&#21270;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.13846</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20808;&#36827;&#30340;&#21311;&#21517;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Advanced Anonymizers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13846
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#24615;LLM&#25512;&#26029;&#30340;&#21311;&#21517;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38544;&#31169;&#30740;&#31350;&#39046;&#22495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#25512;&#26029;&#30495;&#23454;&#19990;&#30028;&#22312;&#32447;&#25991;&#26412;&#20013;&#30340;&#20010;&#20154;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21311;&#21517;&#21270;&#26041;&#27861;&#24403;&#21069;&#24050;&#32463;&#33853;&#21518;&#20110;&#30417;&#31649;&#35201;&#27714;&#21644;&#23545;&#25239;&#23041;&#32961;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#20010;&#20154;&#22914;&#20309;&#26377;&#25928;&#22320;&#20445;&#25252;&#20182;&#20204;&#22312;&#20998;&#20139;&#22312;&#32447;&#25991;&#26412;&#26102;&#30340;&#20010;&#20154;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20004;&#27493;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#29992;&#20110;&#35780;&#20272;&#38754;&#23545;&#23545;&#25239;&#24615;LLM&#30340;&#25512;&#26029;&#26102;&#30340;&#21311;&#21517;&#21270;&#25928;&#26524;&#65292;&#20174;&#32780;&#20801;&#35768;&#33258;&#28982;&#22320;&#27979;&#37327;&#21311;&#21517;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#32416;&#27491;&#20102;&#20197;&#21069;&#25351;&#26631;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#23545;&#25239;&#24615;&#21311;&#21517;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;LLM&#30340;&#24378;&#22823;&#25512;&#26029;&#33021;&#21147;&#26469;&#25351;&#23548;&#25105;&#20204;&#30340;&#21311;&#21517;&#21270;&#36807;&#31243;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21311;&#21517;&#21270;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13846v1 Announce Type: cross  Abstract: Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts. With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. This raises the question of how individuals can effectively protect their personal data in sharing online texts. In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure. In our experimental evaluation, we show on real-world 
&lt;/p&gt;</description></item><item><title>MLXP&#26159;&#19968;&#20010;&#31616;&#21333;&#36731;&#37327;&#30340;&#22522;&#20110;Python&#30340;&#23454;&#39564;&#31649;&#29702;&#24037;&#20855;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#22797;&#21046;&#24615;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.13831</link><description>&lt;p&gt;
MLXP&#65306;&#19968;&#20010;&#29992;&#20110;&#22312;Python&#20013;&#36827;&#34892;&#21487;&#22797;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MLXP: A framework for conducting replicable Machine Learning eXperiments in Python
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13831
&lt;/p&gt;
&lt;p&gt;
MLXP&#26159;&#19968;&#20010;&#31616;&#21333;&#36731;&#37327;&#30340;&#22522;&#20110;Python&#30340;&#23454;&#39564;&#31649;&#29702;&#24037;&#20855;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#22797;&#21046;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30740;&#31350;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#22797;&#26434;&#30340;&#38750;&#30830;&#23450;&#24615;&#31639;&#27861;&#65292;&#24182;&#20381;&#36182;&#20110;&#20247;&#22810;&#36229;&#21442;&#25968;&#36873;&#25321;&#65292;&#22914;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#30830;&#20445;&#21487;&#37325;&#29616;&#21644;&#21487;&#22797;&#21046;&#30340;&#32467;&#26524;&#23545;&#20110;&#25512;&#36827;&#35813;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#36827;&#34892;&#31995;&#32479;&#21270;&#21644;&#32452;&#32455;&#33391;&#22909;&#30340;&#23454;&#39564;&#65292;&#20174;&#32780;&#24471;&#20986;&#31283;&#20581;&#30340;&#32467;&#35770;&#65292;&#21364;&#38656;&#35201;&#25237;&#20837;&#22823;&#37327;&#25216;&#26415;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#20302;&#37319;&#32435;&#29575;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;MLXP&#65292;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#24320;&#28304;&#12289;&#31616;&#21333;&#12289;&#36731;&#37327;&#32423;&#23454;&#39564;&#31649;&#29702;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13831v1 Announce Type: new  Abstract: Replicability in machine learning (ML) research is increasingly concerning due to the utilization of complex non-deterministic algorithms and the dependence on numerous hyper-parameter choices, such as model architecture and training datasets. Ensuring reproducible and replicable results is crucial for advancing the field, yet often requires significant technical effort to conduct systematic and well-organized experiments that yield robust conclusions. Several tools have been developed to facilitate experiment management and enhance reproducibility; however, they often introduce complexity that hinders adoption within the research community, despite being well-handled in industrial settings. To address the challenge of low adoption, we propose MLXP, an open-source, simple, and lightweight experiment management tool based on Python, available at https://github.com/inria-thoth/mlxp . MLXP streamlines the experimental process with minimal p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#28385;&#36275;Lipschitz&#36830;&#32493;&#24615;&#26465;&#20214;&#30340;Conf-MDP&#30340;&#24615;&#33021;&#25913;&#36827;&#30028;&#38480;&#65292;&#24182;&#25512;&#23548;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24615;&#33021;&#25913;&#36827;&#19979;&#30028;</title><link>https://arxiv.org/abs/2402.13821</link><description>&lt;p&gt;
Lipschitz&#21487;&#37197;&#32622;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#24615;&#33021;&#25913;&#36827;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Performance Improvement Bounds for Lipschitz Configurable Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#28385;&#36275;Lipschitz&#36830;&#32493;&#24615;&#26465;&#20214;&#30340;Conf-MDP&#30340;&#24615;&#33021;&#25913;&#36827;&#30028;&#38480;&#65292;&#24182;&#25512;&#23548;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24615;&#33021;&#25913;&#36827;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37197;&#32622;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;Conf-MDPs&#65289;&#26368;&#36817;&#34987;&#24341;&#20837;&#20316;&#20026;&#20256;&#32479;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#19968;&#20010;&#25193;&#23637;&#65292;&#29992;&#20110;&#27169;&#25311;&#22312;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26377;&#21487;&#33021;&#24178;&#39044;&#29615;&#22659;&#20197;&#37197;&#32622;&#19968;&#20123;&#21442;&#25968;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#28385;&#36275;&#27491;&#21017;&#24615;&#26465;&#20214;&#30340;Conf-MDP&#30340;&#19968;&#20010;&#29305;&#23450;&#23376;&#31867;&#65292;&#21363;Lipschitz&#36830;&#32493;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#30001;&#25913;&#21464;&#31574;&#30053;&#21644;&#37197;&#32622;&#24341;&#21457;&#30340;$\gamma$-&#25240;&#25187;&#31283;&#24577;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#30028;&#38480;&#12290;&#35813;&#32467;&#26524;&#25512;&#24191;&#20102;&#24050;&#32463;&#23384;&#22312;&#30340;Conf-MDP&#21644;&#20256;&#32479;MDP&#30340;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24615;&#33021;&#25913;&#36827;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13821v1 Announce Type: new  Abstract: Configurable Markov Decision Processes (Conf-MDPs) have recently been introduced as an extension of the traditional Markov Decision Processes (MDPs) to model the real-world scenarios in which there is the possibility to intervene in the environment in order to configure some of its parameters. In this paper, we focus on a particular subclass of Conf-MDP that satisfies regularity conditions, namely Lipschitz continuity. We start by providing a bound on the Wasserstein distance between $\gamma$-discounted stationary distributions induced by changing policy and configuration. This result generalizes the already existing bounds both for Conf-MDPs and traditional MDPs. Then, we derive a novel performance improvement lower bound.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#12289;&#32467;&#26500;&#21270;&#30340;&#34920;&#31034;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#28508;&#21160;&#21147;&#23398;&#25552;&#39640;&#20102;&#36816;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25554;&#20540;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13820</link><description>&lt;p&gt;
FLD&#65306;&#20613;&#31435;&#21494;&#28508;&#21160;&#21147;&#23398;&#29992;&#20110;&#32467;&#26500;&#21270;&#36816;&#21160;&#34920;&#31034;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13820
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#12289;&#32467;&#26500;&#21270;&#30340;&#34920;&#31034;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#28508;&#21160;&#21147;&#23398;&#25552;&#39640;&#20102;&#36816;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25554;&#20540;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#36712;&#36857;&#20026;&#22522;&#20110;&#29289;&#29702;&#30340;&#36816;&#21160;&#23398;&#20064;&#25552;&#20379;&#21487;&#38752;&#21442;&#32771;&#65292;&#20294;&#22312;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#35206;&#30422;&#30340;&#21306;&#22495;&#65292;&#23384;&#22312;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#25552;&#21462;&#21608;&#26399;&#24615;&#25110;&#20934;&#21608;&#26399;&#24615;&#36816;&#21160;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;&#22312;&#36830;&#32493;&#21442;&#25968;&#21270;&#30340;&#28508;&#31354;&#38388;&#20013;&#30340;&#36816;&#21160;&#21160;&#21147;&#23398;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#36816;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25554;&#20540;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#36816;&#21160;&#21442;&#25968;&#21270;&#21551;&#21457;&#30340;&#36816;&#21160;&#23398;&#20064;&#25511;&#21046;&#22120;&#21487;&#20197;&#22312;&#32447;&#36319;&#36394;&#21508;&#31181;&#36816;&#21160;&#65292;&#21253;&#25324;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#19968;&#20010;&#22238;&#36864;&#26426;&#21046;&#65292;&#25511;&#21046;&#22120;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#20854;&#36319;&#36394;&#31574;&#30053;&#65292;&#24182;&#22312;&#25552;&#20986;&#28508;&#22312;&#21361;&#38505;&#30446;&#26631;&#26102;&#33258;&#21160;&#37319;&#21462;&#23433;&#20840;&#34892;&#21160;&#25191;&#34892;&#12290;&#36890;&#36807;&#21033;&#29992;&#35782;&#21035;&#30340;&#26102;&#31354;&#32467;&#26500;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21551;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13820v1 Announce Type: cross  Abstract: Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage. To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions. The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms. The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training. With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed. By leveraging the identified spatial-temporal structure, our work open
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#24378;&#22823;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#26469;&#39044;&#27979;&#20303;&#38498;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13812</link><description>&lt;p&gt;
&#22522;&#20110;&#22768;&#38899;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22686;&#24378;&#30340;&#20303;&#38498;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#27515;&#20129;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Voice-Driven Mortality Prediction in Hospitalized Heart Failure Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#24378;&#22823;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#26469;&#39044;&#27979;&#20303;&#38498;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#23545;&#24515;&#21147;&#34928;&#31469;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#23384;&#22312;&#30340;&#20840;&#29699;&#20581;&#24247;&#38382;&#39064;&#65292;&#23454;&#26045;&#21019;&#26032;&#26041;&#27861;&#20197;&#22686;&#24378;&#24739;&#32773;&#25252;&#29702;&#23384;&#22312;&#22256;&#38590;&#12290;&#29305;&#21035;&#26159;&#22312;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#20013;&#39044;&#27979;&#27515;&#20129;&#29575;&#26082;&#22256;&#38590;&#21448;&#20851;&#38190;&#65292;&#38656;&#35201;&#20010;&#24615;&#21270;&#25252;&#29702;&#12289;&#31215;&#26497;&#31649;&#29702;&#65292;&#24182;&#25903;&#25345;&#30693;&#24773;&#20915;&#31574;&#20197;&#22686;&#24378;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#19982;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30456;&#32467;&#21512;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#31361;&#20986;&#65292;&#29305;&#21035;&#22312;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#26377;&#25928;&#24615;&#12290;&#22768;&#38899;&#20998;&#26512;&#19982;ML&#31639;&#27861;&#30340;&#21327;&#21516;&#20316;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#19988;&#26131;&#20110;&#33719;&#21462;&#30340;&#35780;&#20272;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#31526;&#21512;&#26631;&#20934;&#21270;&#35821;&#38899;&#21327;&#35758;&#30340;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#39044;&#27979;&#27515;&#20129;&#29575;&#32570;&#20047;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#24378;&#22823;&#26377;&#25928;&#30340;ML&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22768;&#38899;&#29983;&#29289;&#26631;&#24535;&#29289;&#26469;&#39044;&#27979;&#20303;&#38498;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#27515;&#20129;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13812v1 Announce Type: new  Abstract: Addressing heart failure (HF) as a prevalent global health concern poses difficulties in implementing innovative approaches for enhanced patient care. Predicting mortality rates in HF patients, in particular, is difficult yet critical, necessitating individualized care, proactive management, and enabling educated decision-making to enhance outcomes. Recently, the significance of voice biomarkers coupled with Machine Learning (ML) has surged, demonstrating remarkable efficacy, particularly in predicting heart failure. The synergy of voice analysis and ML algorithms provides a non-invasive and easily accessible means to evaluate patients' health. However, there is a lack of voice biomarkers for predicting mortality rates among heart failure patients with standardized speech protocols. Here, we demonstrate a powerful and effective ML model for predicting mortality rates in hospitalized HF patients through the utilization of voice biomarkers
&lt;/p&gt;</description></item><item><title>&#39044;&#26465;&#20214;Langevin&#21160;&#21147;&#23398;&#30340;&#39044;&#26399;&#25439;&#22833;&#19982;&#30446;&#26631;&#20989;&#25968;&#30340;Hessian&#31209;&#25104;&#27491;&#27604;&#65292;&#24182;&#19988;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20855;&#26377;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.13810</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#26465;&#20214;Langevin &#21160;&#21147;&#23398;&#30340;&#39044;&#26399;&#25439;&#22833;&#25581;&#31034;Hessian&#31209;
&lt;/p&gt;
&lt;p&gt;
The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13810
&lt;/p&gt;
&lt;p&gt;
&#39044;&#26465;&#20214;Langevin&#21160;&#21147;&#23398;&#30340;&#39044;&#26399;&#25439;&#22833;&#19982;&#30446;&#26631;&#20989;&#25968;&#30340;Hessian&#31209;&#25104;&#27491;&#27604;&#65292;&#24182;&#19988;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20855;&#26377;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Langevin &#21160;&#21147;&#23398;&#65288;LD&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#20174;&#20998;&#24067;&#20013;&#25277;&#26679;&#21644;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#39044;&#26465;&#20214;LD&#22312;&#30446;&#26631;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#38468;&#36817;&#30340;&#39044;&#26399;&#25439;&#22833;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22312;&#36825;&#20123;&#28857;&#38468;&#36817;&#65292;LD&#20250;&#36864;&#21270;&#20026;&#19968;&#31181;&#36866;&#21512;&#20415;&#21033;&#25968;&#23398;&#22788;&#29702;&#30340;Ornstein-Uhlenbeck&#36807;&#31243;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#39044;&#26465;&#20214;&#30697;&#38453;&#28385;&#36275;&#19982;&#22122;&#22768;&#21327;&#26041;&#24046;&#30340;&#29305;&#23450;&#20851;&#31995;&#26102;&#65292;LD&#30340;&#39044;&#26399;&#25439;&#22833;&#23558;&#25104;&#27491;&#27604;&#20110;&#30446;&#26631;Hessian&#30340;&#31209;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#32467;&#26524;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#32972;&#26223;&#19979;&#30340;&#36866;&#29992;&#24615;&#65292;&#20854;&#20013;Hessian&#31209;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#25429;&#25417;&#39044;&#27979;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#35745;&#31639;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#20998;&#26512;&#27604;&#36739;&#20102;&#31867;&#20284;&#20110;SGD&#21644;&#31867;&#20284;&#20110;Adam&#30340;&#39044;&#22788;&#29702;&#22120;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#21508;&#33258;&#23548;&#33268;&#30340;&#24773;&#20917;&#19979;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13810v1 Announce Type: new  Abstract: Langevin dynamics (LD) is widely used for sampling from distributions and for optimization. In this work, we derive a closed-form expression for the expected loss of preconditioned LD near stationary points of the objective function. We use the fact that at the vicinity of such points, LD reduces to an Ornstein-Uhlenbeck process, which is amenable to convenient mathematical treatment. Our analysis reveals that when the preconditioning matrix satisfies a particular relation with respect to the noise covariance, LD's expected loss becomes proportional to the rank of the objective's Hessian. We illustrate the applicability of this result in the context of neural networks, where the Hessian rank has been shown to capture the complexity of the predictor function but is usually computationally hard to probe. Finally, we use our analysis to compare SGD-like and Adam-like preconditioners and identify the regimes under which each of them leads to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#35780;&#20272;&#25581;&#31034;&#20102;&#36965;&#24863;&#39046;&#22495;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#36235;&#21183;&#65292;&#25506;&#35752;&#20102;&#26032;&#22411;&#26041;&#27861;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20026;&#35299;&#20915;&#29305;&#23450;&#36965;&#24863;&#38590;&#39064;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.13791</link><description>&lt;p&gt;
&#25171;&#24320;&#40657;&#21283;&#23376;&#65306;&#36965;&#24863;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Opening the Black-Box: A Systematic Review on Explainable AI in Remote Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#24615;&#35780;&#20272;&#25581;&#31034;&#20102;&#36965;&#24863;&#39046;&#22495;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#20351;&#29992;&#36235;&#21183;&#65292;&#25506;&#35752;&#20102;&#26032;&#22411;&#26041;&#27861;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20026;&#35299;&#20915;&#29305;&#23450;&#36965;&#24863;&#38590;&#39064;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#40657;&#21283;&#23376;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#20026;&#36965;&#24863;&#39046;&#22495;&#30693;&#35782;&#25552;&#21462;&#30340;&#20027;&#23548;&#24314;&#27169;&#33539;&#24335;&#12290;&#23613;&#31649;&#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25581;&#31034;&#36825;&#20123;&#27169;&#22411;&#20869;&#37096;&#36816;&#34892;&#26426;&#21046;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#36804;&#20170;&#20173;&#32570;&#20047;&#19968;&#20221;&#20840;&#38754;&#27010;&#36848;&#65292;&#24635;&#32467;&#22312;&#36965;&#24863;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#21450;&#20854;&#30446;&#26631;&#12289;&#21457;&#29616;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#31995;&#32479;&#24615;&#35780;&#20272;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20197;&#35782;&#21035;&#21487;&#35299;&#37322;AI&#22312;&#36965;&#24863;&#20013;&#30340;&#20351;&#29992;&#20027;&#35201;&#36235;&#21183;&#65292;&#25581;&#31034;&#35299;&#20915;&#29305;&#23450;&#36965;&#24863;&#25361;&#25112;&#30340;&#26032;&#22411;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#21644;&#26032;&#20852;&#26041;&#21521;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#35299;&#37322;&#35299;&#37322;&#30340;&#24120;&#35265;&#27169;&#24335;&#65292;&#35752;&#35770;&#20102;&#36965;&#24863;&#20013;&#25552;&#21462;&#30340;&#31185;&#23398;&#35265;&#35299;&#65292;&#24182;&#21453;&#24605;&#20102;&#29992;&#20110;&#21487;&#35299;&#37322;AI&#26041;&#27861;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#29616;&#26377;&#25216;&#26415;&#30340;&#23436;&#25972;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13791v1 Announce Type: new  Abstract: In recent years, black-box machine learning approaches have become a dominant modeling paradigm for knowledge extraction in Remote Sensing. Despite the potential benefits of uncovering the inner workings of these models with explainable AI, a comprehensive overview summarizing the used explainable AI methods and their objectives, findings, and challenges in Remote Sensing applications is still missing. In this paper, we address this issue by performing a systematic review to identify the key trends of how explainable AI is used in Remote Sensing and shed light on novel explainable AI approaches and emerging directions that tackle specific Remote Sensing challenges. We also reveal the common patterns of explanation interpretation, discuss the extracted scientific insights in Remote Sensing, and reflect on the approaches used for explainable AI methods evaluation. Our review provides a complete summary of the state-of-the-art in the field.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ExDyna&#30340;&#26032;&#22411;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#22359;&#21644;&#20998;&#21306;&#32452;&#21512;&#26469;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.13781</link><description>&lt;p&gt;
&#20445;&#25345;&#25509;&#36817;&#26368;&#20248;&#26799;&#24230;&#31232;&#30095;&#21270;&#25104;&#26412;&#30340;&#21487;&#25193;&#23637;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Preserving Near-Optimal Gradient Sparsification Cost for Scalable Distributed Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13781
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ExDyna&#30340;&#26032;&#22411;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#22359;&#21644;&#20998;&#21306;&#32452;&#21512;&#26469;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#24320;&#38144;&#26159;&#25193;&#23637;&#20998;&#24067;&#24335;&#35757;&#32451;&#31995;&#32479;&#30340;&#19968;&#22823;&#38556;&#30861;&#12290;&#26799;&#24230;&#31232;&#30095;&#21270;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#36890;&#20449;&#37327;&#32780;&#19981;&#26174;&#33879;&#25439;&#22833;&#27169;&#22411;&#30340;&#20445;&#30495;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#27861;&#30001;&#20110;&#20854;&#31639;&#27861;&#35774;&#35745;&#20302;&#25928;&#65292;&#20174;&#32780;&#23548;&#33268;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ExDyna&#30340;&#26032;&#22411;&#26799;&#24230;&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#20855;&#26377;&#32454;&#31890;&#24230;&#22359;&#21644;&#32452;&#25104;&#38750;&#37325;&#21472;&#20998;&#21306;&#30340;&#36830;&#32493;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13781v1 Announce Type: new  Abstract: Communication overhead is a major obstacle to scaling distributed training systems. Gradient sparsification is a potential optimization approach to reduce the communication volume without significant loss of model fidelity. However, existing gradient sparsification methods have low scalability owing to inefficient design of their algorithms, which raises the communication overhead significantly. In particular, gradient build-up and inadequate sparsity control methods degrade the sparsification performance considerably. Moreover, communication traffic increases drastically owing to workload imbalance of gradient selection between workers.   To address these challenges, we propose a novel gradient sparsification scheme called ExDyna. In ExDyna, the gradient tensor of the model comprises fined-grained blocks, and contiguous blocks are grouped into non-overlapping partitions. Each worker selects gradients in its exclusively allocated partiti
&lt;/p&gt;</description></item><item><title>REMO&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35265;&#21270;&#23398;&#20013;&#26126;&#30830;&#23450;&#20041;&#30340;&#21407;&#23376;&#32452;&#21512;&#35268;&#21017;&#65292;&#22312;1.7&#30334;&#19975;&#20010;&#24050;&#30693;&#21270;&#23398;&#21453;&#24212;&#19978;&#39044;&#35757;&#32451;&#22270;&#24418;/Transformer&#32534;&#30721;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;Masked Reaction Centre Reconstruction (MRCR)&#21644;Reaction Centre Identification (RCI)&#20004;&#20010;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20026;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.13779</link><description>&lt;p&gt;
&#20174;&#21270;&#23398;&#21453;&#24212;&#30693;&#35782;&#20013;&#23398;&#20064;&#19978;&#19979;&#25991;&#20998;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Contextual Molecule Representation Learning from Chemical Reaction Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13779
&lt;/p&gt;
&lt;p&gt;
REMO&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35265;&#21270;&#23398;&#20013;&#26126;&#30830;&#23450;&#20041;&#30340;&#21407;&#23376;&#32452;&#21512;&#35268;&#21017;&#65292;&#22312;1.7&#30334;&#19975;&#20010;&#24050;&#30693;&#21270;&#23398;&#21453;&#24212;&#19978;&#39044;&#35757;&#32451;&#22270;&#24418;/Transformer&#32534;&#30721;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;Masked Reaction Centre Reconstruction (MRCR)&#21644;Reaction Centre Identification (RCI)&#20004;&#20010;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20026;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21033;&#29992;&#20016;&#23500;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#65288;MRL&#65289;&#26102;&#65292;&#27969;&#34892;&#30340;&#25216;&#26415;(&#22914;&#25513;&#30721;&#20122;&#21333;&#20301;&#37325;&#24314;)&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#22240;&#20026;&#20998;&#23376;&#20013;&#21487;&#33021;&#30340;&#21407;&#23376;&#32452;&#21512;&#26041;&#24335;&#33258;&#30001;&#24230;&#36739;&#39640;&#65292;&#32473;&#25513;&#30721;&#37325;&#24314;&#33539;&#24335;&#24102;&#26469;&#20102;&#38590;&#20197;&#36926;&#36234;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;REMO&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#24120;&#35265;&#21270;&#23398;&#20013;&#30340;&#26126;&#30830;&#23450;&#20041;&#30340;&#21407;&#23376;&#32452;&#21512;&#35268;&#21017;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;REMO&#22312;&#25991;&#29486;&#20013;&#24050;&#30693;&#30340;170&#19975;&#20010;&#21270;&#23398;&#21453;&#24212;&#19978;&#36827;&#34892;&#20102;&#22270;/&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#39044;&#35757;&#32451;&#30446;&#26631;: &#25513;&#30721;&#21453;&#24212;&#20013;&#24515;&#37325;&#24314;&#65288;MRCR&#65289;&#21644;&#21453;&#24212;&#20013;&#24515;&#35782;&#21035;&#65288;RCI&#65289;&#12290;REMO&#36890;&#36807;&#21033;&#29992;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;MRL&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13779v1 Announce Type: cross  Abstract: In recent years, self-supervised learning has emerged as a powerful tool to harness abundant unlabelled data for representation learning and has been broadly adopted in diverse areas. However, when applied to molecular representation learning (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm. To tackle this challenge, we introduce REMO, a self-supervised learning framework that takes advantage of well-defined atom-combination rules in common chemistry. Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature. We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO offers a novel solution to MRL by exploi
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13777</link><description>&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65306;&#25945;&#31243;&#12289;&#35843;&#26597;&#21644;&#26410;&#26469;&#26041;&#21521;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13777
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#20174;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#26041;&#38754;&#12290;&#31867;&#20284;&#22320;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#20063;&#38656;&#35201;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#29983;&#25104;&#20989;&#25968;&#20316;&#20026;&#31574;&#30053;&#25110;&#25919;&#31574;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#22240;&#27492;&#19981;&#21516;&#20998;&#25903;&#30340;&#21457;&#23637;&#30456;&#23545;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#24212;&#29992;&#26041;&#38754;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#24402;&#19968;&#21270;&#27969;&#12289;&#21464;&#21387;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;Cas-DiffCom&#65292;&#29992;&#20110;&#35299;&#20915;&#23156;&#20799;&#32437;&#21521;3D&#21307;&#23398;&#22270;&#20687;&#34917;&#20840;&#20013;&#23384;&#22312;&#30340;&#20010;&#20307;&#26102;&#38388;&#32500;&#24230;&#19968;&#33268;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13776</link><description>&lt;p&gt;
Cas-DiffCom: &#23156;&#20799;&#32437;&#21521;&#36229;&#20998;&#36776;&#29575;3D&#21307;&#23398;&#22270;&#20687;&#34917;&#20840;&#30340;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cas-DiffCom: Cascaded diffusion model for infant longitudinal super-resolution 3D medical image completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;Cas-DiffCom&#65292;&#29992;&#20110;&#35299;&#20915;&#23156;&#20799;&#32437;&#21521;3D&#21307;&#23398;&#22270;&#20687;&#34917;&#20840;&#20013;&#23384;&#22312;&#30340;&#20010;&#20307;&#26102;&#38388;&#32500;&#24230;&#19968;&#33268;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#23156;&#20799;&#26399;&#26159;&#34892;&#20026;&#21644;&#31070;&#32463;&#35748;&#30693;&#24555;&#36895;&#19988;&#21160;&#24577;&#21457;&#23637;&#30340;&#37325;&#35201;&#38454;&#27573;&#12290;&#32437;&#21521;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#36890;&#36807;&#25429;&#25417;&#33041;&#32467;&#26500;&#30340;&#21457;&#32946;&#36712;&#36857;&#26469;&#25506;&#31350;&#36825;&#20010;&#20851;&#38190;&#38454;&#27573;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21463;&#35797;&#32773;&#36864;&#20986;&#21644;&#25195;&#25551;&#22833;&#36133;&#65292;&#32437;&#21521;MRI&#37319;&#38598;&#24635;&#20250;&#36935;&#21040;&#20005;&#37325;&#30340;&#25968;&#25454;&#32570;&#22833;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#32437;&#21521;&#23156;&#20799;&#33041;&#22270;&#26500;&#24314;&#21644;&#21457;&#32946;&#36712;&#36857;&#25551;&#32472;&#21464;&#24471;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24863;&#35874;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#31070;&#32463;&#24433;&#20687;&#34917;&#20840;&#24050;&#25104;&#20026;&#19968;&#31181;&#20445;&#30041;&#23613;&#21487;&#33021;&#22810;&#21487;&#29992;&#25968;&#25454;&#30340;&#24378;&#22823;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22270;&#20687;&#34917;&#20840;&#26041;&#27861;&#36890;&#24120;&#22312;&#26102;&#38388;&#32500;&#24230;&#20869;&#30340;&#27599;&#20010;&#20010;&#20307;&#20027;&#20307;&#20869;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#24433;&#21709;&#25972;&#20307;&#36136;&#37327;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#32423;&#32852;&#25193;&#25955;&#27169;&#22411;Cas-DiffCom&#65292;&#29992;&#20110;&#23494;&#38598;&#21644;&#32437;&#21521;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13776v1 Announce Type: cross  Abstract: Early infancy is a rapid and dynamic neurodevelopmental period for behavior and neurocognition. Longitudinal magnetic resonance imaging (MRI) is an effective tool to investigate such a crucial stage by capturing the developmental trajectories of the brain structures. However, longitudinal MRI acquisition always meets a serious data-missing problem due to participant dropout and failed scans, making longitudinal infant brain atlas construction and developmental trajectory delineation quite challenging. Thanks to the development of an AI-based generative model, neuroimage completion has become a powerful technique to retain as much available data as possible. However, current image completion methods usually suffer from inconsistency within each individual subject in the time dimension, compromising the overall quality. To solve this problem, our paper proposed a two-stage cascaded diffusion model, Cas-DiffCom, for dense and longitudinal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Concrete&#20998;&#24067;&#20316;&#20026;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#27010;&#29575;&#27169;&#22411;&#30340;&#20445;&#25345;&#31934;&#24230;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19978;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#20855;&#26377;&#26368;&#20248;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26679;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13765</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#32479;&#35745;&#24314;&#27169;&#23454;&#29616;&#20445;&#25345;&#31934;&#24230;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Concrete&#20998;&#24067;&#20316;&#20026;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#27010;&#29575;&#27169;&#22411;&#30340;&#20445;&#25345;&#31934;&#24230;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19978;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#20855;&#26377;&#26368;&#20248;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26679;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#20998;&#31867;&#27169;&#22411;&#24517;&#39035;&#36827;&#34892;&#26657;&#20934;&#65292;&#20197;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#26657;&#20934;&#26041;&#27861;&#37319;&#29992;&#20102;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26657;&#20934;&#26041;&#27861;&#26080;&#27861;&#20445;&#25345;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#24456;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Concrete&#20998;&#24067;&#20316;&#20026;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#27010;&#29575;&#27169;&#22411;&#30340;&#20445;&#25345;&#31934;&#24230;&#30340;&#26657;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19978;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#20855;&#26377;Concrete&#20998;&#24067;&#21442;&#25968;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#25104;&#29983;&#25104;&#26679;&#26412;&#65292;&#29992;&#20110;&#22312;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#35757;&#32451;&#27010;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#20445;&#25345;&#26657;&#20934;&#20219;&#21153;&#19978;&#21487;&#20197;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13765v1 Announce Type: new  Abstract: Classification models based on deep neural networks (DNNs) must be calibrated to measure the reliability of predictions. Some recent calibration methods have employed a probabilistic model on the probability simplex. However, these calibration methods cannot preserve the accuracy of pre-trained models, even those with a high classification accuracy. We propose an accuracy-preserving calibration method using the Concrete distribution as the probabilistic model on the probability simplex. We theoretically prove that a DNN model trained on cross-entropy loss has optimality as the parameter of the Concrete distribution. We also propose an efficient method that synthetically generates samples for training probabilistic models on the probability simplex. We demonstrate that the proposed method can outperform previous methods in accuracy-preserving calibration tasks using benchmarks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13754</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-assisted quantum architecture search for variational quantum algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13754
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#30830;&#23450;&#21151;&#33021;&#24615;&#37327;&#23376;&#30005;&#36335;&#12290;&#36825;&#20123;&#30005;&#36335;&#24517;&#39035;&#21516;&#26102;&#31526;&#21512;&#24403;&#21069;&#37327;&#23376;&#30828;&#20214;&#38480;&#21046;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#12290;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQA&#65289;&#26159;&#19968;&#31867;&#37327;&#23376;-&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#21487;&#29992;&#37327;&#23376;&#35774;&#22791;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#20391;&#37325;&#20110;&#30005;&#36335;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20248;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;&#35770;&#25991;&#20869;&#36890;&#36807;&#35780;&#20272;&#30005;&#36335;&#30340;&#28145;&#24230;&#12289;&#38376;&#21644;&#21442;&#25968;&#30340;&#24635;&#25968;&#20197;&#21450;&#20934;&#30830;&#24615;&#26469;&#30830;&#23450;&#30005;&#36335;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13754v1 Announce Type: cross  Abstract: A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23457;&#26597;&#20102;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#29983;&#20135;&#32773;&#20860;&#28040;&#36153;&#32773;&#31038;&#21306;&#30340;&#30005;&#21147;&#36127;&#36733;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#21644;&#21487;&#34892;&#24615;</title><link>https://arxiv.org/abs/2402.13752</link><description>&lt;p&gt;
AI-Powered Predictions for Electricity Load in Prosumer Communities
&lt;/p&gt;
&lt;p&gt;
AI-Powered Predictions for Electricity Load in Prosumer Communities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23457;&#26597;&#20102;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#29983;&#20135;&#32773;&#20860;&#28040;&#36153;&#32773;&#31038;&#21306;&#30340;&#30005;&#21147;&#36127;&#36733;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#21644;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20303;&#23429;&#27004;&#23431;&#31038;&#21306;&#30340;&#30005;&#21147;&#28040;&#32791;&#21644;&#20135;&#20986;&#28789;&#27963;&#24615;&#65292;&#21253;&#25324;&#20855;&#26377;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#33021;&#28304;&#20648;&#23384;&#35774;&#26045;&#65288;&#20063;&#31216;&#20026;&#29983;&#20135;&#32773;&#20860;&#28040;&#36153;&#32773;&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#20808;&#36827;&#30340;&#30701;&#26399;&#38656;&#27714;&#21709;&#24212;&#26426;&#21046;&#24471;&#21040;&#26377;&#25928;&#21033;&#29992;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#22914;&#26524;&#22312;&#29983;&#20135;&#32773;&#20860;&#28040;&#36153;&#32773;&#31038;&#21306;&#23618;&#38754;&#36827;&#34892;&#38656;&#27714;&#21709;&#24212;&#65292;&#28789;&#27963;&#24615;&#23601;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#65292;&#22240;&#20026;&#32858;&#21512;&#30340;&#32676;&#20307;&#21487;&#20197;&#26356;&#22909;&#22320;&#21327;&#35843;&#30005;&#21147;&#28040;&#36153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30701;&#26399;&#20248;&#21270;&#30340;&#25928;&#26524;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23545;&#27599;&#26635;&#24314;&#31569;&#29289;&#20197;&#21450;&#25972;&#20010;&#31038;&#21306;&#30340;&#30005;&#21147;&#36127;&#36733;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#30005;&#21147;&#36127;&#36733;&#26354;&#32447;&#20013;&#30340;&#32467;&#26500;&#21464;&#21270;&#21487;&#33021;&#19982;&#19981;&#21516;&#30340;&#22806;&#37096;&#22240;&#32032;&#30456;&#20851;&#32852;&#65292;&#20363;&#22914;&#22825;&#27668;&#26465;&#20214;&#12289;&#26085;&#21382;&#20449;&#24687;&#12289;&#26143;&#26399;&#20960;&#20197;&#21450;&#29992;&#25143;&#34892;&#20026;&#12290;&#26412;&#25991;&#23457;&#26597;&#20102;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#30005;&#21147;&#36127;&#36733;&#39044;&#27979;&#25216;&#26415;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13752v1 Announce Type: cross  Abstract: The flexibility in electricity consumption and production in communities of residential buildings, including those with renewable energy sources and energy storage (a.k.a., prosumers), can effectively be utilized through the advancement of short-term demand response mechanisms. It is known that flexibility can further be increased if demand response is performed at the level of communities of prosumers, since aggregated groups can better coordinate electricity consumption. However, the effectiveness of such short-term optimization is highly dependent on the accuracy of electricity load forecasts both for each building as well as for the whole community. Structural variations in the electricity load profile can be associated with different exogenous factors, such as weather conditions, calendar information and day of the week, as well as user behavior. In this paper, we review a wide range of electricity load forecasting techniques, tha
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;(NAR)&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#23558;&#31639;&#27861;&#30340;&#32467;&#26500;&#21270;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.13744</link><description>&lt;p&gt;
&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#31639;&#27861;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reasoning Algorithmically in Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13744
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;(NAR)&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#23558;&#31639;&#27861;&#30340;&#32467;&#26500;&#21270;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#20855;&#26377;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#30340;&#21457;&#23637;&#20195;&#34920;&#30528;&#19968;&#20010;&#38271;&#26399;&#32780;&#25345;&#20037;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20256;&#32479;&#19978;&#65292;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#20027;&#35201;&#31574;&#30053;&#28041;&#21450;&#37319;&#29992;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#31526;&#21495;&#26126;&#30830;&#22320;&#34920;&#31034;&#30693;&#35782;&#24182;&#36890;&#36807;&#26126;&#30830;&#32534;&#31243;&#35268;&#21017;&#36827;&#34892;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#31995;&#32479;&#20986;&#29616;&#20102;&#20174;&#25968;&#25454;&#20013;&#33258;&#20027;&#23398;&#20064;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#38656;&#35201;&#26368;&#23569;&#30340;&#20154;&#31867;&#25351;&#23548;&#12290;&#32771;&#34385;&#21040;&#36825;&#31181;&#36716;&#21464;&#65292;&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#23545;&#36171;&#20104;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#33021;&#21147;&#30340;&#30740;&#31350;&#24863;&#20852;&#36259;&#65292;&#24182;&#23558;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#19982;&#36923;&#36753;&#25512;&#29702;&#32039;&#23494;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#31070;&#32463;&#31639;&#27861;&#25512;&#29702;&#65288;NAR&#65289;&#20316;&#20026;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#39046;&#22495;&#33073;&#39062;&#32780;&#20986;&#65292;&#26088;&#22312;&#23558;&#31639;&#27861;&#30340;&#32467;&#26500;&#21270;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13744v1 Announce Type: new  Abstract: The development of artificial intelligence systems with advanced reasoning capabilities represents a persistent and long-standing research question. Traditionally, the primary strategy to address this challenge involved the adoption of symbolic approaches, where knowledge was explicitly represented by means of symbols and explicitly programmed rules. However, with the advent of machine learning, there has been a paradigm shift towards systems that can autonomously learn from data, requiring minimal human guidance. In light of this shift, in latest years, there has been increasing interest and efforts at endowing neural networks with the ability to reason, bridging the gap between data-driven learning and logical reasoning. Within this context, Neural Algorithmic Reasoning (NAR) stands out as a promising research field, aiming to integrate the structured and rule-based reasoning of algorithms with the adaptive learning capabilities of neu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22349;&#22604;&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#65292;&#26435;&#37325;&#30340;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#39640;&#24230;&#30456;&#20851;&#65292;&#23548;&#33268;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#12290;</title><link>https://arxiv.org/abs/2402.13728</link><description>&lt;p&gt;
&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#20316;&#20026;&#28145;&#24230;&#31070;&#32463;&#22349;&#22604;&#26426;&#21046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Average gradient outer product as a mechanism for deep neural collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#22349;&#22604;&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#65292;&#26435;&#37325;&#30340;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#39640;&#24230;&#30456;&#20851;&#65292;&#23548;&#33268;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deep Neural Collapse (DNC)&#25351;&#30340;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26368;&#21518;&#20960;&#23618;&#25968;&#25454;&#34920;&#31034;&#30340;&#24778;&#20154;&#21018;&#24615;&#32467;&#26500;&#12290;&#23613;&#31649;&#36825;&#31181;&#29616;&#35937;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#37117;&#24471;&#21040;&#20102;&#27979;&#37327;&#65292;&#20294;&#20854;&#20986;&#29616;&#21482;&#26377;&#37096;&#20998;&#34987;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20805;&#20998;&#35777;&#25454;&#65292;&#34920;&#26126;DNC&#20027;&#35201;&#26159;&#36890;&#36807;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;(AGOP)&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#32780;&#21457;&#29983;&#30340;&#12290;&#30456;&#27604;&#20110;&#35299;&#37322;&#31070;&#32463;&#22349;&#22604;&#30340;&#29305;&#24449;&#19981;&#21487;&#30693;&#26041;&#27861;&#65292;&#22914;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#65292;&#36825;&#19968;&#36827;&#23637;&#26356;&#36827;&#19968;&#27493;&#12290;&#25105;&#20204;&#32487;&#32493;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#26435;&#37325;&#30340;&#21491;&#22855;&#24322;&#21521;&#37327;&#21644;&#22855;&#24322;&#20540;&#26159;DNN&#20013;&#31867;&#20869;&#21464;&#24322;&#22349;&#22604;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#27491;&#22914;&#26368;&#36817;&#30340;&#30740;&#31350;&#25152;&#31034;&#65292;&#36825;&#31181;&#22855;&#24322;&#32467;&#26500;&#19982;AGOP&#30340;&#39640;&#24230;&#30456;&#20851;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#23454;&#39564;&#21644;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;AGOP&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#21457;&#31070;&#32463;&#22349;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13728v1 Announce Type: new  Abstract: Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs). Though the phenomenon has been measured in a wide variety of settings, its emergence is only partially understood. In this work, we provide substantial evidence that DNC formation occurs primarily through deep feature learning with the average gradient outer product (AGOP). This takes a step further compared to efforts that explain neural collapse via feature-agnostic approaches, such as the unconstrained features model. We proceed by providing evidence that the right singular vectors and values of the weights are responsible for the majority of within-class variability collapse in DNNs. As shown in recent work, this singular structure is highly correlated with that of the AGOP. We then establish experimentally and theoretically that AGOP induces neural collapse in a randomly initialized ne
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19982;Fenchel-Young&#25439;&#22833;&#24314;&#31435;&#38142;&#25509;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#31232;&#30095;&#36716;&#25442;&#65292;&#25581;&#31034;&#20102;&#25439;&#22833;&#36793;&#30028;&#12289;&#31232;&#30095;&#24615;&#21644;&#31934;&#30830;&#23384;&#20648;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#21516;&#26102;&#36890;&#36807;SparseMAP&#36716;&#25442;&#23558;&#26694;&#26550;&#25193;&#23637;&#21040;&#32467;&#26500;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.13725</link><description>&lt;p&gt;
&#31232;&#30095;&#32467;&#26500;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sparse and Structured Hopfield Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13725
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;Fenchel-Young&#25439;&#22833;&#24314;&#31435;&#38142;&#25509;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#31232;&#30095;&#36716;&#25442;&#65292;&#25581;&#31034;&#20102;&#25439;&#22833;&#36793;&#30028;&#12289;&#31232;&#30095;&#24615;&#21644;&#31934;&#30830;&#23384;&#20648;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#21516;&#26102;&#36890;&#36807;SparseMAP&#36716;&#25442;&#23558;&#26694;&#26550;&#25193;&#23637;&#21040;&#32467;&#26500;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#30001;&#20110;&#20854;&#19982;&#21464;&#21387;&#22120;&#20013;&#30340;&#27880;&#24847;&#21147;&#30340;&#32852;&#31995;&#65292;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#36890;&#36807;&#19982;Fenchel-Young&#25439;&#22833;&#24314;&#31435;&#32852;&#31995;&#65292;&#20026;&#31232;&#30095;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;&#32467;&#26524;&#26159;&#19968;&#31867;&#26032;&#30340;&#38669;&#26222;&#33778;&#23572;&#24503;-Fenchel-Young&#33021;&#37327;&#65292;&#20854;&#26356;&#26032;&#35268;&#21017;&#26159;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#30340;&#31232;&#30095;&#36716;&#25442;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#25439;&#22833;&#36793;&#30028;&#12289;&#31232;&#30095;&#24615;&#21644;&#31934;&#30830;&#23384;&#20648;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;SparseMAP&#36716;&#25442;&#23558;&#36825;&#19968;&#26694;&#26550;&#25193;&#23637;&#21040;&#32467;&#26500;&#21270;&#38669;&#26222;&#33778;&#23572;&#24503;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#26816;&#32034;&#27169;&#24335;&#20851;&#32852;&#32780;&#19981;&#26159;&#21333;&#20010;&#27169;&#24335;&#12290;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#21644;&#25991;&#26412;&#29702;&#24615;&#21270;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13725v1 Announce Type: new  Abstract: Modern Hopfield networks have enjoyed recent interest due to their connection to attention in transformers. Our paper provides a unified framework for sparse Hopfield networks by establishing a link with Fenchel-Young losses. The result is a new family of Hopfield-Fenchel-Young energies whose update rules are end-to-end differentiable sparse transformations. We reveal a connection between loss margins, sparsity, and exact memory retrieval. We further extend this framework to structured Hopfield networks via the SparseMAP transformation, which can retrieve pattern associations instead of a single pattern. Experiments on multiple instance learning and text rationalization demonstrate the usefulness of our approach.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#25209;&#22788;&#29702;&#22823;&#23567;&#23545;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#36739;&#22823;&#30340;&#25209;&#27425;&#22823;&#23567;&#26377;&#21161;&#20110;&#26356;&#22909;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#23384;&#22312;&#31283;&#23450;&#24615;&#19979;&#38480;&#21644;&#26377;&#25928;&#24615;&#19978;&#38480;&#65292;&#27169;&#22411;&#36136;&#37327;&#21462;&#20915;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30475;&#21040;&#30340;&#35821;&#38899;&#25968;&#25454;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.13723</link><description>&lt;p&gt;
&#25209;&#22823;&#23567;&#23545;&#27604;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Batch Size on Contrastive Self-Supervised Speech Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13723
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#25209;&#22788;&#29702;&#22823;&#23567;&#23545;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#26412;&#30740;&#31350;&#34920;&#26126;&#36739;&#22823;&#30340;&#25209;&#27425;&#22823;&#23567;&#26377;&#21161;&#20110;&#26356;&#22909;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#23384;&#22312;&#31283;&#23450;&#24615;&#19979;&#38480;&#21644;&#26377;&#25928;&#24615;&#19978;&#38480;&#65292;&#27169;&#22411;&#36136;&#37327;&#21462;&#20915;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30475;&#21040;&#30340;&#35821;&#38899;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#20013;&#65292;&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22810;&#20010;GPU&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#38544;&#21547;&#22320;&#23548;&#33268;&#20102;&#36739;&#22823;&#30340;&#26377;&#25928;&#25209;&#22788;&#29702;&#22823;&#23567;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#22788;&#29702;&#22823;&#23567;&#23545;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#65292;&#26080;&#35770;&#26159;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#30417;&#35270;&#30340;&#32479;&#35745;&#20449;&#24687;&#26041;&#38754;&#65292;&#36824;&#26159;&#23545;&#19979;&#28216;&#24494;&#35843;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20351;&#29992;&#20174;87.5&#31186;&#21040;80&#20998;&#38047;&#30340;&#35821;&#38899;&#19981;&#21516;&#25209;&#27425;&#22823;&#23567;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#30456;&#21516;&#36845;&#20195;&#27425;&#25968;&#65292;&#36739;&#22823;&#30340;&#25209;&#27425;&#22823;&#23567;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#31283;&#23450;&#24615;&#23384;&#22312;&#19979;&#38480;&#65292;&#26377;&#25928;&#24615;&#23384;&#22312;&#19978;&#38480;&#12290;&#28982;&#21518;&#25105;&#20204;&#25351;&#20986;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36136;&#37327;&#20027;&#35201;&#21462;&#20915;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30475;&#21040;&#30340;&#35821;&#38899;&#25968;&#25454;&#37327;&#65292;&#21363;&#25209;&#22788;&#29702;&#22823;&#23567;&#19982;&#36845;&#20195;&#27425;&#25968;&#30340;&#20056;&#31215;&#12290;&#25152;&#26377;&#32467;&#26524;&#22343;&#36890;&#36807;&#29420;&#31435;&#23454;&#29616;&#30340;wav2vec 2.0&#26550;&#26500;&#29983;&#25104;&#65292;&#35813;&#26550;&#26500;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#22797;&#29616;&#20102;&#21407;&#22987;&#20316;&#21697;&#30340;&#32467;&#26524;(arXiv:2006.11477)&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13723v1 Announce Type: cross  Abstract: Foundation models in speech are often trained using many GPUs, which implicitly leads to large effective batch sizes. In this paper we study the effect of batch size on pre-training, both in terms of statistics that can be monitored during training, and in the effect on the performance of a downstream fine-tuning task. By using batch sizes varying from 87.5 seconds to 80 minutes of speech we show that, for a fixed amount of iterations, larger batch sizes result in better pre-trained models. However, there is lower limit for stability, and an upper limit for effectiveness. We then show that the quality of the pre-trained model depends mainly on the amount of speech data seen during training, i.e., on the product of batch size and number of iterations. All results are produced with an independent implementation of the wav2vec 2.0 architecture, which to a large extent reproduces the results of the original work (arXiv:2006.11477). Our ext
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#30740;&#31350;&#20013;&#23637;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#21487;&#25104;&#21151;&#22788;&#29702;&#22810;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#20294;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13714</link><description>&lt;p&gt;
&#29983;&#29289;&#20449;&#24687;&#23398;&#30740;&#31350;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Evaluation of Large Language Models in Bioinformatics Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13714
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#30740;&#31350;&#20013;&#23637;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#21487;&#25104;&#21151;&#22788;&#29702;&#22810;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#20294;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#23427;&#20204;&#22312;&#25991;&#26412;&#23436;&#25104;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#24320;&#21019;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#35299;&#20915;&#38382;&#39064;&#30340;&#35821;&#35328;&#30028;&#38754;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#28508;&#21147;&#21644;&#21151;&#25928;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#21508;&#31181;&#20851;&#38190;&#29983;&#29289;&#20449;&#24687;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#28508;&#22312;&#32534;&#30721;&#21306;&#22495;&#30340;&#35782;&#21035;&#65292;&#22522;&#22240;&#21644;&#34507;&#30333;&#36136;&#30340;&#21629;&#21517;&#23454;&#20307;&#25552;&#21462;&#65292;&#25239;&#24494;&#29983;&#29289;&#21644;&#25239;&#30284;&#32957;&#30340;&#26816;&#27979;&#65292;&#20998;&#23376;&#20248;&#21270;&#20197;&#21450;&#35299;&#20915;&#25945;&#32946;&#24615;&#29983;&#29289;&#20449;&#24687;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#32473;&#23450;&#36866;&#24403;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#20687;GPT&#21464;&#31181;&#36825;&#26679;&#30340;LLMs&#21487;&#20197;&#25104;&#21151;&#22788;&#29702;&#22823;&#22810;&#25968;&#36825;&#20123;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#22312;&#22797;&#26434;&#29983;&#29289;&#20449;&#24687;&#23398;&#20219;&#21153;&#32972;&#26223;&#19979;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#12290;&#22312;&#32467;&#35770;&#37096;&#20998;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13714v1 Announce Type: cross  Abstract: Large language models (LLMs) such as ChatGPT have gained considerable interest across diverse research communities. Their notable ability for text completion and generation has inaugurated a novel paradigm for language-interfaced problem solving. However, the potential and efficacy of these models in bioinformatics remain incompletely explored. In this work, we study the performance LLMs on a wide spectrum of crucial bioinformatics tasks. These tasks include the identification of potential coding regions, extraction of named entities for genes and proteins, detection of antimicrobial and anti-cancer peptides, molecular optimization, and resolution of educational bioinformatics problems. Our findings indicate that, given appropriate prompts, LLMs like GPT variants can successfully handle most of these tasks. In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks. In conclusion
&lt;/p&gt;</description></item><item><title>DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13711</link><description>&lt;p&gt;
DSLR&#65306;&#22810;&#26679;&#24615;&#22686;&#24378;&#21644;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13711
&lt;/p&gt;
&lt;p&gt;
DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20013;&#22238;&#25918;&#32531;&#20914;&#21306;&#23545;&#22270;&#25345;&#32493;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#25773;&#30340;GCL&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#33410;&#28857;&#24182;&#23558;&#23427;&#20204;&#23384;&#20648;&#22312;&#37325;&#25773;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#20379;&#22312;&#35757;&#32451;&#21518;&#32493;&#20219;&#21153;&#26102;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#32771;&#34385;&#27599;&#20010;&#22238;&#25918;&#33410;&#28857;&#30340;&#31867;&#21035;&#20195;&#34920;&#24615;&#20250;&#20351;&#22238;&#25918;&#33410;&#28857;&#38598;&#20013;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#20013;&#24515;&#21608;&#22260;&#65292;&#21487;&#33021;&#23384;&#22312;&#36807;&#25311;&#21512;&#20110;&#20301;&#20110;&#37027;&#20123;&#21306;&#22495;&#30340;&#33410;&#28857;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#21152;&#21095;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#23569;&#25968;&#22238;&#25918;&#33410;&#28857;&#26469;&#20445;&#30041;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20855;&#26377;&#19981;&#30456;&#20851;&#37051;&#23621;&#30340;&#22238;&#25918;&#33410;&#28857;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#26174;&#30528;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSLR&#30340;GCL&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#65288;CD&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13711v1 Announce Type: cross  Abstract: We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD)
&lt;/p&gt;</description></item><item><title>&#22312;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#24335;&#35268;&#33539;&#20102;&#31283;&#20581;&#32858;&#21512;&#22120;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#31283;&#20581;&#32858;&#21512;&#22120;&#26080;&#27861;&#23454;&#29616;&#20854;&#30446;&#26631;&#65292;&#35201;&#20040;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#26377;&#38024;&#23545;&#24615;&#30340;&#24694;&#24847;&#26356;&#26032;&#65292;&#35201;&#20040;&#26041;&#27861;&#25104;&#21151;&#29575;&#19981;&#22815;&#12290;</title><link>https://arxiv.org/abs/2402.13700</link><description>&lt;p&gt;
&#22312;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#20013;&#31283;&#20581;&#24615;&#21644;&#23398;&#20064;&#30340;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
On the Conflict of Robustness and Learning in Collaborative Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13700
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#24335;&#35268;&#33539;&#20102;&#31283;&#20581;&#32858;&#21512;&#22120;&#30340;&#39046;&#22495;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#31283;&#20581;&#32858;&#21512;&#22120;&#26080;&#27861;&#23454;&#29616;&#20854;&#30446;&#26631;&#65292;&#35201;&#20040;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#26377;&#38024;&#23545;&#24615;&#30340;&#24694;&#24847;&#26356;&#26032;&#65292;&#35201;&#20040;&#26041;&#27861;&#25104;&#21151;&#29575;&#19981;&#22815;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#65288;CML&#65289;&#20801;&#35768;&#21442;&#19982;&#32773;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#20182;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#12290;&#22312;&#38544;&#31169;&#26159;&#19968;&#20010;&#24378;&#28872;&#35201;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#22914;&#20581;&#24247;&#30456;&#20851;&#24212;&#29992;&#20013;&#65292;&#23433;&#20840;&#20063;&#26159;&#39318;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#36825;&#24847;&#21619;&#30528;&#20445;&#25252;&#38544;&#31169;&#30340;CML&#27969;&#31243;&#24517;&#39035;&#20135;&#29983;&#33021;&#22815;&#36755;&#20986;&#27491;&#30830;&#21487;&#38752;&#20915;&#31574;&#30340;&#27169;&#22411;&#65292;&#29978;&#33267;&#22312;&#21487;&#33021;&#19981;&#21463;&#20449;&#20219;&#21442;&#19982;&#32773;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20381;&#36182;&#20110;&#24110;&#21161;&#36807;&#28388;&#21487;&#33021;&#21361;&#21450;&#35757;&#32451;&#36807;&#31243;&#30340;&#24694;&#24847;&#36129;&#29486;&#30340;&#24230;&#37327;&#30340;&#8220;&#31283;&#20581;&#32858;&#21512;&#22120;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#25991;&#29486;&#20013;&#35268;&#33539;&#21270;&#20102;&#31283;&#20581;&#32858;&#21512;&#22120;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#30340;&#35268;&#33539;&#21270;&#33021;&#22815;&#34920;&#26126;&#29616;&#26377;&#30340;&#31283;&#20581;&#32858;&#21512;&#22120;&#26080;&#27861;&#23454;&#29616;&#20854;&#30446;&#26631;&#65306;&#26080;&#35770;&#26159;&#23427;&#20204;&#20351;&#29992;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#26377;&#38024;&#23545;&#24615;&#30340;&#24694;&#24847;&#26356;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#65307;&#36824;&#26159;&#25552;&#20986;&#30340;&#26041;&#27861;&#25104;&#21151;&#29575;&#19981;&#22815;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13700v1 Announce Type: new  Abstract: Collaborative Machine Learning (CML) allows participants to jointly train a machine learning model while keeping their training data private. In scenarios where privacy is a strong requirement, such as health-related applications, safety is also a primary concern. This means that privacy-preserving CML processes must produce models that output correct and reliable decisions \emph{even in the presence of potentially untrusted participants}. In response to this issue, researchers propose to use \textit{robust aggregators} that rely on metrics which help filter out malicious contributions that could compromise the training process. In this work, we formalize the landscape of robust aggregators in the literature. Our formalization allows us to show that existing robust aggregators cannot fulfill their goal: either they use distance-based metrics that cannot accurately identify targeted malicious updates; or propose methods whose success is i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13699</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#27979;&#37327;&#20998;&#31867;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Explainable Classification Techniques for Quantum Dot Device Measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#31185;&#23398;&#20013;&#65292;&#23545;&#22270;&#20687;&#25968;&#25454;&#30340;&#31283;&#20581;&#29305;&#24449;&#34920;&#31034;&#38656;&#27714;&#22686;&#21152;&#65306;&#22270;&#20687;&#37319;&#38598;&#65292;&#22312;&#24191;&#20041;&#19978;&#25351;&#20108;&#32500;&#25968;&#25454;&#65292;&#29616;&#22312;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;&#27492;&#32771;&#34385;&#30340;&#37327;&#23376;&#20449;&#24687;&#31185;&#23398;&#12290;&#34429;&#28982;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#24191;&#27867;&#20351;&#29992;&#20256;&#32479;&#22270;&#20687;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#30340;&#20351;&#29992;&#27491;&#22312;&#36805;&#36895;&#34987;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#25152;&#21462;&#20195;&#65292;&#21518;&#32773;&#24448;&#24448;&#20197;&#29306;&#29298;&#21487;&#35299;&#37322;&#24615;&#20026;&#20195;&#20215;&#25442;&#21462;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23637;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#21331;&#36234;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#31181;&#25216;&#26415;&#24102;&#26469;&#20102;&#23454;&#36136;&#24615;&#30340;&#30410;&#22788;&#65292;&#24403;&#21069;&#21457;&#23637;&#38454;&#27573;&#38656;&#35201;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13699v1 Announce Type: cross  Abstract: In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#36807;&#22659;&#31995;&#22806;&#34892;&#26143;&#36827;&#34892;&#21442;&#25968;&#34920;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#20998;&#26512;&#36807;&#22659;&#31995;&#22806;&#34892;&#26143;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.13673</link><description>&lt;p&gt;
&#20351;&#29992;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#36807;&#22659;&#31995;&#22806;&#34892;&#26143;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Computing Transiting Exoplanet Parameters with 1D Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13673
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#36807;&#22659;&#31995;&#22806;&#34892;&#26143;&#36827;&#34892;&#21442;&#25968;&#34920;&#24449;&#65292;&#33021;&#22815;&#26377;&#25928;&#38477;&#20302;&#20998;&#26512;&#36807;&#22659;&#31995;&#22806;&#34892;&#26143;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20940;&#26085;&#26041;&#27861;&#20801;&#35768;&#36890;&#36807;&#20998;&#26512;&#24658;&#26143;&#20809;&#21464;&#26354;&#32447;&#26469;&#26816;&#27979;&#21644;&#34920;&#24449;&#34892;&#26143;&#31995;&#32479;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20284;&#20046;&#25552;&#20379;&#20102;&#33258;&#21160;&#21270;&#36825;&#20123;&#20998;&#26512;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20351;&#29992;&#27169;&#25311;&#30340;&#20809;&#21464;&#26354;&#32447;&#65288;&#20854;&#20013;&#27880;&#20837;&#20102;&#31867;&#20284;&#20110;&#36807;&#22659;&#30340;&#20449;&#21495;&#65289;&#30340;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#19968;&#20010;&#27169;&#22411;&#22312;&#23436;&#25972;&#30340;&#20809;&#21464;&#26354;&#32447;&#19978;&#25805;&#20316;&#24182;&#20272;&#35745;&#36712;&#36947;&#21608;&#26399;&#65292;&#21478;&#19968;&#20010;&#27169;&#22411;&#22312;&#25240;&#21472;&#30456;&#20301;&#30340;&#20809;&#21464;&#26354;&#32447;&#19978;&#25805;&#20316;&#24182;&#20272;&#35745;&#36712;&#36947;&#30340;&#21322;&#38271;&#36724;&#21644;&#34892;&#26143;&#19982;&#24658;&#26143;&#21322;&#24452;&#27604;&#30340;&#24179;&#26041;&#12290;&#36825;&#20004;&#20010;&#27169;&#22411;&#37117;&#32463;&#36807;&#20102;&#26469;&#33258;&#20855;&#26377;&#30830;&#35748;&#34892;&#26143;&#30340;TESS&#20809;&#21464;&#26354;&#32447;&#30340;&#30495;&#23454;&#25968;&#25454;&#27979;&#35797;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#33021;&#22815;&#22788;&#29702;&#30495;&#23454;&#25968;&#25454;&#12290;&#24471;&#21040;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20174;&#23487;&#20027;&#24658;&#26143;&#30340;&#21435;&#36235;&#21183;&#21270;&#20809;&#21464;&#26354;&#32447;&#20013;&#34920;&#24449;&#36807;&#22659;&#31995;&#22806;&#34892;&#26143;&#65292;&#24182;&#19988;&#36824;&#20943;&#23569;&#20102;&#25152;&#38656;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13673v1 Announce Type: cross  Abstract: The transit method allows the detection and characterization of planetary systems by analyzing stellar light curves. Convolutional neural networks appear to offer a viable solution for automating these analyses. In this research, two 1D convolutional neural network models, which work with simulated light curves in which transit-like signals were injected, are presented. One model operates on complete light curves and estimates the orbital period, and the other one operates on phase-folded light curves and estimates the semimajor axis of the orbit and the square of the planet-to-star radius ratio. Both models were tested on real data from TESS light curves with confirmed planets to ensure that they are able to work with real data. The results obtained show that 1D CNNs are able to characterize transiting exoplanets from their host star's detrended light curve and, furthermore, reducing both the required time and computational costs comp
&lt;/p&gt;</description></item><item><title>&#27010;&#29575;&#65288;&#26426;&#22120;&#23398;&#20064;&#65289;&#27169;&#22411;&#25552;&#20379;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#22312;&#36136;&#37327;&#31649;&#29702;&#20013;&#29289;&#29702;&#26816;&#26597;&#30340;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#19982;&#27010;&#29575;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#26410;&#23436;&#20840;&#38416;&#26126;</title><link>https://arxiv.org/abs/2402.13666</link><description>&lt;p&gt;
&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65306;&#20851;&#20110;&#29289;&#29702;&#21644;&#34394;&#25311;&#27979;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Measurement Uncertainty: Relating the uncertainties of physical and virtual measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13666
&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#65288;&#26426;&#22120;&#23398;&#20064;&#65289;&#27169;&#22411;&#25552;&#20379;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#22312;&#36136;&#37327;&#31649;&#29702;&#20013;&#29289;&#29702;&#26816;&#26597;&#30340;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#19982;&#27010;&#29575;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#26410;&#23436;&#20840;&#38416;&#26126;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#22823;&#35268;&#27169;&#29983;&#20135;&#30340;&#20135;&#21697;&#32972;&#26223;&#19979;&#65292;&#36136;&#37327;&#31649;&#29702;&#26159;&#22522;&#20110;&#20174;&#22823;&#25209;&#27425;&#20013;&#23454;&#29289;&#26816;&#26597;&#19968;&#23567;&#37096;&#20998;&#26679;&#26412;&#24182;&#25512;&#29702;&#20986;&#25209;&#27425;&#36136;&#37327;&#31526;&#21512;&#24615;&#12290;&#24403;&#23558;&#29289;&#29702;&#26816;&#26597;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#30456;&#32467;&#21512;&#26102;&#65292;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#24050;&#30693;&#30340;&#33267;&#20851;&#37325;&#35201;&#12290;&#21542;&#21017;&#65292;&#24212;&#29992;&#24314;&#31435;&#30340;&#36136;&#37327;&#31649;&#29702;&#27010;&#24565;&#26159;&#19981;&#21512;&#27861;&#30340;&#12290;&#30830;&#23450;&#24615;&#65288;&#26426;&#22120;&#23398;&#20064;&#65289;&#27169;&#22411;&#32570;&#20047;&#23545;&#20854;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#65292;&#22240;&#27492;&#19981;&#21512;&#36866;&#12290;&#27010;&#29575;&#65288;&#26426;&#22120;&#23398;&#20064;&#65289;&#27169;&#22411;&#25552;&#20379;&#20102;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#36136;&#37327;&#31649;&#29702;&#20013;&#24212;&#29992;&#26102;&#65292;&#29289;&#29702;&#26816;&#26597;&#30340;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#19982;&#27010;&#29575;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#32570;&#20047;&#31616;&#26126;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27010;&#29575;&#65288;&#26426;&#22120;&#23398;&#20064;&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13666v1 Announce Type: cross  Abstract: In the context of industrially mass-manufactured products, quality management is based on physically inspecting a small sample from a large batch and reasoning about the batch's quality conformance. When complementing physical inspections with predictions from machine learning models, it is crucial that the uncertainty of the prediction is known. Otherwise, the application of established quality management concepts is not legitimate. Deterministic (machine learning) models lack quantification of their predictive uncertainty and are therefore unsuitable. Probabilistic (machine learning) models provide a predictive uncertainty along with the prediction. However, a concise relationship is missing between the measurement uncertainty of physical inspections and the predictive uncertainty of probabilistic models in their application in quality management. Here, we show how the predictive uncertainty of probabilistic (machine learning) models
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#24402;&#26641;&#30340;&#31283;&#23450;&#26356;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21644;&#35843;&#25972;&#36229;&#21442;&#25968;&#26469;&#24179;&#34913;&#39044;&#27979;&#24615;&#33021;&#21644;&#32463;&#39564;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13655</link><description>&lt;p&gt;
&#22238;&#24402;&#26641;&#30340;&#31283;&#23450;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Stable Update of Regression Trees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13655
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22238;&#24402;&#26641;&#30340;&#31283;&#23450;&#26356;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21644;&#35843;&#25972;&#36229;&#21442;&#25968;&#26469;&#24179;&#34913;&#39044;&#27979;&#24615;&#33021;&#21644;&#32463;&#39564;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26356;&#26032;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33719;&#21462;&#26032;&#20449;&#24687;&#36890;&#24120;&#20250;&#25913;&#21892;&#23427;&#20204;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20063;&#24076;&#26395;&#36991;&#20813;&#36807;&#22810;&#25913;&#21464;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#23646;&#24615;&#34987;&#31216;&#20026;&#31283;&#23450;&#24615;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#31283;&#23450;&#24615;&#24456;&#37325;&#35201;&#65292;&#35299;&#37322;&#24615;&#20063;&#24456;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22266;&#26377;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#22238;&#24402;&#26641;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#32463;&#39564;&#31283;&#23450;&#24615;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#29992;&#20110;&#26356;&#26032;&#22238;&#24402;&#26641;&#30340;&#31639;&#27861;&#65292;&#20197;&#25552;&#20379;&#24179;&#34913;&#39044;&#27979;&#24615;&#33021;&#21644;&#32463;&#39564;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#26681;&#25454;&#21021;&#22987;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#21152;&#26435;&#12290;&#36890;&#36807;&#36229;&#21442;&#25968;&#21487;&#20197;&#35843;&#25972;&#39044;&#27979;&#24615;&#33021;&#21644;&#32463;&#39564;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36825;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#25439;&#22833;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#25968;&#25454;&#29305;&#24449;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13655v1 Announce Type: new  Abstract: Updating machine learning models with new information usually improves their predictive performance, yet, in many applications, it is also desirable to avoid changing the model predictions too much. This property is called stability. In most cases when stability matters, so does explainability. We therefore focus on the stability of an inherently explainable machine learning method, namely regression trees. We aim to use the notion of empirical stability and design algorithms for updating regression trees that provide a way to balance between predictability and empirical stability. To achieve this, we propose a regularization method, where data points are weighted based on the uncertainty in the initial model. The balance between predictability and empirical stability can be adjusted through hyperparameters. This regularization method is evaluated in terms of loss and stability and assessed on a broad range of data characteristics. The r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#19982;&#24341;&#23548;&#65292;&#32467;&#21512;&#27604;&#20363;&#31215;&#20998;&#65288;PI&#65289;&#25511;&#21046;&#22120;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#30784;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#33410;&#27969;&#38400;&#30340;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#20960;&#20046;&#26368;&#20248;&#30340;&#25511;&#21046;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.13654</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25913;&#36827;&#27604;&#20363;&#31215;&#20998;&#25511;&#21046;&#22120;&#22312;&#33410;&#27969;&#38400;&#22522;&#20934;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13654
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24378;&#21270;&#23398;&#20064;&#19982;&#24341;&#23548;&#65292;&#32467;&#21512;&#27604;&#20363;&#31215;&#20998;&#65288;PI&#65289;&#25511;&#21046;&#22120;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22522;&#30784;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#33410;&#27969;&#38400;&#30340;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#20960;&#20046;&#26368;&#20248;&#30340;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#33410;&#27969;&#38400;&#65292;&#35813;&#33410;&#27969;&#38400;&#20855;&#26377;&#19981;&#23545;&#31216;&#30340;&#30913;&#28382;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#20960;&#20046;&#26368;&#20248;&#30340;&#25511;&#21046;&#22120;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#29615;&#22659;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#31934;&#24515;&#35843;&#25972;&#30340;&#27604;&#20363;&#31215;&#20998;&#65288;PI&#65289;&#25511;&#21046;&#22120;&#24320;&#22987;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#19982;&#24341;&#23548;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36890;&#36807;&#20174;&#19982;&#38400;&#38376;&#30340;&#39069;&#22806;&#20132;&#20114;&#20013;&#23398;&#20064;&#26469;&#25913;&#36827;&#38381;&#29615;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#38400;&#38376;&#19978;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#31361;&#26174;&#20102;&#23558;PI&#21644;RL&#26694;&#26550;&#32467;&#21512;&#20197;&#25552;&#39640;&#38750;&#32447;&#24615;&#38543;&#26426;&#31995;&#32479;&#25511;&#21046;&#24615;&#33021;&#30340;&#22909;&#22788;&#12290;&#22312;&#25152;&#26377;&#23454;&#39564;&#27979;&#35797;&#26696;&#20363;&#20013;&#65292;&#32467;&#26524;&#20195;&#29702;&#30340;&#26679;&#26412;&#25928;&#29575;&#37117;&#20248;&#20110;&#20256;&#32479;RL&#20195;&#29702;&#65292;&#24182;&#19988;&#20248;&#20110;PI&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13654v1 Announce Type: cross  Abstract: This paper presents a learning-based control strategy for non-linear throttle valves with an asymmetric hysteresis, leading to a near-optimal controller without requiring any prior knowledge about the environment. We start with a carefully tuned Proportional Integrator (PI) controller and exploit the recent advances in Reinforcement Learning (RL) with Guides to improve the closed-loop behavior by learning from the additional interactions with the valve. We test the proposed control method in various scenarios on three different valves, all highlighting the benefits of combining both PI and RL frameworks to improve control performance in non-linear stochastic systems. In all the experimental test cases, the resulting agent has a better sample efficiency than traditional RL agents and outperforms the PI controller.
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#34507;&#30333;&#36136;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#25552;&#20379;&#19987;&#38376;&#25968;&#25454;&#38598;&#24182;&#24314;&#31435;&#20102;PQA&#26694;&#26550;&#65292;&#21253;&#21547;&#29983;&#29289;&#30456;&#20851;&#30340;&#31185;&#23398;PQA&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#26550;&#26500;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.13653</link><description>&lt;p&gt;
PQA&#65306;&#38646;&#26679;&#26412;&#34507;&#30333;&#36136;&#38382;&#39064;&#22238;&#31572;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#30001;&#31185;&#23398;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
PQA: Zero-shot Protein Question Answering for Free-form Scientific Enquiry with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13653
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#34507;&#30333;&#36136;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#65292;&#25552;&#20379;&#19987;&#38376;&#25968;&#25454;&#38598;&#24182;&#24314;&#31435;&#20102;PQA&#26694;&#26550;&#65292;&#21253;&#21547;&#29983;&#29289;&#30456;&#20851;&#30340;&#31185;&#23398;PQA&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#26550;&#26500;&#21462;&#24471;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#38646;&#26679;&#26412;&#34507;&#30333;&#36136;&#38382;&#39064;&#22238;&#31572;&#65288;PQA&#65289;&#36825;&#19968;&#26032;&#39062;&#20219;&#21153;&#65292;&#29992;&#20110;&#33258;&#30001;&#24418;&#24335;&#31185;&#23398;&#25506;&#31350;&#12290;&#32473;&#23450;&#19968;&#20010;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#21644;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20219;&#21153;&#26159;&#25552;&#20379;&#19968;&#20010;&#31185;&#23398;&#19978;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#36825;&#19968;&#20219;&#21153;&#19981;&#20165;&#25903;&#25345;&#26410;&#26469;&#30340;&#29983;&#29289;&#30740;&#31350;&#65292;&#36824;&#21487;&#20197;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31185;&#23398;&#31934;&#24230;&#25552;&#20379;&#19968;&#20010;&#27979;&#35797;&#22522;&#20934;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;PQA&#27169;&#22411;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;257K&#20010;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#27880;&#37322;&#26377;1.97M&#20010;&#31185;&#23398;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;&#19982;&#29983;&#29289;&#30456;&#20851;&#30340;&#31185;&#23398;PQA&#22522;&#20934;&#12290;&#36890;&#36807;&#20004;&#31181;&#24378;&#22823;&#30340;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;PQA&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#25581;&#31034;&#20102;&#20851;&#38190;&#24615;&#33021;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;PQA&#26694;&#26550;&#65292;&#21517;&#20026;Pika&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#12289;&#27169;&#22411;&#26816;&#26597;&#28857;&#21644;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13653v1 Announce Type: new  Abstract: We introduce the novel task of zero-shot Protein Question Answering (PQA) for free-form scientific enquiry. Given a previously unseen protein sequence and a natural language question, the task is to deliver a scientifically accurate answer. This task not only supports future biological research, but could also provide a test bed for assessing the scientific precision of large language models (LLMs). We contribute the first specialized dataset for PQA model training, containing 257K protein sequences annotated with 1.97M scientific question-answer pairs. Additionally, we propose and study several novel biologically relevant benchmarks for scientific PQA. Employing two robust multi-modal architectures, we establish an initial state-of-the-art performance for PQA and reveal key performance factors through ablation studies. Our comprehensive PQA framework, named Pika, including dataset, code, model checkpoints, and a user-friendly demo, is o
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#20004;&#31181;&#28145;&#24230;&#21367;&#31215;&#26550;&#26500;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#20013;&#21152;&#20837;&#23545;&#25239;&#26679;&#26412;&#21644;&#26102;&#38388;&#22686;&#24378;&#26679;&#26412;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.13651</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24494;&#22810;&#26222;&#21202;&#38647;&#36798;&#20998;&#31867;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of Deep Neural Networks for Micro-Doppler Radar Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13651
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#20004;&#31181;&#28145;&#24230;&#21367;&#31215;&#26550;&#26500;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#22312;&#35757;&#32451;&#20013;&#21152;&#20837;&#23545;&#25239;&#26679;&#26412;&#21644;&#26102;&#38388;&#22686;&#24378;&#26679;&#26412;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#20998;&#31867;&#22120;&#22312;&#38647;&#36798;&#25968;&#25454;&#22788;&#29702;&#26041;&#38754;&#30340;&#24040;&#22823;&#33021;&#21147;&#65292;&#23398;&#20064;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#32780;&#26080;&#27861;&#24456;&#22909;&#27867;&#21270;&#30340;&#39118;&#38505;&#20063;&#38543;&#20043;&#32780;&#26469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#35780;&#20272;&#20102;&#20004;&#31181;&#28145;&#24230;&#21367;&#31215;&#26550;&#26500;&#22312;&#30456;&#21516;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#24403;&#36981;&#24490;&#26631;&#20934;&#35757;&#32451;&#23454;&#36341;&#26102;&#65292;&#20004;&#20010;&#20998;&#31867;&#22120;&#37117;&#23637;&#29616;&#20986;&#23545;&#36755;&#20837;&#34920;&#31034;&#30340;&#24494;&#23567;&#26102;&#38388;&#20559;&#31227;&#30340;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#22686;&#24378;&#24102;&#26377;&#26368;&#23567;&#30340;&#35821;&#20041;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#23567;&#30340;&#26102;&#38388;&#20559;&#31227;&#21644;&#23545;&#25239;&#26679;&#26412;&#37117;&#26159;&#27169;&#22411;&#36807;&#25311;&#21512;&#20110;&#26080;&#27861;&#24456;&#22909;&#27867;&#21270;&#30340;&#29305;&#24449;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#34920;&#26126;&#22312;&#23545;&#25239;&#26679;&#26412;&#21644;&#26102;&#38388;&#22686;&#24378;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#24433;&#21709;&#65292;&#36827;&#32780;&#23548;&#33268;&#26356;&#22909;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#28436;&#31034;&#20102;&#25805;&#20316;&#22312;&#33410;&#22863;-&#36895;&#24230;&#22270;&#34920;&#31034;&#32780;&#19981;&#26159;&#22810;&#26222;&#21202;-&#26102;&#38388;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13651v1 Announce Type: cross  Abstract: With the great capabilities of deep classifiers for radar data processing come the risks of learning dataset-specific features that do not generalize well. In this work, the robustness of two deep convolutional architectures, trained and tested on the same data, is evaluated. When standard training practice is followed, both classifiers exhibit sensitivity to subtle temporal shifts of the input representation, an augmentation that carries minimal semantic content. Furthermore, the models are extremely susceptible to adversarial examples. Both small temporal shifts and adversarial examples are a result of a model overfitting on features that do not generalize well. As a remedy, it is shown that training on adversarial examples and temporally augmented samples can reduce this effect and lead to models that generalise better. Finally, models operating on cadence-velocity diagram representation rather than Doppler-time are demonstrated to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36827;&#34892;&#20102;&#38024;&#23545;&#19968;&#20010;&#31616;&#21333;&#32780;&#38750;&#24120;&#36890;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#30340;&#22823;&#32500;&#20998;&#26512;&#30740;&#31350;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#28085;&#30422;&#20102;&#22810;&#20219;&#21153;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#30340;&#26631;&#31614;&#65292;&#36890;&#36807;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#24037;&#20855;&#34920;&#24449;&#20102;&#20851;&#38190;&#21151;&#33021;&#30340;&#28176;&#36817;&#24615;&#36136;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#20851;&#20110;&#26377;&#25928;&#20351;&#29992;&#35813;&#27169;&#22411;&#30340;&#21453;&#30452;&#35273;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.13646</link><description>&lt;p&gt;
&#23545;&#22810;&#20219;&#21153;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#22823;&#32500;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Large Dimensional Analysis of Multi-task Semi-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36827;&#34892;&#20102;&#38024;&#23545;&#19968;&#20010;&#31616;&#21333;&#32780;&#38750;&#24120;&#36890;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#30340;&#22823;&#32500;&#20998;&#26512;&#30740;&#31350;&#65292;&#35813;&#27169;&#22411;&#21516;&#26102;&#28085;&#30422;&#20102;&#22810;&#20219;&#21153;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#30340;&#26631;&#31614;&#65292;&#36890;&#36807;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#24037;&#20855;&#34920;&#24449;&#20102;&#20851;&#38190;&#21151;&#33021;&#30340;&#28176;&#36817;&#24615;&#36136;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#20851;&#20110;&#26377;&#25928;&#20351;&#29992;&#35813;&#27169;&#22411;&#30340;&#21453;&#30452;&#35273;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#36890;&#29992;&#30340;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#32500;&#30740;&#31350;&#65292;&#21516;&#26102;&#28085;&#30422;&#20102;&#22810;&#20219;&#21153;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#32771;&#34385;&#20102;&#19981;&#30830;&#23450;&#30340;&#26631;&#31614;&#12290;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#19968;&#20123;&#20851;&#38190;&#21151;&#33021;&#30340;&#28176;&#36817;&#24615;&#36136;&#65292;&#20174;&#32780;&#19968;&#26041;&#38754;&#21487;&#20197;&#39044;&#27979;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21478;&#19968;&#26041;&#38754;&#21487;&#20197;&#25581;&#31034;&#19968;&#20123;&#20851;&#20110;&#22914;&#20309;&#39640;&#25928;&#20351;&#29992;&#23427;&#30340;&#21453;&#30452;&#35273;&#25351;&#23548;&#12290;&#35813;&#27169;&#22411;&#24378;&#22823;&#21040;&#36275;&#20197;&#25552;&#20379;&#33391;&#22909;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#24182;&#19988;&#31616;&#21333;&#30452;&#35266;&#21040;&#36275;&#20197;&#28145;&#20837;&#20102;&#35299;&#20854;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13646v1 Announce Type: cross  Abstract: This article conducts a large dimensional study of a simple yet quite versatile classification model, encompassing at once multi-task and semi-supervised learning, and taking into account uncertain labeling. Using tools from random matrix theory, we characterize the asymptotics of some key functionals, which allows us on the one hand to predict the performances of the algorithm, and on the other hand to reveal some counter-intuitive guidance on how to use it efficiently. The model, powerful enough to provide good performance guarantees, is also straightforward enough to provide strong insights into its behavior.
&lt;/p&gt;</description></item><item><title>FlexHB&#36890;&#36807;&#32454;&#31890;&#24230;&#20445;&#30495;&#24230;&#26041;&#27861;&#25552;&#39640;&#20102;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#30340;&#25928;&#29575;&#65292;&#37325;&#26032;&#35774;&#35745;&#20102;&#26089;&#20572;&#26694;&#26550;&#65292;&#24182;&#32467;&#21512;&#20102;&#36830;&#32493;&#20943;&#21322;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13641</link><description>&lt;p&gt;
FlexHB: &#29992;&#20110;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26356;&#39640;&#25928;&#28789;&#27963;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlexHB: a More Efficient and Flexible Framework for Hyperparameter Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13641
&lt;/p&gt;
&lt;p&gt;
FlexHB&#36890;&#36807;&#32454;&#31890;&#24230;&#20445;&#30495;&#24230;&#26041;&#27861;&#25552;&#39640;&#20102;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#30340;&#25928;&#29575;&#65292;&#37325;&#26032;&#35774;&#35745;&#20102;&#26089;&#20572;&#26694;&#26550;&#65292;&#24182;&#32467;&#21512;&#20102;&#36830;&#32493;&#20943;&#21322;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#38382;&#39064;&#65292;&#22914;&#20309;&#35774;&#35745;&#19968;&#31181;&#31639;&#27861;&#26469;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#20339;&#37197;&#32622;&#65311;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#21644;&#22810;&#20445;&#30495;&#24230;BO&#26041;&#27861;&#21033;&#29992;&#26367;&#20195;&#27169;&#22411;&#26681;&#25454;&#21382;&#21490;&#35780;&#20272;&#26469;&#37319;&#26679;&#37197;&#32622;&#12290;&#26356;&#36817;&#26399;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;BO&#19982;HyperBand&#65288;HB&#65289;&#30456;&#32467;&#21512;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21518;&#32773;&#36890;&#36807;&#25552;&#21069;&#20572;&#27490;&#26426;&#21046;&#21152;&#24555;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#36866;&#24403;&#35780;&#20272;&#26041;&#26696;&#30456;&#23545;&#20110;&#40664;&#35748;HyperBand&#30340;&#20248;&#21183;&#65292;&#32780;&#19988;BO&#30340;&#33021;&#21147;&#20173;&#21463;&#21040;&#20542;&#26012;&#35780;&#20272;&#32467;&#26524;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FlexHB&#65292;&#19968;&#31181;&#23558;&#22810;&#20445;&#30495;&#24230;BO&#25512;&#33267;&#26497;&#38480;&#24182;&#37325;&#26032;&#35774;&#35745;&#26089;&#20572;&#26694;&#26550;&#19982;&#36830;&#32493;&#20943;&#21322;&#65288;SH&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#23545;FlexHB&#30340;&#20840;&#38754;&#30740;&#31350;&#34920;&#26126;&#65288;1&#65289;&#25105;&#20204;&#30340;&#32454;&#31890;&#24230;&#20445;&#30495;&#24230;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#30340;&#25928;&#29575;&#65292;&#65288;2&#65289;&#25105;&#20204;&#30340;FlexBand&#26694;&#26550;&#65288;&#33258;&#36866;&#24212;&#37197;&#32622;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13641v1 Announce Type: new  Abstract: Given a Hyperparameter Optimization(HPO) problem, how to design an algorithm to find optimal configurations efficiently? Bayesian Optimization(BO) and the multi-fidelity BO methods employ surrogate models to sample configurations based on history evaluations. More recent studies obtain better performance by integrating BO with HyperBand(HB), which accelerates evaluation by early stopping mechanism. However, these methods ignore the advantage of a suitable evaluation scheme over the default HyperBand, and the capability of BO is still constrained by skewed evaluation results. In this paper, we propose FlexHB, a new method pushing multi-fidelity BO to the limit as well as re-designing a framework for early stopping with Successive Halving(SH). Comprehensive study on FlexHB shows that (1) our fine-grained fidelity method considerably enhances the efficiency of searching optimal configurations, (2) our FlexBand framework (self-adaptive alloc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30417;&#27979;&#19977;&#31181;&#30693;&#21517;DL&#26694;&#26550;&#20197;&#21450;ONNX&#30340;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#20013;&#30340;&#33021;&#32791;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;DL&#27169;&#22411;&#65292;&#21021;&#27493;&#25506;&#31350;&#20102;&#23427;&#20204;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13640</link><description>&lt;p&gt;
&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;: &#36328;&#19981;&#21516;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#33021;&#32791;&#21021;&#27493;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Green AI: A Preliminary Empirical Study on Energy Consumption in DL Models Across Different Runtime Infrastructures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30417;&#27979;&#19977;&#31181;&#30693;&#21517;DL&#26694;&#26550;&#20197;&#21450;ONNX&#30340;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#20013;&#30340;&#33021;&#32791;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;DL&#27169;&#22411;&#65292;&#21021;&#27493;&#25506;&#31350;&#20102;&#23427;&#20204;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13640v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#23398;&#31185; &#25688;&#35201;: &#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26694;&#26550;&#22914;PyTorch&#21644;TensorFlow&#21253;&#25324;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#65292;&#36127;&#36131;&#22312;&#30446;&#26631;&#30828;&#20214;&#19978;&#25191;&#34892;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#31649;&#29702;&#20869;&#23384;&#12289;&#25968;&#25454;&#20256;&#36755;&#20197;&#21450;&#22810;&#21152;&#36895;&#22120;&#25191;&#34892;&#65288;&#22914;&#26524;&#36866;&#29992;&#65289;&#12290;&#27492;&#22806;&#65292;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#37096;&#32626;&#21040;&#19982;&#20854;&#21407;&#29983;&#24320;&#21457;&#29615;&#22659;&#19981;&#21516;&#30340;&#29615;&#22659;&#26159;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#12290;&#36825;&#23548;&#33268;&#24341;&#20837;&#20102;&#35832;&#22914;ONNX&#20043;&#31867;&#30340;&#20132;&#25442;&#26684;&#24335;&#65292;&#20854;&#20013;&#21253;&#25324;&#20854;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#65292;&#20197;&#21450;ONNX Runtime&#65292;&#21487;&#20316;&#20026;&#21487;&#22312;&#19981;&#21516;DL&#26694;&#26550;&#21644;&#35821;&#35328;&#20043;&#38388;&#20351;&#29992;&#30340;&#26631;&#20934;&#26684;&#24335;&#12290;&#23613;&#31649;&#36825;&#20123;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#23545;&#25512;&#29702;&#24615;&#33021;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#20294;&#20197;&#21069;&#27809;&#26377;&#35770;&#25991;&#35843;&#26597;&#36807;&#23427;&#20204;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30417;&#27979;&#20102;&#19977;&#31181;&#30693;&#21517;DL&#26694;&#26550;&#20197;&#21450;ONNX&#30340;&#36816;&#34892;&#26102;&#22522;&#30784;&#35774;&#26045;&#20013;&#30340;&#33021;&#32791;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;DL&#27169;&#22411;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#35843;&#26597;&#26356;&#21152;&#32454;&#33268;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13640v1 Announce Type: cross  Abstract: Deep Learning (DL) frameworks such as PyTorch and TensorFlow include runtime infrastructures responsible for executing trained models on target hardware, managing memory, data transfers, and multi-accelerator execution, if applicable. Additionally, it is a common practice to deploy pre-trained models on environments distinct from their native development settings. This led to the introduction of interchange formats such as ONNX, which includes its runtime infrastructure, and ONNX Runtime, which work as standard formats that can be used across diverse DL frameworks and languages. Even though these runtime infrastructures have a great impact on inference performance, no previous paper has investigated their energy efficiency. In this study, we monitor the energy consumption and inference time in the runtime infrastructures of three well-known DL frameworks as well as ONNX, using three various DL models. To have nuance in our investigatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#36136;&#37327;&#30340;METRIC&#26694;&#26550;&#65292;&#30528;&#37325;&#25506;&#35752;&#25968;&#25454;&#36136;&#37327;&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24378;&#35843;&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30417;&#31649;&#25209;&#20934;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13635</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#21487;&#20449;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#36136;&#37327;&#30340;METRIC&#26694;&#26550;: &#19968;&#39033;&#31995;&#32479;&#24615;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The METRIC-framework for assessing data quality for trustworthy AI in medicine: a systematic review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#36136;&#37327;&#30340;METRIC&#26694;&#26550;&#65292;&#30528;&#37325;&#25506;&#35752;&#25968;&#25454;&#36136;&#37327;&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24378;&#35843;&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30417;&#31649;&#25209;&#20934;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13635v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;: &#26426;&#22120;&#23398;&#20064;(ML)&#30340;&#37319;&#29992;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#28145;&#24230;&#23398;&#20064;(DL)&#24212;&#29992;&#27491;&#22312;&#34067;&#24310;&#21040;&#25105;&#20204;&#29983;&#27963;&#30340;&#21508;&#20010;&#20027;&#35201;&#39046;&#22495;&#20013;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#21457;&#23637;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#23545;&#24739;&#32773;&#30340;&#29983;&#27963;&#26377;&#30528;&#37325;&#22823;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20449;&#24615;&#28041;&#21450;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#36947;&#24503;&#12289;&#25216;&#26415;&#21644;&#38544;&#31169;&#35201;&#27714;&#65292;&#20294;&#25105;&#20204;&#30528;&#37325;&#20110;DL&#20013;&#25968;&#25454;&#36136;&#37327;(&#35757;&#32451;/&#27979;&#35797;)&#30340;&#37325;&#35201;&#24615;&#12290;&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#20915;&#23450;&#20102;ML&#20135;&#21697;&#30340;&#34892;&#20026;&#65292;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#23558;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30340;&#30417;&#31649;&#25209;&#20934;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#25353;&#29031;PRISMA&#25351;&#21335;&#36827;&#34892;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#20351;&#29992;PubMed&#21644;ACM&#25968;&#23383;&#22270;&#20070;&#39302;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;2362&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;62&#39033;&#31526;&#21512;&#25105;&#20204;&#30340;&#36164;&#26684;&#26631;&#20934;&#12290;&#22312;&#36825;&#19968;&#25991;&#29486;&#20013;&#65292;&#25105;&#20204;&#32508;&#21512;&#29616;&#26377;&#30340;&#20851;&#20110;&#25968;&#25454;&#36136;&#37327;&#26694;&#26550;&#30340;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;ML&#22312;&#21307;&#23398;&#20013;&#30340;&#24212;&#29992;&#35270;&#35282;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24635;&#32467;&#20102;&#21307;&#30103;AI&#39046;&#22495;&#25968;&#25454;&#36136;&#37327;&#26694;&#26550;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#19982;ML&#24212;&#29992;&#30340;&#35270;&#35282;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13635v1 Announce Type: cross  Abstract: The adoption of machine learning (ML) and, more specifically, deep learning (DL) applications into all major areas of our lives is underway. The development of trustworthy AI is especially important in medicine due to the large implications for patients' lives. While trustworthiness concerns various aspects including ethical, technical and privacy requirements, we focus on the importance of data quality (training/test) in DL. Since data quality dictates the behaviour of ML products, evaluating data quality will play a key part in the regulatory approval of medical AI products. We perform a systematic review following PRISMA guidelines using the databases PubMed and ACM Digital Library. We identify 2362 studies, out of which 62 records fulfil our eligibility criteria. From this literature, we synthesise the existing knowledge on data quality frameworks and combine it with the perspective of ML applications in medicine. As a result, we p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#20219;&#21153;&#20998;&#37197;&#20915;&#31574;&#26041;&#27861;&#65292;&#35745;&#31639;&#26102;&#38388;&#38543;&#30528;&#29289;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#20165;&#21576;&#32447;&#24615;&#22686;&#38271;&#65292;&#20197;&#35299;&#20915;&#31515;&#21345;&#23572;&#26426;&#22120;&#20154;&#21452;&#33218;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13634</link><description>&lt;p&gt;
&#23398;&#20064;&#31515;&#21345;&#23572;&#26426;&#22120;&#20154;&#30340;&#21452;&#33218;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;
&lt;/p&gt;
&lt;p&gt;
Learning Dual-arm Object Rearrangement for Cartesian Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13634
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#20219;&#21153;&#20998;&#37197;&#20915;&#31574;&#26041;&#27861;&#65292;&#35745;&#31639;&#26102;&#38388;&#38543;&#30528;&#29289;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#20165;&#21576;&#32447;&#24615;&#22686;&#38271;&#65292;&#20197;&#35299;&#20915;&#31515;&#21345;&#23572;&#26426;&#22120;&#20154;&#21452;&#33218;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#19987;&#27880;&#20110;&#20174;&#31515;&#21345;&#23572;&#26426;&#22120;&#20154;&#29616;&#23454;&#24037;&#19994;&#22330;&#26223;&#20013;&#25277;&#35937;&#20986;&#30340;&#21452;&#33218;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#20197;&#26368;&#30701;&#30340;&#24635;&#23436;&#25104;&#26102;&#38388;&#23558;&#25152;&#26377;&#29289;&#20307;&#20174;&#26469;&#28304;&#22320;&#36716;&#31227;&#21040;&#30446;&#26631;&#22320;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26680;&#24515;&#24605;&#24819;&#26159;&#24320;&#21457;&#19968;&#31181;&#26377;&#25928;&#30340;&#29289;&#20307;-&#33218;&#20219;&#21153;&#20998;&#37197;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#21270;&#32047;&#31215;&#20219;&#21153;&#25191;&#34892;&#26102;&#38388;&#24182;&#26368;&#22823;&#21270;&#21452;&#33218;&#21327;&#20316;&#25928;&#29575;&#12290;&#22312;&#20219;&#21153;&#20998;&#37197;&#20013;&#30340;&#19968;&#20010;&#22256;&#38590;&#26159;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#38543;&#30528;&#29289;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#31163;&#32447;&#25628;&#32034;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#26102;&#38388;&#20250;&#30001;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#32780;&#22823;&#24133;&#22686;&#38271;&#12290;&#21463;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20915;&#31574;&#20013;&#30340;&#36866;&#24212;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#22312;&#32447;&#20219;&#21153;&#20998;&#37197;&#20915;&#31574;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#26102;&#38388;&#38543;&#29289;&#20307;&#25968;&#37327;&#30340;&#22686;&#21152;&#20165;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13634v1 Announce Type: cross  Abstract: This work focuses on the dual-arm object rearrangement problem abstracted from a realistic industrial scenario of Cartesian robots. The goal of this problem is to transfer all the objects from sources to targets with the minimum total completion time. To achieve the goal, the core idea is to develop an effective object-to-arm task assignment strategy for minimizing the cumulative task execution time and maximizing the dual-arm cooperation efficiency. One of the difficulties in the task assignment is the scalability problem. As the number of objects increases, the computation time of traditional offline-search-based methods grows strongly for computational complexity. Encouraged by the adaptability of reinforcement learning (RL) in long-sequence task decisions, we propose an online task assignment decision method based on RL, and the computation time of our method only increases linearly with the number of objects. Further, we design an
&lt;/p&gt;</description></item><item><title>UniGraph&#26694;&#26550;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#22270;&#21644;&#20219;&#21153;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.13630</link><description>&lt;p&gt;
UniGraph: &#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#23398;&#20064;&#36328;&#39046;&#22495;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13630
&lt;/p&gt;
&lt;p&gt;
UniGraph&#26694;&#26550;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#22270;&#21644;&#20219;&#21153;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13630v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: ChatGPT &#21644; GPT-4 &#31561;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#23637;&#31034;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#27867;&#21270;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#23427;&#20204;&#26368;&#21021;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20010;&#27010;&#24565;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#26102;&#65292;&#20986;&#29616;&#20102;&#40092;&#26126;&#30340;&#23545;&#27604;&#12290;&#22270;&#23398;&#20064;&#20027;&#35201;&#38598;&#20013;&#22312;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#25110;&#25968;&#25454;&#38598;&#23450;&#21046;&#30340;&#21333;&#20010;&#22270;&#27169;&#22411;&#19978;&#65292;&#32570;&#20047;&#23558;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110;&#22270;&#32467;&#26500;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#29305;&#23450;&#20110;&#22270;&#25968;&#25454;&#30340;&#19981;&#21516;&#29305;&#24449;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;UniGraph&#26694;&#26550;&#65292;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#22270;&#21644;&#20219;&#21153;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13630v1 Announce Type: new  Abstract: Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31995;&#32479;&#22330;&#26223;&#32858;&#31867;&#30340;&#25968;&#25454;&#39537;&#21160;&#23460;&#28201;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#20998;&#26512;&#25552;&#21462;&#31995;&#32479;&#36816;&#34892;&#29305;&#24449;&#65292;&#36827;&#19968;&#27493;&#31616;&#21270;&#31995;&#32479;&#27169;&#22411;&#20197;&#25552;&#39640;&#27867;&#21270;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#38477;&#20302;&#24314;&#27169;&#26102;&#38388;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.13628</link><description>&lt;p&gt;
&#25913;&#36827;&#24314;&#31569;&#28201;&#24230;&#39044;&#27979;&#65306;&#19968;&#31181;&#22522;&#20110;&#31995;&#32479;&#22330;&#26223;&#32858;&#31867;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Building Temperature Forecasting: A Data-driven Approach with System Scenario Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31995;&#32479;&#22330;&#26223;&#32858;&#31867;&#30340;&#25968;&#25454;&#39537;&#21160;&#23460;&#28201;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#20998;&#26512;&#25552;&#21462;&#31995;&#32479;&#36816;&#34892;&#29305;&#24449;&#65292;&#36827;&#19968;&#27493;&#31616;&#21270;&#31995;&#32479;&#27169;&#22411;&#20197;&#25552;&#39640;&#27867;&#21270;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#38477;&#20302;&#24314;&#27169;&#26102;&#38388;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13628v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25277;&#35937;&#65306;&#20379;&#28909;&#12289;&#36890;&#39118;&#21644;&#31354;&#35843;&#31995;&#32479;&#22312;&#32500;&#25345;&#33298;&#36866;&#30340;&#28909;&#29615;&#22659;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21344;&#24314;&#31569;&#37096;&#38376;&#32422;40%&#30340;&#20027;&#35201;&#33021;&#28304;&#20351;&#29992;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#24314;&#31569;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#65292;&#20351;&#29992;&#27169;&#24335;&#21450;&#20854;&#20135;&#29983;&#30340;&#37197;&#32622;&#20801;&#35768;&#25913;&#36827;&#39044;&#27979;&#33021;&#21147;&#30340;&#25511;&#21046;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#20379;&#28909;&#36890;&#39118;&#31354;&#35843;&#31995;&#32479;&#31649;&#29702;&#26469;&#35828;&#65292;&#20026;&#27599;&#20010;&#23376;&#31995;&#32479;&#26500;&#24314;&#35814;&#32454;&#27169;&#22411;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;k-means&#32858;&#31867;&#26041;&#27861;&#30340;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#23460;&#28201;&#39044;&#27979;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#39537;&#21160;&#28201;&#24230;&#39044;&#27979;&#26041;&#27861;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#20998;&#26512;&#25552;&#21462;&#31995;&#32479;&#36816;&#34892;&#29305;&#24449;&#65292;&#24182;&#36827;&#19968;&#27493;&#31616;&#21270;&#31995;&#32479;&#32423;&#27169;&#22411;&#20197;&#25552;&#39640;&#27867;&#21270;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24314;&#27169;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13628v1 Announce Type: new  Abstract: Heat, Ventilation and Air Conditioning (HVAC) systems play a critical role in maintaining a comfortable thermal environment and cost approximately 40% of primary energy usage in the building sector. For smart energy management in buildings, usage patterns and their resulting profiles allow the improvement of control systems with prediction capabilities. However, for large-scale HVAC system management, it is difficult to construct a detailed model for each subsystem. In this paper, a new data-driven room temperature prediction model is proposed based on the k-means clustering method. The proposed data-driven temperature prediction approach extracts the system operation feature through historical data analysis and further simplifies the system-level model to improve generalization and computational efficiency. We evaluate the proposed approach in the real world. The results demonstrated that our approach can significantly reduce modeling t
&lt;/p&gt;</description></item><item><title>&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#39640;&#32500;&#24773;&#20917;&#19979;&#37325;&#25277;&#26679;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#20165;&#24403;$\alpha$&#36275;&#22815;&#22823;&#26102;&#25552;&#20379;&#19968;&#33268;&#21487;&#38752;&#30340;&#35823;&#24046;&#20272;&#35745;&#65292;&#20197;&#21450;&#22312;&#36229;&#21442;&#25968;&#21270;&#21306;&#22495;$\alpha\!&lt;\!1$&#30340;&#24773;&#20917;&#19979;&#23427;&#20204;&#30340;&#39044;&#27979;&#34920;&#29616;</title><link>https://arxiv.org/abs/2402.13622</link><description>&lt;p&gt;
&#22312;&#39640;&#32500;&#27491;&#21017;&#21270;&#22238;&#24402;&#20013;&#23545;&#33258;&#20030;&#21644;&#23376;&#25277;&#26679;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13622
&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#39640;&#32500;&#24773;&#20917;&#19979;&#37325;&#25277;&#26679;&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#20165;&#24403;$\alpha$&#36275;&#22815;&#22823;&#26102;&#25552;&#20379;&#19968;&#33268;&#21487;&#38752;&#30340;&#35823;&#24046;&#20272;&#35745;&#65292;&#20197;&#21450;&#22312;&#36229;&#21442;&#25968;&#21270;&#21306;&#22495;$\alpha\!&lt;\!1$&#30340;&#24773;&#20917;&#19979;&#23427;&#20204;&#30340;&#39044;&#27979;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#20272;&#35745;&#32479;&#35745;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#27969;&#34892;&#37325;&#25277;&#26679;&#26041;&#27861;&#65292;&#22914;&#23376;&#25277;&#26679;&#12289;&#33258;&#20030;&#21644;jackknife&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#39640;&#32500;&#30417;&#30563;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#24773;&#22659;&#19979;&#65292;&#20363;&#22914;&#23725;&#22238;&#24402;&#21644;&#36923;&#36753;&#22238;&#24402;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#26041;&#27861;&#20272;&#35745;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#25552;&#20379;&#20102;&#32039;&#33268;&#30340;&#28176;&#36817;&#25551;&#36848;&#65292;&#32771;&#34385;&#21040;&#26679;&#26412;&#25968;&#37327;$n$&#21644;&#21327;&#21464;&#37327;&#32500;&#24230;$d$&#20197;&#21487;&#27604;&#22266;&#23450;&#36895;&#29575;$\alpha\!=\! n/d$&#22686;&#38271;&#30340;&#26497;&#38480;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;i&#65289;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#37325;&#25277;&#26679;&#26041;&#27861;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#34920;&#29616;&#20986;&#36825;&#20123;&#24773;&#20917;&#20856;&#22411;&#30340;&#21452;&#23792;&#34892;&#20026;&#65307;ii&#65289;&#21482;&#26377;&#22312;$\alpha$&#36275;&#22815;&#22823;&#26102;&#65292;&#23427;&#20204;&#25165;&#25552;&#20379;&#19968;&#33268;&#21487;&#38752;&#30340;&#35823;&#24046;&#20272;&#35745;&#65288;&#25105;&#20204;&#32473;&#20986;&#25910;&#25947;&#29575;&#65289;&#65307;iii&#65289;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#21270;&#21306;&#22495;$\alpha\!&lt;\!1$&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13622v1 Announce Type: cross  Abstract: We investigate popular resampling methods for estimating the uncertainty of statistical models, such as subsampling, bootstrap and the jackknife, and their performance in high-dimensional supervised regression tasks. We provide a tight asymptotic description of the biases and variances estimated by these methods in the context of generalized linear models, such as ridge and logistic regression, taking the limit where the number of samples $n$ and dimension $d$ of the covariates grow at a comparable fixed rate $\alpha\!=\! n/d$. Our findings are three-fold: i) resampling methods are fraught with problems in high dimensions and exhibit the double-descent-like behavior typical of these situations; ii) only when $\alpha$ is large enough do they provide consistent and reliable error estimations (we give convergence rates); iii) in the over-parametrized regime $\alpha\!&lt;\!1$ relevant to modern machine learning practice, their predictions are
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;VLSP 2023&#20013;ComOM&#20219;&#21153;&#30340;&#19968;&#20010;&#25968;&#25454;&#25361;&#25112;&#65292;&#26088;&#22312;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36890;&#36807;&#24320;&#21457;&#20174;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#27604;&#36739;&#24847;&#35265;&#30340;&#25216;&#26415;&#65292;&#21442;&#19982;&#32773;&#38656;&#25552;&#20986;&#33021;&#22815;&#25552;&#21462;&#27604;&#36739;"&#20116;&#20803;&#32452;"&#30340;&#27169;&#22411;&#24182;&#26681;&#25454;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.13613</link><description>&lt;p&gt;
VLSP 2023&#32508;&#36848;--ComOM&#20219;&#21153;&#65306;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#30340;&#27604;&#36739;&#24847;&#35265;&#25366;&#25496;&#25968;&#25454;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for Comparative Opinion Mining from Vietnamese Product Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;VLSP 2023&#20013;ComOM&#20219;&#21153;&#30340;&#19968;&#20010;&#25968;&#25454;&#25361;&#25112;&#65292;&#26088;&#22312;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36890;&#36807;&#24320;&#21457;&#20174;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#27604;&#36739;&#24847;&#35265;&#30340;&#25216;&#26415;&#65292;&#21442;&#19982;&#32773;&#38656;&#25552;&#20986;&#33021;&#22815;&#25552;&#21462;&#27604;&#36739;"&#20116;&#20803;&#32452;"&#30340;&#27169;&#22411;&#24182;&#26681;&#25454;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#36234;&#21335;&#35821;&#20135;&#21697;&#35780;&#35770;&#27604;&#36739;&#24847;&#35265;&#25366;&#25496;&#20849;&#20139;&#20219;&#21153;&#65288;ComOM&#65289;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#35813;&#20219;&#21153;&#20316;&#20026;&#31532;&#21313;&#23626;&#36234;&#21335;&#35821;&#35328;&#21644;&#35821;&#38899;&#22788;&#29702;&#22269;&#38469;&#30740;&#35752;&#20250;&#65288;VLSP 2023&#65289;&#30340;&#19968;&#37096;&#20998;&#20030;&#34892;&#12290;&#27492;&#20849;&#20139;&#20219;&#21153;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#20174;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#27604;&#36739;&#24847;&#35265;&#30340;&#25216;&#26415;&#26469;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#21442;&#19982;&#32773;&#34987;&#25361;&#25112;&#25552;&#20986;&#33021;&#22815;&#20174;&#27604;&#36739;&#21477;&#20013;&#29087;&#32451;&#25552;&#21462;&#27604;&#36739;&#8220;&#20116;&#20803;&#32452;&#8221;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#20027;&#39064;&#12289;&#23458;&#20307;&#12289;&#26041;&#38754;&#12289;&#35859;&#35789;&#21644;&#27604;&#36739;&#31867;&#22411;&#26631;&#31614;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;120&#20010;&#25991;&#26723;&#30340;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;7427&#20010;&#38750;&#27604;&#36739;&#21477;&#21644;1798&#20010;&#21477;&#23376;&#20013;&#30340;2468&#20010;&#27604;&#36739;&#12290;&#21442;&#19982;&#30340;&#27169;&#22411;&#23558;&#26681;&#25454;&#20934;&#30830;&#21305;&#37197;&#23439;&#24179;&#22343;&#30340;&#20116;&#20803;&#32452;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#21644;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13613v1 Announce Type: new  Abstract: This paper presents a comprehensive overview of the Comparative Opinion Mining from Vietnamese Product Reviews shared task (ComOM), held as part of the 10$^{th}$ International Workshop on Vietnamese Language and Speech Processing (VLSP 2023). The primary objective of this shared task is to advance the field of natural language processing by developing techniques that proficiently extract comparative opinions from Vietnamese product reviews. Participants are challenged to propose models that adeptly extract a comparative "quintuple" from a comparative sentence, encompassing Subject, Object, Aspect, Predicate, and Comparison Type Label. We construct a human-annotated dataset comprising $120$ documents, encompassing $7427$ non-comparative sentences and $2468$ comparisons within $1798$ sentences. Participating models undergo evaluation and ranking based on the Exact match macro-averaged quintuple F1 score.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#24320;&#21019;&#20102;&#31471;&#21040;&#31471;&#21457;&#29616;&#31995;&#32479;&#30340;&#26032;&#27169;&#24335;&#65292;&#21033;&#29992;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#25628;&#23547;&#21644;&#39564;&#35777;&#20551;&#35774;&#65292;&#31361;&#26174;&#20102;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13610</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Data-driven Discovery with Large Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13610
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#24320;&#21019;&#20102;&#31471;&#21040;&#31471;&#21457;&#29616;&#31995;&#32479;&#30340;&#26032;&#27169;&#24335;&#65292;&#21033;&#29992;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#25628;&#23547;&#21644;&#39564;&#35777;&#20551;&#35774;&#65292;&#31361;&#26174;&#20102;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#32047;&#31215;&#65292;&#23427;&#20316;&#20026;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25958;&#20419;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31038;&#21306;&#21033;&#29992;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#65288;LGMs&#65289;&#30340;&#33021;&#21147;&#65292;&#24320;&#21457;&#33258;&#21160;&#21270;&#31995;&#32479;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616; -- &#19968;&#31181;&#33539;&#24335;&#65292;&#20174;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#20013;&#32431;&#31929;&#25628;&#32034;&#21644;&#39564;&#35777;&#20551;&#35774;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#25910;&#38598;&#25110;&#29289;&#29702;&#23454;&#39564;&#12290;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#29702;&#24819;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#31995;&#32479;&#30340;&#20960;&#20010;&#26399;&#26395;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;GPT-4&#30340;DATAVOYAGER&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LGMs&#22914;&#20309;&#23454;&#29616;&#20960;&#39033;&#36825;&#20123;&#26399;&#26395;&#26465;&#20214; -- &#36825;&#26159;&#20197;&#21069;&#26080;&#27861;&#20570;&#21040;&#30340;&#25104;&#23601; -- &#21516;&#26102;&#20063;&#31361;&#26174;&#20102;&#24403;&#21069;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#20026;&#24320;&#23637;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13610v1 Announce Type: cross  Abstract: With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery -- a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata -- a feat previously unattainable -- while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;MCMC&#21644;&#26799;&#24230;&#19979;&#38477;&#30340;Ohzeki&#26041;&#27861;&#30340;&#21487;&#35757;&#32451;&#37319;&#26679;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#27493;&#38271;&#65292;&#37319;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#26799;&#24230;&#20272;&#35745;&#26367;&#20195;&#33258;&#21160;&#24494;&#20998;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#26174;&#31034;&#30456;&#23545;&#20110;&#21407;&#22987;&#26041;&#27861;&#26174;&#33879;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;</title><link>https://arxiv.org/abs/2402.13608</link><description>&lt;p&gt;
Markov Chain Monte Carlo&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#21152;&#36895;&#36890;&#36807;&#28145;&#24230;&#23637;&#24320;
&lt;/p&gt;
&lt;p&gt;
Convergence Acceleration of Markov Chain Monte Carlo-based Gradient Descent by Deep Unfolding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13608
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;MCMC&#21644;&#26799;&#24230;&#19979;&#38477;&#30340;Ohzeki&#26041;&#27861;&#30340;&#21487;&#35757;&#32451;&#37319;&#26679;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#27493;&#38271;&#65292;&#37319;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#26799;&#24230;&#20272;&#35745;&#26367;&#20195;&#33258;&#21160;&#24494;&#20998;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#26174;&#31034;&#30456;&#23545;&#20110;&#21407;&#22987;&#26041;&#27861;&#26174;&#33879;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22522;&#20110;&#28145;&#24230;&#23637;&#24320;&#30340;&#37319;&#26679;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65288;COPs&#65289;&#65292;&#35813;&#27714;&#35299;&#22120;&#22522;&#20110;&#32467;&#21512;&#20102;&#39532;&#23572;&#21487;&#22827;&#38142;&#8212;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#21644;&#26799;&#24230;&#19979;&#38477;&#30340;Ohzeki&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#20854;&#27493;&#38271;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#26799;&#24230;&#20272;&#35745;&#65292;&#29992;&#26041;&#24046;&#20272;&#35745;&#20195;&#26367;&#33258;&#21160;&#24494;&#20998;&#65292;&#20174;&#32780;&#35268;&#36991;&#20102;&#30001;&#20110;MCMC&#30340;&#19981;&#21487;&#24494;&#20998;&#24615;&#32780;&#23548;&#33268;&#21453;&#21521;&#20256;&#25773;&#22833;&#36133;&#30340;&#38382;&#39064;&#12290;&#23569;&#25968;COPs&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21407;&#22987;&#30340;Ohzeki&#26041;&#27861;&#30456;&#27604;&#65292;&#25552;&#20986;&#30340;&#27714;&#35299;&#22120;&#26174;&#33879;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13608v1 Announce Type: cross  Abstract: This study proposes a trainable sampling-based solver for combinatorial optimization problems (COPs) using a deep-learning technique called deep unfolding. The proposed solver is based on the Ohzeki method that combines Markov-chain Monte-Carlo (MCMC) and gradient descent, and its step sizes are trained by minimizing a loss function. In the training process, we propose a sampling-based gradient estimation that substitutes auto-differentiation with a variance estimation, thereby circumventing the failure of back propagation due to the non-differentiability of MCMC. The numerical results for a few COPs demonstrated that the proposed solver significantly accelerated the convergence speed compared with the original Ohzeki method.
&lt;/p&gt;</description></item><item><title>User-LLM&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#20351;&#20854;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13598</link><description>&lt;p&gt;
User-LLM: &#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23454;&#29616;&#26377;&#25928;&#30340;LLM&#35821;&#22659;&#21270;
&lt;/p&gt;
&lt;p&gt;
User-LLM: Efficient LLM Contextualization with User Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13598
&lt;/p&gt;
&lt;p&gt;
User-LLM&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#20351;&#20854;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#22797;&#26434;&#19988;&#28508;&#22312;&#22024;&#26434;&#30340;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;User-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#26469;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#12290;&#36825;&#20123;&#23884;&#20837;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20174;&#21508;&#31181;&#29992;&#25143;&#20132;&#20114;&#20013;&#31934;&#28860;&#20986;&#26469;&#30340;&#65292;&#33021;&#22815;&#25429;&#25417;&#28508;&#22312;&#29992;&#25143;&#20559;&#22909;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#36719;&#25552;&#31034;&#23558;&#36825;&#20123;&#29992;&#25143;&#23884;&#20837;&#19982;LLMs&#38598;&#25104;&#36215;&#26469;&#65292;&#20351;LLMs&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#22312;MovieLens&#12289;&#20122;&#39532;&#36874;&#35780;&#35770;&#21644;&#35895;&#27468;&#26412;&#22320;&#35780;&#35770;&#31561;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#21644;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#29992;&#25143;&#30340;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#35821;&#22659;&#21270;&#65292;&#21516;&#26102;&#22312;&#35745;&#31639;&#19978;&#20063;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13598v1 Announce Type: cross  Abstract: Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorpora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20999;&#24179;&#38754;&#31639;&#27861;&#65292;&#38024;&#23545;&#20302;&#32500;&#25968;&#25454;&#30340;k-means&#32858;&#31867;&#38382;&#39064;&#36827;&#34892;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#20985;&#24418;&#20998;&#37197;&#38382;&#39064;&#21644;&#20840;&#23616;&#20248;&#21270;&#29702;&#35770;&#26041;&#27861;&#65292;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#35299;&#20915;&#22823;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#25910;&#25947;&#20110;&#38646;&#26368;&#20248;&#24615;&#24046;&#20540;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.13595</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20840;&#23616;&#35299;&#20915;&#20302;&#32500;k-means&#32858;&#31867;&#38382;&#39064;&#30340;&#20999;&#24179;&#38754;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A cutting plane algorithm for globally solving low dimensional k-means clustering problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20999;&#24179;&#38754;&#31639;&#27861;&#65292;&#38024;&#23545;&#20302;&#32500;&#25968;&#25454;&#30340;k-means&#32858;&#31867;&#38382;&#39064;&#36827;&#34892;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#20985;&#24418;&#20998;&#37197;&#38382;&#39064;&#21644;&#20840;&#23616;&#20248;&#21270;&#29702;&#35770;&#26041;&#27861;&#65292;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#35299;&#20915;&#22823;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#25910;&#25947;&#20110;&#38646;&#26368;&#20248;&#24615;&#24046;&#20540;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;k-means&#32858;&#31867;&#26159;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#38024;&#23545;&#20302;&#32500;&#25968;&#25454;&#30340;k-means&#38382;&#39064;&#65292;&#26412;&#25991;&#23558;&#20854;&#21046;&#23450;&#20026;&#32467;&#26500;&#21270;&#20985;&#24418;&#20998;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#20302;&#32500;&#32467;&#26500;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#20026;&#20855;&#26377;&#22810;&#20010;&#31751;&#30340;&#22823;&#25968;&#25454;&#38598;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#36845;&#20195;&#27714;&#35299;&#19968;&#20010;&#23567;&#20985;&#38382;&#39064;&#21644;&#19968;&#20010;&#22823;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#21487;&#34892;&#35299;&#20197;&#21450;&#25910;&#25947;&#20110;&#38646;&#26368;&#20248;&#24615;&#24046;&#20540;&#30340;&#36793;&#30028;&#12290;&#26412;&#25991;&#32467;&#21512;&#20102;&#20840;&#23616;&#20248;&#21270;&#29702;&#35770;&#26041;&#27861;&#26469;&#21152;&#36895;&#31243;&#24207;&#65292;&#24182;&#25552;&#20379;&#20102;&#23427;&#20204;&#22312;&#24615;&#33021;&#26041;&#38754;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13595v1 Announce Type: cross  Abstract: Clustering is one of the most fundamental tools in data science and machine learning, and k-means clustering is one of the most common such methods. There is a variety of approximate algorithms for the k-means problem, but computing the globally optimal solution is in general NP-hard. In this paper we consider the k-means problem for instances with low dimensional data and formulate it as a structured concave assignment problem. This allows us to exploit the low dimensional structure and solve the problem to global optimality within reasonable time for large data sets with several clusters. The method builds on iteratively solving a small concave problem and a large linear programming problem. This gives a sequence of feasible solutions along with bounds which we show converges to zero optimality gap. The paper combines methods from global optimization theory to accelerate the procedure, and we provide numerical results on their perfor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#34892;&#20026;&#35843;&#25511;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;GuanoZero&#65292;&#20351;AI&#20195;&#29702;&#33021;&#22815;&#25484;&#25569;&#12298;&#20851;&#26086;&#12299;&#28216;&#25103;&#12290;</title><link>https://arxiv.org/abs/2402.13582</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#35843;&#25511;&#25484;&#25569;&#20851;&#26086;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Mastering the Game of Guandan with Deep Reinforcement Learning and Behavior Regulating
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#34892;&#20026;&#35843;&#25511;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;GuanoZero&#65292;&#20351;AI&#20195;&#29702;&#33021;&#22815;&#25484;&#25569;&#12298;&#20851;&#26086;&#12299;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#26159;&#29616;&#23454;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#39318;&#36873;&#24179;&#21488;&#12290; &#24456;&#22810;&#30740;&#31350;&#20851;&#27880;&#20110;&#28216;&#25103;&#20195;&#29702;&#21644;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#12298;&#20851;&#26086;&#12299;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#28216;&#25103;&#65292;&#21363;&#20351;&#26159;&#19987;&#19994;&#30340;&#20154;&#31867;&#29609;&#23478;&#26377;&#26102;&#20063;&#38590;&#20197;&#20570;&#20986;&#27491;&#30830;&#30340;&#20915;&#31574;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GuanZero&#30340;&#26694;&#26550;&#65292;&#35753;AI&#20195;&#29702;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#25484;&#25569;&#36825;&#20010;&#28216;&#25103;&#12290; &#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#26041;&#26696;&#26469;&#35843;&#25511;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13582v1 Announce Type: new  Abstract: Games are a simplified model of reality and often serve as a favored platform for Artificial Intelligence (AI) research. Much of the research is concerned with game-playing agents and their decision making processes. The game of Guandan (literally, "throwing eggs") is a challenging game where even professional human players struggle to make the right decision at times. In this paper we propose a framework named GuanZero for AI agents to master this game using Monte-Carlo methods and deep neural networks. The main contribution of this paper is about regulating agents' behavior through a carefully designed neural network encoding scheme. We then demonstrate the effectiveness of the proposed framework by comparing it with state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;-free &#26041;&#27861; ToDo&#65292;&#36890;&#36807;&#20196;&#29260;&#19979;&#37319;&#26679;&#21152;&#36895; Stable Diffusion &#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.13573</link><description>&lt;p&gt;
&#20219;&#21153;&#24453;&#21150;&#65306;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#39640;&#25928;&#29983;&#25104;&#30340;&#20196;&#29260;&#19979;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
ToDo: Token Downsampling for Efficient Generation of High-Resolution Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13573
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;-free &#26041;&#27861; ToDo&#65292;&#36890;&#36807;&#20196;&#29260;&#19979;&#37319;&#26679;&#21152;&#36895; Stable Diffusion &#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#25105;&#20204;&#21487;&#20197;&#22312;&#21512;&#29702;&#26102;&#38388;&#21644;&#20869;&#23384;&#38480;&#21046;&#20869;&#22788;&#29702;&#30340;&#22270;&#20687;&#22823;&#23567;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#23494;&#38598;&#27880;&#24847;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21253;&#21547;&#20887;&#20313;&#29305;&#24449;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#31232;&#30095;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861; ToDo&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20851;&#38190;&#21644;&#20540;&#20196;&#29260;&#30340;&#20196;&#29260;&#19979;&#37319;&#26679;&#65292;&#21487;&#23558;&#24120;&#35265;&#22823;&#23567;&#30340; Stable Diffusion &#25512;&#29702;&#21152;&#36895;&#33267;&#22810;&#36798;2&#20493;&#65292;&#23545;&#20110;2048x2048&#31561;&#39640;&#20998;&#36776;&#29575;&#65292;&#21152;&#36895;&#27604;&#21487;&#36798;4.5&#20493;&#25110;&#26356;&#39640;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24179;&#34913;&#39640;&#25928;&#21534;&#21520;&#37327;&#21644;&#20445;&#30495;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13573v1 Announce Type: cross  Abstract: Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#22359;AlgoFormer&#65292;&#30456;&#27604;&#26631;&#20934;Transformer&#21644;Looped Transformer&#65292;AlgoFormer&#22312;&#30456;&#21516;&#21442;&#25968;&#25968;&#37327;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#36798;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.13572</link><description>&lt;p&gt;
&#35770;&#19968;&#31181;&#21464;&#31181;Looped Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of a Variant of the Looped Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13572
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#22359;AlgoFormer&#65292;&#30456;&#27604;&#26631;&#20934;Transformer&#21644;Looped Transformer&#65292;AlgoFormer&#22312;&#30456;&#21516;&#21442;&#25968;&#25968;&#37327;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#22312;&#35299;&#20915;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#31243;&#24207;&#65288;&#21253;&#25324;&#31185;&#23398;&#35745;&#31639;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#26041;&#38754;&#65292;Transformer&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#20174;&#34920;&#36798;&#33021;&#21147;&#21644;&#21151;&#33021;&#24615;&#35282;&#24230;&#35299;&#37322;&#65292;&#26631;&#20934;&#30340;Transformer&#33021;&#22815;&#25191;&#34892;&#19968;&#20123;&#31639;&#27861;&#12290;&#20026;&#20102;&#36171;&#20104;Transformer&#31639;&#27861;&#33021;&#21147;&#65292;&#24182;&#21463;&#21040;&#26368;&#36817;&#25552;&#20986;&#30340;Looped Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#22359;&#65292;&#21517;&#20026;Algorithm Transformer&#65288;&#31616;&#31216;AlgoFormer&#65289;&#12290;&#19982;&#26631;&#20934;Transformer&#21644;&#32431;&#31929;&#30340;Looped Transformer&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;AlgoFormer&#22312;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#21442;&#25968;&#26102;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#31034;&#34920;&#36798;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#21463;&#20154;&#31867;&#35774;&#35745;&#30340;&#23398;&#20064;&#31639;&#27861;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;Transformer&#22359;&#21253;&#25324;&#19968;&#20010;&#36127;&#36131;&#36827;&#34892;ta
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13572v1 Announce Type: cross  Abstract: Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer (Yang et al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed Algorithm Transformer (abbreviated as AlgoFormer). Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can achieve significantly higher expressiveness in algorithm representation when using the same number of parameters. In particular, inspired by the structure of human-designed learning algorithms, our transformer block consists of a pre-transformer that is responsible for ta
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;"&#28857;&#26816;&#26597;&#31561;&#20215;&#24615;"&#65292;&#20026;&#21516;&#34892;&#39044;&#27979;&#26426;&#21046;&#30340;&#25928;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;</title><link>https://arxiv.org/abs/2402.13567</link><description>&lt;p&gt;
Spot Check Equivalence&#65306;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20449;&#24687;&#24341;&#20986;&#26426;&#21046;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Spot Check Equivalence: an Interpretable Metric for Information Elicitation Mechanisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;"&#28857;&#26816;&#26597;&#31561;&#20215;&#24615;"&#65292;&#20026;&#21516;&#34892;&#39044;&#27979;&#26426;&#21046;&#30340;&#25928;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#20110;AI&#31995;&#32479;&#22914;&#21516;&#27687;&#27668;&#19968;&#33324;&#37325;&#35201;&#65292;&#26377;&#25928;&#22320;&#20174;&#20247;&#21253;&#24037;&#20316;&#32773;&#20013;&#24341;&#20986;&#20449;&#24687;&#24050;&#25104;&#20026;&#24320;&#21457;&#39640;&#24615;&#33021;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39318;&#35201;&#38382;&#39064;&#12290;&#20004;&#31181;&#26222;&#36941;&#30340;&#33539;&#24335;&#65292;&#21363;&#28857;&#26816;&#26597;&#21644;&#21516;&#34892;&#39044;&#27979;&#65292;&#20351;&#24471;&#35774;&#35745;&#26426;&#21046;&#26469;&#35780;&#20272;&#21644;&#28608;&#21169;&#26469;&#33258;&#20154;&#31867;&#26631;&#27880;&#32773;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#25104;&#20026;&#21487;&#33021;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#33267;&#23569;&#25552;&#20986;&#20102;&#19977;&#31181;&#25351;&#26631;&#26469;&#27604;&#36739;&#36825;&#20123;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#25351;&#26631;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#23548;&#33268;&#20102;&#20998;&#27495;&#29978;&#33267;&#30683;&#30462;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#23558;&#35843;&#21644;&#36825;&#20123;&#19981;&#21516;&#30340;&#25925;&#20107;&#65292;&#23637;&#31034;&#20854;&#20013;&#20004;&#20010;&#25351;&#26631;&#22312;&#26576;&#20123;&#32972;&#26223;&#19979;&#23454;&#38469;&#19978;&#26159;&#30456;&#21516;&#30340;&#65292;&#24182;&#35299;&#37322;&#31532;&#19977;&#20010;&#30340;&#20998;&#27495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;"&#28857;&#26816;&#26597;&#31561;&#20215;&#24615;"&#26469;&#32479;&#19968;&#36825;&#20123;&#19981;&#21516;&#30340;&#32972;&#26223;&#65292;&#20026;&#21516;&#34892;&#39044;&#27979;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;...&#65288;&#26410;&#23436;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13567v1 Announce Type: cross  Abstract: Because high-quality data is like oxygen for AI systems, effectively eliciting information from crowdsourcing workers has become a first-order problem for developing high-performance machine learning algorithms. Two prevalent paradigms, spot-checking and peer prediction, enable the design of mechanisms to evaluate and incentivize high-quality data from human labelers. So far, at least three metrics have been proposed to compare the performances of these techniques [33, 8, 3]. However, different metrics lead to divergent and even contradictory results in various contexts. In this paper, we harmonize these divergent stories, showing that two of these metrics are actually the same within certain contexts and explain the divergence of the third. Moreover, we unify these different contexts by introducing \textit{Spot Check Equivalence}, which offers an interpretable metric for the effectiveness of a peer prediction mechanism. Finally, we pr
&lt;/p&gt;</description></item><item><title>IGAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#22270;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22270;&#39044;&#35757;&#32451;&#27867;&#21270;&#21040;&#24402;&#32435;&#22330;&#26223;&#65292;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#22270;&#21644;&#24494;&#35843;&#22270;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.13556</link><description>&lt;p&gt;
&#24402;&#32435;&#22270;&#23545;&#40784;&#25552;&#31034;&#65306;&#20174;&#35889;&#35282;&#24230;&#24357;&#21512;&#22270;&#39044;&#35757;&#32451;&#21644;&#24402;&#32435;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13556
&lt;/p&gt;
&lt;p&gt;
IGAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#22270;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22270;&#39044;&#35757;&#32451;&#27867;&#21270;&#21040;&#24402;&#32435;&#22330;&#26223;&#65292;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#22270;&#21644;&#24494;&#35843;&#22270;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#22270;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#36890;&#36807;&#25429;&#25417;&#19979;&#28216;&#20219;&#21153;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20043;&#38388;&#30340;&#25968;&#25454;&#21644;&#20219;&#21153;&#24040;&#22823;&#24046;&#36317;&#65292;&#27169;&#22411;&#24615;&#33021;&#20173;&#28982;&#21463;&#38480;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#25552;&#31034;&#24494;&#35843;&#30340;&#21551;&#21457;&#65292;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#20026;&#22312;&#22270;&#39046;&#22495;&#20013;&#24357;&#21512;&#24046;&#36317;&#20570;&#20986;&#20102;&#21162;&#21147;&#12290;&#20294;&#29616;&#26377;&#26041;&#27861;&#20165;&#20165;&#23558;&#24494;&#35843;&#20219;&#21153;&#30340;&#24418;&#24335;&#37325;&#26032;&#34920;&#36848;&#20026;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#22312;&#39044;&#35757;&#32451;&#22270;&#19982;&#24494;&#35843;&#22270;&#20860;&#23481;&#30340;&#21069;&#25552;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#36716;&#23548;&#35774;&#32622;&#20013;&#36816;&#34892;&#12290;&#20026;&#20102;&#23558;&#22270;&#39044;&#35757;&#32451;&#27867;&#21270;&#21040;&#24402;&#32435;&#22330;&#26223;&#65292;&#20854;&#20013;&#24494;&#35843;&#22270;&#21487;&#33021;&#19982;&#39044;&#35757;&#32451;&#22270;&#26174;&#33879;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24402;&#32435;&#22270;&#23545;&#40784;&#25552;&#31034;(IGAP)&#30340;&#26032;&#22411;&#22522;&#20110;&#22270;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13556v1 Announce Type: cross  Abstract: The "Graph pre-training and fine-tuning" paradigm has significantly improved Graph Neural Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks. However, due to the immense gap of data and tasks between the pre-training and fine-tuning stages, the model performance is still limited. Inspired by prompt fine-tuning in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in graph domain. But existing methods simply reformulate the form of fine-tuning tasks to the pre-training ones. With the premise that the pre-training graphs are compatible with the fine-tuning ones, these methods typically operate in transductive setting. In order to generalize graph pre-training to inductive scenario where the fine-tuning graphs might significantly differ from pre-training ones, we propose a novel graph prompt based method called Inductive Graph Alignment Prompt(IGAP). Firstly, we uni
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21465;&#20107;&#29702;&#35299;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#21465;&#20107;&#20013;&#24418;&#25104;&#22270;NARCO&#26469;&#25551;&#36848;&#25972;&#20010;&#32972;&#26223;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#36143;&#20381;&#36182;&#65292;&#20854;&#20013;&#30340;&#36793;&#21453;&#26144;&#20102;&#39640;&#23618;&#27425;&#30340;&#36830;&#36143;&#20851;&#31995;&#65292;&#26080;&#38656;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.13551</link><description>&lt;p&gt;
&#21465;&#20107;&#32972;&#26223;&#30340;&#22270;&#34920;&#31034;&#65306;&#36890;&#36807;&#22238;&#39038;&#24615;&#38382;&#39064;&#30340;&#36830;&#36143;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
Graph Representation of Narrative Context: Coherence Dependency via Retrospective Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13551
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21465;&#20107;&#29702;&#35299;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#21465;&#20107;&#20013;&#24418;&#25104;&#22270;NARCO&#26469;&#25551;&#36848;&#25972;&#20010;&#32972;&#26223;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#36143;&#20381;&#36182;&#65292;&#20854;&#20013;&#30340;&#36793;&#21453;&#26144;&#20102;&#39640;&#23618;&#27425;&#30340;&#36830;&#36143;&#20851;&#31995;&#65292;&#26080;&#38656;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#23454;&#29992;&#30340;&#21465;&#20107;&#29702;&#35299;&#33539;&#24335;&#65292;&#36825;&#26159;&#22522;&#20110;&#19968;&#20010;&#35266;&#23519;&#65306;&#21465;&#36848;&#20013;&#30340;&#20010;&#21035;&#27573;&#33853;&#36890;&#24120;&#26159;&#30456;&#20114;&#20851;&#32852;&#30340;&#65292;&#32780;&#19981;&#26159;&#23396;&#31435;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#21465;&#20107;&#20013;&#24418;&#25104;&#19968;&#20010;&#21517;&#20026;NARCO&#30340;&#22270;&#65292;&#25551;&#36848;&#25972;&#20010;&#32972;&#26223;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#36830;&#36143;&#20381;&#36182;&#12290;&#29305;&#21035;&#26159;&#65292;NARCO&#20013;&#30340;&#36793;&#28085;&#30422;&#20102;&#20004;&#20010;&#19978;&#19979;&#25991;&#29255;&#27573;&#20043;&#38388;&#30340;&#33258;&#30001;&#24418;&#24335;&#22238;&#39038;&#24615;&#38382;&#39064;&#65292;&#21453;&#26144;&#20102;&#39640;&#23618;&#27425;&#30340;&#36830;&#36143;&#20851;&#31995;&#65292;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#24863;&#30693;&#30340;&#21551;&#21457;&#65292;&#20154;&#31867;&#19981;&#26029;&#20174;&#20808;&#21069;&#32972;&#26223;&#20013;&#37325;&#30003;&#30456;&#20851;&#20107;&#20214;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22270;&#26159;&#36890;&#36807;&#25105;&#20204;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;LLM&#25552;&#31034;&#23454;&#20363;&#21270;&#30340;&#65292;&#22240;&#27492;&#26080;&#38656;&#20381;&#36182;&#20154;&#31867;&#27880;&#37322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#20010;&#20851;&#20110;&#20854;&#23454;&#38469;&#25928;&#29992;&#30340;&#29420;&#29305;&#30740;&#31350;&#65292;&#36890;&#36807;&#24635;&#32467;&#35782;&#21035;&#26816;&#39564;&#36793;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24773;&#33410;&#26816;&#32034;&#36827;&#34892;&#26412;&#22320;&#19978;&#19979;&#25991;&#22686;&#24378;&#65292;&#20197;&#21450;&#36890;&#36807;&#38271;&#25991;&#26723;&#38382;&#31572;&#31034;&#20363;&#21270;&#30340;&#26356;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13551v1 Announce Type: new  Abstract: This work introduces a novel and practical paradigm for narrative comprehension, stemming from the observation that individual passages within narratives are often cohesively related than being isolated. We therefore propose to formulate a graph upon narratives dubbed NARCO that depicts a task-agnostic coherence dependency of the entire context. Especially, edges in NARCO encompass retrospective free-form questions between two context snippets reflecting high-level coherent relations, inspired by the cognitive perception of humans who constantly reinstate relevant events from prior context. Importantly, our graph is instantiated through our designed two-stage LLM prompting, thereby without reliance on human annotations. We present three unique studies on its practical utility, examining the edge efficacy via recap identification, local context augmentation via plot retrieval, and broader applications exemplified by long document QA. Expe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DiffPLF&#27169;&#22411;&#65292;&#29992;&#20110;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#30340;&#27010;&#29575;&#36127;&#36733;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25191;&#34892;&#26465;&#20214;&#29983;&#25104;&#20805;&#30005;&#38656;&#27714;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.13548</link><description>&lt;p&gt;
DiffPLF&#65306;&#19968;&#31181;&#29992;&#20110;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#36127;&#33655;&#27010;&#29575;&#39044;&#27979;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of EV Charging Load
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13548
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DiffPLF&#27169;&#22411;&#65292;&#29992;&#20110;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#30340;&#27010;&#29575;&#36127;&#36733;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25191;&#34892;&#26465;&#20214;&#29983;&#25104;&#20805;&#30005;&#38656;&#27714;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30005;&#21160;&#27773;&#36710;&#65288;EV&#65289;&#23545;&#37197;&#30005;&#32593;&#30340;&#28145;&#24230;&#28183;&#36879;&#65292;&#20805;&#30005;&#36127;&#33655;&#39044;&#27979;&#23545;&#20110;&#25512;&#21160;&#20805;&#30005;&#31449;&#36816;&#33829;&#21644;&#38656;&#27714;&#20391;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#20805;&#30005;&#34892;&#20026;&#21644;&#30456;&#20851;&#22806;&#29983;&#22240;&#32032;&#20351;&#24471;&#26410;&#26469;&#30340;&#20805;&#30005;&#36127;&#36733;&#27169;&#24335;&#21464;&#21270;&#21095;&#28872;&#19988;&#38590;&#20197;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;DiffPLF&#30340;&#26032;&#39062;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#30340;&#27010;&#29575;&#36127;&#36733;&#39044;&#27979;&#65292;&#21487;&#20197;&#26126;&#30830;&#22320;&#36817;&#20284;&#20110;&#22522;&#20110;&#21382;&#21490;&#25968;&#25454;&#21644;&#30456;&#20851;&#21327;&#21464;&#37327;&#30340;&#39044;&#27979;&#36127;&#36733;&#20998;&#24067;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#36807;&#31243;&#30340;&#36870;&#36716;&#65292;&#36880;&#28176;&#23558;&#39640;&#26031;&#20808;&#39564;&#36716;&#25442;&#20026;&#23454;&#26102;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#25193;&#25955;&#27169;&#22411;&#19982;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#35843;&#33410;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#25191;&#34892;&#21487;&#33021;&#30340;&#20805;&#30005;&#38656;&#27714;&#37197;&#32622;&#30340;&#26465;&#20214;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#24863;&#30693;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13548v1 Announce Type: new  Abstract: Due to the vast electric vehicle (EV) penetration to distribution grid, charging load forecasting is essential to promote charging station operation and demand-side management.However, the stochastic charging behaviors and associated exogenous factors render future charging load patterns quite volatile and hard to predict. Accordingly, we devise a novel Diffusion model termed DiffPLF for Probabilistic Load Forecasting of EV charging, which can explicitly approximate the predictive load distribution conditioned on historical data and related covariates. Specifically, we leverage a denoising diffusion model, which can progressively convert the Gaussian prior to real time-series data by learning a reversal of the diffusion process. Besides, we couple such diffusion model with a cross-attention-based conditioning mechanism to execute conditional generation for possible charging demand profiles. We also propose a task-informed fine-tuning tec
&lt;/p&gt;</description></item><item><title>ARL2&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#22312;NQ&#21644;MMLU&#19978;&#21462;&#24471;&#20102;5.4%&#21644;4.6%&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13542</link><description>&lt;p&gt;
ARL2: &#36890;&#36807;&#33258;&#23548;&#33258;&#36866;&#24212;&#30456;&#20851;&#24615;&#26631;&#35760;&#23558;&#26816;&#32034;&#22120;&#19982;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13542
&lt;/p&gt;
&lt;p&gt;
ARL2&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#22312;NQ&#21644;MMLU&#19978;&#21462;&#24471;&#20102;5.4%&#21644;4.6%&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13542v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#28304;&#30340;&#30456;&#20851;&#20449;&#24687;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#20943;&#36731;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20998;&#24320;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;LLMs&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#29616;&#26377;&#30340;&#26816;&#32034;&#22120;&#36890;&#24120;&#19982;LLMs&#19981;&#21305;&#37197;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARL2&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#30340;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;ARL2&#21033;&#29992;LLMs&#27880;&#37322;&#21644;&#35780;&#20998;&#30456;&#20851;&#35777;&#25454;&#65292;&#20174;&#32780;&#33021;&#22815;&#20174;&#24378;&#22823;&#30340;LLM&#30417;&#30563;&#20013;&#23398;&#20064;&#26816;&#32034;&#22120;&#12290;&#27492;&#22806;&#65292;ARL2&#20351;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#26469;&#31574;&#21010;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30456;&#20851;&#24615;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;ARL2&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;NQ&#19978;&#25552;&#39640;&#20102;5.4%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;MMLU&#19978;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13542v1 Announce Type: cross  Abstract: Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionall
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#24615;&#33021;GPU&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#32467;&#26500;&#26469;&#39640;&#25928;&#22320;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#23618;&#20887;&#20313;&#24615;&#12289;GPU&#20869;&#23384;&#21344;&#29992;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;GPU&#21033;&#29992;&#29575;&#19981;&#36275;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.13533</link><description>&lt;p&gt;
FinGPT-HPC: &#39640;&#24615;&#33021;&#35745;&#31639;&#19979;&#29992;&#20110;&#37329;&#34701;&#24212;&#29992;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#24615;&#33021;GPU&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#32467;&#26500;&#26469;&#39640;&#25928;&#22320;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#23618;&#20887;&#20313;&#24615;&#12289;GPU&#20869;&#23384;&#21344;&#29992;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;GPU&#21033;&#29992;&#29575;&#19981;&#36275;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#35745;&#31639;&#23494;&#38598;&#24615;&#24456;&#39640;&#12290;&#35745;&#31639;&#24037;&#20316;&#37327;&#21644;&#20869;&#23384;&#21344;&#29992;&#37327;&#38543;&#32500;&#24230;(&#23618;&#23485;&#24230;)&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#22823;&#22810;&#25968;LLM&#21442;&#25968;&#26469;&#33258;&#21464;&#21387;&#22120;&#32467;&#26500;&#30340;&#32447;&#24615;&#23618;&#65292;&#20855;&#26377;&#39640;&#24230;&#20887;&#20313;&#24615;&#12290;&#36825;&#20123;&#32447;&#24615;&#23618;&#36129;&#29486;&#20102;&#36229;&#36807;80%&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#21644;99%&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;LLMs&#65292;&#38656;&#35201;&#35299;&#20915;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1) &#20943;&#23569;&#32447;&#24615;&#23618;&#30340;&#20887;&#20313;&#24615;&#65307;2) &#20943;&#23569;GPU&#20869;&#23384;&#21344;&#29992;&#65307;3) &#22312;&#20351;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#26102;&#25552;&#39640;GPU&#21033;&#29992;&#29575;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#22914;LoRA&#21644;QLoRA&#65292;&#21033;&#29992;&#20302;&#31209;&#30697;&#38453;&#21644;&#37327;&#21270;&#26469;&#20998;&#21035;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292; resulting model &#20173;&#28982;&#28040;&#32791;&#22823;&#37327;GPU&#20869;&#23384;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#39640;&#24615;&#33021;GPU&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#32467;&#26500;&#26469;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13533v1 Announce Type: cross  Abstract: Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and finetune LLMs efficiently, there are three major challenges to address: 1) reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3) improving GPU utilization when using distributed training. Prior methods, such as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the number of trainable parameters and model size, respectively. However, the resulting model still consumes a large amount of GPU memory. In this paper, we present high-performance GPU-based methods that exploit low-rank structures to pretrain and finetun
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#32447;&#24615;&#22238;&#24402;&#30340;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#65292;&#36890;&#36807;&#25913;&#36827;&#20998;&#26512;&#24471;&#20986;&#20102;&#26356;&#32039;&#30340;&#35823;&#24046;&#30028;&#38480;&#21644;&#23454;&#20363;&#29305;&#23450;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.13531</link><description>&lt;p&gt;
&#31169;&#26377;&#26799;&#24230;&#19979;&#38477;&#29992;&#20110;&#32447;&#24615;&#22238;&#24402;&#65306;&#26356;&#32039;&#30340;&#35823;&#24046;&#30028;&#38480;&#21644;&#23454;&#20363;&#29305;&#23450;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Private Gradient Descent for Linear Regression: Tighter Error Bounds and Instance-Specific Uncertainty Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13531
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32447;&#24615;&#22238;&#24402;&#30340;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#19979;&#38477;&#65292;&#36890;&#36807;&#25913;&#36827;&#20998;&#26512;&#24471;&#20986;&#20102;&#26356;&#32039;&#30340;&#35823;&#24046;&#30028;&#38480;&#21644;&#23454;&#20363;&#29305;&#23450;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#26631;&#20934;&#30340;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#32447;&#24615;&#22238;&#24402;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#20102;&#25913;&#36827;&#20998;&#26512;&#65292;&#38024;&#23545;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#12290; &#22312;&#23545;&#36755;&#20837;&#36827;&#34892;&#36866;&#24230;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#36845;&#20195;&#30340;&#20998;&#24067;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#23548;&#33268;&#20102;&#31639;&#27861;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26032;&#32467;&#26524;&#65306;&#23545;&#20110;&#36866;&#24403;&#22266;&#23450;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20165;&#19982;&#25968;&#25454;&#32500;&#24230;&#32447;&#24615;&#30456;&#20851;&#12290; &#36825;&#19982;&#65288;&#38750;&#31169;&#26377;&#65289;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#32500;&#24230;&#30456;&#20851;&#24615;&#20197;&#21450;&#20381;&#36182;&#20110;&#22797;&#26434;&#33258;&#36866;&#24212;&#26799;&#24230;&#35009;&#21098;&#26041;&#26696;&#30340;&#26368;&#36817;&#31169;&#26377;&#31639;&#27861;&#65288;Varshney&#31561;&#20154;&#65292;2022&#24180;; Liu&#31561;&#20154;&#65292;2023&#24180;&#65289;&#30340;&#32500;&#24230;&#30456;&#20851;&#24615;&#30456;&#21305;&#37197;&#12290; &#25105;&#20204;&#23545;&#36845;&#20195;&#20998;&#24067;&#30340;&#20998;&#26512;&#36824;&#20801;&#35768;&#25105;&#20204;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#29992;&#20110;&#33258;&#21160;&#36866;&#24212;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#31639;&#27861;&#30340;&#26041;&#24046;&#30340;&#32463;&#39564;&#20248;&#21270;&#22120;&#12290; &#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13531v1 Announce Type: new  Abstract: We provide an improved analysis of standard differentially private gradient descent for linear regression under the squared error loss. Under modest assumptions on the input, we characterize the distribution of the iterate at each time step.   Our analysis leads to new results on the algorithm's accuracy: for a proper fixed choice of hyperparameters, the sample complexity depends only linearly on the dimension of the data. This matches the dimension-dependence of the (non-private) ordinary least squares estimator as well as that of recent private algorithms that rely on sophisticated adaptive gradient-clipping schemes (Varshney et al., 2022; Liu et al., 2023).   Our analysis of the iterates' distribution also allows us to construct confidence intervals for the empirical optimizer which adapt automatically to the variance of the algorithm on a particular data set. We validate our theorems through experiments on synthetic data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#39044;&#27979;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39044;&#27979;&#20316;&#20026;&#36755;&#20837;&#20934;&#30830;&#25191;&#34892;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#20915;&#31574;&#20013;&#39044;&#27979;&#36136;&#37327;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13530</link><description>&lt;p&gt;
&#20004;&#20010;&#19990;&#30028;&#20013;&#30340;&#26368;&#20339;&#20043;&#36873;&#65306;&#22312;&#26410;&#30693;&#21040;&#36798;&#27169;&#22411;&#19979;&#24102;&#39044;&#27979;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Best of Many in Both Worlds: Online Resource Allocation with Predictions under Unknown Arrival Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#39044;&#27979;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39044;&#27979;&#20316;&#20026;&#36755;&#20837;&#20934;&#30830;&#25191;&#34892;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#22312;&#32447;&#20915;&#31574;&#20013;&#39044;&#27979;&#36136;&#37327;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13530v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#24403;&#20170;&#30340;&#22312;&#32447;&#20915;&#31574;&#32773;&#36890;&#24120;&#21487;&#20197;&#33719;&#24471;&#20851;&#20110;&#26410;&#26469;&#21464;&#37327;&#30340;&#39044;&#27979;&#65292;&#22914;&#21040;&#36798;&#12289;&#38656;&#27714;&#12289;&#24211;&#23384;&#31561;&#12290;&#36825;&#20123;&#39044;&#27979;&#21487;&#20197;&#30001;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31639;&#27861;&#29983;&#25104;&#65292;&#19968;&#30452;&#21040;&#21033;&#29992;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#21644;&#38468;&#21152;&#29305;&#24449;&#20449;&#24687;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#36136;&#37327;&#36890;&#24120;&#23545;&#20915;&#31574;&#32773;&#26469;&#35828;&#26159;&#26410;&#30693;&#30340;&#65292;&#22240;&#27492;&#30450;&#30446;&#22320;&#36981;&#24490;&#39044;&#27979;&#21487;&#33021;&#26159;&#26377;&#23475;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#32473;&#20986;&#23558;&#39044;&#27979;&#20316;&#20026;&#36755;&#20837;&#24182;&#38024;&#23545;&#26410;&#30693;&#39044;&#27979;&#36136;&#37327;&#36827;&#34892;&#31283;&#20581;&#25191;&#34892;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#36825;&#26159;&#25910;&#30410;&#31649;&#29702;&#21644;&#22312;&#32447;&#20915;&#31574;&#21046;&#23450;&#20013;&#26368;&#36890;&#29992;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#20915;&#31574;&#32773;&#25317;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#36164;&#28304;&#65292;&#24182;&#19988;&#35831;&#27714;&#26159;&#39034;&#24207;&#21040;&#26469;&#30340;&#12290;&#23545;&#20110;&#27599;&#20010;&#35831;&#27714;&#65292;&#20915;&#31574;&#32773;&#38656;&#35201;&#20915;&#23450;&#37319;&#21462;&#20309;&#31181;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13530v1 Announce Type: cross  Abstract: Online decision-makers today can often obtain predictions on future variables, such as arrivals, demands, inventories, and so on. These predictions can be generated from simple forecasting algorithms for univariate time-series, all the way to state-of-the-art machine learning models that leverage multiple time-series and additional feature information. However, the prediction quality is often unknown to decisions-makers a priori, hence blindly following the predictions can be harmful. In this paper, we address this problem by giving algorithms that take predictions as inputs and perform robustly against the unknown prediction quality.   We consider the online resource allocation problem, one of the most generic models in revenue management and online decision-making. In this problem, a decision maker has a limited amount of resources, and requests arrive sequentially. For each request, the decision-maker needs to decide on an action, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#65292;&#36890;&#36807;&#25366;&#25496;&#31038;&#20132;&#32593;&#32476;&#20013;&#20851;&#20110;&#39044;&#26399;&#22833;&#36133;&#30340;&#25285;&#24551;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#22522;&#30784;&#35774;&#26045;&#22833;&#36133;&#12290;</title><link>https://arxiv.org/abs/2402.13528</link><description>&lt;p&gt;
&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#65306;&#20174;&#32467;&#26500;&#28798;&#38590;&#21709;&#24212;&#20013;&#25366;&#25496;&#26410;&#26469;&#22833;&#25928;&#25285;&#24551;
&lt;/p&gt;
&lt;p&gt;
Infrastructure Ombudsman: Mining Future Failure Concerns from Structural Disaster Response
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#65292;&#36890;&#36807;&#25366;&#25496;&#31038;&#20132;&#32593;&#32476;&#20013;&#20851;&#20110;&#39044;&#26399;&#22833;&#36133;&#30340;&#25285;&#24551;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#22522;&#30784;&#35774;&#26045;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30740;&#31350;&#38598;&#20013;&#20110;&#30740;&#31350;&#31038;&#20132;&#23186;&#20307;&#19978;&#19982;&#32467;&#26500;&#22833;&#36133;&#30456;&#20851;&#30340;&#35752;&#35770;&#65292;&#20197;&#25913;&#36827;&#28798;&#38590;&#21709;&#24212;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#31038;&#20132;&#32593;&#32476;&#24086;&#23376;&#20013;&#35752;&#35770;&#20851;&#20110;&#39044;&#26399;&#22833;&#36133;&#30340;&#25285;&#24551;&#26159;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#12290;&#22914;&#26524;&#36825;&#20123;&#25285;&#24551;&#34987;&#20256;&#36798;&#32473;&#36866;&#24403;&#30340;&#26426;&#26500;&#65292;&#21487;&#20197;&#24110;&#21161;&#39044;&#38450;&#21644;&#20943;&#36731;&#28508;&#22312;&#30340;&#22522;&#30784;&#35774;&#26045;&#22833;&#36133;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#30784;&#35774;&#26045;&#35843;&#35299;&#21592;&#8212;&#8212;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#29305;&#23450;&#22522;&#30784;&#35774;&#26045;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#32771;&#34385;&#20102;&#32654;&#22269;&#20960;&#36215;&#26368;&#36817;&#30340;&#32467;&#26500;&#22833;&#25928;&#20107;&#20214;&#12290;&#25105;&#20204;&#21576;&#29616;&#20102;&#19968;&#20221;&#39318;&#21019;&#24615;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;Reddit&#21644;YouTube&#20013;&#25366;&#25496;&#30340;2,662&#20010;&#31038;&#20132;&#32593;&#32476;&#23454;&#20363;&#65292;&#29992;&#20110;&#36825;&#19968;&#26032;&#39062;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13528v1 Announce Type: cross  Abstract: Current research concentrates on studying discussions on social media related to structural failures to improve disaster response strategies. However, detecting social web posts discussing concerns about anticipatory failures is under-explored. If such concerns are channeled to the appropriate authorities, it can aid in the prevention and mitigation of potential infrastructural failures. In this paper, we develop an infrastructure ombudsman -- that automatically detects specific infrastructure concerns. Our work considers several recent structural failures in the US. We present a first-of-its-kind dataset of 2,662 social web instances for this novel task mined from Reddit and YouTube.
&lt;/p&gt;</description></item><item><title>MatchNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#21516;&#26102;&#20351;&#29992;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#20248;&#21270;&#22823;&#22411;&#32593;&#32476;&#23478;&#26063;&#65292;&#24182;&#33258;&#21160;&#25628;&#32034;&#38024;&#23545;&#19981;&#21516;&#30828;&#20214;&#24179;&#21488;&#30340;&#23450;&#21046;&#32593;&#32476;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#31232;&#30095;&#26631;&#31614;&#25968;&#25454;&#29615;&#22659;&#20013;&#20248;&#21270;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13525</link><description>&lt;p&gt;
MatchNAS&#65306;&#36890;&#36807;&#33258;&#21160;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31227;&#26893;&#20248;&#21270;&#31232;&#30095;&#26631;&#31614;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
MatchNAS: Optimizing Edge AI in Sparse-Label Data Contexts via Automating Deep Neural Network Porting for Mobile Deployment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13525
&lt;/p&gt;
&lt;p&gt;
MatchNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#21516;&#26102;&#20351;&#29992;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#20248;&#21270;&#22823;&#22411;&#32593;&#32476;&#23478;&#26063;&#65292;&#24182;&#33258;&#21160;&#25628;&#32034;&#38024;&#23545;&#19981;&#21516;&#30828;&#20214;&#24179;&#21488;&#30340;&#23450;&#21046;&#32593;&#32476;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#31232;&#30095;&#26631;&#31614;&#25968;&#25454;&#29615;&#22659;&#20013;&#20248;&#21270;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36793;&#32536;&#26234;&#33021;&#22312;&#24378;&#22823;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; (DNNs) &#30340;&#25903;&#25345;&#19979;&#36805;&#29467;&#21457;&#23637;&#12290;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#26696;&#26159;&#22312;&#24378;&#22823;&#30340;&#20113;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451; DNNs&#65292;&#28982;&#21518;&#22312;&#32463;&#36807;&#36731;&#37327;&#21270;&#21518;&#23558;&#20854;&#31227;&#26893;&#21040;&#31227;&#21160;&#35774;&#22791;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MatchNAS&#65292;&#19968;&#31181;&#29992;&#20110;&#23558; DNN &#31227;&#26893;&#21040;&#31227;&#21160;&#35774;&#22791;&#30340;&#26032;&#39062;&#26041;&#26696;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21516;&#26102;&#20351;&#29992;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#20248;&#21270;&#19968;&#20010;&#22823;&#22411;&#32593;&#32476;&#23478;&#26063;&#65292;&#28982;&#21518;&#33258;&#21160;&#25628;&#32034;&#38024;&#23545;&#19981;&#21516;&#30828;&#20214;&#24179;&#21488;&#30340;&#23450;&#21046;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13525v1 Announce Type: new  Abstract: Recent years have seen the explosion of edge intelligence with powerful Deep Neural Networks (DNNs). One popular scheme is training DNNs on powerful cloud servers and subsequently porting them to mobile devices after being lightweight. Conventional approaches manually specialized DNNs for various edge platforms and retrain them with real-world data. However, as the number of platforms increases, these approaches become labour-intensive and computationally prohibitive. Additionally, real-world data tends to be sparse-label, further increasing the difficulty of lightweight models. In this paper, we propose MatchNAS, a novel scheme for porting DNNs to mobile devices. Specifically, we simultaneously optimise a large network family using both labelled and unlabelled data and then automatically search for tailored networks for different hardware platforms. MatchNAS acts as an intermediary that bridges the gap between cloud-based DNNs and edge-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21464;&#21270;&#27599;&#20010;&#32500;&#24230;&#30340;&#27604;&#20363;&#26469;&#35843;&#26597;&#31354;&#38388;&#20449;&#24687;&#23545;&#20110;&#33041;&#30005;&#22270;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#30340;&#37325;&#35201;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#31354;&#38388;&#20449;&#24687;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#30456;&#20851;&#65292;&#19982;&#39057;&#35889;&#20449;&#24687;&#30340;&#30456;&#20851;&#24615;&#30456;&#21516;</title><link>https://arxiv.org/abs/2402.13523</link><description>&lt;p&gt;
&#24179;&#34913;&#33041;&#30005;&#22270;&#20013;&#30340;&#39057;&#35889;&#12289;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#29992;&#20110;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Balancing Spectral, Temporal and Spatial Information for EEG-based Alzheimer's Disease Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13523
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21464;&#21270;&#27599;&#20010;&#32500;&#24230;&#30340;&#27604;&#20363;&#26469;&#35843;&#26597;&#31354;&#38388;&#20449;&#24687;&#23545;&#20110;&#33041;&#30005;&#22270;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#30340;&#37325;&#35201;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#31354;&#38388;&#20449;&#24687;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#30456;&#20851;&#65292;&#19982;&#39057;&#35889;&#20449;&#24687;&#30340;&#30456;&#20851;&#24615;&#30456;&#21516;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#27835;&#30103;&#21069;&#26223;&#38656;&#35201;&#24320;&#21457;&#32463;&#27982;&#26377;&#25928;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31579;&#26597;&#26041;&#27861;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#26159;&#26368;&#32463;&#27982;&#30340;&#25104;&#20687;&#27169;&#24335;&#20043;&#19968;&#12290;&#26368;&#36817;&#30340;&#33041;&#30005;&#22270;&#20998;&#26512;&#24037;&#20316;&#24050;&#32463;&#36716;&#21521;&#21033;&#29992;&#31354;&#38388;&#20449;&#24687;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#22914;&#22270;&#20449;&#21495;&#22788;&#29702;&#25110;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#30456;&#23545;&#20110;&#39057;&#35889;&#25110;&#26102;&#38388;&#20449;&#24687;&#30340;&#31354;&#38388;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#65292;&#36890;&#36807;&#25913;&#21464;&#27599;&#20010;&#32500;&#24230;&#25152;&#21344;&#27604;&#20363;&#26469;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#35268;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#21508;&#31181;&#32500;&#24230;&#20998;&#36776;&#29575;&#37197;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31354;&#38388;&#20449;&#24687;&#22987;&#32456;&#27604;&#26102;&#38388;&#20449;&#24687;&#26356;&#30456;&#20851;&#65292;&#19982;&#39057;&#35889;&#20449;&#24687;&#30340;&#30456;&#20851;&#24615;&#30456;&#21516;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#32771;&#34385;&#31354;&#38388;&#20449;&#24687;&#36827;&#34892;&#22522;&#20110;&#33041;&#30005;&#22270;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13523v1 Announce Type: cross  Abstract: The prospect of future treatment warrants the development of cost-effective screening for Alzheimer's disease (AD). A promising candidate in this regard is electroencephalography (EEG), as it is one of the most economic imaging modalities. Recent efforts in EEG analysis have shifted towards leveraging spatial information, employing novel frameworks such as graph signal processing or graph neural networks. Here, we systematically investigate the importance of spatial information relative to spectral or temporal information by varying the proportion of each dimension for AD classification. To do so, we test various dimension resolution configurations on two routine EEG datasets. We find that spatial information is consistently more relevant than temporal information and equally relevant as spectral information. These results emphasise the necessity to consider spatial information for EEG-based AD classification. On our second dataset, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#21040;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#25581;&#31034;&#20102;&#29983;&#25104;Transformer&#21160;&#24577;&#30340;&#26426;&#29702;&#21644;&#30456;&#20851;&#26465;&#20214;&#65292;&#20026;&#19968;&#33268;&#20272;&#35745;&#25552;&#20379;&#20102;&#20445;&#35777;&#65292;&#24182;&#22312;IID&#26679;&#26412;&#19979;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.13512</link><description>&lt;p&gt;
&#20174;&#33258;&#27880;&#24847;&#21147;&#21040;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65306;&#25581;&#31034;&#29983;&#25104;Transformer&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#21040;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#25581;&#31034;&#20102;&#29983;&#25104;Transformer&#21160;&#24577;&#30340;&#26426;&#29702;&#21644;&#30456;&#20851;&#26465;&#20214;&#65292;&#20026;&#19968;&#33268;&#20272;&#35745;&#25552;&#20379;&#20102;&#20445;&#35777;&#65292;&#24182;&#22312;IID&#26679;&#26412;&#19979;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;Transformer&#26550;&#26500;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#19968;&#32452;&#25552;&#31034;&#21644;&#19982;&#27169;&#22411;&#37319;&#26679;&#30340;&#20851;&#32852;&#36755;&#20986;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#21333;&#23618;&#33258;&#27880;&#24847;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#20043;&#38388;&#30340;&#31934;&#30830;&#26144;&#23556;&#65306;&#23558;&#25552;&#31034;&#36755;&#20837;&#27169;&#22411;&#20250;&#26681;&#25454;&#19978;&#19979;&#25991;&#26465;&#20214;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65288;CCMC&#65289;&#23545;&#36755;&#20986;&#26631;&#35760;&#36827;&#34892;&#37319;&#26679;&#65292;&#35813;&#38142;&#21152;&#26435;&#20102;&#22522;&#26412;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#36716;&#31227;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20301;&#32622;&#32534;&#30721;&#23548;&#33268;&#20102;&#36716;&#31227;&#27010;&#29575;&#30340;&#20301;&#32622;&#30456;&#20851;&#32553;&#25918;&#12290;&#22522;&#20110;&#36825;&#31181;&#24418;&#24335;&#20027;&#20041;&#65292;&#25105;&#20204;&#20026;&#25552;&#31034;&#20998;&#24067;&#24320;&#21457;&#20102;&#21487;&#36776;&#35782;&#24615;/&#35206;&#30422;&#26465;&#20214;&#65292;&#30830;&#20445;&#19968;&#33268;&#20272;&#35745;&#65292;&#24182;&#22312;IID&#26679;&#26412;&#19979;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#21333;&#20010;&#36755;&#20986;&#36712;&#36857;&#29983;&#25104;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13512v1 Announce Type: cross  Abstract: Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model. We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/coverage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from
&lt;/p&gt;</description></item><item><title>SimPro&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#65292;&#36890;&#36807;&#21019;&#26032;&#22320;&#25913;&#36827;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#65292;&#26126;&#30830;&#20998;&#31163;&#26465;&#20214;&#21644;&#36793;&#32536;&#31867;&#21035;&#20998;&#24067;&#30340;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.13505</link><description>&lt;p&gt;
SimPro&#65306;&#19968;&#20010;&#31616;&#21333;&#30340;&#27010;&#29575;&#26694;&#26550;&#23454;&#29616;&#36924;&#30495;&#30340;&#38271;&#23614;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13505
&lt;/p&gt;
&lt;p&gt;
SimPro&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#65292;&#36890;&#36807;&#21019;&#26032;&#22320;&#25913;&#36827;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#65292;&#26126;&#30830;&#20998;&#31163;&#26465;&#20214;&#21644;&#36793;&#32536;&#31867;&#21035;&#20998;&#24067;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#38598;&#20013;&#22312;&#35299;&#20915;&#19968;&#20010;&#26356;&#20026;&#36924;&#30495;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65306;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#21516;&#26102;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#31867;&#21035;&#20998;&#24067;&#26082;&#26410;&#30693;&#21448;&#21487;&#33021;&#19981;&#21305;&#37197;&#12290;&#24403;&#21069;&#36825;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#24448;&#24448;&#39044;&#35774;&#20102;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#31867;&#21035;&#20998;&#24067;&#30340;&#20005;&#26684;&#20551;&#35774;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#20165;&#36866;&#24212;&#20110;&#26576;&#20123;&#20998;&#24067;&#33539;&#22260;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#24230;&#36866;&#24212;&#24615;&#30340;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;SimPro&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20851;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#30340;&#39044;&#23450;&#20041;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24314;&#31435;&#22312;&#19968;&#20010;&#27010;&#29575;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#26126;&#30830;&#20998;&#31163;&#26465;&#20214;&#21644;&#36793;&#32536;&#31867;&#21035;&#20998;&#24067;&#30340;&#24314;&#27169;&#65292;&#21019;&#26032;&#22320;&#25913;&#36827;&#20102;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#12290;&#36825;&#31181;&#20998;&#31163;&#20419;&#36827;&#20102;&#22312;&#26368;&#22823;&#21270;&#36807;&#31243;&#20013;&#23545;&#31867;&#21035;&#20998;&#24067;&#36827;&#34892;&#20272;&#35745;&#30340;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13505v1 Announce Type: new  Abstract: Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization p
&lt;/p&gt;</description></item><item><title>HetTree&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#26641;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26500;&#24314;&#35821;&#20041;&#26641;&#25968;&#25454;&#32467;&#26500;&#25429;&#25417;&#20803;&#36335;&#24452;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#30340;&#24322;&#26500;&#22270;&#20013;&#30340;&#26641;&#24418;&#23618;&#27425;&#32467;&#26500;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13496</link><description>&lt;p&gt;
HetTree: &#24322;&#26500;&#26641;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HetTree: Heterogeneous Tree Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13496
&lt;/p&gt;
&lt;p&gt;
HetTree&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#26641;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26500;&#24314;&#35821;&#20041;&#26641;&#25968;&#25454;&#32467;&#26500;&#25429;&#25417;&#20803;&#36335;&#24452;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#30340;&#24322;&#26500;&#22270;&#20013;&#30340;&#26641;&#24418;&#23618;&#27425;&#32467;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36807;&#21435;&#30475;&#21040;&#20102;&#23545;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#22240;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#26159;&#24322;&#26500;&#30340;&#65292;&#20174;&#24341;&#29992;&#22270;&#21040;&#30005;&#23376;&#37038;&#20214;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#20803;&#36335;&#24452;&#20043;&#38388;&#30340;&#26641;&#24418;&#23618;&#27425;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#26159;&#30001;&#19981;&#21516;&#30340;&#33410;&#28857;&#31867;&#22411;&#21644;&#20851;&#31995;&#31867;&#22411;&#33258;&#28982;&#26500;&#25104;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HetTree&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#26641;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21487;&#25193;&#23637;&#19988;&#26377;&#25928;&#30340;&#26041;&#24335;&#24314;&#27169;&#22270;&#32467;&#26500;&#21644;&#24322;&#26500;&#26041;&#38754;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HetTree&#26500;&#24314;&#20102;&#19968;&#20010;&#35821;&#20041;&#26641;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25429;&#25417;&#20803;&#36335;&#24452;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#26641;&#24418;&#32534;&#30721;&#25216;&#26415;&#36890;&#36807;&#26681;&#25454;&#23376;&#33410;&#28857;&#19982;&#29238;&#33410;&#28857;&#30340;&#30456;&#20284;&#24615;&#26469;&#21152;&#26435;&#23376;&#33410;&#28857;&#30340;&#36129;&#29486;&#26469;&#32858;&#21512;&#23376;&#33410;&#28857;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26641;&#24418;&#32534;&#30721;&#26410;&#33021;&#25429;&#25417;&#25972;&#20010;&#29238;&#23376;&#23618;&#27425;&#32467;&#26500;&#65292;&#22240;&#20026;&#21482;&#32771;&#34385;&#20102;&#29238;&#33410;&#28857;&#12290;&#22240;&#27492;&#65292;HetTree&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23376;&#26641;&#27880;&#24847;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13496v1 Announce Type: new  Abstract: The recent past has seen an increasing interest in Heterogeneous Graph Neural Networks (HGNNs) since many real-world graphs are heterogeneous in nature, from citation graphs to email graphs. However, existing methods ignore a tree hierarchy among metapaths, which is naturally constituted by different node types and relation types. In this paper, we present HetTree, a novel heterogeneous tree graph neural network that models both the graph structure and heterogeneous aspects in a scalable and effective manner. Specifically, HetTree builds a semantic tree data structure to capture the hierarchy among metapaths. Existing tree encoding techniques aggregate children nodes by weighting the contribution of children nodes based on similarity to the parent node. However, we find that this tree encoding fails to capture the entire parent-children hierarchy by only considering the parent node. Hence, HetTree uses a novel subtree attention mechanism
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#38544;&#36523;&#23545;&#25163;&#25915;&#20987;&#65292;&#25351;&#20986;&#22823;&#37096;&#20998;&#29616;&#26377;&#25915;&#20987;&#23481;&#26131;&#34987;&#25552;&#20986;&#30340;&#26816;&#27979;&#26041;&#27861;&#21457;&#29616;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#38544;&#36523;&#25915;&#20987;&#27010;&#24565;&#24182;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20998;&#26512;&#21457;&#29616;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#25104;&#21151;&#21457;&#21160;&#38544;&#36523;&#25915;&#20987;&#65292;&#20026;MAB&#30340;&#23433;&#20840;&#39118;&#38505;&#24102;&#26469;&#26032;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.13487</link><description>&lt;p&gt;
&#38024;&#23545;&#38544;&#34109;&#24615;&#23545;&#31574;&#30340;&#38544;&#36523;&#23545;&#25163;&#25915;&#20987; &#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stealthy Adversarial Attacks on Stochastic Multi-Armed Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13487
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#38544;&#36523;&#23545;&#25163;&#25915;&#20987;&#65292;&#25351;&#20986;&#22823;&#37096;&#20998;&#29616;&#26377;&#25915;&#20987;&#23481;&#26131;&#34987;&#25552;&#20986;&#30340;&#26816;&#27979;&#26041;&#27861;&#21457;&#29616;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#38544;&#36523;&#25915;&#20987;&#27010;&#24565;&#24182;&#36827;&#34892;&#20102;&#25506;&#35752;&#65292;&#20998;&#26512;&#21457;&#29616;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#25104;&#21151;&#21457;&#21160;&#38544;&#36523;&#25915;&#20987;&#65292;&#20026;MAB&#30340;&#23433;&#20840;&#39118;&#38505;&#24102;&#26469;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#31639;&#27861;&#30340;&#23545;&#25163;&#24615;&#25915;&#20987;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#32858;&#28966;&#22312;&#22870;&#21169;&#25237;&#27602;&#25915;&#20987;&#65292;&#24182;&#21457;&#29616;&#22823;&#22810;&#25968;&#29616;&#26377;&#25915;&#20987;&#21487;&#20197;&#34987;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#22343;&#21248;&#24615;&#26816;&#27979;&#30340;&#26816;&#27979;&#26041;&#27861;&#36731;&#26131;&#21457;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#22870;&#21169;&#25805;&#32437;&#26041;&#38754;&#30340;&#25915;&#20987;&#24615;&#36739;&#24378;&#12290;&#36825;&#28608;&#21169;&#25105;&#20204;&#30740;&#31350;&#38024;&#23545;&#38543;&#26426;MAB&#30340;&#38544;&#36523;&#25915;&#20987;&#27010;&#24565;&#65292;&#24182;&#35843;&#26597;&#30001;&#27492;&#20135;&#29983;&#30340;&#25915;&#20987;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#38024;&#23545;&#20004;&#31181;&#24120;&#29992;&#30340;MAB&#31639;&#27861;UCB1&#21644;$\epsilon$-greedy&#65292;&#38544;&#36523;&#25915;&#20987;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#29615;&#22659;&#26465;&#20214;&#21644;&#31532;&#19968;&#36718;&#25289;&#21160;&#33218;&#25968;&#30340;&#23454;&#29616;&#22870;&#21169;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#19968;&#33324;MAB&#31639;&#27861;&#37197;&#22791;&#25105;&#20204;&#30340;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#30340;&#24773;&#20917;&#65292;&#21457;&#29616;&#20960;&#20046;&#24635;&#26159;&#33021;&#22815;&#36827;&#34892;&#19968;&#27425;&#25104;&#21151;&#30340;&#38544;&#36523;&#25915;&#20987;&#12290;&#36825;&#20026;MAB&#30340;&#23433;&#20840;&#39118;&#38505;&#24102;&#26469;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13487v1 Announce Type: new  Abstract: Adversarial attacks against stochastic multi-armed bandit (MAB) algorithms have been extensively studied in the literature. In this work, we focus on reward poisoning attacks and find most existing attacks can be easily detected by our proposed detection method based on the test of homogeneity, due to their aggressive nature in reward manipulations. This motivates us to study the notion of stealthy attack against stochastic MABs and investigate the resulting attackability. Our analysis shows that against two popularly employed MAB algorithms, UCB1 and $\epsilon$-greedy, the success of a stealthy attack depends on the environmental conditions and the realized reward of the arm pulled in the first round. We also analyze the situation for general MAB algorithms equipped with our attack detection method and find that it is possible to have a stealthy attack that almost always succeeds. This brings new insights into the security risks of MAB 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;ProPD&#65292;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#20196;&#29260;&#26641;&#20462;&#21098;&#21644;&#29983;&#25104;&#30340;&#39640;&#25928;LLM&#24182;&#34892;&#35299;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25552;&#21069;&#20462;&#21098;&#26426;&#21046;&#21644;&#21160;&#24577;&#20196;&#29260;&#26641;&#29983;&#25104;&#31639;&#27861;&#26469;&#25552;&#39640;&#39564;&#35777;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13485</link><description>&lt;p&gt;
ProPD&#65306;LLM&#24182;&#34892;&#35299;&#30721;&#30340;&#21160;&#24577;&#20196;&#29260;&#26641;&#20462;&#21098;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13485
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;ProPD&#65292;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#20196;&#29260;&#26641;&#20462;&#21098;&#21644;&#29983;&#25104;&#30340;&#39640;&#25928;LLM&#24182;&#34892;&#35299;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25552;&#21069;&#20462;&#21098;&#26426;&#21046;&#21644;&#21160;&#24577;&#20196;&#29260;&#26641;&#29983;&#25104;&#31639;&#27861;&#26469;&#25552;&#39640;&#39564;&#35777;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#21319;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25928;&#29575;&#21463;&#21040;&#33258;&#22238;&#24402;&#20196;&#29260;&#29983;&#25104;&#20013;&#22266;&#26377;&#38480;&#21046;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#24102;&#26377;&#20196;&#29260;&#26641;&#39564;&#35777;&#30340;&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#65292;&#20363;&#22914;Medusa&#65292;&#20197;&#25913;&#21892;&#35299;&#30721;&#24182;&#34892;&#24615;&#21644;&#25928;&#29575;&#65292;&#20294;&#30001;&#20110;&#20854;&#29420;&#31435;&#20196;&#29260;&#39044;&#27979;&#26041;&#27861;&#20197;&#21450;&#22823;&#26641;&#22823;&#23567;&#21644;&#25209;&#22788;&#29702;&#26102;&#20135;&#29983;&#30340;&#26174;&#33879;&#39564;&#35777;&#24320;&#38144;&#65292;&#23427;&#32463;&#24120;&#38590;&#20197;&#20445;&#25345;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ProPD&#65292;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#20196;&#29260;&#26641;&#20462;&#21098;&#21644;&#29983;&#25104;&#30340;&#39640;&#25928;LLM&#24182;&#34892;&#35299;&#30721;&#26694;&#26550;&#12290;ProPD&#20855;&#26377;&#19968;&#31181;&#20808;&#36827;&#30340;&#25552;&#21069;&#20462;&#21098;&#26426;&#21046;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#28040;&#38500;&#19981;&#22826;&#21487;&#33021;&#30340;&#20196;&#29260;&#24207;&#21015;&#20197;&#25552;&#39640;&#39564;&#35777;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#20196;&#29260;&#26641;&#29983;&#25104;&#31639;&#27861;&#26469;&#24179;&#34913;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13485v1 Announce Type: cross  Abstract: Recent advancements in generative large language models (LLMs) have significantly boosted the performance in natural language processing tasks. However, their efficiency is hampered by the inherent limitations in autoregressive token generation. While parallel decoding with token tree verification, e.g., Medusa, has been proposed to improve decoding parallelism and efficiency, it often struggles with maintaining contextual relationships due to its independent token prediction approach and incurs significant verification overhead, especially with large tree sizes and batch processing. In this paper, we propose ProPD, an efficient LLM parallel decoding framework based on dynamic token tree pruning and generation. ProPD features an advanced early pruning mechanism to efficiently eliminate unpromising token sequences to improve verification efficiency. Additionally, it introduces a dynamic token tree generation algorithm to balance the com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#39046;&#22495;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#30456;&#20851;&#31034;&#20363;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#29983;&#25104;&#26679;&#26412;&#19981;&#22815;&#29702;&#24819;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.13482</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#36164;&#28304;&#39046;&#22495;&#20219;&#21153;&#30340;&#26816;&#32034;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#39046;&#22495;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#30456;&#20851;&#31034;&#20363;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#29983;&#25104;&#26679;&#26412;&#19981;&#22815;&#29702;&#24819;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26679;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#20005;&#37325;&#19979;&#38477;&#12290;&#35768;&#22810;&#29616;&#26377;&#20316;&#21697;&#36890;&#36807;&#20174;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#65292;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#31181;&#23376;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#38750;&#24120;&#23569;&#65292;&#36825;&#20351;&#24471;&#29983;&#25104;&#30340;&#26679;&#26412;&#19981;&#22815;&#29702;&#24819;&#19988;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20854;&#20182;&#25968;&#25454;&#38598;&#20013;&#20016;&#23500;&#30340;&#31034;&#20363;&#19982;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19982;&#32473;&#23450;&#31181;&#23376;&#25968;&#25454;&#30456;&#20284;&#24615;&#22522;&#20110;&#20854;&#20182;&#25968;&#25454;&#38598;&#26816;&#32034;&#30456;&#20851;&#23454;&#20363;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#25110;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#25552;&#31034;LLM&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13482v1 Announce Type: cross  Abstract: Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information 
&lt;/p&gt;</description></item><item><title>STENCIL&#21033;&#29992;&#27425;&#27169;&#20114;&#20449;&#24687;&#36873;&#25321;&#24369;&#26631;&#35760;&#30340;&#31232;&#26377;&#31867;&#23454;&#20363;&#65292;&#24182;&#36890;&#36807;&#26631;&#27880;&#32773;&#24378;&#26631;&#35760;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#21644;&#31232;&#26377;&#31867;F-1&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.13468</link><description>&lt;p&gt;
STENCIL&#65306;&#22522;&#20110;&#27425;&#27169;&#20114;&#20449;&#24687;&#30340;&#20919;&#21551;&#21160;&#20027;&#21160;&#23398;&#20064;&#24369;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
STENCIL: Submodular Mutual Information Based Weak Supervision for Cold-Start Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13468
&lt;/p&gt;
&lt;p&gt;
STENCIL&#21033;&#29992;&#27425;&#27169;&#20114;&#20449;&#24687;&#36873;&#25321;&#24369;&#26631;&#35760;&#30340;&#31232;&#26377;&#31867;&#23454;&#20363;&#65292;&#24182;&#36890;&#36807;&#26631;&#27880;&#32773;&#24378;&#26631;&#35760;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#21644;&#31232;&#26377;&#31867;F-1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;NLP&#24212;&#29992;&#20013;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#38656;&#35201;&#26356;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#35745;&#25968;&#22686;&#21152;&#26102;&#12290;&#20027;&#21160;&#23398;&#20064;&#35797;&#22270;&#25366;&#25496;&#21644;&#27880;&#37322;&#26410;&#26631;&#35760;&#30340;&#23454;&#20363;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#24555;&#36895;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#65292;&#26159;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#30340;&#24120;&#35265;&#36873;&#25321;&#65307;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#35201;&#20040;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#21021;&#22987;&#26631;&#27880;&#25968;&#25454;&#65292;&#35201;&#20040;&#35201;&#27714;&#25913;&#36827;&#31232;&#26377;&#31867;&#20043;&#21069;&#38656;&#35201;&#22810;&#36718;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STENCIL&#65292;&#23427;&#21033;&#29992;&#19968;&#32452;&#25991;&#26412;&#31034;&#20363;&#21644;&#26368;&#36817;&#25552;&#20986;&#30340;&#27425;&#27169;&#20114;&#20449;&#24687;&#26469;&#36873;&#25321;&#19968;&#32452;&#24369;&#26631;&#35760;&#30340;&#31232;&#26377;&#31867;&#23454;&#20363;&#65292;&#28982;&#21518;&#30001;&#26631;&#27880;&#32773;&#23545;&#20854;&#36827;&#34892;&#24378;&#26631;&#35760;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;STENCIL&#22312;&#22810;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23558;&#25972;&#20307;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;10%-24%&#65292;&#23558;&#31232;&#26377;&#31867;F-1&#20998;&#25968;&#25552;&#39640;&#20102;17%-40%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13468v1 Announce Type: cross  Abstract: As supervised fine-tuning of pre-trained models within NLP applications increases in popularity, larger corpora of annotated data are required, especially with increasing parameter counts in large language models. Active learning, which attempts to mine and annotate unlabeled instances to improve model performance maximally fast, is a common choice for reducing the annotation cost; however, most methods typically ignore class imbalance and either assume access to initial annotated data or require multiple rounds of active learning selection before improving rare classes. We present STENCIL, which utilizes a set of text exemplars and the recently proposed submodular mutual information to select a set of weakly labeled rare-class instances that are then strongly labeled by an annotator. We show that STENCIL improves overall accuracy by $10\%-24\%$ and rare-class F-1 score by $17\%-40\%$ on multiple text classification datasets over commo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#25915;&#20987;LLMs&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26799;&#24230;&#24341;&#23548;&#21518;&#38376;&#35302;&#21457;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#25104;&#21151;&#22320;&#30772;&#22351;&#27169;&#22411;&#36755;&#20986;&#65292;&#20165;&#25913;&#21464;1%&#30340;&#25351;&#23548;&#35843;&#20248;&#26679;&#26412;&#21363;&#21487;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#29575;&#36798;&#21040;&#32422;80&#65285;&#12290;</title><link>https://arxiv.org/abs/2402.13459</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#25351;&#23548;&#35843;&#20248;&#26399;&#38388;&#25805;&#32437;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning to Poison Large Language Models During Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13459
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#25915;&#20987;LLMs&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26799;&#24230;&#24341;&#23548;&#21518;&#38376;&#35302;&#21457;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#25104;&#21151;&#22320;&#30772;&#22351;&#27169;&#22411;&#36755;&#20986;&#65292;&#20165;&#25913;&#21464;1%&#30340;&#25351;&#23548;&#35843;&#20248;&#26679;&#26412;&#21363;&#21487;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#29575;&#36798;&#21040;&#32422;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#37325;&#22823;&#31361;&#30772;&#12290;&#34429;&#28982;&#23427;&#20204;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;LLMs&#38754;&#20020;&#30528;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#30340;&#28431;&#27934;&#65292;&#20854;&#20013;&#23545;&#25163;&#23558;&#21518;&#38376;&#35302;&#21457;&#22120;&#25554;&#20837;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#25805;&#32437;&#36755;&#20986;&#20197;&#36827;&#34892;&#24694;&#24847;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#65292;&#26088;&#22312;&#21033;&#29992;&#25351;&#23548;&#35843;&#20248;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#35782;&#21035;LLMs&#20013;&#30340;&#39069;&#22806;&#23433;&#20840;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26799;&#24230;&#24341;&#23548;&#21518;&#38376;&#35302;&#21457;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#35782;&#21035;&#25932;&#23545;&#35302;&#21457;&#22120;&#65292;&#30830;&#20445;&#23545;&#20256;&#32479;&#38450;&#24481;&#25163;&#27573;&#30340;&#35268;&#36991;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;LLMs&#21644;&#20219;&#21153;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#34920;&#26126;&#22312;&#30772;&#22351;&#27169;&#22411;&#36755;&#20986;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#25104;&#21151;&#29575;&#65307;&#20165;&#23545;4,000&#20010;&#25351;&#23548;&#35843;&#20248;&#26679;&#26412;&#20013;&#30340;1&#65285;&#36827;&#34892;&#27880;&#20837;&#23601;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#29575;&#65288;PDR&#65289;&#32422;&#20026;80&#65285;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39640;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13459v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has marked significant achievements in language processing and reasoning capabilities. Despite their advancements, LLMs face vulnerabilities to data poisoning attacks, where adversaries insert backdoor triggers into training data to manipulate outputs for malicious purposes. This work further identifies additional security risks in LLMs by designing a new data poisoning attack tailored to exploit the instruction tuning process. We propose a novel gradient-guided backdoor trigger learning approach to identify adversarial triggers efficiently, ensuring an evasion of detection by conventional defenses while maintaining content integrity. Through experimental validation across various LLMs and tasks, our strategy demonstrates a high success rate in compromising model outputs; poisoning only 1\% of 4,000 instruction tuning samples leads to a Performance Drop Rate (PDR) of around 80\%. Our work high
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#23548;&#19982;&#30456;&#20851;&#24615;&#21644;&#35206;&#30422;&#24615;&#30456;&#20851;&#30340;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#30028;&#38480;&#65292;&#20026;&#23376;&#27169;&#20114;&#20449;&#24687;&#22312;&#30446;&#26631;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#20013;&#30340;&#34920;&#29616;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;</title><link>https://arxiv.org/abs/2402.13454</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#30340;&#23376;&#27169;&#20449;&#24687;&#37327;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Theoretical Analysis of Submodular Information Measures for Targeted Data Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13454
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#23548;&#19982;&#30456;&#20851;&#24615;&#21644;&#35206;&#30422;&#24615;&#30456;&#20851;&#30340;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#30028;&#38480;&#65292;&#20026;&#23376;&#27169;&#20114;&#20449;&#24687;&#22312;&#30446;&#26631;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#20013;&#30340;&#34920;&#29616;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#22686;&#21152;&#65292;&#23450;&#20301;&#29305;&#23450;&#25968;&#25454;&#23376;&#38598;&#30340;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#20102;&#24110;&#21161;&#23454;&#29616;&#36825;&#19968;&#33021;&#21147;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;&#23376;&#27169;&#20114;&#20449;&#24687;&#65288;SMI&#65289;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#26377;&#25928;&#24212;&#29992;&#20110;&#25191;&#34892;&#20351;&#29992;&#31034;&#20363;&#26597;&#35810;&#38598;&#36827;&#34892;&#23450;&#20301;&#23376;&#38598;&#36873;&#25321;&#30340;&#22810;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#24037;&#20316;&#37117;&#27809;&#26377;&#22312;&#29702;&#35770;&#19978;&#20445;&#35777;SMI&#23545;&#20110;&#23376;&#38598;&#30456;&#20851;&#24615;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#35206;&#30422;&#24615;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#39318;&#27425;&#36890;&#36807;&#25512;&#23548;&#19982;&#30456;&#20851;&#24615;&#21644;&#30446;&#26631;&#25968;&#25454;&#35206;&#30422;&#30456;&#20851;&#30340;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#30028;&#38480;&#65292;&#25552;&#20379;&#20102;&#27492;&#31867;&#20445;&#35777;&#12290;&#36890;&#36807;&#36825;&#20123;&#30028;&#38480;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#24050;&#32463;&#34920;&#29616;&#25104;&#21151;&#30340;SMI&#20989;&#25968;&#22312;&#29702;&#35770;&#19978;&#30830;&#20445;&#23454;&#29616;&#33391;&#22909;&#30340;&#26597;&#35810;&#30456;&#20851;&#24615;&#21644;&#26597;&#35810;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13454v1 Announce Type: new  Abstract: With increasing volume of data being used across machine learning tasks, the capability to target specific subsets of data becomes more important. To aid in this capability, the recently proposed Submodular Mutual Information (SMI) has been effectively applied across numerous tasks in literature to perform targeted subset selection with the aid of a exemplar query set. However, all such works are deficient in providing theoretical guarantees for SMI in terms of its sensitivity to a subset's relevance and coverage of the targeted data. For the first time, we provide such guarantees by deriving similarity-based bounds on quantities related to relevance and coverage of the targeted data. With these bounds, we show that the SMI functions, which have empirically shown success in multiple applications, are theoretically sound in achieving good query relevance and query coverage.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#26694;&#26550;LocalHealth&#65292;&#29992;&#20110;&#39044;&#27979;&#24403;&#22320;&#31934;&#31070;&#20581;&#24247;&#32467;&#26524;&#12290;&#36890;&#36807;&#19982;GPT3.5&#32467;&#21512;&#20351;&#29992;&#65292;&#35813;&#26694;&#26550;&#22312;MH&#30417;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.13452</link><description>&lt;p&gt;
&#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#31934;&#31070;&#20581;&#24247;&#30417;&#27979;&#26694;&#26550;&#65306;&#20174;&#24403;&#22320;&#25512;&#25991;&#21040;&#24403;&#22320;&#20581;&#24247;
&lt;/p&gt;
&lt;p&gt;
LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#26694;&#26550;LocalHealth&#65292;&#29992;&#20110;&#39044;&#27979;&#24403;&#22320;&#31934;&#31070;&#20581;&#24247;&#32467;&#26524;&#12290;&#36890;&#36807;&#19982;GPT3.5&#32467;&#21512;&#20351;&#29992;&#65292;&#35813;&#26694;&#26550;&#22312;MH&#30417;&#27979;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;Twitter&#25968;&#25454;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20379;&#20102;&#23427;&#22312;&#24320;&#21457;&#34917;&#20805;&#20581;&#24247;&#30417;&#27979;&#31995;&#32479;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#35777;&#25454;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#30417;&#27979;&#20844;&#20849;&#20581;&#24247;&#65292;&#37325;&#28857;&#20851;&#27880;&#31934;&#31070;&#20581;&#24247;&#65288;MH&#65289;&#32467;&#26524;&#12290;&#25105;&#20204;&#20551;&#35774;&#24403;&#22320;&#21457;&#24067;&#30340;&#25512;&#25991;&#21487;&#20197;&#34920;&#26126;&#24403;&#22320;&#30340;&#31934;&#31070;&#20581;&#24247;&#32467;&#26524;&#65292;&#24182;&#25910;&#38598;&#20102;&#26469;&#33258;&#32654;&#22269;765&#20010;&#22320;&#21306;&#65288;&#20154;&#21475;&#26222;&#26597;&#20998;&#32452;&#65289;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#22320;&#21306;&#30340;&#36825;&#20123;&#25512;&#25991;&#19982;&#30142;&#30149;&#25511;&#21046;&#20013;&#24515;&#65288;CDC&#65289;&#25253;&#21578;&#30340;&#30456;&#24212;MH&#32467;&#26524;&#37197;&#23545;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;LocalTweets&#12290;&#20511;&#21161;LocalTweets&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Twitter&#30340;MH&#30417;&#27979;&#31995;&#32479;&#30340;&#39318;&#20010;&#20154;&#21475;&#32423;&#35780;&#20272;&#20219;&#21153;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#39640;&#25928;&#26377;&#25928;&#30340;&#26041;&#27861;LocalHealth&#65292;&#29992;&#20110;&#26681;&#25454;LocalTweets&#39044;&#27979;MH&#32467;&#26524;&#12290;&#24403;&#19982;GPT3.5&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;LocalHealth&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;F1&#20540;&#21644;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#36798;&#21040;0.7429&#21644;79.78\%&#65292;F1&#20540;&#25552;&#39640;&#20102;59\%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13452v1 Announce Type: cross  Abstract: Prior research on Twitter (now X) data has provided positive evidence of its utility in developing supplementary health surveillance systems. In this study, we present a new framework to surveil public health, focusing on mental health (MH) outcomes. We hypothesize that locally posted tweets are indicative of local MH outcomes and collect tweets posted from 765 neighborhoods (census block groups) in the USA. We pair these tweets from each neighborhood with the corresponding MH outcome reported by the Center for Disease Control (CDC) to create a benchmark dataset, LocalTweets. With LocalTweets, we present the first population-level evaluation task for Twitter-based MH surveillance systems. We then develop an efficient and effective method, LocalHealth, for predicting MH outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\% improvement in F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24613;&#35786;&#31185;&#20013;&#20943;&#23569;&#31561;&#24453;&#26102;&#38388;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;ED-Copilot&#31995;&#32479;&#26469;&#25512;&#33616;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.13448</link><description>&lt;p&gt;
ED-Copilot: &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35786;&#26029;&#36741;&#21161;&#20943;&#23569;&#24613;&#35786;&#31185;&#31561;&#24453;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24613;&#35786;&#31185;&#20013;&#20943;&#23569;&#31561;&#24453;&#26102;&#38388;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;ED-Copilot&#31995;&#32479;&#26469;&#25512;&#33616;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24613;&#35786;&#31185;&#65288;ED&#65289;&#20013;&#65292;&#24739;&#32773;&#22312;&#35786;&#26029;&#21069;&#38656;&#35201;&#36827;&#34892;&#20998;&#35786;&#21644;&#22810;&#31181;&#23454;&#39564;&#23460;&#26816;&#27979;&#12290;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#65292;&#23548;&#33268;&#24613;&#35786;&#31185;&#25317;&#25380;&#65292;&#26174;&#33879;&#24433;&#21709;&#24739;&#32773;&#27515;&#20129;&#29575;&#12289;&#21307;&#30103;&#38169;&#35823;&#12289;&#20154;&#21592;&#26543;&#31469;&#31561;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#65288;&#26102;&#38388;&#65289;&#25104;&#26412;&#26377;&#25928;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21327;&#21161;&#24613;&#35786;&#31185;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#35786;&#26029;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#25105;&#20204;&#19982;&#24613;&#35786;&#31185;&#20020;&#24202;&#21307;&#29983;&#21512;&#20316;&#31574;&#21010;&#20102;MIMIC-ED-Assist&#65292;&#36825;&#26159;&#19968;&#20010;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#24314;&#35758;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#24613;&#35786;&#31561;&#24453;&#26102;&#38388;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#65292;&#24182;&#22312;&#27491;&#30830;&#39044;&#27979;&#35832;&#22914;&#27515;&#20129;&#20043;&#31867;&#20851;&#38190;&#32467;&#26524;&#26041;&#38754;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;ED-Copilot&#65292;&#23427;&#20381;&#27425;&#24314;&#35758;&#24739;&#32773;&#29305;&#23450;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;ED-Copilot&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#24739;&#32773;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#24182;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13448v1 Announce Type: cross  Abstract: In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaCKD&#30340;&#27169;&#24335;&#32858;&#31867;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#21387;&#32553;&#20869;&#23384;&#35775;&#38382;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20869;&#23384;&#35775;&#38382;&#24207;&#21015;&#32858;&#31867;&#24182;&#35757;&#32451;&#27169;&#24335;&#29305;&#23450;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#26368;&#32456;&#35757;&#32451;&#20986;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.13441</link><description>&lt;p&gt;
PaCKD: &#27169;&#24335;&#32858;&#31867;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#21387;&#32553;&#20869;&#23384;&#35775;&#38382;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PaCKD: Pattern-Clustered Knowledge Distillation for Compressing Memory Access Prediction Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13441
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaCKD&#30340;&#27169;&#24335;&#32858;&#31867;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#21387;&#32553;&#20869;&#23384;&#35775;&#38382;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20869;&#23384;&#35775;&#38382;&#24207;&#21015;&#32858;&#31867;&#24182;&#35757;&#32451;&#27169;&#24335;&#29305;&#23450;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#26368;&#32456;&#35757;&#32451;&#20986;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#29992;&#20110;&#20934;&#30830;&#30340;&#20869;&#23384;&#35775;&#38382;&#39044;&#27979;&#65288;MAP&#65289;&#30340;&#26377;&#25928;&#27169;&#22411;&#65292;&#36825;&#26159;&#36890;&#36807;&#25968;&#25454;&#39044;&#21462;&#26469;&#32531;&#35299;&#20869;&#23384;&#24310;&#36831;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;DNN&#30340;MAP&#27169;&#22411;&#23384;&#22312;&#35832;&#22914;&#26174;&#33879;&#30340;&#29289;&#29702;&#23384;&#20648;&#31354;&#38388;&#21644;&#25512;&#29702;&#24310;&#36831;&#19981;&#20339;&#31561;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;&#36825;&#20123;&#38480;&#21046;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaCKD&#30340;&#27169;&#24335;&#32858;&#31867;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#21387;&#32553;MAP&#27169;&#22411;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;PaCKD&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#23558;&#20869;&#23384;&#35775;&#38382;&#24207;&#21015;&#32858;&#31867;&#21040;&#28041;&#21450;&#30456;&#20284;&#27169;&#24335;&#30340;&#19981;&#21516;&#20998;&#21306;&#20013;&#65292;&#20026;&#27599;&#20010;&#20998;&#21306;&#35757;&#32451;&#22823;&#22411;&#27169;&#24335;&#29305;&#23450;&#30340;&#29992;&#20110;&#20869;&#23384;&#35775;&#38382;&#39044;&#27979;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#20197;&#21450;&#36890;&#36807;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#24335;&#29305;&#23450;&#25945;&#24072;&#37027;&#37324;&#25552;&#28860;&#30693;&#35782;&#26469;&#35757;&#32451;&#21333;&#20010;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13441v1 Announce Type: new  Abstract: Deep neural networks (DNNs) have proven to be effective models for accurate Memory Access Prediction (MAP), a critical task in mitigating memory latency through data prefetching. However, existing DNN-based MAP models suffer from the challenges such as significant physical storage space and poor inference latency, primarily due to their large number of parameters. These limitations render them impractical for deployment in real-world scenarios. In this paper, we propose PaCKD, a Pattern-Clustered Knowledge Distillation approach to compress MAP models while maintaining the prediction performance. The PaCKD approach encompasses three steps: clustering memory access sequences into distinct partitions involving similar patterns, training large pattern-specific teacher models for memory access prediction for each partition, and training a single lightweight student model by distilling the knowledge from the trained pattern-specific teachers. 
&lt;/p&gt;</description></item><item><title>&#23558;&#23398;&#20064;&#26816;&#32034;&#25216;&#26415;&#24212;&#29992;&#20110;&#25552;&#21319;&#39046;&#33521;&#30340;&#24037;&#20316;&#25628;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#35780;&#20272;&#27714;&#32844;&#32773;&#36164;&#26684;&#30340;&#22270;&#34920;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#38142;&#25509;&#36827;&#34892;&#26816;&#32034;&#65292;&#20197;&#25552;&#39640;&#30003;&#35831;&#32773;&#36136;&#37327;&#21644;&#20248;&#21270;&#27714;&#32844;&#32773;&#21442;&#19982;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13435</link><description>&lt;p&gt;
&#23398;&#20064;&#26816;&#32034;&#29992;&#20110;&#24037;&#20316;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Learning to Retrieve for Job Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13435
&lt;/p&gt;
&lt;p&gt;
&#23558;&#23398;&#20064;&#26816;&#32034;&#25216;&#26415;&#24212;&#29992;&#20110;&#25552;&#21319;&#39046;&#33521;&#30340;&#24037;&#20316;&#25628;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#26500;&#24314;&#35780;&#20272;&#27714;&#32844;&#32773;&#36164;&#26684;&#30340;&#22270;&#34920;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#38142;&#25509;&#36827;&#34892;&#26816;&#32034;&#65292;&#20197;&#25552;&#39640;&#30003;&#35831;&#32773;&#36136;&#37327;&#21644;&#20248;&#21270;&#27714;&#32844;&#32773;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Web&#35268;&#27169;&#25628;&#32034;&#31995;&#32479;&#36890;&#24120;&#36890;&#36807;&#20004;&#27493;&#39588;&#33539;&#24335;&#26469;&#35299;&#20915;&#21487;&#20280;&#32553;&#24615;&#25361;&#25112;&#65306;&#26816;&#32034;&#21644;&#25490;&#21517;&#12290;&#26816;&#32034;&#27493;&#39588;&#65292;&#20063;&#31216;&#20026;&#20505;&#36873;&#36873;&#25321;&#65292;&#36890;&#24120;&#28041;&#21450;&#25552;&#21462;&#26631;&#20934;&#21270;&#23454;&#20307;&#65292;&#21019;&#24314;&#21453;&#21521;&#32034;&#24341;&#65292;&#24182;&#25191;&#34892;&#26816;&#32034;&#30340;&#26415;&#35821;&#21305;&#37197;&#12290;&#36825;&#31181;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;&#26597;&#35810;&#27169;&#22411;&#24320;&#21457;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;&#23398;&#20064;&#26816;&#32034;&#25216;&#26415;&#24212;&#29992;&#20110;&#25552;&#21319;&#39046;&#33521;&#30340;&#24037;&#20316;&#25628;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#12290;&#22312;&#25512;&#24191;&#24037;&#20316;&#39046;&#22495;&#65292;&#20851;&#38190;&#30446;&#26631;&#26159;&#25552;&#39640;&#30003;&#35831;&#32773;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#20026;&#25307;&#32856;&#23458;&#25143;&#25552;&#20379;&#20215;&#20540;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#30830;&#35748;&#30340;&#38599;&#20323;&#25968;&#25454;&#26500;&#24314;&#19968;&#20010;&#35780;&#20272;&#27714;&#32844;&#32773;&#23545;&#24037;&#20316;&#36164;&#26684;&#30340;&#22270;&#34920;&#65292;&#24182;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#38142;&#25509;&#36827;&#34892;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#23398;&#20064;&#27169;&#22411;&#26131;&#20110;&#35299;&#37322;&#65292;&#35843;&#35797;&#21644;&#35843;&#25972;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26377;&#26426;&#24037;&#20316;&#30340;&#37325;&#28857;&#26159;&#20248;&#21270;&#27714;&#32844;&#32773;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13435v1 Announce Type: cross  Abstract: Web-scale search systems typically tackle the scalability challenge with a two-step paradigm: retrieval and ranking. The retrieval step, also known as candidate selection, often involves extracting standardized entities, creating an inverted index, and performing term matching for retrieval. Such traditional methods require manual and time-consuming development of query models. In this paper, we discuss applying learning-to-retrieve technology to enhance LinkedIns job search and recommendation systems. In the realm of promoted jobs, the key objective is to improve the quality of applicants, thereby delivering value to recruiter customers. To achieve this, we leverage confirmed hire data to construct a graph that evaluates a seeker's qualification for a job, and utilize learned links for retrieval. Our learned model is easy to explain, debug, and adjust. On the other hand, the focus for organic jobs is to optimize seeker engagement. We 
&lt;/p&gt;</description></item><item><title>DrBenchmark&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#24357;&#34917;&#23545;&#26368;&#26032;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#35780;&#20272;&#30340;&#19981;&#36275;&#65292;&#24182;&#32771;&#34385;&#21040;&#27861;&#35821;&#30340;&#29420;&#29305;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13432</link><description>&lt;p&gt;
DrBenchmark: &#19968;&#20010;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13432
&lt;/p&gt;
&lt;p&gt;
DrBenchmark&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#24357;&#34917;&#23545;&#26368;&#26032;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#35780;&#20272;&#30340;&#19981;&#36275;&#65292;&#24182;&#32771;&#34385;&#21040;&#27861;&#35821;&#30340;&#29420;&#29305;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#35780;&#20272;&#21327;&#35758;&#30340;&#21464;&#21270;&#65292;&#27604;&#36739;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#20010;&#20844;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#32858;&#21512;&#21040;&#19968;&#20010;&#22522;&#20934;&#20013;&#65292;&#20801;&#35768;&#20174;&#21508;&#31181;&#35282;&#24230;&#35780;&#20272;PLMs&#30340;&#20869;&#22312;&#21697;&#36136;&#12290;&#23613;&#31649;&#36825;&#19968;&#20513;&#35758;&#20173;&#28982;&#23616;&#38480;&#20110;&#23569;&#25968;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#33521;&#35821;&#21644;&#20013;&#25991;&#65292;&#20294;&#24050;&#32463;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23637;&#24320;&#12290;&#36825;&#19968;&#38480;&#21046;&#38459;&#30861;&#20102;&#23545;&#26368;&#26032;&#30340;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#30340;&#35780;&#20215;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#22312;&#23569;&#37327;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#19988;&#20351;&#29992;&#30340;&#21327;&#35758;&#19981;&#22815;&#26631;&#20934;&#21270;&#65292;&#35201;&#20040;&#20351;&#29992;&#19968;&#33324;&#30340;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#30740;&#31350;&#24046;&#36317;&#65292;&#24182;&#32771;&#34385;&#21040;&#27861;&#35821;&#30340;&#29420;&#29305;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13432v1 Announce Type: cross  Abstract: The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with pre-trained language models (PLMs). However, comparing these models has proven challenging due to variations in evaluation protocols across different models. A fair solution is to aggregate diverse downstream tasks into a benchmark, allowing for the assessment of intrinsic PLMs qualities from various perspectives. Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese. This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks. To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biom
&lt;/p&gt;</description></item><item><title>LinkSAGE&#26159;&#19968;&#20010;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#24037;&#20316;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#35757;&#32451;&#21644;&#26381;&#21153;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#39046;&#33521;&#19987;&#19994;&#32593;&#32476;&#20013;&#36827;&#34892;&#20010;&#24615;&#21270;&#24037;&#20316;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.13430</link><description>&lt;p&gt;
LinkSAGE: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#24037;&#20316;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
LinkSAGE: Optimizing Job Matching Using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13430
&lt;/p&gt;
&lt;p&gt;
LinkSAGE&#26159;&#19968;&#20010;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#24037;&#20316;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#35757;&#32451;&#21644;&#26381;&#21153;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#39046;&#33521;&#19987;&#19994;&#32593;&#32476;&#20013;&#36827;&#34892;&#20010;&#24615;&#21270;&#24037;&#20316;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LinkSAGE&#65292;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#38598;&#25104;&#21040;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#24037;&#20316;&#21305;&#37197;&#31995;&#32479;&#20013;&#65292;&#26088;&#22312;&#24212;&#23545;&#39046;&#33521;&#24222;&#22823;&#19987;&#19994;&#32593;&#32476;&#30340;&#22797;&#26434;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#24037;&#20316;&#24066;&#22330;&#22270;&#65292;&#36825;&#26159;&#24037;&#19994;&#30028;&#35268;&#27169;&#26368;&#22823;&#12289;&#26368;&#22797;&#26434;&#30340;&#22270;&#20043;&#19968;&#65292;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#33410;&#28857;&#21644;&#36793;&#12290;&#36825;&#20010;&#22270;&#19981;&#20165;&#24191;&#27867;&#65292;&#32780;&#19988;&#35814;&#32454;&#20016;&#23500;&#65292;&#21253;&#21547;&#20250;&#21592;&#21644;&#24037;&#20316;&#33410;&#28857;&#20197;&#21450;&#20851;&#38190;&#23646;&#24615;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#24191;&#38420;&#32780;&#20132;&#32455;&#30340;&#32593;&#32476;&#12290;LinkSAGE&#30340;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#20854;&#35757;&#32451;&#21644;&#26381;&#21153;&#26041;&#27861;&#65292;&#23427;&#26377;&#25928;&#22320;&#23558;&#24863;&#30693;&#22270;&#23398;&#20064;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;GNN&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#35757;&#32451;GNN&#27169;&#22411;&#19982;&#29616;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#20998;&#31163;&#65292;&#28040;&#38500;&#20102;&#39057;&#32321;&#37325;&#26032;&#35757;&#32451;GNN&#30340;&#38656;&#35201;&#65292;&#21516;&#26102;&#20445;&#25345;&#22270;&#20449;&#21495;&#26368;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13430v1 Announce Type: cross  Abstract: We present LinkSAGE, an innovative framework that integrates Graph Neural Networks (GNNs) into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIns extensive professional network. Our approach capitalizes on a novel job marketplace graph, the largest and most intricate of its kind in industry, with billions of nodes and edges. This graph is not merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network. A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive graph learning on a heterogeneous, evolving graph with an encoder-decoder GNN model. This methodology decouples the training of the GNN model from that of existing Deep Neural Nets (DNN) models, eliminating the need for frequent GNN retraining while maintaining up-to-date graph signals in ne
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#25972;&#20010;&#20998;&#24067;&#22312;&#22238;&#24402;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#26469;&#33258;&#20110;&#20248;&#21270;&#30340;&#25913;&#36827;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.13425</link><description>&lt;p&gt;
&#22312;&#22238;&#24402;&#20013;&#25506;&#35752;&#30452;&#26041;&#22270;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Investigating the Histogram Loss in Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13425
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25972;&#20010;&#20998;&#24067;&#22312;&#22238;&#24402;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#26469;&#33258;&#20110;&#20248;&#21270;&#30340;&#25913;&#36827;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#26159;&#65292;&#22312;&#22238;&#24402;&#20013;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#25972;&#20010;&#20998;&#24067;&#65292;&#21363;&#20351;&#21482;&#38656;&#35201;&#22343;&#20540;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290; &#36825;&#31181;&#39069;&#22806;&#30340;&#24314;&#27169;&#36890;&#24120;&#20250;&#24102;&#26469;&#24615;&#33021;&#22686;&#30410;&#65292;&#20294;&#32972;&#21518;&#30340;&#21407;&#22240;&#23578;&#19981;&#23436;&#20840;&#28165;&#26970;&#12290; &#26412;&#25991;&#30740;&#31350;&#20102;&#22238;&#24402;&#20013;&#30340;&#19968;&#31181;&#26368;&#26032;&#26041;&#27861;&#65292;&#21363;&#30452;&#26041;&#22270;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#20998;&#24067;&#21644;&#28789;&#27963;&#30452;&#26041;&#22270;&#39044;&#27979;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#26469;&#23398;&#20064;&#30446;&#26631;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290; &#25105;&#20204;&#35774;&#35745;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20026;&#20160;&#20040;&#20197;&#21450;&#20309;&#26102;&#20250;&#20986;&#29616;&#24615;&#33021;&#22686;&#30410;&#65292;&#20197;&#21450;&#25439;&#22833;&#30340;&#19981;&#21516;&#32452;&#20214;&#22914;&#20309;&#20026;&#27492;&#20570;&#20986;&#36129;&#29486;&#12290; &#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#23398;&#20064;&#20998;&#24067;&#30340;&#22909;&#22788;&#26469;&#33258;&#20110;&#20248;&#21270;&#30340;&#25913;&#36827;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#26041;&#22270;&#25439;&#22833;&#22312;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13425v1 Announce Type: cross  Abstract: It is becoming increasingly common in regression to train neural networks that model the entire distribution even if only the mean is required for prediction. This additional modeling often comes with performance gain and the reasons behind the improvement are not fully known. This paper investigates a recent approach to regression, the Histogram Loss, which involves learning the conditional distribution of the target variable by minimizing the cross-entropy between a target distribution and a flexible histogram prediction. We design theoretical and empirical analyses to determine why and when this performance gain appears, and how different components of the loss contribute to it. Our results suggest that the benefits of learning distributions in this setup come from improvements in optimization rather than learning a better representation. We then demonstrate the viability of the Histogram Loss in common deep learning applications wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#31181;&#32771;&#34385;&#36710;&#36742;&#12289;&#39550;&#39542;&#21592;&#21644;&#29615;&#22659;&#25968;&#25454;&#30340;&#20840;&#26032;&#22810;&#31867;&#39550;&#39542;&#21592;&#20998;&#24515;&#39118;&#38505;&#35780;&#20272;&#65288;MDDRA&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#28789;&#27963;&#35843;&#25972;&#21442;&#25968;&#21644;&#26435;&#37325;&#20197;&#32771;&#34385;&#19981;&#21516;&#20005;&#37325;&#32423;&#21035;&#30340;&#20107;&#20214;&#65292;&#20174;&#32780;&#20943;&#23569;&#30001;&#39550;&#39542;&#21592;&#20998;&#24515;&#24341;&#36215;&#30340;&#36947;&#36335;&#20107;&#25925;&#12290;</title><link>https://arxiv.org/abs/2402.13421</link><description>&lt;p&gt;
&#39550;&#39542;&#21592;&#20998;&#24515;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#23450;&#37327;&#39118;&#38505;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Quantitative Risk Assessment Machine Learning Model for Drivers Distraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13421
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#31181;&#32771;&#34385;&#36710;&#36742;&#12289;&#39550;&#39542;&#21592;&#21644;&#29615;&#22659;&#25968;&#25454;&#30340;&#20840;&#26032;&#22810;&#31867;&#39550;&#39542;&#21592;&#20998;&#24515;&#39118;&#38505;&#35780;&#20272;&#65288;MDDRA&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#28789;&#27963;&#35843;&#25972;&#21442;&#25968;&#21644;&#26435;&#37325;&#20197;&#32771;&#34385;&#19981;&#21516;&#20005;&#37325;&#32423;&#21035;&#30340;&#20107;&#20214;&#65292;&#20174;&#32780;&#20943;&#23569;&#30001;&#39550;&#39542;&#21592;&#20998;&#24515;&#24341;&#36215;&#30340;&#36947;&#36335;&#20107;&#25925;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#38505;&#32531;&#35299;&#25216;&#26415;&#23545;&#36991;&#20813;&#19982;&#39550;&#39542;&#34892;&#20026;&#30456;&#20851;&#30340;&#20107;&#25925;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#29420;&#21019;&#30340;&#22810;&#31867;&#39550;&#39542;&#21592;&#20998;&#24515;&#39118;&#38505;&#35780;&#20272;&#65288;MDDRA&#65289;&#27169;&#22411;&#65292;&#22312;&#26053;&#31243;&#20013;&#32771;&#34385;&#20102;&#36710;&#36742;&#12289;&#39550;&#39542;&#21592;&#21644;&#29615;&#22659;&#25968;&#25454;&#12290;MDDRA&#23558;&#39550;&#39542;&#21592;&#22312;&#39118;&#38505;&#30697;&#38453;&#20013;&#20998;&#31867;&#20026;&#23433;&#20840;&#12289;&#31895;&#24515;&#25110;&#21361;&#38505;&#65292;&#21487;&#28789;&#27963;&#35843;&#25972;&#21442;&#25968;&#21644;&#26435;&#37325;&#20197;&#32771;&#34385;&#27599;&#20010;&#20107;&#20214;&#22312;&#29305;&#23450;&#20005;&#37325;&#32423;&#21035;&#19978;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20351;&#29992;Field Operation Test&#65288;TeleFOT&#65289;&#25910;&#38598;&#20102;&#22312;&#33521;&#22269;&#19996;&#31859;&#24503;&#20848;&#22320;&#21306;&#20351;&#29992;&#30456;&#21516;&#36335;&#32447;&#30340;&#39550;&#39542;&#21592;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;&#32467;&#26524;&#26174;&#31034;&#20943;&#23569;&#30001;&#39550;&#39542;&#21592;&#20998;&#24515;&#24341;&#36215;&#30340;&#36947;&#36335;&#20107;&#25925;&#26159;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20998;&#24515;&#65288;&#39550;&#39542;&#21592;&#12289;&#36710;&#36742;&#21644;&#29615;&#22659;&#65289;&#19982;&#22522;&#20110;&#25345;&#32493;&#20998;&#24515;&#20005;&#37325;&#24471;&#20998;&#30340;&#20998;&#31867;&#20005;&#37325;&#31243;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#39550;&#39542;&#21592;&#20998;&#24515;&#36827;&#34892;&#20998;&#31867;&#21644;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13421v1 Announce Type: new  Abstract: Risk mitigation techniques are critical to avoiding accidents associated with driving behaviour. We provide a novel Multi-Class Driver Distraction Risk Assessment (MDDRA) model that considers the vehicle, driver, and environmental data during a journey. MDDRA categorises the driver on a risk matrix as safe, careless, or dangerous. It offers flexibility in adjusting the parameters and weights to consider each event on a specific severity level. We collect real-world data using the Field Operation Test (TeleFOT), covering drivers using the same routes in the East Midlands, United Kingdom (UK). The results show that reducing road accidents caused by driver distraction is possible. We also study the correlation between distraction (driver, vehicle, and environment) and the classification severity based on a continuous distraction severity score. Furthermore, we apply machine learning techniques to classify and predict driver distraction acco
&lt;/p&gt;</description></item><item><title>EvolMPNN&#36890;&#36807;&#36827;&#21270;&#24863;&#30693;&#30340;&#26041;&#24335;&#25429;&#25417;&#34507;&#30333;&#36136;&#31361;&#21464;&#23545;&#20110;&#38170;&#23450;&#34507;&#30333;&#36136;&#30340;&#24433;&#21709;&#65292;&#24182;&#26368;&#32456;&#29983;&#25104;&#32508;&#21512;&#34507;&#30333;&#36136;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.13418</link><description>&lt;p&gt;
EvolMPNN&#65306;&#36890;&#36807;&#36827;&#21270;&#32534;&#30721;&#39044;&#27979;&#21516;&#28304;&#34507;&#30333;&#36136;&#30340;&#31361;&#21464;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
EvolMPNN: Predicting Mutational Effect on Homologous Proteins by Evolution Encoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13418
&lt;/p&gt;
&lt;p&gt;
EvolMPNN&#36890;&#36807;&#36827;&#21270;&#24863;&#30693;&#30340;&#26041;&#24335;&#25429;&#25417;&#34507;&#30333;&#36136;&#31361;&#21464;&#23545;&#20110;&#38170;&#23450;&#34507;&#30333;&#36136;&#30340;&#24433;&#21709;&#65292;&#24182;&#26368;&#32456;&#29983;&#25104;&#32508;&#21512;&#34507;&#30333;&#36136;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#34507;&#30333;&#36136;&#23646;&#24615;&#23545;&#29983;&#29289;&#21644;&#21307;&#23398;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#34507;&#30333;&#24037;&#31243;&#36890;&#36807;&#23545;&#20856;&#22411;&#34507;&#30333;&#36136;&#65288;&#31216;&#20026;&#37326;&#29983;&#22411;&#65289;&#36827;&#34892;&#31361;&#21464;&#65292;&#26500;&#24314;&#21516;&#28304;&#34507;&#30333;&#36136;&#23478;&#26063;&#24182;&#30740;&#31350;&#20854;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#23481;&#26131;&#24573;&#30053;&#32454;&#24494;&#30340;&#31361;&#21464;&#65292;&#26080;&#27861;&#25429;&#25417;&#34507;&#30333;&#36136;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvolMPNN&#65292;&#19968;&#31181;&#20855;&#26377;&#36827;&#21270;&#24863;&#30693;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23398;&#20064;&#36827;&#21270;&#24863;&#30693;&#30340;&#34507;&#30333;&#36136;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13418v1 Announce Type: new  Abstract: Predicting protein properties is paramount for biological and medical advancements. Current protein engineering mutates on a typical protein, called the wild-type, to construct a family of homologous proteins and study their properties. Yet, existing methods easily neglect subtle mutations, failing to capture the effect on the protein properties. To this end, we propose EvolMPNN, Evolution-aware Message Passing Neural Network, to learn evolution-aware protein embeddings. EvolMPNN samples sets of anchor proteins, computes evolutionary information by means of residues and employs a differentiable evolution-aware aggregation scheme over these sampled anchors. This way EvolMPNNcan capture the mutation effect on proteins with respect to the anchor proteins. Afterwards, the aggregated evolution-aware embeddings are integrated with sequence embeddings to generate final comprehensive protein embeddings. Our model shows up to 6.4% better than sta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550; LlmCorr&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;LLM&#21487;&#20197;&#20316;&#20026;&#20107;&#21518;&#26657;&#27491;&#22120;&#65292;&#20026;&#20219;&#24847;ML&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20986;&#20462;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.13414</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20316;&#20107;&#21518;&#26657;&#27491;&#22120;
&lt;/p&gt;
&lt;p&gt;
Harnessing Large Language Models as Post-hoc Correctors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13414
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550; LlmCorr&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;LLM&#21487;&#20197;&#20316;&#20026;&#20107;&#21518;&#26657;&#27491;&#22120;&#65292;&#20026;&#20219;&#24847;ML&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20986;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#38271;&#24182;&#38656;&#27714;&#26356;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#19982;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21644;&#24494;&#35843;&#30456;&#20851;&#30340;&#36153;&#29992;&#27491;&#22312;&#36805;&#36895;&#22686;&#21152;&#12290;&#21463;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#30340;&#20196;&#20154;&#30633;&#30446;&#25104;&#23601;&#21551;&#21457;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;LLMs&#33021;&#21542;&#20197;&#26497;&#20302;&#25104;&#26412;&#26377;&#25928;&#22320;&#25913;&#21892;ML&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550; LlmCorr&#65292;&#19968;&#20010;LLM&#21487;&#20197;&#20316;&#20026;&#20107;&#21518;&#26657;&#27491;&#22120;&#65292;&#20026;&#20219;&#24847;ML&#27169;&#22411;&#30340;&#39044;&#27979;&#25552;&#20986;&#20462;&#27491;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#25972;&#21512;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#20449;&#24687;&#21644;ML&#27169;&#22411;&#23545;&#39564;&#35777;&#38598;&#30340;&#39044;&#27979;&#26469;&#24418;&#25104;&#19968;&#20010;&#19978;&#19979;&#25991;&#30693;&#35782;&#25968;&#25454;&#24211;&#12290;&#21033;&#29992;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#35201;&#27714;LLM&#24635;&#32467;ML&#27169;&#22411;&#29359;&#38169;&#35823;&#30340;&#23454;&#20363;&#20197;&#21450;&#20027;&#35201;&#39044;&#27979;&#19982;&#30495;&#23454;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#38543;&#21518;&#65292;LLM&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13414v1 Announce Type: cross  Abstract: As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can tr
&lt;/p&gt;</description></item><item><title>&#26032;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25193;&#23637;&#29289;&#29702;&#20449;&#24687;&#30340;&#30828;&#32422;&#26463;&#65292;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#30340;&#24314;&#27169;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13412</link><description>&lt;p&gt;
&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#25193;&#23637;&#29289;&#29702;&#20449;&#24687;&#30340;&#30828;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Scaling physics-informed hard constraints with mixture-of-experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13412
&lt;/p&gt;
&lt;p&gt;
&#26032;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25193;&#23637;&#29289;&#29702;&#20449;&#24687;&#30340;&#30828;&#32422;&#26463;&#65292;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#30340;&#24314;&#27169;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#24378;&#21152;&#24050;&#30693;&#30340;&#29289;&#29702;&#32422;&#26463;&#65292;&#27604;&#22914;&#23432;&#24658;&#23450;&#24459;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#24402;&#32435;&#20559;&#24046;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#25311;&#29289;&#29702;&#21160;&#24577;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#25910;&#25947;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;&#34429;&#28982;&#36825;&#20123;&#32422;&#26463;&#21487;&#20197;&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#24809;&#32602;&#36719;&#24615;&#22320;&#24378;&#21152;&#65292;&#20294;&#26368;&#36817;&#19981;&#21516;iable&#29289;&#29702;&#21644;&#20248;&#21270;&#30340;&#36827;&#23637;&#36890;&#36807;&#23558;PDE&#32422;&#26463;&#20248;&#21270;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#29420;&#23618;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#36825;&#20351;&#24471;&#23545;&#29289;&#29702;&#32422;&#26463;&#30340;&#36981;&#23432;&#26356;&#21152;&#20005;&#26684;&#12290;&#28982;&#32780;&#65292;&#24378;&#21152;&#30828;&#32422;&#26463;&#26174;&#33879;&#22686;&#21152;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22797;&#26434;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#38656;&#35201;&#22312;&#32593;&#26684;&#20013;&#30340;&#22823;&#37327;&#28857;&#19978;&#27714;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#34920;&#31034;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#32422;&#26463;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#23454;&#26045;&#30828;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13412v1 Announce Type: cross  Abstract: Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks. This enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint. To address this challenge, we develop a scalable approach to enforce hard 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#23558;&#21508;&#31181;&#24418;&#24335;&#30340;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20808;&#39564;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#31526;&#21512;&#39046;&#22495;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#21518;&#39564;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.13410</link><description>&lt;p&gt;
&#20855;&#26377;&#39046;&#22495;&#30693;&#35782;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Networks with Domain Knowledge Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13410
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#23558;&#21508;&#31181;&#24418;&#24335;&#30340;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20808;&#39564;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#31526;&#21512;&#39046;&#22495;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#21518;&#39564;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#37327;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20026;BNNs&#25351;&#23450;&#33021;&#22815;&#25429;&#25417;&#30456;&#20851;&#39046;&#22495;&#30693;&#35782;&#30340;&#20808;&#39564;&#24448;&#24448;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#23558;&#21508;&#31181;&#24418;&#24335;&#30340;&#39046;&#22495;&#30693;&#35782;&#65288;&#21363;&#21487;&#20197;&#29992;&#25439;&#22833;&#20989;&#25968;&#34920;&#31034;&#30340;&#20219;&#20309;&#30693;&#35782;&#65289;&#25972;&#21512;&#21040;BNN&#20808;&#39564;&#20013;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#30340;&#21518;&#39564;&#25512;&#26029;&#21644;&#25277;&#26679;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#23545;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;&#20808;&#39564;&#20998;&#37197;&#39640;&#27010;&#29575;&#36136;&#37327;&#32473;&#26356;&#31526;&#21512;&#25105;&#20204;&#39046;&#22495;&#30693;&#35782;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#23548;&#33268;&#21518;&#39564;&#26679;&#26412;&#20063;&#34920;&#29616;&#20986;&#36825;&#31181;&#34892;&#20026;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39046;&#22495;&#30693;&#35782;&#20808;&#39564;&#30340;BNNs&#20248;&#20110;&#20855;&#26377;&#26631;&#20934;&#20808;&#39564;&#65288;&#20363;&#22914;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#12289;&#39640;&#26031;&#36807;&#31243;&#65289;&#30340;&#27169;&#22411;&#65292;&#22312;&#25104;&#21151;&#25972;&#21512;&#22810;&#31181;&#31867;&#22411;&#30340;&#20808;&#39564;&#20449;&#24687;&#65288;&#20363;&#22914;&#20844;&#24179;&#24615;&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13410v1 Announce Type: new  Abstract: Bayesian neural networks (BNNs) have recently gained popularity due to their ability to quantify model uncertainty. However, specifying a prior for BNNs that captures relevant domain knowledge is often extremely challenging. In this work, we propose a framework for integrating general forms of domain knowledge (i.e., any knowledge that can be represented by a loss function) into a BNN prior through variational inference, while enabling computationally efficient posterior inference and sampling. Specifically, our approach results in a prior over neural network weights that assigns high probability mass to models that better align with our domain knowledge, leading to posterior samples that also exhibit this behavior. We show that BNNs using our proposed domain knowledge priors outperform those with standard priors (e.g., isotropic Gaussian, Gaussian process), successfully incorporating diverse types of prior information such as fairness, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#38750;&#20132;&#20114;&#24335;&#21644;&#20132;&#20114;&#24335;&#22810;&#20445;&#30495;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#21152;&#36895;&#29289;&#29702;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25506;&#32034;&#22810;&#32500;&#38750;&#21487;&#24494;&#21442;&#25968;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#22914;&#30456;&#22270;&#21644;&#32452;&#25104;&#31354;&#38388;&#31561;&#65292;&#36816;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.13402</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#20132;&#20114;&#24335;&#21644;&#20132;&#20114;&#24335;&#22810;&#20445;&#30495;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#21152;&#36895;&#29289;&#29702;&#21457;&#29616;&#65306;&#24403;&#21069;&#25361;&#25112;&#19982;&#26410;&#26469;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Towards accelerating physical discovery via non-interactive and interactive multi-fidelity Bayesian Optimization: Current challenges and future opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#38750;&#20132;&#20114;&#24335;&#21644;&#20132;&#20114;&#24335;&#22810;&#20445;&#30495;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#21152;&#36895;&#29289;&#29702;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25506;&#32034;&#22810;&#32500;&#38750;&#21487;&#24494;&#21442;&#25968;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#22914;&#30456;&#22270;&#21644;&#32452;&#25104;&#31354;&#38388;&#31561;&#65292;&#36816;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#21644;&#23454;&#39564;&#26448;&#26009;&#21457;&#29616;&#37117;&#38754;&#20020;&#30528;&#25506;&#32034;&#22810;&#32500;&#19988;&#36890;&#24120;&#38750;&#21487;&#24494;&#21442;&#25968;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#20855;&#26377;&#22810;&#20010;&#30456;&#20114;&#20316;&#29992;&#30340;&#21704;&#23494;&#39039;&#37327;&#30340;&#30456;&#22270;&#12289;&#32452;&#21512;&#22270;&#20070;&#39302;&#30340;&#32452;&#25104;&#31354;&#38388;&#12289;&#21152;&#24037;&#31354;&#38388;&#21644;&#20998;&#23376;&#23884;&#20837;&#31354;&#38388;&#31561;&#12290;&#36890;&#24120;&#36825;&#20123;&#31995;&#32479;&#35780;&#20272;&#21333;&#20010;&#23454;&#20363;&#30340;&#25104;&#26412;&#26114;&#36149;&#25110;&#32791;&#26102;&#65292;&#22240;&#27492;&#22522;&#20110;&#31351;&#20030;&#32593;&#26684;&#25110;&#38543;&#26426;&#25628;&#32034;&#30340;&#32463;&#20856;&#26041;&#27861;&#36807;&#20110;&#25968;&#25454;&#23494;&#38598;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#36125;&#21494;&#26031;&#20248;&#21270;&#65289;&#30340;&#27987;&#21402;&#20852;&#36259;&#65292;&#20854;&#20013;&#33258;&#36866;&#24212;&#25506;&#32034;&#22522;&#20110;&#20154;&#31867;&#23398;&#20064;&#65288;&#21457;&#29616;&#65289;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#36125;&#21494;&#26031;&#20248;&#21270;&#22522;&#20110;&#39044;&#23450;&#20041;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#31574;&#30053;&#32431;&#31929;&#30001;&#25968;&#25454;&#39537;&#21160;&#12290;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#20197;&#37096;&#20998;&#24050;&#30693;&#29289;&#29702;&#23450;&#24459;&#30340;&#24418;&#24335;&#25552;&#20986;&#23545;&#31995;&#32479;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13402v1 Announce Type: new  Abstract: Both computational and experimental material discovery bring forth the challenge of exploring multidimensional and often non-differentiable parameter spaces, such as phase diagrams of Hamiltonians with multiple interactions, composition spaces of combinatorial libraries, processing spaces, and molecular embedding spaces. Often these systems are expensive or time-consuming to evaluate a single instance, and hence classical approaches based on exhaustive grid or random search are too data intensive. This resulted in strong interest towards active learning methods such as Bayesian optimization (BO) where the adaptive exploration occurs based on human learning (discovery) objective. However, classical BO is based on a predefined optimization target, and policies balancing exploration and exploitation are purely data driven. In practical settings, the domain expert can pose prior knowledge on the system in form of partially known physics laws
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#20803;&#21644;&#22810;&#31867;&#21035;&#35774;&#32622;&#19979;&#30340;&#33258;&#20027;&#23398;&#20064;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32500;&#24230;$SDdim$&#26469;&#31934;&#30830;&#21051;&#30011;&#20219;&#20309;&#27010;&#24565;&#31867;&#21035;&#30340;&#33258;&#20027;&#23398;&#20064;&#38169;&#35823;&#19978;&#30028;&#65292;&#24182;&#21033;&#29992;&#8220;&#26631;&#35760;&#28216;&#25103;&#8221;&#36827;&#34892;&#35299;&#37322;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20363;&#23376;&#20013;&#30340;&#35745;&#31639;&#32467;&#26524;&#21644;&#23545;&#33258;&#20027;&#23398;&#20064;&#30340;&#23398;&#20064;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.13400</link><description>&lt;p&gt;
&#33258;&#20027;&#23398;&#20064;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
The Dimension of Self-Directed Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#20803;&#21644;&#22810;&#31867;&#21035;&#35774;&#32622;&#19979;&#30340;&#33258;&#20027;&#23398;&#20064;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32500;&#24230;$SDdim$&#26469;&#31934;&#30830;&#21051;&#30011;&#20219;&#20309;&#27010;&#24565;&#31867;&#21035;&#30340;&#33258;&#20027;&#23398;&#20064;&#38169;&#35823;&#19978;&#30028;&#65292;&#24182;&#21033;&#29992;&#8220;&#26631;&#35760;&#28216;&#25103;&#8221;&#36827;&#34892;&#35299;&#37322;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20363;&#23376;&#20013;&#30340;&#35745;&#31639;&#32467;&#26524;&#21644;&#23545;&#33258;&#20027;&#23398;&#20064;&#30340;&#23398;&#20064;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#33258;&#20027;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#26159;&#33258;1990&#24180;&#20195;&#21021;&#20197;&#26469;&#21560;&#24341;&#22312;&#32447;&#23398;&#20064;&#29702;&#35770;&#31038;&#21306;&#20851;&#27880;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#23398;&#20064;&#32773;&#34987;&#20801;&#35768;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#19979;&#19968;&#20010;&#25968;&#25454;&#28857;&#26469;&#36827;&#34892;&#39044;&#27979;&#65292;&#19982;&#23545;&#25239;&#24615;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#19981;&#21516;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#20803;&#21644;&#22810;&#31867;&#21035;&#35774;&#32622;&#19979;&#30340;&#33258;&#20027;&#23398;&#20064;&#22797;&#26434;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#32500;&#24230;&#65292;&#21363;$SDdim$&#65292;&#31934;&#30830;&#22320;&#21051;&#30011;&#20102;&#20219;&#20309;&#27010;&#24565;&#31867;&#21035;&#30340;&#33258;&#20027;&#23398;&#20064;&#38169;&#35823;&#19978;&#30028;&#12290;$SDdim$&#32972;&#21518;&#30340;&#30452;&#35273;&#21487;&#20197;&#29702;&#35299;&#20026;&#19968;&#20010;&#31216;&#20026;&#8220;&#26631;&#35760;&#28216;&#25103;&#8221;&#30340;&#21452;&#20154;&#28216;&#25103;&#12290;&#21033;&#29992;&#36825;&#20010;&#21452;&#20154;&#28216;&#25103;&#65292;&#25105;&#20204;&#23545;&#35768;&#22810;&#20363;&#23376;&#36827;&#34892;&#20102;$SDdim$&#30340;&#35745;&#31639;&#65292;&#29305;&#21035;&#26159;&#22312;&#36724;&#23545;&#40784;&#30697;&#24418;&#12289;VC&#32500;&#25968;&#20026;$1$&#30340;&#31867;&#21035;&#21644;&#32447;&#24615;&#20998;&#38548;&#22120;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#20851;&#20110;&#33258;&#20027;&#23398;&#20064;&#30340;&#23398;&#20064;&#24046;&#36317;&#65292;&#37325;&#28857;&#20851;&#27880;&#33258;&#20027;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13400v1 Announce Type: cross  Abstract: Understanding the self-directed learning complexity has been an important problem that has captured the attention of the online learning theory community since the early 1990s. Within this framework, the learner is allowed to adaptively choose its next data point in making predictions unlike the setting in adversarial online learning.   In this paper, we study the self-directed learning complexity in both the binary and multi-class settings, and we develop a dimension, namely $SDdim$, that exactly characterizes the self-directed learning mistake-bound for any concept class. The intuition behind $SDdim$ can be understood as a two-player game called the "labelling game". Armed with this two-player game, we calculate $SDdim$ on a whole host of examples with notable results on axis-aligned rectangles, VC dimension $1$ classes, and linear separators. We demonstrate several learnability gaps with a central focus on self-directed learning and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27010;&#29575;&#22635;&#20805;&#25935;&#24863;&#29305;&#24449;&#65292;&#32852;&#21512;&#23398;&#20064;&#32676;&#20307;&#26465;&#20214;&#24615;&#32570;&#22833;&#27010;&#29575;&#65292;&#22686;&#24378;&#19968;&#33324;&#20844;&#24179;&#39118;&#38505;&#65292;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#25913;&#36827;&#24179;&#34913;</title><link>https://arxiv.org/abs/2402.13393</link><description>&lt;p&gt;
&#38024;&#23545;&#32676;&#20307;&#26465;&#20214;&#24615;&#32570;&#22833;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#20844;&#24179;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Fairness Risks for Group-conditionally Missing Demographics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13393
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#22635;&#20805;&#25935;&#24863;&#29305;&#24449;&#65292;&#32852;&#21512;&#23398;&#20064;&#32676;&#20307;&#26465;&#20214;&#24615;&#32570;&#22833;&#27010;&#29575;&#65292;&#22686;&#24378;&#19968;&#33324;&#20844;&#24179;&#39118;&#38505;&#65292;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#25913;&#36827;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#20998;&#31867;&#27169;&#22411;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#22240;&#20026;&#23545;&#26576;&#20123;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#30340;&#27495;&#35270;&#38382;&#39064;&#26085;&#30410;&#24341;&#36215;&#25285;&#24551;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#35201;&#27714;&#23436;&#20840;&#20102;&#35299;&#25935;&#24863;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#30001;&#20110;&#38544;&#31169;&#12289;&#27861;&#24459;&#38382;&#39064;&#21644;&#20010;&#20154;&#23545;&#27495;&#35270;&#30340;&#24656;&#24807;&#32780;&#19981;&#20999;&#23454;&#38469;&#12290;&#25105;&#20204;&#23558;&#35299;&#20915;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#19981;&#21487;&#29992;&#24615;&#30340;&#32676;&#20307;&#20381;&#36182;&#24615;&#65292;&#20363;&#22914;&#65292;&#26576;&#20123;&#24180;&#40836;&#33539;&#22260;&#30340;&#20154;&#21487;&#33021;&#26356;&#19981;&#24895;&#36879;&#38706;&#20182;&#20204;&#30340;&#24180;&#40836;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23545;&#25935;&#24863;&#29305;&#24449;&#36827;&#34892;&#27010;&#29575;&#22635;&#20805;&#65292;&#21516;&#26102;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#32852;&#21512;&#23398;&#20064;&#32676;&#20307;&#26465;&#20214;&#24615;&#32570;&#22833;&#30340;&#27010;&#29575;&#65292;&#23558;&#19968;&#33324;&#20844;&#24179;&#39118;&#38505;&#19982;&#20043;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#25913;&#36827;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13393v1 Announce Type: new  Abstract: Fairness-aware classification models have gained increasing attention in recent years as concerns grow on discrimination against some demographic groups. Most existing models require full knowledge of the sensitive features, which can be impractical due to privacy, legal issues, and an individual's fear of discrimination. The key challenge we will address is the group dependency of the unavailability, e.g., people of some age range may be more reluctant to reveal their age. Our solution augments general fairness risks with probabilistic imputations of the sensitive features, while jointly learning the group-conditionally missing probabilities in a variational auto-encoder. Our model is demonstrated effective on both image and tabular datasets, achieving an improved balance between accuracy and fairness.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21152;&#36895;&#20855;&#26377;RoPE&#30340;transformer&#25512;&#26029;&#30340;&#25216;&#24039;&#65292;&#36890;&#36807;&#39044;&#35745;&#31639;&#31532;&#19968;&#23618;&#26469;&#38477;&#20302;&#24310;&#36831;&#21644;&#25104;&#26412;&#65292;&#26368;&#22823;&#33410;&#30465;&#21462;&#20915;&#20110;&#24635;&#23618;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.13388</link><description>&lt;p&gt;
Transformer &#25216;&#24039;&#65306;&#39044;&#35745;&#31639;&#31532;&#19968;&#23618;
&lt;/p&gt;
&lt;p&gt;
Transformer tricks: Precomputing the first layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21152;&#36895;&#20855;&#26377;RoPE&#30340;transformer&#25512;&#26029;&#30340;&#25216;&#24039;&#65292;&#36890;&#36807;&#39044;&#35745;&#31639;&#31532;&#19968;&#23618;&#26469;&#38477;&#20302;&#24310;&#36831;&#21644;&#25104;&#26412;&#65292;&#26368;&#22823;&#33410;&#30465;&#21462;&#20915;&#20110;&#24635;&#23618;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31616;&#30701;&#30340;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#21152;&#36895;&#20855;&#26377; RoPE&#65288;&#22914; LLaMA&#12289;Mistral &#21644; PaLM&#65289;&#30340; transformer &#25512;&#26029;&#30340;&#25216;&#24039;&#12290;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#31532;&#19968;&#20010; transformer &#23618;&#30340;&#22823;&#37096;&#20998;&#20869;&#23481;&#21487;&#20197;&#39044;&#20808;&#35745;&#31639;&#65292;&#20174;&#32780;&#23548;&#33268;&#31245;&#20302;&#30340;&#24310;&#36831;&#21644;&#26356;&#20302;&#30340;&#27599;&#20196;&#29260;&#25104;&#26412;&#12290;&#22240;&#20026;&#36825;&#31181;&#25216;&#24039;&#20165;&#20248;&#21270;&#20102;&#19968;&#23618;&#65292;&#30456;&#23545;&#33410;&#30465;&#21462;&#20915;&#20110;&#24635;&#23618;&#25968;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#21482;&#26377; 4 &#23618;&#30340;&#27169;&#22411;&#65288;&#22914; Whisper tiny&#65289;&#65292;&#26368;&#22823;&#33410;&#30465;&#20165;&#38480;&#20110; 25%&#65292;&#32780;&#23545;&#20110; 32 &#23618;&#27169;&#22411;&#65288;&#22914; Mistral-7B&#65289;&#65292;&#33410;&#30465;&#21017;&#26159; 3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13388v1 Announce Type: new  Abstract: This short paper describes a trick to speed up inference of transformers with RoPE (such as LLaMA, Mistral, and PaLM). For these models, a large portion of the first transformer layer can be precomputed, which results in slightly lower latency and lower cost-per-token. Because this trick optimizes only one layer, the relative savings depend on the total number of layers. For example, the maximum savings for a model with only 4 layers (such as Whisper tiny) is limited to 25%, while a 32-layer model (such as Mistral-7B) is limited to 3% savings.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#39318;&#27425;&#37319;&#29992;&#21464;&#21387;&#22120;&#39044;&#27979;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;CPLEX&#21644;LSTM&#12290;</title><link>https://arxiv.org/abs/2402.13380</link><description>&lt;p&gt;
&#36808;&#21521;&#21464;&#21387;&#22120;&#65306;&#29992;&#21464;&#21387;&#22120;&#24443;&#24213;&#25913;&#21464;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#39318;&#27425;&#37319;&#29992;&#21464;&#21387;&#22120;&#39044;&#27979;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;CPLEX&#21644;LSTM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#23481;&#37327;&#38480;&#21046;&#25209;&#37327;&#29983;&#20135;&#38382;&#39064;&#65288;CLSP&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#21033;&#29992;&#21464;&#21387;&#22120;&#26469;&#39044;&#27979;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#22788;&#29702;&#39034;&#24207;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#38750;&#24120;&#36866;&#21512;&#39044;&#27979;&#27599;&#20010;CLSP&#21608;&#26399;&#20013;&#34920;&#31034;&#29983;&#20135;&#35774;&#32622;&#20915;&#31574;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#12290;&#36825;&#20010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#21160;&#24577;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;CLSP&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#21518;&#22788;&#29702;&#21464;&#21387;&#22120;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;CPLEX&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13380v1 Announce Type: new  Abstract: In this study, we introduce an innovative deep learning framework that employs a transformer model to address the challenges of mixed-integer programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP). Our approach, to our knowledge, is the first to utilize transformers to predict the binary variables of a mixed-integer programming (MIP) problem. Specifically, our approach harnesses the encoder decoder transformer's ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the CLSP. This problem is inherently dynamic, and we need to handle sequential decision making under constraints. We present an efficient algorithm in which CLSP solutions are learned through a transformer neural network. The proposed post-processed transformer algorithm surpasses the state-of-the-art solver, CPLEX and Long Short-Term Memory (LSTM) in solution time
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#20301;&#20803;&#35009;&#21028;&#65288;Meta-Ref&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#35757;&#32451;&#26679;&#26412;&#30340;&#23398;&#20064;&#29575;&#65292;&#20197;&#22312;&#19981;&#21516;&#22320;&#28857;&#20043;&#38388;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#25351;&#23548;&#65292;&#20174;&#32780;&#28040;&#38500;&#22320;&#28857;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.13379</link><description>&lt;p&gt;
&#24555;&#36895;&#35843;&#25972;&#23450;&#20301;&#20844;&#24179;&#24615;&#30340;&#35009;&#21028;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Referee-Meta-Learning for Fast Adaptation of Locational Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13379
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#20301;&#20803;&#35009;&#21028;&#65288;Meta-Ref&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#35757;&#32451;&#26679;&#26412;&#30340;&#23398;&#20064;&#29575;&#65292;&#20197;&#22312;&#19981;&#21516;&#22320;&#28857;&#20043;&#38388;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#25351;&#23548;&#65292;&#20174;&#32780;&#28040;&#38500;&#22320;&#28857;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#22320;&#28857;&#30340;&#25968;&#25454;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24448;&#24448;&#26174;&#31034;&#20986;&#23545;&#26576;&#20123;&#22320;&#28857;&#30340;&#38544;&#24335;&#20559;&#22909;&#65292;&#36825;&#26500;&#25104;&#20102;&#30772;&#22351;&#31639;&#27861;&#31354;&#38388;&#20844;&#24179;&#24615;&#30340;&#20559;&#35265;&#65292;&#36825;&#31181;&#19981;&#20844;&#24179;&#24456;&#23481;&#26131;&#22312;&#23454;&#36341;&#20013;&#24191;&#27867;&#37319;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#26102;&#24341;&#20837;&#20559;&#35265;&#36827;&#32780;&#24433;&#21709;&#21518;&#32493;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#20013;&#20851;&#20110;&#22320;&#28857;&#20559;&#35265;&#30340;&#30740;&#31350;&#22823;&#22810;&#34987;&#20302;&#20272;&#20102;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#20110;&#22320;&#28857;&#30340;&#20559;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#20301;&#20803;&#35009;&#21028;&#65288;Meta-Ref&#65289;&#26469;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#20803;&#35757;&#32451;&#21644;&#20803;&#27979;&#35797;&#12290;Meta-Ref&#21160;&#24577;&#35843;&#25972;&#20102;&#32473;&#23450;&#22320;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#23398;&#20064;&#29575;&#65292;&#36890;&#36807;&#26126;&#30830;&#32771;&#34385;&#22320;&#28857;&#20559;&#35265;&#21644;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#26469;&#25552;&#20513;&#36328;&#22320;&#28857;&#30340;&#20844;&#24179;&#24615;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#26469;&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#39044;&#27979;&#22120;&#21644;&#19968;&#20010;&#31649;&#29702;&#30340;&#25972;&#21512;Meta-Ref&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13379v1 Announce Type: new  Abstract: When dealing with data from distinct locations, machine learning algorithms tend to demonstrate an implicit preference of some locations over the others, which constitutes biases that sabotage the spatial fairness of the algorithm. This unfairness can easily introduce biases in subsequent decision-making given broad adoptions of learning-based solutions in practice. However, locational biases in AI are largely understudied. To mitigate biases over locations, we propose a locational meta-referee (Meta-Ref) to oversee the few-shot meta-training and meta-testing of a deep neural network. Meta-Ref dynamically adjusts the learning rates for training samples of given locations to advocate a fair performance across locations, through an explicit consideration of locational biases and the characteristics of input data. We present a three-phase training framework to learn both a meta-learning-based predictor and an integrated Meta-Ref that govern
&lt;/p&gt;</description></item><item><title>FIDLAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#26368;&#20248;&#27946;&#27700;&#31649;&#29702;&#65292;&#20934;&#30830;&#36827;&#34892;&#27700;&#21069;&#37322;&#25918;&#12290;</title><link>https://arxiv.org/abs/2402.13371</link><description>&lt;p&gt;
FIDLAR: &#27700;&#28798;&#20943;&#28798;&#30340;&#39044;&#27979;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
FIDLAR: Forecast-Informed Deep Learning Architecture for Flood Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13371
&lt;/p&gt;
&lt;p&gt;
FIDLAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#26368;&#20248;&#27946;&#27700;&#31649;&#29702;&#65292;&#20934;&#30830;&#36827;&#34892;&#27700;&#21069;&#37322;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27839;&#28023;&#27827;&#27969;&#31995;&#32479;&#20013;&#65292;&#39057;&#32321;&#21457;&#29983;&#30340;&#27946;&#27700;&#24448;&#24448;&#22312;&#22823;&#39118;&#26292;&#25110;&#28385;&#28526;&#26102;&#21457;&#29983;&#65292;&#23545;&#29983;&#21629;&#21644;&#36130;&#20135;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22312;&#26530;&#32445;&#32467;&#26500;(&#22914;&#27700;&#22365;&#12289;&#38392;&#38376;&#12289;&#27893;&#31449;&#21644;&#27700;&#24211;)&#22312;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#21069;&#36827;&#34892;&#25112;&#30053;&#24615;&#22320;&#37322;&#25918;&#27700;&#65292;&#36825;&#20123;&#27946;&#27700;&#21487;&#20197;&#24471;&#21040;&#20943;&#36731;&#29978;&#33267;&#39044;&#38450;&#12290;&#24403;&#22320;&#27700;&#21033;&#31649;&#29702;&#26426;&#26500;&#36890;&#24120;&#20351;&#29992;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#8220;&#22522;&#20110;&#35268;&#21017;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21382;&#21490;&#21644;&#32463;&#36807;&#26102;&#38388;&#39564;&#35777;&#30340;&#20154;&#31867;&#32463;&#39564;&#25351;&#23450;&#39044;&#20808;&#37322;&#25918;&#27700;&#65292;&#20294;&#24448;&#24448;&#23548;&#33268;&#36807;&#37327;&#25110;&#19981;&#36275;&#30340;&#27700;&#37327;&#37322;&#25918;&#12290;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#26159;&#19968;&#20010;&#26367;&#20195;&#26041;&#27861;&#65292;&#26159;&#22522;&#20110;&#29289;&#29702;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#23613;&#31649;&#38656;&#35201;&#36827;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FIDLAR&#30340;&#39044;&#27979;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#27700;&#21069;&#37322;&#25918;&#65292;&#23454;&#29616;&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#27946;&#27700;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13371v1 Announce Type: new  Abstract: In coastal river systems, frequent floods, often occurring during major storms or king tides, pose a severe threat to lives and property. However, these floods can be mitigated or even prevented by strategically releasing water before extreme weather events with hydraulic structures such as dams, gates, pumps, and reservoirs. A standard approach used by local water management agencies is the "rule-based" method, which specifies predetermined pre-releases of water based on historical and time-tested human experience, but which tends to result in excess or inadequate water release. The model predictive control (MPC), a physics-based model for prediction, is an alternative approach, albeit involving computationally intensive calculations. In this paper, we propose a Forecast Informed Deep Learning Architecture, FIDLAR, to achieve rapid and optimal flood management with precise water pre-releases. FIDLAR seamlessly integrates two neural netw
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32508;&#21512;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#20915;&#23450;&#27169;&#22411;&#24615;&#33021;&#30340;&#38544;&#34255;&#20851;&#38190;&#22240;&#32032;&#65292;&#20026;DMs&#30340;&#25512;&#36827;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.13369</link><description>&lt;p&gt;
&#24322;&#35758;&#23665;&#35895;&#65306;&#25193;&#25955;&#27169;&#22411;&#30340;&#32508;&#21512;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Uncanny Valley: A Comprehensive Analysis of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13369
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32508;&#21512;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#20915;&#23450;&#27169;&#22411;&#24615;&#33021;&#30340;&#38544;&#34255;&#20851;&#38190;&#22240;&#32032;&#65292;&#20026;DMs&#30340;&#25512;&#36827;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#25105;&#20204;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26680;&#24515;&#25805;&#20316;&#21407;&#21017;&#65292;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#19981;&#21516;DM&#26550;&#26500;&#20013;&#30340;&#20851;&#38190;&#26041;&#38754;&#65306;i&#65289;&#22122;&#22768;&#26102;&#38388;&#34920;&#65292;ii&#65289;&#37319;&#26679;&#22120;&#21644;iii&#65289;&#24341;&#23548;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20840;&#38754;&#23457;&#26597;&#25581;&#31034;&#20102;&#23427;&#20204;&#38544;&#34255;&#30340;&#22522;&#26412;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#38544;&#34255;&#22522;&#30784;&#35201;&#32032;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24378;&#35843;&#20102;&#20915;&#23450;&#27169;&#22411;&#24615;&#33021;&#30340;&#38544;&#34255;&#20851;&#38190;&#22240;&#32032;&#65292;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#25512;&#21160;DMs&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22122;&#22768;&#26102;&#38388;&#34920;&#12289;&#37319;&#26679;&#22120;&#21644;&#24341;&#23548;&#30340;&#37197;&#32622;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#65307;&#28982;&#32780;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#37197;&#32622;&#19979;&#22312;&#19968;&#20010;&#38750;&#24120;&#30456;&#20284;&#30340;&#31283;&#23450;&#36136;&#37327;&#27700;&#24179;&#19978;&#36798;&#21040;&#65292;&#25581;&#31034;&#20102;&#20915;&#23450;&#26368;&#20339;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13369v1 Announce Type: cross  Abstract: Through Diffusion Models (DMs), we have made significant advances in generating high-quality images. Our exploration of these models delves deeply into their core operational principles by systematically investigating key aspects across various DM architectures: i) noise schedules, ii) samplers, and iii) guidance. Our comprehensive examination of these models sheds light on their hidden fundamental mechanisms, revealing the concealed foundational elements that are essential for their effectiveness. Our analyses emphasize the hidden key factors that determine model performance, offering insights that contribute to the advancement of DMs. Past findings show that the configuration of noise schedules, samplers, and guidance is vital to the quality of generated images; however, models reach a stable level of quality across different configurations at a remarkably similar point, revealing that the decisive factors for optimal performance pre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#20849;&#20139;&#30340;&#31163;&#25955;&#27010;&#24565;&#26469;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#26631;&#35760;&#23376;&#32452;&#65292;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#28040;&#38500;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.13368</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#27010;&#24565;&#21457;&#29616;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Concept Discovery Mitigates Spurious Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13368
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27010;&#24565;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#20849;&#20139;&#30340;&#31163;&#25955;&#27010;&#24565;&#26469;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#26631;&#35760;&#23376;&#32452;&#65292;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#28040;&#38500;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#23481;&#26131;&#20135;&#29983;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#20135;&#29983;&#33030;&#24369;&#30340;&#39044;&#27979;&#24182;&#24341;&#20837;&#24847;&#22806;&#30340;&#20559;&#35265;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#36890;&#24120;&#28041;&#21450;&#20381;&#36182;&#20808;&#39564;&#30693;&#35782;&#21644;&#32676;&#32452;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#33021;&#24182;&#19981;&#23481;&#26131;&#33719;&#24471;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26080;&#30417;&#30563;&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#19982;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#19968;&#31181;&#26032;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#26159;&#30452;&#25509;&#25512;&#26029;&#19982;&#26631;&#31614;&#20855;&#26377;&#19981;&#21516;&#30456;&#20851;&#24615;&#30340;&#23376;&#32452;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#21457;&#29616;&#27010;&#24565;&#65306;&#22312;&#36755;&#20837;&#26679;&#26412;&#20043;&#38388;&#20849;&#20139;&#30340;&#31163;&#25955;&#24605;&#24819;&#12290;&#20511;&#21161;&#29616;&#26377;&#30340;&#29289;&#20307;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoBalT&#65306;&#19968;&#31181;&#27010;&#24565;&#24179;&#34913;&#25216;&#26415;&#65292;&#26377;&#25928;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#23545;&#23376;&#32452;&#36827;&#34892;&#26631;&#35760;&#12290;&#22312;&#27700;&#40479;&#12289;CelebA&#21644;ImageNet-9&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#38024;&#23545;&#23376;&#32676;&#20307;&#21464;&#21270;&#30340;&#35780;&#20272;&#34920;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13368v1 Announce Type: new  Abstract: Models prone to spurious correlations in training data often produce brittle predictions and introduce unintended biases. Addressing this challenge typically involves methods relying on prior knowledge and group annotation to remove spurious correlations, which may not be readily available in many applications. In this paper, we establish a novel connection between unsupervised object-centric learning and mitigation of spurious correlations. Instead of directly inferring sub-groups with varying correlations with labels, our approach focuses on discovering concepts: discrete ideas that are shared across input samples. Leveraging existing object-centric representation learning, we introduce CoBalT: a concept balancing technique that effectively mitigates spurious correlations without requiring human labeling of subgroups. Evaluation across the Waterbirds, CelebA and ImageNet-9 benchmark datasets for subpopulation shifts demonstrate superio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28120;&#27760;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#39118;&#38505;&#19982;&#24378;Oracle&#23398;&#20064;&#32773;&#30456;&#21305;&#37197;&#65292;&#24182;&#23558;&#24369;Oracle&#23398;&#20064;&#32773;&#30340;&#39118;&#38505;&#20316;&#20026;&#33258;&#36866;&#24212;&#23398;&#20064;&#32773;&#39118;&#38505;&#30340;&#19968;&#20010;&#23454;&#38469;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.13366</link><description>&lt;p&gt;
&#32479;&#35745;&#35838;&#31243;&#23398;&#20064;&#65306;&#23454;&#29616;Oracle&#39118;&#38505;&#30340;&#28120;&#27760;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Statistical curriculum learning: An elimination algorithm achieving an oracle risk
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13366
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28120;&#27760;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#39118;&#38505;&#19982;&#24378;Oracle&#23398;&#20064;&#32773;&#30456;&#21305;&#37197;&#65292;&#24182;&#23558;&#24369;Oracle&#23398;&#20064;&#32773;&#30340;&#39118;&#38505;&#20316;&#20026;&#33258;&#36866;&#24212;&#23398;&#20064;&#32773;&#39118;&#38505;&#30340;&#19968;&#20010;&#23454;&#38469;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21442;&#25968;&#39044;&#27979;&#35774;&#32622;&#19979;&#30340;&#32479;&#35745;&#29256;&#26412;&#35838;&#31243;&#23398;&#20064;&#65288;CL&#65289;&#12290;&#23398;&#20064;&#32773;&#38656;&#35201;&#20272;&#35745;&#30446;&#26631;&#21442;&#25968;&#21521;&#37327;&#65292;&#24182;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20174;&#30446;&#26631;&#27169;&#22411;&#25110;&#20854;&#20182;&#31867;&#20284;&#20110;&#30446;&#26631;&#27169;&#22411;&#20294;&#22122;&#22768;&#36739;&#23567;&#30340;&#28304;&#27169;&#22411;&#20013;&#25910;&#38598;&#26679;&#26412;&#12290;&#26681;&#25454;&#20182;&#20204;&#25509;&#25910;&#30340;&#36741;&#21161;&#20449;&#24687;&#27700;&#24179;&#65292;&#25105;&#20204;&#32771;&#34385;&#19977;&#31181;&#31867;&#22411;&#30340;&#23398;&#20064;&#32773;&#12290;&#22312;&#21333;&#19968;&#26469;&#28304;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28120;&#27760;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#39118;&#38505;&#19982;&#24378;-Oracle&#23398;&#20064;&#32773;&#30340;&#39118;&#38505;&#30456;&#21305;&#37197;&#12290;&#22312;&#22810;&#28304;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20027;&#24352;&#24369;-Oracle&#23398;&#20064;&#32773;&#30340;&#39118;&#38505;&#26159;&#33258;&#36866;&#24212;&#23398;&#20064;&#32773;&#39118;&#38505;&#30340;&#19968;&#20010;&#29616;&#23454;&#22522;&#20934;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#37325;&#28120;&#27760;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13366v1 Announce Type: new  Abstract: We consider a statistical version of curriculum learning (CL) in a parametric prediction setting. The learner is required to estimate a target parameter vector, and can adaptively collect samples from either the target model, or other source models that are similar to the target model, but less noisy. We consider three types of learners, depending on the level of side-information they receive. The first two, referred to as strong/weak-oracle learners, receive high/low degrees of information about the models, and use these to learn. The third, a fully adaptive learner, estimates the target parameter vector without any prior information. In the single source case, we propose an elimination learning method, whose risk matches that of a strong-oracle learner. In the multiple source case, we advocate that the risk of the weak-oracle learner is a realistic benchmark for the risk of adaptive learners. We develop an adaptive multiple elimination
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#22270;&#20687;&#20998;&#26512;&#21644;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#19968;&#20010;KOHH&#34432;&#21051;&#30340;4H-SiC&#26230;&#29255;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#25152;&#26377;&#32570;&#38519;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#33258;&#21160;&#21270;&#25552;&#21462;</title><link>https://arxiv.org/abs/2402.13353</link><description>&lt;p&gt;
&#32467;&#21512;&#26080;&#30417;&#30563;&#21644;&#30417;&#30563;&#23398;&#20064;&#22312;&#26174;&#24494;&#38236;&#23398;&#20013;&#23454;&#29616;&#23545;&#20840;&#29255;4H-SiC&#26230;&#29255;&#30340;&#32570;&#38519;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Combining unsupervised and supervised learning in microscopy enables defect analysis of a full 4H-SiC wafer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13353
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#21508;&#31181;&#22270;&#20687;&#20998;&#26512;&#21644;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#19968;&#20010;KOHH&#34432;&#21051;&#30340;4H-SiC&#26230;&#29255;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#25152;&#26377;&#32570;&#38519;&#31867;&#22411;&#21644;&#20301;&#32622;&#30340;&#33258;&#21160;&#21270;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#21644;&#20998;&#26512;&#21322;&#23548;&#20307;&#26448;&#26009;&#20013;&#30340;&#21508;&#31181;&#32570;&#38519;&#31867;&#22411;&#26159;&#20102;&#35299;&#28508;&#22312;&#26426;&#21046;&#20197;&#21450;&#35843;&#25972;&#29983;&#20135;&#36807;&#31243;&#30340;&#37325;&#35201;&#21069;&#25552;&#12290;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#20998;&#26512;&#36890;&#24120;&#38656;&#35201;&#20687;&#20998;&#21106;&#21644;&#30446;&#26631;&#26816;&#27979;&#36825;&#26679;&#30340;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#21508;&#31181;&#22270;&#20687;&#20998;&#26512;&#21644;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#31283;&#20581;&#19988;&#20934;&#30830;&#30340;&#33258;&#21160;&#21270;&#22270;&#20687;&#20998;&#26512;&#27969;&#31243;&#65292;&#33021;&#22815;&#20174;&#22823;&#32422;40,000&#24352;&#21333;&#29420;&#22270;&#20687;&#25340;&#25509;&#22312;&#19968;&#36215;&#30340;KOH&#34432;&#21051;&#30340;4H-SiC&#26230;&#29255;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#25552;&#21462;&#25152;&#26377;&#32570;&#38519;&#30340;&#31867;&#22411;&#21644;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13353v1 Announce Type: cross  Abstract: Detecting and analyzing various defect types in semiconductor materials is an important prerequisite for understanding the underlying mechanisms as well as tailoring the production processes. Analysis of microscopy images that reveal defects typically requires image analysis tasks such as segmentation and object detection. With the permanently increasing amount of data that is produced by experiments, handling these tasks manually becomes more and more impossible. In this work, we combine various image analysis and data mining techniques for creating a robust and accurate, automated image analysis pipeline. This allows for extracting the type and position of all defects in a microscopy image of a KOH-etched 4H-SiC wafer that was stitched together from approximately 40,000 individual images.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.13352</link><description>&lt;p&gt;
KetGPT -- &#20351;&#29992;Transformer&#23545;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13352
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31639;&#27861;&#65292;&#34920;&#31034;&#20026;&#37327;&#23376;&#30005;&#36335;&#65292;&#21487;&#29992;&#20316;&#35780;&#20272;&#37327;&#23376;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22312;&#35813;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#65292;&#23548;&#33268;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#30005;&#36335;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#30005;&#36335;&#24182;&#19981;&#26159;&#20195;&#34920;&#24615;&#22522;&#20934;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#37327;&#23376;&#31995;&#32479;&#21046;&#36896;&#30340;&#30495;&#23454;&#37327;&#23376;&#31639;&#27861;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;&#36825;&#31181;&#32570;&#20047;&#8220;&#26377;&#29992;&#8221;&#30340;&#37327;&#23376;&#22522;&#20934;&#26500;&#25104;&#20102;&#25512;&#21160;&#37327;&#23376;&#32534;&#35793;&#22120;&#21644;&#30828;&#20214;&#24320;&#21457;&#19982;&#27604;&#36739;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KetGPT&#65292;&#19968;&#31181;&#20197;OpenQASM&#35821;&#35328;&#29983;&#25104;&#21512;&#25104;&#30005;&#36335;&#30340;&#24037;&#20855;&#65292;&#20854;&#32467;&#26500;&#26159;&#22522;&#20110;&#25512;&#23548;&#33258;&#37327;&#23376;&#30005;&#36335;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13352v1 Announce Type: cross  Abstract: Quantum algorithms, represented as quantum circuits, can be used as benchmarks for assessing the performance of quantum systems. Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits. Random circuits are, however, not representative benchmarks as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured. This shortage of `useful' quantum benchmarks poses a challenge to advancing the development and comparison of quantum compilers and hardware.   This research aims to enhance the existing quantum circuit datasets by generating what we refer to as `realistic-looking' circuits by employing the Transformer machine learning architecture. For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived f
&lt;/p&gt;</description></item><item><title>&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#22312;&#28608;&#21169;&#24615;&#25506;&#32034;&#38382;&#39064;&#20013;&#26159;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#22810;&#31181;&#25193;&#23637;&#35774;&#32622;&#65292;&#20026;&#25193;&#23637;&#20102;&#30340;IE&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#33324;&#24615;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.13338</link><description>&lt;p&gt;
&#36890;&#36807;&#36807;&#28388;&#21518;&#39564;&#37319;&#26679;&#28608;&#21169;&#24615;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Incentivized Exploration via Filtered Posterior Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13338
&lt;/p&gt;
&lt;p&gt;
&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#22312;&#28608;&#21169;&#24615;&#25506;&#32034;&#38382;&#39064;&#20013;&#26159;&#19968;&#31181;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#22810;&#31181;&#25193;&#23637;&#35774;&#32622;&#65292;&#20026;&#25193;&#23637;&#20102;&#30340;IE&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#33324;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#31038;&#20132;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#8220;&#28608;&#21169;&#24615;&#25506;&#32034;&#8221;&#65288;IE&#65289;&#65292;&#20854;&#20013;&#22996;&#25176;&#20154;&#65288;&#25512;&#33616;&#31639;&#27861;&#65289;&#21487;&#20197;&#21033;&#29992;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#26469;&#28608;&#21169;&#39034;&#24207;&#21040;&#36798;&#30340;&#20195;&#29702;&#37319;&#21462;&#25506;&#32034;&#24615;&#34892;&#21160;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21518;&#39564;&#37319;&#26679;&#65292;&#36825;&#26159;&#22810;&#33218;&#36172;&#21338;&#26426;&#25991;&#29486;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#20316;&#20026;IE&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20174;&#31169;&#20154;&#20195;&#29702;&#31867;&#22411;&#21040;&#20449;&#24687;&#24615;&#25512;&#33616;&#20877;&#21040;&#30456;&#20851;&#30340;&#36125;&#21494;&#26031;&#20808;&#39564;&#65292;&#25193;&#23637;&#20102;IE&#30340;&#29616;&#26377;&#33539;&#22260;&#12290;&#25105;&#20204;&#23545;IE&#20013;&#30340;&#21518;&#39564;&#37319;&#26679;&#36827;&#34892;&#20102;&#19968;&#33324;&#24615;&#20998;&#26512;&#65292;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#23558;&#36825;&#20123;&#25193;&#23637;&#35774;&#32622;&#32435;&#20837;&#20026;&#25512;&#35770;&#65292;&#24182;&#22312;&#24674;&#22797;&#29616;&#26377;&#32467;&#26524;&#30340;&#21516;&#26102;&#23558;&#20854;&#35270;&#20026;&#29305;&#27530;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13338v1 Announce Type: new  Abstract: We study "incentivized exploration" (IE) in social learning problems where the principal (a recommendation algorithm) can leverage information asymmetry to incentivize sequentially-arriving agents to take exploratory actions. We identify posterior sampling, an algorithmic approach that is well known in the multi-armed bandits literature, as a general-purpose solution for IE. In particular, we expand the existing scope of IE in several practically-relevant dimensions, from private agent types to informative recommendations to correlated Bayesian priors. We obtain a general analysis of posterior sampling in IE which allows us to subsume these extended settings as corollaries, while also recovering existing results as special cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#20272;&#35745;&#28151;&#21512;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#26426;&#22120;&#23398;&#20064;(DML)&#26469;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#20272;&#35745;&#22240;&#26524;&#21442;&#25968;&#26041;&#38754;&#20248;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26041;&#27861;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#36991;&#20813;&#31561;&#25928;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13332</link><description>&lt;p&gt;
&#29992;&#20110;&#22240;&#26524;&#28151;&#21512;&#24314;&#27169;&#30340;&#21452;&#26426;&#22120;&#23398;&#20064;&#8212;&#8212;&#22320;&#29699;&#31185;&#23398;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double machine learning for causal hybrid modeling -- applications in the Earth sciences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#36890;&#36807;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#20272;&#35745;&#28151;&#21512;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;&#26426;&#22120;&#23398;&#20064;(DML)&#26469;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#20272;&#35745;&#22240;&#26524;&#21442;&#25968;&#26041;&#38754;&#20248;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26041;&#27861;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#36991;&#20813;&#31561;&#25928;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13332v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#28151;&#21512;&#24314;&#27169;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#31185;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;&#35299;&#37322;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#36981;&#23432;&#33258;&#28982;&#35268;&#24459;&#12290;&#28982;&#32780;&#65292;&#22312;&#28151;&#21512;&#24314;&#27169;&#20013;&#65292;&#31561;&#25928;&#24615;&#21644;&#27491;&#21017;&#21270;&#20559;&#24046;&#23545;&#20110;&#23454;&#29616;&#36825;&#20123;&#30446;&#30340;&#26500;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#20272;&#35745;&#28151;&#21512;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#20351;&#29992;&#21452;&#26426;&#22120;&#23398;&#20064;(DML)&#26469;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#20004;&#20010;&#28041;&#21450;&#20108;&#27687;&#21270;&#30899;&#36890;&#37327;&#30340;&#22320;&#29699;&#31185;&#23398;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#22312;$Q_{10}$&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;DML&#30340;&#28151;&#21512;&#24314;&#27169;&#27604;&#31471;&#21040;&#31471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26041;&#27861;&#26356;&#20248;&#65292;&#35777;&#26126;&#20102;&#25928;&#29575;&#39640;&#12289;&#40065;&#26834;&#24615;&#24378;&#65292;&#24182;&#19988;&#35268;&#36991;&#20102;&#27491;&#21017;&#21270;&#26041;&#27861;&#24102;&#26469;&#30340;&#20559;&#24046;&#21644;&#31561;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#30899;&#36890;&#37327;&#20998;&#37197;&#65292;&#23637;&#29616;&#20102;&#36866;&#24212;&#19981;&#21516;&#22240;&#26524;&#25928;&#24212;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22240;&#26524;&#28151;&#21512;&#24314;&#27169;&#27010;&#24565;&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13332v1 Announce Type: new  Abstract: Hybrid modeling integrates machine learning with scientific knowledge with the goal of enhancing interpretability, generalization, and adherence to natural laws. Nevertheless, equifinality and regularization biases pose challenges in hybrid modeling to achieve these purposes. This paper introduces a novel approach to estimating hybrid models via a causal inference framework, specifically employing Double Machine Learning (DML) to estimate causal effects. We showcase its use for the Earth sciences on two problems related to carbon dioxide fluxes. In the $Q_{10}$ model, we demonstrate that DML-based hybrid modeling is superior in estimating causal parameters over end-to-end deep neural network (DNN) approaches, proving efficiency, robustness to bias from regularization methods, and circumventing equifinality. Our approach, applied to carbon flux partitioning, exhibits flexibility in accommodating heterogeneous causal effects. The study emp
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22914;&#20309;&#22312;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20005;&#26684;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#29468;&#24819;&#29983;&#25104;&#25110;&#24378;&#21270;&#23398;&#20064;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35770;&#29289;&#29702;&#21644;&#25968;&#23398;&#39046;&#22495;&#20013;&#20005;&#35880;&#24615;&#19982;&#29702;&#35299;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;</title><link>https://arxiv.org/abs/2402.13321</link><description>&lt;p&gt;
&#20174;&#22330;&#35770;&#21040;&#24222;&#21152;&#33713;&#29468;&#24819;&#65306;&#26426;&#22120;&#23398;&#20064;&#22312;&#20005;&#35880;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rigor with Machine Learning from Field Theory to the Poincar\'e Conjecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13321
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22914;&#20309;&#22312;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20005;&#26684;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#29468;&#24819;&#29983;&#25104;&#25110;&#24378;&#21270;&#23398;&#20064;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#22312;&#29702;&#35770;&#29289;&#29702;&#21644;&#25968;&#23398;&#39046;&#22495;&#20013;&#20005;&#35880;&#24615;&#19982;&#29702;&#35299;&#20043;&#38388;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21464;&#24471;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#22312;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#31361;&#30772;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#26159;&#38543;&#26426;&#30340;&#12289;&#23481;&#26131;&#20986;&#38169;&#30340;&#12289;&#19988;&#40657;&#21283;&#23376;&#19968;&#33324;&#12290;&#37027;&#20040;&#65292;&#23427;&#20204;&#24212;&#35813;&#22914;&#20309;&#22312;&#29702;&#35770;&#29289;&#29702;&#21644;&#32431;&#25968;&#23398;&#31561;&#24378;&#35843;&#20005;&#35880;&#21644;&#29702;&#35299;&#30340;&#39046;&#22495;&#20013;&#34987;&#21033;&#29992;&#21602;&#65311;&#22312;&#36825;&#20010;&#35270;&#35282;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#22312;&#33258;&#28982;&#31185;&#23398;&#20013;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21462;&#24471;&#20005;&#35880;&#24615;&#12290;&#38750;&#20005;&#35880;&#30340;&#26041;&#27861;&#21487;&#33021;&#36890;&#36807;&#29468;&#24819;&#29983;&#25104;&#25110;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#39564;&#35777;&#65292;&#20174;&#32780;&#20135;&#29983;&#20005;&#35880;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#25216;&#26415;&#23545;&#20005;&#35880;&#24615;&#30340;&#24212;&#29992;&#65292;&#20174;&#24358;&#29702;&#35770;&#21040;&#20302;&#32500;&#25299;&#25169;&#20013;&#30340;&#24179;&#28369;$4$d&#24222;&#21152;&#33713;&#29468;&#24819;&#12290;&#20154;&#20204;&#36824;&#21487;&#20197;&#24819;&#35937;&#22312;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#19982;&#25968;&#23398;&#25110;&#29702;&#35770;&#29289;&#29702;&#20043;&#38388;&#24314;&#31435;&#30452;&#25509;&#26725;&#26753;&#12290;&#20030;&#20363;&#26469;&#35828;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#30001;&#31070;&#32463;&#32593;&#32476;&#29702;&#35770;&#28608;&#21457;&#30340;&#22330;&#35770;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#20851;&#20110;&#40654;&#26364;&#27969;&#24418;&#27969;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13321v1 Announce Type: cross  Abstract: Machine learning techniques are increasingly powerful, leading to many breakthroughs in the natural sciences, but they are often stochastic, error-prone, and blackbox. How, then, should they be utilized in fields such as theoretical physics and pure mathematics that place a premium on rigor and understanding? In this Perspective we discuss techniques for obtaining rigor in the natural sciences with machine learning. Non-rigorous methods may lead to rigorous results via conjecture generation or verification by reinforcement learning. We survey applications of these techniques-for-rigor ranging from string theory to the smooth $4$d Poincar\'e conjecture in low-dimensional topology. One can also imagine building direct bridges between machine learning theory and either mathematics or theoretical physics. As examples, we describe a new approach to field theory motivated by neural network theory, and a theory of Riemannian metric flows indu
&lt;/p&gt;</description></item><item><title>&#27969;&#24335;&#23398;&#20064;&#26159;&#26377;&#26395;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#20013;&#27010;&#24565;&#28418;&#31227;&#30340;&#26368;&#26377;&#24076;&#26395;&#26041;&#27861;&#20043;&#19968;&#65292;&#26412;&#30740;&#31350;&#23545;&#20854;&#22312;&#39044;&#27979;&#26377;&#23475;&#34299;&#21326;&#32454;&#32990;&#25439;&#23475;&#26041;&#38754;&#30340;&#25928;&#21147;&#36827;&#34892;&#20102;&#27979;&#35797;&#24182;&#19982;&#25209;&#22788;&#29702;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2402.13304</link><description>&lt;p&gt;
&#26377;&#23475;&#34299;&#21326;&#32454;&#32990;&#25439;&#23475;(DSP)&#30340;&#39044;&#27979;&#65306;&#27969;&#24335;&#23398;&#20064;&#21644;&#25209;&#22788;&#29702;&#23398;&#20064;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Harmful algal bloom forecasting. A comparison between stream and batch learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13304
&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#23398;&#20064;&#26159;&#26377;&#26395;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#20013;&#27010;&#24565;&#28418;&#31227;&#30340;&#26368;&#26377;&#24076;&#26395;&#26041;&#27861;&#20043;&#19968;&#65292;&#26412;&#30740;&#31350;&#23545;&#20854;&#22312;&#39044;&#27979;&#26377;&#23475;&#34299;&#21326;&#32454;&#32990;&#25439;&#23475;&#26041;&#38754;&#30340;&#25928;&#21147;&#36827;&#34892;&#20102;&#27979;&#35797;&#24182;&#19982;&#25209;&#22788;&#29702;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#23475;&#34299;&#21326;&#32454;&#32990;&#25439;&#23475;(DSP)&#26159;&#19968;&#31181;&#20840;&#29699;&#20581;&#24247;&#23041;&#32961;&#65292;&#28304;&#20110;&#36125;&#31867;&#21463;&#21040;&#30002;&#34299;&#20135;&#29983;&#30340;&#27602;&#32032;&#27745;&#26579;&#12290;&#36825;&#31181;&#30142;&#30149;&#30001;&#20110;&#26222;&#36941;&#24615;&#21457;&#29983;&#12289;&#39640;&#33268;&#30149;&#29575;&#21644;&#36125;&#31867;&#25345;&#32493;&#30340;&#27602;&#24615;&#65292;&#23545;&#20844;&#20849;&#21355;&#29983;&#21644;&#36125;&#31867;&#20135;&#19994;&#26500;&#25104;&#21361;&#38505;&#12290;&#27602;&#32032;&#20135;&#29983;&#34299;&#31867;&#29983;&#29289;&#37327;&#39640;&#30340;&#24773;&#20917;&#65292;&#22914;DSP&#65292;&#34987;&#31216;&#20026;&#26377;&#23475;&#34299;&#21326;&#32454;&#32990;&#25439;&#23475;(HABs)&#12290;&#30417;&#27979;&#21644;&#39044;&#27979;&#31995;&#32479;&#23545;&#20110;&#20943;&#36731;HABs&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#39044;&#27979;&#26377;&#23475;&#34299;&#21326;&#32454;&#32990;&#25439;&#23475;&#28041;&#21450;&#19968;&#20010;&#20197;&#26102;&#38388;&#24207;&#21015;&#20026;&#22522;&#30784;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#26377;&#19968;&#20010;&#24378;&#28872;&#30340;&#21382;&#21490;&#23395;&#33410;&#24615;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#32780;&#65292;&#36817;&#26399;&#30001;&#20110;&#27668;&#35937;&#21644;&#28023;&#27915;&#20107;&#20214;&#21464;&#21270;&#32780;&#24341;&#36215;&#30340;&#24322;&#24120;&#29616;&#35937;&#24050;&#34987;&#35266;&#23519;&#21040;&#12290;&#27969;&#24335;&#23398;&#20064;&#26159;&#35299;&#20915;&#20855;&#26377;&#27010;&#24565;&#28418;&#31227;&#30340;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#39044;&#27979;HABs&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#65292;&#24182;&#19988;&#38656;&#35201;&#19982;&#25209;&#22788;&#29702;&#23398;&#20064;&#36827;&#34892;&#27604;&#36739;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13304v1 Announce Type: cross  Abstract: Diarrhetic Shellfish Poisoning (DSP) is a global health threat arising from shellfish contaminated with toxins produced by dinoflagellates. The condition, with its widespread incidence, high morbidity rate, and persistent shellfish toxicity, poses risks to public health and the shellfish industry. High biomass of toxin-producing algae such as DSP are known as Harmful Algal Blooms (HABs). Monitoring and forecasting systems are crucial for mitigating HABs impact. Predicting harmful algal blooms involves a time-series-based problem with a strong historical seasonal component, however, recent anomalies due to changes in meteorological and oceanographic events have been observed. Stream Learning stands out as one of the most promising approaches for addressing time-series-based problems with concept drifts. However, its efficacy in predicting HABs remains unproven and needs to be tested in comparison with Batch Learning. Historical data ava
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#22797;&#26434;&#35821;&#20041;bug&#65292;&#36890;&#36807;&#26032;&#30340;&#26597;&#35810;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#36317;&#31163;&#20195;&#30721;&#20851;&#31995;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13291</link><description>&lt;p&gt;
DeepCode AI Fix: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
DeepCode AI Fix: Fixing Security Vulnerabilities with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13291
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#22797;&#26434;&#35821;&#20041;bug&#65292;&#36890;&#36807;&#26032;&#30340;&#26597;&#35810;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#36317;&#31163;&#20195;&#30721;&#20851;&#31995;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#39046;&#22495;&#33021;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#23613;&#31649;&#26377;&#22823;&#37327;&#30740;&#31350;&#24037;&#20316;&#65292;&#21019;&#24314;&#19968;&#20010;&#23545;&#20110;&#22797;&#26434;&#35821;&#20041;&#38169;&#35823;&#65288;&#22914;&#23433;&#20840;&#28431;&#27934;&#65289;&#25928;&#26524;&#33391;&#22909;&#30340;&#31995;&#32479;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#26159;&#21033;&#29992;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#32534;&#31243;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#35299;&#20915;&#20195;&#30721;&#20462;&#22797;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#23398;&#20064;&#38271;&#36317;&#31163;&#30340;&#20195;&#30721;&#20851;&#31995;&#65292;&#36825;&#26159;&#19968;&#20010;&#22825;&#28982;&#20381;&#36182;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20026;&#22797;&#26434;&#31243;&#24207;&#38169;&#35823;&#21644;&#20854;&#23545;&#24212;&#20462;&#22797;&#21019;&#24314;&#19968;&#20010;&#22823;&#22411;&#19988;&#24178;&#20928;&#30340;&#25968;&#25454;&#38598;&#24182;&#19981;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#26597;&#35810;&#21644;&#24494;&#35843;LLMs&#30340;&#26041;&#27861;&#12290;&#36825;&#20010;&#24819;&#27861;&#26159;&#21033;&#29992;&#31243;&#24207;&#20998;&#26512;&#26469;&#38480;&#21046;LLMs&#30340;&#20851;&#27880;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13291v1 Announce Type: cross  Abstract: The automated program repair field has attracted substantial interest over the years, but despite significant research efforts, creating a system that works well for complex semantic bugs such as security vulnerabilities has proven difficult. A promising direction to solve this challenge is by leveraging large language models (LLMs), which are increasingly used to solve various programming tasks. In this paper, we investigate the effectiveness of LLMs for solving code-repair task. We show that the task is difficult as it requires the model to learn long-range code relationships, a task that inherently relies on extensive amounts of training data. At the same time, creating a large, clean dataset for complex program bugs and their corresponding fixes is non-trivial. We propose a technique to address these challenges with a new approach for querying and fine-tuning LLMs. The idea is to use program analysis to limit the LLM's attention me
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27745;&#26579;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35270;&#35282;&#65292;&#29992;&#20110;&#25805;&#32437;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#25512;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;&#22810;&#31181;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#24212;&#29992;&#21644;&#23454;&#35777;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.13287</link><description>&lt;p&gt;
&#36890;&#36807;&#27745;&#26579;&#25209;&#37327;&#25968;&#25454;&#25805;&#32437;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Manipulating hidden-Markov-model inferences by corrupting batch data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13287
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27745;&#26579;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35270;&#35282;&#65292;&#29992;&#20110;&#25805;&#32437;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#25512;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;&#22810;&#31181;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#24212;&#29992;&#21644;&#23454;&#35777;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#36890;&#24120;&#20551;&#35774;&#25968;&#25454;&#27969;&#26159;&#26410;&#34987;&#27745;&#26579;&#21644;&#21512;&#27861;&#30340;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#33258;&#31169;&#30340;&#23545;&#25163;&#21487;&#33021;&#26377;&#21160;&#26426;&#26469;&#30772;&#22351;&#36825;&#20123;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#21464;&#20915;&#31574;&#32773;&#30340;&#25512;&#26029;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35270;&#35282;&#65292;&#29992;&#20110;&#36890;&#36807;&#27745;&#26579;&#25968;&#25454;&#26469;&#25805;&#32437;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#25512;&#26029;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#22871;&#27745;&#26579;&#38382;&#39064;&#65292;&#29992;&#20110;&#36807;&#28388;&#12289;&#24179;&#28369;&#21644;&#35299;&#30721;&#25512;&#26029;&#65292;&#21033;&#29992;&#23545;&#25239;&#39118;&#38505;&#20998;&#26512;&#26041;&#27861;&#12290;&#25552;&#20986;&#20102;&#22810;&#20010;&#38543;&#26426;&#35268;&#21010;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#32467;&#21512;&#20102;&#29616;&#23454;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#21516;&#30340;&#25915;&#20987;&#32773;&#30446;&#26631;&#12290;&#36890;&#36807;&#20174;&#39057;&#29575;&#20027;&#20041;&#21644;&#36125;&#21494;&#26031;&#35282;&#24230;&#20132;&#26367;&#22320;&#35266;&#23519;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19977;&#31181;&#19968;&#33324;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#27979;&#35797;&#65292;&#35828;&#26126;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#24320;&#21457;&#30340;&#26041;&#27861;&#20197;&#20854;&#35299;&#20915;&#36136;&#37327;&#20026;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13287v1 Announce Type: cross  Abstract: Time-series models typically assume untainted and legitimate streams of data. However, a self-interested adversary may have incentive to corrupt this data, thereby altering a decision maker's inference. Within the broader field of adversarial machine learning, this research provides a novel, probabilistic perspective toward the manipulation of hidden Markov model inferences via corrupted data. In particular, we provision a suite of corruption problems for filtering, smoothing, and decoding inferences leveraging an adversarial risk analysis approach. Multiple stochastic programming models are set forth that incorporate realistic uncertainties and varied attacker objectives. Three general solution methods are developed by alternatively viewing the problem from frequentist and Bayesian perspectives. The efficacy of each method is illustrated via extensive, empirical testing. The developed methods are characterized by their solution qualit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992; PAC-Bayes &#29702;&#35770;&#21644; Gibbs &#20998;&#24067;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#26694;&#26550;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#24847;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20801;&#35768;&#23545;&#27867;&#21270;&#24046;&#36317;&#36827;&#34892;&#23450;&#21046;&#21270;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.13285</link><description>&lt;p&gt;
&#21033;&#29992; PAC-Bayes &#29702;&#35770;&#21644; Gibbs &#20998;&#24067;&#25512;&#23548;&#24102;&#26377;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Leveraging PAC-Bayes Theory and Gibbs Distributions for Generalization Bounds with Complexity Measures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992; PAC-Bayes &#29702;&#35770;&#21644; Gibbs &#20998;&#24067;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#26694;&#26550;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#24847;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20801;&#35768;&#23545;&#27867;&#21270;&#24046;&#36317;&#36827;&#34892;&#23450;&#21046;&#21270;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#20064;&#29702;&#35770;&#20013;&#65292;&#27867;&#21270;&#30028;&#38480;&#36890;&#24120;&#28041;&#21450;&#30001;&#32771;&#34385;&#30340;&#29702;&#35770;&#26694;&#26550;&#26045;&#21152;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#26412;&#25991;&#21033;&#29992;&#20102;&#20998;&#35299;&#30340; PAC-Bayes &#30028;&#38480;&#26694;&#26550;&#65292;&#25512;&#23548;&#20986;&#19968;&#20010;&#21487;&#23454;&#20363;&#21270;&#20026;&#20219;&#24847;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#20197;&#27010;&#29575;&#21516;&#26102;&#28085;&#30422;&#20551;&#35774;&#21644;&#23398;&#20064;&#26679;&#26412;&#65292;&#21487;&#20197;&#26681;&#25454;&#27867;&#21270;&#24046;&#36317;&#35843;&#25972;&#22797;&#26434;&#24230;&#65292;&#22240;&#20026;&#23427;&#21487;&#23450;&#21046;&#20197;&#36866;&#24212;&#20551;&#35774;&#31867;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13285v1 Announce Type: cross  Abstract: In statistical learning theory, a generalization bound usually involves a complexity measure imposed by the considered theoretical framework. This limits the scope of such bounds, as other forms of capacity measures or regularizations are used in algorithms. In this paper, we leverage the framework of disintegrated PAC-Bayes bounds to derive a general generalization bound instantiable with arbitrary complexity measures. One trick to prove such a result involves considering a commonly used family of distributions: the Gibbs distributions. Our bound stands in probability jointly over the hypothesis and the learning sample, which allows the complexity to be adapted to the generalization gap as it can be customized to fit both the hypothesis class and the task.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19982;Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink)&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.13277</link><description>&lt;p&gt;
&#20351;&#29992;SMOTETomek&#22312;WSNs&#20013;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MLSTL-WSN: Machine Learning-based Intrusion Detection using SMOTETomek in WSNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13277
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19982;Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink)&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;(WSNs)&#22312;&#22522;&#30784;&#35774;&#26045;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#22266;&#23450;&#21644;&#31227;&#21160;&#20256;&#24863;&#22120;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#33258;&#32452;&#32455;&#24182;&#24314;&#31435;&#22810;&#36339;&#36830;&#25509;&#36827;&#34892;&#36890;&#20449;&#65292;&#20849;&#21516;&#24863;&#30693;&#12289;&#25910;&#38598;&#12289;&#22788;&#29702;&#21644;&#20256;&#36755;&#26377;&#20851;&#21608;&#22260;&#29615;&#22659;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#65292;WSNs&#38754;&#20020;&#30528;&#21487;&#33021;&#30772;&#22351;&#21151;&#33021;&#30340;&#24555;&#36895;&#21644;&#26377;&#23475;&#30340;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;WSN&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#36935;&#21040;&#20102;&#20302;&#26816;&#27979;&#29575;&#12289;&#35745;&#31639;&#24320;&#38144;&#21644;&#35823;&#25253;&#35686;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#28304;&#20110;&#20256;&#24863;&#22120;&#33410;&#28857;&#36164;&#28304;&#32422;&#26463;&#12289;&#25968;&#25454;&#20887;&#20313;&#20197;&#21450;&#32593;&#32476;&#20869;&#39640;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#19982;Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink)&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#34701;&#21512;&#21512;&#25104;&#20102;&#23569;&#25968;&#23454;&#20363;&#24182;&#28040;&#38500;&#20102;Tomek&#38142;&#25509;&#65292;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13277v1 Announce Type: cross  Abstract: Wireless Sensor Networks (WSNs) play a pivotal role as infrastructures, encompassing both stationary and mobile sensors. These sensors self-organize and establish multi-hop connections for communication, collectively sensing, gathering, processing, and transmitting data about their surroundings. Despite their significance, WSNs face rapid and detrimental attacks that can disrupt functionality. Existing intrusion detection methods for WSNs encounter challenges such as low detection rates, computational overhead, and false alarms. These issues stem from sensor node resource constraints, data redundancy, and high correlation within the network. To address these challenges, we propose an innovative intrusion detection approach that integrates Machine Learning (ML) techniques with the Synthetic Minority Oversampling Technique Tomek Link (SMOTE-TomekLink) algorithm. This blend synthesizes minority instances and eliminates Tomek links, result
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#29699;&#28909;&#24102;&#27668;&#26059;&#24378;&#24230;&#39044;&#27979;&#27169;&#22411;MSCAR&#65292;&#39318;&#27425;&#23558;&#22240;&#26524;&#20851;&#31995;&#19982;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13270</link><description>&lt;p&gt;
&#20840;&#29699;&#28909;&#24102;&#27668;&#26059;&#24378;&#24230;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#22810;&#23610;&#24230;&#22240;&#26524;&#33258;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Global Tropical Cyclone Intensity Forecasting with Multi-modal Multi-scale Causal Autoregressive Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#29699;&#28909;&#24102;&#27668;&#26059;&#24378;&#24230;&#39044;&#27979;&#27169;&#22411;MSCAR&#65292;&#39318;&#27425;&#23558;&#22240;&#26524;&#20851;&#31995;&#19982;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#65288;TC&#65289;&#24378;&#24230;&#23545;&#20110;&#21046;&#23450;&#28798;&#23475;&#39118;&#38505;&#20943;&#23569;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26469;&#33258;ERA5&#25968;&#25454;&#30340;&#26377;&#38480;&#26102;&#31354;&#20449;&#24687;&#65292;&#24182;&#24573;&#35270;&#36825;&#20123;&#29289;&#29702;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#26410;&#33021;&#20805;&#20998;&#25429;&#25417;&#24378;&#24230;&#39044;&#27979;&#25152;&#38656;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#24335;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22810;&#23610;&#24230;&#22240;&#26524;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;MSCAR&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#22240;&#26524;&#20851;&#31995;&#19982;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#20840;&#29699;TC&#24378;&#24230;&#33258;&#22238;&#24402;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#25552;&#20379;&#24191;&#27867;&#31354;&#38388;&#21464;&#37327;&#30340;TC&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#22522;&#20110;&#21355;&#26143;&#21644;ERA5&#30340;&#28909;&#24102;&#27668;&#26059;&#25968;&#25454;&#38598;&#65288;SETCD&#65289;&#65292;&#23427;&#26159;&#19982;TC&#26377;&#20851;&#30340;&#26368;&#38271;&#21644;&#26368;&#20840;&#38754;&#30340;&#20840;&#29699;&#25968;&#25454;&#38598;&#12290;&#23545;&#35813;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;MSCAR&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13270v1 Announce Type: cross  Abstract: Accurate forecasting of Tropical cyclone (TC) intensity is crucial for formulating disaster risk reduction strategies. Current methods predominantly rely on limited spatiotemporal information from ERA5 data and neglect the causal relationships between these physical variables, failing to fully capture the spatial and temporal patterns required for intensity forecasting. To address this issue, we propose a Multi-modal multi-Scale Causal AutoRegressive model (MSCAR), which is the first model that combines causal relationships with large-scale multi-modal data for global TC intensity autoregressive forecasting. Furthermore, given the current absence of a TC dataset that offers a wide range of spatial variables, we present the Satellite and ERA5-based Tropical Cyclone Dataset (SETCD), which stands as the longest and most comprehensive global dataset related to TCs. Experiments on the dataset show that MSCAR outperforms the state-of-the-art
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13224</link><description>&lt;p&gt;
&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#25511;&#21046;&#22823;&#22411;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;
&lt;/p&gt;
&lt;p&gt;
Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#65288;EVCS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34701;&#21512;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#22914;&#25554;&#27133;&#21151;&#29575;&#38480;&#21046;&#12289;&#21512;&#21516;&#38408;&#20540;&#36229;&#38480;&#24809;&#32602;&#20197;&#21450;&#30005;&#21160;&#27773;&#36710;&#65288;EVs&#65289;&#30340;&#26089;&#26399;&#26029;&#24320;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#25511;&#21046;EVCS&#30340;&#38382;&#39064;&#24418;&#24335;&#65292;&#24182;&#23454;&#26045;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#21363;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#20197;&#21450;&#33021;&#37327;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#39547;&#30041;&#26102;&#38388;&#20381;&#36182;&#38543;&#26426;&#36807;&#31243;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#22686;&#24378;&#20102;&#25104;&#26412;&#38477;&#20302;&#30340;&#21516;&#26102;&#20445;&#25345;&#23458;&#25143;&#28385;&#24847;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;22&#22825;&#27169;&#25311;&#23637;&#31034;&#20102;&#20004;&#31181;&#25552;&#20986;&#26041;&#27861;&#30456;&#23545;&#20110;&#20004;&#20010;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#20004;&#38454;&#27573;&#26041;&#27861;&#35777;&#26126;&#20102;&#38024;&#23545;&#26089;&#26399;&#26029;&#24320;&#30340;&#40065;&#26834;&#24615;&#65292;&#32771;&#34385;&#20102;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13224v1 Announce Type: cross  Abstract: This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more
&lt;/p&gt;</description></item><item><title>&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30446;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#30340;&#30446;&#26631;&#29305;&#24615;&#65292;&#32780;&#23578;&#26410;&#36798;&#21040;&#29983;&#25104;&#33021;&#21147;&#19982;&#20854;&#20182;&#39046;&#22495;&#30340;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.13221</link><description>&lt;p&gt;
CHILI: &#29992;&#20110;&#25512;&#36827;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#21270;&#23398;&#20449;&#24687;&#30340;&#22823;&#22411;&#26080;&#26426;&#32435;&#31859;&#26448;&#26009;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13221
&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30446;&#21069;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#27979;&#20998;&#23376;&#21644;&#26448;&#26009;&#30340;&#30446;&#26631;&#29305;&#24615;&#65292;&#32780;&#23578;&#26410;&#36798;&#21040;&#29983;&#25104;&#33021;&#21147;&#19982;&#20854;&#20182;&#39046;&#22495;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#23637;&#20027;&#35201;&#21463;&#21270;&#23398;&#24212;&#29992;&#30340;&#39537;&#21160;&#65292;&#22240;&#20026;&#22270;&#19968;&#30452;&#26159;&#20998;&#23376;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#34429;&#28982;&#26089;&#26399;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23567;&#26377;&#26426;&#20998;&#23376;&#19978;&#65292;&#20294;&#26368;&#36817;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#33539;&#22260;&#24050;&#32463;&#25193;&#23637;&#21040;&#21253;&#25324;&#26080;&#26426;&#26448;&#26009;&#12290;&#24314;&#27169;&#26080;&#26426;&#26230;&#20307;&#26448;&#26009;&#30340;&#21608;&#26399;&#24615;&#21644;&#23545;&#31216;&#24615;&#24102;&#26469;&#29420;&#29305;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#12290;&#36716;&#21521;&#26080;&#26426;&#32435;&#31859;&#26448;&#26009;&#20250;&#22686;&#21152;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#27599;&#20010;&#22270;&#20013;&#33410;&#28857;&#25968;&#37327;&#30340;&#33539;&#22260;&#21487;&#33021;&#24456;&#24191;&#65288;$10$&#21040;$10^5$&#65289;&#12290;&#29616;&#26377;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#36890;&#36807;&#22270;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#30446;&#26631;&#29305;&#24615;&#65292;&#26469;&#34920;&#24449;&#20998;&#23376;&#21644;&#26448;&#26009;&#12290;&#20294;&#26159;&#65292;&#22270;&#26426;&#22120;&#23398;&#20064;&#26368;&#28608;&#21160;&#20154;&#24515;&#30340;&#24212;&#29992;&#23558;&#22312;&#20854;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#65292;&#30446;&#21069;&#19982;&#22270;&#20687;&#25110;&#25991;&#26412;&#31561;&#20854;&#20182;&#39046;&#22495;&#36824;&#19981;&#22312;&#21516;&#19968;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13221v1 Announce Type: new  Abstract: Advances in graph machine learning (ML) have been driven by applications in chemistry as graphs have remained the most expressive representations of molecules. While early graph ML methods focused primarily on small organic molecules, recently, the scope of graph ML has expanded to include inorganic materials. Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ML methods are unable to address. Moving to inorganic nanomaterials increases complexity as the scale of number of nodes within each graph can be broad ($10$ to $10^5$). The bulk of existing graph ML focuses on characterising molecules and materials by predicting target properties with graphs as input. However, the most exciting applications of graph ML will be in their generative capabilities, which is currently not at par with other domains such as images or text.   We invite the graph ML community to address th
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#22522;&#20110;&#22270;&#24577;&#29702;&#35299;&#21644;&#23454;&#29616;&#65292;&#21487;&#29992;&#20316;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#65292;&#25110;&#20316;&#20026;&#26500;&#24314;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.13001</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#20027;&#35201;&#26694;&#26550;&#26469;&#33258;&#37327;&#23376;&#22270;&#24577;
&lt;/p&gt;
&lt;p&gt;
A unifying primary framework for quantum graph neural networks from quantum graph states
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13001
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#22522;&#20110;&#22270;&#24577;&#29702;&#35299;&#21644;&#23454;&#29616;&#65292;&#21487;&#29992;&#20316;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#65292;&#25110;&#20316;&#20026;&#26500;&#24314;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24577;&#34987;&#29992;&#26469;&#23558;&#25968;&#23398;&#22270;&#34920;&#31034;&#20026;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#31283;&#23450;&#23376;&#30721;&#25110;&#30452;&#25509;&#30340;&#37327;&#23376;&#38376;&#21644;&#37327;&#23376;&#29366;&#24577;&#26469;&#26500;&#24314;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#22270;&#24577;&#21152;&#20197;&#29702;&#35299;&#21644;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#34987;&#29992;&#20316;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#26469;&#34920;&#31034;&#31070;&#32463;&#32593;&#32476;&#65292;&#25110;&#20316;&#20026;&#26500;&#24314;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13001v1 Announce Type: cross  Abstract: Graph states are used to represent mathematical graphs as quantum states on quantum computers. They can be formulated through stabilizer codes or directly quantum gates and quantum states. In this paper we show that a quantum graph neural network model can be understood and realized based on graph states. We show that they can be used either as a parameterized quantum circuits to represent neural networks or as an underlying structure to construct graph neural networks on quantum computers.
&lt;/p&gt;</description></item><item><title>PARCv2&#36890;&#36807;&#24341;&#20837;&#24494;&#20998;&#31639;&#23376;&#25193;&#23637;&#20102;PARC&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#19981;&#31283;&#23450;&#12289;&#30636;&#24577;&#21644;&#20256;&#36755;&#20027;&#23548;&#31995;&#32479;&#30340;&#26102;&#31354;&#21160;&#21147;&#23398;&#12290;</title><link>https://arxiv.org/abs/2402.12503</link><description>&lt;p&gt;
PARCv2&#65306;&#29289;&#29702;&#24863;&#30693;&#24490;&#29615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#21160;&#21147;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12503
&lt;/p&gt;
&lt;p&gt;
PARCv2&#36890;&#36807;&#24341;&#20837;&#24494;&#20998;&#31639;&#23376;&#25193;&#23637;&#20102;PARC&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#19981;&#31283;&#23450;&#12289;&#30636;&#24577;&#21644;&#20256;&#36755;&#20027;&#23548;&#31995;&#32479;&#30340;&#26102;&#31354;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12503v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#23545;&#19981;&#31283;&#23450;&#30340;&#12289;&#24555;&#36895;&#30636;&#24577;&#21644;&#20248;&#21183;&#20256;&#36755;&#20027;&#23548;&#30340;&#29289;&#29702;&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#26159;&#29289;&#29702;&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#65288;PADL&#65289;&#38754;&#20020;&#30340;&#36843;&#20999;&#25361;&#25112;&#12290;&#22797;&#26434;&#31995;&#32479;&#30340;&#29289;&#29702;&#30001;&#22823;&#22411;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#31995;&#32479;&#21644;&#24102;&#26377;&#38750;&#32447;&#24615;&#32467;&#26500;&#30340;&#36741;&#21161;&#26412;&#26500;&#27169;&#22411;&#25152;&#25511;&#21046;&#65292;&#21516;&#26102;&#36824;&#21253;&#25324;&#34920;&#29616;&#20986;&#24613;&#21095;&#26799;&#24230;&#21644;&#24555;&#36895;&#21464;&#24418;&#26448;&#26009;&#30028;&#38754;&#30340;&#28436;&#21270;&#29366;&#24577;&#22330;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#19988;&#36890;&#29992;&#30340;&#24402;&#32435;&#20559;&#35265;&#26041;&#27861;&#65292;&#29992;&#20110;&#27169;&#25311;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#22330;&#28436;&#21464;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#26368;&#36817;&#30340;&#29289;&#29702;&#24863;&#30693;&#24490;&#29615;&#21367;&#31215;&#65288;PARC&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#19968;&#31181;&#21306;&#20998;-&#31215;&#20998;&#22120;&#32467;&#26500;&#65292;&#24402;&#32435;&#22320;&#27169;&#25311;&#20102;&#36890;&#29992;&#29289;&#29702;&#31995;&#32479;&#30340;&#26102;&#31354;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;PARC&#30340;&#21151;&#33021;&#65292;&#20197;&#27169;&#25311;&#19981;&#31283;&#23450;&#12289;&#30636;&#24577;&#21644;&#20256;&#36755;&#20027;&#23548;&#31995;&#32479;&#12290;&#36825;&#20010;&#25193;&#23637;&#27169;&#22411;&#34987;&#31216;&#20026;PARCv2&#65292;&#37197;&#22791;&#20102;&#24494;&#20998;&#31639;&#23376;&#26469;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12503v1 Announce Type: new  Abstract: Modeling unsteady, fast transient, and advection-dominated physics problems is a pressing challenge for physics-aware deep learning (PADL). The physics of complex systems is governed by large systems of partial differential equations (PDEs) and ancillary constitutive models with nonlinear structures, as well as evolving state fields exhibiting sharp gradients and rapidly deforming material interfaces. Here, we investigate an inductive bias approach that is versatile and generalizable to model generic nonlinear field evolution problems. Our study focuses on the recent physics-aware recurrent convolutions (PARC), which incorporates a differentiator-integrator architecture that inductively models the spatiotemporal dynamics of generic physical systems. We extend the capabilities of PARC to simulate unsteady, transient, and advection-dominant systems. The extended model, referred to as PARCv2, is equipped with differential operators to model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Rectify-Router&#35299;&#20915;&#20102;MoE&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#25152;&#24102;&#26469;&#30340;&#20196;&#29260;&#20002;&#22833;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#36890;&#36807;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.12399</link><description>&lt;p&gt;
&#23558;&#24223;&#26009;&#21464;&#24223;&#20026;&#23453;&#65306;&#30699;&#27491;MoE&#30340;Top-k&#36335;&#30001;&#22120;
&lt;/p&gt;
&lt;p&gt;
Turn Waste into Worth: Rectifying Top-$k$ Router of MoE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Rectify-Router&#35299;&#20915;&#20102;MoE&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#25152;&#24102;&#26469;&#30340;&#20196;&#29260;&#20002;&#22833;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#36890;&#36807;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22240;&#20854;&#35745;&#31639;&#25928;&#29575;&#32780;&#21463;&#21040;&#27426;&#36814;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#30001;&#20110;&#19981;&#24179;&#34913;&#30340;&#36335;&#30001;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#36807;&#39640;&#12290;&#19968;&#20123;&#19987;&#23478;&#20250;&#28322;&#20986;&#65292;&#20854;&#20013;&#36229;&#20986;&#30340;&#20196;&#29260;&#20250;&#34987;&#20002;&#24323;&#12290;&#32780;&#19968;&#20123;&#19987;&#23478;&#26159;&#31354;&#38386;&#30340;&#65292;&#36825;&#20123;&#19987;&#23478;&#20250;&#22635;&#20805;&#20026;&#38646;&#65292;&#36127;&#38754;&#24433;&#21709;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20002;&#24323;&#20196;&#29260;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rectify-Router&#65292;&#21253;&#25324;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#12290;Intra-GPU&#30699;&#27491;&#22788;&#29702;&#20002;&#24323;&#30340;&#20196;&#29260;&#65292;&#23558;&#23427;&#20204;&#26377;&#25928;&#22320;&#36335;&#30001;&#21040;GPU&#20869;&#30340;&#19987;&#23478;&#65292;&#36991;&#20813;&#36328;GPU&#36890;&#20449;&#12290;Fill-in&#30699;&#27491;&#36890;&#36807;&#29992;&#20855;&#26377;&#39640;&#36335;&#30001;&#20998;&#25968;&#30340;&#20196;&#29260;&#26367;&#25442;&#22635;&#20805;&#20196;&#29260;&#26469;&#35299;&#20915;&#22635;&#20805;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12399v1 Announce Type: cross  Abstract: Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectificati
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#65292;&#30001;&#27169;&#25311;&#35282;&#33394;&#21327;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;</title><link>https://arxiv.org/abs/2402.12391</link><description>&lt;p&gt;
&#23454;&#29616;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#31185;&#23398;&#21457;&#29616;&#30340;AI&#31185;&#23398;&#23478;&#22242;&#38431;
&lt;/p&gt;
&lt;p&gt;
Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12391
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#65292;&#30001;&#27169;&#25311;&#35282;&#33394;&#21327;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#31185;&#23398;&#21457;&#29616;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20174;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#12290;TAIS&#21253;&#25324;&#27169;&#25311;&#35282;&#33394;&#65292;&#21253;&#25324;&#39033;&#30446;&#32463;&#29702;&#12289;&#25968;&#25454;&#24037;&#31243;&#24072;&#21644;&#39046;&#22495;&#19987;&#23478;&#65292;&#27599;&#20010;&#35282;&#33394;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#34920;&#12290;&#36825;&#20123;&#35282;&#33394;&#21327;&#20316;&#20197;&#22797;&#21046;&#25968;&#25454;&#31185;&#23398;&#23478;&#36890;&#24120;&#25191;&#34892;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12391v1 Announce Type: cross  Abstract: Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM). These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a benchmark dataset t
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12343</link><description>&lt;p&gt;
&#27169;&#25311;&#22833;&#35843;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#21487;&#33021;&#20250;&#36866;&#24471;&#20854;&#21453;&#65281;
&lt;/p&gt;
&lt;p&gt;
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12343
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#19982;&#20154;&#31867;&#36827;&#34892;&#23433;&#20840;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25915;&#20987;&#26694;&#26550;&#65292;&#34920;&#26126;&#23433;&#20840;&#23545;&#40784;&#20063;&#21487;&#33021;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#26080;&#24847;&#20013;&#20419;&#25104;&#26377;&#23475;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#21629;&#21517;&#20026;&#27169;&#25311;&#22833;&#35843;&#65288;ED&#65289;&#65292;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#19981;&#33391;&#22320;&#32452;&#21512;&#20102;&#19968;&#23545;&#24320;&#28304;&#39044;&#35757;&#32451;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26377;&#23475;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ED&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;Llama-1&#12289;Llama-2&#12289;Mistral&#21644;Alpaca&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ED&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#23475;&#24615;&#22686;&#21152;&#20102;&#19968;&#20493;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#65292;&#20197;&#36739;&#22823;&#20248;&#21183;&#22312;48&#20010;&#35780;&#20272;&#23376;&#38598;&#20013;&#30340;43&#20010;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#26377;&#23475;&#29575;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#65292;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12062</link><description>&lt;p&gt;
&#22240;&#26524;&#24179;&#31561;&#20445;&#25252;&#19982;&#31639;&#27861;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Equal Protection as Algorithmic Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21746;&#23398;&#30340;&#25991;&#29486;&#24418;&#25104;&#20102;&#19981;&#21516;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#26631;&#20934;&#12290;&#20854;&#20013;&#26368;&#21463;&#20105;&#35758;&#30340;&#20998;&#31867;&#24179;&#31561;&#35201;&#27714;&#65292;&#39044;&#27979;&#31639;&#27861;&#30340;&#38169;&#35823;&#20998;&#31867;&#22312;&#34987;&#20445;&#25252;&#29305;&#24449;&#25152;&#25351;&#31034;&#30340;&#32676;&#20307;&#20013;&#20197;&#30456;&#31561;&#39057;&#29575;&#21457;&#29983;&#12290;&#23613;&#31649;&#20998;&#31867;&#24179;&#31561;&#20855;&#26377;&#30452;&#35266;&#21560;&#24341;&#21147;&#65292;&#20294;&#24050;&#21463;&#21040;&#25915;&#20987;&#12290;&#25105;&#20204;&#36716;&#21521;&#19968;&#20010;&#30456;&#20851;&#21407;&#21017;&#65292;&#21363;&#24179;&#31561;&#20445;&#25252;&#65292;&#35813;&#21407;&#21017;&#26368;&#21021;&#26159;&#22312;&#21009;&#20107;&#21496;&#27861;&#39046;&#22495;&#21457;&#23637;&#36215;&#26469;&#30340;&#12290;&#24179;&#31561;&#20445;&#25252;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#65288;&#23558;&#22312;&#35268;&#23450;&#30340;&#24847;&#20041;&#19978;&#20855;&#20307;&#35828;&#26126;&#65289;&#36827;&#34892;&#22343;&#31561;&#21270;&#65292;&#32780;&#19981;&#26159;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#27604;&#29575;&#22343;&#31561;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#31561;&#20445;&#25252;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20998;&#31867;&#24179;&#31561;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12062v1 Announce Type: cross  Abstract: Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classificati
&lt;/p&gt;</description></item><item><title>Leaky ReLU&#21442;&#25968;$\alpha=-1$&#22312;&#35757;&#32451;&#35823;&#24046;&#21644;&#27867;&#21270;&#35823;&#24046;&#30028;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.11942</link><description>&lt;p&gt;
Leaky ReLU&#23545;&#36229;&#21442;&#25968;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#27867;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The effect of Leaky ReLUs on the training and generalization of overparameterized networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11942
&lt;/p&gt;
&lt;p&gt;
Leaky ReLU&#21442;&#25968;$\alpha=-1$&#22312;&#35757;&#32451;&#35823;&#24046;&#21644;&#27867;&#21270;&#35823;&#24046;&#30028;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#21508;&#31181;&#27844;&#28431;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20989;&#25968;&#30340;&#36229;&#21442;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#30340;&#35757;&#32451;&#21644;&#27867;&#21270;&#35823;&#24046;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20180;&#32454;&#22320;&#23545;&#36825;&#20123;NNs&#30340;&#35757;&#32451;&#35823;&#24046;&#30340;&#25910;&#25947;&#36895;&#29575;&#21644;&#27867;&#21270;&#35823;&#24046;&#36827;&#34892;&#20102;&#19978;&#30028;&#20272;&#35745;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#30028;&#38480;&#23545;Leaky ReLU&#21442;&#25968;$\alpha$&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;$\alpha=-1$&#65292;&#23545;&#24212;&#20110;&#32477;&#23545;&#20540;&#28608;&#27963;&#20989;&#25968;&#65292;&#23545;&#20110;&#35757;&#32451;&#35823;&#24046;&#30028;&#26159;&#26368;&#20248;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#29305;&#23450;&#35774;&#32622;&#20013;&#65292;&#36825;&#20063;&#26159;&#27867;&#21270;&#35823;&#24046;&#30028;&#30340;&#26368;&#20248;&#36873;&#25321;&#12290;&#25968;&#20540;&#23454;&#39564;&#22312;&#23454;&#36341;&#20013;&#25903;&#25345;&#20102;&#29702;&#35770;&#24341;&#23548;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11942v1 Announce Type: new  Abstract: We investigate the training and generalization errors of overparameterized neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU) functions. More specifically, we carefully upper bound both the convergence rate of the training error and the generalization error of such NNs and investigate the dependence of these bounds on the Leaky ReLU parameter, $\alpha$. We show that $\alpha =-1$, which corresponds to the absolute value activation function, is optimal for the training error bound. Furthermore, in special settings, it is also optimal for the generalization error bound. Numerical experiments empirically support the practical choices guided by the theory.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26725;&#29983;&#25104;&#27169;&#22411; Re-Dock&#65292;&#29992;&#20110;&#28789;&#27963;&#21644;&#29616;&#23454;&#30340;&#20998;&#23376;&#23545;&#25509;&#65292;&#36890;&#36807;&#33021;&#37327;&#21040;&#20960;&#20309;&#26144;&#23556;&#26469;&#20849;&#21516;&#24314;&#27169;&#32467;&#21512;&#33021;&#21644;&#26500;&#35937;&#65292;&#22635;&#34917;&#20102;&#23545;&#25509;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#26500;&#35937;&#39044;&#27979;&#26041;&#38754;&#30340;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.11459</link><description>&lt;p&gt;
Re-Dock: &#26397;&#21521;&#20855;&#26377;&#25193;&#25955;&#26725;&#30340;&#28789;&#27963;&#21644;&#29616;&#23454;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11459
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26725;&#29983;&#25104;&#27169;&#22411; Re-Dock&#65292;&#29992;&#20110;&#28789;&#27963;&#21644;&#29616;&#23454;&#30340;&#20998;&#23376;&#23545;&#25509;&#65292;&#36890;&#36807;&#33021;&#37327;&#21040;&#20960;&#20309;&#26144;&#23556;&#26469;&#20849;&#21516;&#24314;&#27169;&#32467;&#21512;&#33021;&#21644;&#26500;&#35937;&#65292;&#22635;&#34917;&#20102;&#23545;&#25509;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#26500;&#35937;&#39044;&#27979;&#26041;&#38754;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#32467;&#26500;&#65292;&#21363;&#20998;&#23376;&#23545;&#25509;&#20219;&#21153;&#23545;&#20110;&#33647;&#29289;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23436;&#25972;&#34507;&#30333;&#36136;&#32467;&#26500;&#65288;&#23545;&#25509;&#65292;&#19988;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#19981;&#21487;&#36798;&#65289;&#25110;&#24573;&#30053;&#21475;&#34955;&#20391;&#38142;&#26500;&#35937;&#65292;&#23548;&#33268;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#26500;&#35937;&#39044;&#27979;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26410;&#32463;&#25506;&#32034;&#30340;&#20219;&#21153;&#65292;&#21629;&#21517;&#20026;&#26580;&#24615;&#23545;&#25509;&#65292;&#20197;&#21516;&#26102;&#39044;&#27979;&#37197;&#20307;&#21644;&#21475;&#34955;&#20391;&#38142;&#30340;&#23039;&#21183;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25193;&#23637;&#21040;&#20960;&#20309;&#27969;&#24418;&#30340;&#26032;&#22411;&#25193;&#25955;&#26725;&#29983;&#25104;&#27169;&#22411; Re-Dock&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21463;&#29275;&#39039;-&#27431;&#25289;&#26041;&#31243;&#21551;&#21457;&#30340;&#33021;&#37327;&#21040;&#20960;&#20309;&#26144;&#23556;&#65292;&#20197;&#20849;&#21516;&#24314;&#27169;&#32467;&#21512;&#33021;&#21644;&#26500;&#35937;&#65292;&#20197;&#21453;&#26144;&#33021;&#37327;&#32422;&#26463;&#23545;&#25509;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#35774;&#35745;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;apo-dock&#21644;cross-dock d
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11459v1 Announce Type: cross  Abstract: Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;&#30340;&#20449;&#20219;&#21306;&#22495;&#33021;&#22815;&#26377;&#25928;&#22320;&#27934;&#23519;&#27169;&#22411;&#34892;&#20026;&#12289;&#20445;&#35777;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#35299;&#37322;&#30340;&#37325;&#29992;</title><link>https://arxiv.org/abs/2402.11168</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Trust Regions for Explanations via Black-Box Probabilistic Certification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;&#30340;&#20449;&#20219;&#21306;&#22495;&#33021;&#22815;&#26377;&#25928;&#22320;&#27934;&#23519;&#27169;&#22411;&#34892;&#20026;&#12289;&#20445;&#35777;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#35299;&#37322;&#30340;&#37325;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#24615;&#36136;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#35299;&#26512;&#20010;&#21035;&#20915;&#31574;&#32972;&#21518;&#30340;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#40657;&#30418;&#65288;&#27010;&#29575;&#24615;&#65289;&#35299;&#37322;&#35748;&#35777;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#40657;&#30418;&#27169;&#22411;&#65292;&#21482;&#26377;&#26597;&#35810;&#35775;&#38382;&#26435;&#65292;&#19968;&#20010;&#31034;&#20363;&#30340;&#35299;&#37322;&#20197;&#21450;&#19968;&#20010;&#36136;&#37327;&#24230;&#37327;&#65288;&#22914;&#36924;&#30495;&#24230;&#12289;&#31283;&#23450;&#24615;&#65289;&#65292;&#25105;&#20204;&#26159;&#21542;&#33021;&#25214;&#21040;&#26368;&#22823;&#30340;&#36229;&#31435;&#26041;&#20307;&#65288;&#21363; $\ell_{\infty}$ &#29699;&#65289;&#65292;&#20197;&#31034;&#20363;&#20026;&#20013;&#24515;&#65292;&#20351;&#24471;&#24403;&#35299;&#37322;&#34987;&#24212;&#29992;&#20110;&#36229;&#31435;&#26041;&#20307;&#20869;&#30340;&#25152;&#26377;&#31034;&#20363;&#26102;&#65288;&#39640;&#27010;&#29575;&#19979;&#65289;&#36136;&#37327;&#26631;&#20934;&#24471;&#21040;&#28385;&#36275;&#65288;&#27604;&#22914;&#36924;&#30495;&#24230;&#39640;&#20110;&#26576;&#20010;&#20540;&#65289;&#65311;&#33021;&#22815;&#39640;&#25928;&#22320;&#25214;&#21040;&#36825;&#26679;&#19968;&#20010;&#20449;&#20219;&#21306;&#22495;&#26377;&#22810;&#37325;&#22909;&#22788;&#65306;i&#65289;&#27934;&#23519;&#27169;&#22411;&#22312;&#19968;&#20010;&#21306;&#22495;&#20869;&#30340;&#34892;&#20026;&#65292;&#20855;&#26377;&#20445;&#35777;&#65307;ii&#65289;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#24471;&#21040;&#20445;&#35777;&#65307;iii&#65289;&#35299;&#37322;&#30340;&#37325;&#29992;&#65292;&#21487;&#20197;&#33410;&#30465;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#37329;&#38065;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11168v1 Announce Type: cross  Abstract: Given the black box nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of black box (probabilistic) explanation certification. We ask the question: Given a black box model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\ell_{\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a \emph{trust region} has multiple benefits: i) insight into model behavior in a \emph{region}, with a \emph{guarantee}; ii) ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse}, which can save time, energy and mone
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10991</link><description>&lt;p&gt;
&#21152;&#36895;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Semi-Asynchronous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10991
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#22312;&#20854;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#65292;&#22914;Federated Averaging&#65288;FedAvg&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24050;&#32463;&#34987;&#35777;&#26126;&#25910;&#25947;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23458;&#25143;&#31471;&#20197;&#21516;&#27493;&#26041;&#24335;&#23558;&#20854;&#26412;&#22320;&#26356;&#26032;&#19978;&#20256;&#33267;&#26381;&#21153;&#22120;&#65292;&#36825;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#21464;&#24471;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#32487;&#32493;&#20351;&#29992;&#38472;&#26087;&#30340;&#20840;&#23616;&#27169;&#22411;&#23545;&#20854;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#20165;&#32858;&#21512;&#20102;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#20854;&#30456;&#23545;&#36129;&#29486;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#38472;&#26087;&#31243;&#24230;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;</title><link>https://arxiv.org/abs/2402.10980</link><description>&lt;p&gt;
CHEMREASONER&#65306;&#20351;&#29992;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#36827;&#34892;&#21551;&#21457;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10980
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 &#31867;&#22411;&#20844;&#21578;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21457;&#29616;&#26032;&#30340;&#20652;&#21270;&#21058;&#23545;&#20110;&#35774;&#35745;&#26032;&#30340;&#26356;&#39640;&#25928;&#30340;&#21270;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#23454;&#29616;&#21521;&#21487;&#25345;&#32493;&#26410;&#26469;&#30340;&#36807;&#28193;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#19977;&#32500;&#21407;&#23376;&#34920;&#31034;&#30340;&#21453;&#39304;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#26500;&#24314;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#23548;&#30340;&#20551;&#35774;&#19982;&#22522;&#20110;&#21407;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#21453;&#39304;&#30340;&#36845;&#20195;&#32452;&#21512;&#65292;&#31215;&#26497;&#25628;&#32034;&#39640;&#25928;&#20652;&#21270;&#21058;&#12290;&#22312;&#20013;&#38388;&#25628;&#32034;&#27493;&#39588;&#30830;&#23450;&#30340;&#20652;&#21270;&#21058;&#32463;&#36807;&#22522;&#20110;&#31354;&#38388;&#23450;&#21521;&#12289;&#21453;&#24212;&#36884;&#24452;&#21644;&#31283;&#23450;&#24615;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#22522;&#20110;&#21560;&#38468;&#33021;&#21644;&#21183;&#22418;&#30340;&#35780;&#20998;&#20989;&#25968;&#24341;&#23548;&#22312;LLM&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#21521;&#33021;&#37327;&#26377;&#21033;&#12289;&#39640;&#25928;&#30340;&#20652;&#21270;&#21058;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20197;&#33258;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798; 1000 &#19975;&#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22788;&#29702;&#26368;&#38271;&#36755;&#20837;&#30340;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10790</link><description>&lt;p&gt;
&#22312;&#19968;&#20010; 1000 &#19975;&#26681;&#33609;&#22427;&#20013;&#23547;&#25214;&#38024;&#65306;&#24490;&#29615;&#35760;&#24518;&#25214;&#21040;&#20102;&#35821;&#35328;&#27169;&#22411;&#19981;&#25797;&#38271;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10790
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798; 1000 &#19975;&#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22788;&#29702;&#26368;&#38271;&#36755;&#20837;&#30340;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335; Transformer &#27169;&#22411;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; BABILong&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22312;&#25552;&#21462;&#21644;&#22788;&#29702;&#24191;&#27867;&#25991;&#26412;&#20013;&#20998;&#24067;&#24335;&#20107;&#23454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324; GPT-4 &#21644; RAG &#30340;&#22522;&#20934;&#65292;&#32467;&#26524;&#26174;&#31034;&#24120;&#35265;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#26368;&#22810; $10^4$ &#20010;&#20803;&#32032;&#30340;&#24207;&#21015;&#12290;&#30456;&#21453;&#65292;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#26368;&#22810; $10^7$ &#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#12290;&#36825;&#19968;&#25104;&#23601;&#26631;&#24535;&#30528;&#36804;&#20170;&#20026;&#27490;&#20219;&#20309;&#24320;&#28304;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22788;&#29702;&#30340;&#26368;&#38271;&#36755;&#20837;&#65292;&#26174;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10790v1 Announce Type: cross  Abstract: This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $10^7$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;Johnson-Lindenstrauss&#65288;JL&#65289;&#24341;&#29702;&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#65292;&#31616;&#21270;&#21644;&#32479;&#19968;&#20102;&#21508;&#31181;&#26500;&#36896;&#65292;&#21253;&#25324;&#29699;&#24418;&#12289;&#39640;&#26031;&#12289;&#20108;&#36827;&#21046;&#30828;&#24065;&#21644;&#27425;&#39640;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#23558;Hanson-Wright&#19981;&#31561;&#24335;&#25299;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#26631;&#24535;&#30528;&#23545;&#25968;&#25454;&#22266;&#26377;&#20960;&#20309;&#30340;&#20445;&#25345;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10232</link><description>&lt;p&gt;
Johnson-Lindenstrauss&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Simple, unified analysis of Johnson-Lindenstrauss with applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10232
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;Johnson-Lindenstrauss&#65288;JL&#65289;&#24341;&#29702;&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#65292;&#31616;&#21270;&#21644;&#32479;&#19968;&#20102;&#21508;&#31181;&#26500;&#36896;&#65292;&#21253;&#25324;&#29699;&#24418;&#12289;&#39640;&#26031;&#12289;&#20108;&#36827;&#21046;&#30828;&#24065;&#21644;&#27425;&#39640;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#21019;&#26032;&#24615;&#22320;&#23558;Hanson-Wright&#19981;&#31561;&#24335;&#25299;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#26631;&#24535;&#30528;&#23545;&#25968;&#25454;&#22266;&#26377;&#20960;&#20309;&#30340;&#20445;&#25345;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Johnson-Lindenstrauss&#65288;JL&#65289;&#24341;&#29702;&#30340;&#31616;&#21333;&#32479;&#19968;&#20998;&#26512;&#65292;&#36825;&#26159;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#30340;&#38477;&#32500;&#39046;&#22495;&#20013;&#30340;&#22522;&#30707;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#31616;&#21270;&#20102;&#29702;&#35299;&#65292;&#36824;&#23558;&#21508;&#31181;&#26500;&#36896;&#32479;&#19968;&#21040;JL&#26694;&#26550;&#19979;&#65292;&#21253;&#25324;&#29699;&#24418;&#12289;&#39640;&#26031;&#12289;&#20108;&#36827;&#21046;&#30828;&#24065;&#21644;&#27425;&#39640;&#26031;&#27169;&#22411;&#12290;&#36825;&#31181;&#31616;&#21270;&#21644;&#32479;&#19968;&#22312;&#20445;&#25345;&#25968;&#25454;&#22266;&#26377;&#20960;&#20309;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23545;&#20174;&#27969;&#31639;&#27861;&#21040;&#24378;&#21270;&#23398;&#20064;&#31561;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#31616;&#21270;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#29699;&#24418;&#26500;&#36896;&#26377;&#25928;&#24615;&#30340;&#31532;&#19968;&#20010;&#20005;&#26684;&#35777;&#26126;&#12290;&#25105;&#20204;&#36129;&#29486;&#30340;&#26680;&#24515;&#26159;&#23558;Hanson-Wright&#19981;&#31561;&#24335;&#25299;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#20855;&#26377;&#26126;&#30830;&#30340;&#24120;&#25968;&#65292;&#36825;&#26631;&#24535;&#30528;&#25991;&#29486;&#20013;&#36136;&#30340;&#39134;&#36291;&#12290;&#36890;&#36807;&#36816;&#29992;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#27010;&#29575;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10232v1 Announce Type: new  Abstract: In this work, we present a simple and unified analysis of the Johnson-Lindenstrauss (JL) lemma, a cornerstone in the field of dimensionality reduction critical for managing high-dimensional data. Our approach not only simplifies the understanding but also unifies various constructions under the JL framework, including spherical, Gaussian, binary coin, and sub-Gaussian models. This simplification and unification make significant strides in preserving the intrinsic geometry of data, essential across diverse applications from streaming algorithms to reinforcement learning. Notably, we deliver the first rigorous proof of the spherical construction's effectiveness within this simplified framework. At the heart of our contribution is an innovative extension of the Hanson-Wright inequality to high dimensions, complete with explicit constants, marking a substantial leap in the literature. By employing simple yet powerful probabilistic tools and 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#21644;&#24102;&#23485;&#20998;&#37197;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#36866;&#24212;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#24322;&#26500;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10097</link><description>&lt;p&gt;
&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#20013;&#20855;&#26377;&#29420;&#31435;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10097
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#21644;&#24102;&#23485;&#20998;&#37197;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#36866;&#24212;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#24322;&#26500;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#23458;&#25143;&#31471;&#36827;&#34892;&#38543;&#26426;&#23376;&#38598;&#37319;&#26679;&#26469;&#35299;&#20915;&#36831;&#21040;&#32773;&#38382;&#39064;&#24182;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#32852;&#21512;&#31995;&#32479;&#21644;&#25968;&#25454;&#24322;&#26500;&#35774;&#35745;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21487;&#33021;&#19982;&#23454;&#38469;&#30340;&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#19981;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20513;&#19968;&#31181;&#26032;&#30340;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#23454;&#38469;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#32771;&#34385;&#36890;&#20449;&#21644;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#24102;&#26377;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#30340;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#25910;&#25947;&#30028;&#38480;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24102;&#23485;&#20998;&#37197;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#25947;&#36718;&#25968;&#19978;&#30028;&#21644;&#27599;&#36718;&#39044;&#26399;&#35757;&#32451;&#26102;&#38388;&#30340;&#39640;&#25928;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#23454;&#38469;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10097v1 Announce Type: new  Abstract: Federated Learning (FL) algorithms commonly sample a random subset of clients to address the straggler issue and improve communication efficiency. While recent works have proposed various client sampling methods, they have limitations in joint system and data heterogeneity design, which may not align with practical heterogeneous wireless networks. In this work, we advocate a new independent client sampling strategy to minimize the wall-clock training time of FL, while considering data heterogeneity and system heterogeneity in both communication and computation. We first derive a new convergence bound for non-convex loss functions with independent client sampling and then propose an adaptive bandwidth allocation scheme. Furthermore, we propose an efficient independent client sampling algorithm based on the upper bounds on the convergence rounds and the expected per-round training time, to minimize the wall-clock time of FL, while consider
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#21253;&#25324;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#25968;&#25454;&#25216;&#26415;&#31561;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09660</link><description>&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
User Modeling and User Profiling: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09660
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#21253;&#25324;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#25968;&#25454;&#25216;&#26415;&#31561;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24050;&#32463;&#20419;&#20351;&#20808;&#36827;&#30340;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#30011;&#20687;&#25216;&#26415;&#65292;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#20307;&#39564;&#12290;&#36825;&#20123;&#25216;&#26415;&#26088;&#22312;&#22522;&#20110;&#19982;&#36825;&#20123;&#31995;&#32479;&#30340;&#20114;&#21160;&#20013;&#29983;&#25104;&#30340;&#22823;&#37327;&#25968;&#25454;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#12290;&#26412;&#25991;&#23545;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21382;&#21490;&#27010;&#36848;&#65292;&#36861;&#28335;&#20102;&#20174;&#26089;&#26399;&#30340;&#21051;&#26495;&#27169;&#22411;&#21040;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#28085;&#30422;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#25152;&#26377;&#27963;&#21160;&#20027;&#39064;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#31361;&#20986;&#20102;&#21521;&#26356;&#22797;&#26434;&#30340;&#29992;&#25143;&#30011;&#20687;&#26041;&#27861;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#38544;&#24335;&#25968;&#25454;&#25910;&#38598;&#12289;&#22810;&#34892;&#20026;&#24314;&#27169;&#20197;&#21450;&#22270;&#25968;&#25454;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09660v1 Announce Type: new  Abstract: The integration of artificial intelligence (AI) into daily life, particularly through information retrieval and recommender systems, has necessitated advanced user modeling and profiling techniques to deliver personalized experiences. These techniques aim to construct accurate user representations based on the rich amounts of data generated through interactions with these systems. This paper presents a comprehensive survey of the current state, evolution, and future directions of user modeling and profiling research. We provide a historical overview, tracing the development from early stereotype models to the latest deep learning techniques, and propose a novel taxonomy that encompasses all active topics in this research area, including recent trends. Our survey highlights the paradigm shifts towards more sophisticated user profiling methods, emphasizing implicit data collection, multi-behavior modeling, and the integration of graph data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;, &#39640;&#20142;&#20102;&#22312;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#21644;&#35299;&#37322;&#24471;&#20986;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19978;&#30340;&#22256;&#38590;&#24615;</title><link>https://arxiv.org/abs/2402.09056</link><description>&lt;p&gt;
&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#21542;&#20934;&#30830;&#22320;&#34920;&#31034;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;, &#39640;&#20142;&#20102;&#22312;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#21644;&#35299;&#37322;&#24471;&#20986;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19978;&#30340;&#22256;&#38590;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19981;&#20165;&#24212;&#36820;&#22238;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36824;&#24212;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#24120;&#29992;&#20110;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#36817;&#24180;&#26469;&#65292;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#31561;&#26367;&#20195;&#26041;&#27861;&#20063;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#21518;&#32773;&#26412;&#36136;&#19978;&#25193;&#23637;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#26524;&#30340;&#20108;&#38454;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#20013;&#21487;&#20197;&#25552;&#21462;&#35748;&#35782;&#65288;&#21644;&#38543;&#26426;&#65289;&#19981;&#30830;&#23450;&#24615;&#30340;&#24230;&#37327;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;&#65292;&#24378;&#35843;&#20102;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#35299;&#37322;&#32467;&#26524;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#24615;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#35774;&#32622;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#35745;&#25968;&#30340;&#24191;&#27867;&#26041;&#27861;&#65292;&#36825;&#31687;&#35770;&#25991;&#20026;&#21487;&#36776;&#35782;&#24615;&#21644;&#25910;&#25947;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09056v1 Announce Type: new Abstract: Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-o
&lt;/p&gt;</description></item><item><title>SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08653</link><description>&lt;p&gt;
SAGMAN: &#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#24418;&#19978;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08653
&lt;/p&gt;
&lt;p&gt;
SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23545;&#36755;&#20837;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;SAGMAN&#30340;&#35889;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#39564;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;GNN&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#27969;&#24418;&#20043;&#38388;&#24341;&#36215;&#30340;&#36317;&#31163;&#22833;&#30495;: &#24403;&#36755;&#20837;&#27969;&#34892;&#20013;&#20004;&#20010;&#38468;&#36817;&#30340;&#33410;&#28857;&#65288;&#36890;&#36807;GNN&#27169;&#22411;&#65289;&#34987;&#26144;&#23556;&#21040;&#36755;&#20986;&#27969;&#34892;&#19978;&#30340;&#20004;&#20010;&#36828;&#31163;&#30340;&#33410;&#28857;&#26102;&#65292;&#24847;&#21619;&#30528;&#23384;&#22312;&#36739;&#22823;&#30340;&#36317;&#31163;&#22833;&#30495;&#65292;&#20174;&#32780;&#23548;&#33268;GNN&#30340;&#31283;&#23450;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#65288;GDR&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#35889;&#22270;&#23884;&#20837;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#26469;&#21019;&#24314;&#20302;&#32500;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22522;&#20110;&#22270;&#30340;&#27969;&#24418;&#65292;&#20197;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;SAGMAN&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#27599;&#20010;&#33410;&#28857;&#22312;&#38754;&#23545;&#19981;&#21516;&#36793;&#32536;&#25110;&#29305;&#24449;&#25200;&#21160;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;&#65292;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#21363;&#27169;&#22411;&#36234;&#22823;&#25110;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#24182;&#19981;&#19968;&#23450;&#20250;&#38543;&#20043;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20960;&#20010;&#25112;&#30053;&#29615;&#22659;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.07588</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Rethinking Scaling Laws for Learning in Strategic Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;&#65292;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#21363;&#27169;&#22411;&#36234;&#22823;&#25110;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#24182;&#19981;&#19968;&#23450;&#20250;&#38543;&#20043;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20960;&#20010;&#25112;&#30053;&#29615;&#22659;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37096;&#32626;&#21453;&#26144;&#20986;&#19968;&#20010;&#20849;&#35782;&#65306;&#27169;&#22411;&#36234;&#26377;&#34920;&#36798;&#33021;&#21147;&#65292;&#36234;&#25317;&#26377;&#22823;&#37327;&#25968;&#25454;&#65292;&#23601;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;&#38543;&#30528;&#27169;&#22411;&#22312;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#30528;&#25112;&#30053;&#29615;&#22659;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#27169;&#22411;&#19982;&#25112;&#30053;&#20114;&#21160;&#23545;&#27604;&#20363;&#23450;&#24459;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36825;&#20010;&#33258;&#28982;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#27604;&#20363;&#23450;&#24459;&#35266;&#28857;&#65292;&#21363;&#24615;&#33021;&#24182;&#19981;&#19968;&#23450;&#38543;&#30528;&#27169;&#22411;&#30340;&#25193;&#22823;&#21644;/&#25110;&#34920;&#36798;&#33021;&#21147;&#30340;&#22686;&#24378;&#65288;&#21363;&#20351;&#26377;&#26080;&#38480;&#25968;&#25454;&#65289;&#32780;&#21333;&#35843;&#25552;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#25112;&#30053;&#22238;&#24402;&#12289;&#25112;&#30053;&#20998;&#31867;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#20363;&#23376;&#23637;&#31034;&#20102;&#36825;&#19968;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#20363;&#23376;&#23637;&#31034;&#20102;&#25112;&#30053;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#27169;&#22411;&#25110;&#31574;&#30053;&#31867;&#30340;&#34920;&#36798;&#33021;&#21147;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\unicode{x2013}$and the more data one has access to$\unicode{x2013}$the more one can improve performance. As models get deployed in a variety of real world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects scaling laws. We find that strategic interactions can break the conventional view of scaling laws$\unicode{x2013}$meaning that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data). We show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning through examples of strategic environments in which$\unicode{x2013}$by simply restricting the expressivity of one's model or policy class$\uni
&lt;/p&gt;</description></item><item><title>&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#21487;&#33719;&#24471;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#65292;&#19982;&#26410;&#32463;&#38408;&#20540;&#22788;&#29702;&#30340;Oja&#21521;&#37327;&#30456;&#27604;&#65292;&#36825;&#22823;&#22823;&#20943;&#23567;&#20102;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.07240</link><description>&lt;p&gt;
&#38408;&#20540;Oja&#26159;&#21542;&#36866;&#29992;&#20110;&#31232;&#30095;PCA&#65311;
&lt;/p&gt;
&lt;p&gt;
Thresholded Oja does Sparse PCA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07240
&lt;/p&gt;
&lt;p&gt;
&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#21487;&#33719;&#24471;&#19968;&#20010;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#65292;&#19982;&#26410;&#32463;&#38408;&#20540;&#22788;&#29702;&#30340;Oja&#21521;&#37327;&#30456;&#27604;&#65292;&#36825;&#22823;&#22823;&#20943;&#23567;&#20102;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#24403;&#27604;&#20540;$d/n \rightarrow c &gt; 0$&#26102;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#30340;&#38382;&#39064;&#12290;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#65292;&#20851;&#20110;&#31232;&#30095;PCA&#30340;&#26368;&#20248;&#29575;&#24050;&#32463;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#20854;&#20013;&#25152;&#26377;&#25968;&#25454;&#37117;&#21487;&#20197;&#29992;&#20110;&#22810;&#27425;&#20256;&#36882;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#21475;&#29305;&#24449;&#21521;&#37327;&#26159;$s$-&#31232;&#30095;&#26102;&#65292;&#20855;&#26377;$O(d)$&#23384;&#20648;&#21644;$O(nd)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#27969;&#31639;&#27861;&#36890;&#24120;&#35201;&#27714;&#24378;&#21021;&#22987;&#21270;&#26465;&#20214;&#65292;&#21542;&#21017;&#20250;&#26377;&#27425;&#20248;&#38169;&#35823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#23545;Oja&#31639;&#27861;&#30340;&#36755;&#20986;&#65288;Oja&#21521;&#37327;&#65289;&#36827;&#34892;&#38408;&#20540;&#21644;&#37325;&#26032;&#24402;&#19968;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#38169;&#35823;&#29575;&#12290;&#36825;&#38750;&#24120;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#27809;&#26377;&#38408;&#20540;&#65292;Oja&#21521;&#37327;&#30340;&#35823;&#24046;&#24456;&#22823;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#38480;&#21046;&#26410;&#24402;&#19968;&#21270;&#30340;Oja&#21521;&#37327;&#30340;&#39033;&#19978;&#65292;&#36825;&#28041;&#21450;&#23558;&#19968;&#32452;&#29420;&#31435;&#38543;&#26426;&#30697;&#38453;&#30340;&#20056;&#31215;&#22312;&#38543;&#26426;&#21021;&#22987;&#21521;&#37327;&#19978;&#30340;&#25237;&#24433;&#12290; &#36825;&#26159;&#38750;&#24179;&#20961;&#19988;&#26032;&#39062;&#30340;&#65292;&#22240;&#20026;&#20197;&#21069;&#30340;Oja&#31639;&#27861;&#20998;&#26512;&#27809;&#26377;&#32771;&#34385;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.07240v2 Announce Type: cross  Abstract: We consider the problem of Sparse Principal Component Analysis (PCA) when the ratio $d/n \rightarrow c &gt; 0$. There has been a lot of work on optimal rates on sparse PCA in the offline setting, where all the data is available for multiple passes. In contrast, when the population eigenvector is $s$-sparse, streaming algorithms that have $O(d)$ storage and $O(nd)$ time complexity either typically require strong initialization conditions or have a suboptimal error. We show that a simple algorithm that thresholds and renormalizes the output of Oja's algorithm (the Oja vector) obtains a near-optimal error rate. This is very surprising because, without thresholding, the Oja vector has a large error. Our analysis centers around bounding the entries of the unnormalized Oja vector, which involves the projection of a product of independent random matrices on a random initial vector. This is nontrivial and novel since previous analyses of Oja's al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TREET&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Donsker-Vardhan&#34920;&#31034;&#27861;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#31283;&#23450;&#36807;&#31243;&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20272;&#35745;TE&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#20248;&#21270;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#21644;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06919</link><description>&lt;p&gt;
TREET: &#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
TREET: TRansfer Entropy Estimation via Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TREET&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Donsker-Vardhan&#34920;&#31034;&#27861;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#31283;&#23450;&#36807;&#31243;&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20272;&#35745;TE&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#20248;&#21270;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#21644;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#36755;&#29109;&#65288;TE&#65289;&#26159;&#20449;&#24687;&#35770;&#20013;&#25581;&#31034;&#36807;&#31243;&#20043;&#38388;&#20449;&#24687;&#27969;&#21160;&#26041;&#21521;&#30340;&#24230;&#37327;&#65292;&#23545;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREET&#30340;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#31283;&#23450;&#36807;&#31243;&#30340;TE&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;Donsker-Vardhan&#65288;DV&#65289;&#34920;&#31034;&#27861;&#23545;TE&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31070;&#32463;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;TREET&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#20102;&#22686;&#21152;&#20854;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#34920;&#31034;&#24341;&#29702;&#30340;&#20272;&#35745;TE&#20248;&#21270;&#26041;&#26696;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#26469;&#20248;&#21270;&#20855;&#26377;&#35760;&#24518;&#24615;&#30340;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#65292;&#36825;&#26159;&#20449;&#24687;&#35770;&#20013;&#30340;&#19968;&#20010;&#20856;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer entropy (TE) is a measurement in information theory that reveals the directional flow of information between processes, providing valuable insights for a wide range of real-world applications. This work proposes Transfer Entropy Estimation via Transformers (TREET), a novel transformer-based approach for estimating the TE for stationary processes. The proposed approach employs Donsker-Vardhan (DV) representation to TE and leverages the attention mechanism for the task of neural estimation. We propose a detailed theoretical and empirical study of the TREET, comparing it to existing methods. To increase its applicability, we design an estimated TE optimization scheme that is motivated by the functional representation lemma. Afterwards, we take advantage of the joint optimization scheme to optimize the capacity of communication channels with memory, which is a canonical optimization problem in information theory, and show the memory capabilities of our estimator. Finally, we apply
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35302;&#35273;&#21453;&#39304;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#22475;&#34255;&#30340;&#29289;&#20307;&#12290;&#36890;&#36807;&#27169;&#25311;&#20256;&#24863;&#22120;&#22122;&#22768;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#20986;&#29616;&#30340;&#23398;&#20064;&#25512;&#21160;&#34892;&#20026;&#65292;&#24182;&#25104;&#21151;&#23558;&#20854;&#36801;&#31227;&#21040;&#23454;&#38469;&#30828;&#20214;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.04536</link><description>&lt;p&gt;
&#22522;&#20110;&#35302;&#35273;&#30340;&#20174;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#29289;&#20307;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tactile-based Object Retrieval From Granular Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04536
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35302;&#35273;&#21453;&#39304;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#22475;&#34255;&#30340;&#29289;&#20307;&#12290;&#36890;&#36807;&#27169;&#25311;&#20256;&#24863;&#22120;&#22122;&#22768;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#20986;&#29616;&#30340;&#23398;&#20064;&#25512;&#21160;&#34892;&#20026;&#65292;&#24182;&#25104;&#21151;&#23558;&#20854;&#36801;&#31227;&#21040;&#23454;&#38469;&#30828;&#20214;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GEOTACT&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#22475;&#34255;&#30340;&#29289;&#20307;&#12290;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38656;&#35201;&#19982;&#39063;&#31890;&#20171;&#36136;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#19988;&#20165;&#20381;&#38752;&#35302;&#35273;&#21453;&#39304;&#26469;&#23436;&#25104;&#65292;&#22240;&#20026;&#19968;&#20010;&#22475;&#34255;&#30340;&#29289;&#20307;&#21487;&#33021;&#23436;&#20840;&#34987;&#35270;&#35273;&#38544;&#34255;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#35302;&#35273;&#21453;&#39304;&#26412;&#36523;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#19982;&#21608;&#22260;&#20171;&#36136;&#36827;&#34892;&#26222;&#36941;&#25509;&#35302;&#65292;&#24182;&#19988;&#30001;&#35302;&#35273;&#35835;&#25968;&#24341;&#36215;&#30340;&#22266;&#26377;&#22122;&#22768;&#27700;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#25311;&#20256;&#24863;&#22120;&#22122;&#22768;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38382;&#39064;&#34920;&#36848;&#23548;&#33268;&#20102;&#23398;&#20064;&#25512;&#21160;&#34892;&#20026;&#30340;&#33258;&#28982;&#20986;&#29616;&#65292;&#25805;&#20316;&#22120;&#20351;&#29992;&#36825;&#20123;&#34892;&#20026;&#26469;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#24182;&#23558;&#29289;&#20307;&#24341;&#23548;&#21040;&#31283;&#23450;&#30340;&#25235;&#21462;&#20301;&#32622;&#65292;&#23613;&#31649;&#23384;&#22312;&#20551;&#30340;&#21644;&#22122;&#22768;&#30340;&#35302;&#35273;&#35835;&#25968;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22521;&#35757;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20223;&#30495;&#20013;&#23398;&#20064;&#36825;&#20123;&#34892;&#20026;&#65292;&#24182;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;GEOTACT&#26159;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media. This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision. Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings. To address these challenges, we use a learning method trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We also introduce a training curriculum that enables learning these behaviors in simulation, followed by zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is the first meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FDA&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#39046;&#22495;&#23398;&#20064;&#26469;&#25171;&#30772;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#20013;&#30340;&#25968;&#25454;&#23396;&#23707;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21487;&#23398;&#20064;&#29305;&#24449;&#34917;&#20607;&#27169;&#22359;&#21644;&#20998;&#24067;&#24863;&#30693;&#32479;&#35745;&#19968;&#33268;&#24615;&#27169;&#22359;&#65292;&#29992;&#20110;&#22686;&#24378;&#20013;&#38388;&#29305;&#24449;&#20132;&#27969;&#21644;&#25968;&#25454;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04273</link><description>&lt;p&gt;
&#25171;&#30772;&#25968;&#25454;&#23396;&#23707;&#65306;&#36328;&#39046;&#22495;&#23398;&#20064;&#23454;&#29616;&#29420;&#31435;&#31169;&#26377;&#28304;&#30340;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Breaking Data Silos: Cross-Domain Learning for Multi-Agent Perception from Independent Private Sources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FDA&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#36328;&#39046;&#22495;&#23398;&#20064;&#26469;&#25171;&#30772;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#20013;&#30340;&#25968;&#25454;&#23396;&#23707;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21487;&#23398;&#20064;&#29305;&#24449;&#34917;&#20607;&#27169;&#22359;&#21644;&#20998;&#24067;&#24863;&#30693;&#32479;&#35745;&#19968;&#33268;&#24615;&#27169;&#22359;&#65292;&#29992;&#20110;&#22686;&#24378;&#20013;&#38388;&#29305;&#24449;&#20132;&#27969;&#21644;&#25968;&#25454;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#31995;&#32479;&#20013;&#30340;&#19981;&#21516;&#26234;&#33021;&#20307;&#21487;&#33021;&#26469;&#33258;&#19981;&#21516;&#20844;&#21496;&#12290;&#27599;&#20010;&#20844;&#21496;&#21487;&#33021;&#37117;&#20351;&#29992;&#30456;&#21516;&#30340;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21508;&#20010;&#26234;&#33021;&#20307;&#30340;&#25968;&#25454;&#28304;&#22312;&#27599;&#20010;&#20844;&#21496;&#20013;&#26159;&#30456;&#20114;&#29420;&#31435;&#21644;&#31169;&#26377;&#30340;&#65292;&#23548;&#33268;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#31995;&#32479;&#20013;&#35757;&#32451;&#19981;&#21516;&#26234;&#33021;&#20307;&#30340;&#19981;&#21516;&#31169;&#26377;&#25968;&#25454;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;&#19978;&#36848;&#20998;&#24067;&#24046;&#24322;&#36896;&#25104;&#30340;&#25968;&#25454;&#23396;&#23707;&#21487;&#33021;&#23548;&#33268;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#20998;&#24067;&#24046;&#24322;&#23545;&#29616;&#26377;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#25171;&#30772;&#25968;&#25454;&#23396;&#23707;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38754;&#21521;&#36328;&#39046;&#22495;&#23398;&#20064;&#30340;&#29305;&#24449;&#20998;&#24067;&#24863;&#30693;&#32858;&#21512;&#65288;FDA&#65289;&#26694;&#26550;&#65292;&#20197;&#20943;&#36731;&#22810;&#26234;&#33021;&#20307;&#24863;&#30693;&#20013;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;FDA&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#21487;&#23398;&#20064;&#29305;&#24449;&#34917;&#20607;&#27169;&#22359;&#21644;&#20998;&#24067;&#24863;&#30693;&#32479;&#35745;&#19968;&#33268;&#24615;&#27169;&#22359;&#65292;&#26088;&#22312;&#21152;&#24378;&#20013;&#38388;&#29305;&#24449;&#20132;&#27969;&#21644;&#25968;&#25454;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The diverse agents in multi-agent perception systems may be from different companies. Each company might use the identical classic neural network architecture based encoder for feature extraction. However, the data source to train the various agents is independent and private in each company, leading to the Distribution Gap of different private data for training distinct agents in multi-agent perception system. The data silos by the above Distribution Gap could result in a significant performance decline in multi-agent perception. In this paper, we thoroughly examine the impact of the distribution gap on existing multi-agent perception systems. To break the data silos, we introduce the Feature Distribution-aware Aggregation (FDA) framework for cross-domain learning to mitigate the above Distribution Gap in multi-agent perception. FDA comprises two key components: Learnable Feature Compensation Module and Distribution-aware Statistical Consistency Module, both aimed at enhancing interme
&lt;/p&gt;</description></item><item><title>SEABO&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#26681;&#25454;&#19987;&#23478;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#24471;&#21040;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.03807</link><description>&lt;p&gt;
SEABO: &#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SEABO: A Simple Search-Based Method for Offline Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03807
&lt;/p&gt;
&lt;p&gt;
SEABO&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#26681;&#25454;&#19987;&#23478;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#24471;&#21040;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30001;&#20110;&#33021;&#22815;&#20174;&#38745;&#24577;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24182;&#28040;&#38500;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#38656;&#27714;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31163;&#32447;RL&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#26631;&#26377;&#22870;&#21169;&#26631;&#31614;&#30340;&#31163;&#32447;&#36716;&#25442;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#26377;&#26102;&#26159;&#22256;&#38590;&#30340;&#12289;&#21171;&#21160;&#23494;&#38598;&#30340;&#25110;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25226;&#37325;&#28857;&#25918;&#22312;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#35774;&#32622;&#19978;&#65292;&#26088;&#22312;&#22522;&#20110;&#19987;&#23478;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#24471;&#21040;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;IL&#26041;&#27861;&#65292;&#31216;&#20026;SEABO&#12290;SEABO&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#23558;&#36739;&#22823;&#30340;&#22870;&#21169;&#20998;&#37197;&#32473;&#19982;&#19987;&#23478;&#28436;&#31034;&#20013;&#26368;&#25509;&#36817;&#30340;&#36716;&#25442;&#65292;&#21542;&#21017;&#20998;&#37197;&#36739;&#23567;&#30340;&#22870;&#21169;&#12290;&#22312;&#22810;&#20010;D4RL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SEABO&#33021;&#22815;&#36798;&#21040;&#19982;&#31163;&#32447;RL&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.03781</link><description>&lt;p&gt;
MolTC: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20998;&#23376;&#20851;&#31995;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolTC: Towards Molecular Relational Modeling In Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#65288;MRL&#65289;&#26088;&#22312;&#29702;&#35299;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#25512;&#36827;&#29983;&#29289;&#21270;&#23398;&#30740;&#31350;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37319;&#29992;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MRL&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#24222;&#22823;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#21644;&#20808;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#25968;&#25454;&#65292;&#22240;&#27492;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20998;&#23376;&#22270;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#21152;&#21095;&#20102;&#20449;&#24687;&#30340;&#28010;&#36153;&#65292;&#22240;&#20026;&#23427;&#38459;&#30861;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#30456;&#20114;&#20316;&#29992;&#29702;&#30001;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#29702;&#35770;&#23545;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#39044;&#27979;&#65292;&#31216;&#20026;MolTC&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22270;&#20687;&#28210;&#26579;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#24555;&#36895;&#37325;&#26500;&#21644;&#29983;&#25104;&#30495;&#23454;&#19990;&#30028;3D&#22330;&#26223;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#21644;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03445</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#28210;&#26579;&#30340;&#21435;&#22122;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion via Image-Based Rendering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22270;&#20687;&#28210;&#26579;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#24555;&#36895;&#37325;&#26500;&#21644;&#29983;&#25104;&#30495;&#23454;&#19990;&#30028;3D&#22330;&#26223;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#21644;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;3D&#22330;&#26223;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#21512;&#25104;&#22312;&#19977;&#32500;&#31354;&#38388;&#20013;&#23436;&#20840;&#19968;&#33268;&#30340;&#21487;&#20449;&#20869;&#23481;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#22914;&#31070;&#32463;&#36752;&#23556;&#22330;&#22312;&#35270;&#22270;&#21512;&#25104;&#21644;3D&#37325;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#29983;&#25104;&#33021;&#21147;&#65292;&#23427;&#20204;&#26080;&#27861;&#21512;&#25104;&#26410;&#35266;&#23519;&#21306;&#22495;&#20013;&#30340;&#21487;&#20449;&#32454;&#33410;&#12290;&#30456;&#21453;&#65292;&#29616;&#26377;&#30340;&#29983;&#25104;&#26041;&#27861;&#36890;&#24120;&#19981;&#33021;&#22312;&#37326;&#22806;&#37325;&#26500;&#20855;&#26377;&#35814;&#32454;&#30340;&#22823;&#35268;&#27169;&#22330;&#26223;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#23481;&#37327;&#26377;&#38480;&#30340;3D&#22330;&#26223;&#34920;&#31034;&#65292;&#38656;&#35201;&#23545;&#40784;&#30340;&#30456;&#26426;&#23039;&#24577;&#25110;&#20381;&#36182;&#39069;&#22806;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39318;&#20010;&#33021;&#22815;&#24555;&#36895;&#36827;&#34892;&#35814;&#32454;&#37325;&#26500;&#21644;&#29983;&#25104;&#30495;&#23454;&#19990;&#30028;3D&#22330;&#26223;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#8212;&#8212;IB-planes&#65292;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#34920;&#31034;&#22823;&#35268;&#27169;3D&#22330;&#26223;&#65292;&#24182;&#26681;&#25454;&#27599;&#24352;&#22270;&#20687;&#20013;&#21487;&#35265;&#30340;&#32454;&#33410;&#21160;&#24577;&#20998;&#37197;&#26356;&#22810;&#23481;&#37327;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating 3D scenes is a challenging open problem, which requires synthesizing plausible content that is fully consistent in 3D space. While recent methods such as neural radiance fields excel at view synthesis and 3D reconstruction, they cannot synthesize plausible details in unobserved regions since they lack a generative capability. Conversely, existing generative methods are typically not capable of reconstructing detailed, large-scale scenes in the wild, as they use limited-capacity 3D scene representations, require aligned camera poses, or rely on additional regularizers. In this work, we introduce the first diffusion model able to perform fast, detailed reconstruction and generation of real-world 3D scenes. To achieve this, we make three contributions. First, we introduce a new neural scene representation, IB-planes, that can efficiently and accurately represent large 3D scenes, dynamically allocating more capacity as needed to capture details visible in each image. Second, we 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;&#20013;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.02695</link><description>&lt;p&gt;
&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Exploiting Class Probabilities for Black-box Sentence-level Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;&#20013;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#32423;&#25915;&#20987;&#26159;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#21477;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#36825;&#20123;&#21477;&#23376;&#19982;&#27491;&#30830;&#20998;&#31867;&#30340;&#21477;&#23376;&#21516;&#20041;&#65292;&#20294;&#34987;&#20998;&#31867;&#22120;&#38169;&#35823;&#22320;&#20998;&#31867;&#12290;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#65292;&#20998;&#31867;&#22120;&#21482;&#33021;&#36890;&#36807;&#23545;&#26597;&#35810;&#36755;&#20837;&#30340;&#21453;&#39304;&#36827;&#34892;&#35775;&#38382;&#65292;&#36825;&#20027;&#35201;&#20197;&#31867;&#21035;&#27010;&#29575;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;&#23613;&#31649;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#21487;&#20197;&#33719;&#24471;&#26356;&#24378;&#22823;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#20294;&#30001;&#20110;&#22312;&#21477;&#32423;&#25915;&#20987;&#20013;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#23384;&#22312;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#35201;&#20040;&#19981;&#20351;&#29992;&#21453;&#39304;&#65292;&#35201;&#20040;&#20165;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#40657;&#30418;&#21477;&#32423;&#25915;&#20987;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#19978;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#40657;&#30418;&#21477;&#32423;&#25915;&#20987;&#20013;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#26159;&#21542;&#20540;&#24471;&#25110;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20998;&#31867;&#22120;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#19982;&#22522;&#32447;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack's success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CTAug&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#26080;&#32541;&#25972;&#21512;&#21040;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#12290;&#36890;&#36807;&#25913;&#36827;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;&#22270;&#23398;&#20064;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#22270;&#30340;&#34920;&#24449;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17580</link><description>&lt;p&gt;
&#20855;&#26377;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning with Cohesive Subgraph Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CTAug&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#26080;&#32541;&#25972;&#21512;&#21040;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#12290;&#36890;&#36807;&#25913;&#36827;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;&#22270;&#23398;&#20064;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#22270;&#30340;&#34920;&#24449;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#24050;&#25104;&#20026;&#23398;&#20064;&#21508;&#31181;&#22270;&#34920;&#24449;&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#21253;&#25324;&#31038;&#20132;&#21644;&#29983;&#29289;&#21307;&#23398;&#32593;&#32476;&#12290;GCL&#24191;&#27867;&#20351;&#29992;&#38543;&#26426;&#22270;&#25299;&#25169;&#22686;&#24378;&#65292;&#22914;&#22343;&#21248;&#33410;&#28857;&#20002;&#22833;&#65292;&#29983;&#25104;&#22686;&#24378;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#38543;&#26426;&#22686;&#24378;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#24182;&#24694;&#21270;&#21518;&#32493;&#30340;&#34920;&#24449;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#22270;&#22686;&#24378;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#26377;&#21487;&#33021;&#25552;&#39640;GCL&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CTAug&#30340;&#26032;&#39062;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#26080;&#32541;&#22320;&#23558;&#20869;&#32858;&#24847;&#35782;&#25972;&#21512;&#21040;&#21508;&#31181;&#29616;&#26377;&#30340;GCL&#26426;&#21046;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;CTAug&#21253;&#25324;&#20004;&#20010;&#19987;&#38376;&#30340;&#27169;&#22359;&#65306;&#25299;&#25169;&#22686;&#24378;&#22686;&#24378;&#21644;&#22270;&#23398;&#20064;&#22686;&#24378;&#12290;&#21069;&#32773;&#29983;&#25104;&#35880;&#24910;&#20445;&#30041;&#20869;&#32858;&#24615;&#36136;&#30340;&#22686;&#24378;&#22270;&#65292;&#32780;&#21518;&#32773;&#22686;&#24378;&#20102;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL) has emerged as a state-of-the-art strategy for learning representations of diverse graphs including social and biomedical networks. GCL widely uses stochastic graph topology augmentation, such as uniform node dropping, to generate augmented graphs. However, such stochastic augmentations may severely damage the intrinsic properties of a graph and deteriorate the following representation learning process. We argue that incorporating an awareness of cohesive subgraphs during the graph augmentation and learning processes has the potential to enhance GCL performance. To this end, we propose a novel unified framework called CTAug, to seamlessly integrate cohesion awareness into various existing GCL mechanisms. In particular, CTAug comprises two specialized modules: topology augmentation enhancement and graph learning enhancement. The former module generates augmented graphs that carefully preserve cohesion properties, while the latter module bolsters the grap
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#26368;&#36817;&#27169;&#20223;&#23398;&#20064;&#21644;&#20445;&#23432;RL&#31639;&#27861;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#31163;&#32447;&#21160;&#21147;&#23398;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23569;&#26679;&#26412;&#36716;&#31227;&#36807;&#31243;&#20013;&#24341;&#20837;&#24809;&#32602;&#26469;&#35843;&#33410;&#28304;&#35757;&#32451;&#31574;&#30053;&#29983;&#25104;&#30340;&#36712;&#36857;&#12290;</title><link>https://arxiv.org/abs/2312.15474</link><description>&lt;p&gt;
&#22312;&#31163;&#32447;&#21160;&#21147;&#23398;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23569;&#26679;&#26412;&#36716;&#31227;&#30340;&#20445;&#23432;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Conservative Approach for Few-Shot Transfer in Off-Dynamics Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15474
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#26368;&#36817;&#27169;&#20223;&#23398;&#20064;&#21644;&#20445;&#23432;RL&#31639;&#27861;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#22312;&#31163;&#32447;&#21160;&#21147;&#23398;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23569;&#26679;&#26412;&#36716;&#31227;&#36807;&#31243;&#20013;&#24341;&#20837;&#24809;&#32602;&#26469;&#35843;&#33410;&#28304;&#35757;&#32451;&#31574;&#30053;&#29983;&#25104;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#21160;&#21147;&#23398;&#24378;&#21270;&#23398;&#20064;&#65288;ODRL&#65289;&#26088;&#22312;&#23558;&#31574;&#30053;&#20174;&#28304;&#29615;&#22659;&#36716;&#31227;&#21040;&#20855;&#26377;&#19981;&#21516;&#20294;&#30456;&#20284;&#21160;&#21147;&#23398;&#29305;&#24449;&#30340;&#30446;&#26631;&#29615;&#22659;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20256;&#32479;RL&#20195;&#29702;&#36807;&#24230;&#20381;&#36182;&#28304;&#29615;&#22659;&#30340;&#21160;&#21147;&#23398;&#65292;&#23548;&#33268;&#21457;&#29616;&#22312;&#35813;&#29615;&#22659;&#20013;&#34920;&#29616;&#21331;&#36234;&#30340;&#31574;&#30053;&#65292;&#20294;&#22312;&#30446;&#26631;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#23569;&#26679;&#26412;&#26694;&#26550;&#20013;&#65292;&#24341;&#20837;&#20102;&#26469;&#33258;&#30446;&#26631;&#29615;&#22659;&#30340;&#26377;&#38480;&#25968;&#37327;&#36716;&#25442;&#20197;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#26368;&#36817;&#27169;&#20223;&#23398;&#20064;&#21644;&#20445;&#23432;RL&#31639;&#27861;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#24809;&#32602;&#26469;&#35843;&#33410;&#28304;&#35757;&#32451;&#31574;&#30053;&#29983;&#25104;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#19981;&#21516;&#31163;&#32447;&#21160;&#21147;&#23398;&#26465;&#20214;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#35775;&#38382;&#30446;&#26631;&#29615;&#22659;&#26159;&#26497;&#31471;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15474v2 Announce Type: replace  Abstract: Off-dynamics Reinforcement Learning (ODRL) seeks to transfer a policy from a source environment to a target environment characterized by distinct yet similar dynamics. In this context, traditional RL agents depend excessively on the dynamics of the source environment, resulting in the discovery of policies that excel in this environment but fail to provide reasonable performance in the target one. In the few-shot framework, a limited number of transitions from the target environment are introduced to facilitate a more effective transfer. Addressing this challenge, we propose an innovative approach inspired by recent advancements in Imitation Learning and conservative RL algorithms. The proposed method introduces a penalty to regulate the trajectories generated by the source-trained policy. We evaluate our method across various environments representing diverse off-dynamics conditions, where access to the target environment is extreme
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#28040;&#24687;&#20195;&#29702;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#27969;&#34892;&#24179;&#21488;&#65292;&#20026;&#25968;&#25454;&#20013;&#24515;GenAI&#27169;&#22411;&#30340;&#38656;&#27714;&#22686;&#21152;&#25552;&#20379;&#20102;&#20581;&#22766;&#30340;&#25968;&#25454;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;</title><link>https://arxiv.org/abs/2312.14647</link><description>&lt;p&gt;
&#38754;&#21521;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#28040;&#24687;&#20195;&#29702;&#65306;&#35843;&#30740;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Towards Message Brokers for Generative AI: Survey, Challenges, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#28040;&#24687;&#20195;&#29702;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#27969;&#34892;&#24179;&#21488;&#65292;&#20026;&#25968;&#25454;&#20013;&#24515;GenAI&#27169;&#22411;&#30340;&#38656;&#27714;&#22686;&#21152;&#25552;&#20379;&#20102;&#20581;&#22766;&#30340;&#25968;&#25454;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#21270;&#19990;&#30028;&#20013;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#25193;&#23637;&#20854;&#24433;&#21709;&#33539;&#22260;&#33267;&#21508;&#31181;&#24212;&#29992;&#12290;&#36825;&#31181;&#37319;&#29992;&#28608;&#22686;&#24341;&#21457;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;GenAI&#27169;&#22411;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#21152;&#65292;&#31361;&#26174;&#20986;&#20581;&#22766;&#30340;&#25968;&#25454;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#30340;&#24517;&#35201;&#24615;&#12290;&#28040;&#24687;&#20195;&#29702;&#22312;&#36825;&#19968;&#38656;&#27714;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#20204;&#20316;&#20026;&#21508;&#20010;&#31995;&#32479;&#32452;&#20214;&#20043;&#38388;&#25968;&#25454;&#20256;&#36755;&#30340;&#37325;&#35201;&#36890;&#36947;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#28145;&#20837;&#20998;&#26512;&#20256;&#32479;&#21644;&#29616;&#20195;&#28040;&#24687;&#20195;&#29702;&#65292;&#25552;&#20379;&#27969;&#34892;&#24179;&#21488;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32771;&#34385;&#20102;&#35768;&#22810;&#26631;&#20934;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#24320;&#28304;&#21487;&#29992;&#24615;&#12289;&#38598;&#25104;&#30417;&#25511;&#24037;&#20855;&#12289;&#28040;&#24687;&#20248;&#20808;&#32423;&#26426;&#21046;&#12289;&#24182;&#34892;&#22788;&#29702;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#12289;&#20998;&#21457;&#21644;&#38598;&#32676;&#21151;&#33021;&#12289;&#35748;&#35777;&#27969;&#31243;&#12289;&#25968;&#25454;&#25345;&#20037;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14647v2 Announce Type: replace-cross  Abstract: In today's digital world, Generative Artificial Intelligence (GenAI) such as Large Language Models (LLMs) is becoming increasingly prevalent, extending its reach across diverse applications. This surge in adoption has sparked a significant increase in demand for data-centric GenAI models, highlighting the necessity for robust data communication infrastructures. Central to this need are message brokers, which serve as essential channels for data transfer within various system components. This survey aims to delve into a comprehensive analysis of traditional and modern message brokers, offering a comparative study of prevalent platforms. Our study considers numerous criteria including, but not limited to, open-source availability, integrated monitoring tools, message prioritization mechanisms, capabilities for parallel processing, reliability, distribution and clustering functionalities, authentication processes, data persistence
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#24615;&#20998;&#26512;&#20559;&#24207;&#38598;&#21512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#26080;&#20132;&#24182;&#27867;&#28145;&#24230; (ufg) &#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#31034;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#22522;&#20110;ufg&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26377;&#24456;&#22823;&#21306;&#21035;&#12290;</title><link>https://arxiv.org/abs/2312.12839</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#20132;&#24182;&#30340;&#27867;&#28145;&#24230;&#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comparing Machine Learning Algorithms by Union-Free Generic Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#24615;&#20998;&#26512;&#20559;&#24207;&#38598;&#21512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340;&#26080;&#20132;&#24182;&#27867;&#28145;&#24230; (ufg) &#27604;&#36739;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#31034;&#20363;&#12290;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#22522;&#20110;ufg&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26377;&#24456;&#22823;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#20989;&#25968;&#27010;&#24565;&#30340;&#25551;&#36848;&#24615;&#20998;&#26512;&#20559;&#24207;&#38598;&#21512;&#30340;&#26694;&#26550;&#12290;&#23613;&#31649;&#32447;&#24615;&#31354;&#38388;&#21644;&#24230;&#37327;&#31354;&#38388;&#30340;&#30740;&#31350;&#38750;&#24120;&#28145;&#20837;&#65292;&#20294;&#20851;&#20110;&#20559;&#24207;&#38598;&#21512;&#31561;&#38750;&#26631;&#20934;&#25968;&#25454;&#31867;&#22411;&#30340;&#28145;&#24230;&#20989;&#25968;&#30340;&#35752;&#35770;&#20960;&#20046;&#27809;&#26377;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#20559;&#24207;&#38598;&#21512;&#30340;&#33879;&#21517;&#31616;&#21333;&#28145;&#24230;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#26080;&#20132;&#24182;&#27867;&#28145;&#24230; (ufg)&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;ufg&#28145;&#24230;&#26469;&#27604;&#36739;&#22522;&#20110;&#22810;&#32500;&#24615;&#33021;&#25351;&#26631;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#65292;&#23545;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#22120;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26377;&#24076;&#26395;&#22320;&#23637;&#31034;&#20102;&#22522;&#20110;ufg&#26041;&#27861;&#30340;&#19981;&#21516;&#20998;&#26512;&#26041;&#27861;&#30340;&#24191;&#27867;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31034;&#20363;&#35828;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#26377;&#24456;&#22823;&#21306;&#21035;&#65292;&#22240;&#27492;&#20026;&#20998;&#31867;&#22120;&#27604;&#36739;&#30340;&#28909;&#28872;&#35752;&#35770;&#22686;&#28155;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework for descriptively analyzing sets of partial orders based on the concept of depth functions. Despite intensive studies in linear and metric spaces, there is very little discussion on depth functions for non-standard data types such as partial orders. We introduce an adaptation of the well-known simplicial depth to the set of all partial orders, the union-free generic (ufg) depth. Moreover, we utilize our ufg depth for a comparison of machine learning algorithms based on multidimensional performance measures. Concretely, we provide two examples of classifier comparisons on samples of standard benchmark data sets. Our results demonstrate promisingly the wide variety of different analysis approaches based on ufg methods. Furthermore, the examples outline that our approach differs substantially from existing benchmarking approaches, and thus adds a new perspective to the vivid debate on classifier comparison.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36880;&#27493;&#25193;&#23637;&#21333;&#20010;&#33410;&#28857;&#21040;&#30446;&#26631;&#22270;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#23545;&#25152;&#26377;&#33410;&#28857;&#23545;&#30340;&#25972;&#20010;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22270;&#29983;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#22810;&#23610;&#24230;&#29983;&#25104;&#20445;&#25345;&#20102;&#39640;&#34920;&#36798;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.11529</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#26412;&#22320;&#25193;&#23637;&#23454;&#29616;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient and Scalable Graph Generation through Iterative Local Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11529
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#25193;&#23637;&#21333;&#20010;&#33410;&#28857;&#21040;&#30446;&#26631;&#22270;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#23545;&#25152;&#26377;&#33410;&#28857;&#23545;&#30340;&#25972;&#20010;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22270;&#29983;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#22810;&#23610;&#24230;&#29983;&#25104;&#20445;&#25345;&#20102;&#39640;&#34920;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#30340;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20195;&#34920;&#25152;&#26377;&#33410;&#28857;&#23545;&#30340;&#25972;&#20010;&#32852;&#21512;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#21516;&#26102;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#22270;&#32467;&#26500;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#22411;&#22270;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#23558;&#21333;&#20010;&#33410;&#28857;&#25193;&#23637;&#21040;&#30446;&#26631;&#22270;&#26469;&#29983;&#25104;&#22270;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#20197;&#26412;&#22320;&#21270;&#26041;&#24335;&#28155;&#21152;&#33410;&#28857;&#21644;&#36793;&#65292;&#39318;&#20808;&#26500;&#24314;&#20840;&#23616;&#32467;&#26500;&#65292;&#28982;&#21518;&#32454;&#21270;&#23616;&#37096;&#32454;&#33410;&#12290;&#23616;&#37096;&#29983;&#25104;&#36991;&#20813;&#20102;&#23545;&#25152;&#26377;&#33410;&#28857;&#23545;&#19978;&#30340;&#25972;&#20010;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#23545;&#20110;&#33410;&#28857;&#25968;&#32780;&#35328;&#23454;&#29616;&#20102;&#22823;&#24133;&#30340;&#35745;&#31639;&#33410;&#32422;&#65292;&#24182;&#36890;&#36807;&#22810;&#23610;&#24230;&#29983;&#25104;&#20445;&#25345;&#20102;&#39640;&#34920;&#36798;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20844;&#35748;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11529v2 Announce Type: replace-cross  Abstract: In the realm of generative models for graphs, extensive research has been conducted. However, most existing methods struggle with large graphs due to the complexity of representing the entire joint distribution across all node pairs and capturing both global and local graph structures simultaneously. To overcome these issues, we introduce a method that generates a graph by progressively expanding a single node to a target graph. In each step, nodes and edges are added in a localized manner through denoising diffusion, building first the global structure, and then refining the local details. The local generation avoids modeling the entire joint distribution over all node pairs, achieving substantial computational savings with subquadratic runtime relative to node count while maintaining high expressivity through multiscale generation. Our experiments show that our model achieves state-of-the-art performance on well-established b
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#30340;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#20998;&#26512;&#25152;&#38656;&#30340;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#38750;&#24120;&#25968;&#22495;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.10107</link><description>&lt;p&gt;
&#36808;&#21521;&#38754;&#21521;&#19978;&#19979;&#25991;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#65306;&#29702;&#35299;&#36793;&#32536;&#20256;&#36882;&#23398;&#20064;&#30340;&#22909;&#22788;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Context-Aware Domain Generalization: Understanding the Benefits and Limits of Marginal Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10107
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#30340;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#20998;&#26512;&#25152;&#38656;&#30340;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#38750;&#24120;&#25968;&#22495;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20851;&#20110;&#36755;&#20837;$X$&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#22914;&#20309;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#39044;&#27979;&#30340;&#26465;&#20214;&#12290;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#36793;&#32536;&#20256;&#36882;&#23398;&#20064;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#30340;&#27010;&#24565;&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;&#25968;&#25454;&#28857;&#30340;&#25490;&#21015;&#19981;&#21464;&#34920;&#31034;&#65292;&#36825;&#20123;&#25968;&#25454;&#28857;&#26469;&#33258;&#20110;&#19982;&#36755;&#20837;&#26412;&#36523;&#30456;&#21516;&#30340;&#22495;&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#26041;&#27861;&#22312;&#21407;&#21017;&#19978;&#21487;&#20197;&#20135;&#29983;&#22909;&#22788;&#30340;&#26465;&#20214;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#21046;&#23450;&#20102;&#20004;&#20010;&#22312;&#23454;&#36341;&#20013;&#21487;&#20197;&#36731;&#26494;&#39564;&#35777;&#30340;&#24517;&#35201;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36793;&#32536;&#20256;&#36882;&#23398;&#20064;&#26041;&#27861;&#26377;&#26395;&#20855;&#26377;&#31283;&#20581;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#35265;&#35299;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#25105;&#20204;&#30340;&#26631;&#20934;&#26377;&#25928;&#22320;&#21306;&#20998;&#20102;&#26377;&#21033;&#21644;&#19981;&#21033;&#30340;&#22330;&#26223;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#27169;&#22411;&#38754;&#20020;&#38750;&#24120;&#25968;&#22495;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10107v2 Announce Type: replace-cross  Abstract: In this work, we analyze the conditions under which information about the context of an input $X$ can improve the predictions of deep learning models in new domains. Following work in marginal transfer learning in Domain Generalization (DG), we formalize the notion of context as a permutation-invariant representation of a set of data points that originate from the same domain as the input itself. We offer a theoretical analysis of the conditions under which this approach can, in principle, yield benefits, and formulate two necessary criteria that can be easily verified in practice. Additionally, we contribute insights into the kind of distribution shifts for which the marginal transfer learning approach promises robustness. Empirical analysis shows that our criteria are effective in discerning both favorable and unfavorable scenarios. Finally, we demonstrate that we can reliably detect scenarios where a model is tasked with unw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#25151;&#38388;&#20869;&#37325;&#24314;&#22768;&#22330;&#30340;&#25968;&#25454;&#39537;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#35757;&#32451;&#21644;&#29983;&#25104;&#22768;&#22330;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#37325;&#24314;&#12290;</title><link>https://arxiv.org/abs/2312.08821</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#37325;&#24314;&#22768;&#22330;
&lt;/p&gt;
&lt;p&gt;
Reconstruction of Sound Field through Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#25151;&#38388;&#20869;&#37325;&#24314;&#22768;&#22330;&#30340;&#25968;&#25454;&#39537;&#21160;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#21270;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#35757;&#32451;&#21644;&#29983;&#25104;&#22768;&#22330;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25151;&#38388;&#20869;&#37325;&#24314;&#22768;&#22330;&#26159;&#20960;&#31181;&#24212;&#29992;&#20013;&#30340;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#27604;&#22914;&#22768;&#25511;&#12289;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#25110;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#37325;&#24314;&#25151;&#38388;&#20869;&#22768;&#22330;&#30340;&#22768;&#23398;&#22330;&#25391;&#24133;&#65292;&#37325;&#28857;&#20851;&#27880;&#27169;&#24577;&#39057;&#29575;&#33539;&#22260;&#12290;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#26465;&#20214;&#21270;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#30340;&#20351;&#29992;&#65292;&#32463;&#36807;&#35757;&#32451;&#29992;&#20110;&#22312;&#25193;&#23637;&#39046;&#22495;&#20869;&#37325;&#24314;&#22768;&#22330;&#65288;SF-Diff&#65289;&#12290;&#35813;&#26550;&#26500;&#34987;&#35774;&#35745;&#20026;&#22312;&#19968;&#32452;&#19981;&#21516;&#39057;&#29575;&#30340;&#26377;&#38480;&#21487;&#29992;&#27979;&#37327;&#26465;&#20214;&#19979;&#65292;&#29983;&#25104;&#30446;&#26631;&#26410;&#30693;&#20301;&#32622;&#30340;&#22768;&#22330;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;SF-Diff&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#30340;&#37325;&#24314;&#65292;&#20248;&#20110;&#22522;&#20110;&#26680;&#25554;&#20540;&#30340;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08821v2 Announce Type: replace-cross  Abstract: Reconstructing the sound field in a room is an important task for several applications, such as sound control and augmented (AR) or virtual reality (VR). In this paper, we propose a data-driven generative model for reconstructing the magnitude of acoustic fields in rooms with a focus on the modal frequency range. We introduce, for the first time, the use of a conditional Denoising Diffusion Probabilistic Model (DDPM) trained in order to reconstruct the sound field (SF-Diff) over an extended domain. The architecture is devised in order to be conditioned on a set of limited available measurements at different frequencies and generate the sound field in target, unknown, locations. The results show that SF-Diff is able to provide accurate reconstructions, outperforming a state-of-the-art baseline based on kernel interpolation.
&lt;/p&gt;</description></item><item><title>&#22810;&#32500;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#22312;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#25928;&#65292;&#32780;&#22810;&#32500;&#26041;&#27861;&#21450;&#32467;&#26524;&#20998;&#24067;&#22343;&#23545;&#20844;&#24179;&#24615;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2312.04404</link><description>&lt;p&gt;
&#35770;&#22810;&#32500;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Multi-dimensional Local Differential Privacy on Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04404
&lt;/p&gt;
&lt;p&gt;
&#22810;&#32500;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#22312;&#20943;&#23569;&#19981;&#20844;&#24179;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#25928;&#65292;&#32780;&#22810;&#32500;&#26041;&#27861;&#21450;&#32467;&#26524;&#20998;&#24067;&#22343;&#23545;&#20844;&#24179;&#24615;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#22312;&#20154;&#20204;&#30340;&#29983;&#27963;&#20013;&#20570;&#20986;&#37325;&#35201;&#20915;&#31574;&#12290;&#30001;&#20110;&#25805;&#32437;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#20915;&#31574;&#65292;&#38656;&#35201;&#35299;&#20915;&#19968;&#20123;&#20262;&#29702;&#20851;&#20999;&#20197;&#20415;&#36866;&#24403;&#22320;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20844;&#24179;&#24615;&#21644;&#38544;&#31169;&#24615;&#12290;&#19982;&#20043;&#21069;&#20851;&#27880;&#21333;&#19968;&#25935;&#24863;&#23646;&#24615;&#30340;&#38598;&#20013;&#24335;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25110;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#65288;&#21363;&#22810;&#32500;&#25968;&#25454;&#65289;&#23384;&#22312;&#26102; LDP &#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#23545;&#21512;&#25104;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#38750;&#24120;&#30456;&#20851;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#29305;&#21035;&#26159;&#65292;&#65288;1&#65289;&#22810;&#32500; LDP &#26159;&#20943;&#23569;&#24046;&#36317;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#65288;2&#65289;&#22312;&#20302;&#38544;&#31169;&#20445;&#35777;&#19979;&#65292;LDP &#30340;&#22810;&#32500;&#26041;&#27861;&#65288;&#29420;&#31435; vs. &#32452;&#21512;&#65289;&#24456;&#37325;&#35201;&#65292;&#65288;3&#65289;&#32467;&#26524; Y &#20998;&#24067;&#23545;&#21738;&#20010;&#32676;&#20307;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04404v3 Announce Type: replace  Abstract: Automated decision systems are increasingly used to make consequential decisions in people's lives. Due to the sensitivity of the manipulated data as well as the resulting decisions, several ethical concerns need to be addressed for the appropriate use of such technologies, in particular, fairness and privacy. Unlike previous work, which focused on centralized differential privacy (DP) or local DP (LDP) for a single sensitive attribute, in this paper, we examine the impact of LDP in the presence of several sensitive attributes (i.e., multi-dimensional data) on fairness. Detailed empirical analysis on synthetic and benchmark datasets revealed very relevant observations. In particular, (1) multi-dimensional LDP is an efficient approach to reduce disparity, (2) the multi-dimensional approach of LDP (independent vs. combined) matters only at low privacy guarantees, and (3) the outcome Y distribution has an important effect on which group
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#38543;&#26426;&#35797;&#39564;&#35774;&#35745;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#65292;&#33021;&#22815;&#37327;&#21270;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#24378;&#24230;&#65292;&#24182;&#20272;&#35745;&#20854;&#19979;&#30028;&#65292;&#26377;&#25928;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#35782;&#21035;&#28151;&#28102;&#12290;</title><link>https://arxiv.org/abs/2312.03871</link><description>&lt;p&gt;
&#38544;&#34109;&#32780;&#21487;&#37327;&#21270;&#65306;&#20351;&#29992;&#38543;&#26426;&#35797;&#39564;&#30340;&#28151;&#28102;&#24378;&#24230;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Hidden yet quantifiable: A lower bound for confounding strength using randomized trials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03871
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38543;&#26426;&#35797;&#39564;&#35774;&#35745;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#65292;&#33021;&#22815;&#37327;&#21270;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#24378;&#24230;&#65292;&#24182;&#20272;&#35745;&#20854;&#19979;&#30028;&#65292;&#26377;&#25928;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#35782;&#21035;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#33410;&#22863;&#31934;&#20934;&#21307;&#23398;&#26102;&#20195;&#65292;&#35266;&#23519;&#24615;&#30740;&#31350;&#22312;&#27491;&#30830;&#35780;&#20272;&#20020;&#24202;&#23454;&#36341;&#20013;&#26032;&#30103;&#27861;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#20174;&#38750;&#38543;&#26426;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#22240;&#26524;&#32467;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#35797;&#39564;&#26469;&#37327;&#21270;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#30340;&#26032;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32479;&#35745;&#26816;&#39564;&#26469;&#26816;&#27979;&#24378;&#24230;&#36229;&#36807;&#32473;&#23450;&#38408;&#20540;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26816;&#39564;&#26469;&#20272;&#35745;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#24378;&#24230;&#30340;&#28176;&#36817;&#26377;&#25928;&#19979;&#30028;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#32479;&#35745;&#26816;&#39564;&#30340;&#21151;&#25928;&#21644;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#19979;&#30028;&#22914;&#20309;&#33021;&#22815;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#27491;&#30830;&#35782;&#21035;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#30340;&#23384;&#22312;&#21644;&#19981;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03871v2 Announce Type: replace-cross  Abstract: In the era of fast-paced precision medicine, observational studies play a major role in properly evaluating new treatments in clinical practice. Yet, unobserved confounding can significantly compromise causal conclusions drawn from non-randomized data. We propose a novel strategy that leverages randomized trials to quantify unobserved confounding. First, we design a statistical test to detect unobserved confounding with strength above a given threshold. Then, we use the test to estimate an asymptotically valid lower bound on the unobserved confounding strength. We evaluate the power and validity of our statistical test on several synthetic and semi-synthetic datasets. Further, we show how our lower bound can correctly identify the absence and presence of unobserved confounding in a real-world setting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#30340; LSTSVR-PI&#27169;&#22411;&#32467;&#21512;&#20102;&#26368;&#23567;&#20108;&#20056;&#21452;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#21644;&#29305;&#26435;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#25928;&#29575;&#65292;&#24182;&#24314;&#31435;&#20102;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2312.02596</link><description>&lt;p&gt;
LSTSVR-PI: &#20855;&#26377;&#29305;&#26435;&#20449;&#24687;&#30340;&#26368;&#23567;&#20108;&#20056;&#21452;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
LSTSVR-PI: Least square twin support vector regression with privileged information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02596
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#30340; LSTSVR-PI&#27169;&#22411;&#32467;&#21512;&#20102;&#26368;&#23567;&#20108;&#20056;&#21452;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#21644;&#29305;&#26435;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#25928;&#29575;&#65292;&#24182;&#24314;&#31435;&#20102;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#65292;&#25945;&#24072;&#22312;&#21508;&#31181;&#35838;&#22530;&#25945;&#23398;&#27169;&#24335;&#20013;&#25198;&#28436;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#31867;&#20284;&#22320;&#65292;&#20511;&#37492;&#20154;&#31867;&#23398;&#20064;&#30340;&#36825;&#19968;&#26041;&#38754;&#65292;&#20351;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65288;LUPI&#65289;&#33539;&#24335;&#22312;&#35757;&#32451;&#38454;&#27573;&#24341;&#20837;&#39069;&#22806;&#20449;&#24687;&#26469;&#25351;&#23548;&#23398;&#20064;&#27169;&#22411;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#30340;&#26368;&#23567;&#20108;&#20056;&#21452;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;(LSTSVR-PI)&#30340;&#19981;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LUPI&#33539;&#24335;&#38598;&#25104;&#21040;&#26368;&#23567;&#20108;&#20056;&#21452;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#20013;&#20197;&#21033;&#29992;&#39069;&#22806;&#20449;&#24687;&#28304;&#12290;&#25552;&#20986;&#30340;LSTSVR-PI&#35299;&#20915;&#20102;&#32447;&#24615;&#26041;&#31243;&#32452;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22522;&#20110;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;Rademacher&#22797;&#26434;&#24615;&#24314;&#31435;&#20102;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#34701;&#20837;&#20102;&#32467;&#26500;&#39118;&#38505;&#26368;&#23567;&#21270;&#21407;&#21017;&#12290;&#25552;&#20986;&#30340;LSTSVR-PI&#22635;&#34917;&#20102;&#24403;&#20195;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02596v2 Announce Type: replace  Abstract: In an educational setting, a teacher plays a crucial role in various classroom teaching patterns. Similarly, mirroring this aspect of human learning, the learning using privileged information (LUPI) paradigm introduces additional information to instruct learning models during the training stage. A different approach to train the twin variant of the regression model is provided by the new least square twin support vector regression using privileged information (LSTSVR-PI), which integrates the LUPI paradigm to utilize additional sources of information into the least square twin support vector regression. The proposed LSTSVR-PI solves system of linear equations which adds up to the efficiency of the model. Further, we also establish a generalization error bound based on the Rademacher complexity of the proposed model and incorporate the structural risk minimization principle. The proposed LSTSVR-PI fills the gap between the contemporar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2312.02119</link><description>&lt;p&gt;
&#25915;&#20987;&#26641;&#65306;&#33258;&#21160;&#30772;&#35299;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02119
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#20173;&#22312;&#29983;&#25104;&#26377;&#23475;&#12289;&#24102;&#20559;&#35265;&#21644;&#26377;&#27602;&#20869;&#23481;&#65292;&#36825;&#19968;&#28857;&#30001;&#20154;&#20026;&#35774;&#35745;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#26222;&#36941;&#23384;&#22312;&#24471;&#20197;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#65292;&#20165;&#38656;&#35201;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;TAP&#21033;&#29992;LLM&#26469;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#36845;&#20195;&#22320;&#20248;&#21270;&#20505;&#36873;&#65288;&#25915;&#20987;&#65289;&#25552;&#31034;&#65292;&#30452;&#21040;&#29983;&#25104;&#30340;&#25552;&#31034;&#20043;&#19968;&#36234;&#29425;&#30446;&#26631;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#22312;&#23558;&#25552;&#31034;&#21457;&#36865;&#32473;&#30446;&#26631;&#20043;&#21069;&#65292;TAP&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#24182;&#31227;&#38500;&#21487;&#33021;&#19981;&#20250;&#23548;&#33268;&#36234;&#29425;&#30340;&#25552;&#31034;&#12290;&#20351;&#29992;&#24605;&#32500;&#26641;&#25512;&#29702;&#20351;TAP&#33021;&#22815;&#22312;&#22823;&#37327;&#25552;&#31034;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#32780;&#20462;&#21098;&#21017;&#20943;&#23569;&#20102;&#21457;&#36865;&#32473;&#30446;&#26631;&#30340;&#24635;&#26597;&#35810;&#25968;&#37327;&#12290;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;TAP&#29983;&#25104;&#30340;&#25552;&#31034;&#36234;&#29425;&#20102;&#36229;&#36807;80%&#30340;&#26368;&#20808;&#36827;LLMs&#65288;&#21253;&#25324;GPT4&#21644;GPT4-Turbo&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02119v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thought reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80%
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20113;&#20809;&#23398;&#21402;&#24230;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#22320;&#29699;&#35266;&#27979;&#32972;&#26223;&#19979;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.14024</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20113;&#20809;&#23398;&#21402;&#24230;&#27979;&#37327;&#30340;&#22810;&#20809;&#35889;&#25104;&#20687;&#20202;&#20113;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Creating and Leveraging a Synthetic Dataset of Cloud Optical Thickness Measures for Cloud Detection in MSI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20113;&#20809;&#23398;&#21402;&#24230;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#22320;&#29699;&#35266;&#27979;&#32972;&#26223;&#19979;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#22242;&#36890;&#24120;&#20250;&#36974;&#34109;&#22320;&#29699;&#34920;&#38754;&#30340;&#20809;&#23398;&#21355;&#26143;&#30417;&#27979;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22303;&#22320;&#35206;&#30422;&#26144;&#23556;&#12289;&#28023;&#27915;&#33394;&#24425;&#20998;&#26512;&#21644;&#20892;&#30000;&#30417;&#27979;&#31561;&#22320;&#29699;&#35266;&#27979;&#27963;&#21160;&#12290; &#22312;&#36965;&#24863;&#39046;&#22495;&#20869;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#22320;&#29699;&#35266;&#27979;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20113;&#26816;&#27979;&#21644;&#36807;&#28388;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#25913;&#36827;&#31354;&#38388;&#12290; ML&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#22312;&#22320;&#29699;&#35266;&#27979;&#32972;&#26223;&#19979;&#36890;&#24120;&#24456;&#38590;&#33719;&#24471;&#65292;&#36825;&#22312;&#20113;&#20809;&#23398;&#21402;&#24230;&#65288;COT&#65289;&#20272;&#31639;&#26041;&#38754;&#23588;&#20026;&#26126;&#26174;&#12290; &#21487;&#38752;&#30340;COT&#20272;&#35745;&#30456;&#27604;&#20351;&#29992;&#24120;&#35268;&#20113;&#31867;&#21035;&#33021;&#26356;&#31934;&#32454;&#21644;&#24212;&#29992;&#30456;&#20851;&#22320;&#25511;&#21046;&#12290; &#20026;&#20102;&#32531;&#35299;COT&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14024v2 Announce Type: replace-cross  Abstract: Cloud formations often obscure optical satellite-based monitoring of the Earth's surface, thus limiting Earth observation (EO) activities such as land cover mapping, ocean color analysis, and cropland monitoring. The integration of machine learning (ML) methods within the remote sensing domain has significantly improved performance on a wide range of EO tasks, including cloud detection and filtering, but there is still much room for improvement. A key bottleneck is that ML methods typically depend on large amounts of annotated data for training, which is often difficult to come by in EO contexts. This is especially true when it comes to cloud optical thickness (COT) estimation. A reliable estimation of COT enables more fine-grained and application-dependent control compared to using pre-specified cloud categories, as is commonly done in practice. To alleviate the COT data scarcity problem, in this work we propose a novel synthe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#21453;&#21521;&#24314;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#26032;&#22411;&#30340;&#36830;&#32493;&#21464;&#37327;&#23884;&#20837;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#24402;&#19968;&#21270;&#30340;&#38656;&#35201;&#65292;&#20445;&#30041;&#20449;&#24687;&#24182;&#21019;&#36896;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2311.11343</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#21453;&#21521;&#24314;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#36830;&#32493;&#21464;&#37327;&#30340;&#26032;&#22411;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
A Generative Model for Accelerated Inverse Modelling Using a Novel Embedding for Continuous Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11343
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#21453;&#21521;&#24314;&#27169;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#26032;&#22411;&#30340;&#36830;&#32493;&#21464;&#37327;&#23884;&#20837;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#24402;&#19968;&#21270;&#30340;&#38656;&#35201;&#65292;&#20445;&#30041;&#20449;&#24687;&#24182;&#21019;&#36896;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#65292;&#24555;&#36895;&#21407;&#22411;&#21046;&#20316;&#20855;&#26377;&#25152;&#38656;&#24615;&#33021;&#30340;&#26448;&#26009;&#30340;&#25361;&#25112;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#20197;&#25214;&#21040;&#21512;&#36866;&#30340;&#24494;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#32473;&#23450;&#24615;&#33021;&#23547;&#25214;&#24494;&#32467;&#26500;&#36890;&#24120;&#26159;&#19968;&#20010;&#19981;&#36866;&#23450;&#38382;&#39064;&#65292;&#21487;&#33021;&#23384;&#22312;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#20351;&#29992;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#65292;&#38656;&#35201;&#23558;&#36830;&#32493;&#23646;&#24615;&#21464;&#37327;&#20316;&#20026;&#27169;&#22411;&#30340;&#26465;&#20214;&#36755;&#20837;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#24182;&#23558;&#20854;&#19982;&#19968;&#31181;&#22522;&#20110;&#28014;&#28857;&#25968;&#30340;&#20108;&#36827;&#21046;&#34920;&#31034;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22411;&#23884;&#20837;&#31574;&#30053;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#36825;&#31181;&#26041;&#27861;&#28040;&#38500;&#20102;&#24402;&#19968;&#21270;&#30340;&#38656;&#35201;&#65292;&#20445;&#30041;&#20102;&#20449;&#24687;&#65292;&#24182;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26465;&#20214;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11343v2 Announce Type: replace  Abstract: In materials science, the challenge of rapid prototyping materials with desired properties often involves extensive experimentation to find suitable microstructures. Additionally, finding microstructures for given properties is typically an ill-posed problem where multiple solutions may exist. Using generative machine learning models can be a viable solution which also reduces the computational cost. This comes with new challenges because, e.g., a continuous property variable as conditioning input to the model is required. We investigate the shortcomings of an existing method and compare this to a novel embedding strategy for generative models that is based on the binary representation of floating point numbers. This eliminates the need for normalization, preserves information, and creates a versatile embedding space for conditioning the generative model. This technique can be applied to condition a network on any number, to provide 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;Thompson&#25277;&#26679;&#31639;&#27861;&#65292;&#24378;&#35843;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#65292;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#27492;&#20462;&#25913;&#20165;&#23545;&#32047;&#31215;&#36951;&#25022;&#20135;&#29983;&#24658;&#23450;&#30340;&#24809;&#32602;&#12290;</title><link>https://arxiv.org/abs/2311.09483</link><description>&lt;p&gt;
&#20855;&#26377;&#29992;&#25143;&#23450;&#20041;&#30446;&#26631;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#29992;&#20110;&#20581;&#24247;&#34892;&#20026;&#25913;&#21464;
&lt;/p&gt;
&lt;p&gt;
Adaptive Interventions with User-Defined Goals for Health Behavior Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;Thompson&#25277;&#26679;&#31639;&#27861;&#65292;&#24378;&#35843;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#65292;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#27492;&#20462;&#25913;&#20165;&#23545;&#32047;&#31215;&#36951;&#25022;&#20135;&#29983;&#24658;&#23450;&#30340;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#27963;&#21160;&#19981;&#36275;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#20844;&#20849;&#20581;&#24247;&#38382;&#39064;&#65292;&#19982;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;2&#22411;&#31958;&#23615;&#30149;&#31561;&#19981;&#33391;&#20581;&#24247;&#32467;&#26524;&#30456;&#20851;&#12290;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#31243;&#24207;&#20026;&#20302;&#25104;&#26412;&#12289;&#21487;&#25193;&#23637;&#30340;&#36523;&#20307;&#27963;&#21160;&#20419;&#36827;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#65292;&#28982;&#32780;&#36890;&#24120;&#25928;&#26524;&#36739;&#23567;&#65292;&#31896;&#38468;&#29575;&#20302;&#65292;&#29305;&#21035;&#26159;&#19982;&#20154;&#31867;&#36741;&#23548;&#30456;&#27604;&#12290;&#30446;&#26631;&#35774;&#23450;&#26159;&#20581;&#24247;&#36741;&#23548;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#20013;&#19968;&#30452;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;Thompson&#25277;&#26679;&#31639;&#27861;&#30340;&#20462;&#25913;&#65292;&#37325;&#28857;&#25918;&#22312;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#12290;&#20316;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#30340;&#19968;&#27493;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#21516;&#26102;&#20248;&#21270;&#20010;&#20154;&#20559;&#22909;&#21644;&#30446;&#26631;&#30340;&#24179;&#34913;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#25913;&#21482;&#23545;&#32047;&#31215;&#36951;&#25022;&#36896;&#25104;&#19968;&#20010;&#24120;&#25968;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09483v2 Announce Type: replace-cross  Abstract: Physical inactivity remains a major public health concern, having associations with adverse health outcomes such as cardiovascular disease and type-2 diabetes. Mobile health applications present a promising avenue for low-cost, scalable physical activity promotion, yet often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. Goal-setting is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. This paper introduces a modification to the Thompson sampling algorithm that places emphasis on individualized goal-setting by optimizing personalized reward functions. As a step towards supporting goal-setting, this paper offers a balanced approach that can leverage shared structure while optimizing individual preferences and goals. We prove that our modification incurs only a constant penalty on the cumulative regret 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#37197;&#30005;&#31995;&#32479;&#21516;&#27493;&#26102;&#38388;&#29366;&#24577;&#20272;&#35745;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#36755;&#20837;&#25200;&#21160;&#35270;&#20026;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#20998;&#26512;&#39564;&#35777;&#65292;&#24182;&#24378;&#35843;&#20102;&#25209;&#24402;&#19968;&#21270;&#22312;&#25552;&#39640;&#38382;&#39064;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#35813;&#26694;&#26550;&#22312;&#20462;&#25913;&#21518;&#30340;IEEE 34&#33410;&#28857;&#31995;&#32479;&#21644;&#30495;&#23454;&#30340;&#22823;&#22411;&#20998;&#24067;&#31995;&#32479;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2311.06973</link><description>&lt;p&gt;
&#20998;&#26512;&#39564;&#35777;&#21516;&#27493;&#26102;&#38388;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Analytical Verification of Deep Neural Network Performance for Time-Synchronized Distribution System State Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#37197;&#30005;&#31995;&#32479;&#21516;&#27493;&#26102;&#38388;&#29366;&#24577;&#20272;&#35745;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#36755;&#20837;&#25200;&#21160;&#35270;&#20026;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#36827;&#34892;&#20998;&#26512;&#39564;&#35777;&#65292;&#24182;&#24378;&#35843;&#20102;&#25209;&#24402;&#19968;&#21270;&#22312;&#25552;&#39640;&#38382;&#39064;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#35813;&#26694;&#26550;&#22312;&#20462;&#25913;&#21518;&#30340;IEEE 34&#33410;&#28857;&#31995;&#32479;&#21644;&#30495;&#23454;&#30340;&#22823;&#22411;&#20998;&#24067;&#31995;&#32479;&#19978;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36827;&#34892;&#23454;&#26102;&#19981;&#21487;&#35266;&#27979;&#20998;&#24067;&#31995;&#32479;&#30340;&#21516;&#27493;&#26102;&#38388;&#29366;&#24577;&#20272;&#35745;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#20010;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35813;&#29366;&#24577;&#20272;&#35745;&#22120;&#22312;&#36755;&#20837;&#27979;&#37327;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#30340;&#20998;&#26512;&#30028;&#38480;&#12290;&#24050;&#32463;&#26377;&#20154;&#34920;&#26126;&#65292;&#20165;&#22522;&#20110;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#24615;&#33021;&#21487;&#33021;&#19981;&#33021;&#26377;&#25928;&#22320;&#35828;&#26126;&#35757;&#32451;&#22909;&#30340;DNN&#22788;&#29702;&#36755;&#20837;&#25200;&#21160;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#25200;&#21160;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#38382;&#39064;&#20174;&#20998;&#26512;&#19978;&#39564;&#35777;&#20102;DNN&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#25209;&#24402;&#19968;&#21270;&#22312;&#35299;&#20915;MILP&#20844;&#24335;&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;&#20462;&#25913;&#21518;&#30340;IEEE 34&#33410;&#28857;&#31995;&#32479;&#21644;&#19968;&#20010;&#30495;&#23454;&#30340;&#22823;&#22411;&#20998;&#24067;&#31995;&#32479;&#19978;&#36827;&#34892;&#21516;&#27493;&#26102;&#38388;&#30340;&#37197;&#30005;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#26469;&#36827;&#34892;&#39564;&#35777;&#65292;&#36825;&#20004;&#20010;&#31995;&#32479;&#37117;&#26159;&#36890;&#36807;&#24494;&#30456;&#20301;&#27979;&#37327;&#19981;&#23436;&#20840;&#35266;&#27979;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, we demonstrated success of a time-synchronized state estimator using deep neural networks (DNNs) for real-time unobservable distribution systems. In this letter, we provide analytical bounds on the performance of that state estimator as a function of perturbations in the input measurements. It has already been shown that evaluating performance based on only the test dataset might not effectively indicate a trained DNN's ability to handle input perturbations. As such, we analytically verify robustness and trustworthiness of DNNs to input perturbations by treating them as mixed-integer linear programming (MILP) problems. The ability of batch normalization in addressing the scalability limitations of the MILP formulation is also highlighted. The framework is validated by performing time-synchronized distribution system state estimation for a modified IEEE 34-node system and a real-world large distribution system, both of which are incompletely observed by micro-phasor measuremen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#26469;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2310.18948</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#36335;&#24452;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#39044;&#27979;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18948
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#26469;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#19978;&#20132;&#36890;&#23545;&#20110;&#23454;&#29616;&#20840;&#29699;&#32463;&#27982;&#22686;&#38271;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#22312;&#21487;&#25345;&#32493;&#24615;&#21644;&#20445;&#25252;&#28626;&#21361;&#28023;&#27915;&#29289;&#31181;&#26041;&#38754;&#23653;&#34892;&#29983;&#24577;&#20041;&#21153;&#65292;&#23588;&#20854;&#26159;&#20445;&#25252;&#22823;&#22411;&#40120;&#31867;&#31181;&#32676;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;(AIS)&#25968;&#25454;&#36890;&#36807;&#25552;&#20379;&#33337;&#33334;&#36816;&#21160;&#30340;&#23454;&#26102;&#27969;&#25968;&#25454;&#65292;&#21487;&#20197;&#23454;&#29616;&#24378;&#21270;&#30340;&#20132;&#36890;&#30417;&#25511;&#65292;&#20174;&#32780;&#36991;&#20813;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#65292;&#20174;&#32780;&#39044;&#38450;&#33337;&#33334;&#19982;&#40120;&#40060;&#30340;&#30896;&#25758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(Bi-LSTM)&#26500;&#24314;&#20102;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;1&#21040;3&#23567;&#26102;&#30340;AIS&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#39044;&#27979;&#25509;&#19979;&#26469;12&#23567;&#26102;&#30340;&#33337;&#33334;&#36712;&#36857;&#12290;&#25105;&#20204;&#20174;&#21382;&#21490;AIS&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#36335;&#32447;&#21644;&#30446;&#30340;&#22320;&#30340;&#27010;&#29575;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#27169;&#22411;&#38543;&#21518;&#39044;&#27979;&#33337;&#33334;&#30340;&#36712;&#36857;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#36335;&#32447;&#21644;&#30446;&#30340;&#22320;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maritime transportation is paramount in achieving global economic growth, entailing concurrent ecological obligations in sustainability and safeguarding endangered marine species, most notably preserving large whale populations. In this regard, the Automatic Identification System (AIS) data plays a significant role by offering real-time streaming data on vessel movement, allowing enhanced traffic monitoring. This study explores using AIS data to prevent vessel-to-whale collisions by forecasting long-term vessel trajectories from engineered AIS data sequences. For such a task, we have developed an encoder-decoder model architecture using Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1 to 3 hours of AIS data as input. We feed the model with probabilistic features engineered from historical AIS data that refer to each trajectory's potential route and destination. The model then predicts the vessel's trajectory, considerin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36879;&#35270;&#35282;&#24230;&#65292;&#24378;&#35843;&#20248;&#21270;&#32423;&#32852;&#25490;&#21517;&#31995;&#32479;&#30340;&#36866;&#24212;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#36866;&#24212;&#19981;&#21516;&#32423;&#32852;&#25490;&#21517;&#22330;&#26223;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#24182;&#36991;&#20813;&#27425;&#20248;&#24773;&#20917;&#30340;&#21457;&#29983;&#12290;</title><link>https://arxiv.org/abs/2310.10462</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#31070;&#32463;&#25490;&#21517;&#26694;&#26550;&#65306;&#38754;&#21521;&#32423;&#32852;&#25490;&#21517;&#31995;&#32479;&#30340;&#26368;&#22823;&#21270;&#19994;&#21153;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Adaptive Neural Ranking Framework: Toward Maximized Business Goal for Cascade Ranking Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10462
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36879;&#35270;&#35282;&#24230;&#65292;&#24378;&#35843;&#20248;&#21270;&#32423;&#32852;&#25490;&#21517;&#31995;&#32479;&#30340;&#36866;&#24212;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#36866;&#24212;&#19981;&#21516;&#32423;&#32852;&#25490;&#21517;&#22330;&#26223;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#24182;&#36991;&#20813;&#27425;&#20248;&#24773;&#20917;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32423;&#32852;&#25490;&#21517;&#22312;&#22312;&#32447;&#24191;&#21578;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;top-k&#36873;&#25321;&#38382;&#39064;&#65292;&#32780;&#23398;&#20064;&#25490;&#24207;&#26159;&#20248;&#21270;&#32423;&#32852;&#25490;&#21517;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#24335;&#12290;&#20197;&#21069;&#20851;&#20110;&#23398;&#20064;&#25490;&#24207;&#30340;&#24037;&#20316;&#36890;&#24120;&#20391;&#37325;&#20110;&#35753;&#27169;&#22411;&#23398;&#20064;&#23436;&#25972;&#39034;&#24207;&#25110;top-k&#39034;&#24207;&#65292;&#24182;&#37319;&#29992;&#30456;&#24212;&#30340;&#25490;&#21517;&#25351;&#26631;&#65288;&#22914;OPA&#21644;NDCG@k&#65289;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30446;&#26631;&#26080;&#27861;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#21508;&#31181;&#32423;&#32852;&#25490;&#21517;&#22330;&#26223;&#65307;&#32780;&#29616;&#26377;&#30340;&#22522;&#20110;&#24230;&#37327;&#30340;&#26041;&#27861;&#22914;Lambda&#26694;&#26550;&#21482;&#33021;&#20248;&#21270;&#26377;&#38480;&#25351;&#26631;&#30340;&#31895;&#30053;&#19978;&#30028;&#65292;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#21644;&#24615;&#33021;&#19981;&#23545;&#40784;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#32423;&#32852;&#25490;&#21517;&#31995;&#32479;&#30340;&#36879;&#35270;&#35282;&#24230;&#65292;&#24378;&#35843;&#20248;&#21270;&#30446;&#26631;&#23545;&#25968;&#25454;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10462v2 Announce Type: replace  Abstract: Cascade ranking is widely used for large-scale top-k selection problems in online advertising and recommendation systems, and learning-to-rank is an important way to optimize the models in cascade ranking. Previous works on learning-to-rank usually focus on letting the model learn the complete order or top-k order, and adopt the corresponding rank metrics (e.g. OPA and NDCG@k) as optimization targets. However, these targets can not adapt to various cascade ranking scenarios with varying data complexities and model capabilities; and the existing metric-driven methods such as the Lambda framework can only optimize a rough upper bound of limited metrics, potentially resulting in sub-optimal and performance misalignment. To address these issues, we propose a novel perspective on optimizing cascade ranking systems by highlighting the adaptability of optimization targets to data complexities and model capabilities. Concretely, we employ mu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#33268;&#27169;&#22411;&#21487;&#37325;&#22797;&#24615;&#29616;&#35937;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#26080;&#35770;&#27169;&#22411;&#26694;&#26550;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#22914;&#20309;&#65292;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#37117;&#33021;&#22815;&#19968;&#33268;&#22320;&#36798;&#21040;&#30456;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#35780;&#20998;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21463;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#34920;&#29616;&#20986;&#20004;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#27169;&#24335;&#65306;&#35760;&#24518;&#21270;&#27169;&#24335;&#21644;&#27867;&#21270;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2310.05264</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
The Emergence of Reproducibility and Consistency in Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05264
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#33268;&#27169;&#22411;&#21487;&#37325;&#22797;&#24615;&#29616;&#35937;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#26080;&#35770;&#27169;&#22411;&#26694;&#26550;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#22914;&#20309;&#65292;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#37117;&#33021;&#22815;&#19968;&#33268;&#22320;&#36798;&#21040;&#30456;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#35780;&#20998;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21463;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#30340;&#24433;&#21709;&#65292;&#34920;&#29616;&#20986;&#20004;&#31181;&#19981;&#21516;&#30340;&#35757;&#32451;&#27169;&#24335;&#65306;&#35760;&#24518;&#21270;&#27169;&#24335;&#21644;&#27867;&#21270;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#26377;&#36259;&#19988;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#19968;&#33268;&#30340;&#27169;&#22411;&#21487;&#37325;&#22797;&#24615;&#8221;&#65306;&#22312;&#32473;&#23450;&#30456;&#21516;&#30340;&#36215;&#22987;&#22122;&#22768;&#36755;&#20837;&#21644;&#30830;&#23450;&#24615;&#37319;&#26679;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#20135;&#29983;&#38750;&#24120;&#30456;&#20284;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#34920;&#26126;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#26080;&#35770;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#12289;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#36807;&#31243;&#22914;&#20309;&#65292;&#22312;&#25968;&#25454;&#20998;&#24067;&#21644;&#35780;&#20998;&#20989;&#25968;&#19978;&#37117;&#33021;&#22815;&#19968;&#33268;&#22320;&#36798;&#21040;&#30456;&#21516;&#30340;&#32467;&#26524;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#23398;&#20064;&#21463;&#35757;&#25968;&#25454;&#35268;&#27169;&#24433;&#21709;&#19979;&#30340;&#19981;&#21516;&#20998;&#24067;&#12290;&#36825;&#19968;&#28857;&#24471;&#21040;&#20102;&#20004;&#31181;&#19981;&#21516;&#35757;&#32451;&#27169;&#24335;&#19979;&#27169;&#22411;&#21487;&#37325;&#22797;&#24615;&#30340;&#20307;&#29616;&#65306;&#65288;i&#65289;&#8220;&#35760;&#24518;&#21270;&#27169;&#24335;&#8221;&#65292;&#20854;&#20013;&#25193;&#25955;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#20110;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#65292;&#21644;&#65288;ii&#65289;&#8220;&#27867;&#21270;&#27169;&#24335;&#8221;&#65292;&#20854;&#20013;&#27169;&#22411;&#23398;&#20064;&#21040;&#20102;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05264v2 Announce Type: replace  Abstract: In this work, we investigate an intriguing and prevalent phenomenon of diffusion models which we term as "consistent model reproducibility": given the same starting noise input and a deterministic sampler, different diffusion models often yield remarkably similar outputs. We confirm this phenomenon through comprehensive experiments, implying that different diffusion models consistently reach the same data distribution and scoring function regardless of diffusion model frameworks, model architectures, or training procedures. More strikingly, our further investigation implies that diffusion models are learning distinct distributions affected by the training data size. This is supported by the fact that the model reproducibility manifests in two distinct training regimes: (i) "memorization regime", where the diffusion model overfits to the training data distribution, and (ii) "generalization regime", where the model learns the underlyin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#25490;&#21517;&#26631;&#20934;Equal-Opportunity Ranking&#65288;EOR&#65289;&#65292;&#23558;&#24213;&#23618;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24046;&#24322;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#32452;&#20869;&#20844;&#24179;&#25277;&#22870;&#23454;&#29616;&#20844;&#24179;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2309.01610</link><description>&lt;p&gt;
&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#20844;&#24179;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Fair Ranking under Disparate Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01610
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#25490;&#21517;&#26631;&#20934;Equal-Opportunity Ranking&#65288;EOR&#65289;&#65292;&#23558;&#24213;&#23618;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24046;&#24322;&#32771;&#34385;&#22312;&#20869;&#65292;&#36890;&#36807;&#32452;&#20869;&#20844;&#24179;&#25277;&#22870;&#23454;&#29616;&#20844;&#24179;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21517;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#21487;&#31649;&#29702;&#30340;&#36873;&#39033;&#23376;&#38598;&#19978;&#12290;&#23427;&#20316;&#20026;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#30340;&#20351;&#29992;&#33539;&#22260;&#20174;&#22312;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#23637;&#31034;&#28508;&#22312;&#30456;&#20851;&#20135;&#21697;&#21040;&#20026;&#20154;&#24037;&#23457;&#26597;&#20248;&#20808;&#22788;&#29702;&#22823;&#23398;&#30003;&#35831;&#12290;&#34429;&#28982;&#25490;&#21517;&#21487;&#20197;&#36890;&#36807;&#23558;&#20851;&#27880;&#38598;&#20013;&#22312;&#26368;&#26377;&#21069;&#36884;&#30340;&#36873;&#39033;&#19978;&#20351;&#20154;&#31867;&#35780;&#20272;&#26356;&#21152;&#39640;&#25928;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#24213;&#23618;&#30456;&#20851;&#24615;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#22312;&#19981;&#21516;&#32452;&#21035;&#30340;&#36873;&#39033;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#25490;&#21517;&#21487;&#33021;&#20250;&#24341;&#20837;&#19981;&#20844;&#24179;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#24046;&#24322;&#20284;&#20046;&#26222;&#36941;&#23384;&#22312;&#65292;&#24120;&#24120;&#23545;&#23569;&#25968;&#32676;&#20307;&#36896;&#25104;&#25439;&#23475;&#65292;&#22240;&#20026;&#36825;&#20123;&#32676;&#20307;&#30340;&#30456;&#20851;&#24615;&#20272;&#35745;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#25110;&#21512;&#36866;&#30340;&#29305;&#24449;&#32780;&#20855;&#26377;&#26356;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20844;&#24179;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Equal-Opportunity Ranking&#65288;EOR&#65289;&#20316;&#20026;&#25490;&#21517;&#30340;&#26032;&#20844;&#24179;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#23427;&#23545;&#24212;&#20110;&#22312;&#30456;&#20851;&#36873;&#39033;&#20043;&#38388;&#36827;&#34892;&#32452;&#20869;&#20844;&#24179;&#25277;&#22870;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01610v2 Announce Type: replace  Abstract: Ranking is a ubiquitous method for focusing the attention of human evaluators on a manageable subset of options. Its use as part of human decision-making processes ranges from surfacing potentially relevant products on an e-commerce site to prioritizing college applications for human review. While ranking can make human evaluation more effective by focusing attention on the most promising options, we argue that it can introduce unfairness if the uncertainty of the underlying relevance model differs between groups of options. Unfortunately, such disparity in uncertainty appears widespread, often to the detriment of minority groups for which relevance estimates can have higher uncertainty due to a lack of data or appropriate features. To address this fairness issue, we propose Equal-Opportunity Ranking (EOR) as a new fairness criterion for ranking and show that it corresponds to a group-wise fair lottery among the relevant options even
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21464;&#25506;&#27979;&#22836;&#26550;&#26500;&#20250;&#23545;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#20135;&#29983;&#24433;&#21709;&#65292;&#22312;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#35780;&#20272;&#20102;&#26356;&#22823;&#23481;&#37327;&#30340;&#25506;&#27979;&#22836;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#24615;&#33021;&#21644;&#25512;&#26029;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2308.14456</link><description>&lt;p&gt;
&#35821;&#38899;&#33258;&#30417;&#30563;&#34920;&#31034;&#22522;&#20934;&#27979;&#35797;&#65306;&#26356;&#22823;&#30340;&#25506;&#27979;&#22836;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Speech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21464;&#25506;&#27979;&#22836;&#26550;&#26500;&#20250;&#23545;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#20135;&#29983;&#24433;&#21709;&#65292;&#22312;&#35821;&#38899;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#35780;&#20272;&#20102;&#26356;&#22823;&#23481;&#37327;&#30340;&#25506;&#27979;&#22836;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#24615;&#33021;&#21644;&#25512;&#26029;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#22312;&#20943;&#23569;&#27880;&#37322;&#25968;&#25454;&#37327;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#22823;&#37327;&#25552;&#20986;&#30340;&#26041;&#27861;&#20419;&#20351;&#20986;&#29616;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#19968;&#32452;&#25506;&#32034;&#35821;&#38899;&#20449;&#21495;&#21508;&#20010;&#26041;&#38754;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#32771;&#34385;&#21040;&#30340;&#20219;&#21153;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#22823;&#22810;&#25968;&#25552;&#35758;&#20173;&#20381;&#36182;&#20110;&#19968;&#20010;&#23558;&#20923;&#32467;&#30340;SSL&#34920;&#31034;&#26144;&#23556;&#21040;&#20219;&#21153;&#26631;&#31614;&#30340;&#19979;&#28216;&#26550;&#26500;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22914;&#20309;&#25913;&#21464;&#25506;&#27979;&#22836;&#26550;&#26500;&#20250;&#24433;&#21709;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#25913;&#21464;&#19979;&#28216;&#26550;&#26500;&#32467;&#26500;&#20250;&#23548;&#33268;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#25490;&#21517;&#20986;&#29616;&#26174;&#33879;&#27874;&#21160;&#12290;&#19982;&#35821;&#38899;SSL&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24120;&#35265;&#20570;&#27861;&#30456;&#23545;&#31435;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26356;&#22823;&#23481;&#37327;&#30340;&#25506;&#27979;&#22836;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#23545;&#24615;&#33021;&#21644;&#25512;&#26029;&#25104;&#26412;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14456v2 Announce Type: replace-cross  Abstract: Self-supervised learning (SSL) leverages large datasets of unlabeled speech to reach impressive performance with reduced amounts of annotated data. The high number of proposed approaches fostered the emergence of comprehensive benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, while the number of considered tasks has been growing, most proposals rely upon a single downstream architecture that maps the frozen SSL representations to the task labels. This study examines how benchmarking results are affected by changes in the probing head architecture. Interestingly, we found that altering the downstream architecture structure leads to significant fluctuations in the performance ranking of the evaluated models. Against common practices in speech SSL benchmarking, we evaluate larger-capacity probing heads, showing their impact on performance, inference cos
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#20462;&#22797;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#30830;&#35748;&#20559;&#35265;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#27010;&#24565;&#19982;&#65288;&#20122;&#31526;&#21495;&#65289;&#35299;&#37322;&#20043;&#38388;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#20041;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2307.00897</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#20462;&#22797;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#30830;&#35748;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fixing confirmation bias in feature attribution methods via semantic match
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#20462;&#22797;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#30830;&#35748;&#20559;&#35265;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#27010;&#24565;&#19982;&#65288;&#20122;&#31526;&#21495;&#65289;&#35299;&#37322;&#20043;&#38388;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#20041;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#26512;&#40657;&#30418;&#27169;&#22411;&#22797;&#26434;&#34892;&#20026;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19968;&#20123;&#23398;&#32773;&#25351;&#20986;&#36825;&#31867;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65306;&#23427;&#20204;&#19981;&#33021;&#21487;&#38752;&#22320;&#29992;&#20154;&#31867;&#27010;&#24565;&#36827;&#34892;&#35299;&#37322;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#20165;&#20165;&#21487;&#35270;&#21270;&#19968;&#31995;&#21015;&#29305;&#24449;&#36129;&#29486;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#26080;&#27861;&#24471;&#20986;&#20851;&#20110;&#27169;&#22411;&#20869;&#37096;&#34920;&#31034;&#30340;&#32467;&#35770;&#65292;&#32780;&#30830;&#35748;&#20559;&#35265;&#21487;&#33021;&#20250;&#35753;&#29992;&#25143;&#20135;&#29983;&#20851;&#20110;&#27169;&#22411;&#34892;&#20026;&#30340;&#38169;&#35823;&#20449;&#24565;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#39564;&#35777;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#20551;&#35774;&#26159;&#21542;&#24471;&#21040;&#20102;&#29305;&#24449;&#24402;&#22240;&#30340;&#30830;&#35748;&#12290;&#36825;&#23601;&#26159;&#25105;&#20204;&#25152;&#35828;&#30340;&#20154;&#31867;&#27010;&#24565;&#19982;&#65288;&#20122;&#31526;&#21495;&#65289;&#35299;&#37322;&#20043;&#38388;&#30340;&#8220;&#35821;&#20041;&#21305;&#37197;&#8221;&#12290;&#22312; Cin\`a&#31561;&#20154;[2023]&#25552;&#20986;&#30340;&#27010;&#24565;&#26694;&#26550;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#22312;&#23454;&#36341;&#20013;&#35780;&#20272;&#35821;&#20041;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36825;&#19968;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00897v2 Announce Type: replace-cross  Abstract: Feature attribution methods have become a staple method to disentangle the complex behavior of black box models. Despite their success, some scholars have argued that such methods suffer from a serious flaw: they do not allow a reliable interpretation in terms of human concepts. Simply put, visualizing an array of feature contributions is not enough for humans to conclude something about a model's internal representations, and confirmation bias can trick users into false beliefs about model behavior. We argue that a structured approach is required to test whether our hypotheses on the model are confirmed by the feature attributions. This is what we call the "semantic match" between human concepts and (sub-symbolic) explanations. Building on the conceptual framework put forward in Cin\`a et al. [2023], we propose a structured approach to evaluate semantic match in practice. We showcase the procedure in a suite of experiments spa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20998;&#24067;&#21305;&#37197;&#23454;&#29616;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#21462;&#20027;&#23548;&#23398;&#20064;&#36807;&#31243;&#30340;&#39640;&#32423;&#27169;&#24335;&#65292;&#20197;&#23454;&#29616;&#20840;&#23616;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2306.10447</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#24067;&#21305;&#37197;&#23454;&#29616;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Globally Interpretable Graph Learning via Distribution Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.10447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20998;&#24067;&#21305;&#37197;&#23454;&#29616;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#21462;&#20027;&#23548;&#23398;&#20064;&#36807;&#31243;&#30340;&#39640;&#32423;&#27169;&#24335;&#65292;&#20197;&#23454;&#29616;&#20840;&#23616;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#25429;&#25417;&#20851;&#38190;&#22270;&#27169;&#24335;&#30340;&#24378;&#22823;&#27169;&#22411;&#12290;&#29616;&#22312;&#65292;&#20154;&#20204;&#27491;&#22312;&#35797;&#22270;&#35299;&#37322;&#27169;&#22411;&#34892;&#20026;&#32780;&#19981;&#26159;&#23558;&#20854;&#35270;&#20026;&#40657;&#30418;&#23376;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26412;&#22320;&#35299;&#37322;&#65292;&#25581;&#31034;&#27599;&#20010;&#20010;&#20307;&#23454;&#20363;&#30340;&#21306;&#20998;&#27169;&#24335;&#65292;&#20294;&#36825;&#19981;&#33021;&#30452;&#25509;&#21453;&#26144;&#23454;&#20363;&#20043;&#38388;&#30340;&#39640;&#23618;&#27169;&#22411;&#34892;&#20026;&#12290;&#20026;&#20102;&#33719;&#24471;&#20840;&#23616;&#35265;&#35299;&#65292;&#25105;&#20204;&#26088;&#22312;&#22238;&#31572;&#19968;&#20010;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30740;&#31350;&#30340;&#37325;&#35201;&#38382;&#39064;&#65306;&#22914;&#20309;&#20026;&#22270;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#20840;&#23616;&#35299;&#37322;&#65311;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#21046;&#23450;&#20026;&#20840;&#23616;&#21487;&#35299;&#37322;&#30340;&#22270;&#23398;&#20064;&#65292;&#26088;&#22312;&#25552;&#21462;&#20027;&#23548;&#23398;&#20064;&#36807;&#31243;&#30340;&#39640;&#32423;&#21644;&#20154;&#31867;&#21487;&#35299;&#35835;&#30340;&#27169;&#24335;&#65292;&#36825;&#26679;&#22312;&#36825;&#31181;&#27169;&#24335;&#19978;&#35757;&#32451;&#21487;&#20197;&#24674;&#22797;&#31867;&#20284;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#20010;&#24320;&#22987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#20445;&#30495;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.10447v2 Announce Type: replace  Abstract: Graph neural networks (GNNs) have emerged as a powerful model to capture critical graph patterns. Instead of treating them as black boxes in an end-to-end fashion, attempts are arising to explain the model behavior. Existing works mainly focus on local interpretation to reveal the discriminative pattern for each individual instance, which however cannot directly reflect the high-level model behavior across instances. To gain global insights, we aim to answer an important question that is not yet well studied: how to provide a global interpretation for the graph learning procedure? We formulate this problem as globally interpretable graph learning, which targets on distilling high-level and human-intelligible patterns that dominate the learning procedure, such that training on this pattern can recover a similar model. As a start, we propose a novel model fidelity metric, tailored for evaluating the fidelity of the resulting model trai
&lt;/p&gt;</description></item><item><title>TESS&#26159;&#19968;&#20010;&#20840;&#38750;&#33258;&#22238;&#24402;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36923;&#36753;&#31354;&#38388;&#32780;&#19981;&#26159;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#24212;&#29992;&#25193;&#25955;&#36807;&#31243;&#65292;&#36827;&#34892;&#20102;&#33258;&#26465;&#20214;&#21333;&#32431;&#24418;&#25193;&#25955;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#25193;&#25955;&#27493;&#39588;&#26356;&#23569;&#12290;</title><link>https://arxiv.org/abs/2305.08379</link><description>&lt;p&gt;
TESS&#65306;&#25991;&#26412;&#21040;&#25991;&#26412;&#33258;&#26465;&#20214;&#21333;&#32431;&#24418;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
TESS: Text-to-Text Self-Conditioned Simplex Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.08379
&lt;/p&gt;
&lt;p&gt;
TESS&#26159;&#19968;&#20010;&#20840;&#38750;&#33258;&#22238;&#24402;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36923;&#36753;&#31354;&#38388;&#32780;&#19981;&#26159;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#24212;&#29992;&#25193;&#25955;&#36807;&#31243;&#65292;&#36827;&#34892;&#20102;&#33258;&#26465;&#20214;&#21333;&#32431;&#24418;&#25193;&#25955;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#25193;&#25955;&#27493;&#39588;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#22312;&#21508;&#31181;&#36830;&#32493;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#29983;&#25104;&#26041;&#27861;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#33258;&#28982;&#35821;&#35328;&#26159;&#31163;&#25955;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#25193;&#25955;&#27493;&#39588;&#26469;&#29983;&#25104;&#25991;&#26412;&#65292;&#36825;&#20351;&#24471;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#21464;&#24471;&#26114;&#36149;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#21040;&#25991;&#26412;&#33258;&#26465;&#20214;&#21333;&#32431;&#24418;&#25193;&#25955;&#65288;TESS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38750;&#33258;&#22238;&#24402;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#37319;&#29992;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#33258;&#26465;&#20214;&#65292;&#23558;&#25193;&#25955;&#36807;&#31243;&#24212;&#29992;&#20110;&#36923;&#36753;&#31354;&#38388;&#32780;&#19981;&#26159;&#23398;&#20064;&#23884;&#20837;&#31354;&#38388;&#12290;&#36890;&#36807;&#23545;&#21253;&#25324;&#24635;&#32467;&#12289;&#25991;&#26412;&#31616;&#21270;&#12289;&#37322;&#20041;&#29983;&#25104;&#21644;&#38382;&#39064;&#29983;&#25104;&#22312;&#20869;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;TESS&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#22312;&#38656;&#35201;&#26356;&#23569;&#30340;&#25193;&#25955;&#27493;&#39588;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26368;&#23567;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.08379v2 Announce Type: replace  Abstract: Diffusion models have emerged as a powerful paradigm for generation, obtaining strong performance in various continuous domains. However, applying continuous diffusion models to natural language remains challenging due to its discrete nature and the need for a large number of diffusion steps to generate text, making diffusion-based generation expensive. In this work, we propose Text-to-text Self-conditioned Simplex Diffusion (TESS), a text diffusion model that is fully non-autoregressive, employs a new form of self-conditioning, and applies the diffusion process on the logit simplex space rather than the learned embedding space. Through extensive experiments on natural language understanding and generation tasks including summarization, text simplification, paraphrase generation, and question generation, we demonstrate that TESS outperforms state-of-the-art non-autoregressive models, requires fewer diffusion steps with minimal drop i
&lt;/p&gt;</description></item><item><title>PC-JeDi&#26159;&#19968;&#31181;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#29983;&#25104;&#31890;&#23376;&#20113;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#25442;&#22120;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#33021;&#20197;&#26465;&#20214;&#29983;&#25104;&#26041;&#24335;&#20135;&#29983;&#20855;&#26377;&#25152;&#38656;&#36136;&#37327;&#21644;&#27178;&#21160;&#37327;&#30340;&#21943;&#27880;&#12290;</title><link>https://arxiv.org/abs/2303.05376</link><description>&lt;p&gt;
PC-JeDi: &#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#29992;&#20110;&#29983;&#25104;&#31890;&#23376;&#20113;&#30340;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PC-JeDi: Diffusion for Particle Cloud Generation in High Energy Physics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.05376
&lt;/p&gt;
&lt;p&gt;
PC-JeDi&#26159;&#19968;&#31181;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#29983;&#25104;&#31890;&#23376;&#20113;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#25442;&#22120;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#33021;&#20197;&#26465;&#20214;&#29983;&#25104;&#26041;&#24335;&#20135;&#29983;&#20855;&#26377;&#25152;&#38656;&#36136;&#37327;&#21644;&#27178;&#21160;&#37327;&#30340;&#21943;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PC-JeDi&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#33021;&#29289;&#29702;&#20013;&#39640;&#25928;&#29983;&#25104;&#21943;&#27880;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#36866;&#29992;&#20110;&#29983;&#25104;&#31890;&#23376;&#20113;&#30340;&#21464;&#25442;&#22120;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#32622;&#25442;&#31561;&#21464;&#24615;&#12290;PC-JeDi&#22312;&#35780;&#20272;&#29983;&#25104;&#21943;&#27880;&#36136;&#37327;&#30340;&#20960;&#20010;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#21069;&#21521;&#20256;&#36882;&#32780;&#27604;&#20854;&#20182;&#27169;&#22411;&#24930;&#65292;&#20294;&#20173;&#28982;&#27604;&#20256;&#32479;&#35814;&#32454;&#27169;&#25311;&#24555;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;PC-JeDi&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#26469;&#20026;&#20004;&#31181;&#19981;&#21516;&#30340;&#31890;&#23376;&#65292;&#22840;&#20811;&#21644;&#33014;&#23376;&#65292;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#36136;&#37327;&#21644;&#27178;&#21160;&#37327;&#30340;&#21943;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.05376v2 Announce Type: replace-cross  Abstract: In this paper, we present a new method to efficiently generate jets in High Energy Physics called PC-JeDi. This method utilises score-based diffusion models in conjunction with transformers which are well suited to the task of generating jets as particle clouds due to their permutation equivariance. PC-JeDi achieves competitive performance with current state-of-the-art methods across several metrics that evaluate the quality of the generated jets. Although slower than other models, due to the large number of forward passes required by diffusion models, it is still substantially faster than traditional detailed simulation. Furthermore, PC-JeDi uses conditional generation to produce jets with a desired mass and transverse momentum for two different particles, top quarks and gluons.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#32452;&#21512;&#20248;&#21270;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#20174;&#26368;&#20248;&#30340;&#23436;&#20840;&#20449;&#24687;&#35299;&#20013;&#23398;&#20064;&#22312;&#32447;&#27966;&#36963;&#21644;&#20877;&#24179;&#34913;&#31574;&#30053;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#23454;&#38469;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#29616;&#21033;&#28070;&#25552;&#39640;&#20102;6.3%&#12290;</title><link>https://arxiv.org/abs/2302.03963</link><description>&lt;p&gt;
&#23398;&#20064;&#22411;&#22312;&#32447;&#20248;&#21270;&#29992;&#20110;&#33258;&#20027;&#31227;&#21160;&#20986;&#34892;&#38656;&#27714;&#36710;&#38431;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning-based Online Optimization for Autonomous Mobility-on-Demand Fleet Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03963
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#32452;&#21512;&#20248;&#21270;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#20174;&#26368;&#20248;&#30340;&#23436;&#20840;&#20449;&#24687;&#35299;&#20013;&#23398;&#20064;&#22312;&#32447;&#27966;&#36963;&#21644;&#20877;&#24179;&#34913;&#31574;&#30053;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#23454;&#38469;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#23454;&#29616;&#21033;&#28070;&#25552;&#39640;&#20102;6.3%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20986;&#34892;&#38656;&#27714;&#31995;&#32479;&#26159;&#20943;&#32531;&#22478;&#24066;&#20132;&#36890;&#30456;&#20851;&#22806;&#37096;&#24615;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#65292;&#22914;&#22478;&#24066;&#21306;&#22495;&#20869;&#20132;&#36890;&#24037;&#20855;&#25968;&#37327;&#22686;&#21152;&#21644;&#20132;&#36890;&#30456;&#20851;&#27745;&#26579;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#36710;&#38431;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#20027;&#20986;&#34892;&#38656;&#27714;&#31995;&#32479;&#30340;&#22312;&#32447;&#25511;&#21046;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#32452;&#21512;&#20248;&#21270;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#20174;&#26368;&#20248;&#30340;&#23436;&#20840;&#20449;&#24687;&#35299;&#20013;&#23398;&#20064;&#22312;&#32447;&#27966;&#36963;&#21644;&#20877;&#24179;&#34913;&#25919;&#31574;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#36710;&#38431;&#35268;&#27169;&#21644;&#21508;&#31181;&#35831;&#27714;&#23494;&#24230;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#28151;&#21512;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#21033;&#28070;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#36138;&#23146;&#21644;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#26041;&#27861;&#65292;&#20363;&#22914;&#22312;&#21508;&#31181;&#20851;&#38190;&#32489;&#25928;&#25351;&#26631;&#26041;&#38754;&#27604;&#26368;&#39640;&#34920;&#29616;&#25552;&#39640;&#20102;&#39640;&#36798;17.1%&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;6.3%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03963v2 Announce Type: replace-cross  Abstract: Autonomous mobility-on-demand systems are a viable alternative to mitigate many transportation-related externalities in cities, such as rising vehicle volumes in urban areas and transportation-related pollution. However, the success of these systems heavily depends on efficient and effective fleet control strategies. In this context, we study online control algorithms for autonomous mobility-on-demand systems and develop a novel hybrid combinatorial optimization enriched machine learning pipeline which learns online dispatching and rebalancing policies from optimal full-information solutions. We test our hybrid pipeline on large-scale real-world scenarios with different vehicle fleet sizes and various request densities. We show that our approach outperforms state-of-the-art greedy, and model-predictive control approaches with respect to various KPIs, e.g., by up to 17.1% and on average by 6.3% in terms of realized profit.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#21644;&#24179;&#26041;&#21644;&#36924;&#36817;&#35299;&#20915;&#24102;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#32479;&#19968;&#23450;&#29702;&#35777;&#26126;&#36825;&#20123;&#26041;&#26696;&#30340;&#25910;&#25947;&#24615;&#65292;&#24341;&#20837;&#25955;&#23556;&#19981;&#31561;&#24335;&#20197;&#32531;&#35299;&#32500;&#24230;&#28798;&#38590;&#38382;&#39064;&#65292;&#24182;&#22312;&#23398;&#20064;&#21521;&#37327;&#22330;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2301.06339</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#21644;&#24179;&#26041;&#21644;&#36924;&#36817;&#24102;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Approximation of optimization problems with constraints through kernel Sum-Of-Squares
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.06339
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#21644;&#24179;&#26041;&#21644;&#36924;&#36817;&#35299;&#20915;&#24102;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#32479;&#19968;&#23450;&#29702;&#35777;&#26126;&#36825;&#20123;&#26041;&#26696;&#30340;&#25910;&#25947;&#24615;&#65292;&#24341;&#20837;&#25955;&#23556;&#19981;&#31561;&#24335;&#20197;&#32531;&#35299;&#32500;&#24230;&#28798;&#38590;&#38382;&#39064;&#65292;&#24182;&#22312;&#23398;&#20064;&#21521;&#37327;&#22330;&#30340;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#26080;&#38480;&#32500;&#31354;&#38388;&#20013;&#26080;&#38480;&#25968;&#37327;&#30340;&#19981;&#31561;&#24335;&#32422;&#26463;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#23384;&#22312;&#65292;&#20174;&#20840;&#23616;&#20248;&#21270;&#21040;&#26368;&#20248;&#36755;&#36816;&#12290;&#36825;&#20123;&#38382;&#39064;&#22312;&#20197;&#21069;&#30340;&#20960;&#31687;&#25991;&#31456;&#20013;&#20998;&#21035;&#36890;&#36807;&#26680;&#21644;&#24179;&#26041;&#21644;&#65288;kSoS&#65289;&#36924;&#36817;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#26696;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#36880;&#28857;&#19981;&#31561;&#24335;&#22312;&#19968;&#31867;&#38750;&#36127;kSoS&#20989;&#25968;&#20013;&#36716;&#21270;&#20026;&#31561;&#24335;&#12290;&#36827;&#19968;&#27493;&#20551;&#35774;&#38382;&#39064;&#20013;&#20986;&#29616;&#30340;&#20989;&#25968;&#26159;&#20809;&#28369;&#30340;&#65292;&#19987;&#27880;&#20110;&#36880;&#28857;&#30456;&#31561;&#32422;&#26463;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#25955;&#23556;&#19981;&#31561;&#24335;&#26469;&#32531;&#35299;&#22312;&#37319;&#26679;&#32422;&#26463;&#26041;&#38754;&#30340;&#32500;&#24230;&#28798;&#38590;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#24102;&#26377;&#36793;&#32536;&#20449;&#24687;&#30340;&#21521;&#37327;&#22330;&#20013;&#24471;&#21040;&#20102;&#35828;&#26126;&#65292;&#36825;&#37324;&#30340;&#19981;&#21464;&#24615;&#26159;&#19968;&#20010;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.06339v2 Announce Type: replace-cross  Abstract: Handling an infinite number of inequality constraints in infinite-dimensional spaces occurs in many fields, from global optimization to optimal transport. These problems have been tackled individually in several previous articles through kernel Sum-Of-Squares (kSoS) approximations. We propose here a unified theorem to prove convergence guarantees for these schemes. Pointwise inequalities are turned into equalities within a class of nonnegative kSoS functions. Assuming further that the functions appearing in the problem are smooth, focusing on pointwise equality constraints enables the use of scattering inequalities to mitigate the curse of dimensionality in sampling the constraints. Our approach is illustrated in learning vector fields with side information, here the invariance of a set.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#37325;&#21472;&#20998;&#32452; lasso &#30340;&#38750;&#37325;&#21472;&#32479;&#35745;&#36924;&#36817;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#20026;&#29616;&#20195;&#38382;&#39064;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2211.09221</link><description>&lt;p&gt;
&#38024;&#23545;&#37325;&#21472;&#20998;&#32452; lasso &#30340;&#38750;&#37325;&#21472;&#32479;&#35745;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
The non-overlapping statistical approximation to overlapping group lasso
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.09221
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#37325;&#21472;&#20998;&#32452; lasso &#30340;&#38750;&#37325;&#21472;&#32479;&#35745;&#36924;&#36817;&#26041;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#20026;&#29616;&#20195;&#38382;&#39064;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452; lasso &#26159;&#32479;&#35745;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#32452;&#20174;&#27169;&#22411;&#20013;&#28040;&#38500;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#32452;&#37325;&#21472;&#26102;&#65292;&#30001;&#20110;&#37325;&#21472;&#32452;&#24341;&#36215;&#30340;&#19981;&#21487;&#20998;&#24615;&#65292;&#20248;&#21270;&#32452; lasso &#24809;&#32602;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#21487;&#33021;&#20250;&#21464;&#24471;&#32791;&#26102;&#65292;&#36825;&#19968;&#29942;&#39048;&#20005;&#37325;&#38480;&#21046;&#20102;&#37325;&#21472;&#20998;&#32452; lasso &#27491;&#21017;&#21270;&#22312;&#35768;&#22810;&#29616;&#20195;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#27604;&#22914;&#22522;&#22240;&#36890;&#36335;&#36873;&#25321;&#21644;&#22270;&#27169;&#22411;&#20272;&#35745;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20998;&#30340;&#24809;&#32602;&#20316;&#20026;&#37325;&#21472;&#20998;&#32452; lasso &#24809;&#32602;&#30340;&#36924;&#36817;&#12290;&#30001;&#20110;&#21487;&#20998;&#24615;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#24809;&#32602;&#30340;&#27491;&#21017;&#21270;&#35745;&#31639;&#30456;&#23545;&#20110;&#37325;&#21472;&#20998;&#32452; lasso &#35201;&#24555;&#24471;&#22810;&#65292;&#23588;&#20854;&#23545;&#20110;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#24809;&#32602;&#26159;&#37325;&#21472;&#32452; lasso &#30340;&#26368;&#20005;&#26684;&#30340;&#21487;&#20998;&#26494;&#24347;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.09221v3 Announce Type: replace-cross  Abstract: Group lasso is a commonly used regularization method in statistical learning in which parameters are eliminated from the model according to predefined groups. However, when the groups overlap, optimizing the group lasso penalized objective can be time-consuming on large-scale problems because of the non-separability induced by the overlapping groups. This bottleneck has seriously limited the application of overlapping group lasso regularization in many modern problems, such as gene pathway selection and graphical model estimation. In this paper, we propose a separable penalty as an approximation of the overlapping group lasso penalty. Thanks to the separability, the computation of regularization based on our penalty is substantially faster than that of the overlapping group lasso, especially for large-scale and high-dimensional problems. We show that the penalty is the tightest separable relaxation of the overlapping group lass
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#30456;&#20284;&#24230;&#35780;&#20998;&#20989;&#25968;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#36136;&#30340;&#20851;&#38190;&#24037;&#20855;&#26159;ROC&#26354;&#32447;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;&#19982;ROC&#26354;&#32447;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#38754;&#37096;&#35782;&#21035;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2211.07245</link><description>&lt;p&gt;
&#35780;&#20272;&#30456;&#20284;&#24230;&#35780;&#20998;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#38754;&#37096;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#19982;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing Uncertainty in Similarity Scoring: Performance &amp; Fairness in Face Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.07245
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#30456;&#20284;&#24230;&#35780;&#20998;&#20989;&#25968;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#36136;&#30340;&#20851;&#38190;&#24037;&#20855;&#26159;ROC&#26354;&#32447;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;&#19982;ROC&#26354;&#32447;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#38754;&#37096;&#35782;&#21035;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ROC&#26354;&#32447;&#26159;&#35780;&#20272;&#30456;&#20284;&#24230;&#35780;&#20998;&#20989;&#25968;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#36136;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#20026;&#20102;&#22522;&#20110;&#32463;&#39564;ROC&#20998;&#26512;&#24471;&#20986;&#21487;&#38752;&#32467;&#35770;&#65292;&#20934;&#30830;&#35780;&#20272;&#19982;&#24863;&#20852;&#36259;&#30340;ROC&#26354;&#32447;&#30340;&#32479;&#35745;&#29256;&#26412;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#26159;&#32477;&#23545;&#24517;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#37325;&#35201;&#31038;&#20250;&#24433;&#21709;&#30340;&#24212;&#29992;&#65292;&#22914;&#38754;&#37096;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#20284;&#24615;&#20989;&#25968;&#30340;&#32463;&#39564;ROC&#26354;&#32447;&#20197;&#21450;&#29992;&#20110;&#35780;&#20272;&#20844;&#24179;&#24615;&#30340;&#21103;&#20135;&#21697;&#25351;&#26631;&#30340;&#28176;&#36817;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#65292;&#30001;&#20110;&#22312;&#30456;&#20284;&#24230;&#35780;&#20998;&#24773;&#20917;&#19979;&#65292;&#35823;&#25509;&#21463;/&#25298;&#32477;&#29575;&#30340;&#24418;&#24335;&#20026;U-&#32479;&#35745;&#37327;&#65292;&#25152;&#20197;&#22825;&#30495;&#30340;&#33258;&#21161;&#27861;&#21487;&#33021;&#20250;&#21361;&#21450;&#35780;&#20272;&#36807;&#31243;&#12290;&#24517;&#39035;&#20351;&#29992;&#19987;&#38376;&#30340;&#37325;&#26032;&#23621;&#20013;&#25216;&#26415;&#12290;&#38500;&#36827;&#34892;&#30340;&#29702;&#35770;&#20998;&#26512;&#22806;&#65292;&#36824;&#20351;&#29992;&#30495;&#23454;&#20154;&#33080;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.07245v2 Announce Type: replace-cross  Abstract: The ROC curve is the major tool for assessing not only the performance but also the fairness properties of a similarity scoring function. In order to draw reliable conclusions based on empirical ROC analysis, accurately evaluating the uncertainty level related to statistical versions of the ROC curves of interest is absolutely necessary, especially for applications with considerable societal impact such as Face Recognition. In this article, we prove asymptotic guarantees for empirical ROC curves of similarity functions as well as for by-product metrics useful to assess fairness. We also explain that, because the false acceptance/rejection rates are of the form of U-statistics in the case of similarity scoring, the naive bootstrap approach may jeopardize the assessment procedure. A dedicated recentering technique must be used instead. Beyond the theoretical analysis carried out, various experiments using real face image datasets
&lt;/p&gt;</description></item><item><title>CLEEGN&#26159;&#19968;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#33258;&#21160;&#33041;&#30005;&#22270;&#37325;&#24314;&#65292;&#22522;&#20110;&#29420;&#31435;&#20110;&#21463;&#35797;&#32773;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#26657;&#20934;&#21363;&#21487;&#22312;&#26032;&#29992;&#25143;&#19978;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2210.05988</link><description>&lt;p&gt;
CLEEGN&#65306;&#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#33258;&#21160;&#33041;&#30005;&#22270;&#37325;&#24314;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CLEEGN: A Convolutional Neural Network for Plug-and-Play Automatic EEG Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.05988
&lt;/p&gt;
&lt;p&gt;
CLEEGN&#26159;&#19968;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#33258;&#21160;&#33041;&#30005;&#22270;&#37325;&#24314;&#65292;&#22522;&#20110;&#29420;&#31435;&#20110;&#21463;&#35797;&#32773;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#26657;&#20934;&#21363;&#21487;&#22312;&#26032;&#29992;&#25143;&#19978;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#19968;&#31181;&#24863;&#30693;&#39640;&#26102;&#31354;&#20998;&#36776;&#29575;&#19979;&#30382;&#23618;&#31070;&#32463;&#30005;&#29983;&#29702;&#27963;&#21160;&#30340;&#22823;&#33041;&#30417;&#27979;&#27169;&#24577;&#12290;&#24212;&#29992;EEG&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#20449;&#21495;&#36136;&#37327;&#19981;&#31283;&#23450;&#65292;&#23481;&#26131;&#22312;&#35760;&#24405;&#36807;&#31243;&#20013;&#21463;&#21040;&#19981;&#21487;&#36991;&#20813;&#30340;&#20266;&#36857;&#24433;&#21709;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;EEG&#20266;&#36857;&#21435;&#38500;&#21644;&#37325;&#24314;&#25216;&#26415;&#20165;&#36866;&#29992;&#20110;&#31163;&#32447;&#20998;&#26512;&#65292;&#25110;&#38656;&#35201;&#20010;&#24615;&#21270;&#35757;&#32451;&#25968;&#25454;&#20197;&#36827;&#34892;&#22312;&#32447;&#37325;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CLEEGN&#65292;&#19968;&#31181;&#29992;&#20110;&#21363;&#25554;&#21363;&#29992;&#33258;&#21160;EEG&#37325;&#24314;&#30340;&#26032;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;CLEEGN&#22522;&#20110;&#19968;&#20010;&#29420;&#31435;&#20110;&#21463;&#35797;&#32773;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#26032;&#29992;&#25143;&#19978;&#25805;&#20316;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#26657;&#20934;&#12290;CLEEGN&#30340;&#24615;&#33021;&#32463;&#36807;&#22810;&#20010;&#35780;&#20272;&#39564;&#35777;&#65292;&#21253;&#25324;&#27874;&#24418;&#35266;&#23519;&#12289;&#37325;&#24314;&#35823;&#24046;&#35780;&#20272;&#20197;&#21450;&#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#30340;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.05988v2 Announce Type: replace-cross  Abstract: Human electroencephalography (EEG) is a brain monitoring modality that senses cortical neuroelectrophysiological activity in high-temporal resolution. One of the greatest challenges posed in applications of EEG is the unstable signal quality susceptible to inevitable artifacts during recordings. To date, most existing techniques for EEG artifact removal and reconstruction are applicable to offline analysis solely, or require individualized training data to facilitate online reconstruction. We have proposed CLEEGN, a novel convolutional neural network for plug-and-play automatic EEG reconstruction. CLEEGN is based on a subject-independent pre-trained model using existing data and can operate on a new user without any further calibration. The performance of CLEEGN was validated using multiple evaluations including waveform observation, reconstruction error assessment, and decoding accuracy on well-studied labeled datasets. The re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Kernel Infomax&#30340;&#33258;&#30417;&#30563;&#22270;&#26680;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#34920;&#24449;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#26102;&#38388;&#24615;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2209.00655</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#26680;Infomax&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#33258;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Representation Learning on Electronic Health Records with Graph Kernel Infomax
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00655
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Kernel Infomax&#30340;&#33258;&#30417;&#30563;&#22270;&#26680;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#34920;&#24449;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#26102;&#38388;&#24615;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#34920;&#24449;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#23578;&#26410;&#34987;&#20805;&#20998;&#21457;&#25496;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#36825;&#26377;&#21033;&#20110;&#21508;&#31181;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#65292;&#20363;&#22914;&#33647;&#29289;&#32467;&#26524;&#39044;&#27979;&#25110;&#24739;&#32773;&#30456;&#20284;&#24615;&#25628;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#22270;&#26680;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;Graph Kernel Infomax&#65292;&#29992;&#20110;&#22788;&#29702;EHR&#30340;&#22270;&#24418;&#34920;&#24449;&#65292;&#20197;&#20811;&#26381;&#20808;&#21069;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00655v2 Announce Type: replace  Abstract: Learning Electronic Health Records (EHRs) representation is a preeminent yet under-discovered research topic. It benefits various clinical decision support applications, e.g., medication outcome prediction or patient similarity search. Current approaches focus on task-specific label supervision on vectorized sequential EHR, which is not applicable to large-scale unsupervised scenarios. Recently, contrastive learning shows great success on self-supervised representation learning problems. However, complex temporality often degrades the performance. We propose Graph Kernel Infomax, a self-supervised graph kernel learning approach on the graphical representation of EHR, to overcome the previous problems. Unlike the state-of-the-art, we do not change the graph structure to construct augmented views. Instead, we use Kernel Subspace Augmentation to embed nodes into two geometrically different manifold views. The entire framework is trained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28201;&#21644;&#20445;&#23432;Q&#23398;&#20064;&#65288;MCQ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;OOD&#21160;&#20316;&#20998;&#37197;&#36866;&#24403;&#30340;&#20266;Q&#20540;&#26469;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#19981;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20215;&#20540;&#20989;&#25968;&#30340;&#20445;&#23432;&#24615;&#65292;&#36991;&#20813;&#36807;&#24230;&#39640;&#20272;&#36229;&#20986;&#20998;&#24067;&#30340;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2206.04745</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28201;&#21644;&#20445;&#23432;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mildly Conservative Q-Learning for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.04745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28201;&#21644;&#20445;&#23432;Q&#23398;&#20064;&#65288;MCQ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;OOD&#21160;&#20316;&#20998;&#37197;&#36866;&#24403;&#30340;&#20266;Q&#20540;&#26469;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#19981;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20215;&#20540;&#20989;&#25968;&#30340;&#20445;&#23432;&#24615;&#65292;&#36991;&#20813;&#36807;&#24230;&#39640;&#20272;&#36229;&#20986;&#20998;&#24067;&#30340;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23450;&#20041;&#20102;&#20174;&#38745;&#24577;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#32780;&#26080;&#38656;&#25345;&#32493;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#23398;&#20064;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#20351;&#24471;&#20215;&#20540;&#20989;&#25968;&#20445;&#25345;&#20445;&#23432;&#25104;&#20026;&#24517;&#35201;&#65292;&#20197;&#30830;&#20445;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#21160;&#20316;&#19981;&#20250;&#34987;&#20005;&#37325;&#39640;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22914;&#23545;&#26410;&#35265;&#21160;&#20316;&#36827;&#34892;&#24809;&#32602;&#25110;&#19982;&#34892;&#20026;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#37117;&#36807;&#20110;&#24754;&#35266;&#65292;&#25233;&#21046;&#20102;&#20215;&#20540;&#20989;&#25968;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#38459;&#30861;&#20102;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#23398;&#20064;&#20013;&#30340;&#28201;&#21644;&#20294;&#36275;&#22815;&#20445;&#23432;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28201;&#21644;&#20445;&#23432;Q&#23398;&#20064;&#65288;MCQ&#65289;&#65292;&#36890;&#36807;&#20026;OOD&#21160;&#20316;&#20998;&#37197;&#36866;&#24403;&#30340;&#20266;Q&#20540;&#26469;&#31215;&#26497;&#35757;&#32451;&#23427;&#20204;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;MCQ&#20250;&#20135;&#29983;&#19968;&#20010;&#33267;&#23569;&#19982;&#34892;&#20026;&#31574;&#30053;&#19968;&#26679;&#22909;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#19981;&#20250;&#21457;&#29983;&#38169;&#35823;&#30340;&#36807;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.04745v3 Announce Type: replace-cross  Abstract: Offline reinforcement learning (RL) defines the task of learning from a static logged dataset without continually interacting with the environment. The distribution shift between the learned policy and the behavior policy makes it necessary for the value function to stay conservative such that out-of-distribution (OOD) actions will not be severely overestimated. However, existing approaches, penalizing the unseen actions or regularizing with the behavior policy, are too pessimistic, which suppresses the generalization of the value function and hinders the performance improvement. This paper explores mild but enough conservatism for offline learning while not harming generalization. We propose Mildly Conservative Q-learning (MCQ), where OOD actions are actively trained by assigning them proper pseudo Q values. We theoretically show that MCQ induces a policy that behaves at least as well as the behavior policy and no erroneous ov
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#26500;&#24314;&#21487;&#20449;&#36182;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32508;&#21512;&#36335;&#32447;&#22270;&#65292;&#20851;&#27880;&#35299;&#20915;&#24615;&#33021;&#23548;&#21521;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#23384;&#22312;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12289;&#27495;&#35270;&#38382;&#39064;&#21644;&#36164;&#28304;&#28040;&#32791;&#36807;&#22810;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2205.07424</link><description>&lt;p&gt;
&#20540;&#24471;&#20449;&#36182;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#26041;&#38754;&#12289;&#26041;&#27861;&#21644;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Graph Neural Networks: Aspects, Methods and Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.07424
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#26500;&#24314;&#21487;&#20449;&#36182;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32508;&#21512;&#36335;&#32447;&#22270;&#65292;&#20851;&#27880;&#35299;&#20915;&#24615;&#33021;&#23548;&#21521;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#23384;&#22312;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12289;&#27495;&#35270;&#38382;&#39064;&#21644;&#36164;&#28304;&#28040;&#32791;&#36807;&#22810;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#19981;&#20165;&#20165;&#20851;&#27880;&#20219;&#21153;&#34920;&#29616;&#65292;&#36824;&#30528;&#37325;&#25351;&#20986;&#24615;&#33021;&#23548;&#21521;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#23384;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#12289;&#27495;&#35270;&#24369;&#21183;&#32676;&#20307;&#12289;&#22312;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#28040;&#32791;&#36164;&#28304;&#36807;&#22810;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#24847;&#22806;&#20260;&#23475;&#65292;&#26377;&#24517;&#35201;&#26500;&#24314;&#20855;&#26377;&#20449;&#36182;&#24615;&#30340;&#39640;&#25928;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#20174;&#28041;&#21450;&#30340;&#21508;&#31181;&#35745;&#31639;&#25216;&#26415;&#35270;&#35282;&#26500;&#24314;&#21487;&#20449;&#36182;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.07424v2 Announce Type: replace-cross  Abstract: Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications like recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects like vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterised by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarise existin
&lt;/p&gt;</description></item><item><title>&#20914;&#31361;&#22238;&#36991;&#26799;&#24230;&#19979;&#38477;&#65288;CAGrad&#65289;&#26159;&#38024;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#26799;&#24230;&#20914;&#31361;&#38382;&#39064;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#19981;&#21516;&#20219;&#21153;&#26799;&#24230;&#19981;&#19968;&#33268;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2110.14048</link><description>&lt;p&gt;
&#20914;&#31361;&#22238;&#36991;&#26799;&#24230;&#19979;&#38477;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conflict-Averse Gradient Descent for Multi-task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.14048
&lt;/p&gt;
&lt;p&gt;
&#20914;&#31361;&#22238;&#36991;&#26799;&#24230;&#19979;&#38477;&#65288;CAGrad&#65289;&#26159;&#38024;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#26799;&#24230;&#20914;&#31361;&#38382;&#39064;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#19981;&#21516;&#20219;&#21153;&#26799;&#24230;&#19981;&#19968;&#33268;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20849;&#20139;&#27169;&#22411;&#32467;&#26500;&#26469;&#23454;&#29616;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#12290;&#26631;&#20934;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#25152;&#26377;&#20219;&#21153;&#30340;&#24179;&#22343;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20010;&#30446;&#26631;&#36890;&#24120;&#20250;&#23548;&#33268;&#27599;&#20010;&#20219;&#21153;&#30340;&#26368;&#32456;&#34920;&#29616;&#27604;&#29420;&#31435;&#23398;&#20064;&#23427;&#20204;&#26102;&#26356;&#24046;&#12290;&#22312;&#20248;&#21270;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20914;&#31361;&#26799;&#24230;&#65292;&#21363;&#19981;&#21516;&#20219;&#21153;&#30446;&#26631;&#30340;&#26799;&#24230;&#19981;&#22826;&#19968;&#33268;&#65292;&#22240;&#27492;&#36981;&#24490;&#24179;&#22343;&#26799;&#24230;&#26041;&#21521;&#21487;&#33021;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#26377;&#23475;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25805;&#32437;&#20219;&#21153;&#26799;&#24230;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#20294;&#26159;&#22823;&#22810;&#25968;&#26041;&#27861;&#32570;&#20047;&#25910;&#25947;&#20445;&#35777;&#21644;/&#25110;&#21487;&#33021;&#25910;&#25947;&#21040;&#20219;&#20309;&#24085;&#32047;&#25176;&#31283;&#23450;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21483;&#20570;&#20914;&#31361;&#22238;&#36991;&#26799;&#24230;&#19979;&#38477;&#65288;CAGrad&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#24179;&#22343;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.14048v2 Announce Type: replace-cross  Abstract: The goal of multi-task learning is to enable more efficient learning than single task learning by sharing model structures for a diverse set of tasks. A standard multi-task learning objective is to minimize the average loss across all tasks. While straightforward, using this objective often results in much worse final performance for each task than learning them independently. A major challenge in optimizing a multi-task model is the conflicting gradients, where gradients of different task objectives are not well aligned so that following the average gradient direction can be detrimental to specific tasks' performance. Previous work has proposed several heuristics to manipulate the task gradients for mitigating this problem. But most of them lack convergence guarantee and/or could converge to any Pareto-stationary point. In this paper, we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the average loss funct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65288;MBMM&#65289;&#30340;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#36719;&#32858;&#31867;&#12290;MBMM&#36890;&#36807;&#20854;&#28789;&#27963;&#30340;&#22810;&#20803;&#36125;&#22612;&#20998;&#24067;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#36866;&#24212;&#19981;&#21516;&#30340;&#32858;&#31867;&#24418;&#29366;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16708</link><description>&lt;p&gt;
&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65306;&#20855;&#26377;&#28789;&#27963;&#32858;&#31867;&#24418;&#29366;&#30340;&#27010;&#29575;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible Cluster Shapes. (arXiv:2401.16708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65288;MBMM&#65289;&#30340;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#36719;&#32858;&#31867;&#12290;MBMM&#36890;&#36807;&#20854;&#28789;&#27963;&#30340;&#22810;&#20803;&#36125;&#22612;&#20998;&#24067;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#36866;&#24212;&#19981;&#21516;&#30340;&#32858;&#31867;&#24418;&#29366;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65288;MBMM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#29992;&#20110;&#36719;&#32858;&#31867;&#12290;MBMM&#36890;&#36807;&#22810;&#20803;&#36125;&#22612;&#20998;&#24067;&#30340;&#28789;&#27963;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#36866;&#24212;&#19981;&#21516;&#30340;&#32858;&#31867;&#24418;&#29366;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MBMM&#30340;&#23646;&#24615;&#65292;&#25551;&#36848;&#20102;&#21442;&#25968;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36866;&#21512;&#21508;&#31181;&#32858;&#31867;&#24418;&#29366;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#20195;&#30721;&#21311;&#21517;&#21457;&#24067;&#22312;\url{https://github.com/hhchen1105/mbmm/}&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the multivariate beta mixture model (MBMM), a new probabilistic model for soft clustering. MBMM adapts to diverse cluster shapes because of the flexible probability density function of the multivariate beta distribution. We introduce the properties of MBMM, describe the parameter learning procedure, and present the experimental results, showing that MBMM fits diverse cluster shapes on synthetic and real datasets. The code is released anonymously at \url{https://github.com/hhchen1105/mbmm/}.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.16468</link><description>&lt;p&gt;
&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
High-Quality Image Restoration Following Human Instructions. (arXiv:2401.16468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#28041;&#21450;&#20174;&#36864;&#21270;&#35266;&#27979;&#20013;&#24674;&#22797;&#20986;&#39640;&#36136;&#37327;&#30340;&#24178;&#20928;&#22270;&#20687;&#12290;&#20840;&#33021;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36864;&#21270;&#31867;&#22411;&#30340;&#20449;&#24687;&#20316;&#20026;&#25552;&#31034;&#26469;&#26377;&#25928;&#22320;&#24674;&#22797;&#21508;&#31181;&#31867;&#22411;&#21644;&#32423;&#21035;&#30340;&#36864;&#21270;&#22270;&#20687;&#65292;&#24182;&#24341;&#23548;&#24674;&#22797;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;&#36864;&#21270;&#22270;&#20687;&#20013;&#24674;&#22797;&#20986;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#32771;&#34385;&#22810;&#31181;&#36864;&#21270;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;InstructIR&#22312;&#22270;&#20687;&#21435;&#22122;&#12289;&#38632;&#27700;&#21435;&#38500;&#12289;&#21435;&#27169;&#31946;&#12289;&#21435;&#38654;&#21644;(&#20302;&#20809;)&#22270;&#20687;&#22686;&#24378;&#31561;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;InstructIR&#22312;&#20043;&#21069;&#30340;&#20840;&#33021;&#24674;&#22797;&#26041;&#27861;&#19978;&#25552;&#39640;&#20102;1dB&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#32467;&#26524;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30340;&#26032;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image restoration is a fundamental problem that involves recovering a high-quality clean image from its degraded observation. All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model. In this work, we present the first approach that uses human-written instructions to guide the image restoration model. Given natural language prompts, our model can recover high-quality images from their degraded counterparts, considering multiple degradation types. Our method, InstructIR, achieves state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, dehazing, and (low-light) image enhancement. InstructIR improves +1dB over previous all-in-one restoration methods. Moreover, our dataset and results represent a novel benchmark for new research on text-guided image restoration and enhancement. Our code, datasets and models a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#23398;&#20064;&#26102;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35757;&#32451;&#38598;&#32500;&#24230;&#26377;&#20851;&#30340;&#27867;&#21270;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#35748;&#20026;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#26356;&#39640;&#30340;&#22266;&#26377;&#8220;&#26631;&#31614;&#38160;&#24230;&#8221;&#21487;&#33021;&#26159;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#26174;&#33879;&#24046;&#24322;&#30340;&#37096;&#20998;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2401.08865</link><description>&lt;p&gt;
Intrinsic Dataset Properties&#23545;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65306;&#25581;&#31034;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#20043;&#38388;&#30340;&#23398;&#20064;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images. (arXiv:2401.08865v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#28982;&#22270;&#20687;&#21644;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#23398;&#20064;&#26102;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#35757;&#32451;&#38598;&#32500;&#24230;&#26377;&#20851;&#30340;&#27867;&#21270;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#35748;&#20026;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#26356;&#39640;&#30340;&#22266;&#26377;&#8220;&#26631;&#31614;&#38160;&#24230;&#8221;&#21487;&#33021;&#26159;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#26174;&#33879;&#24046;&#24322;&#30340;&#37096;&#20998;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#22270;&#20687;&#39046;&#22495;&#23398;&#20064;&#26102;&#30340;&#24046;&#24322;&#65292;&#36825;&#22312;&#20174;&#33258;&#28982;&#22270;&#20687;&#21040;&#20854;&#20182;&#19987;&#38376;&#39046;&#22495;&#65288;&#22914;&#21307;&#23398;&#22270;&#20687;&#65289;&#37319;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26102;&#36890;&#24120;&#34987;&#24573;&#35270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#35757;&#32451;&#38598;&#30340;&#22266;&#26377;&#32500;&#24230;($d_{data}$)&#19982;&#32593;&#32476;&#30340;&#27867;&#21270;&#38169;&#35823;&#19968;&#33324;&#20250;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#65288;&#25918;&#23556;&#23398;&#65289;&#21644;&#33258;&#28982;&#22270;&#20687;&#39046;&#22495;&#20043;&#38388;&#30340;&#36825;&#31181;&#20851;&#31995;&#30340;&#38497;&#23789;&#31243;&#24230;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#19988;&#26080;&#29616;&#26377;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#24314;&#31435;&#24182;&#32463;&#39564;&#35777;&#19968;&#20010;&#19982;$d_{data}$&#30456;&#20851;&#30340;&#27867;&#21270;&#32553;&#25918;&#23450;&#24459;&#26469;&#35299;&#20915;&#36825;&#20010;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#32771;&#34385;&#21040;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#26356;&#39640;&#30340;&#22266;&#26377;&#8220;&#26631;&#31614;&#38160;&#24230;&#8221;($K_F$)&#36825;&#19968;&#24230;&#37327;&#25351;&#26631;&#21487;&#20197;&#37096;&#20998;&#35299;&#37322;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#26174;&#33879;&#32553;&#25918;&#24046;&#24322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#27979;&#37327;&#36825;&#19968;&#25351;&#26631;&#21487;&#20197;&#25552;&#20379;&#30340;&#39069;&#22806;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension ($d_{data}$) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to $d_{data}$, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic "label sharpness" ($K_F$) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#35774;&#22791;&#30340;&#35774;&#22791;&#26412;&#22320;&#22522;&#30784;&#27169;&#22411;&#32852;&#37030;&#24494;&#35843;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#24322;&#26500;&#20302;&#31209;&#36817;&#20284;&#65288;LoRA&#65289;&#65292;&#35299;&#20915;&#20102;&#36164;&#28304;&#21463;&#38480;&#21644;&#24322;&#26500;&#35774;&#22791;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.06432</link><description>&lt;p&gt;
&#24322;&#26500;&#20302;&#31209;&#36817;&#20284;&#29992;&#20110;&#35774;&#22791;&#26412;&#22320;&#22522;&#30784;&#27169;&#22411;&#32852;&#37030;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Low-Rank Approximation for Federated Fine-tuning of On-Device Foundation Models. (arXiv:2401.06432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#35774;&#22791;&#30340;&#35774;&#22791;&#26412;&#22320;&#22522;&#30784;&#27169;&#22411;&#32852;&#37030;&#24494;&#35843;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#24322;&#26500;&#20302;&#31209;&#36817;&#20284;&#65288;LoRA&#65289;&#65292;&#35299;&#20915;&#20102;&#36164;&#28304;&#21463;&#38480;&#21644;&#24322;&#26500;&#35774;&#22791;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#36890;&#36807;&#24494;&#35843;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#25110;&#20219;&#21153;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36827;&#19968;&#27493;&#21033;&#29992;&#35774;&#22791;&#19978;&#30340;&#26412;&#22320;&#25968;&#25454;&#23454;&#29616;&#20102;&#31169;&#26377;&#21270;&#30340;FM&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;FMs&#30340;&#22823;&#23610;&#23544;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#21644;&#24322;&#26500;&#35774;&#22791;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21442;&#25968;&#23610;&#23544;&#36739;&#23567;&#30340;FM&#65292;&#31216;&#20026;&#35774;&#22791;&#26412;&#22320;FM&#65288;ODFMs&#65289;&#12290;&#34429;&#28982;ODFMs&#20801;&#35768;&#35774;&#22791;&#19978;&#30340;&#25512;&#26029;&#65292;&#20294;&#35745;&#31639;&#38480;&#21046;&#20173;&#28982;&#38459;&#30861;&#20102;&#39640;&#25928;&#30340;&#32852;&#37030;&#24494;&#35843;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;ODFM&#32852;&#37030;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#24322;&#26500;&#20302;&#31209;&#36817;&#20284;&#65288;LoRA&#65289;&#65292;&#35299;&#20915;&#20102;&#31995;&#32479;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21516;&#36136;LoRA&#31209;&#38754;&#20020;&#30528;&#36807;&#25311;&#21512;&#21644;&#25910;&#25947;&#32531;&#24930;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25552;&#20986;&#20102;HetLoRA&#65292;&#23427;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20351;&#29992;&#24322;&#36136;&#30340;&#31209;&#24182;&#28040;&#38500;&#20102;&#21516;&#36136;HetLoRA&#30340;&#32570;&#28857;&#12290;&#36890;&#36807;&#22312;&#26412;&#22320;&#24212;&#29992;&#31209;&#33258;&#21098;&#26525;&#65292;&#24182;&#22312;&#26381;&#21153;&#22120;&#19978;&#24212;&#29992;&#31232;&#30095;&#21152;&#26435;&#32858;&#21512;&#65292;&#25105;&#20204;&#23436;&#25104;&#20102;&#25688;&#35201;&#20013;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models (FMs) adapt surprisingly well to specific domains or tasks with fine-tuning. Federated learning (FL) further enables private FM fine-tuning using the local data on devices. However, the standard FMs' large size poses challenges for resource-constrained and heterogeneous devices. To address this, we consider FMs with reduced parameter sizes, referred to as on-device FMs (ODFMs). While ODFMs allow on-device inference, computational constraints still hinder efficient federated fine-tuning. We propose a parameter-efficient federated fine-tuning method for ODFMs using heterogeneous low-rank approximations (LoRAs) that addresses system and data heterogeneity. We show that homogeneous LoRA ranks face a trade-off between overfitting and slow convergence, and propose HetLoRA, which employs heterogeneous ranks across clients and eliminates the shortcomings of homogeneous HetLoRA. By applying rank self-pruning locally and sparsity-weighted aggregation at the server, we com
&lt;/p&gt;</description></item><item><title>GNNShap&#26159;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;GNNShap&#36890;&#36807;&#25277;&#26679;&#12289;&#24182;&#34892;&#21270;&#35745;&#31639;&#31561;&#25216;&#26415;&#25552;&#39640;&#20102;&#35299;&#37322;&#36895;&#24230;&#21644;&#31934;&#32454;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.04829</link><description>&lt;p&gt;
GNNShap: &#20351;&#29992;Shapley&#20540;&#24555;&#36895;&#32780;&#20934;&#30830;&#35299;&#37322;GNN&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
GNNShap: Fast and Accurate GNN Explanations using Shapley Values. (arXiv:2401.04829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04829
&lt;/p&gt;
&lt;p&gt;
GNNShap&#26159;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#33021;&#22815;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;GNNShap&#36890;&#36807;&#25277;&#26679;&#12289;&#24182;&#34892;&#21270;&#35745;&#31639;&#31561;&#25216;&#26415;&#25552;&#39640;&#20102;&#35299;&#37322;&#36895;&#24230;&#21644;&#31934;&#32454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#26159;&#19968;&#31181;&#22312;&#31185;&#23398;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;GNN&#34987;&#35748;&#20026;&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#24456;&#38590;&#29702;&#35299;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#39044;&#27979;&#12290;&#22522;&#20110;&#21338;&#24328;&#35770;&#30340;Shapley&#20540;&#26041;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#65292;&#20294;&#22312;&#22270;&#39046;&#22495;&#20013;&#30740;&#31350;&#36739;&#23569;&#12290;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;Shapley&#20540;&#30340;GNN&#35299;&#37322;&#26041;&#27861;&#65292;&#28982;&#32780;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#23427;&#20204;&#21482;&#32771;&#34385;&#20102;&#26377;&#38480;&#30340;&#26679;&#26412;&#26469;&#36817;&#20284;Shapley&#20540;&#65307;&#26377;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23567;&#21644;&#22823;&#30340;&#32852;&#30431;&#22823;&#23567;&#65292;&#24182;&#19988;&#23427;&#20204;&#27604;&#20854;&#20182;&#35299;&#37322;&#26041;&#27861;&#24930;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#20013;&#31561;&#35268;&#27169;&#30340;&#22270;&#20013;&#26080;&#27861;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNNShap&#65292;&#23427;&#25552;&#20379;&#36793;&#30340;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#22270;&#25552;&#20379;&#20102;&#26356;&#33258;&#28982;&#21644;&#31934;&#32454;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#25152;&#26377;&#32852;&#30431;&#22823;&#23567;&#36827;&#34892;&#25277;&#26679;&#12289;&#22312;GPU&#19978;&#24182;&#34892;&#25277;&#26679;&#21644;&#21152;&#36895;&#27169;&#22411;&#31561;&#26041;&#38754;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are popular machine learning models for graphs with many applications across scientific domains. However, GNNs are considered black box models, and it is challenging to understand how the model makes predictions. Game theory-based Shapley value approaches are popular explanation methods in other domains but are not well-studied for graphs. Some studies have proposed Shapley value-based GNN explanations, yet they have several limitations: they consider limited samples to approximate Shapley values; some mainly focus on small and large coalition sizes, and they are an order of magnitude slower than other explanation methods, making them inapplicable to even moderate-size graphs. In this work, we propose GNNShap, which provides explanations for edges since they provide more natural explanations for graphs and more fine-grained explanations. We overcome the limitations by sampling from all coalition sizes, parallelizing the sampling on GPUs, and speeding up mod
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#22270;&#21644;&#24207;&#21015;&#34920;&#31034;&#30340;&#21313;&#31181;&#28431;&#27934;&#26816;&#27979;&#22120;&#35299;&#37322;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#21333;&#32431;&#30340;&#24544;&#35802;&#24230;&#35780;&#20272;&#19981;&#36275;&#22815;&#12290;</title><link>http://arxiv.org/abs/2401.02686</link><description>&lt;p&gt;
&#36229;&#36234;&#24544;&#35802;&#24230;&#65306;&#35299;&#37322;&#22522;&#20110;&#23398;&#20064;&#30340;&#28431;&#27934;&#26816;&#27979;&#22120;&#30340;&#28431;&#27934;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Beyond Fidelity: Explaining Vulnerability Localization of Learning-based Detectors. (arXiv:2401.02686v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02686
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#22270;&#21644;&#24207;&#21015;&#34920;&#31034;&#30340;&#21313;&#31181;&#28431;&#27934;&#26816;&#27979;&#22120;&#35299;&#37322;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#21333;&#32431;&#30340;&#24544;&#35802;&#24230;&#35780;&#20272;&#19981;&#36275;&#22815;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#28431;&#27934;&#26816;&#27979;&#22120;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#20915;&#31574;&#36807;&#31243;&#30340;&#19981;&#36879;&#26126;&#24615;&#20351;&#23433;&#20840;&#20998;&#26512;&#24072;&#38590;&#20197;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#65292;&#36890;&#36807;&#31361;&#20986;&#37325;&#35201;&#29305;&#24449;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20854;&#20182;&#39046;&#22495;&#24050;&#32463;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#35299;&#37322;&#26041;&#27861;&#23545;&#20110;&#28431;&#27934;&#26816;&#27979;&#22120;&#23398;&#20064;&#21644;&#29702;&#35299;&#30340;&#32454;&#31890;&#24230;&#30340;&#28431;&#27934;&#30456;&#20851;&#20195;&#30721;&#34892;&#31561;&#20851;&#38190;&#29305;&#24449;&#30340;&#28145;&#20837;&#35780;&#20272;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#23450;&#37327;&#25351;&#26631;&#8212;&#8212;&#24544;&#35802;&#24230;&#21644;&#28431;&#27934;&#34892;&#35206;&#30422;&#29575;&#8212;&#8212;&#35780;&#20272;&#20102;&#22522;&#20110;&#22270;&#21644;&#24207;&#21015;&#34920;&#31034;&#30340;&#21313;&#31181;&#28431;&#27934;&#26816;&#27979;&#22120;&#35299;&#37322;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20165;&#20165;&#20381;&#38752;&#24544;&#35802;&#24230;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vulnerability detectors based on deep learning (DL) models have proven their effectiveness in recent years. However, the shroud of opacity surrounding the decision-making process of these detectors makes it difficult for security analysts to comprehend. To address this, various explanation approaches have been proposed to explain the predictions by highlighting important features, which have been demonstrated effective in other domains such as computer vision and natural language processing. Unfortunately, an in-depth evaluation of vulnerability-critical features, such as fine-grained vulnerability-related code lines, learned and understood by these explanation approaches remains lacking. In this study, we first evaluate the performance of ten explanation approaches for vulnerability detectors based on graph and sequence representations, measured by two quantitative metrics including fidelity and vulnerability line coverage rate. Our results show that fidelity alone is not sufficient f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2311.01200</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Continual Learning Under Language Shift. (arXiv:2311.01200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25345;&#32493;&#23398;&#20064;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21069;&#21521;&#36716;&#31227;&#25928;&#26524;&#36739;&#22909;&#19988;&#19982;&#35821;&#35328;&#39034;&#24207;&#26080;&#20851;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#21487;&#33021;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#24040;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;&#22312;&#38543;&#26102;&#38388;&#25512;&#31227;&#32780;&#20986;&#29616;&#26032;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26356;&#26032;&#27169;&#22411;&#32780;&#19981;&#26159;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#25910;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26032;&#35821;&#35328;&#20986;&#29616;&#26102;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#26102;&#30340;&#22909;&#22788;&#21644;&#24330;&#31471;&#65292;&#21363;&#22312;&#35821;&#35328;&#36716;&#25442;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#20174;&#21333;&#35821;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#20986;&#21457;&#65292;&#25105;&#20204;&#36880;&#27493;&#28155;&#21152;&#20102;&#26469;&#33258;&#25386;&#23041;&#35821;&#21644;&#20912;&#23707;&#35821;&#30340;&#25968;&#25454;&#65292;&#20197;&#30740;&#31350;&#21069;&#21521;&#21644;&#21518;&#21521;&#36716;&#31227;&#25928;&#26524;&#22914;&#20309;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#39034;&#24207;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#23398;&#20064;&#29575;&#35843;&#24230;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#21069;&#21521;&#36716;&#31227;&#20027;&#35201;&#26159;&#27491;&#21521;&#30340;&#65292;&#19981;&#21463;&#35821;&#35328;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20294;&#21518;&#21521;&#36716;&#31227;&#21017;&#21487;&#33021;&#26159;&#27491;&#21521;&#30340;&#25110;&#36127;&#21521;&#30340;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#26032;&#35821;&#35328;&#30340;&#39034;&#24207;&#21644;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#37322;&#36825;&#20123;&#27169;&#24335;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20960;&#31181;&#35821;&#35328;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent increase in data and model scale for language model pre-training has led to huge training costs. In scenarios where new data become available over time, updating a model instead of fully retraining it would therefore provide significant gains. In this paper, we study the benefits and downsides of updating a language model when new data comes from new languages - the case of continual learning under language shift. Starting from a monolingual English language model, we incrementally add data from Norwegian and Icelandic to investigate how forward and backward transfer effects depend on the pre-training order and characteristics of languages, for different model sizes and learning rate schedulers. Our results show that, while forward transfer is largely positive and independent of language order, backward transfer can be either positive or negative depending on the order and characteristics of new languages. To explain these patterns we explore several language similarity metr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#29109;&#36825;&#19968;&#26032;&#39062;&#30340;&#31163;&#25955;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#24357;&#34917;&#20102;&#31163;&#25955;&#25968;&#25454;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#24471;&#20998;&#29109;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;(SEDD)&#24182;&#22312;GPT-2&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.16834</link><description>&lt;p&gt;
&#36890;&#36807;&#20272;&#35745;&#25968;&#25454;&#20998;&#24067;&#27604;&#20363;&#30340;&#31163;&#25955;&#25193;&#25955;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Discrete Diffusion Language Modeling by Estimating the Ratios of the Data Distribution. (arXiv:2310.16834v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#24471;&#20998;&#29109;&#36825;&#19968;&#26032;&#39062;&#30340;&#31163;&#25955;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#24357;&#34917;&#20102;&#31163;&#25955;&#25968;&#25454;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#25552;&#20986;&#20102;&#24471;&#20998;&#29109;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;(SEDD)&#24182;&#22312;GPT-2&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#35768;&#22810;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#20855;&#26377;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#31561;&#31163;&#25955;&#25968;&#25454;&#39046;&#22495;&#20013;&#21364;&#34920;&#29616;&#19981;&#20339;&#12290;&#20851;&#38190;&#26159;&#65292;&#26631;&#20934;&#30340;&#25193;&#25955;&#27169;&#22411;&#20381;&#36182;&#20110;&#25104;&#29087;&#30340;&#24471;&#20998;&#21305;&#37197;&#29702;&#35770;&#65292;&#20294;&#26159;&#23558;&#20854;&#25512;&#24191;&#21040;&#31163;&#25955;&#32467;&#26500;&#24182;&#27809;&#26377;&#21462;&#24471;&#30456;&#21516;&#30340;&#32463;&#39564;&#25910;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#24471;&#20998;&#29109;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#26469;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#23427;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#31283;&#23450;&#65292;&#21487;&#20197;&#24418;&#25104;&#26368;&#22823;&#20284;&#28982;&#35757;&#32451;&#30340;ELBO&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#21435;&#22122;&#21464;&#20307;&#39640;&#25928;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#24471;&#20998;&#29109;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#65288;SEDD&#65289;&#25193;&#23637;&#21040;GPT-2&#30340;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;&#23454;&#29616;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#20284;&#28982;&#24230;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#31639;&#27861;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#27604;&#36739;&#22823;&#23567;&#30456;&#20284;&#30340;SEDD&#21644;GPT-2&#27169;&#22411;&#26102;&#65292;SEDD&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#22256;&#24785;&#24230;&#65288;&#36890;&#24120;&#22312;&#22522;&#32447;&#30340;+$10\%$&#20869;&#65292;&#24182;&#19988;&#26377;&#26102;&#36229;&#36807;&#22522;&#32447;&#65289;&#12290;&#27492;&#22806;&#65292;SEDD&#27169;&#22411;&#23398;&#21040;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel discrete score matching loss that is more stable than existing methods, forms an ELBO for maximum likelihood training, and can be efficiently optimized with a denoising variant. We scale our Score Entropy Discrete Diffusion models (SEDD) to the experimental setting of GPT-2, achieving highly competitive likelihoods while also introducing distinct algorithmic advantages. In particular, when comparing similarly sized SEDD and GPT-2 models, SEDD attains comparable perplexities (normally within $+10\%$ of and sometimes outperforming the baseline). Furthermore, SEDD models lear
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;MDP&#20013;&#23398;&#20064;&#969;-regular&#30446;&#26631;&#65292;&#19981;&#38656;&#35201;&#31995;&#32479;&#25299;&#25169;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.12248</link><description>&lt;p&gt;
MDP&#20013;LTL&#21644;&#969;-regular&#30446;&#26631;&#30340;PAC&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs. (arXiv:2310.12248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;PAC&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;MDP&#20013;&#23398;&#20064;&#969;-regular&#30446;&#26631;&#65292;&#19981;&#38656;&#35201;&#31995;&#32479;&#25299;&#25169;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#24207;&#36923;&#36753;&#65288;LTL&#65289;&#21644;&#969;-regular&#30446;&#26631;&#26159;&#36817;&#26399;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#36798;&#38750;&#39532;&#23572;&#21487;&#22827;&#30446;&#26631;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;MDP&#20013;&#30340;&#969;-regular&#30446;&#26631;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20174;&#31995;&#32479;&#30340;&#37319;&#26679;&#36712;&#36857;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#31995;&#32479;&#25299;&#25169;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear temporal logic (LTL) and omega-regular objectives -- a superset of LTL -- have seen recent use as a way to express non-Markovian objectives in reinforcement learning. We introduce a model-based probably approximately correct (PAC) learning algorithm for omega-regular objectives in Markov decision processes. Unlike prior approaches, our algorithm learns from sampled trajectories of the system and does not require prior knowledge of the system's topology.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#39046;&#22495;&#24212;&#29992;&#19981;&#21516;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#26694;&#26550;&#65292;&#24182;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#27604;&#36739;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26426;&#21046;&#30340;&#26131;&#38598;&#25104;&#24615;&#21644;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#20854;&#25104;&#20026;BCI&#20013;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11198</link><description>&lt;p&gt;
EEG&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#65306;&#19968;&#31181;&#19982;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#30456;&#27604;&#36739;&#30340;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EEG motor imagery decoding: A framework for comparative analysis with channel attention mechanisms. (arXiv:2310.11198v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#39046;&#22495;&#24212;&#29992;&#19981;&#21516;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#26694;&#26550;&#65292;&#24182;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#27604;&#36739;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26426;&#21046;&#30340;&#26131;&#38598;&#25104;&#24615;&#21644;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#20854;&#25104;&#20026;BCI&#20013;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25506;&#35752;&#22312;&#22823;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#24212;&#29992;&#21508;&#31181;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#20110;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#12290;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#21487;&#20197;&#35270;&#20026;&#20256;&#32479;&#29992;&#20110;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#30340;&#31354;&#38388;&#28388;&#27874;&#22120;&#30340;&#24378;&#22823;&#28436;&#36827;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#36825;&#20123;&#26426;&#21046;&#25972;&#21512;&#21040;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#26694;&#26550;&#20013;&#65292;&#31995;&#32479;&#22320;&#27604;&#36739;&#23427;&#20204;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36731;&#37327;&#32423;&#30340;&#22522;&#20934;&#26550;&#26500;&#65292;&#26088;&#22312;&#26080;&#32541;&#38598;&#25104;&#19981;&#21516;&#30340;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#21482;&#30740;&#31350;&#19968;&#20010;&#20851;&#27880;&#26426;&#21046;&#65292;&#24182;&#19988;&#36890;&#24120;&#26500;&#24314;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#12289;&#26377;&#26102;&#23884;&#22871;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#30456;&#21516;&#24773;&#20917;&#19979;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#20851;&#27880;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;&#26131;&#20110;&#38598;&#25104;&#19981;&#21516;&#30340;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#20197;&#21450;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#30740;&#31350;&#21644;&#25512;&#36827;BCI&#20013;&#30340;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this study is to investigate the application of various channel attention mechanisms within the domain of brain-computer interface (BCI) for motor imagery decoding. Channel attention mechanisms can be seen as a powerful evolution of spatial filters traditionally used for motor imagery decoding. This study systematically compares such mechanisms by integrating them into a lightweight architecture framework to evaluate their impact. We carefully construct a straightforward and lightweight baseline architecture designed to seamlessly integrate different channel attention mechanisms. This approach is contrary to previous works which only investigate one attention mechanism and usually build a very complex, sometimes nested architecture. Our framework allows us to evaluate and compare the impact of different attention mechanisms under the same circumstances. The easy integration of different channel attention mechanisms as well as the low computational complexity enables us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#23548;&#20102;&#28041;&#21450;&#20219;&#24847;&#20984;&#27604;&#36739;&#20989;&#25968;&#30340;&#36890;&#29992;&#20449;&#24687;&#29702;&#35770;&#21644;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#26368;&#32039;&#30028;&#38480;&#26159;&#30001;&#20984;&#20849;&#36717;&#30340;&#32047;&#31215;&#29983;&#25104;&#20989;&#25968;(CGF)&#26500;&#25104;&#30340;&#65292;&#20351;&#24471;&#36825;&#20123;&#30028;&#38480;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#32467;&#26500;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.10534</link><description>&lt;p&gt;
&#23545;&#27604;&#20998;&#31867;&#22120;&#22312;&#27867;&#21270;&#30028;&#38480;&#20013;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparing Comparators in Generalization Bounds. (arXiv:2310.10534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#23548;&#20102;&#28041;&#21450;&#20219;&#24847;&#20984;&#27604;&#36739;&#20989;&#25968;&#30340;&#36890;&#29992;&#20449;&#24687;&#29702;&#35770;&#21644;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#26368;&#32039;&#30028;&#38480;&#26159;&#30001;&#20984;&#20849;&#36717;&#30340;&#32047;&#31215;&#29983;&#25104;&#20989;&#25968;(CGF)&#26500;&#25104;&#30340;&#65292;&#20351;&#24471;&#36825;&#20123;&#30028;&#38480;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#32467;&#26500;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#23548;&#20102;&#28041;&#21450;&#20219;&#24847;&#20984;&#27604;&#36739;&#20989;&#25968;&#30340;&#36890;&#29992;&#20449;&#24687;&#29702;&#35770;&#21644;PAC-Bayesian&#27867;&#21270;&#30028;&#38480;&#65292;&#35813;&#20989;&#25968;&#27979;&#37327;&#35757;&#32451;&#35823;&#24046;&#21644;&#26679;&#26412;&#35823;&#24046;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#35813;&#30028;&#38480;&#22312;&#27604;&#36739;&#20989;&#25968;&#30340;&#32047;&#31215;&#29983;&#25104;&#20989;&#25968;(CG), &#34987;&#30028;&#23450;&#22312;&#19968;&#26063;&#38480;&#21046;&#20998;&#24067;&#20989;&#25968;&#30340;CGF&#19978;&#38480;&#30340;&#20551;&#35774;&#19979;&#25104;&#31435;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#27604;&#36739;&#20989;&#25968;&#26159;CGF&#30340;&#20984;&#20849;&#36717;&#65292;&#20063;&#34987;&#31216;&#20026;Cram\'er&#20989;&#25968;&#26102;&#65292;&#24471;&#21040;&#30340;&#30028;&#38480;&#26159;&#26368;&#32039;&#30340;&#12290;&#36825;&#20010;&#32467;&#35770;&#26356;&#24191;&#27867;&#22320;&#36866;&#29992;&#20110;&#20855;&#26377;&#31867;&#20284;&#32467;&#26500;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36825;&#35777;&#23454;&#20102;&#24050;&#30693;&#30028;&#38480;&#22312;&#26377;&#30028;&#21644;&#27425;&#39640;&#26031;&#25439;&#22833;&#24773;&#20917;&#19979;&#30340;&#36817;&#26368;&#20248;&#24615;&#65292;&#24182;&#19988;&#22312;&#20854;&#20182;&#38480;&#21046;&#20998;&#24067;&#19979;&#24471;&#21040;&#20102;&#26032;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We derive generic information-theoretic and PAC-Bayesian generalization bounds involving an arbitrary convex comparator function, which measures the discrepancy between the training and population loss. The bounds hold under the assumption that the cumulant-generating function (CGF) of the comparator is upper-bounded by the corresponding CGF within a family of bounding distributions. We show that the tightest possible bound is obtained with the comparator being the convex conjugate of the CGF of the bounding distribution, also known as the Cram\'er function. This conclusion applies more broadly to generalization bounds with a similar structure. This confirms the near-optimality of known bounds for bounded and sub-Gaussian losses and leads to novel bounds under other bounding distributions.
&lt;/p&gt;</description></item><item><title>QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08041</link><description>&lt;p&gt;
QLLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#39640;&#25928;&#20302;&#20301;&#23485;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08041
&lt;/p&gt;
&lt;p&gt;
QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#25152;&#38656;&#36164;&#28304;&#36807;&#22823;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;Quantization-Aware Training&#65292;QAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;Post-Training Quantization&#65292;PTQ&#65289;&#25104;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26356;&#23454;&#38469;&#30340;&#26041;&#27861;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#29305;&#23450;&#36890;&#36947;&#20013;&#30340;&#28608;&#27963;&#31163;&#32676;&#20540;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#21518;&#35757;&#32451;&#37327;&#21270;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QLLM&#65292;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;QLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#36890;&#36947;&#25286;&#20998;&#21644;&#36890;&#36947;&#32452;&#35013;&#65292;&#22312;&#20445;&#35777;&#20302;&#20301;&#23485;&#30340;&#24773;&#20917;&#19979;&#23558;&#31163;&#32676;&#36890;&#36947;&#20998;&#35299;&#25104;&#22810;&#20010;&#23376;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#39640;&#32500;&#31354;&#38388;&#20013;&#20449;&#24687;&#25658;&#24102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07972</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#20449;&#24687;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Interpretable Diffusion via Information Decomposition. (arXiv:2310.07972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#39640;&#32500;&#31354;&#38388;&#20013;&#20449;&#24687;&#25658;&#24102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29992;&#20110;&#22797;&#26434;&#20851;&#31995;&#30340;&#26465;&#20214;&#29983;&#25104;&#21644;&#23494;&#24230;&#24314;&#27169;&#65292;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#20851;&#31995;&#30340;&#26412;&#36136;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#20934;&#30830;&#29702;&#35299;&#21333;&#35789;&#21644;&#22270;&#20687;&#37096;&#20998;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25110;&#32773;&#39044;&#27979;&#24178;&#39044;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#31934;&#30830;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#12290;&#20114;&#20449;&#24687;&#21644;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#31934;&#30830;&#34920;&#36798;&#21487;&#20197;&#36890;&#36807;&#21435;&#22122;&#27169;&#22411;&#26469;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#20063;&#21487;&#20197;&#36731;&#26494;&#20272;&#35745;&#22312;&#29305;&#23450;&#22270;&#20687;&#21644;&#26631;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36827;&#19968;&#27493;&#23545;&#20449;&#24687;&#36827;&#34892;&#20998;&#35299;&#65292;&#20197;&#29702;&#35299;&#39640;&#32500;&#31354;&#38388;&#20013;&#21738;&#20123;&#21464;&#37327;&#25658;&#24102;&#20449;&#24687;&#65292;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#38750;&#36127;&#20449;&#24687;&#20998;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutu
&lt;/p&gt;</description></item><item><title>&#20851;&#31995;&#21367;&#31215;&#32593;&#32476;&#26159;&#19968;&#20010;&#23398;&#20064;&#26174;&#24335;&#23618;&#27425;&#20851;&#31995;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#20869;&#31215;&#20851;&#31995;&#27169;&#22359;&#21644;&#20851;&#31995;&#21367;&#31215;&#23618;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#20803;&#28388;&#27874;&#22120;&#30340;&#32676;&#32452;&#27604;&#36739;&#65292;&#33021;&#22815;&#34920;&#36798;&#26356;&#39640;&#38454;&#12289;&#23618;&#27425;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.03240</link><description>&lt;p&gt;
&#20851;&#31995;&#21367;&#31215;&#32593;&#32476;&#65306;&#23398;&#20064;&#23618;&#27425;&#20851;&#31995;&#34920;&#31034;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Relational Convolutional Networks: A framework for learning representations of hierarchical relations. (arXiv:2310.03240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03240
&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#21367;&#31215;&#32593;&#32476;&#26159;&#19968;&#20010;&#23398;&#20064;&#26174;&#24335;&#23618;&#27425;&#20851;&#31995;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#20869;&#31215;&#20851;&#31995;&#27169;&#22359;&#21644;&#20851;&#31995;&#21367;&#31215;&#23618;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#20803;&#28388;&#27874;&#22120;&#30340;&#32676;&#32452;&#27604;&#36739;&#65292;&#33021;&#22815;&#34920;&#36798;&#26356;&#39640;&#38454;&#12289;&#23618;&#27425;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#20010;&#25104;&#29087;&#30340;&#30740;&#31350;&#39046;&#22495;&#26159;&#24320;&#21457;&#33021;&#22815;&#23398;&#20064;&#26174;&#24335;&#20851;&#31995;&#29305;&#24449;&#34920;&#31034;&#30340;&#26550;&#26500;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#23398;&#20064;&#23618;&#27425;&#20851;&#31995;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20851;&#31995;&#21367;&#31215;&#32593;&#32476;&#8221;&#30340;&#26550;&#26500;&#26694;&#26550;&#12290;&#32473;&#23450;&#19968;&#31995;&#21015;&#23545;&#35937;&#65292;&#19968;&#20010;&#8220;&#22810;&#32500;&#20869;&#31215;&#20851;&#31995;&#8221;&#27169;&#22359;&#29983;&#25104;&#19968;&#20010;&#25551;&#36848;&#25152;&#26377;&#25104;&#23545;&#20851;&#31995;&#30340;&#20851;&#31995;&#24352;&#37327;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#8220;&#20851;&#31995;&#21367;&#31215;&#8221;&#23618;&#23558;&#20851;&#31995;&#24352;&#37327;&#36716;&#21270;&#20026;&#19968;&#20010;&#26032;&#23545;&#35937;&#24207;&#21015;&#65292;&#27599;&#20010;&#23545;&#35937;&#25551;&#36848;&#21069;&#19968;&#23618;&#26576;&#32676;&#23545;&#35937;&#20869;&#30340;&#20851;&#31995;&#12290;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28388;&#27874;&#22120;&#65292;&#22270;&#20803;&#28388;&#27874;&#22120;&#20195;&#34920;&#35201;&#19982;&#20851;&#31995;&#24352;&#37327;&#22312;&#27599;&#20010;&#20998;&#32452;&#20013;&#36827;&#34892;&#27604;&#36739;&#30340;&#20851;&#31995;&#27169;&#26495;&#12290;&#36890;&#36807;&#37325;&#22797;&#36825;&#20010;&#36807;&#31243;&#65292;&#24471;&#21040;&#26356;&#39640;&#38454;&#12289;&#23618;&#27425;&#30340;&#20851;&#31995;&#34920;&#31034;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26550;&#26500;&#30340;&#21160;&#26426;&#21644;&#32454;&#33410;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#35777;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
A maturing area of research in deep learning is the development of architectures that can learn explicit representations of relational features. In this paper, we focus on the problem of learning representations of hierarchical relations, proposing an architectural framework we call "relational convolutional networks". Given a sequence of objects, a "multi-dimensional inner product relation" module produces a relation tensor describing all pairwise relations. A "relational convolution" layer then transforms the relation tensor into a sequence of new objects, each describing the relations within some group of objects at the previous layer. Graphlet filters, analogous to filters in convolutional neural networks, represent a template of relations against which the relation tensor is compared at each grouping. Repeating this yields representations of higher-order, hierarchical relations. We present the motivation and details of the architecture, together with a set of experiments to demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36127;&#36317;&#31163;&#26680;&#30340;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;(MMD)&#30340;&#26465;&#20214;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#21518;&#39564;&#25277;&#26679;&#21644;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#12290;&#36890;&#36807;&#31163;&#25955;&#30340;Wasserstein&#26799;&#24230;&#27969;&#36817;&#20284;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;&#31890;&#23376;&#27969;&#26159;&#36866;&#24403;&#21151;&#33021;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#36229;&#20998;&#36776;&#29575;&#31561;&#36870;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03054</link><description>&lt;p&gt;
&#22522;&#20110;&#36127;&#36317;&#31163;&#26680;&#30340;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;(MMD)&#26799;&#24230;&#27969;&#30340;&#21518;&#39564;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel. (arXiv:2310.03054v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36127;&#36317;&#31163;&#26680;&#30340;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;(MMD)&#30340;&#26465;&#20214;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#21518;&#39564;&#25277;&#26679;&#21644;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#12290;&#36890;&#36807;&#31163;&#25955;&#30340;Wasserstein&#26799;&#24230;&#27969;&#36817;&#20284;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;&#31890;&#23376;&#27969;&#26159;&#36866;&#24403;&#21151;&#33021;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#22312;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#36229;&#20998;&#36776;&#29575;&#31561;&#36870;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36127;&#36317;&#31163;&#26680;&#30340;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;(MMD)&#30340;&#26465;&#20214;&#27969;&#29992;&#20110;&#21518;&#39564;&#25277;&#26679;&#21644;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#12290;&#36825;&#20010;MMD&#65292;&#20063;&#34987;&#31216;&#20026;&#33021;&#37327;&#36317;&#31163;&#65292;&#20855;&#26377;&#20687;&#36890;&#36807;&#20999;&#29255;&#21644;&#25490;&#24207;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#30340;&#20960;&#20010;&#26377;&#30410;&#23646;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#30340;Wasserstein&#26799;&#24230;&#27969;&#26469;&#36817;&#20284;&#30495;&#23454;&#24773;&#20917;&#21644;&#35266;&#23519;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#24182;&#20026;&#21518;&#39564;&#20998;&#24067;&#24314;&#31435;&#20102;&#35823;&#24046;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31890;&#23376;&#27969;&#30830;&#23454;&#26159;&#36866;&#24403;&#21151;&#33021;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#33021;&#21147;&#36890;&#36807;&#25968;&#23383;&#31034;&#20363;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#21253;&#25324;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#21644;&#35832;&#22914;&#36229;&#20998;&#36776;&#29575;&#12289;&#20462;&#22797;&#21644;&#20302;&#21058;&#37327;&#21644;&#26377;&#38480;&#35282;&#24230;&#35774;&#32622;&#19979;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#31561;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose conditional flows of the maximum mean discrepancy (MMD) with the negative distance kernel for posterior sampling and conditional generative modeling. This MMD, which is also known as energy distance, has several advantageous properties like efficient computation via slicing and sorting. We approximate the joint distribution of the ground truth and the observations using discrete Wasserstein gradient flows and establish an error bound for the posterior distributions. Further, we prove that our particle flow is indeed a Wasserstein gradient flow of an appropriate functional. The power of our method is demonstrated by numerical examples including conditional image generation and inverse problems like superresolution, inpainting and computed tomography in low-dose and limited-angle settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#36890;&#36807;&#39640;&#32500;&#30697;&#38453;&#21644;&#23884;&#20837;&#30340;&#22806;&#31215;&#26469;&#27169;&#25311;&#20869;&#23618;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#19982;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#22823;&#23567;&#30456;&#20851;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02984</link><description>&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Associative Memories. (arXiv:2310.02984v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#36890;&#36807;&#39640;&#32500;&#30697;&#38453;&#21644;&#23884;&#20837;&#30340;&#22806;&#31215;&#26469;&#27169;&#25311;&#20869;&#23618;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#19982;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#22823;&#23567;&#30456;&#20851;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#24456;&#21487;&#33021;&#28041;&#21450;&#21040;&#25277;&#35937;&#35268;&#21017;&#30340;&#21457;&#29616;&#21644;&#35760;&#24518;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#32852;&#24819;&#35760;&#24518;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#39640;&#32500;&#30697;&#38453;&#65292;&#30001;&#23884;&#20837;&#30340;&#22806;&#31215;&#32452;&#25104;&#65292;&#19982;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23618;&#30456;&#20851;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20851;&#20110;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#35268;&#27169;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21253;&#25324;&#22522;&#20110;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#21644;&#35299;&#37322;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#23545;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01468</link><description>&lt;p&gt;
&#23454;&#20307;&#25512;&#26029;&#31454;&#25216;&#22330;&#65306;&#25506;&#31350;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#26126;&#30830;&#25552;&#38382;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#21547;&#31946;&#19981;&#28165;&#30340;&#26597;&#35810;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#34892;&#20026;&#38590;&#20197;&#39044;&#27979;&#24182;&#20135;&#29983;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#26377;&#25928;&#35299;&#20915;&#27495;&#20041;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#33021;&#21147;&#38656;&#35201;&#23545;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#36827;&#34892;&#22797;&#26434;&#30340;&#29702;&#35299;&#12289;&#29366;&#24577;&#36319;&#36394;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#27979;&#37327;&#36825;&#31181;&#33021;&#21147;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#65292;&#35780;&#20272;&#20102;LLMs&#25512;&#26029;&#33258;&#24049;&#19981;&#30693;&#36947;&#20294;&#34987;&#27861;&#23448;&#25581;&#31034;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#8220;&#23454;&#20307;&#25512;&#26029;&#28216;&#25103;&#8221;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#22823;&#30340;LLMs...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
&lt;/p&gt;</description></item><item><title>TranDRL&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#25429;&#25417;&#21644;&#32463;&#27982;&#39640;&#25928;&#32500;&#25252;&#24314;&#35758;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16935</link><description>&lt;p&gt;
TranDRL&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework. (arXiv:2309.16935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16935
&lt;/p&gt;
&lt;p&gt;
TranDRL&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#25429;&#25417;&#21644;&#32463;&#27982;&#39640;&#25928;&#32500;&#25252;&#24314;&#35758;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#31995;&#32479;&#38656;&#35201;&#21487;&#38752;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#31574;&#30053;&#26469;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#24182;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#26469;&#20248;&#21270;&#32500;&#25252;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;Transformer&#27169;&#22411;&#26469;&#26377;&#25928;&#25429;&#25417;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#35774;&#22791;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;DRL&#32452;&#20214;&#25552;&#20379;&#20102;&#32463;&#27982;&#39640;&#25928;&#21644;&#21450;&#26102;&#30340;&#32500;&#25252;&#24314;&#35758;&#12290;&#25105;&#20204;&#22312;NASA C-MPASS&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;RUL&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24037;&#19994;&#36816;&#33829;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#24102;&#26469;&#20102;&#26356;&#22810;&#21457;&#23637;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial systems demand reliable predictive maintenance strategies to enhance operational efficiency and reduce downtime. This paper introduces a novel, integrated framework that leverages the power of transformer neural networks and deep reinforcement learning (DRL) algorithms to optimize maintenance actions. Our approach employs the transformer model to effectively capture complex temporal patterns in sensor data, thereby accurately predicting the Remaining Useful Life (RUL) of equipment. Simultaneously, the DRL component of our framework provides cost-effective and timely maintenance recommendations. We validate the efficacy of our framework on the NASA C-MPASS dataset, where it demonstrates significant advancements in both RUL prediction accuracy and the optimization of maintenance actions. Consequently, our pioneering approach provides an innovative data-driven methodology for prescriptive maintenance, addressing key challenges in industrial operations and leading the way to mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14681</link><description>&lt;p&gt;
&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26377;&#24517;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#33391;&#22909;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#26631;&#20934;&#33539;&#24335;&#20013;&#23384;&#22312;&#20197;&#19979;&#24330;&#31471;&#65306;&#26131;&#21463;&#36873;&#23450;&#28436;&#31034;&#30340;&#24433;&#21709;&#65292;&#29983;&#25104;&#36825;&#20123;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;ICL&#65292;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19981;&#20381;&#36182;&#20154;&#24037;&#28436;&#31034;&#30340;&#33539;&#20363;&#12290;SEC&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#65292;&#19981;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#20316;&#20026;ICL&#20013;&#30340;&#28436;&#31034;&#65292;&#32780;&#26159;&#35201;&#27714;LLMs&#39318;&#20808;&#33258;&#34892;&#21019;&#24314;&#28436;&#31034;&#65292;&#28982;&#21518;&#29983;&#25104;&#26368;&#32456;&#36755;&#20986;&#12290;SEC&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21487;&#36866;&#24212;&#21407;&#22987;ICL&#21644;&#8220;&#24605;&#32500;&#38142;&#8221;&#65288;CoT&#65289;&#65292;&#24182;&#19988;&#26356;&#21152;&#20415;&#25463;&#65306;&#22240;&#20026;&#21487;&#20197;&#33410;&#30465;&#31034;&#20363;&#21644;&#29702;&#30001;&#30340;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#31639;&#26415;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understandin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2309.10400</link><description>&lt;p&gt;
PoSE: &#36890;&#36807;&#20301;&#32622;&#36339;&#36291;&#24335;&#35757;&#32451;&#25552;&#39640;LLMs&#23545;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#26377;&#25928;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Positional Skip-wise (PoSE)&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20110;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;PoSE&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#27169;&#25311;&#38271;&#36755;&#20837;&#65292;&#23558;&#35757;&#32451;&#38271;&#24230;&#19982;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#20998;&#31163;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#38271;&#36755;&#20837;&#24207;&#21015;&#20013;&#36873;&#25321;&#33509;&#24178;&#30701;&#22359;&#65292;&#24182;&#24341;&#20837;&#19981;&#21516;&#30340;&#36339;&#36291;&#20559;&#32622;&#39033;&#26469;&#20462;&#25913;&#27599;&#20010;&#22359;&#30340;&#20301;&#32622;&#32034;&#24341;&#12290;&#36825;&#20123;&#36339;&#36291;&#20559;&#32622;&#39033;&#20197;&#21450;&#27599;&#20010;&#22359;&#30340;&#38271;&#24230;&#22312;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#37117;&#20250;&#21464;&#21270;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#25152;&#26377;&#20301;&#32622;&#65292;&#32780;&#26080;&#38656;&#23545;&#23436;&#25972;&#38271;&#24230;&#30340;&#36755;&#20837;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#23545;&#23436;&#25972;&#38271;&#24230;&#36827;&#34892;&#24494;&#35843;&#30456;&#27604;&#65292;PoSE&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;&#21033;&#29992;&#36825;&#19968;&#20248;&#21183;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65292;PoSE&#19982;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. Furthermore, we empirically confirm that PoSE is compatible with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24314;&#27169;&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#23545;&#24515;&#33039;&#20877;&#21516;&#27493;&#27835;&#30103;&#30340;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#33616;&#25910;&#38598;&#39069;&#22806;&#30340;SPECT MPI&#21464;&#37327;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08415</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;CRT&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A new method of modeling the multi-stage decision-making process of CRT using machine learning with uncertainty quantification. (arXiv:2309.08415v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24314;&#27169;&#30340;&#22810;&#38454;&#27573;&#20915;&#31574;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#23545;&#24515;&#33039;&#20877;&#21516;&#27493;&#27835;&#30103;&#30340;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#33616;&#25910;&#38598;&#39069;&#22806;&#30340;SPECT MPI&#21464;&#37327;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#24739;&#32773;&#24515;&#33039;&#20877;&#21516;&#27493;&#27835;&#30103;&#65288;CRT&#65289;&#30340;&#21453;&#24212;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#25512;&#33616;&#22312;&#22522;&#32447;&#20020;&#24202;&#21464;&#37327;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#30340;&#29305;&#24449;&#19981;&#36275;&#26102;&#25910;&#38598;&#39069;&#22806;&#30340;&#21333;&#20809;&#23376;&#21457;&#23556;&#35745;&#31639;&#26426;&#20307;&#23618;&#25668;&#24433;&#24515;&#32908;&#28748;&#27880;&#26174;&#20687;&#65288;SPECT MPI&#65289;&#21464;&#37327;&#12290;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#32435;&#20837;&#20102;218&#21517;&#25509;&#21463;&#38745;&#24687;&#38376;&#25511;SPECT MPI&#30340;&#24739;&#32773;&#12290;CRT&#21453;&#24212;&#34987;&#23450;&#20041;&#20026;6&#20010;&#26376;&#38543;&#35775;&#26102;&#24038;&#23460;&#23556;&#34880;&#20998;&#25968;&#65288;LVEF&#65289;&#22686;&#21152;&gt; 5%&#12290;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#38598;&#25104;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#32467;&#26524;&#12290;CRT&#30340;&#21453;&#24212;&#29575;&#20026;55.5%&#65288;n = 121&#65289;&#65292;&#25972;&#20307;&#30007;&#24615;&#21344;61.0%&#65288;n = 133&#65289;&#65292;&#24179;&#22343;&#24180;&#40836;62.0&#23681;&#65292;LVEF&#20026;27.7&#12290;&#35813;&#22810;&#38454;&#27573;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#38598;&#25104;&#27169;&#22411;2&#65288;&#21033;&#29992;&#20102;&#39069;&#22806;&#30340;SPECT&#25968;&#25454;&#65289;&#30456;&#20284;&#65292;AUC&#20998;&#21035;&#20026;0.75&#21644;0.77&#65292;&#20934;&#30830;&#24615;&#20998;&#21035;&#20026;0.71&#21644;...
&lt;/p&gt;
&lt;p&gt;
Aims. The purpose of this study is to create a multi-stage machine learning model to predict cardiac resynchronization therapy (CRT) response for heart failure (HF) patients. This model exploits uncertainty quantification to recommend additional collection of single-photon emission computed tomography myocardial perfusion imaging (SPECT MPI) variables if baseline clinical variables and features from electrocardiogram (ECG) are not sufficient. Methods. 218 patients who underwent rest-gated SPECT MPI were enrolled in this study. CRT response was defined as an increase in left ventricular ejection fraction (LVEF) &gt; 5% at a 6 month follow-up. A multi-stage ML model was created by combining two ensemble models. Results. The response rate for CRT was 55.5% (n = 121) with overall male gender 61.0% (n = 133), an average age of 62.0, and LVEF of 27.7. The multi-stage model performed similarly to Ensemble 2 (which utilized the additional SPECT data) with AUC of 0.75 vs. 0.77, accuracy of 0.71 vs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07808</link><description>&lt;p&gt;
&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#30340;&#20851;&#38190;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20840;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#65292;&#22312;&#36825;&#31181;&#25216;&#26415;&#20013;&#65292;&#25972;&#20010;&#39550;&#39542;&#27969;&#31243;&#34987;&#26367;&#25442;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#30001;&#20110;&#20854;&#32467;&#26500;&#31616;&#21333;&#21644;&#25512;&#29702;&#26102;&#38388;&#24555;&#65292;&#22240;&#27492;&#21464;&#24471;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#39550;&#39542;&#27969;&#31243;&#20013;&#30340;&#32452;&#20214;&#65292;&#20294;&#20854;&#31616;&#21333;&#24615;&#20063;&#23548;&#33268;&#35299;&#37322;&#24615;&#38382;&#39064;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#35757;&#32451;&#24471;&#21040;&#30340;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#20132;&#36890;&#35268;&#21017;&#65292;&#21516;&#26102;&#20063;&#24456;&#38590;&#21457;&#29616;&#20854;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#22240;&#20026;&#32570;&#20047;&#20013;&#38388;&#36755;&#20986;&#12290;&#21516;&#26102;&#65292;&#20256;&#24863;&#22120;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#34892;&#24615;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#24863;&#30693;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
More research attention has recently been given to end-to-end autonomous driving technologies where the entire driving pipeline is replaced with a single neural network because of its simpler structure and faster inference time. Despite this appealing approach largely reducing the components in driving pipeline, its simplicity also leads to interpretability problems and safety issues arXiv:2003.06404. The trained policy is not always compliant with the traffic rules and it is also hard to discover the reason for the misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are also critical to autonomous driving's security and feasibility to perceive the surrounding environment under complex driving scenarios. In this paper, we proposed P-CSG, a novel penalty-based imitation learning approach with cross semantics generation sensor fusion technologies to increase the overall performance of End-to-End Autonomous Driving. We conducted an assessment of our model's perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22495;&#22768;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#27700;&#19979;&#22768;&#38899;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#26469;&#20998;&#26512;&#27700;&#19979;&#22768;&#23398;&#25968;&#25454;&#12290;&#36890;&#36807;&#32858;&#31867;&#21644;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#20505;&#36873;&#26631;&#31614;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#31354;&#27668;&#26538;&#22768;&#30340;&#39640;&#25928;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.03451</link><description>&lt;p&gt;
&#36328;&#22495;&#22768;&#38899;&#35782;&#21035;&#29992;&#20110;&#39640;&#25928;&#30340;&#27700;&#19979;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Sound Recognition for Efficient Underwater Data Analysis. (arXiv:2309.03451v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#22495;&#22768;&#38899;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38750;&#27700;&#19979;&#22768;&#38899;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#26469;&#20998;&#26512;&#27700;&#19979;&#22768;&#23398;&#25968;&#25454;&#12290;&#36890;&#36807;&#32858;&#31867;&#21644;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#20505;&#36873;&#26631;&#31614;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#31354;&#27668;&#26538;&#22768;&#30340;&#39640;&#25928;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22312;&#24191;&#35889;&#38750;&#27700;&#19979;&#65288;&#31354;&#20013;&#65289;&#22768;&#38899;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#20998;&#26512;&#28023;&#37327;&#27700;&#19979;&#22768;&#23398;&#25968;&#25454;&#12290;&#37492;&#20110;&#26631;&#35760;&#22823;&#37327;&#27700;&#19979;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#26041;&#27861;&#26469;&#21152;&#36895;&#36825;&#19968;&#21171;&#21160;&#23494;&#38598;&#22411;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#31532;&#19968;&#37096;&#20998;&#28041;&#21450;&#20351;&#29992;&#31354;&#20013;&#22768;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#29305;&#24449;&#21521;&#37327;&#23545;&#27700;&#19979;&#25968;&#25454;&#36827;&#34892;PCA&#21644;UMAP&#21487;&#35270;&#21270;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20108;&#32500;&#31354;&#38388;&#20013;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#21548;&#21462;&#36825;&#20123;&#32858;&#31867;&#20013;&#30340;&#28857;&#20197;&#20102;&#35299;&#20854;&#23450;&#20041;&#29305;&#24615;&#12290;&#36825;&#31181;&#21019;&#26032;&#26041;&#27861;&#31616;&#21270;&#20102;&#36873;&#25321;&#20505;&#36873;&#26631;&#31614;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#36807;&#31243;&#12290;&#22312;&#31532;&#20108;&#37096;&#20998;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#36873;&#23450;&#30340;&#27700;&#19979;&#25968;&#25454;&#21644;&#38750;&#27700;&#19979;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#37327;&#20998;&#26512;&#65292;&#34913;&#37327;&#20102;&#25105;&#20204;&#27169;&#22411;&#35782;&#21035;&#31354;&#27668;&#26538;&#22768;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel deep learning approach for analyzing massive underwater acoustic data by leveraging a model trained on a broad spectrum of non-underwater (aerial) sounds. Recognizing the challenge in labeling vast amounts of underwater data, we propose a two-fold methodology to accelerate this labor-intensive procedure.  The first part of our approach involves PCA and UMAP visualization of the underwater data using the feature vectors of an aerial sound recognition model. This enables us to cluster the data in a two dimensional space and listen to points within these clusters to understand their defining characteristics. This innovative method simplifies the process of selecting candidate labels for further training.  In the second part, we train a neural network model using both the selected underwater data and the non-underwater dataset. We conducted a quantitative analysis to measure the precision, recall, and F1 score of our model for recognizing airgun sounds, a common
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#29616;&#35937;&#65292;&#21457;&#29616;&#22266;&#23450;&#19981;&#21464;&#30340;&#23376;&#31354;&#38388;&#23548;&#33268;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#12290;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#24179;&#28369;&#21521;&#37327;&#30340;&#23384;&#22312;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16800</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31561;&#32423;&#23849;&#22604;&#23548;&#33268;&#24179;&#28369;&#36807;&#24230;&#21644;&#20851;&#32852;&#36807;&#39640;
&lt;/p&gt;
&lt;p&gt;
Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks. (arXiv:2308.16800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#29616;&#35937;&#65292;&#21457;&#29616;&#22266;&#23450;&#19981;&#21464;&#30340;&#23376;&#31354;&#38388;&#23548;&#33268;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#12290;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#24179;&#28369;&#21521;&#37327;&#30340;&#23384;&#22312;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#30340;&#26032;&#29702;&#35770;&#35265;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22266;&#23450;&#19981;&#21464;&#23376;&#31354;&#38388;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23427;&#34920;&#29616;&#20986;&#19968;&#31181;&#30456;&#23545;&#30340;&#34892;&#20026;&#65292;&#19981;&#21463;&#29305;&#24449;&#36716;&#25442;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38416;&#26126;&#20102;&#19982;&#25910;&#25947;&#21040;&#24120;&#25968;&#29366;&#24577;&#21644;&#33410;&#28857;&#29366;&#24577;&#30340;&#36807;&#20998;&#20998;&#31163;&#30456;&#20851;&#30340;&#26368;&#26032;&#35266;&#23519;&#32467;&#26524;&#65292;&#22240;&#20026;&#23376;&#31354;&#38388;&#30340;&#25918;&#22823;&#21482;&#21462;&#20915;&#20110;&#32858;&#21512;&#20989;&#25968;&#30340;&#39057;&#35889;&#12290;&#22312;&#32447;&#24615;&#22330;&#26223;&#20013;&#65292;&#36825;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#30001;&#20302;&#32500;&#23376;&#31354;&#38388;&#20027;&#23548;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#29305;&#24449;&#36716;&#25442;&#26080;&#20851;&#30340;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#12290;&#24403;&#24179;&#28369;&#21521;&#37327;&#36328;&#36234;&#36825;&#20010;&#23376;&#31354;&#38388;&#26102;&#65292;&#36825;&#20250;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#22312;&#25105;&#20204;&#30340;&#29702;&#35770;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#30410;&#29305;&#24615;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#39640;&#20851;&#32852;&#21644;&#31561;&#32423;&#23849;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our study reveals new theoretical insights into over-smoothing and feature over-correlation in deep graph neural networks. We show the prevalence of invariant subspaces, demonstrating a fixed relative behavior that is unaffected by feature transformations. Our work clarifies recent observations related to convergence to a constant state and a potential over-separation of node states, as the amplification of subspaces only depends on the spectrum of the aggregation function. In linear scenarios, this leads to node representations being dominated by a low-dimensional subspace with an asymptotic convergence rate independent of the feature transformations. This causes a rank collapse of the node representations, resulting in over-smoothing when smooth vectors span this subspace, and over-correlation even when over-smoothing is avoided. Guided by our theory, we propose a sum of Kronecker products as a beneficial property that can provably prevent over-smoothing, over-correlation, and rank c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#24067;&#24335;POMDP&#24418;&#24335;&#65292;&#22312;&#28040;&#24687;&#36716;&#21457;&#19978;&#23454;&#29616;&#20102;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29420;&#31435;&#20915;&#31574;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#20449;&#24687;&#20132;&#25442;&#26041;&#24335;&#30340;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16198</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#24067;&#24335;POMDP&#24418;&#24335;&#65292;&#22312;&#28040;&#24687;&#36716;&#21457;&#19978;&#23454;&#29616;&#20102;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29420;&#31435;&#20915;&#31574;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#20449;&#24687;&#20132;&#25442;&#26041;&#24335;&#30340;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#39640;&#25928;&#21487;&#38752;&#30340;&#20449;&#24687;&#20256;&#25773;&#23545;&#25903;&#25345;&#20851;&#38190;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#28798;&#38590;&#21709;&#24212;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#20316;&#20026;&#23454;&#29616;&#26356;&#20026;&#20998;&#25955;&#12289;&#39640;&#25928;&#21644;&#21327;&#20316;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20449;&#24687;&#20256;&#25773;&#30340;&#20998;&#24067;&#24335;POMDP&#65288;Decentralized-POMDP&#65289;&#24418;&#24335;&#65292;&#20351;&#24471;&#27599;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#29420;&#31435;&#20915;&#23450;&#28040;&#24687;&#30340;&#36716;&#21457;&#12290;&#36825;&#26500;&#25104;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#65288;MPR&#65289;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#65292;&#37319;&#29992;&#20855;&#26377;&#21160;&#24577;&#27880;&#24847;&#21147;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#26469;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;L-DGN&#21644;HL-DGN&#65292;&#23427;&#20204;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#25442;&#30340;&#20449;&#24687;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#20998;&#25955;&#26041;&#27861;&#19982;&#22522;&#20110;MPR&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern communication systems, efficient and reliable information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative solutions. We propose a Decentralized-POMDP formulation for information dissemination, empowering each agent to independently decide on message forwarding. This constitutes a significant paradigm shift from traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention Networks (GAT) with dynamic attention to capture essential network features. We propose two approaches, L-DGN and HL-DGN, which differ in the information that is exchanged among agents. We evaluate the performance of our decentralized approaches, by compari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20999;&#21521;&#29305;&#24449;&#35270;&#35282;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#21464;&#25442;&#21644;&#32467;&#26500;&#27491;&#21017;&#21270;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26680;&#23545;&#40784;&#29616;&#35937;&#30340;&#32454;&#33268;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.15478</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#20999;&#21521;&#29305;&#24449;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Tangent Feature Perspective of Neural Networks. (arXiv:2308.15478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20999;&#21521;&#29305;&#24449;&#35270;&#35282;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32447;&#24615;&#21464;&#25442;&#21644;&#32467;&#26500;&#27491;&#21017;&#21270;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26680;&#23545;&#40784;&#29616;&#35937;&#30340;&#32454;&#33268;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#29702;&#35299;&#22312;&#20999;&#21521;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#27169;&#22411;&#65292;&#20854;&#20013;&#29305;&#24449;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#32771;&#34385;&#29305;&#24449;&#30340;&#32447;&#24615;&#21464;&#25442;&#65292;&#20174;&#32780;&#36890;&#36807;&#21452;&#32447;&#24615;&#25554;&#20540;&#32422;&#26463;&#22312;&#21442;&#25968;&#21644;&#21464;&#25442;&#19978;&#36827;&#34892;&#32852;&#21512;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#19982;&#20855;&#26377;&#32467;&#26500;&#21270;&#27491;&#21017;&#21270;&#30340;&#31561;&#20215;&#32447;&#24615;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#20855;&#26377;&#36817;&#20284;&#20302;&#31209;&#35299;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25105;&#20204;&#23545;&#29305;&#24449;&#20197;&#21450;&#26680;&#20989;&#25968;&#30340;&#21464;&#21270;&#33719;&#24471;&#20102;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#20026;&#24403;&#30446;&#26631;&#20989;&#25968;&#22312;&#20999;&#21521;&#29305;&#24449;&#19978;&#34920;&#24449;&#19981;&#22909;&#26102;&#30340;&#26680;&#23545;&#40784;&#29616;&#35937;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#38500;&#20102;&#22312;&#31616;&#21333;&#22238;&#24402;&#38382;&#39064;&#19978;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#35266;&#23519;&#32467;&#26524;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20999;&#21521;&#29305;&#24449;&#20998;&#31867;&#30340;&#33258;&#36866;&#24212;&#29305;&#24449;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to better understand feature learning in neural networks, we propose a framework for understanding linear models in tangent feature space where the features are allowed to be transformed during training. We consider linear transformations of features, resulting in a joint optimization over parameters and transformations with a bilinear interpolation constraint. We show that this optimization problem has an equivalent linearly constrained optimization with structured regularization that encourages approximately low rank solutions. Specializing to neural network structure, we gain insights into how the features and thus the kernel function change, providing additional nuance to the phenomenon of kernel alignment when the target function is poorly represented using tangent features. In addition to verifying our theoretical observations in real neural networks on a simple regression problem, we empirically show that an adaptive feature implementation of tangent feature classificat
&lt;/p&gt;</description></item><item><title>chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14120</link><description>&lt;p&gt;
&#25480;&#26435;&#20020;&#24202;&#21307;&#29983;&#24182;&#27665;&#20027;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20020;&#24202;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; (arXiv:2308.14120v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14120
&lt;/p&gt;
&lt;p&gt;
chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24320;&#21457;&#32773;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#65289;&#21644;&#20174;&#19994;&#32773;&#65288;&#22914;&#20020;&#24202;&#21307;&#29983;&#65289;&#20043;&#38388;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#65292;&#38459;&#30861;&#20102;ML&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;chatGPT Advanced Data Analysis&#65288;ADA&#65289;&#65292;&#21363;GPT-4&#30340;&#25193;&#23637;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#24182;&#39640;&#25928;&#25191;&#34892;ML&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21521;chatGPT ADA&#25552;&#20379;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#30340;&#22823;&#22411;&#35797;&#39564;&#30340;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21644;&#30740;&#31350;&#35814;&#32454;&#20449;&#24687;&#65292;&#27809;&#26377;&#32473;&#20986;&#20855;&#20307;&#25351;&#23548;&#12290;ChatGPT ADA&#22522;&#20110;&#21407;&#22987;&#30740;&#31350;&#30340;&#35757;&#32451;&#25968;&#25454;&#33258;&#20027;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#22914;&#30284;&#30151;&#21457;&#23637;&#12289;&#30284;&#30151;&#36827;&#23637;&#12289;&#30142;&#30149;&#24182;&#21457;&#30151;&#25110;&#33268;&#30149;&#22522;&#22240;&#24207;&#21015;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;ML&#27169;&#22411;&#19982;&#20854;&#24050;&#21457;&#34920;&#30340;&#23545;&#24212;&#29289;&#30456;&#21305;&#37197;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;chatGPT ADA&#20026;&#27665;&#20027;&#21270;&#21307;&#23398;&#20013;&#30340;ML&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20351;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#33719;&#24471;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#24182;&#25512;&#21160;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLM&#65289;&#27604;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLM&#65289;&#25928;&#26524;&#26356;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#34429;&#28982;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#37117;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20294;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#21040;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#21160;&#24577;&#36981;&#24490;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#19981;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.06912</link><description>&lt;p&gt;
CausalLM&#19981;&#36866;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CausalLM is not optimal for in-context learning. (arXiv:2308.06912v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06912
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20351;&#29992;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLM&#65289;&#27604;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLM&#65289;&#25928;&#26524;&#26356;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#65292;&#34429;&#28982;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#37117;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20294;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#21040;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#65292;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#21160;&#24577;&#36981;&#24490;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#19981;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#65288;PrefixLM&#65289;&#34920;&#29616;&#26356;&#22909;&#65292;&#20854;&#20801;&#35768;&#19978;&#19979;&#25991;&#26679;&#26412;&#30456;&#20114;&#20851;&#27880;&#65307;&#30456;&#27604;&#20043;&#19979;&#65292;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65288;CausalLM&#65289;&#20351;&#29992;&#33258;&#22238;&#24402;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#31105;&#27490;&#19978;&#19979;&#25991;&#26679;&#26412;&#20851;&#27880;&#26410;&#26469;&#30340;&#26679;&#26412;&#12290;&#34429;&#28982;&#36825;&#20010;&#32467;&#26524;&#26159;&#30452;&#35266;&#30340;&#65292;&#20294;&#20174;&#29702;&#35770;&#35282;&#24230;&#24182;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#37319;&#29992;&#29702;&#35770;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#22312;&#29305;&#23450;&#21442;&#25968;&#26500;&#24314;&#19979;&#65292;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#21644;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;&#20004;&#31181;&#35821;&#35328;&#27169;&#22411;&#37117;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#31283;&#23450;&#28857;&#65292;&#20294;&#21069;&#32512;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#21040;&#32447;&#24615;&#22238;&#24402;&#30340;&#26368;&#20248;&#35299;&#65292;&#32780;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#25947;&#21160;&#24577;&#36981;&#24490;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#21363;&#20351;&#26679;&#26412;&#25968;&#37327;&#36235;&#20110;&#26080;&#31351;&#65292;&#20063;&#19981;&#33021;&#20445;&#35777;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#32463;&#39564;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic a
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2307.05209</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#30340;&#19978;&#19979;&#25991;&#39044;&#35268;&#21010;&#20197;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05209
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#27861;&#36866;&#24212;&#36731;&#24494;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;&#20026;&#20102;&#22312;&#36716;&#31227;&#21040;&#26410;&#35265;&#20219;&#21153;&#26102;&#21152;&#24555;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#22870;&#21169;&#26426;&#22120;&#26159;&#22522;&#20110;&#24403;&#21069;&#20219;&#21153;&#30340;&#22870;&#21169;&#21644;&#21160;&#24577;&#29983;&#25104;&#23376;&#20219;&#21153;&#30340;&#29366;&#24577;&#26426;&#25277;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20195;&#29702;&#25552;&#20379;&#20102;&#24403;&#21069;&#25277;&#35937;&#29366;&#24577;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#24182;&#22870;&#21169;&#23427;&#20204;&#36798;&#25104;&#36825;&#20123;&#36716;&#25442;&#12290;&#36825;&#20123;&#34920;&#31034;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#36935;&#21040;&#30340;&#31526;&#21495;&#21644;&#36716;&#25442;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#36801;&#31227;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34920;&#31034;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#39550;&#39542;&#21592;&#20957;&#35270;&#20272;&#35745;&#30340;&#22522;&#26412;&#30693;&#35782;&#12289;&#20272;&#35745;&#26041;&#27861;&#20197;&#21450;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#35752;&#35770;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#31639;&#27861;&#25216;&#26415;&#65292;&#23545;&#39550;&#39542;&#21592;&#30340;&#20957;&#35270;&#34892;&#20026;&#36827;&#34892;&#20102;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.01470</link><description>&lt;p&gt;
&#23545;&#39550;&#39542;&#21592;&#20957;&#35270;&#20272;&#35745;&#21644;&#22312;&#20957;&#35270;&#34892;&#20026;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Driver Gaze Estimation and Application in Gaze Behavior Understanding. (arXiv:2307.01470v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#39550;&#39542;&#21592;&#20957;&#35270;&#20272;&#35745;&#30340;&#22522;&#26412;&#30693;&#35782;&#12289;&#20272;&#35745;&#26041;&#27861;&#20197;&#21450;&#22312;&#23454;&#38469;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#35752;&#35770;&#19981;&#21516;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#31639;&#27861;&#25216;&#26415;&#65292;&#23545;&#39550;&#39542;&#21592;&#30340;&#20957;&#35270;&#34892;&#20026;&#36827;&#34892;&#20102;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39550;&#39542;&#21592;&#30340;&#20957;&#35270;&#22312;&#39550;&#39542;&#21592;&#27880;&#24847;&#21147;&#26816;&#27979;&#12289;&#35270;&#35273;&#20998;&#24515;&#26816;&#27979;&#12289;&#20957;&#35270;&#34892;&#20026;&#29702;&#35299;&#21644;&#26500;&#24314;&#39550;&#39542;&#21592;&#36741;&#21161;&#31995;&#32479;&#31561;&#20957;&#35270;&#22522;&#20110;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#39550;&#39542;&#21592;&#30340;&#20957;&#35270;&#22522;&#30784;&#30693;&#35782;&#12289;&#39550;&#39542;&#21592;&#20957;&#35270;&#20272;&#35745;&#26041;&#27861;&#21450;&#20854;&#22312;&#29616;&#23454;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#24635;&#32467;&#12290;&#39318;&#20808;&#25105;&#20204;&#35752;&#35770;&#19982;&#39550;&#39542;&#21592;&#20957;&#35270;&#30456;&#20851;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#21253;&#25324;&#22836;&#25140;&#24335;&#21644;&#36828;&#31243;&#35774;&#32622;&#30340;&#20957;&#35270;&#20272;&#35745;&#20197;&#21450;&#27599;&#31181;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#25152;&#20351;&#29992;&#30340;&#26415;&#35821;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21015;&#20030;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#39550;&#39542;&#21592;&#20957;&#35270;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#25152;&#20351;&#29992;&#30340;&#35774;&#22791;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29992;&#20110;&#39550;&#39542;&#21592;&#20957;&#35270;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#20027;&#35201;&#28041;&#21450;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#12290;&#25509;&#30528;&#65292;&#20272;&#35745;&#30340;&#39550;&#39542;&#21592;&#20957;&#35270;&#34987;&#29992;&#20110;&#29702;&#35299;&#22312;&#39550;&#39542;&#36807;&#31243;&#20013;&#30340;&#20957;&#35270;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driver gaze plays an important role in different gaze-based applications such as driver attentiveness detection, visual distraction detection, gaze behavior understanding, and building driver assistance system. The main objective of this study is to perform a comprehensive summary of driver gaze fundamentals, methods to estimate driver gaze, and it's applications in real world driving scenarios. We first discuss the fundamentals related to driver gaze, involving head-mounted and remote setup based gaze estimation and the terminologies used for each of these data collection methods. Next, we list out the existing benchmark driver gaze datasets, highlighting the collection methodology and the equipment used for such data collection. This is followed by a discussion of the algorithms used for driver gaze estimation, which primarily involves traditional machine learning and deep learning based techniques. The estimated driver gaze is then used for understanding gaze behavior while maneuver
&lt;/p&gt;</description></item><item><title>NeuralFuse&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16869</link><description>&lt;p&gt;
NeuralFuse: &#23398;&#20064;&#25913;&#21892;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes. (arXiv:2306.16869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16869
&lt;/p&gt;
&lt;p&gt;
NeuralFuse&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#20854;&#33021;&#37327;&#28040;&#32791;&#20173;&#28982;&#26159;&#19968;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#38477;&#20302;&#20379;&#30005;&#30005;&#21387;&#26159;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#38477;&#20302;&#20379;&#30005;&#30005;&#21387;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#65292;&#22240;&#20026;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38745;&#24577;&#38543;&#26426;&#23384;&#20648;&#22120;(SRAM)&#20013;&#65292;&#32780;SRAM&#20013;&#20250;&#21457;&#29983;&#38543;&#26426;&#20301;&#32763;&#36716;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NeuralFuse&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#22312;&#20302;&#30005;&#21387;&#29615;&#22659;&#20013;&#35299;&#20915;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;NeuralFuse&#22312;&#26631;&#31216;&#30005;&#21387;&#21644;&#20302;&#30005;&#21387;&#24773;&#20917;&#19979;&#37117;&#33021;&#20445;&#25252;DNN&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;NeuralFuse&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#26377;&#38480;&#35775;&#38382;&#30340;DNN&#65292;&#20363;&#22914;&#19981;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#25110;&#20113;&#31471;API&#30340;&#36828;&#31243;&#35775;&#38382;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;1%&#30340;&#20301;&#38169;&#35823;&#29575;&#19979;&#65292;NeuralFuse&#21487;&#20197;&#23558;SRAM&#20869;&#23384;&#35775;&#38382;&#33021;&#37327;&#38477;&#20302;&#39640;&#36798;24%&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have become ubiquitous in machine learning, but their energy consumption remains a notable issue. Lowering the supply voltage is an effective strategy for reducing energy consumption. However, aggressively scaling down the supply voltage can lead to accuracy degradation due to random bit flips in static random access memory (SRAM) where model parameters are stored. To address this challenge, we introduce NeuralFuse, a novel add-on module that addresses the accuracy-energy tradeoff in low-voltage regimes by learning input transformations to generate error-resistant data representations. NeuralFuse protects DNN accuracy in both nominal and low-voltage scenarios. Moreover, NeuralFuse is easy to implement and can be readily applied to DNNs with limited access, such as non-configurable hardware or remote access to cloud-based APIs. Experimental results demonstrate that, at a 1% bit error rate, NeuralFuse can reduce SRAM memory access energy by up to 24% while imp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40784;&#27425;&#31354;&#38388;&#19978;&#30340;&#28508;&#22312;SDE&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20301;&#29699;&#19978;&#30340;SDE&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#30452;&#35266;&#30340;&#34920;&#36798;&#24335;&#26469;&#35745;&#31639;&#36817;&#20284;&#21518;&#39564;&#21644;&#20808;&#39564;&#36807;&#31243;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.16248</link><description>&lt;p&gt;
&#40784;&#27425;&#31354;&#38388;&#19978;&#30340;&#28508;&#22312;SDE
&lt;/p&gt;
&lt;p&gt;
Latent SDEs on Homogeneous Spaces. (arXiv:2306.16248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40784;&#27425;&#31354;&#38388;&#19978;&#30340;&#28508;&#22312;SDE&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20301;&#29699;&#19978;&#30340;SDE&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#30452;&#35266;&#30340;&#34920;&#36798;&#24335;&#26469;&#35745;&#31639;&#36817;&#20284;&#21518;&#39564;&#21644;&#20808;&#39564;&#36807;&#31243;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#20013;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#65288;&#21487;&#33021;&#26159;&#22797;&#26434;&#30340;&#65289;&#35266;&#27979;&#38543;&#26426;&#36807;&#31243;&#30001;&#28508;&#22312;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#25152;&#39537;&#21160;&#12290;&#21463;&#21040;&#23398;&#20064;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#65288;&#20960;&#20046;&#20219;&#24847;&#65289;&#28508;&#22312;&#31070;&#32463;SDE&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#20363;&#22914;&#25928;&#29575;&#26799;&#24230;&#35745;&#31639;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#24182;&#30740;&#31350;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#23376;&#31867;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;SDE&#22312;&#19968;&#20010;&#40784;&#27425;&#28508;&#22312;&#31354;&#38388;&#19978;&#28436;&#21464;&#65292;&#24182;&#30001;&#30456;&#24212;&#65288;&#30697;&#38453;&#65289;Lie&#32676;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#25152;&#35825;&#23548;&#12290;&#22312;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21333;&#20301;$n$-&#29699;&#19978;&#30340;SDE&#21487;&#20197;&#35828;&#26159;&#36825;&#19968;&#35774;&#32622;&#20013;&#26368;&#30456;&#20851;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#21464;&#20998;&#25512;&#26029;&#20013;&#65292;&#21333;&#20301;&#29699;&#19981;&#20165;&#26377;&#21161;&#20110;&#20351;&#29992;&#30495;&#27491;&#26080;&#20449;&#24687;&#30340;&#20808;&#39564;SDE&#65292;&#32780;&#19988;&#25105;&#20204;&#36824;&#33719;&#24471;&#20102;&#20851;&#20110;&#36817;&#20284;&#21518;&#39564;&#21644;&#20808;&#39564;&#36807;&#31243;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#29305;&#21035;&#31616;&#21333;&#21644;&#30452;&#35266;&#30340;&#34920;&#36798;&#24335;&#65292;&#36825;&#22312;&#35777;&#25454;&#19979;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of variational Bayesian inference in a latent variable model where a (possibly complex) observed stochastic process is governed by the solution of a latent stochastic differential equation (SDE). Motivated by the challenges that arise when trying to learn an (almost arbitrary) latent neural SDE from large-scale data, such as efficient gradient computation, we take a step back and study a specific subclass instead. In our case, the SDE evolves on a homogeneous latent space and is induced by stochastic dynamics of the corresponding (matrix) Lie group. In learning problems, SDEs on the unit $n$-sphere are arguably the most relevant incarnation of this setup. Notably, for variational inference, the sphere not only facilitates using a truly uninformative prior SDE, but we also obtain a particularly simple and intuitive expression for the Kullback-Leibler divergence between the approximate posterior and prior process in the evidence lower bound. Experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#32423;&#31070;&#32463;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20284;&#28982;&#20989;&#25968;&#19981;&#21487;&#35745;&#31639;&#20294;&#21487;&#20197;&#36890;&#36807;&#21069;&#21521;&#27169;&#25311;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#26368;&#20248;&#27010;&#29575;&#25512;&#26029;&#65292;&#30528;&#37325;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#21487;&#20197;&#23548;&#33268;&#26356;&#32039;&#20945;&#30340;&#21442;&#25968;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2306.12584</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#32423;&#31070;&#32463;&#27169;&#25311;&#30340;&#20107;&#20214;&#38598;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Neural Simulation-Based Inference Over Event Ensembles. (arXiv:2306.12584v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#32423;&#31070;&#32463;&#27169;&#25311;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20284;&#28982;&#20989;&#25968;&#19981;&#21487;&#35745;&#31639;&#20294;&#21487;&#20197;&#36890;&#36807;&#21069;&#21521;&#27169;&#25311;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#26368;&#20248;&#27010;&#29575;&#25512;&#26029;&#65292;&#30528;&#37325;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#21487;&#20197;&#23548;&#33268;&#26356;&#32039;&#20945;&#30340;&#21442;&#25968;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#20107;&#20214;&#38598;&#26159;&#24120;&#35265;&#30340;&#35266;&#27979;&#20540;&#38598;&#21512;&#65292;&#23427;&#20204;&#20849;&#21516;&#32422;&#26463;&#20102;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#23618;&#32423;&#32467;&#26500;&#65292;&#20854;&#20013;&#8220;&#23616;&#37096;&#8221;&#21442;&#25968;&#24433;&#21709;&#21333;&#20010;&#20107;&#20214;&#65292;&#8220;&#20840;&#23616;&#8221;&#21442;&#25968;&#24433;&#21709;&#25972;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20284;&#28982;&#20989;&#25968;&#19981;&#21487;&#35745;&#31639;&#20294;&#21487;&#20197;&#36890;&#36807;&#21069;&#21521;&#27169;&#25311;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#26368;&#20248;&#27010;&#29575;&#25512;&#26029;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20284;&#28982;&#20989;&#25968;&#65288;&#27604;&#65289;&#25110;&#21518;&#39564;&#27010;&#29575;&#30340;&#31070;&#32463;&#20272;&#35745;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#26126;&#30830;&#32771;&#34385;&#27169;&#22411;&#23618;&#32423;&#32467;&#26500;&#21487;&#20197;&#23548;&#33268;&#26356;&#32039;&#20945;&#30340;&#21442;&#25968;&#32422;&#26463;&#12290;&#25105;&#20204;&#20197;&#29289;&#29702;&#31185;&#23398;&#20026;&#20363;&#30740;&#31350;&#20102;&#26412;&#25991;&#35752;&#35770;&#30340;&#20869;&#23481;&#65292;&#30528;&#37325;&#20110;&#31890;&#23376;&#29289;&#29702;&#23398;&#65288;&#31890;&#23376;&#23545;&#25758;&#26426;&#25968;&#25454;&#65289;&#21644;&#22825;&#20307;&#29289;&#29702;&#23398;&#65288;&#24378;&#24341;&#21147;&#36879;&#38236;&#35266;&#27979;&#65289;&#30340;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
When analyzing real-world data it is common to work with event ensembles, which comprise sets of observations that collectively constrain the parameters of an underlying model of interest. Such models often have a hierarchical structure, where "local" parameters impact individual events and "global" parameters influence the entire dataset. We introduce practical approaches for optimal dataset-wide probabilistic inference in cases where the likelihood is intractable, but simulations can be realized via forward modeling. We construct neural estimators for the likelihood(-ratio) or posterior and show that explicitly accounting for the model's hierarchical structure can lead to tighter parameter constraints. We ground our discussion using case studies from the physical sciences, focusing on examples from particle physics (particle collider data) and astrophysics (strong gravitational lensing observations).
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#26395;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.08107</link><description>&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;AutoML&#65306;&#24403;&#21069;&#25361;&#25112;&#65292;&#26410;&#26469;&#26426;&#36935;&#21644;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks. (arXiv:2306.08107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08107
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#26395;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#29305;&#21035;&#26159;&#22312;NLP&#39046;&#22495;&#65292;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#32463;&#21382;&#20102;&#19968;&#31995;&#21015;&#31361;&#30772;&#12290;&#25105;&#20204;&#35774;&#24819;&#65292;&#20004;&#20010;&#39046;&#22495;&#36890;&#36807;&#32039;&#23494;&#30340;&#34701;&#21512;&#21487;&#20197;&#24444;&#27492;&#25512;&#21160;&#26497;&#38480;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#19968;&#24895;&#26223;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;AutoML&#21644;LLMs&#20043;&#38388;&#30340;&#20849;&#29983;&#20851;&#31995;&#28508;&#21147;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#23427;&#20204;&#22914;&#20309;&#20114;&#30456;&#21463;&#30410;&#12290;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#20174;&#19981;&#21516;&#35282;&#24230;&#22686;&#24378;LLMs&#30340;AutoML&#26041;&#27861;&#30340;&#26426;&#20250;&#20197;&#21450;&#21033;&#29992;AutoML&#36827;&#19968;&#27493;&#25913;&#36827;LLMs&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;&#39118;&#38505;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#26377;&#21487;&#33021;&#39072;&#35206;NLP&#21644;AutoML&#20004;&#20010;&#39046;&#22495;&#12290;&#36890;&#36807;&#24378;&#35843;&#21487;&#24819;&#35937;&#30340;&#21327;&#21516;&#20316;&#29992;&#21644;&#39118;&#38505;&#65292;&#25105;&#20204;&#26088;&#22312;&#20419;&#36827;&#22312;&#20132;&#21449;&#28857;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersect
&lt;/p&gt;</description></item><item><title>SUNG&#26159;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36890;&#36807;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#21644;&#24212;&#29992;&#20445;&#23432;Q&#20540;&#20272;&#35745;&#30340;&#25351;&#23548;&#19979;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32769;&#21270;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.07541</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#32479;&#19968;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning. (arXiv:2306.07541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07541
&lt;/p&gt;
&lt;p&gt;
SUNG&#26159;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36890;&#36807;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#21644;&#24212;&#29992;&#20445;&#23432;Q&#20540;&#20272;&#35745;&#30340;&#25351;&#23548;&#19979;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32769;&#21270;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#20381;&#38752;&#25968;&#25454;&#39537;&#21160;&#33539;&#20363;&#23398;&#20064;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#28982;&#32780;&#65292;&#21463;&#38480;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#36136;&#37327;&#65292;&#20854;&#24615;&#33021;&#24120;&#24120;&#19981;&#22815;&#20248;&#31168;&#12290;&#22240;&#27492;&#65292;&#22312;&#37096;&#32626;&#20043;&#21069;&#36890;&#36807;&#39069;&#22806;&#30340;&#22312;&#32447;&#20132;&#20114;&#36827;&#19968;&#27493;&#24494;&#35843;&#26234;&#33021;&#20307;&#26159;&#26377;&#24517;&#35201;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#21463;&#21040;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#30340;&#21046;&#32422;&#65292;&#21363;&#21463;&#38480;&#30340;&#25506;&#32034;&#34892;&#20026;&#21644;&#29366;&#24577;-&#21160;&#20316;&#20998;&#24067;&#20559;&#31227;&#65292;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32479;&#19968;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#65288;SUNG&#65289;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24037;&#20855;&#33258;&#28982;&#22320;&#32479;&#19968;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SUNG&#36890;&#36807;&#22522;&#20110;VAE&#30340;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#23494;&#24230;&#20272;&#35745;&#22120;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#39640;&#25928;&#25506;&#32034;&#65292;SUNG&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#20048;&#35266;&#25506;&#32034;&#31574;&#30053;&#65292;&#20197;&#36873;&#25321;&#20855;&#26377;&#39640;&#20215;&#20540;&#21644;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20449;&#24687;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;SUNG&#36890;&#36807;&#22312;&#19981;&#30830;&#23450;&#24615;&#25351;&#23548;&#19979;&#24212;&#29992;&#20445;&#23432;Q&#20540;&#20272;&#35745;&#26469;&#24320;&#21457;&#19968;&#31181;&#33258;&#36866;&#24212;&#21033;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Atari&#21644;MuJoCo&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SUNG&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#25509;&#36817;&#22312;&#32447;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conserva
&lt;/p&gt;</description></item><item><title>SENS&#26159;&#19968;&#31181;&#22522;&#20110;&#33609;&#22270;&#30340;&#29983;&#25104;&#21644;&#32534;&#36753;3D&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;ViT&#34917;&#19969;&#32534;&#30721;&#23558;&#33609;&#22270;&#26144;&#23556;&#21040;&#31070;&#32463;&#38544;&#24335;&#24418;&#29366;&#26550;&#26500;&#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#25429;&#25417;&#29992;&#25143;&#33609;&#22270;&#30340;&#24847;&#22270;&#36827;&#34892;&#29983;&#25104;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#30452;&#35266;&#30340;&#22522;&#20110;&#33609;&#22270;&#30340;&#24418;&#29366;&#32534;&#36753;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06088</link><description>&lt;p&gt;
SENS&#65306;&#22522;&#20110;&#33609;&#22270;&#30340;&#38544;&#24335;&#31070;&#32463;&#24418;&#29366;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
SENS: Sketch-based Implicit Neural Shape Modeling. (arXiv:2306.06088v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06088
&lt;/p&gt;
&lt;p&gt;
SENS&#26159;&#19968;&#31181;&#22522;&#20110;&#33609;&#22270;&#30340;&#29983;&#25104;&#21644;&#32534;&#36753;3D&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;ViT&#34917;&#19969;&#32534;&#30721;&#23558;&#33609;&#22270;&#26144;&#23556;&#21040;&#31070;&#32463;&#38544;&#24335;&#24418;&#29366;&#26550;&#26500;&#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#21487;&#20197;&#25429;&#25417;&#29992;&#25143;&#33609;&#22270;&#30340;&#24847;&#22270;&#36827;&#34892;&#29983;&#25104;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#30452;&#35266;&#30340;&#22522;&#20110;&#33609;&#22270;&#30340;&#24418;&#29366;&#32534;&#36753;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SENS&#65292;&#19968;&#31181;&#20174;&#25163;&#32472;&#33609;&#22270;&#20013;&#29983;&#25104;&#21644;&#32534;&#36753;3D&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#37027;&#20123;&#25277;&#35937;&#30340;&#33609;&#22270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#24555;&#36895;&#36731;&#26494;&#22320;&#33609;&#32472;&#24418;&#29366;&#65292;&#28982;&#21518;&#23558;&#33609;&#22270;&#26144;&#23556;&#21040;&#19968;&#20010;&#38754;&#21521;&#37096;&#20214;&#30340;&#31070;&#32463;&#38544;&#24335;&#24418;&#29366;&#26550;&#26500;&#30340;&#28508;&#31354;&#38388;&#20013;&#12290;SENS&#20998;&#26512;&#33609;&#22270;&#24182;&#23558;&#20854;&#37096;&#20214;&#32534;&#30721;&#25104;ViT&#34917;&#19969;&#32534;&#30721;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;transformer&#35299;&#30721;&#22120;&#20013;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#36866;&#29992;&#20110;&#32534;&#36753;3D&#38544;&#24335;&#31070;&#32463;&#24418;&#29366;&#30340;&#24418;&#29366;&#23884;&#20837;&#12290;SENS&#19981;&#20165;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#22522;&#20110;&#33609;&#22270;&#30340;&#29983;&#25104;&#21644;&#32534;&#36753;&#65292;&#32780;&#19988;&#22312;&#25429;&#25417;&#29992;&#25143;&#33609;&#22270;&#24847;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#29983;&#25104;&#21508;&#31181;&#26032;&#39062;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;3D&#24418;&#29366;&#65292;&#29978;&#33267;&#21487;&#20197;&#20174;&#25277;&#35937;&#30340;&#33609;&#22270;&#20013;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#23458;&#35266;&#25351;&#26631;&#35780;&#20272;&#26631;&#20934;&#21644;&#20915;&#23450;&#24615;&#29992;&#25143;&#30740;&#31350;&#26469;&#23637;&#31034;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#20004;&#32773;&#22343;&#34920;&#26126;&#23427;&#22312;&#20855;&#26377;&#20013;&#31561;&#25277;&#35937;&#31243;&#24230;&#30340;&#33609;&#22270;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#30452;&#35266;&#30340;&#22522;&#20110;&#33609;&#22270;&#30340;&#24418;&#29366;&#32534;&#36753;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SENS, a novel method for generating and editing 3D models from hand-drawn sketches, including those of an abstract nature. Our method allows users to quickly and easily sketch a shape, and then maps the sketch into the latent space of a part-aware neural implicit shape architecture. SENS analyzes the sketch and encodes its parts into ViT patch encoding, then feeds them into a transformer decoder that converts them to shape embeddings, suitable for editing 3D neural implicit shapes. SENS not only provides intuitive sketch-based generation and editing, but also excels in capturing the intent of the user's sketch to generate a variety of novel and expressive 3D shapes, even from abstract sketches. We demonstrate the effectiveness of our model compared to the state-of-the-art using objective metric evaluation criteria and a decisive user study, both indicating strong performance on sketches with a medium level of abstraction. Furthermore, we showcase its intuitive sketch-based s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#39640;&#32500;&#20960;&#20309;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#20013;&#24378;&#21046;&#26045;&#21152;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#25551;&#36848;&#20102;&#28145;&#24230;&#32593;&#32476;&#21098;&#26525;&#27604;&#29575;&#30340;&#30456;&#21464;&#28857;&#65292;&#35813;&#28857;&#31561;&#20110;&#26576;&#20123;&#20984;&#20307;&#30340;&#24179;&#26041;&#39640;&#26031;&#23485;&#24230;&#38500;&#20197;&#21442;&#25968;&#30340;&#21407;&#22987;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.05857</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#21487;&#20197;&#34987;&#21098;&#26525;&#21040;&#22810;&#20040;&#31232;&#30095;&#65306;&#20960;&#20309;&#35270;&#35282;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Sparse Can We Prune A Deep Network: A Geometric Viewpoint. (arXiv:2306.05857v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#39640;&#32500;&#20960;&#20309;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#20013;&#24378;&#21046;&#26045;&#21152;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#25551;&#36848;&#20102;&#28145;&#24230;&#32593;&#32476;&#21098;&#26525;&#27604;&#29575;&#30340;&#30456;&#21464;&#28857;&#65292;&#35813;&#28857;&#31561;&#20110;&#26576;&#20123;&#20984;&#20307;&#30340;&#24179;&#26041;&#39640;&#26031;&#23485;&#24230;&#38500;&#20197;&#21442;&#25968;&#30340;&#21407;&#22987;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#20043;&#19968;&#12290;&#34429;&#28982;&#23427;&#21487;&#20197;&#25552;&#20379;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#21516;&#26102;&#20063;&#24378;&#21152;&#20102;&#37325;&#22823;&#30340;&#23384;&#20648;&#36127;&#25285;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#30740;&#31350;&#32593;&#32476;&#21098;&#26525;&#12290;&#19968;&#20010;&#33258;&#28982;&#32780;&#22522;&#26412;&#30340;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21098;&#26525;&#19968;&#20010;&#28145;&#24230;&#32593;&#32476;&#21040;&#22810;&#20040;&#31232;&#30095;&#65288;&#20960;&#20046;&#19981;&#24433;&#21709;&#24615;&#33021;&#65289;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#31532;&#19968;&#21407;&#29702;&#26041;&#27861;&#65292;&#20855;&#20307;&#22320;&#65292;&#21482;&#36890;&#36807;&#22312;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#20013;&#24378;&#21046;&#26045;&#21152;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#39640;&#32500;&#20960;&#20309;&#30340;&#35282;&#24230;&#25551;&#36848;&#21098;&#26525;&#27604;&#29575;&#30340;&#23574;&#38160;&#30456;&#21464;&#28857;&#65292;&#35813;&#28857;&#23545;&#24212;&#20110;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21098;&#26525;&#27604;&#29575;&#30340;&#30456;&#21464;&#28857;&#31561;&#20110;&#26576;&#20123;&#20984;&#20307;&#30340;&#24179;&#26041;&#39640;&#26031;&#23485;&#24230;&#65292;&#36825;&#20123;&#20984;&#20307;&#26159;&#30001;$l_1$-&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#24471;&#20986;&#30340;&#65292;&#38500;&#20197;&#21442;&#25968;&#30340;&#21407;&#22987;&#32500;&#24230;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21098;&#26525;&#36807;&#31243;&#20013;&#21442;&#25968;&#30340;&#20998;&#24067;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overparameterization constitutes one of the most significant hallmarks of deep neural networks. Though it can offer the advantage of outstanding generalization performance, it meanwhile imposes substantial storage burden, thus necessitating the study of network pruning. A natural and fundamental question is: How sparse can we prune a deep network (with almost no hurt on the performance)? To address this problem, in this work we take a first principles approach, specifically, by merely enforcing the sparsity constraint on the original loss function, we're able to characterize the sharp phase transition point of pruning ratio, which corresponds to the boundary between the feasible and the infeasible, from the perspective of high-dimensional geometry. It turns out that the phase transition point of pruning ratio equals the squared Gaussian width of some convex body resulting from the $l_1$-regularized loss function, normalized by the original dimension of parameters. As a byproduct, we pr
&lt;/p&gt;</description></item><item><title>dotears&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;DAG&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#26469;&#25512;&#26029;&#21333;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;&#23427;&#30452;&#25509;&#20272;&#35745;&#22806;&#29983;&#35823;&#24046;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#24490;&#29615;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19215</link><description>&lt;p&gt;
dotears: &#20351;&#29992;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#19968;&#33268;&#30340;DAG&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
dotears: Scalable, consistent DAG estimation using observational and interventional data. (arXiv:2305.19215v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19215
&lt;/p&gt;
&lt;p&gt;
dotears&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;DAG&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#26469;&#25512;&#26029;&#21333;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;&#23427;&#30452;&#25509;&#20272;&#35745;&#22806;&#29983;&#35823;&#24046;&#32467;&#26500;&#65292;&#36991;&#20813;&#20102;&#24490;&#29615;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270; (DAG)&#38754;&#20020;&#30528;&#21487;&#36776;&#35782;&#24615;&#32570;&#22833;&#21644;&#35299;&#20915;&#26041;&#26696;&#32452;&#21512;&#31354;&#38388;&#30340;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#39640;&#20102;&#35266;&#27979;&#25968;&#25454;&#20013;&#22522;&#20110;&#24471;&#20998;&#30340;DAG&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#25805;&#20316;&#24615;&#65292;&#20294;&#23545;&#22806;&#29983;&#35823;&#24046;&#26041;&#24046;&#30340;&#32467;&#26500;&#25935;&#24863;&#12290;&#21516;&#26102;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#23398;&#20064;&#22806;&#29983;&#26041;&#24046;&#32467;&#26500;&#38656;&#35201;&#32467;&#26500;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#38024;&#23545;&#26032;&#30340;&#29983;&#29289;&#25216;&#26415;&#65292;&#23558;&#39640;&#24230;&#24182;&#34892;&#30340;&#22522;&#22240;&#24178;&#39044;&#19982;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;dotears&#65292;&#36890;&#36807;&#36830;&#32493;&#20248;&#21270;&#21033;&#29992;&#35266;&#27979;&#21644;&#24178;&#39044;&#25968;&#25454;&#26469;&#25512;&#26029;&#21333;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;dotears&#21033;&#29992;&#24178;&#39044;&#30340;&#21487;&#39044;&#27979;&#30340;&#32467;&#26500;&#21518;&#26524;&#30452;&#25509;&#20272;&#35745;&#22806;&#29983;&#35823;&#24046;&#32467;&#26500;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#24490;&#29615;&#20272;&#35745;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#20174;&#32463;&#39564;&#21644;&#20998;&#26512;&#26041;&#38754;&#36827;&#34892;&#20102;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning causal directed acyclic graphs (DAGs) from data is complicated by a lack of identifiability and the combinatorial space of solutions. Recent work has improved tractability of score-based structure learning of DAGs in observational data, but is sensitive to the structure of the exogenous error variances. On the other hand, learning exogenous variance structure from observational data requires prior knowledge of structure. Motivated by new biological technologies that link highly parallel gene interventions to a high-dimensional observation, we present $\texttt{dotears}$ [doo-tairs], a scalable structure learning framework which leverages observational and interventional data to infer a single causal structure through continuous optimization. $\texttt{dotears}$ exploits predictable structural consequences of interventions to directly estimate the exogenous error structure, bypassing the circular estimation problem. We extend previous work to show, both empirically and analytical
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26059;&#36716;&#20248;&#21270;&#22120;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#21487;&#20197;&#31616;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#65292;&#29978;&#33267;&#22312;&#20960;&#20046;&#19981;&#38656;&#35843;&#25972;&#22522;&#32447;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.17212</link><description>&lt;p&gt;
&#26059;&#36716;&#20248;&#21270;&#22120;&#65306;&#31616;&#21333;&#32780;&#24378;&#20581;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotational Optimizers: Simple &amp; Robust DNN Training. (arXiv:2305.17212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26059;&#36716;&#20248;&#21270;&#22120;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#21487;&#20197;&#31616;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#65292;&#29978;&#33267;&#22312;&#20960;&#20046;&#19981;&#38656;&#35843;&#25972;&#22522;&#32447;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21160;&#24577;&#21462;&#20915;&#20110;&#23398;&#20064;&#29575;&#12289;&#26435;&#37325;&#34928;&#20943;&#12289;&#21021;&#22987;&#21270;&#31561;&#36229;&#21442;&#25968;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#20132;&#20114;&#20316;&#29992;&#21487;&#20197;&#22312;&#23610;&#24230;&#19981;&#21464;&#23618;&#65288;&#22914;&#24402;&#19968;&#21270;&#23618;&#65289;&#20013;&#20135;&#29983;&#29699;&#38754;&#36816;&#21160;&#21160;&#24577;&#65292;&#36825;&#20123;&#21160;&#24577;&#25910;&#25947;&#21040;&#24179;&#34913;&#29366;&#24577;&#65292;&#20854;&#20013;&#26435;&#37325;&#33539;&#25968;&#21644;&#39044;&#26399;&#26059;&#36716;&#26356;&#26032;&#22823;&#23567;&#26159;&#22266;&#23450;&#30340;&#12290;&#25105;&#20204;&#23545;AdamW&#12289;&#24102;&#21160;&#37327;&#30340;SGD&#21644;Lion&#20013;&#30340;&#36825;&#20010;&#24179;&#34913;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#19981;&#21516;&#36229;&#21442;&#25968;&#21450;&#20854;&#30456;&#20114;&#20316;&#29992;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20123;&#20248;&#21270;&#22120;&#30340;&#26059;&#36716;&#21464;&#20307;&#65288;RVs&#65289;&#65292;&#24378;&#21046;&#39044;&#26399;&#35282;&#24230;&#26356;&#26032;&#22823;&#23567;&#19982;&#25972;&#20010;&#35757;&#32451;&#26399;&#38388;&#30340;&#24179;&#34913;&#20540;&#30456;&#21305;&#37197;&#12290;&#36825;&#31616;&#21270;&#20102;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#28040;&#38500;&#25910;&#25947;&#21040;&#24179;&#34913;&#29366;&#24577;&#30340;&#30636;&#24577;&#30456;&#24212;&#12290;&#25105;&#20204;&#30340;&#26059;&#36716;&#20248;&#21270;&#22120;&#21487;&#20197;&#21305;&#37197;&#21407;&#22987;&#21464;&#20307;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#38656;&#35201;&#23545;&#22522;&#32447;&#36229;&#21442;&#25968;&#36827;&#34892;&#26368;&#23569;&#25110;&#19981;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training dynamics of modern deep neural networks depend on complex interactions between the learning rate, weight decay, initialization, and other hyperparameters. These interactions can give rise to Spherical Motion Dynamics in scale-invariant layers (e.g., normalized layers), which converge to an equilibrium state, where the weight norm and the expected rotational update size are fixed. Our analysis of this equilibrium in AdamW, SGD with momentum, and Lion provides new insights into the effects of different hyperparameters and their interactions on the training process. We propose rotational variants (RVs) of these optimizers that force the expected angular update size to match the equilibrium value throughout training. This simplifies the training dynamics by removing the transient phase corresponding to the convergence to an equilibrium. Our rotational optimizers can match the performance of the original variants, often with minimal or no tuning of the baseline hyperparameters,
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.11527</link><description>&lt;p&gt;
InstructIE: &#19968;&#20221;&#22522;&#20110;&#25351;&#20196;&#30340;&#20013;&#25991;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11527
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#31216;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462; (Instruction-based IE)&#65292;&#23427;&#26088;&#22312;&#35201;&#27714;&#31995;&#32479;&#36981;&#24490;&#29305;&#23450;&#30340;&#25351;&#20196;&#25110;&#25351;&#21335;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340; 270,000 &#20010;&#24369;&#30417;&#30563;&#25968;&#25454;&#21644; 1,000 &#20010;&#39640;&#36136;&#37327;&#20247;&#21253;&#27880;&#37322;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;InstructIE&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/zjunlp/DeepKE/tree/main/example/llm &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23545;&#24050;&#35757;&#32451;&#22909;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#21518;&#26399;&#32534;&#36753;&#65292;&#20197;&#20415;&#32534;&#36753;&#25481;&#26576;&#20123;&#26465;&#20214;&#20998;&#25903;&#65292;&#36825;&#20123;&#26465;&#20214;&#20998;&#25903;&#24456;&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#12290;&#36890;&#36807;&#31934;&#31616;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#32593;&#32476;&#23454;&#29616;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#25928;&#12289;&#39640;&#25928;&#12289;&#20855;&#26377;&#21487;&#25511;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11351</link><description>&lt;p&gt;
&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Data Redaction from Conditional Generative Models. (arXiv:2305.11351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23545;&#24050;&#35757;&#32451;&#22909;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#21518;&#26399;&#32534;&#36753;&#65292;&#20197;&#20415;&#32534;&#36753;&#25481;&#26576;&#20123;&#26465;&#20214;&#20998;&#25903;&#65292;&#36825;&#20123;&#26465;&#20214;&#20998;&#25903;&#24456;&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#12290;&#36890;&#36807;&#31934;&#31616;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#32593;&#32476;&#23454;&#29616;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26377;&#25928;&#12289;&#39640;&#25928;&#12289;&#20855;&#26377;&#21487;&#25511;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22240;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#20256;&#32479;&#30340;&#32531;&#35299;&#26041;&#27861;&#21253;&#25324;&#37325;&#26032;&#35757;&#32451;&#12289;&#36807;&#28388;&#25110;&#32534;&#36753;&#65307;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35201;&#20040;&#20250;&#34987;&#31532;&#19977;&#26041;&#22238;&#36991;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#22914;&#20309;&#21518;&#26399;&#32534;&#36753;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#20854;&#32534;&#36753;&#25481;&#26576;&#20123;&#26465;&#20214;&#20998;&#25903;&#65292;&#36825;&#20123;&#26465;&#20214;&#20998;&#25903;&#24456;&#21487;&#33021;&#20250;&#29983;&#25104;&#19981;&#33391;&#20869;&#23481;&#12290;&#36825;&#26159;&#36890;&#36807;&#31934;&#31616;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#32593;&#32476;&#26469;&#23454;&#29616;&#30340;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#12289;&#20855;&#26377;&#21487;&#25511;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#33021;&#29992;&#20110;&#19968;&#31867;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#25968;&#25454;&#32534;&#36753;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#65292;&#30456;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#32534;&#36753;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#39640;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models are known to produce undesirable samples such as harmful content. Traditional mitigation methods include re-training from scratch, filtering, or editing; however, these are either computationally expensive or can be circumvented by third parties. In this paper, we take a different approach and study how to post-edit an already-trained conditional generative model so that it redacts certain conditionals that will, with high probability, lead to undesirable content. This is done by distilling the conditioning network in the models, giving a solution that is effective, efficient, controllable, and universal for a class of deep generative models. We conduct experiments on redacting prompts in text-to-image models and redacting voices in text-to-speech models. Our method is computationally light, leads to better redaction quality and robustness than baseline methods while still retaining high generation quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#39640;&#32500;&#24230;&#29699;&#20307;&#19978;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#20351;&#29992;SGD&#31639;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11602</link><description>&lt;p&gt;
&#24102;&#24212;&#29992;&#20110;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#30340;&#21442;&#25968;&#21270;&#29699;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convergence of stochastic gradient descent on parameterized sphere with applications to variational Monte Carlo simulation. (arXiv:2303.11602v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11602
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#39640;&#32500;&#24230;&#29699;&#20307;&#19978;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#20351;&#29992;SGD&#31639;&#27861;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#30001;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#20026;&#24120;&#25968;&#20493;&#30340;&#39640;&#32500;&#29699;&#19978;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31867;&#22411;&#31639;&#27861;&#12290;&#25105;&#20204;&#20026;&#26377;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#24182;&#22312;&#29702;&#35770;&#21644;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#39318;&#27425;&#25552;&#20379;&#20102;&#26080;&#30417;&#30563;&#35774;&#32622;&#30340;&#25910;&#25947;&#35777;&#26126;&#65292;&#35813;&#35774;&#32622;&#23545;&#24212;&#20110;&#37327;&#23376;&#29289;&#29702;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#65288;VMC&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze stochastic gradient descent (SGD) type algorithms on a high-dimensional sphere which is parameterized by a neural network up to a normalization constant. We provide a new algorithm for the setting of supervised learning and show its convergence both theoretically and numerically. We also provide the first proof of convergence for the unsupervised setting, which corresponds to the widely used variational Monte Carlo (VMC) method in quantum physics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24418;&#24335;&#21270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#30740;&#31350;&#36235;&#21183;&#65292;&#35752;&#35770;&#20102;&#33258;&#21160;&#35774;&#35745;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06532</link><description>&lt;p&gt;
&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Automated Design of Metaheuristic Algorithms. (arXiv:2303.06532v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24418;&#24335;&#21270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#30740;&#31350;&#36235;&#21183;&#65292;&#35752;&#35770;&#20102;&#33258;&#21160;&#35774;&#35745;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, and discusses the potential future directions and open issues in this field.
&lt;/p&gt;
&lt;p&gt;
&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30001;&#20110;&#20854;&#33021;&#22815;&#29420;&#31435;&#20110;&#38382;&#39064;&#32467;&#26500;&#21644;&#38382;&#39064;&#39046;&#22495;&#36827;&#34892;&#25628;&#32034;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36890;&#24120;&#65292;&#38656;&#35201;&#20154;&#31867;&#19987;&#23478;&#25163;&#21160;&#35843;&#25972;&#31639;&#27861;&#20197;&#36866;&#24212;&#35299;&#20915;&#30446;&#26631;&#38382;&#39064;&#12290;&#25163;&#21160;&#35843;&#25972;&#36807;&#31243;&#21487;&#33021;&#26159;&#36153;&#21147;&#30340;&#12289;&#23481;&#26131;&#20986;&#38169;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#24341;&#36215;&#20102;&#23545;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#21644;&#38656;&#27714;&#65292;&#20197;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;&#33258;&#21160;&#35774;&#35745;&#21487;&#20197;&#20351;&#39640;&#24615;&#33021;&#31639;&#27861;&#23545;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21487;&#29992;&#65307;&#36890;&#36807;&#21033;&#29992;&#35745;&#31639;&#33021;&#21147;&#26469;&#20805;&#20998;&#25506;&#32034;&#28508;&#22312;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#33258;&#21160;&#35774;&#35745;&#21487;&#20197;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#30340;&#35774;&#35745;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#24037;&#20316;&#30340;&#20849;&#21516;&#28857;&#21644;&#24046;&#24322;&#36827;&#34892;&#35843;&#26597;&#65292;&#25552;&#20986;&#20102;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24418;&#24335;&#21270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#30740;&#31350;&#36235;&#21183;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaheuristic algorithms have attracted wide attention from academia and industry due to their capability of conducting search independent of problem structures and problem domains. Often, human experts are requested to manually tailor algorithms to fit for solving a targeted problem. The manual tailoring process may be laborious, error-prone, and require intensive specialized knowledge. This gives rise to increasing interests and demands for automated design of metaheuristic algorithms with less human intervention. The automated design could make high-performance algorithms accessible to a much broader range of researchers and practitioners; and by leveraging computing power to fully explore the potential design choices, automated design could reach or even surpass human-level design. This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, by conducting a survey on the common grounds and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#38543;&#26426;&#36807;&#31243;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20132;&#26367;&#21327;&#26041;&#24046;&#20272;&#35745;&#21644;&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#26469;&#34913;&#37327;&#22810;&#21464;&#37327;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#12290;</title><link>http://arxiv.org/abs/2212.04631</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#20989;&#25968;&#65306;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#38543;&#26426;&#36807;&#31243;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Normalized Cross Density Functional: A Framework to Quantify Statistical Dependence for Random Processes. (arXiv:2212.04631v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#38543;&#26426;&#36807;&#31243;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20132;&#26367;&#21327;&#26041;&#24046;&#20272;&#35745;&#21644;&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#26469;&#34913;&#37327;&#22810;&#21464;&#37327;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38463;&#23572;&#24343;&#38647;&#24503;&#183;&#38647;&#23612;&#65288;Alfr\'ed R\'enyi&#65289;&#30340;&#21151;&#33021;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#36830;&#32493;&#38543;&#26426;&#36807;&#31243;&#65288;r.p.&#65289;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#26032;&#39062;&#30340;&#22810;&#21464;&#37327;&#23450;&#20041;&#12290;&#23558;&#38543;&#26426;&#36807;&#31243;&#26679;&#26412;&#23545;&#30340;&#20114;&#20449;&#24687;&#30340;&#23545;&#25968;&#35770;&#35777;&#21629;&#21517;&#20026;&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#65288;NCD&#65289;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#23545;&#31216;&#21644;&#33258;&#20276;&#30340;&#27491;&#23450;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#26368;&#22823;&#21270;&#20132;&#26367;&#21327;&#26041;&#24046;&#20272;&#35745;&#65288;ACE&#65289;&#36882;&#24402;&#24212;&#29992;&#20110;&#36755;&#20837;&#26679;&#26412;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#65292;&#31526;&#21512;&#38647;&#23612;&#30340;&#26368;&#22823;&#30456;&#20851;&#24615;&#30340;&#25152;&#26377;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NCD&#30340;&#29305;&#24449;&#35889;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#21464;&#37327;&#24230;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;r.p.&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#12290;&#21033;&#29992;r.p.&#30340;&#23454;&#29616;&#65292;&#20063;&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#22810;&#21464;&#37327;&#32479;&#35745;&#20381;&#36182;&#24615;&#12290;&#25552;&#20986;&#30340;&#21151;&#33021;&#26368;&#22823;&#30456;&#20851;&#31639;&#27861;&#65288;FMCA&#65289;&#24212;&#29992;&#20110;&#30001;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#19978;&#65292;&#36890;&#36807;&#36924;&#36817;&#32852;&#21512;&#35757;&#32451;&#26469;&#21516;&#26102;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel multivariate definition of statistical dependence between two continuous random processes (r.p.) using a functional methodology inspired by Alfr\'ed R\'enyi. The argument of the logarithm of mutual information between pairs of samples of a r.p., named here the normalized cross density (NCD), defines a symmetric and self-adjoint positive definite function. We show that maximizing the alternating covariance estimation (ACE) recursion, applied to each of the joint probability density of input sample pairs, obeys all the properties of Renyi's maximal correlation. We propose the NCD's eigenspectrum as a novel multivariate measure of the statistical dependence between the input and output r.p.  The multivariate statistical dependence can also be estimated directly from r.p. realizations. The proposed functional maximum correlation algorithm (FMCA) is applied to a machine learning architecture built from two neural networks that learn concurrently by approximating 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25903;&#25345;3D&#29702;&#35299;&#20219;&#21153;&#30340;&#25193;&#25955;&#27169;&#22411;RenderDiffusion&#65292;&#21482;&#38656;&#20351;&#29992;&#21333;&#30524;2D&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21435;&#22122;&#26550;&#26500;&#65292;&#29983;&#25104;&#21644;&#28210;&#26579;&#20013;&#38388;&#30340;&#19977;&#32500;&#34920;&#31034;&#65292;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;3D&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09869</link><description>&lt;p&gt;
RenderDiffusion: &#29992;&#20110;3D&#37325;&#24314;&#12289;&#20462;&#22797;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RenderDiffusion: Image Diffusion for 3D Reconstruction, Inpainting and Generation. (arXiv:2211.09869v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25903;&#25345;3D&#29702;&#35299;&#20219;&#21153;&#30340;&#25193;&#25955;&#27169;&#22411;RenderDiffusion&#65292;&#21482;&#38656;&#20351;&#29992;&#21333;&#30524;2D&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21435;&#22122;&#26550;&#26500;&#65292;&#29983;&#25104;&#21644;&#28210;&#26579;&#20013;&#38388;&#30340;&#19977;&#32500;&#34920;&#31034;&#65292;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#25552;&#20379;&#24378;&#26377;&#21147;&#30340;3D&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#26377;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#22343;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#25903;&#25345;&#29992;&#20110;3D&#29702;&#35299;&#25152;&#38656;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#35270;&#35282;&#19968;&#33268;&#30340;3D&#29983;&#25104;&#25110;&#21333;&#35270;&#35282;&#29289;&#20307;&#37325;&#24314;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RenderDiffusion&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;3D&#29983;&#25104;&#21644;&#25512;&#26029;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21482;&#38656;&#20351;&#29992;&#21333;&#30524;2D&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#12290;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#20687;&#21435;&#22122;&#26550;&#26500;&#65292;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#29983;&#25104;&#21644;&#28210;&#26579;&#22330;&#26223;&#30340;&#20013;&#38388;&#19977;&#32500;&#34920;&#31034;&#12290;&#36825;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#24378;&#21046;&#23454;&#29616;&#20102;&#19968;&#20010;&#24378;&#30340;&#24402;&#32435;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;3D&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;2D&#30417;&#30563;&#12290;&#29983;&#25104;&#30340;3D&#34920;&#31034;&#21487;&#20197;&#20174;&#20219;&#20309;&#35270;&#35282;&#28210;&#26579;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;RenderDiffusion&#22312;FFHQ&#12289;AFHQ&#12289;ShapeNet&#21644;CLEVR&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#20102;&#22312;&#29983;&#25104;3D&#22330;&#26223;&#21644;&#20174;2D&#22270;&#20687;&#25512;&#26029;3D&#22330;&#26223;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models currently achieve state-of-the-art performance for both conditional and unconditional image generation. However, so far, image diffusion models do not support tasks required for 3D understanding, such as view-consistent 3D generation or single-view object reconstruction. In this paper, we present RenderDiffusion, the first diffusion model for 3D generation and inference, trained using only monocular 2D supervision. Central to our method is a novel image denoising architecture that generates and renders an intermediate three-dimensional representation of a scene in each denoising step. This enforces a strong inductive structure within the diffusion process, providing a 3D consistent representation while only requiring 2D supervision. The resulting 3D representation can be rendered from any view. We evaluate RenderDiffusion on FFHQ, AFHQ, ShapeNet and CLEVR datasets, showing competitive performance for generation of 3D scenes and inference of 3D scenes from 2D images. Ad
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38598;&#25104;&#36816;&#21160;&#21644;&#36830;&#32493;&#24615;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#29289;&#20307;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#20013;&#20135;&#29983;&#26356;&#20248;&#30340;&#29289;&#20307;&#32534;&#30721;&#65292;&#24182;&#22312;&#29289;&#20307;&#21457;&#29616;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#24635;&#20307;&#28508;&#22312;&#29289;&#20307;&#34920;&#31034;&#31561;&#26041;&#38754;&#21462;&#24471;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.09771</link><description>&lt;p&gt;
&#36890;&#36807;&#36816;&#21160;&#21644;&#29289;&#20307;&#36830;&#32493;&#24615;&#25552;&#21319;&#29289;&#20307;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Object Representation Learning via Motion and Object Continuity. (arXiv:2211.09771v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09771
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#36816;&#21160;&#21644;&#36830;&#32493;&#24615;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#29289;&#20307;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#20013;&#20135;&#29983;&#26356;&#20248;&#30340;&#29289;&#20307;&#32534;&#30721;&#65292;&#24182;&#22312;&#29289;&#20307;&#21457;&#29616;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#24635;&#20307;&#28508;&#22312;&#29289;&#20307;&#34920;&#31034;&#31561;&#26041;&#38754;&#21462;&#24471;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25913;&#36827;&#65292;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#26032;&#39062;&#30340;&#26550;&#26500;&#20559;&#35265;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#21487;&#33021;&#20026;&#19979;&#28216;&#20219;&#21153;&#20135;&#29983;&#27425;&#20248;&#30340;&#29289;&#20307;&#32534;&#30721;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#29289;&#20307;&#36816;&#21160;&#21644;&#36830;&#32493;&#24615;&#65292;&#21363;&#29289;&#20307;&#19981;&#33021;&#31361;&#28982;&#20986;&#29616;&#21644;&#28040;&#22833;&#12290;&#36825;&#36890;&#36807;&#20004;&#20010;&#26426;&#21046;&#23454;&#29616;&#65306;&#65288;i&#65289;&#36890;&#36807;&#38598;&#25104;&#20809;&#27969;&#25552;&#20379;&#29289;&#20307;&#20301;&#32622;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#65288;ii&#65289;&#23545;&#36830;&#32493;&#22270;&#20687;&#24103;&#20043;&#38388;&#30340;&#23545;&#27604;&#29289;&#20307;&#36830;&#32493;&#24615;&#25439;&#22833;&#36827;&#34892;&#24314;&#27169;&#12290;&#19982;&#24320;&#21457;&#26174;&#24335;&#28145;&#24230;&#26550;&#26500;&#19981;&#21516;&#65292;&#24471;&#21040;&#30340;&#36816;&#21160;&#21644;&#29289;&#20307;&#36830;&#32493;&#24615;&#65288;MOC&#65289;&#26041;&#26696;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#22522;&#32447;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29609;Atari&#28216;&#25103;&#32780;&#35328;&#65292;SOTA&#27169;&#22411;&#22312;&#29289;&#20307;&#21457;&#29616;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#24635;&#20307;&#28508;&#22312;&#29289;&#20307;&#34920;&#31034;&#26041;&#38754;&#37117;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25972;&#21512;&#36816;&#21160;&#21644;&#36830;&#32493;&#24615;&#30340;&#26126;&#26174;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent unsupervised multi-object detection models have shown impressive performance improvements, largely attributed to novel architectural inductive biases. Unfortunately, they may produce suboptimal object encodings for downstream tasks. To overcome this, we propose to exploit object motion and continuity, i.e., objects do not pop in and out of existence. This is accomplished through two mechanisms: (i) providing priors on the location of objects through integration of optical flow, and (ii) a contrastive object continuity loss across consecutive image frames. Rather than developing an explicit deep architecture, the resulting Motion and Object Continuity (MOC) scheme can be instantiated using any baseline object detection model. Our results show large improvements in the performances of a SOTA model in terms of object discovery, convergence speed and overall latent object representations, particularly for playing Atari games. Overall, we show clear benefits of integrating motion and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ODE&#26041;&#27861;&#30340;&#28176;&#36817;&#32479;&#35745;&#26041;&#27861;&#35299;&#20915;$d$&#32500;&#38543;&#26426;&#36924;&#36817;&#36882;&#24402;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2110.14427</link><description>&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#28176;&#36817;&#32479;&#35745;&#30340;ODE&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The ODE Method for Asymptotic Statistics in Stochastic Approximation and Reinforcement Learning. (arXiv:2110.14427v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ODE&#26041;&#27861;&#30340;&#28176;&#36817;&#32479;&#35745;&#26041;&#27861;&#35299;&#20915;$d$&#32500;&#38543;&#26426;&#36924;&#36817;&#36882;&#24402;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;$d$&#32500;&#38543;&#26426;&#36924;&#36817;&#36882;&#24402;$$\theta_{n+1}=\theta_n+\alpha_{n+1}f(\theta_n, \Phi_{n+1})$$&#20854;&#20013;$\Phi$&#26159;&#19968;&#20010;&#22312;&#19968;&#33324;&#29366;&#24577;&#31354;&#38388;$\textsf{X}$&#19978;&#20855;&#26377;&#24179;&#31283;&#20998;&#24067;$\pi$&#30340;&#20960;&#20309;&#36941;&#21382;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;$f&#65306;\Re^d\times\textsf{X}\to\Re^d$&#12290;&#22312;&#31216;&#20026;&#65288;DV3&#65289;&#30340;Donsker-Varadhan Lyapunov&#28418;&#31227;&#26465;&#20214;&#30340;&#19968;&#31181;&#29256;&#26412;&#21644;&#23545;&#20855;&#26377;&#21521;&#37327;&#22330;$\bar{f}(\theta)=\textsf{E}[f(\theta,\Phi)]$&#20197;&#21450;$\Phi\sim\pi$&#30340;&#22343;&#20540;&#27969;&#30340;&#31283;&#23450;&#24615;&#26465;&#20214;&#19979;&#65292;&#24314;&#31435;&#20102;&#20027;&#35201;&#32467;&#26524;&#12290;(i) $\{\theta_n\}$&#20197;&#27010;&#29575;1&#21644;$L_4$&#25910;&#25947;&#20110;$\bar{f}(\theta)$&#30340;&#21807;&#19968;&#26681;$\theta^*$&#12290;(ii) &#24314;&#31435;&#20102;&#27867;&#20989;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#20197;&#21450;&#24402;&#19968;&#21270;&#35823;&#24046;&#19968;&#32500;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;(iii) &#23545;&#20110;&#24402;&#19968;&#21270;&#29256;&#26412;$z_n{=:} \sqrt{n} (\theta^{\text{PR}}_n -\theta^*)$&#30340;&#24179;&#22343;&#21442;&#25968;$\theta^{\text{PR}}_n {=:} n^{-1} \sum_{k=1}^n\theta_k$ &#65292;&#22312;&#27493;&#38271;&#30340;&#26631;&#20934;&#20551;&#35774;&#19979;&#65292;&#24314;&#31435;&#20102;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper concerns the $d$-dimensional stochastic approximation recursion, $$ \theta_{n+1}= \theta_n + \alpha_{n + 1} f(\theta_n, \Phi_{n+1}) $$ in which $\Phi$ is a geometrically ergodic Markov chain on a general state space $\textsf{X}$ with stationary distribution $\pi$, and $f:\Re^d\times\textsf{X}\to\Re^d$.  The main results are established under a version of the Donsker-Varadhan Lyapunov drift condition known as (DV3), and a stability condition for the mean flow with vector field $\bar{f}(\theta)=\textsf{E}[f(\theta,\Phi)]$, with $\Phi\sim\pi$.  (i) $\{ \theta_n\}$ is convergent a.s. and in $L_4$ to the unique root $\theta^*$ of $\bar{f}(\theta)$.  (ii) A functional CLT is established, as well as the usual one-dimensional CLT for the normalized error.  (iii) The CLT holds for the normalized version, $z_n{=:} \sqrt{n} (\theta^{\text{PR}}_n -\theta^*)$, of the averaged parameters, $\theta^{\text{PR}}_n {=:} n^{-1} \sum_{k=1}^n\theta_k$, subject to standard assumptions on the step-s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36991;&#20813;&#22312;&#21487;&#33021;&#29359;&#38169;&#35823;&#26102;&#20570;&#20986;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#20013;&#36991;&#20813;&#20005;&#37325;&#21518;&#26524;&#12290;&#35843;&#26597;&#20171;&#32461;&#20102;&#25298;&#32477;&#36873;&#39033;&#30340;&#26465;&#20214;&#12289;&#35780;&#20272;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2107.11277</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Learning with a Reject Option: A survey. (arXiv:2107.11277v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.11277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36991;&#20813;&#22312;&#21487;&#33021;&#29359;&#38169;&#35823;&#26102;&#20570;&#20986;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#20013;&#36991;&#20813;&#20005;&#37325;&#21518;&#26524;&#12290;&#35843;&#26597;&#20171;&#32461;&#20102;&#25298;&#32477;&#36873;&#39033;&#30340;&#26465;&#20214;&#12289;&#35780;&#20272;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24635;&#26159;&#20570;&#20986;&#39044;&#27979;&#65292;&#21363;&#20351;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#22312;&#35768;&#22810;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#20013;&#65292;&#24212;&#36991;&#20813;&#36825;&#31181;&#34892;&#20026;&#65292;&#22240;&#20026;&#38169;&#35823;&#21487;&#33021;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#12290;&#23613;&#31649;&#22312;1970&#24180;&#24050;&#32463;&#30740;&#31350;&#36807;&#65292;&#20294;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#26426;&#22120;&#23398;&#20064;&#23376;&#39046;&#22495;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#21487;&#33021;&#29359;&#38169;&#35823;&#26102;&#36991;&#20813;&#20570;&#20986;&#39044;&#27979;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#20013;&#25298;&#32477;&#36873;&#39033;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#23548;&#33268;&#20004;&#31181;&#25298;&#32477;&#24773;&#20917;&#65288;&#27169;&#31946;&#21644;&#26032;&#22855;&#25298;&#32477;&#65289;&#30340;&#26465;&#20214;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#24418;&#24335;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#21644;&#20998;&#31867;&#20102;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#21644;&#25298;&#32477;&#36136;&#37327;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#29616;&#26377;&#30340;&#24102;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#25551;&#36848;&#20102;&#23398;&#20064;&#36825;&#20123;&#27169;&#22411;&#30340;&#26631;&#20934;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#24212;&#29992;&#39046;&#22495;&#30340;&#31034;&#20363;&#65292;&#24182;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake.  This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machi
&lt;/p&gt;</description></item></channel></rss>