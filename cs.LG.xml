<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Point-Bind&#21644;Point-LLM&#26159;&#29992;&#20110;3D&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25351;&#23548;&#36319;&#38543;&#30340;&#22810;&#27169;&#24577;&#28857;&#20113;&#23545;&#40784;&#27169;&#22411;&#65292;&#33021;&#23454;&#29616;&#20219;&#24847;&#21040;3D&#29983;&#25104;&#12289;3D&#23884;&#20837;&#31639;&#26415;&#21644;3D&#24320;&#25918;&#19990;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#19988;Point-LLM&#33021;&#23454;&#29616;3D&#21644;&#22810;&#27169;&#24577;&#38382;&#31572;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00615</link><description>&lt;p&gt;
Point-Bind&#21644;Point-LLM&#65306;&#29992;&#20110;3D&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25351;&#23548;&#36319;&#38543;&#30340;&#22810;&#27169;&#24577;&#28857;&#20113;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Point-Bind &amp; Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following. (arXiv:2309.00615v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00615
&lt;/p&gt;
&lt;p&gt;
Point-Bind&#21644;Point-LLM&#26159;&#29992;&#20110;3D&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#25351;&#23548;&#36319;&#38543;&#30340;&#22810;&#27169;&#24577;&#28857;&#20113;&#23545;&#40784;&#27169;&#22411;&#65292;&#33021;&#23454;&#29616;&#20219;&#24847;&#21040;3D&#29983;&#25104;&#12289;3D&#23884;&#20837;&#31639;&#26415;&#21644;3D&#24320;&#25918;&#19990;&#30028;&#30340;&#29702;&#35299;&#65292;&#24182;&#19988;Point-LLM&#33021;&#23454;&#29616;3D&#21644;&#22810;&#27169;&#24577;&#38382;&#31572;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Point-Bind&#65292;&#19968;&#20010;&#23558;&#28857;&#20113;&#19982;2D&#22270;&#20687;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#23545;&#40784;&#30340;3D&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#22312;ImageBind&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23558;3D&#21644;&#22810;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#35768;&#22810;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#20219;&#24847;&#21040;3D&#29983;&#25104;&#12289;3D&#23884;&#20837;&#31639;&#26415;&#21644;3D&#24320;&#25918;&#19990;&#30028;&#30340;&#29702;&#35299;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Point-LLM&#65292;&#31532;&#19968;&#20010;&#36981;&#24490;3D&#22810;&#27169;&#24577;&#25351;&#20196;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#35843;&#20248;&#25216;&#26415;&#65292;Point-LLM&#23558;Point-Bind&#30340;&#35821;&#20041;&#27880;&#20837;&#21040;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#65292;&#20363;&#22914;LLaMA&#65292;&#19981;&#38656;&#35201;3D&#25351;&#20196;&#25968;&#25454;&#20294;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;3D&#21644;&#22810;&#27169;&#24577;&#38382;&#31572;&#33021;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;&#23558;3D&#28857;&#20113;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#21551;&#31034;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/ZiyuGuo99/Point-Bind_Point-LLM&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#36890;&#36807;&#35780;&#20272;&#22522;&#32447;&#38450;&#24481;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#31574;&#30053;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2309.00614</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#22522;&#32447;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Baseline Defenses for Adversarial Attacks Against Aligned Language Models. (arXiv:2309.00614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00614
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#23545;&#25239;&#25915;&#20987;&#38382;&#39064;&#65292;&#36890;&#36807;&#35780;&#20272;&#22522;&#32447;&#38450;&#24481;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#31574;&#30053;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#23545;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#20854;&#23433;&#20840;&#28431;&#27934;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25991;&#26412;&#20248;&#21270;&#22120;&#21487;&#20197;&#29983;&#25104;&#32469;&#36807;&#23457;&#26597;&#21644;&#23545;&#40784;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;&#20511;&#37492;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#20016;&#23500;&#30740;&#31350;&#25104;&#26524;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#38382;&#39064;&#20837;&#25163;&#65306;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20160;&#20040;&#26679;&#30340;&#23041;&#32961;&#27169;&#22411;&#26159;&#23454;&#29992;&#30340;&#65311;&#22522;&#32447;&#38450;&#24481;&#25216;&#26415;&#22312;&#36825;&#20010;&#26032;&#39046;&#22495;&#20013;&#34920;&#29616;&#22914;&#20309;&#65311;LLM&#23433;&#20840;&#24615;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#26377;&#20309;&#19981;&#21516;&#65311;&#25105;&#20204;&#23545;&#20027;&#23548;&#23545;&#25239;LLM&#25915;&#20987;&#30340;&#20960;&#31181;&#22522;&#32447;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#35780;&#20272;&#65292;&#35752;&#35770;&#20102;&#27599;&#31181;&#31574;&#30053;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#19977;&#31181;&#31867;&#22411;&#30340;&#38450;&#24481;&#65306;&#26816;&#27979;&#65288;&#22522;&#20110;&#22256;&#24785;&#24230;&#65289;&#12289;&#36755;&#20837;&#39044;&#22788;&#29702;&#65288;&#25913;&#20889;&#21644;&#37325;&#26032;&#26631;&#35760;&#21270;&#65289;&#21644;&#23545;&#25239;&#35757;&#32451;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#30333;&#30418;&#21644;&#28784;&#30418;&#35774;&#32622;&#65292;&#24182;&#35752;&#35770;&#20102;&#27599;&#31181;&#32771;&#34385;&#30340;&#38450;&#24481;&#31574;&#30053;&#22312;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models quickly become ubiquitous, their security vulnerabilities are critical to understand. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?  We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. Surprisingly, we find much more succ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMILIE&#30340;&#36845;&#20195;&#22810;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36845;&#20195;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#26799;&#24230;&#25511;&#21046;&#25805;&#20316;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#32534;&#36753;&#30340;&#31890;&#24230;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.00613</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#36845;&#20195;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Iterative Multi-granular Image Editing using Diffusion Models. (arXiv:2309.00613v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMILIE&#30340;&#36845;&#20195;&#22810;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36845;&#20195;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#26799;&#24230;&#25511;&#21046;&#25805;&#20316;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#32534;&#36753;&#30340;&#31890;&#24230;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#21019;&#24847;&#19987;&#19994;&#20154;&#21592;&#29983;&#25104;&#33402;&#26415;&#21644;&#23457;&#32654;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#35270;&#35273;&#36164;&#20135;&#30340;&#26041;&#24335;&#12290;&#20026;&#20102;&#20805;&#20998;&#25903;&#25345;&#36825;&#26679;&#30340;&#21019;&#24847;&#21162;&#21147;&#65292;&#35813;&#36807;&#31243;&#24212;&#20855;&#22791;&#20197;&#19979;&#33021;&#21147;&#65306;1&#65289;&#36845;&#20195;&#22320;&#32534;&#36753;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;2&#65289;&#25511;&#21046;&#25152;&#38656;&#21464;&#21270;&#30340;&#31354;&#38388;&#33539;&#22260;&#65288;&#20840;&#23616;&#12289;&#23616;&#37096;&#25110;&#20171;&#20110;&#20004;&#32773;&#20043;&#38388;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#23454;&#29992;&#30340;&#38382;&#39064;&#35774;&#23450;&#27491;&#24335;&#21270;&#20026;&#36845;&#20195;&#22810;&#31890;&#24230;&#32534;&#36753;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#21512;&#25104;&#21644;&#32534;&#36753;&#26041;&#38754;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#19968;&#27425;&#24615;&#25805;&#20316;&#65288;&#21363;&#27809;&#26377;&#36845;&#20195;&#32534;&#36753;&#33021;&#21147;&#65289;&#65292;&#24182;&#19988;&#19981;&#33021;&#33258;&#28982;&#20135;&#29983;&#22810;&#31890;&#24230;&#25511;&#21046;&#65288;&#21363;&#28085;&#30422;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#32534;&#36753;&#30340;&#20840;&#35889;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EMILIE&#65306;&#36845;&#20195;&#22810;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#22120;&#12290;EMILIE&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#36845;&#20195;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#20419;&#36827;&#36845;&#20195;&#32534;&#36753;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#26799;&#24230;&#25511;&#21046;&#25805;&#20316;&#26469;&#23454;&#29616;&#23545;&#25152;&#38656;&#21464;&#21270;&#30340;&#31890;&#24230;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control opera
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#20462;&#27491;&#24341;&#21147;&#27169;&#25311;&#20013;&#25552;&#21462;&#23431;&#23449;&#23398;&#21442;&#25968;&#65292;&#24182;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.00612</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#23431;&#23449;&#23610;&#24230;&#20013;&#30340;&#20462;&#27491;&#24341;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bayesian deep learning for cosmic volumes with modified gravity. (arXiv:2309.00612v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#20462;&#27491;&#24341;&#21147;&#27169;&#25311;&#20013;&#25552;&#21462;&#23431;&#23449;&#23398;&#21442;&#25968;&#65292;&#24182;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#19968;&#20195;&#30340;&#26143;&#31995;&#35843;&#26597;&#23558;&#25552;&#20379;&#21069;&#25152;&#26410;&#26377;&#30340;&#25968;&#25454;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#23431;&#23449;&#23610;&#24230;&#19978;&#27979;&#35797;&#24341;&#21147;&#12290;&#23545;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#20581;&#22766;&#23431;&#23449;&#23398;&#20998;&#26512;&#38656;&#35201;&#21033;&#29992;&#32534;&#30721;&#22312;&#23431;&#23449;&#32593;&#20013;&#30340;&#38750;&#32447;&#24615;&#20449;&#24687;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#24037;&#20855;&#65292;&#28982;&#32780;&#21364;&#19981;&#33021;&#25552;&#20379;&#20808;&#39564;&#30340;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#20462;&#27491;&#24341;&#21147;&#65288;MG&#65289;&#27169;&#25311;&#20013;&#25552;&#21462;&#23431;&#23449;&#23398;&#21442;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#23454;&#29616;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#65292;&#20998;&#21035;&#32771;&#34385;&#20102;&#19968;&#20010;&#24102;&#26377;&#21333;&#19968;&#36125;&#21494;&#26031;&#26368;&#21518;&#19968;&#23618;&#65288;BLL&#65289;&#30340;&#24773;&#20917;&#65292;&#21644;&#19968;&#20010;&#22312;&#25152;&#26377;&#23618;&#38754;&#19978;&#37117;&#20855;&#26377;&#36125;&#21494;&#26031;&#23618;&#65288;FullB&#65289;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#31354;&#38388;&#23494;&#24230;&#22330;&#21644;&#19968;&#22871;2000&#20010;&#20165;&#21253;&#21547;&#26263;&#29289;&#36136;&#31890;&#23376;&#32593;&#26684;$ N $-&#20307;&#27169;&#25311;&#30340;&#21151;&#29575;&#35889;&#23545;&#36825;&#20004;&#20010;BNN&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#27169;&#25311;&#21253;&#25324;&#22522;&#20110;MG-PICOLA&#30340;&#20462;&#27491;&#24341;&#21147;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;&#36793;&#38271;&#20026;256 $h^{-1}$ Mpc&#30340;&#31435;&#26041;&#20307;&#20307;&#31215;&#65292;&#20854;&#20013;&#21253;&#21547;128$&#12290;
&lt;/p&gt;
&lt;p&gt;
The new generation of galaxy surveys will provide unprecedented data allowing us to test gravity at cosmological scales. A robust cosmological analysis of the large-scale structure demands exploiting the nonlinear information encoded in the cosmic web. Machine Learning techniques provide such tools, however, do not provide a priori assessment of uncertainties. This study aims at extracting cosmological parameters from modified gravity (MG) simulations through deep neural networks endowed with uncertainty estimations. We implement Bayesian neural networks (BNNs) with an enriched approximate posterior distribution considering two cases: one with a single Bayesian last layer (BLL), and another one with Bayesian layers at all levels (FullB). We train both BNNs with real-space density fields and power-spectra from a suite of 2000 dark matter only particle mesh $N$-body simulations including modified gravity models relying on MG-PICOLA covering 256 $h^{-1}$ Mpc side cubical volumes with 128$
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#23436;&#25104;&#24341;&#25806;&#26469;&#36827;&#19968;&#27493;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#20013;&#21512;&#25104;&#26356;&#22810;&#26377;&#25928;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.00608</link><description>&lt;p&gt;
Copiloting the Copilots: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23436;&#25104;&#24341;&#25806;&#34701;&#21512;&#29992;&#20110;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair. (arXiv:2309.00608v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00608
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#21033;&#29992;&#23436;&#25104;&#24341;&#25806;&#26469;&#36827;&#19968;&#27493;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#20013;&#21512;&#25104;&#26356;&#22810;&#26377;&#25928;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#20013;&#65292;&#23545;&#20110;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#23454;&#38469;&#31995;&#32479;&#21512;&#25104;&#27491;&#30830;&#30340;&#20462;&#34917;&#31243;&#24207;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#24320;&#21457;&#20154;&#21592;&#22312;&#21508;&#31181;&#32534;&#30721;&#20219;&#21153;&#20013;&#20855;&#26377;&#24110;&#21161;&#65292;&#24182;&#19988;&#24050;&#30452;&#25509;&#24212;&#29992;&#20110;&#20462;&#34917;&#31243;&#24207;&#30340;&#21512;&#25104;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;LLMs&#23558;&#31243;&#24207;&#35270;&#20026;&#20196;&#29260;&#24207;&#21015;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#23545;&#30446;&#26631;&#32534;&#31243;&#35821;&#35328;&#30340;&#24213;&#23618;&#35821;&#20041;&#32422;&#26463;&#19968;&#26080;&#25152;&#30693;&#12290;&#36825;&#23548;&#33268;&#29983;&#25104;&#20102;&#22823;&#37327;&#38745;&#24577;&#26080;&#25928;&#30340;&#20462;&#34917;&#31243;&#24207;&#65292;&#38459;&#30861;&#20102;&#35813;&#25216;&#26415;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Repilot&#65292;&#19968;&#31181;&#22312;&#20462;&#22797;&#36807;&#31243;&#20013;&#36890;&#36807;&#21512;&#25104;&#26356;&#22810;&#26377;&#25928;&#20462;&#34917;&#31243;&#24207;&#20174;&#32780;&#36827;&#19968;&#27493;&#25903;&#25345;AI&#8220;&#21103;&#39550;&#39542;&#21592;&#8221;&#65288;&#21363;LLMs&#65289;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#35768;&#22810;LLMs&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#36755;&#20986;&#65288;&#21363;&#36880;&#20010;&#20196;&#29260;&#29983;&#25104;&#65289;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#32534;&#20889;&#31243;&#24207;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#23436;&#25104;&#24341;&#25806;&#26174;&#33879;&#25552;&#21319;&#21644;&#24341;&#23548;&#12290;Repilot&#21327;&#21516;&#21512;&#25104;&#20102;&#20462;&#34917;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful "copilots" in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI "copilots" (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; ROBAI &#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#24555;&#36895;&#35782;&#21035;&#24182;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#36830;&#32493;&#22238;&#21512;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#35813;&#31639;&#27861;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#21644;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#35201;&#27714;&#19979;&#22343;&#23454;&#29616;&#20102;&#28176;&#36827;&#26368;&#20248;&#36951;&#25022;&#65292;&#24182;&#19988;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#19979;&#20165;&#38656; $\mathcal{O}(\log T)$ &#22238;&#21512;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#22312;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#19979;&#20165;&#38656; $\mathcal{O}(\log^2 T)$ &#22238;&#21512;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#33218;&#12290;</title><link>http://arxiv.org/abs/2309.00591</link><description>&lt;p&gt;
&#24555;&#36895;&#21644;&#36951;&#25022;&#26368;&#23567;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65306;&#22522;&#26412;&#38480;&#21046;&#21644;&#20302;&#22797;&#26434;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms. (arXiv:2309.00591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; ROBAI &#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#24555;&#36895;&#35782;&#21035;&#24182;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#36830;&#32493;&#22238;&#21512;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#35813;&#31639;&#27861;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#21644;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#35201;&#27714;&#19979;&#22343;&#23454;&#29616;&#20102;&#28176;&#36827;&#26368;&#20248;&#36951;&#25022;&#65292;&#24182;&#19988;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#19979;&#20165;&#38656; $\mathcal{O}(\log T)$ &#22238;&#21512;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#22312;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#19979;&#20165;&#38656; $\mathcal{O}(\log^2 T)$ &#22238;&#21512;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#33218;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#21452;&#37325;&#30446;&#26631;&#30340;&#38543;&#26426;&#22810;&#33218;&#32769;&#34382;&#26426;(MAB)&#38382;&#39064;&#65306;(i) &#24555;&#36895;&#35782;&#21035;&#24182;&#36873;&#25321;&#26368;&#20339;&#33218;&#65292;&#20197;&#21450;(ii) &#22312;&#19968;&#31995;&#21015;T&#20010;&#36830;&#32493;&#22238;&#21512;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#23613;&#31649;&#27599;&#20010;&#30446;&#26631;&#37117;&#24050;&#32463;&#24471;&#21040;&#20102;&#29420;&#31435;&#30340;&#28145;&#20837;&#30740;&#31350;&#65292;&#21363;(i)&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#21644;(ii)&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#20294;&#26159;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#20010;&#30446;&#26631;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#65292;&#23613;&#31649;&#23427;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#8220;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#8221;(ROBAI)&#65292;&#26088;&#22312;&#23454;&#29616;&#36825;&#20004;&#20010;&#21452;&#37325;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#20855;&#26377;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#21644;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#35201;&#27714;&#30340;ROBAI&#65292;&#25105;&#20204;&#20998;&#21035;&#25552;&#20986;&#20102;$\mathsf{EOCP}$&#31639;&#27861;&#21450;&#20854;&#21464;&#20307;&#65292;&#19981;&#20165;&#22312;&#39640;&#26031;&#32769;&#34382;&#26426;&#21644;&#19968;&#33324;&#32769;&#34382;&#26426;&#20013;&#36798;&#21040;&#20102;&#28176;&#36827;&#26368;&#20248;&#36951;&#25022;&#65292;&#32780;&#19988;&#22312;&#39044;&#23450;&#20572;&#27490;&#26102;&#38388;&#19979;&#65292;&#22312;$\mathcal{O}(\log T)$&#22238;&#21512;&#20869;&#36873;&#25321;&#20102;&#26368;&#20339;&#33218;&#65292;&#22312;&#33258;&#36866;&#24212;&#20572;&#27490;&#26102;&#38388;&#19979;&#65292;&#36873;&#25321;&#20102;&#26368;&#20339;&#33218;&#22312;$\mathcal{O}(\log^2 T)$&#22238;&#21512;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a stochastic multi-armed bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of $T$ consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces \emph{Regret Optimal Best Arm Identification} (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present the $\mathsf{EOCP}$ algorithm and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in $\mathcal{O}(\log T)$ rounds with pre-determined stopping time and $\mathcal{O}(\log^2 T)$ rounds with adaptive stopping ti
&lt;/p&gt;</description></item><item><title>PolyGET&#26159;&#19968;&#20010;&#26032;&#30340;&#32858;&#21512;&#29289;&#21147;&#22330;&#26694;&#26550;&#65292;&#20351;&#29992;&#31561;&#21464;&#25442;&#22120;&#27169;&#22411;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#37327;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#19981;&#21516;&#32858;&#21512;&#29289;&#23478;&#26063;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#21147;&#22330;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00585</link><description>&lt;p&gt;
PolyGET: &#20351;&#29992;&#31561;&#21464;&#25442;&#22120;&#36827;&#34892;&#20934;&#30830;&#36890;&#29992;&#30340;&#21147;&#22330;&#21152;&#36895;&#32858;&#21512;&#29289;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
PolyGET: Accelerating Polymer Simulations by Accurate and Generalizable Forcefield with Equivariant Transformer. (arXiv:2309.00585v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00585
&lt;/p&gt;
&lt;p&gt;
PolyGET&#26159;&#19968;&#20010;&#26032;&#30340;&#32858;&#21512;&#29289;&#21147;&#22330;&#26694;&#26550;&#65292;&#20351;&#29992;&#31561;&#21464;&#25442;&#22120;&#27169;&#22411;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#37327;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#19981;&#21516;&#32858;&#21512;&#29289;&#23478;&#26063;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#23427;&#36890;&#36807;&#20248;&#21270;&#21147;&#22330;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#21512;&#29289;&#27169;&#25311;&#21516;&#26102;&#20855;&#22791;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;(ML)&#21147;&#22330;&#20197;&#23454;&#29616;&#21407;&#23376;&#35770;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#32463;&#39564;&#21147;&#22330;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ML&#21147;&#22330;&#36890;&#24120;&#20165;&#38480;&#20110;&#21333;&#20998;&#23376;&#27169;&#25311;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#27169;&#25311;&#19981;&#22815;&#31283;&#20581;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PolyGET&#65292;&#19968;&#20010;&#26032;&#30340;&#32858;&#21512;&#29289;&#21147;&#22330;&#36890;&#29992;&#31561;&#21464;&#25442;&#22120;&#26694;&#26550;&#12290;PolyGET&#20351;&#29992;&#31216;&#20026;&#31561;&#21464;&#25442;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#25429;&#25417;&#21407;&#23376;&#20043;&#38388;&#30340;&#22797;&#26434;&#37327;&#23376;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36328;&#19981;&#21516;&#32858;&#21512;&#29289;&#23478;&#26063;&#36827;&#34892;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#19987;&#27880;&#20110;&#20248;&#21270;&#21147;&#65292;&#19982;&#29616;&#26377;&#30340;&#21516;&#26102;&#20248;&#21270;&#33021;&#37327;&#21644;&#21147;&#30340;&#26041;&#27861;&#19981;&#21516;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#20197;&#21147;&#20026;&#20013;&#24515;&#30340;&#30446;&#26631;&#20989;&#25968;&#36991;&#20813;&#20102;&#33021;&#37327;&#21644;&#21147;&#20043;&#38388;&#30340;&#31454;&#20105;&#30446;&#26631;&#65292;&#20174;&#32780;&#20801;&#35768;&#22312;&#19981;&#21516;&#30340;&#32858;&#21512;&#29289;&#19978;&#23398;&#20064;&#32479;&#19968;&#30340;&#21147;&#22330;ML&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polymer simulation with both accuracy and efficiency is a challenging task. Machine learning (ML) forcefields have been developed to achieve both the accuracy of ab initio methods and the efficiency of empirical force fields. However, existing ML force fields are usually limited to single-molecule settings, and their simulations are not robust enough. In this paper, we present PolyGET, a new framework for Polymer Forcefields with Generalizable Equivariant Transformers. PolyGET is designed to capture complex quantum interactions between atoms and generalize across various polymer families, using a deep learning model called Equivariant Transformers. We propose a new training paradigm that focuses exclusively on optimizing forces, which is different from existing methods that jointly optimize forces and energy. This simple force-centric objective function avoids competing objectives between energy and forces, thereby allowing for learning a unified forcefield ML model over different poly
&lt;/p&gt;</description></item><item><title>Laminar&#26159;&#19968;&#20010;&#26032;&#30340;&#26080;&#26381;&#21153;&#22120;&#27969;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#20195;&#30721;&#25628;&#32034;&#21644;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#22686;&#24378;&#20102;&#26694;&#26550;&#30340;&#21151;&#33021;&#65292;&#31616;&#21270;&#20102;&#27969;&#24335;&#35745;&#31639;&#30340;&#25191;&#34892;&#65292;&#26356;&#39640;&#25928;&#22320;&#31649;&#29702;&#25968;&#25454;&#27969;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.00584</link><description>&lt;p&gt;
Laminar:&#19968;&#31181;&#20855;&#26377;&#35821;&#20041;&#20195;&#30721;&#25628;&#32034;&#21644;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#30340;&#26032;&#22411;&#26080;&#26381;&#21153;&#22120;&#27969;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Laminar: A New Serverless Stream-based Framework with Semantic Code Search and Code Completion. (arXiv:2309.00584v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00584
&lt;/p&gt;
&lt;p&gt;
Laminar&#26159;&#19968;&#20010;&#26032;&#30340;&#26080;&#26381;&#21153;&#22120;&#27969;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#20195;&#30721;&#25628;&#32034;&#21644;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#22686;&#24378;&#20102;&#26694;&#26550;&#30340;&#21151;&#33021;&#65292;&#31616;&#21270;&#20102;&#27969;&#24335;&#35745;&#31639;&#30340;&#25191;&#34892;&#65292;&#26356;&#39640;&#25928;&#22320;&#31649;&#29702;&#25968;&#25454;&#27969;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Laminar&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;dispel4py&#30340;&#26032;&#22411;&#26080;&#26381;&#21153;&#22120;&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#20010;&#24182;&#34892;&#27969;&#24335;&#25968;&#25454;&#27969;&#24211;&#12290;Laminar&#36890;&#36807;&#19987;&#29992;&#30340;&#27880;&#20876;&#34920;&#39640;&#25928;&#22320;&#31649;&#29702;&#27969;&#31243;&#24037;&#20316;&#27969;&#21644;&#32452;&#20214;&#65292;&#25552;&#20379;&#20102;&#26080;&#32541;&#30340;&#26080;&#26381;&#21153;&#22120;&#20307;&#39564;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;Laminar&#36890;&#36807;&#35821;&#20041;&#20195;&#30721;&#25628;&#32034;&#12289;&#20195;&#30721;&#27010;&#36848;&#21644;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#25552;&#21319;&#20102;&#26694;&#26550;&#30340;&#21151;&#33021;&#12290;&#36825;&#39033;&#36129;&#29486;&#36890;&#36807;&#31616;&#21270;&#27969;&#24335;&#35745;&#31639;&#30340;&#25191;&#34892;&#12289;&#26356;&#39640;&#25928;&#22320;&#31649;&#29702;&#25968;&#25454;&#27969;&#65292;&#24182;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#22686;&#24378;&#20102;&#26080;&#26381;&#21153;&#22120;&#35745;&#31639;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Laminar, a novel serverless framework based on dispel4py, a parallel stream-based dataflow library. Laminar efficiently manages streaming workflows and components through a dedicated registry, offering a seamless serverless experience. Leveraging large lenguage models, Laminar enhances the framework with semantic code search, code summarization, and code completion. This contribution enhances serverless computing by simplifying the execution of streaming computations, managing data streams more efficiently, and offering a valuable tool for both researchers and practitioners.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;&#65288;GINO&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22823;&#35268;&#27169;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#31526;&#12290;GINO&#20351;&#29992;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#21644;&#28857;&#20113;&#34920;&#31034;&#26469;&#22788;&#29702;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#22270;&#21644;&#20613;&#37324;&#21494;&#32467;&#26500;&#30340;&#31070;&#32463;&#31639;&#23376;&#23454;&#29616;&#31163;&#25955;&#19968;&#33268;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;GINO&#22312;&#22823;&#35268;&#27169;&#27169;&#25311;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00583</link><description>&lt;p&gt;
&#22522;&#20110;&#20960;&#20309;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#22823;&#35268;&#27169;&#19977;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Geometry-Informed Neural Operator for Large-Scale 3D PDEs. (arXiv:2309.00583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00583
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;&#65288;GINO&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22823;&#35268;&#27169;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#31526;&#12290;GINO&#20351;&#29992;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#21644;&#28857;&#20113;&#34920;&#31034;&#26469;&#22788;&#29702;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#22270;&#21644;&#20613;&#37324;&#21494;&#32467;&#26500;&#30340;&#31070;&#32463;&#31639;&#23376;&#23454;&#29616;&#31163;&#25955;&#19968;&#33268;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;GINO&#22312;&#22823;&#35268;&#27169;&#27169;&#25311;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#20449;&#24687;&#30340;&#31070;&#32463;&#31639;&#23376;&#65288;GINO&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#19981;&#21516;&#20960;&#20309;&#24418;&#29366;&#30340;&#22823;&#35268;&#27169;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#31526;&#12290;GINO&#20351;&#29992;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#21644;&#36755;&#20837;&#24418;&#29366;&#30340;&#28857;&#20113;&#34920;&#31034;&#65292;&#20197;&#21450;&#22522;&#20110;&#22270;&#21644;&#20613;&#37324;&#21494;&#32467;&#26500;&#30340;&#31070;&#32463;&#31639;&#23376;&#26469;&#23398;&#20064;&#35299;&#31639;&#31526;&#12290;&#22270;&#31070;&#32463;&#31639;&#23376;&#22788;&#29702;&#19981;&#35268;&#21017;&#32593;&#26684;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#25104;&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#35268;&#21017;&#28508;&#22312;&#32593;&#26684;&#12290;GINO&#26159;&#31163;&#25955;&#19968;&#33268;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#24212;&#29992;&#20110;&#36830;&#32493;&#22495;&#30340;&#20219;&#24847;&#31163;&#25955;&#21270;&#65292;&#24182;&#22312;&#31163;&#25955;&#21270;&#32454;&#21270;&#26102;&#25910;&#25947;&#21040;&#36830;&#32493;&#31639;&#23376;&#12290;&#20026;&#20102;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#27169;&#25311;&#19978;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#21253;&#21547;&#26368;&#39640;500&#19975;&#38647;&#35834;&#25968;&#30340;3D&#36710;&#36742;&#20960;&#20309;&#34892;&#19994;&#26631;&#20934;&#31354;&#27668;&#21160;&#21147;&#23398;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the geometry-informed neural operator (GINO), a highly efficient approach to learning the solution operator of large-scale partial differential equations with varying geometries. GINO uses a signed distance function and point-cloud representations of the input shape and neural operators based on graph and Fourier architectures to learn the solution operator. The graph neural operator handles irregular grids and transforms them into and from regular latent grids on which Fourier neural operator can be efficiently applied. GINO is discretization-convergent, meaning the trained model can be applied to arbitrary discretization of the continuous domain and it converges to the continuum operator as the discretization is refined. To empirically validate the performance of our method on large-scale simulation, we generate the industry-standard aerodynamics dataset of 3D vehicle geometries with Reynolds numbers as high as five million. For this large-scale 3D fluid simulation, numeri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Lloyd&#31639;&#27861;&#22312;&#25200;&#21160;&#26679;&#26412;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#21021;&#22987;&#21270;&#21644;&#25200;&#21160;&#30456;&#23545;&#20110;&#20122;&#39640;&#26031;&#22122;&#22768;&#36739;&#23567;&#30340;&#20551;&#35774;&#19979;&#65292;&#31639;&#27861;&#22312;O(log(n))&#27425;&#36845;&#20195;&#21518;&#30340;&#38169;&#32858;&#31867;&#29575;&#22312;&#25351;&#25968;&#19979;&#30028;&#21463;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.00578</link><description>&lt;p&gt;
Lloyd&#31639;&#27861;&#22312;&#25200;&#21160;&#19979;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Consistency of Lloyd's Algorithm Under Perturbations. (arXiv:2309.00578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Lloyd&#31639;&#27861;&#22312;&#25200;&#21160;&#26679;&#26412;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#36866;&#24403;&#21021;&#22987;&#21270;&#21644;&#25200;&#21160;&#30456;&#23545;&#20110;&#20122;&#39640;&#26031;&#22122;&#22768;&#36739;&#23567;&#30340;&#20551;&#35774;&#19979;&#65292;&#31639;&#27861;&#22312;O(log(n))&#27425;&#36845;&#20195;&#21518;&#30340;&#38169;&#32858;&#31867;&#29575;&#22312;&#25351;&#25968;&#19979;&#30028;&#21463;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;Lloyd&#31639;&#27861;&#26159;&#26368;&#24120;&#29992;&#30340;&#32858;&#31867;&#31639;&#27861;&#20043;&#19968;&#12290;&#23427;&#21551;&#21457;&#20102;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#30740;&#31350;&#20102;&#31639;&#27861;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23545;&#22320;&#38754;&#30495;&#23454;&#32858;&#31867;&#30340;&#27491;&#30830;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;2016&#24180;&#65292;&#21346;&#21644;&#21608;&#34920;&#26126;&#65292;&#22312;&#27491;&#30830;&#21021;&#22987;&#21270;&#31639;&#27861;&#30340;&#21069;&#25552;&#19979;&#65292;Lloyd&#31639;&#27861;&#22312;&#20174;&#20122;&#39640;&#26031;&#28151;&#21512;&#20013;&#29420;&#31435;&#25277;&#21462;&#30340;n&#20010;&#26679;&#26412;&#19978;&#30340;&#38169;&#32858;&#31867;&#29575;&#22312;O(log(n))&#27425;&#36845;&#20195;&#21518;&#25351;&#25968;&#19979;&#30028;&#21463;&#38480;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#30495;&#23454;&#26679;&#26412;&#26159;&#26410;&#35266;&#27979;&#21040;&#30340;&#65292;&#38656;&#35201;&#36890;&#36807;&#39044;&#22788;&#29702;&#27969;&#27700;&#32447;&#65288;&#22914;&#21512;&#36866;&#30340;&#25968;&#25454;&#30697;&#38453;&#19978;&#30340;&#35889;&#26041;&#27861;&#65289;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36866;&#24403;&#21021;&#22987;&#21270;&#21644;&#25200;&#21160;&#30456;&#23545;&#20110;&#20122;&#39640;&#26031;&#22122;&#22768;&#36739;&#23567;&#30340;&#20551;&#35774;&#19979;&#65292;Lloyd&#31639;&#27861;&#22312;&#20174;&#20122;&#39640;&#26031;&#28151;&#21512;&#20013;&#25200;&#21160;&#26679;&#26412;&#19978;&#30340;&#38169;&#32858;&#31867;&#29575;&#22312;O(log(n))&#27425;&#36845;&#20195;&#21518;&#21516;&#26679;&#25351;&#25968;&#19979;&#30028;&#21463;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of unsupervised learning, Lloyd's algorithm is one of the most widely used clustering algorithms. It has inspired a plethora of work investigating the correctness of the algorithm under various settings with ground truth clusters. In particular, in 2016, Lu and Zhou have shown that the mis-clustering rate of Lloyd's algorithm on $n$ independent samples from a sub-Gaussian mixture is exponentially bounded after $O(\log(n))$ iterations, assuming proper initialization of the algorithm. However, in many applications, the true samples are unobserved and need to be learned from the data via pre-processing pipelines such as spectral methods on appropriate data matrices. We show that the mis-clustering rate of Lloyd's algorithm on perturbed samples from a sub-Gaussian mixture is also exponentially bounded after $O(\log(n))$ iterations under the assumptions of proper initialization and that the perturbation is small relative to the sub-Gaussian noise. In canonical settings with g
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;&#22270;&#20687;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21367;&#31215;&#31070;&#32463;&#29305;&#24449;&#20551;&#35774;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#21644;&#29702;&#35770;&#35777;&#26126;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;&#28388;&#27874;&#22120;&#30340;&#21327;&#26041;&#24046;&#19982;&#36755;&#20837;&#34917;&#19969;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#20043;&#38388;&#30340;&#39640;&#24230;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#36890;&#36807;&#22522;&#20110;&#34917;&#19969;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#23454;&#29616;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#26222;&#36941;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00570</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Mechanism of feature learning in convolutional neural networks. (arXiv:2309.00570v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;&#22270;&#20687;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#30340;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21367;&#31215;&#31070;&#32463;&#29305;&#24449;&#20551;&#35774;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#21644;&#29702;&#35770;&#35777;&#26126;&#65292;&#35770;&#25991;&#23637;&#31034;&#20102;&#28388;&#27874;&#22120;&#30340;&#21327;&#26041;&#24046;&#19982;&#36755;&#20837;&#34917;&#19969;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#20043;&#38388;&#30340;&#39640;&#24230;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#36890;&#36807;&#22522;&#20110;&#34917;&#19969;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#23454;&#29616;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#22270;&#20687;&#25968;&#25454;&#20013;&#23398;&#20064;&#29305;&#24449;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#26679;&#19968;&#20010;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21367;&#31215;&#31070;&#32463;&#29305;&#24449;&#20551;&#35774;&#65292;&#23427;&#25351;&#20986;&#20219;&#20309;&#21367;&#31215;&#23618;&#20013;&#28388;&#27874;&#22120;&#30340;&#21327;&#26041;&#24046;&#19982;&#35813;&#23618;&#36755;&#20837;&#30340;&#34917;&#19969;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#20551;&#35774;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#21253;&#25324;&#22312;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;&#22914;AlexNet&#65292;VGG&#21644;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#30340;ResNets&#65289;&#30340;&#21367;&#31215;&#23618;&#20013;&#65292;&#21457;&#29616;&#28388;&#27874;&#22120;&#30340;&#21327;&#26041;&#24046;&#19982;&#22522;&#20110;&#34917;&#19969;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25903;&#25345;&#29702;&#35770;&#30340;&#35777;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#34917;&#19969;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#26469;&#23637;&#31034;&#25105;&#20204;&#32467;&#26524;&#30340;&#26222;&#36941;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#21367;&#31215;&#26680;&#26426;&#22120;&#20013;&#30340;&#28145;&#24230;&#29305;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#31639;&#27861;&#31216;&#20026;(Deep) ConvRFM&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#24674;&#22797;&#20986;&#31867;&#20284;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the mechanism of how convolutional neural networks learn features from image data is a fundamental problem in machine learning and computer vision. In this work, we identify such a mechanism. We posit the Convolutional Neural Feature Ansatz, which states that covariances of filters in any convolutional layer are proportional to the average gradient outer product (AGOP) taken with respect to patches of the input to that layer. We present extensive empirical evidence for our ansatz, including identifying high correlation between covariances of filters and patch-based AGOPs for convolutional layers in standard neural architectures, such as AlexNet, VGG, and ResNets pre-trained on ImageNet. We also provide supporting theoretical evidence. We then demonstrate the generality of our result by using the patch-based AGOP to enable deep feature learning in convolutional kernel machines. We refer to the resulting algorithm as (Deep) ConvRFM and show that our algorithm recovers simil
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20174;&#32467;&#26500; MRI &#20013;&#21512;&#25104;&#20855;&#26377;&#20934;&#30830;&#23450;&#37327;&#33021;&#21147;&#30340; amyloid-beta PET &#22270;&#20687;&#30340;&#21487;&#34892;&#24615;&#65292;&#20026;&#20165;&#20381;&#38752; MRI &#33719;&#24471; amyloid-beta &#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2309.00569</link><description>&lt;p&gt;
&#20174;&#32467;&#26500; MRI &#20013;&#21512;&#25104; Amyloid-Beta &#36724;&#21521;&#24179;&#38754; PET&#65306;&#29992;&#20110;&#31579;&#26597;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Amyloid-Beta Axial Plane PET Synthesis from Structural MRI: An Image Translation Approach for Screening Alzheimer's Disease. (arXiv:2309.00569v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00569
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20174;&#32467;&#26500; MRI &#20013;&#21512;&#25104;&#20855;&#26377;&#20934;&#30830;&#23450;&#37327;&#33021;&#21147;&#30340; amyloid-beta PET &#22270;&#20687;&#30340;&#21487;&#34892;&#24615;&#65292;&#20026;&#20165;&#20381;&#38752; MRI &#33719;&#24471; amyloid-beta &#20449;&#24687;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;&#65292;&#20174;&#32467;&#26500; MRI &#20013;&#20135;&#29983;&#21512;&#25104;&#30340;&#23450;&#37327;&#20934;&#30830;&#30340; amyloid-beta PET &#22270;&#20687;&#12290;&#20351;&#29992;&#20102; amyloid-beta PET &#21644;&#32467;&#26500; MRI &#30340;&#22270;&#20687;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21512;&#25104;&#30340; PET &#22270;&#20687;&#22312;&#24418;&#29366;&#12289;&#23545;&#27604;&#24230;&#21644;&#25972;&#20307;&#30340; SSIM &#21644; PSNR &#19978;&#19982;&#30495;&#23454;&#22270;&#20687;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20284;&#24230;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32467;&#26500;&#21040;&#23450;&#37327;&#22270;&#20687;&#30340;&#32763;&#35793;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;&#30340; MRI &#20013;&#33719;&#24471; amyloid-beta &#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, an image translation model is implemented to produce synthetic amyloid-beta PET images from structural MRI that are quantitatively accurate. Image pairs of amyloid-beta PET and structural MRI were used to train the model. We found that the synthetic PET images could be produced with a high degree of similarity to truth in terms of shape, contrast and overall high SSIM and PSNR. This work demonstrates that performing structural to quantitative image translation is feasible to enable the access amyloid-beta information from only MRI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#22312;&#35299;&#37322;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#31354;&#38388;&#38646;&#20540;&#21644;&#27491;&#21017;&#21270;&#23545;&#22238;&#24402;&#31995;&#25968;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#20844;&#24335;&#26469;&#27604;&#36739;&#22238;&#24402;&#31995;&#25968;&#19982;&#29289;&#29702;&#24037;&#31243;&#30693;&#35782;&#24471;&#21040;&#30340;&#31995;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#35299;&#37322;&#24615;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00564</link><description>&lt;p&gt;
&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#30340;&#35299;&#37322;&#65306;&#31354;&#38388;&#38646;&#20540;&#21644;&#27491;&#21017;&#21270;&#22312;&#30005;&#27744;&#25968;&#25454;&#19978;&#30340;&#24433;&#21709;&#30340;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data. (arXiv:2309.00564v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#22312;&#35299;&#37322;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#31354;&#38388;&#38646;&#20540;&#21644;&#27491;&#21017;&#21270;&#23545;&#22238;&#24402;&#31995;&#25968;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#20844;&#24335;&#26469;&#27604;&#36739;&#22238;&#24402;&#31995;&#25968;&#19982;&#29289;&#29702;&#24037;&#31243;&#30693;&#35782;&#24471;&#21040;&#30340;&#31995;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#35299;&#37322;&#24615;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#32771;&#34385;&#21040;&#20174;&#21270;&#23398;&#25110;&#29983;&#29289;&#31995;&#32479;&#20013;&#32463;&#24120;&#24471;&#21040;&#30340;&#22522;&#30784;&#24179;&#28369;&#28508;&#22312;&#36807;&#31243;&#30340;&#31163;&#25955;&#27979;&#37327;&#25968;&#25454;&#12290;&#22312;&#39640;&#32500;&#24230;&#20013;&#35299;&#37322;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#31354;&#38388;&#38646;&#20540;&#21450;&#20854;&#19982;&#27491;&#21017;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#20250;&#22609;&#36896;&#22238;&#24402;&#31995;&#25968;&#12290;&#25968;&#25454;&#30340;&#31354;&#38388;&#38646;&#20540;&#21253;&#21547;&#25152;&#26377;&#28385;&#36275;$\mathbf{Xw}=\mathbf{0}$&#30340;&#31995;&#25968;&#65292;&#20174;&#32780;&#20801;&#35768;&#38750;&#24120;&#19981;&#21516;&#30340;&#31995;&#25968;&#20135;&#29983;&#30456;&#21516;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20248;&#21270;&#20844;&#24335;&#26469;&#27604;&#36739;&#22238;&#24402;&#31995;&#25968;&#21644;&#36890;&#36807;&#29289;&#29702;&#24037;&#31243;&#30693;&#35782;&#24471;&#21040;&#30340;&#31995;&#25968;&#65292;&#20197;&#20102;&#35299;&#31995;&#25968;&#24046;&#24322;&#30340;&#21738;&#20123;&#37096;&#20998;&#25509;&#36817;&#20110;&#31354;&#38388;&#38646;&#20540;&#12290;&#36825;&#31181;&#31354;&#38388;&#38646;&#20540;&#26041;&#27861;&#22312;&#19968;&#20010;&#21512;&#25104;&#31034;&#20363;&#21644;&#38146;&#31163;&#23376;&#30005;&#27744;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;&#26681;&#25454;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#36873;&#25321;&#21512;&#36866;&#30340;&#27491;&#21017;&#21270;&#21644;z-score&#22788;&#29702;&#65292;&#21487;&#20197;&#24471;&#21040;&#21487;&#35299;&#37322;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional linear regression is important in many scientific fields. This article considers discrete measured data of underlying smooth latent processes, as is often obtained from chemical or biological systems. Interpretation in high dimensions is challenging because the nullspace and its interplay with regularization shapes regression coefficients. The data's nullspace contains all coefficients that satisfy $\mathbf{Xw}=\mathbf{0}$, thus allowing very different coefficients to yield identical predictions. We developed an optimization formulation to compare regression coefficients and coefficients obtained by physical engineering knowledge to understand which part of the coefficient differences are close to the nullspace. This nullspace method is tested on a synthetic example and lithium-ion battery data. The case studies show that regularization and z-scoring are design choices that, if chosen corresponding to prior physical knowledge, lead to interpretable regression results. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#20114;&#23398;&#20064;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;Bandit&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#26377;&#38480;&#33218;&#21644;&#32447;&#24615;Bandit&#38382;&#39064;&#36951;&#25022;&#30340;&#19979;&#30028;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#38544;&#31169;&#39044;&#31639;&#19979;&#30340;&#38590;&#24230;&#21306;&#22495;&#65292;&#24182;&#21457;&#29616;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#27604;&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#26356;&#26377;&#25928;&#22320;&#20445;&#25252;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00557</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#21644;&#38598;&#20013;&#24335;&#24046;&#20998;&#38544;&#31169;&#22312;Bandit&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interactive and Concentrated Differential Privacy for Bandits. (arXiv:2309.00557v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20132;&#20114;&#23398;&#20064;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#38544;&#31169;&#20445;&#25252;&#30340;Bandit&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#26377;&#38480;&#33218;&#21644;&#32447;&#24615;Bandit&#38382;&#39064;&#36951;&#25022;&#30340;&#19979;&#30028;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19981;&#21516;&#38544;&#31169;&#39044;&#31639;&#19979;&#30340;&#38590;&#24230;&#21306;&#22495;&#65292;&#24182;&#21457;&#29616;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;&#21487;&#20197;&#27604;&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#26356;&#26377;&#25928;&#22320;&#20445;&#25252;&#38544;&#31169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30456;&#24212;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bandit&#38382;&#39064;&#22312;&#20132;&#20114;&#24335;&#23398;&#20064;&#26041;&#26696;&#21644;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#20110;&#25935;&#24863;&#30340;&#29992;&#25143;&#25968;&#25454;&#65292;&#22240;&#27492;&#38544;&#31169;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#20132;&#20114;&#24335;&#24046;&#20998;&#38544;&#31169;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#22522;&#20110;&#21487;&#20449;&#38598;&#20013;&#24335;&#20915;&#31574;&#32773;&#30340;Bandit&#38382;&#39064;&#30340;&#38544;&#31169;&#24615;&#12290;&#34429;&#28982;&#24050;&#32463;&#23545;&#32431;&#949;-&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#30340;Bandit&#38382;&#39064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#22312;&#29702;&#35299;&#38646;&#38598;&#20013;&#24046;&#20998;&#38544;&#31169;(zCDP)&#30340;Bandit&#38382;&#39064;&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#38024;&#23545;&#26377;&#38480;&#33218;&#21644;&#32447;&#24615;Bandit&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36951;&#25022;&#30340;&#26368;&#23567;&#26368;&#22823;&#21644;&#38382;&#39064;&#30456;&#20851;&#19979;&#30028;&#65292;&#20174;&#32780;&#37327;&#21270;&#20102;&#36825;&#20123;&#24773;&#20917;&#19979;&#961;-&#20840;&#23616;zCDP&#30340;&#20195;&#20215;&#12290;&#36825;&#20123;&#19979;&#30028;&#25581;&#31034;&#20102;&#22522;&#20110;&#38544;&#31169;&#39044;&#31639;&#961;&#30340;&#20004;&#20010;&#22256;&#38590;&#21306;&#22495;&#65292;&#24182;&#34920;&#26126;&#961;-&#20840;&#23616;zCDP&#27604;&#32431;&#949;-&#20840;&#23616;&#24046;&#20998;&#38544;&#31169;&#20135;&#29983;&#30340;&#36951;&#25022;&#26356;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#38480;&#33218;&#21644;&#32447;&#24615;Bandit&#38382;&#39064;&#30340;&#961;-&#20840;&#23616;zCDP&#31639;&#27861;&#65292;&#21363;AdaC-UCB&#21644;AdaC-GOPE&#12290;&#36825;&#20004;&#20010;&#31639;&#27861;&#37117;&#20351;&#29992;&#20102;&#39640;&#26031;&#26426;&#21046;&#30340;&#20849;&#21516;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandits play a crucial role in interactive learning schemes and modern recommender systems. However, these systems often rely on sensitive user data, making privacy a critical concern. This paper investigates privacy in bandits with a trusted centralized decision-maker through the lens of interactive Differential Privacy (DP). While bandits under pure $\epsilon$-global DP have been well-studied, we contribute to the understanding of bandits under zero Concentrated DP (zCDP). We provide minimax and problem-dependent lower bounds on regret for finite-armed and linear bandits, which quantify the cost of $\rho$-global zCDP in these settings. These lower bounds reveal two hardness regimes based on the privacy budget $\rho$ and suggest that $\rho$-global zCDP incurs less regret than pure $\epsilon$-global DP. We propose two $\rho$-global zCDP bandit algorithms, AdaC-UCB and AdaC-GOPE, for finite-armed and linear bandits respectively. Both algorithms use a common recipe of Gaussian mechanism 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#31579;&#36873;&#33258;&#28982;&#23545;&#31435;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#24369;&#30417;&#30563;&#26631;&#27880;&#33719;&#24471;&#30340;&#27010;&#29575;&#26631;&#31614;&#26469;&#23454;&#29616;&#36825;&#19968;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00543</link><description>&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;&#21307;&#30103;AI&#31995;&#32479;&#65292;&#31579;&#36873;&#22825;&#28982;&#23545;&#31435;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare. (arXiv:2309.00543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00543
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#31579;&#36873;&#33258;&#28982;&#23545;&#31435;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#24369;&#30417;&#30563;&#26631;&#27880;&#33719;&#24471;&#30340;&#27010;&#29575;&#26631;&#31614;&#26469;&#23454;&#29616;&#36825;&#19968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#30340;&#21307;&#30103;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;AI&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#23545;&#21512;&#25104;&#23545;&#31435;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#20123;&#31034;&#20363;&#26159;&#36890;&#36807;&#21521;&#28165;&#27905;&#36755;&#20837;&#25968;&#25454;&#28155;&#21152;&#38590;&#20197;&#23519;&#35273;&#30340;&#25200;&#21160;&#32780;&#21046;&#20316;&#20986;&#26469;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21512;&#25104;&#23545;&#31435;&#31034;&#20363;&#24182;&#19981;&#33021;&#20934;&#30830;&#21453;&#26144;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#25968;&#25454;&#30340;&#32972;&#26223;&#19979;&#12290;&#22240;&#27492;&#65292;&#23545;&#21512;&#25104;&#23545;&#31435;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#26410;&#24517;&#33021;&#22815;&#36716;&#21270;&#20026;&#23545;&#33258;&#28982;&#20135;&#29983;&#30340;&#23545;&#31435;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#36825;&#23545;&#20110;&#21487;&#20449;&#36182;&#30340;AI&#32780;&#35328;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31579;&#36873;&#30001;&#33258;&#28982;&#23545;&#31435;&#31034;&#20363;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#33258;&#21160;&#24369;&#30417;&#30563;&#26631;&#27880;&#33719;&#24471;&#30340;&#27010;&#29575;&#26631;&#31614;&#65292;&#36825;&#31181;&#26631;&#31614;&#32467;&#21512;&#20102;&#22024;&#26434;&#19988;&#26131;&#33719;&#24471;&#30340;&#26631;&#27880;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have shown promising predictive accuracy for time-series healthcare applications. However, ensuring the robustness of these models is vital for building trustworthy AI systems. Existing research predominantly focuses on robustness to synthetic adversarial examples, crafted by adding imperceptible perturbations to clean input data. However, these synthetic adversarial examples do not accurately reflect the most challenging real-world scenarios, especially in the context of healthcare data. Consequently, robustness to synthetic adversarial examples may not necessarily translate to robustness against naturally occurring adversarial examples, which is highly desirable for trustworthy AI. We propose a method to curate datasets comprised of natural adversarial examples to evaluate model robustness. The method relies on probabilistic labels obtained from automated weakly-supervised labeling that combines noisy and cheap-to-obtain labeling heuristics. Based on these labels
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#30340;&#33258;&#36866;&#24212;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#36817;&#20284;&#26080;&#35760;&#24518;&#21333;&#21464;&#37327;&#36830;&#32493;&#20989;&#25968;&#65292;&#22312;&#23398;&#20064;&#36136;&#37327;&#19982;&#22797;&#26434;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.00530</link><description>&lt;p&gt;
&#22522;&#20110;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#30340;&#33258;&#36866;&#24212;&#20989;&#25968;&#36924;&#36817;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptive function approximation based on the Discrete Cosine Transform (DCT). (arXiv:2309.00530v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#30340;&#33258;&#36866;&#24212;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#36817;&#20284;&#26080;&#35760;&#24518;&#21333;&#21464;&#37327;&#36830;&#32493;&#20989;&#25968;&#65292;&#22312;&#23398;&#20064;&#36136;&#37327;&#19982;&#22797;&#26434;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20313;&#24358;&#20316;&#20026;&#22522;&#20989;&#25968;&#29992;&#20110;&#36817;&#20284;&#26080;&#35760;&#24518;&#21333;&#21464;&#37327;&#36830;&#32493;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20351;&#29992;&#20102;&#30417;&#30563;&#23398;&#20064;&#26469;&#33719;&#21462;&#36924;&#36817;&#31995;&#25968;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#65288;DCT&#65289;&#12290;&#30001;&#20110;&#20313;&#24358;&#22522;&#20989;&#25968;&#30340;&#26377;&#38480;&#21160;&#24577;&#21644;&#27491;&#20132;&#24615;&#65292;&#31616;&#21333;&#30340;&#26799;&#24230;&#31639;&#27861;&#65292;&#22914;&#24402;&#19968;&#21270;&#26368;&#23567;&#22343;&#26041;&#65288;NLMS&#65289;&#65292;&#21487;&#20197;&#20174;&#20013;&#21463;&#30410;&#65292;&#24182;&#21576;&#29616;&#20986;&#21487;&#25511;&#21644;&#21487;&#39044;&#27979;&#30340;&#25910;&#25947;&#26102;&#38388;&#21644;&#35823;&#24046;&#19981;&#21305;&#37197;&#12290;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#65292;&#35813;&#25216;&#26415;&#22312;&#23398;&#20064;&#36136;&#37327;&#19982;&#22797;&#26434;&#24615;&#26041;&#38754;&#20301;&#21015;&#21069;&#33541;&#65292;&#24182;&#34987;&#25552;&#20986;&#20316;&#20026;&#26356;&#22797;&#26434;&#30340;&#30417;&#30563;&#23398;&#20064;&#31995;&#32479;&#20013;&#20351;&#29992;&#30340;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#25216;&#26415;&#12290;&#27169;&#25311;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#24198;&#31069;&#20102;1973&#24180;Nasir Ahmed&#21457;&#34920;DCT&#30340;50&#21608;&#24180;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the cosine as basis function for the approximation of univariate and continuous functions without memory. This work studies a supervised learning to obtain the approximation coefficients, instead of using the Discrete Cosine Transform (DCT). Due to the finite dynamics and orthogonality of the cosine basis functions, simple gradient algorithms, such as the Normalized Least Mean Squares (NLMS), can benefit from it and present a controlled and predictable convergence time and error misadjustment. Due to its simplicity, the proposed technique ranks as the best in terms of learning quality versus complexity, and it is presented as an attractive technique to be used in more complex supervised learning systems. Simulations illustrate the performance of the approach. This paper celebrates the 50th anniversary of the publication of the DCT by Nasir Ahmed in 1973.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#38543;&#26426;&#32593;&#32476;&#19978;&#36827;&#34892;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;DOT-ADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#22312;&#32447;&#23398;&#20064;&#12289;&#24322;&#27493;&#35745;&#31639;&#12289;&#19981;&#21487;&#38752;&#36890;&#20449;&#21644;&#19981;&#31934;&#30830;&#35745;&#31639;&#31561;&#25361;&#25112;&#65292;&#22312;&#19968;&#22823;&#31867;&#20984;&#23398;&#20064;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.00520</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#32593;&#32476;&#19978;&#30340;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Distributed Learning over Random Networks. (arXiv:2309.00520v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#38543;&#26426;&#32593;&#32476;&#19978;&#36827;&#34892;&#22312;&#32447;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;DOT-ADMM&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#22312;&#32447;&#23398;&#20064;&#12289;&#24322;&#27493;&#35745;&#31639;&#12289;&#19981;&#21487;&#38752;&#36890;&#20449;&#21644;&#19981;&#31934;&#30830;&#35745;&#31639;&#31561;&#25361;&#25112;&#65292;&#22312;&#19968;&#22823;&#31867;&#20984;&#23398;&#20064;&#38382;&#39064;&#19978;&#33719;&#24471;&#20102;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#37096;&#32626;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20351;&#24471;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#26159;&#25910;&#38598;&#26412;&#22320;&#25968;&#25454;&#65292;&#28982;&#21518;&#21512;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#19981;&#30452;&#25509;&#20849;&#20139;&#25968;&#25454;&#12290;&#34429;&#28982;&#20998;&#24067;&#24335;&#23398;&#20064;&#22312;&#20445;&#25252;&#26234;&#33021;&#20307;&#38544;&#31169;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#35774;&#35745;&#21644;&#20998;&#26512;&#36866;&#24403;&#30340;&#31639;&#27861;&#26041;&#38754;&#20063;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#20197;&#19979;&#30001;&#23454;&#38469;&#23454;&#26045;&#25152;&#39537;&#21160;&#30340;&#25361;&#25112;&#65306;&#65288;i&#65289;&#22312;&#32447;&#23398;&#20064;&#65292;&#20854;&#20013;&#26412;&#22320;&#25968;&#25454;&#38543;&#26102;&#38388;&#21464;&#21270;&#65307;&#65288;ii&#65289;&#24322;&#27493;&#26234;&#33021;&#20307;&#35745;&#31639;&#65307;&#65288;iii&#65289;&#19981;&#21487;&#38752;&#21644;&#26377;&#38480;&#30340;&#36890;&#20449;&#65307;&#65288;iv&#65289;&#19981;&#31934;&#30830;&#30340;&#26412;&#22320;&#35745;&#31639;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20998;&#24067;&#24335;&#25805;&#20316;&#29702;&#35770;&#65288;DOT&#65289;&#29256;&#26412;&#30340;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#65292;&#31216;&#20043;&#20026;DOT-ADMM&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#22312;&#22823;&#31867;&#20984;&#23398;&#20064;&#38382;&#39064;&#19978;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent deployment of multi-agent systems in a wide range of scenarios has enabled the solution of learning problems in a distributed fashion. In this context, agents are tasked with collecting local data and then cooperatively train a model, without directly sharing the data. While distributed learning offers the advantage of preserving agents' privacy, it also poses several challenges in terms of designing and analyzing suitable algorithms. This work focuses specifically on the following challenges motivated by practical implementation: (i) online learning, where the local data change over time; (ii) asynchronous agent computations; (iii) unreliable and limited communications; and (iv) inexact local computations. To tackle these challenges, we introduce the Distributed Operator Theoretical (DOT) version of the Alternating Direction Method of Multipliers (ADMM), which we call the DOT-ADMM Algorithm. We prove that it converges with a linear rate for a large class of convex learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#20840;&#23616;&#26368;&#23567;&#20540;&#38468;&#36817;&#30340;&#32467;&#26500;&#21644;&#26799;&#24230;&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#36739;&#24378;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2309.00508</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20840;&#23616;&#26368;&#23567;&#20540;&#38468;&#36817;&#30340;&#32467;&#26500;&#21644;&#26799;&#24230;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks. (arXiv:2309.00508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#20840;&#23616;&#26368;&#23567;&#20540;&#38468;&#36817;&#30340;&#32467;&#26500;&#21644;&#26799;&#24230;&#21160;&#21147;&#23398;&#65292;&#25581;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#36739;&#24378;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#20840;&#23616;&#26368;&#23567;&#20540;&#38468;&#36817;&#30340;&#25439;&#22833;&#20989;&#25968;&#34920;&#38754;&#30340;&#32467;&#26500;&#65292;&#30830;&#23450;&#20102;&#33021;&#22815;&#23454;&#29616;&#23436;&#32654;&#27867;&#21270;&#30340;&#21442;&#25968;&#38598;&#65292;&#24182;&#23436;&#25972;&#25551;&#36848;&#20102;&#20854;&#21608;&#22260;&#30340;&#26799;&#24230;&#27969;&#21160;&#24577;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22797;&#26434;&#30340;&#25439;&#22833;&#20989;&#25968;&#34920;&#38754;&#30340;&#19968;&#20123;&#31616;&#21333;&#26041;&#38754;&#65292;&#24182;&#25581;&#31034;&#20102;&#27169;&#22411;&#12289;&#30446;&#26631;&#20989;&#25968;&#12289;&#26679;&#26412;&#21644;&#21021;&#22987;&#21270;&#23545;&#35757;&#32451;&#21160;&#21147;&#23398;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#65288;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#65289;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under mild assumptions, we investigate the structure of loss landscape of two-layer neural networks near global minima, determine the set of parameters which give perfect generalization, and fully characterize the gradient flows around it. With novel techniques, our work uncovers some simple aspects of the complicated loss landscape and reveals how model, target function, samples and initialization affect the training dynamics differently. Based on these results, we also explain why (overparametrized) neural networks could generalize well.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#30005;&#21147;&#31995;&#32479;&#30417;&#27979;&#19982;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#20307;&#21253;&#25324;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#65292;&#20197;&#21450;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#21160;&#24577;&#37197;&#30005;&#32593;&#37325;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.00498</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#30005;&#21147;&#31995;&#32479;&#30417;&#27979;&#19982;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Deep Learning Methods in Monitoring and Optimization of Electric Power Systems. (arXiv:2309.00498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00498
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#30005;&#21147;&#31995;&#32479;&#30417;&#27979;&#19982;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#20307;&#21253;&#25324;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#65292;&#20197;&#21450;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#21160;&#24577;&#37197;&#30005;&#32593;&#37325;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#21338;&#22763;&#35770;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#25552;&#39640;&#30005;&#21147;&#31995;&#32479;&#30417;&#27979;&#19982;&#20248;&#21270;&#31639;&#27861;&#30340;&#24212;&#29992;&#12290;&#35813;&#35770;&#25991;&#30340;&#31532;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#24212;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#22686;&#24378;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#20272;&#35745;&#12290;&#35813;&#35770;&#25991;&#30340;&#31532;&#20108;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#21160;&#24577;&#37197;&#30005;&#32593;&#37325;&#26500;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#20223;&#30495;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This PhD thesis thoroughly examines the utilization of deep learning techniques as a means to advance the algorithms employed in the monitoring and optimization of electric power systems. The first major contribution of this thesis involves the application of graph neural networks to enhance power system state estimation. The second key aspect of this thesis focuses on utilizing reinforcement learning for dynamic distribution network reconfiguration. The effectiveness of the proposed methods is affirmed through extensive experimentation and simulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#20266;&#24433;&#20943;&#23569;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#22312;&#37325;&#24314;&#20043;&#21518;&#36827;&#34892;&#22788;&#29702;&#65292;&#32780;&#26412;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#22270;&#20687;&#22495;&#36827;&#34892;&#22810;&#27493;&#39588;&#21435;&#20266;&#24433;&#65292;&#20351;&#24471;&#30456;&#23545;&#22256;&#38590;&#21435;&#38500;&#30340;&#20266;&#24433;&#20063;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#12290;</title><link>http://arxiv.org/abs/2309.00494</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#22810;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#20266;&#24433;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Multi-stage Deep Learning Artifact Reduction for Computed Tomography. (arXiv:2309.00494v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#20266;&#24433;&#20943;&#23569;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#22312;&#37325;&#24314;&#20043;&#21518;&#36827;&#34892;&#22788;&#29702;&#65292;&#32780;&#26412;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#22270;&#20687;&#22495;&#36827;&#34892;&#22810;&#27493;&#39588;&#21435;&#20266;&#24433;&#65292;&#20351;&#24471;&#30456;&#23545;&#22256;&#38590;&#21435;&#38500;&#30340;&#20266;&#24433;&#20063;&#33021;&#22815;&#26377;&#25928;&#28040;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20013;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#33719;&#21462;&#30340;&#25237;&#24433;&#22270;&#20687;&#35745;&#31639;&#20986;&#29289;&#20307;&#20869;&#37096;&#32467;&#26500;&#30340;&#22270;&#20687;&#12290;&#36825;&#20123;&#37325;&#24314;&#22270;&#20687;&#30340;&#36136;&#37327;&#23545;&#20110;&#20934;&#30830;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#36825;&#31181;&#36136;&#37327;&#21487;&#33021;&#20250;&#34987;&#21508;&#31181;&#25104;&#20687;&#20266;&#24433;&#38477;&#20302;&#12290;&#20026;&#20102;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#65292;&#33719;&#21462;&#30340;&#25237;&#24433;&#22270;&#20687;&#36890;&#24120;&#36890;&#36807;&#30001;&#22810;&#20010;&#21435;&#20266;&#24433;&#27493;&#39588;&#32452;&#25104;&#30340;&#27969;&#31243;&#36827;&#34892;&#22788;&#29702;&#65292;&#36825;&#20123;&#27493;&#39588;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#22270;&#20687;&#22495;&#65288;&#20363;&#22914;&#65292;&#25237;&#24433;&#22270;&#20687;&#30340;&#24322;&#24120;&#20540;&#21435;&#38500;&#21644;&#37325;&#24314;&#22270;&#20687;&#30340;&#21435;&#22122;&#65289;&#12290;&#36825;&#20123;&#20266;&#24433;&#21435;&#38500;&#26041;&#27861;&#21033;&#29992;&#20102;&#26576;&#20123;&#20266;&#24433;&#22312;&#29305;&#23450;&#22495;&#30456;&#23545;&#20110;&#20854;&#20182;&#22495;&#26356;&#23481;&#26131;&#21435;&#38500;&#30340;&#20107;&#23454;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20266;&#24433;&#21435;&#38500;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#22312;&#37325;&#24314;&#20043;&#21518;&#20316;&#20026;&#21518;&#22788;&#29702;&#26041;&#27861;&#24212;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#37325;&#24314;&#22495;&#30456;&#23545;&#22256;&#38590;&#21435;&#38500;&#30340;&#20266;&#24433;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#21435;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Computed Tomography (CT), an image of the interior structure of an object is computed from a set of acquired projection images. The quality of these reconstructed images is essential for accurate analysis, but this quality can be degraded by a variety of imaging artifacts. To improve reconstruction quality, the acquired projection images are often processed by a pipeline consisting of multiple artifact-removal steps applied in various image domains (e.g., outlier removal on projection images and denoising of reconstruction images). These artifact-removal methods exploit the fact that certain artifacts are easier to remove in a certain domain compared with other domains.  Recently, deep learning methods have shown promising results for artifact removal for CT images. However, most existing deep learning methods for CT are applied as a post-processing method after reconstruction. Therefore, artifacts that are relatively difficult to remove in the reconstruction domain may not be effec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#23545;&#22312;O-RAN&#20999;&#29255;&#20013;&#20351;&#29992;&#30340;DRL&#25216;&#26415;&#30340;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00489</link><description>&lt;p&gt;
&#39044;&#27979;&#23545;DRL&#25216;&#26415;&#22312;O-RAN&#20999;&#29255;&#20013;&#30340;&#25910;&#25947;&#24615;&#26377;&#20309;&#24433;&#21709;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN Slicing?. (arXiv:2309.00489v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#23545;&#22312;O-RAN&#20999;&#29255;&#20013;&#20351;&#29992;&#30340;DRL&#25216;&#26415;&#30340;&#25910;&#25947;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27785;&#28024;&#24335;&#24212;&#29992;&#65288;&#22914;&#34394;&#25311;&#29616;&#23454;&#28216;&#25103;&#21644;&#20803;&#23431;&#23449;&#26381;&#21153;&#65289;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#20302;&#24310;&#36831;&#21644;&#21487;&#38752;&#30340;&#36830;&#25509;&#12290;&#20026;&#20102;&#25552;&#20379;&#26080;&#32541;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#24320;&#25918;&#24335;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#26550;&#26500;&#21644;6G&#32593;&#32476;&#34987;&#26399;&#26395;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;O-RAN&#33539;&#24335;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#65292;RAN&#20999;&#29255;&#21487;&#20197;&#26681;&#25454;&#27785;&#28024;&#24335;&#26381;&#21153;&#30340;&#38656;&#27714;&#20998;&#37197;&#32593;&#32476;&#36164;&#28304;&#65292;&#22312;&#21333;&#20010;&#29289;&#29702;&#22522;&#30784;&#35774;&#26045;&#19978;&#21019;&#24314;&#22810;&#20010;&#34394;&#25311;&#32593;&#32476;&#12290;&#22312;O-RAN&#25991;&#29486;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#36890;&#24120;&#29992;&#20110;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12290;&#28982;&#32780;&#65292;DRL&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#37319;&#29992;&#36895;&#24230;&#36739;&#24930;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;DRL&#20195;&#29702;&#22312;&#21021;&#22987;&#37096;&#32626;&#26102;&#21644;&#32593;&#32476;&#26465;&#20214;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#26102;&#30340;&#25910;&#25947;&#32531;&#24930;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#23545;&#20132;&#36890;&#38656;&#27714;&#30340;&#25910;&#25947;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of immersive applications such as virtual reality (VR) gaming and metaverse services depends on low latency and reliable connectivity. To provide seamless user experiences, the open radio access network (O-RAN) architecture and 6G networks are expected to play a crucial role. RAN slicing, a critical component of the O-RAN paradigm, enables network resources to be allocated based on the needs of immersive services, creating multiple virtual networks on a single physical infrastructure. In the O-RAN literature, deep reinforcement learning (DRL) algorithms are commonly used to optimize resource allocation. However, the practical adoption of DRL in live deployments has been sluggish. This is primarily due to the slow convergence and performance instabilities suffered by the DRL agents both upon initial deployment and when there are significant changes in network conditions. In this paper, we investigate the impact of time series forecasting of traffic demands on the convergence
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#24863;&#30693;&#30340;&#32447;&#22270;&#36716;&#25442;&#22120;&#65288;Galformer&#65289;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;2D&#21644;3D&#27169;&#24577;&#32534;&#30721;&#20998;&#23376;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#20102;&#20114;&#34917;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.00483</link><description>&lt;p&gt;
&#20960;&#20309;&#24863;&#30693;&#30340;&#32447;&#22270;&#36716;&#25442;&#22120;&#39044;&#35757;&#32451;&#29992;&#20110;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geometry-aware Line Graph Transformer Pre-training for Molecular Property Prediction. (arXiv:2309.00483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#24863;&#30693;&#30340;&#32447;&#22270;&#36716;&#25442;&#22120;&#65288;Galformer&#65289;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;2D&#21644;3D&#27169;&#24577;&#32534;&#30721;&#20998;&#23376;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#20102;&#20114;&#34917;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30001;&#20110;&#26631;&#35760;&#20998;&#23376;&#30340;&#31232;&#32570;&#24615;&#65292;&#23545;&#20110;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#23398;&#20064;&#27867;&#21270;&#20998;&#23376;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36890;&#24120;&#23558;&#20998;&#23376;&#35270;&#20026;&#20108;&#32500;&#25299;&#25169;&#22270;&#26469;&#24314;&#27169;&#65292;&#20294;&#26159;&#24050;&#32463;&#21457;&#29616;&#20998;&#23376;&#30340;&#19977;&#32500;&#20960;&#20309;&#23545;&#30830;&#23450;&#20998;&#23376;&#21151;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20309;&#24863;&#30693;&#30340;&#32447;&#22270;&#36716;&#25442;&#22120;&#65288;Galformer&#65289;&#39044;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;2D&#21644;3D&#27169;&#24577;&#22686;&#24378;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#27169;&#24577;&#32447;&#22270;&#36716;&#25442;&#22120;&#20027;&#24178;&#26469;&#32534;&#30721;&#20998;&#23376;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#12290;&#35774;&#35745;&#30340;&#20027;&#24178;&#32467;&#21512;&#20102;&#26377;&#25928;&#30340;&#32467;&#26500;&#32534;&#30721;&#65292;&#20174;&#20004;&#31181;&#27169;&#24577;&#25429;&#25417;&#22270;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Molecular property prediction with deep learning has gained much attention over the past years. Owing to the scarcity of labeled molecules, there has been growing interest in self-supervised learning methods that learn generalizable molecular representations from unlabeled data. Molecules are typically treated as 2D topological graphs in modeling, but it has been discovered that their 3D geometry is of great importance in determining molecular functionalities. In this paper, we propose the Geometry-aware line graph transformer (Galformer) pre-training, a novel self-supervised learning framework that aims to enhance molecular representation learning with 2D and 3D modalities. Specifically, we first design a dual-modality line graph transformer backbone to encode the topological and geometric information of a molecule. The designed backbone incorporates effective structural encodings to capture graph structures from both modalities. Then we devise two complementary pre-training tasks at 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20998;&#26512;&#25345;&#32493;&#23398;&#20064;&#32773;&#30340;&#26032;&#25351;&#26631;&#65292;&#38024;&#23545;&#29616;&#26377;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#20102;&#20998;&#31867;&#20219;&#21153;&#36880;&#28176;&#22686;&#21152;&#38590;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.00462</link><description>&lt;p&gt;
&#20998;&#26512;&#25345;&#32493;&#23398;&#20064;&#32773;&#30340;&#26032;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
New metrics for analyzing continual learners. (arXiv:2309.00462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20998;&#26512;&#25345;&#32493;&#23398;&#20064;&#32773;&#30340;&#26032;&#25351;&#26631;&#65292;&#38024;&#23545;&#29616;&#26377;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#35299;&#20915;&#20102;&#20998;&#31867;&#20219;&#21153;&#36880;&#28176;&#22686;&#21152;&#38590;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20174;&#22266;&#23450;&#30340;&#31867;&#21035;&#38598;&#21512;&#20013;&#36827;&#34892;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#23545;&#20110;&#36830;&#32493;&#27969;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#65292;&#20854;&#20013;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#25353;&#39034;&#24207;&#21576;&#29616;&#12290;&#36825;&#31181;&#24773;&#20917;&#34987;&#31216;&#20026;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#65292;&#23545;&#20110;&#26631;&#20934;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#65292;&#23427;&#20204;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21516;&#26102;&#38590;&#20197;&#20445;&#25345;&#26087;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#36825;&#31181;&#31283;&#23450;&#24615;&#19982;&#21487;&#22609;&#24615;&#30340;&#22256;&#22659;&#20173;&#28982;&#26159;&#25345;&#32493;&#23398;&#20064;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#25351;&#26631;&#26469;&#20805;&#20998;&#34913;&#37327;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#19968;&#20010;&#25351;&#26631;&#32771;&#34385;&#21040;&#20998;&#31867;&#20219;&#21153;&#30340;&#36880;&#28176;&#22686;&#21152;&#30340;&#38590;&#24230;&#65292;&#36825;&#20174;&#26412;&#36136;&#19978;&#23548;&#33268;&#20219;&#20309;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24403;&#21069;&#25351;&#26631;&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#30830;&#23450;&#20102;&#35774;&#32622;&#24341;&#36215;&#30340;&#36951;&#24536;&#30340;&#23384;&#22312;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32771;&#34385;&#20219;&#21153;&#36880;&#28176;&#22686;&#21152;&#38590;&#24230;&#30340;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have shown remarkable performance when trained on independent and identically distributed data from a fixed set of classes. However, in real-world scenarios, it can be desirable to train models on a continuous stream of data where multiple classification tasks are presented sequentially. This scenario, known as Continual Learning (CL) poses challenges to standard learning algorithms which struggle to maintain knowledge of old tasks while learning new ones. This stability-plasticity dilemma remains central to CL and multiple metrics have been proposed to adequately measure stability and plasticity separately. However, none considers the increasing difficulty of the classification task, which inherently results in performance loss for any model. In that sense, we analyze some limitations of current metrics and identify the presence of setup-induced forgetting. Therefore, we propose new metrics that account for the task's increasing difficulty. Through experiments on 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24615;&#30340;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#28165;&#29702;&#21644;&#27714;&#35299;&#20809;&#23398;&#36816;&#21160;&#25429;&#25417;&#25968;&#25454;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20351;&#29992;&#29305;&#23450;&#30340;&#22270;&#21367;&#31215;&#25805;&#20316;&#65292;&#25552;&#21462;&#26631;&#35760;&#21644;&#20851;&#33410;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#36716;&#21270;&#20026;&#26356;&#20934;&#30830;&#30340;&#21160;&#20316;&#12290;&#36890;&#36807;&#30740;&#31350;&#26631;&#35760;&#30340;&#21160;&#20316;&#19982;&#20854;&#30456;&#37051;&#26631;&#35760;&#30340;&#30456;&#20851;&#24615;&#65292;&#35770;&#25991;&#33021;&#22815;&#39640;&#25928;&#22320;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21152;&#36895;&#24230;&#36718;&#24275;&#35782;&#21035;&#36319;&#36394;&#38169;&#35823;&#24341;&#36215;&#30340;&#24322;&#24120;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00428</link><description>&lt;p&gt;
&#20809;&#23398;&#36816;&#21160;&#25429;&#25417;&#30340;&#22522;&#20110;&#23616;&#37096;&#24615;&#30340;&#31070;&#32463;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Locality-based Neural Solver for Optical Motion Capture. (arXiv:2309.00428v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#24615;&#30340;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#28165;&#29702;&#21644;&#27714;&#35299;&#20809;&#23398;&#36816;&#21160;&#25429;&#25417;&#25968;&#25454;&#12290;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20351;&#29992;&#29305;&#23450;&#30340;&#22270;&#21367;&#31215;&#25805;&#20316;&#65292;&#25552;&#21462;&#26631;&#35760;&#21644;&#20851;&#33410;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#36716;&#21270;&#20026;&#26356;&#20934;&#30830;&#30340;&#21160;&#20316;&#12290;&#36890;&#36807;&#30740;&#31350;&#26631;&#35760;&#30340;&#21160;&#20316;&#19982;&#20854;&#30456;&#37051;&#26631;&#35760;&#30340;&#30456;&#20851;&#24615;&#65292;&#35770;&#25991;&#33021;&#22815;&#39640;&#25928;&#22320;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21152;&#36895;&#24230;&#36718;&#24275;&#35782;&#21035;&#36319;&#36394;&#38169;&#35823;&#24341;&#36215;&#30340;&#24322;&#24120;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23616;&#37096;&#24615;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#28165;&#29702;&#21644;&#27714;&#35299;&#20809;&#23398;&#36816;&#21160;&#25429;&#25417;&#25968;&#25454;&#12290;&#32473;&#23450;&#22122;&#22768;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#26631;&#35760;&#21644;&#20851;&#33410;&#35270;&#20026;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#65292;&#24182;&#20351;&#29992;&#22270;&#21367;&#31215;&#25805;&#20316;&#25552;&#21462;&#26631;&#35760;&#21644;&#20851;&#33410;&#30340;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#28165;&#26224;&#30340;&#21160;&#20316;&#12290;&#20026;&#20102;&#22788;&#29702;&#24322;&#24120;&#26631;&#35760;&#65288;&#20363;&#22914;&#36974;&#25377;&#25110;&#20855;&#26377;&#36739;&#22823;&#30340;&#36319;&#36394;&#35823;&#24046;&#65289;&#65292;&#20851;&#38190;&#27934;&#23519;&#21147;&#22312;&#20110;&#26631;&#35760;&#30340;&#21160;&#20316;&#19982;&#20854;&#30452;&#25509;&#30456;&#37051;&#30340;&#26631;&#35760;&#30340;&#21160;&#20316;&#20043;&#38388;&#23384;&#22312;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#19982;&#20854;&#20182;&#26631;&#35760;&#30340;&#30456;&#20851;&#24615;&#36739;&#23567;&#65292;&#21363;&#23616;&#37096;&#24615;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#35760;&#65288;&#20363;&#22914;&#30001;&#20110;&#36974;&#25377;&#24341;&#36215;&#30340;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#30740;&#31350;&#21152;&#36895;&#24230;&#36718;&#24275;&#26469;&#35782;&#21035;&#30001;&#20110;&#36319;&#36394;&#38169;&#35823;&#24341;&#36215;&#30340;&#24322;&#24120;&#26631;&#35760;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#20855;&#26377;&#23631;&#34109;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#23631;&#34109;&#26041;&#26696;&#26088;&#22312;&#27169;&#25311;&#36974;&#25377;&#21644;&#22122;&#22768;&#31561;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel locality-based learning method for cleaning and solving optical motion capture data. Given noisy marker data, we propose a new heterogeneous graph neural network which treats markers and joints as different types of nodes, and uses graph convolution operations to extract the local features of markers and joints and transform them to clean motions. To deal with anomaly markers (e.g. occluded or with big tracking errors), the key insight is that a marker's motion shows strong correlations with the motions of its immediate neighboring markers but less so with other markers, a.k.a. locality, which enables us to efficiently fill missing markers (e.g. due to occlusion). Additionally, we also identify marker outliers due to tracking errors by investigating their acceleration profiles. Finally, we propose a training regime based on representation learning and data augmentation, by training the model on data with masking. The masking schemes aim to mimic the occluded and nois
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#32422;&#26463;&#36923;&#36753;&#32534;&#31243;&#36827;&#34892;&#35299;&#37322;&#24615;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#20915;&#31574;&#26641;&#25552;&#20379;&#22768;&#26126;&#24615;&#12289;&#20132;&#20114;&#24335;&#30340;&#35299;&#37322;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#35299;&#37322;&#26041;&#27861;&#20013;&#23545;&#32972;&#26223;&#30693;&#35782;&#30340;&#19981;&#20805;&#20998;&#32467;&#21512;&#21644;&#19982;&#29992;&#25143;&#30340;&#32570;&#20047;&#25277;&#35937;&#21644;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.00422</link><description>&lt;p&gt;
&#20351;&#29992;&#32422;&#26463;&#36923;&#36753;&#32534;&#31243;&#36827;&#34892;&#35299;&#37322;&#24615;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Declarative Reasoning on Explanations Using Constraint Logic Programming. (arXiv:2309.00422v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00422
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#32422;&#26463;&#36923;&#36753;&#32534;&#31243;&#36827;&#34892;&#35299;&#37322;&#24615;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#20915;&#31574;&#26641;&#25552;&#20379;&#22768;&#26126;&#24615;&#12289;&#20132;&#20114;&#24335;&#30340;&#35299;&#37322;&#65292;&#20811;&#26381;&#20102;&#24403;&#21069;&#35299;&#37322;&#26041;&#27861;&#20013;&#23545;&#32972;&#26223;&#30693;&#35782;&#30340;&#19981;&#20805;&#20998;&#32467;&#21512;&#21644;&#19982;&#29992;&#25143;&#30340;&#32570;&#20047;&#25277;&#35937;&#21644;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#19981;&#36879;&#26126;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#26085;&#30410;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;&#35299;&#37322;&#26041;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#23545;&#32972;&#26223;&#30693;&#35782;&#30340;&#19981;&#20805;&#20998;&#32467;&#21512;&#65292;&#20197;&#21450;&#19982;&#29992;&#25143;&#30340;&#32570;&#20047;&#25277;&#35937;&#21644;&#20114;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32422;&#26463;&#36923;&#36753;&#32534;&#31243;&#65288;CLP&#65289;&#30340;REASONX&#35299;&#37322;&#26041;&#27861;&#12290;REASONX&#21487;&#20197;&#20026;&#20915;&#31574;&#26641;&#25552;&#20379;&#22768;&#26126;&#24615;&#30340;&#20132;&#20114;&#24335;&#35299;&#37322;&#65292;&#36825;&#20123;&#20915;&#31574;&#26641;&#21487;&#20197;&#26159;&#20998;&#26512;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25110;&#20219;&#20309;&#40657;&#30418;&#27169;&#22411;&#30340;&#20840;&#23616;/&#23616;&#37096;&#26367;&#20195;&#27169;&#22411;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#32447;&#24615;&#32422;&#26463;&#21644;&#22522;&#20110;&#20107;&#23454;&#21644;&#23545;&#27604;&#23454;&#20363;&#30340;&#29305;&#24449;&#30340;MILP&#20248;&#21270;&#26469;&#34920;&#36798;&#32972;&#26223;&#30693;&#35782;&#25110;&#24120;&#35782;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#25237;&#24433;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#19982;&#31572;&#26696;&#32422;&#26463;&#36827;&#34892;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20171;&#32461;&#20102;REASONX&#30340;&#26550;&#26500;&#65292;&#23427;&#30001;&#25509;&#36817;&#29992;&#25143;&#30340;Python&#23618;&#21644;CLP&#23618;&#32452;&#25104;&#12290;REASONX&#30340;&#26680;&#24515;&#25191;&#34892;&#24341;&#25806;&#26159;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#35821;&#20041;&#30340;Prolog&#20803;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining opaque Machine Learning (ML) models is an increasingly relevant problem. Current explanation in AI (XAI) methods suffer several shortcomings, among others an insufficient incorporation of background knowledge, and a lack of abstraction and interactivity with the user. We propose REASONX, an explanation method based on Constraint Logic Programming (CLP). REASONX can provide declarative, interactive explanations for decision trees, which can be the ML models under analysis or global/local surrogate models of any black-box model. Users can express background or common sense knowledge using linear constraints and MILP optimization over features of factual and contrastive instances, and interact with the answer constraints at different levels of abstraction through constraint projection. We present here the architecture of REASONX, which consists of a Python layer, closer to the user, and a CLP layer. REASONX's core execution engine is a Prolog meta-program with declarative seman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22238;&#24402;&#30340;&#26465;&#20214;&#29983;&#23384;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#38754;&#31215;&#20316;&#20026;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#21464;&#37327;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00417</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#23384;&#39044;&#27979;&#20013;&#30340;&#38754;&#31215;&#35268;&#33539;COBRA
&lt;/p&gt;
&lt;p&gt;
Area-norm COBRA on Conditional Survival Prediction. (arXiv:2309.00417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22238;&#24402;&#30340;&#26465;&#20214;&#29983;&#23384;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#20351;&#29992;&#38754;&#31215;&#20316;&#20026;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#21464;&#37327;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#32452;&#21512;&#22238;&#24402;&#31574;&#30053;&#26469;&#35745;&#31639;&#26465;&#20214;&#29983;&#23384;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22238;&#24402;&#30340;&#24369;&#23398;&#20064;&#22120;&#26469;&#21019;&#24314;&#25152;&#25552;&#20986;&#30340;&#38598;&#25104;&#25216;&#26415;&#12290;&#25152;&#25552;&#20986;&#30340;&#32452;&#21512;&#22238;&#24402;&#31574;&#30053;&#20351;&#29992;&#30456;&#20284;&#24230;&#24230;&#37327;&#20316;&#20026;&#20004;&#20010;&#29983;&#23384;&#26354;&#32447;&#20043;&#38388;&#30340;&#38754;&#31215;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#26126;&#20854;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#29983;&#23384;&#26862;&#26519;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#22312;&#32452;&#21512;&#22238;&#24402;&#35774;&#32622;&#20013;&#36873;&#25321;&#26368;&#37325;&#35201;&#21464;&#37327;&#30340;&#26032;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#27169;&#25311;&#30740;&#31350;&#65292;&#34920;&#26126;&#25105;&#20204;&#23545;&#21464;&#37327;&#30456;&#20851;&#24615;&#30340;&#25552;&#35758;&#25928;&#26524;&#24456;&#22909;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#35828;&#26126;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper explores a different variation of combined regression strategy to calculate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The proposed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simulation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#20010;&#24615;&#21270;&#12289;&#38544;&#31169;&#20445;&#35777;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#19977;&#20803;&#20132;&#20114;&#20316;&#29992;&#12290;&#24046;&#20998;&#38544;&#31169;&#21450;&#20854;&#21464;&#20307;&#34987;&#24212;&#29992;&#20026;&#25552;&#20379;&#27491;&#24335;&#38544;&#31169;&#20445;&#35777;&#30340;&#21069;&#27839;&#26631;&#20934;&#12290;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#20013;&#23547;&#27714;&#20844;&#24179;&#24615;&#21464;&#24471;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.00416</link><description>&lt;p&gt;
&#25512;&#36827;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65306;&#22242;&#20307;&#38544;&#31169;&#12289;&#20844;&#24179;&#24615;&#31561;&#26041;&#38754;&#30340;&#31361;&#30772;
&lt;/p&gt;
&lt;p&gt;
Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond. (arXiv:2309.00416v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#20010;&#24615;&#21270;&#12289;&#38544;&#31169;&#20445;&#35777;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#30340;&#19977;&#20803;&#20132;&#20114;&#20316;&#29992;&#12290;&#24046;&#20998;&#38544;&#31169;&#21450;&#20854;&#21464;&#20307;&#34987;&#24212;&#29992;&#20026;&#25552;&#20379;&#27491;&#24335;&#38544;&#31169;&#20445;&#35777;&#30340;&#21069;&#27839;&#26631;&#20934;&#12290;&#22312;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#20013;&#23547;&#27714;&#20844;&#24179;&#24615;&#21464;&#24471;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26159;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#21644;&#21327;&#20316;&#26041;&#24335;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#19968;&#32452;&#21442;&#19982;&#30340;&#23458;&#25143;&#31471;&#22788;&#29702;&#26412;&#22320;&#23384;&#20648;&#30340;&#25968;&#25454;&#65292;&#20165;&#20849;&#20139;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#26412;&#22320;&#36755;&#20837;&#30340;&#25104;&#26412;&#20989;&#25968;&#33719;&#24471;&#30340;&#27169;&#22411;&#26356;&#26032;&#12290;FL&#34987;&#25552;&#20986;&#20316;&#20026;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#31181;&#36884;&#24452;&#65292;&#20294;&#24050;&#34987;&#35777;&#26126;&#26131;&#21463;&#31169;&#20154;&#20449;&#24687;&#27844;&#38706;&#12289;&#27169;&#22411;&#20010;&#24615;&#21270;&#32570;&#22833;&#20197;&#21450;&#21487;&#33021;&#23548;&#33268;&#26576;&#20123;&#32676;&#20307;&#27604;&#20854;&#20182;&#32676;&#20307;&#26356;&#20844;&#24179;&#30340;&#35757;&#32451;&#27169;&#22411;&#31561;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;FL&#26694;&#26550;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20010;&#24615;&#21270;&#12289;&#38544;&#31169;&#20445;&#35777;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#19977;&#20803;&#20132;&#20114;&#20316;&#29992;&#12290;&#24046;&#20998;&#38544;&#31169;&#21450;&#20854;&#21464;&#20307;&#24050;&#34987;&#30740;&#31350;&#21644;&#24212;&#29992;&#20026;&#25552;&#20379;&#27491;&#24335;&#38544;&#31169;&#20445;&#35777;&#30340;&#21069;&#27839;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;FL&#20013;&#30340;&#23458;&#25143;&#31471;&#24448;&#24448;&#25317;&#26377;&#38750;&#24120;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#20195;&#34920;&#30528;&#24322;&#36136;&#30340;&#31038;&#21306;&#65292;&#36825;&#20351;&#24471;&#20445;&#35777;&#20844;&#24179;&#24615;&#21464;&#24471;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a framework for training machine learning models in a distributed and collaborative manner. During training, a set of participating clients process their data stored locally, sharing only the model updates obtained by minimizing a cost function over their local inputs. FL was proposed as a stepping-stone towards privacy-preserving machine learning, but it has been shown vulnerable to issues such as leakage of private information, lack of personalization of the model, and the possibility of having a trained model that is fairer to some groups than to others. In this paper, we address the triadic interaction among personalization, privacy guarantees, and fairness attained by models trained within the FL framework. Differential privacy and its variants have been studied and applied as cutting-edge standards for providing formal privacy guarantees. However, clients in FL often hold very diverse datasets representing heterogeneous communities, making it important 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22330;&#26223;&#25991;&#26412;&#21435;&#38500;&#65288;SSTR&#65289;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#30446;&#26631;&#35789;&#27719;&#26469;&#21435;&#38500;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#30446;&#26631;&#35789;&#27719;&#30340;&#21435;&#38500;&#12290;</title><link>http://arxiv.org/abs/2309.00410</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#22330;&#26223;&#25991;&#26412;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
Selective Scene Text Removal. (arXiv:2309.00410v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22330;&#26223;&#25991;&#26412;&#21435;&#38500;&#65288;SSTR&#65289;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#30446;&#26631;&#35789;&#27719;&#26469;&#21435;&#38500;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#30446;&#26631;&#35789;&#27719;&#30340;&#21435;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#25991;&#26412;&#21435;&#38500;&#65288;Scene text removal&#65292;STR&#65289;&#26159;&#19968;&#31181;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#65292;&#29992;&#20110;&#21435;&#38500;&#22330;&#26223;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#21306;&#22495;&#12290;&#20256;&#32479;&#30340;STR&#26041;&#27861;&#20250;&#21024;&#38500;&#25152;&#26377;&#30340;&#22330;&#26223;&#25991;&#26412;&#12290;&#36825;&#24847;&#21619;&#30528;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#36873;&#25321;&#35201;&#21024;&#38500;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#22330;&#26223;&#25991;&#26412;&#21435;&#38500;&#65288;Selective scene text removal&#65292;SSTR&#65289;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#21482;&#21024;&#38500;&#29992;&#25143;&#25351;&#23450;&#30340;&#30446;&#26631;&#35789;&#27719;&#12290;&#34429;&#28982;SSTR&#27604;STR&#26356;&#22797;&#26434;&#65292;&#20294;&#26159;&#25105;&#20204;&#25552;&#20986;&#30340;&#22810;&#27169;&#22359;&#32467;&#26500;&#20351;&#24471;SSTR&#30340;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22914;&#39044;&#26399;&#22320;&#21435;&#38500;&#30446;&#26631;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene text removal (STR) is the image transformation task to remove text regions in scene images. The conventional STR methods remove all scene text. This means that the existing methods cannot select text to be removed. In this paper, we propose a novel task setting named selective scene text removal (SSTR) that removes only target words specified by the user. Although SSTR is a more complex task than STR, the proposed multi-module structure enables efficient training for SSTR. Experimental results show that the proposed method can remove target words as expected.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#25552;&#21319;&#34507;&#30333;&#36136;&#37325;&#35774;&#35745;&#31639;&#27861;AOBB-K*&#30340;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22686;&#24378;&#29256;&#12289;&#24102;&#26377;&#21160;&#24577;&#21551;&#21457;&#24335;&#21644;&#24102;&#26377;&#19979;&#28322;&#20248;&#21270;&#30340;&#26032;&#29256;&#26412;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00408</link><description>&lt;p&gt;
&#25552;&#21319;&#22522;&#20110;AND/OR&#30340;&#35745;&#31639;&#34507;&#30333;&#36136;&#35774;&#35745;&#65306;&#21160;&#24577;&#21551;&#21457;&#24335;&#21644;&#21487;&#27867;&#21270;&#30340;UFO
&lt;/p&gt;
&lt;p&gt;
Boosting AND/OR-Based Computational Protein Design: Dynamic Heuristics and Generalizable UFO. (arXiv:2309.00408v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00408
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#19987;&#27880;&#20110;&#25552;&#21319;&#34507;&#30333;&#36136;&#37325;&#35774;&#35745;&#31639;&#27861;AOBB-K*&#30340;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22686;&#24378;&#29256;&#12289;&#24102;&#26377;&#21160;&#24577;&#21551;&#21457;&#24335;&#21644;&#24102;&#26377;&#19979;&#28322;&#20248;&#21270;&#30340;&#26032;&#29256;&#26412;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#35745;&#31639;&#22312;&#31070;&#32463;&#32593;&#32476;&#31561;&#25216;&#26415;&#30340;&#25512;&#21160;&#19979;&#21462;&#24471;&#20102;&#39134;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#37325;&#35201;&#20219;&#21153;&#23545;&#20110;&#36825;&#20123;&#25216;&#26415;&#26469;&#35828;&#24182;&#19981;&#36866;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#20256;&#32479;&#25512;&#29702;&#26041;&#26696;&#36827;&#34892;&#21019;&#26032;&#12290;&#20854;&#20013;&#19968;&#20010;&#20219;&#21153;&#23601;&#26159;&#34507;&#30333;&#36136;&#37325;&#35774;&#35745;&#12290;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#37325;&#35774;&#35745;&#31639;&#27861;AOBB-K*&#65292;&#22312;&#23567;&#35268;&#27169;&#34507;&#30333;&#36136;&#37325;&#35774;&#35745;&#38382;&#39064;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;BBK*&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#28982;&#32780;&#65292;AOBB-K*&#22312;&#25193;&#23637;&#24615;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#25193;&#23637;AOBB-K*&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#29256;&#26412;&#65306;AOBB-K*-b&#65288;&#22686;&#24378;&#29256;&#65289;&#12289;AOBB-K*-DH&#65288;&#24102;&#26377;&#21160;&#24577;&#21551;&#21457;&#24335;&#65289;&#21644;AOBB-K*-UFO&#65288;&#24102;&#26377;&#19979;&#28322;&#20248;&#21270;&#65289;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific computing has experienced a surge empowered by advancements in technologies such as neural networks. However, certain important tasks are less amenable to these technologies, benefiting from innovations to traditional inference schemes. One such task is protein re-design. Recently a new re-design algorithm, AOBB-K*, was introduced and was competitive with state-of-the-art BBK* on small protein re-design problems. However, AOBB-K* did not scale well. In this work we focus on scaling up AOBB-K* and introduce three new versions: AOBB-K*-b (boosted), AOBB-K*-DH (with dynamic heuristics), and AOBB-K*-UFO (with underflow optimization) that significantly enhance scalability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#26356;&#28789;&#27963;&#30340;&#32534;&#30721;&#29305;&#24449;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#32039;&#23494;&#22320;&#19979;&#30028;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#12290;</title><link>http://arxiv.org/abs/2309.00380</link><description>&lt;p&gt;
&#29992;&#25490;&#24207;&#19981;&#21464;&#30340;&#32534;&#30721;&#22120;&#21644;&#26356;&#32039;&#30340;&#21464;&#20998;&#36793;&#30028;&#23398;&#20064;&#22810;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds. (arXiv:2309.00380v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#26356;&#28789;&#27963;&#30340;&#32534;&#30721;&#29305;&#24449;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#32039;&#23494;&#22320;&#19979;&#30028;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20027;&#39064;&#12290;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120; (VAE) &#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#23427;&#23398;&#20064;&#33021;&#22815;&#20849;&#21516;&#35299;&#37322;&#22810;&#31181;&#27169;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#21508;&#31181;&#23458;&#35266;&#20989;&#25968;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#24448;&#24448;&#20197;&#22810;&#27169;&#24577;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#30340;&#19979;&#30028;&#20197;&#21450;&#20449;&#24687;&#35770;&#26041;&#38754;&#30340;&#32771;&#34385;&#20026;&#21160;&#26426;&#12290;&#20026;&#20102;&#23545;&#19981;&#21516;&#27169;&#24577;&#23376;&#38598;&#36827;&#34892;&#32534;&#30721;&#65292;&#25105;&#20204;&#32463;&#24120;&#20351;&#29992;&#24182;&#23637;&#31034;&#20102;&#20135;&#21697;&#22411;&#19987;&#23478; (PoE) &#25110;&#32773;&#28151;&#21512;&#22411;&#19987;&#23478; (MoE) &#32858;&#21512;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#22312;&#29983;&#25104;&#36136;&#37327;&#25110;&#32773;&#22810;&#27169;&#24577;&#19968;&#33268;&#24615;&#31561;&#26041;&#38754;&#20855;&#26377;&#19981;&#21516;&#30340;&#26435;&#34913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#33021;&#22815;&#32039;&#23494;&#22320;&#19979;&#30028;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#30340;&#21464;&#20998;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#32534;&#30721;&#29305;&#24449;&#32452;&#21512;&#36215;&#26469;&#65292;&#24320;&#21457;&#20102;&#26356;&#28789;&#27963;&#30340;&#32858;&#21512;&#26041;&#26696;&#65292;&#36825;&#20123;&#26041;&#26696;&#25512;&#24191;&#20102; PoE &#25110;&#32773; MoE &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Devising deep latent variable models for multi-modal data has been a long-standing theme in machine learning research. Multi-modal Variational Autoencoders (VAEs) have been a popular generative model class that learns latent representations which jointly explain multiple modalities. Various objective functions for such models have been suggested, often motivated as lower bounds on the multi-modal data log-likelihood or from information-theoretic considerations. In order to encode latent variables from different modality subsets, Product-of-Experts (PoE) or Mixture-of-Experts (MoE) aggregation schemes have been routinely used and shown to yield different trade-offs, for instance, regarding their generative quality or consistency across multiple modalities. In this work, we consider a variational bound that can tightly lower bound the data log-likelihood. We develop more flexible aggregation schemes that generalise PoE or MoE approaches by combining encoded features from different modali
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#39118;&#38505;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20811;&#26381;&#20102;&#21333;&#31867;&#20998;&#31867;&#24322;&#24120;&#26816;&#27979;&#20013;&#23545;&#26410;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#21482;&#21253;&#21547;&#27491;&#24120;&#23454;&#20363;&#30340;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00379</link><description>&lt;p&gt;
&#22522;&#20110;&#39118;&#38505;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection with semi-supervised classification based on risk estimators. (arXiv:2309.00379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00379
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39118;&#38505;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#20811;&#26381;&#20102;&#21333;&#31867;&#20998;&#31867;&#24322;&#24120;&#26816;&#27979;&#20013;&#23545;&#26410;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#21482;&#21253;&#21547;&#27491;&#24120;&#23454;&#20363;&#30340;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#31867;&#20998;&#31867;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#26159;&#20854;&#20381;&#36182;&#20110;&#26410;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21482;&#21253;&#21547;&#27491;&#24120;&#23454;&#20363;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20998;&#31867;&#30340;&#26032;&#22411;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#22120;&#30340;&#21322;&#30417;&#30563;&#27973;&#23618;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#36127;&#65288;&#26377;&#20559;&#65289;&#39118;&#38505;&#20272;&#35745;&#22120;&#30340;&#21322;&#30417;&#30563;&#28145;&#23618;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#20110;&#20004;&#20010;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#30340;&#20272;&#35745;&#35823;&#24046;&#30028;&#21644;&#36229;&#20986;&#39118;&#38505;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36873;&#25321;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#27973;&#23618;&#27169;&#22411;&#19979;&#32463;&#39564;&#39118;&#38505;&#30340;&#38750;&#36127;&#24615;&#22312;&#29305;&#23450;&#25439;&#22833;&#20989;&#25968;&#19979;&#25104;&#31435;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#25552;&#20379;&#20102;&#23545;&#20110;&#22522;&#20110;&#39118;&#38505;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#24378;&#26377;&#21147;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant limitation of one-class classification anomaly detection methods is their reliance on the assumption that unlabeled training data only contains normal instances. To overcome this impractical assumption, we propose two novel classification-based anomaly detection methods. Firstly, we introduce a semi-supervised shallow anomaly detection method based on an unbiased risk estimator. Secondly, we present a semi-supervised deep anomaly detection method utilizing a nonnegative (biased) risk estimator. We establish estimation error bounds and excess risk bounds for both risk minimizers. Additionally, we propose techniques to select appropriate regularization parameters that ensure the nonnegativity of the empirical risk in the shallow model under specific loss functions. Our extensive experiments provide strong evidence of the effectiveness of the risk-based anomaly detection methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#38271;&#31243;&#22270;&#34920;&#22522;&#20934;&#65288;LRGB&#65289;&#36827;&#34892;&#20102;&#37325;&#26032;&#35780;&#20272;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#65292;&#20808;&#21069;&#30340;&#25253;&#21578;&#24615;&#33021;&#24046;&#36317;&#34987;&#39640;&#20272;&#20102;&#65292;&#32780;&#32463;&#36807;&#22522;&#26412;&#36229;&#21442;&#25968;&#20248;&#21270;&#21518;&#65292;&#24046;&#36317;&#23436;&#20840;&#28040;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29305;&#24449;&#24402;&#19968;&#21270;&#30340;&#32570;&#22833;&#21644;&#38142;&#25509;&#39044;&#27979;&#24230;&#37327;&#30340;&#34394;&#20551;&#23454;&#29616;&#23545;LRGB&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00367</link><description>&lt;p&gt;
&#38271;&#31243;&#22270;&#34920;&#22522;&#20934;&#30340;&#37325;&#26032;&#35780;&#20272;&#65306;&#24046;&#36317;&#21435;&#21738;&#20799;&#20102;&#65311;
&lt;/p&gt;
&lt;p&gt;
Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark. (arXiv:2309.00367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38271;&#31243;&#22270;&#34920;&#22522;&#20934;&#65288;LRGB&#65289;&#36827;&#34892;&#20102;&#37325;&#26032;&#35780;&#20272;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#35777;&#20998;&#26512;&#21457;&#29616;&#65292;&#20808;&#21069;&#30340;&#25253;&#21578;&#24615;&#33021;&#24046;&#36317;&#34987;&#39640;&#20272;&#20102;&#65292;&#32780;&#32463;&#36807;&#22522;&#26412;&#36229;&#21442;&#25968;&#20248;&#21270;&#21518;&#65292;&#24046;&#36317;&#23436;&#20840;&#28040;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29305;&#24449;&#24402;&#19968;&#21270;&#30340;&#32570;&#22833;&#21644;&#38142;&#25509;&#39044;&#27979;&#24230;&#37327;&#30340;&#34394;&#20551;&#23454;&#29616;&#23545;LRGB&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#38271;&#31243;&#22270;&#34920;&#22522;&#20934;(LRGB&#65292;Dwivedi&#31561;&#65292;2022)&#24341;&#20837;&#20102;&#19968;&#32452;&#19982;&#39030;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#23494;&#20999;&#30456;&#20851;&#30340;&#22270;&#34920;&#23398;&#20064;&#20219;&#21153;&#12290;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#22270;&#24418;&#21464;&#25442;&#22120;&#26126;&#26174;&#20248;&#20110;&#28040;&#24687;&#20256;&#36882;GNN&#65288;MPGNN&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;LRGB&#19978;&#30340;&#22810;&#20010;MPGNN&#22522;&#32447;&#20197;&#21450;&#22270;&#24418;&#21464;&#25442;&#22120;GPS&#65288;Ramp\'a\v{s}ek&#31561;&#65292;2022&#65289;&#36827;&#34892;&#20102;&#20180;&#32454;&#37325;&#26032;&#35780;&#20272;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#20110;&#23376;&#20248;&#36229;&#21442;&#25968;&#36873;&#25321;&#19981;&#24403;&#32780;&#39640;&#20272;&#20102;&#25253;&#21578;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#22522;&#26412;&#36229;&#21442;&#25968;&#20248;&#21270;&#21518;&#65292;&#36328;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#24615;&#33021;&#24046;&#36317;&#23436;&#20840;&#28040;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;LRGB&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#32570;&#20047;&#29305;&#24449;&#24402;&#19968;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#20986;&#20102;LRGB&#30340;&#38142;&#25509;&#39044;&#27979;&#24230;&#37327;&#30340;&#34394;&#20551;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#24314;&#31435;&#26356;&#39640;&#30340;&#23454;&#35777;&#20005;&#35880;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent Long-Range Graph Benchmark (LRGB, Dwivedi et al. 2022) introduced a set of graph learning tasks strongly dependent on long-range interaction between vertices. Empirical evidence suggests that on these tasks Graph Transformers significantly outperform Message Passing GNNs (MPGNNs). In this paper, we carefully reevaluate multiple MPGNN baselines as well as the Graph Transformer GPS (Ramp\'a\v{s}ek et al. 2022) on LRGB. Through a rigorous empirical analysis, we demonstrate that the reported performance gap is overestimated due to suboptimal hyperparameter choices. It is noteworthy that across multiple datasets the performance gap completely vanishes after basic hyperparameter optimization. In addition, we discuss the impact of lacking feature normalization for LRGB's vision datasets and highlight a spurious implementation of LRGB's link prediction metric. The principal aim of our paper is to establish a higher standard of empirical rigor within the graph machine learning commun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#20110;&#20919;&#21551;&#21160;&#38382;&#39064;&#20013;&#30340;&#20559;&#22909;&#33719;&#21462;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#32570;&#20047;&#29992;&#25143;&#23384;&#22312;&#25110;&#35775;&#38382;&#20854;&#20182;&#29992;&#25143;&#25968;&#25454;&#21463;&#38480;&#12290;&#25105;&#20204;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29992;&#25143;&#24037;&#20316;&#37327;&#26368;&#22823;&#21270;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#20559;&#22909;&#33719;&#21462;&#36807;&#31243;&#20013;&#37319;&#29992;&#26080;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00356</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#29992;&#20110;&#20559;&#22909;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Explainable Active Learning for Preference Elicitation. (arXiv:2309.00356v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#20110;&#20919;&#21551;&#21160;&#38382;&#39064;&#20013;&#30340;&#20559;&#22909;&#33719;&#21462;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#32570;&#20047;&#29992;&#25143;&#23384;&#22312;&#25110;&#35775;&#38382;&#20854;&#20182;&#29992;&#25143;&#25968;&#25454;&#21463;&#38480;&#12290;&#25105;&#20204;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29992;&#25143;&#24037;&#20316;&#37327;&#26368;&#22823;&#21270;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#20559;&#22909;&#33719;&#21462;&#36807;&#31243;&#20013;&#37319;&#29992;&#26080;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#20837;&#20102;&#35299;&#26032;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#38543;&#21518;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#38656;&#35201;&#26234;&#33021;&#22320;&#22788;&#29702;&#29992;&#25143;&#20132;&#20114;&#65292;&#21363;&#25552;&#20986;&#30456;&#20851;&#38382;&#39064;&#20197;&#26377;&#25928;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#29305;&#23450;&#24773;&#26223;&#65292;&#22312;&#35813;&#24773;&#26223;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#32570;&#20047;&#36275;&#22815;&#30340;&#29992;&#25143;&#23384;&#22312;&#25110;&#35775;&#38382;&#20854;&#20182;&#29992;&#25143;&#25968;&#25454;&#21463;&#38480;&#65292;&#38459;&#30861;&#20102;&#21033;&#29992;&#31995;&#32479;&#20013;&#29616;&#26377;&#25968;&#25454;&#30340;&#29992;&#25143;&#24314;&#27169;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;(AL)&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#22312;&#26368;&#23567;&#29992;&#25143;&#24037;&#20316;&#37327;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#20449;&#24687;&#33719;&#21462;&#12290;AL&#20174;&#19968;&#20010;&#22823;&#22411;&#26080;&#26631;&#31614;&#38598;&#21512;&#20013;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#21521;&#35810;&#38382;&#39044;&#27979;&#26631;&#31614;&#65292;&#24182;&#26368;&#32456;&#26356;&#26032;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#35299;&#37322;&#24615;&#20559;&#22909;&#33719;&#21462;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;&#26080;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;ML&#30340;&#38598;&#25104;&#36807;&#31243;&#12290;&#23427;&#21033;&#29992;&#29992;&#25143;&#23545;&#31995;&#32479;&#36820;&#22238;&#25512;&#33616;&#30340;&#21453;&#39304;&#65288;&#32473;&#20104;&#31995;&#32479;&#30340;&#27880;&#24847;&#25110;&#21916;&#22909;&#65289;&#21644;&#29992;&#25143;&#23545;&#38382;&#39064;&#30340;&#21453;&#39304;&#21521;&#20182;&#20204;&#35299;&#37322;&#21644;&#36741;&#21161;&#20445;&#25345;&#29992;&#25143;&#28385;&#24847;&#24230;&#21644;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaining insights into the preferences of new users and subsequently personalizing recommendations necessitate managing user interactions intelligently, namely, posing pertinent questions to elicit valuable information effectively. In this study, our focus is on a specific scenario of the cold-start problem, where the recommendation system lacks adequate user presence or access to other users' data is restricted, obstructing employing user profiling methods utilizing existing data in the system. We employ Active Learning (AL) to solve the addressed problem with the objective of maximizing information acquisition with minimal user effort. AL operates for selecting informative data from a large unlabeled set to inquire an oracle to label them and eventually updating a machine learning (ML) model. We operate AL in an integrated process of unsupervised, semi-supervised, and supervised ML within an explanatory preference elicitation process. It harvests user feedback (given for the system's 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#33258;&#21160;&#23454;&#39564;&#24179;&#21488;&#65292;&#29992;&#20110;&#23450;&#21046;&#35774;&#35745;&#20855;&#26377;&#30446;&#26631;&#20809;&#23398;&#24615;&#36136;&#30340;&#32435;&#31859;&#39063;&#31890;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#22120;&#65292;&#33021;&#22815;&#22312;&#20165;200&#27425;&#36845;&#20195;&#20869;&#39640;&#25928;&#22320;&#21512;&#25104;&#20855;&#26377;&#25152;&#38656;&#21560;&#25910;&#20809;&#35889;&#30340;&#38134;&#32435;&#31859;&#39063;&#31890;&#65292;&#24182;&#25581;&#31034;&#20102;&#28041;&#21450;&#26592;&#27308;&#37240;&#30416;&#25928;&#24212;&#30340;&#26032;&#21270;&#23398;&#21453;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.00349</link><description>&lt;p&gt;
&#31169;&#20154;&#23450;&#21046;&#32435;&#31859;&#39063;&#31890;&#21512;&#25104;&#19982;&#21270;&#23398;&#30693;&#35782;&#21457;&#29616;&#30340;&#33258;&#21160;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Bespoke Nanoparticle Synthesis and Chemical Knowledge Discovery Via Autonomous Experimentations. (arXiv:2309.00349v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#33258;&#21160;&#23454;&#39564;&#24179;&#21488;&#65292;&#29992;&#20110;&#23450;&#21046;&#35774;&#35745;&#20855;&#26377;&#30446;&#26631;&#20809;&#23398;&#24615;&#36136;&#30340;&#32435;&#31859;&#39063;&#31890;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#22120;&#65292;&#33021;&#22815;&#22312;&#20165;200&#27425;&#36845;&#20195;&#20869;&#39640;&#25928;&#22320;&#21512;&#25104;&#20855;&#26377;&#25152;&#38656;&#21560;&#25910;&#20809;&#35889;&#30340;&#38134;&#32435;&#31859;&#39063;&#31890;&#65292;&#24182;&#25581;&#31034;&#20102;&#28041;&#21450;&#26592;&#27308;&#37240;&#30416;&#25928;&#24212;&#30340;&#26032;&#21270;&#23398;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#20351;&#29992;&#22810;&#20010;&#21512;&#25104;&#21464;&#37327;&#30340;&#32435;&#31859;&#26448;&#26009;&#21512;&#25104;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#26497;&#20854;&#32321;&#37325;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#32452;&#21512;&#25506;&#32034;&#26041;&#27861;&#38750;&#24120;&#26114;&#36149;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#20010;&#33258;&#21160;&#23454;&#39564;&#24179;&#21488;&#65292;&#29992;&#20110;&#23450;&#21046;&#35774;&#35745;&#20855;&#26377;&#30446;&#26631;&#20809;&#23398;&#24615;&#36136;&#30340;&#32435;&#31859;&#39063;&#31890;&#65288;NPs&#65289;&#12290;&#35813;&#24179;&#21488;&#22312;&#32435;&#31859;&#39063;&#31890;&#25209;&#37327;&#21512;&#25104;&#27169;&#22359;&#21644;UV-Vis&#20809;&#35889;&#27169;&#22359;&#20043;&#38388;&#20197;&#38381;&#29615;&#26041;&#24335;&#36816;&#34892;&#65292;&#26681;&#25454;AI&#20248;&#21270;&#24314;&#27169;&#30340;&#21453;&#39304;&#12290;&#20197;&#38134;&#65288;Ag&#65289;&#32435;&#31859;&#39063;&#31890;&#20026;&#20195;&#34920;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23454;&#29616;&#26089;&#20572;&#27490;&#20934;&#21017;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#22120;&#33021;&#22815;&#22312;&#20165;200&#27425;&#36845;&#20195;&#65288;&#22312;&#20116;&#31181;&#21512;&#25104;&#35797;&#21058;&#20013;&#36827;&#34892;&#20248;&#21270;&#65289;&#20869;&#39640;&#25928;&#22320;&#20135;&#29983;&#20855;&#26377;&#25152;&#38656;&#21560;&#25910;&#20809;&#35889;&#30340;Ag&#32435;&#31859;&#39063;&#31890;&#12290;&#38500;&#20102;&#26448;&#26009;&#24320;&#21457;&#25928;&#29575;&#21331;&#36234;&#20043;&#22806;&#65292;&#23545;&#21512;&#25104;&#21464;&#37327;&#30340;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#28041;&#21450;&#26592;&#27308;&#37240;&#30416;&#25928;&#24212;&#30340;&#26032;&#21270;&#23398;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimization of nanomaterial synthesis using numerous synthetic variables is considered to be extremely laborious task because the conventional combinatorial explorations are prohibitively expensive. In this work, we report an autonomous experimentation platform developed for the bespoke design of nanoparticles (NPs) with targeted optical properties. This platform operates in a closed-loop manner between a batch synthesis module of NPs and a UV- Vis spectroscopy module, based on the feedback of the AI optimization modeling. With silver (Ag) NPs as a representative example, we demonstrate that the Bayesian optimizer implemented with the early stopping criterion can efficiently produce Ag NPs precisely possessing the desired absorption spectra within only 200 iterations (when optimizing among five synthetic reagents). In addition to the outstanding material developmental efficiency, the analysis of synthetic variables further reveals a novel chemistry involving the effects of citrate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#25903;&#25345;&#20896;&#29366;&#21160;&#33033;CT&#34880;&#31649;&#36896;&#24433;&#24739;&#32773;&#30340;&#39118;&#38505;&#20998;&#23618;&#21644;&#19979;&#19968;&#27493;&#27979;&#35797;&#36873;&#25321;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#22312;&#39118;&#38505;&#20998;&#23618;&#21644;&#39044;&#27979;&#19979;&#28216;&#27979;&#35797;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00330</link><description>&lt;p&gt;
&#20896;&#29366;&#21160;&#33033;CT&#34880;&#31649;&#36896;&#24433;&#24739;&#32773;&#39118;&#38505;&#20998;&#23618;&#21644;&#19979;&#19968;&#27493;&#39044;&#27979;&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multitask Deep Learning for Accurate Risk Stratification and Prediction of Next Steps for Coronary CT Angiography Patients. (arXiv:2309.00330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#25903;&#25345;&#20896;&#29366;&#21160;&#33033;CT&#34880;&#31649;&#36896;&#24433;&#24739;&#32773;&#30340;&#39118;&#38505;&#20998;&#23618;&#21644;&#19979;&#19968;&#27493;&#27979;&#35797;&#36873;&#25321;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#22312;&#39118;&#38505;&#20998;&#23618;&#21644;&#39044;&#27979;&#19979;&#28216;&#27979;&#35797;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35786;&#26029;&#24615;&#35843;&#26597;&#23545;&#30097;&#20284;&#21644;&#24050;&#30830;&#35786;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#30340;&#24739;&#32773;&#30340;&#39118;&#38505;&#20998;&#23618;&#21644;&#20020;&#24202;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20855;&#22823;&#22810;&#20165;&#19987;&#27880;&#20110;&#36873;&#25321;&#38376;&#35786;&#27979;&#35797;&#65292;&#32780;&#21482;&#26377;&#23569;&#25968;&#31995;&#32479;&#21253;&#21547;&#26377;&#20851;&#19979;&#28216;&#27979;&#35797;&#25110;&#27835;&#30103;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#25903;&#25345;&#20896;&#29366;&#21160;&#33033;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CCTA&#65289;&#24739;&#32773;&#30340;&#39118;&#38505;&#20998;&#23618;&#21644;&#19979;&#28216;&#27979;&#35797;&#36873;&#25321;&#12290;&#20998;&#26512;&#21253;&#25324;&#22312;2006&#24180;&#33267;2017&#24180;&#38388;&#36827;&#34892;CCTA&#26816;&#26597;&#30340;14,021&#21517;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#24863;&#30693;&#22120;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;CCTA&#25253;&#21578;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;CAD&#39118;&#38505;&#20998;&#23618;&#20013;&#23454;&#29616;&#20102;0.76&#30340;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#65292;&#22312;&#39044;&#27979;&#19979;&#28216;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;0.72&#30340;AUC&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#24739;&#32773;&#38656;&#35201;&#36827;&#34892;&#19979;&#28216;&#27979;&#35797;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnostic investigation has an important role in risk stratification and clinical decision making of patients with suspected and documented Coronary Artery Disease (CAD). However, the majority of existing tools are primarily focused on the selection of gatekeeper tests, whereas only a handful of systems contain information regarding the downstream testing or treatment. We propose a multi-task deep learning model to support risk stratification and down-stream test selection for patients undergoing Coronary Computed Tomography Angiography (CCTA). The analysis included 14,021 patients who underwent CCTA between 2006 and 2017. Our novel multitask deep learning framework extends the state-of-the art Perceiver model to deal with real-world CCTA report data. Our model achieved an Area Under the receiver operating characteristic Curve (AUC) of 0.76 in CAD risk stratification, and 0.72 AUC in predicting downstream tests. Our proposed deep learning model can accurately estimate the likelihood o
&lt;/p&gt;</description></item><item><title>Mi-Go is a testing framework that uses YouTube as a data source to evaluate speech recognition models like OpenAI's Whisper. It leverages diverse real-world scenarios, multiple languages and accents, and compares machine-generated transcriptions with human-made subtitles to identify potential misuse of YouTube subtitles.</title><link>http://arxiv.org/abs/2309.00329</link><description>&lt;p&gt;
Mi-Go: &#20351;&#29992;YouTube&#20316;&#20026;&#25968;&#25454;&#28304;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20687;OpenAI&#30340;Whisper&#36825;&#26679;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mi-Go: Test Framework which uses YouTube as Data Source for Evaluating Speech Recognition Models like OpenAI's Whisper. (arXiv:2309.00329v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00329
&lt;/p&gt;
&lt;p&gt;
Mi-Go is a testing framework that uses YouTube as a data source to evaluate speech recognition models like OpenAI's Whisper. It leverages diverse real-world scenarios, multiple languages and accents, and compares machine-generated transcriptions with human-made subtitles to identify potential misuse of YouTube subtitles.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Mi-Go&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#36890;&#29992;&#35821;&#38899;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#26679;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;YouTube&#20316;&#20026;&#20016;&#23500;&#19988;&#25345;&#32493;&#26356;&#26032;&#30340;&#25968;&#25454;&#28304;&#65292;&#28085;&#30422;&#22810;&#31181;&#35821;&#35328;&#12289;&#21475;&#38899;&#12289;&#26041;&#35328;&#12289;&#35828;&#35805;&#39118;&#26684;&#21644;&#38899;&#39057;&#36136;&#37327;&#27700;&#24179;&#12290;&#20026;&#20102;&#35777;&#26126;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#26412;&#25991;&#20197;OpenAI&#24320;&#21457;&#30340;Whisper&#27169;&#22411;&#20316;&#20026;&#27979;&#35797;&#23545;&#35937;&#12290;&#27979;&#35797;&#28041;&#21450;&#20351;&#29992;124&#20010;YouTube&#35270;&#39057;&#26469;&#27979;&#35797;&#25152;&#26377;Whisper&#27169;&#22411;&#30340;&#29256;&#26412;&#12290;&#32467;&#26524;&#24378;&#35843;&#20102;YouTube&#20316;&#20026;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#23453;&#36149;&#30340;&#27979;&#35797;&#24179;&#21488;&#30340;&#23454;&#29992;&#24615;&#65292;&#30830;&#20445;&#20854;&#23545;&#21508;&#31181;&#35821;&#35328;&#21644;&#22768;&#23398;&#26465;&#20214;&#30340;&#20581;&#22766;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23545;&#27604;&#26426;&#22120;&#29983;&#25104;&#30340;&#36716;&#24405;&#19982;&#20154;&#24037;&#21046;&#20316;&#30340;&#23383;&#24149;&#65292;Mi-Go&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#21457;&#29616;&#21487;&#33021;&#30340;YouTube&#23383;&#24149;&#28389;&#29992;&#65292;&#22914;&#25628;&#32034;&#24341;&#25806;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces Mi-Go, a novel testing framework aimed at evaluating the performance and adaptability of general-purpose speech recognition machine learning models across diverse real-world scenarios. The framework leverages YouTube as a rich and continuously updated data source, accounting for multiple languages, accents, dialects, speaking styles, and audio quality levels. To demonstrate the effectiveness of the framework, the Whisper model, developed by OpenAI, was employed as a test object. The tests involve using a total of 124 YouTube videos to test all Whisper model versions. The results underscore the utility of YouTube as a valuable testing platform for speech recognition models, ensuring their robustness, accuracy, and adaptability to diverse languages and acoustic conditions. Additionally, by contrasting the machine-generated transcriptions against human-made subtitles, the Mi-Go framework can help pinpoint potential misuse of YouTube subtitles, like Search Engine Op
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38477;&#32500;&#21644;&#22810;&#20445;&#30495;&#24230;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#30340;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#22312;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#26377;&#38480;&#25110;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#20302;&#20445;&#30495;&#24230;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#21040;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#20020;&#30028;&#30636;&#21464;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.00325</link><description>&lt;p&gt;
&#22810;&#23618;&#27425;&#20943;&#23567;&#38454;&#27425;&#20195;&#29702;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity reduced-order surrogate modeling. (arXiv:2309.00325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38477;&#32500;&#21644;&#22810;&#20445;&#30495;&#24230;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#30340;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#22312;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#26377;&#38480;&#25110;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#20302;&#20445;&#30495;&#24230;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#21040;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#20020;&#30028;&#30636;&#21464;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#20445;&#30495;&#24230;&#30340;&#37096;&#20998;&#24494;&#20998;&#26041;&#31243;&#25968;&#20540;&#27169;&#25311;&#21487;&#20197;&#26174;&#33879;&#38480;&#21046;&#25152;&#32771;&#34385;&#30340;&#21442;&#25968;&#37197;&#32622;&#25968;&#37327;&#21644;/&#25110;&#23545;&#32473;&#23450;&#31995;&#32479;&#24314;&#27169;&#30340;&#26102;&#38388;&#31383;&#21475;&#35780;&#20272;&#12290;&#22810;&#23618;&#27425;&#20195;&#29702;&#24314;&#27169;&#26088;&#22312;&#21033;&#29992;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#20294;&#19981;&#22826;&#20934;&#30830;&#30340;&#20302;&#20445;&#30495;&#24230;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#22312;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#26377;&#38480;&#25110;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20302;&#20445;&#30495;&#24230;&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#20934;&#30830;&#25429;&#25417;&#21040;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#20020;&#30028;&#30636;&#21464;&#30340;&#21457;&#29983;&#65292;&#20351;&#20854;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#31574;&#30053;&#65292;&#23558;&#38477;&#32500;&#21644;&#22810;&#20445;&#30495;&#24230;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#30456;&#32467;&#21512;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#23558;&#32463;&#20856;&#30340;&#36866;&#24212;&#20540;&#20998;&#35299;&#65288;POD&#65289;&#24212;&#29992;&#20110;&#39640;&#20445;&#30495;&#24230;&#27169;&#22411;&#26469;&#29983;&#25104;&#31354;&#38388;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-fidelity numerical simulations of partial differential equations (PDEs) given a restricted computational budget can significantly limit the number of parameter configurations considered and/or time window evaluated for modeling a given system. Multi-fidelity surrogate modeling aims to leverage less accurate, lower-fidelity models that are computationally inexpensive in order to enhance predictive accuracy when high-fidelity data are limited or scarce. However, low-fidelity models, while often displaying important qualitative spatio-temporal features, fail to accurately capture the onset of instability and critical transients observed in the high-fidelity models, making them impractical as surrogate models. To address this shortcoming, we present a new data-driven strategy that combines dimensionality reduction with multi-fidelity neural network surrogates. The key idea is to generate a spatial basis by applying the classical proper orthogonal decomposition (POD) to high-fidelity s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24320;&#21457;&#24182;&#30740;&#31350;&#20102;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39640;&#25928;&#39044;&#27979;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24494;&#32467;&#26500;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00305</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#26448;&#26009;&#31185;&#23398;&#27169;&#25311;&#20195;&#29702;&#27169;&#22411;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24494;&#32467;&#26500;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties. (arXiv:2309.00305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#24320;&#21457;&#24182;&#30740;&#31350;&#20102;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39640;&#25928;&#39044;&#27979;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#24494;&#32467;&#26500;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#12289;&#27668;&#35937;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#24037;&#31243;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#30830;&#23450;&#12289;&#29702;&#35299;&#21644;&#39044;&#27979;&#25152;&#35859;&#30340;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#32467;&#26500;&#25351;&#30340;&#26159;&#29289;&#36136;&#12289;&#26448;&#26009;&#25110;&#29289;&#36136;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#32780;&#24615;&#36136;&#26159;&#19968;&#31181;&#32467;&#26524;&#29305;&#24615;&#65292;&#36890;&#24120;&#20197;&#38750;&#24179;&#20961;&#30340;&#26041;&#24335;&#21462;&#20915;&#20110;&#32467;&#26500;&#30340;&#31354;&#38388;&#32454;&#33410;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#37319;&#29992;&#27491;&#21521;&#27169;&#25311;&#27169;&#22411;&#36827;&#34892;&#12290;&#26368;&#36817;&#65292;&#22312;&#36825;&#20123;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#24212;&#29992;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#22686;&#24378;&#21644;&#21152;&#36895;&#27169;&#25311;&#27169;&#22411;&#65292;&#25110;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#30740;&#31350;&#20102;&#22522;&#20110;&#20004;&#20010;&#19981;&#21516;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#30340;&#20845;&#31181;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65306;&#29992;&#20110;&#39044;&#27979;&#30913;&#24615;&#22495;&#24418;&#25104;&#30340;&#20108;&#32500;&#28784;&#24230;&#27169;&#22411;&#30340;&#25968;&#25454;&#65292;&#20197;&#21450;&#34920;&#31034;&#21452;&#30456;&#24494;&#32467;&#26500;&#28436;&#21464;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining, understanding, and predicting the so-called structure-property relation is an important task in many scientific disciplines, such as chemistry, biology, meteorology, physics, engineering, and materials science. Structure refers to the spatial distribution of, e.g., substances, material, or matter in general, while property is a resulting characteristic that usually depends in a non-trivial way on spatial details of the structure. Traditionally, forward simulations models have been used for such tasks. Recently, several machine learning algorithms have been applied in these scientific fields to enhance and accelerate simulation models or as surrogate models. In this work, we develop and investigate the applications of six machine learning techniques based on two different datasets from the domain of materials science: data from a two-dimensional Ising model for predicting the formation of magnetic domains and data representing the evolution of dual-phase microstructures fro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#27773;&#36710;&#36187;&#36710;&#39046;&#22495;&#20013;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00296</link><description>&lt;p&gt;
&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33258;&#20027;&#36187;&#36710;
&lt;/p&gt;
&lt;p&gt;
End-to-end Lidar-Driven Reinforcement Learning for Autonomous Racing. (arXiv:2309.00296v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28608;&#20809;&#38647;&#36798;&#30340;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#27773;&#36710;&#36187;&#36710;&#39046;&#22495;&#20013;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#23548;&#33322;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#21270;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#20026;&#20256;&#32479;&#26041;&#27861;&#38590;&#20197;&#35299;&#20915;&#30340;&#22797;&#26434;&#38382;&#39064;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#38382;&#39064;&#23450;&#20041;&#27169;&#31946;&#19988;&#38590;&#20197;&#37327;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#35299;&#20915;&#26041;&#26696;&#22914;&#24378;&#21270;&#23398;&#20064;&#23588;&#20854;&#26377;&#20215;&#20540;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#21644;&#35757;&#32451;&#19968;&#20010;&#21033;&#29992;&#21069;&#39304;&#21407;&#22987;&#28608;&#20809;&#38647;&#36798;&#21644;&#36895;&#24230;&#25968;&#25454;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;&#32463;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#21518;&#65292;&#35813;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#22312;&#30495;&#23454;&#36187;&#36710;&#22330;&#26223;&#20013;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;&#36825;&#39033;&#30740;&#31350;&#31361;&#20986;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22686;&#24378;&#33258;&#20027;&#36187;&#36710;&#24615;&#33021;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#21644;&#28508;&#22312;&#22909;&#22788;&#65292;&#29305;&#21035;&#26159;&#22312;&#20808;&#21069;&#22320;&#22270;&#20449;&#24687;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has emerged as a transformative approach in the domains of automation and robotics, offering powerful solutions to complex problems that conventional methods struggle to address. In scenarios where the problem definitions are elusive and challenging to quantify, learning-based solutions such as RL become particularly valuable. One instance of such complexity can be found in the realm of car racing, a dynamic and unpredictable environment that demands sophisticated decision-making algorithms. This study focuses on developing and training an RL agent to navigate a racing environment solely using feedforward raw lidar and velocity data in a simulated context. The agent's performance, trained in the simulation environment, is then experimentally evaluated in a real-world racing scenario. This exploration underlines the feasibility and potential benefits of RL algorithm enhancing autonomous racing performance, especially in the environments where prior map inform
&lt;/p&gt;</description></item><item><title>RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.00267</link><description>&lt;p&gt;
RLAIF: &#20351;&#29992;AI&#21453;&#39304;&#26469;&#25193;&#23637;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. (arXiv:2309.00267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00267
&lt;/p&gt;
&lt;p&gt;
RLAIF&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;AI&#21453;&#39304;&#20195;&#26367;&#20154;&#31867;&#26631;&#27880;&#20559;&#22909;&#65292;&#30456;&#27604;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#35748;&#21487;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#28508;&#21147;&#35299;&#20915;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#26631;&#31614;&#26159;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;RLHF&#21644;&#21033;&#29992;&#29616;&#25104;&#30340;LLM&#36827;&#34892;&#26631;&#35760;&#30340;RL from AI Feedback (RLAIF)&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#37117;&#33021;&#33719;&#24471;&#31867;&#20284;&#30340;&#25913;&#21892;&#25928;&#26524;&#12290;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;&#20154;&#31867;&#35780;&#20272;&#32773;&#22312;&#32422;70%&#30340;&#26696;&#20363;&#20013;&#37117;&#26356;&#21916;&#27426;RLAIF&#21644;RLHF&#20135;&#29983;&#30340;&#25991;&#26412;&#65292;&#32780;&#19981;&#26159;&#22522;&#20934;&#30340;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#34987;&#35201;&#27714;&#35780;&#20272;RLAIF&#21644;RLHF&#30340;&#25688;&#35201;&#26102;&#65292;&#20154;&#31867;&#20197;&#30456;&#21516;&#30340;&#27604;&#29575;&#26356;&#21916;&#27426;&#20004;&#32773;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;RLAIF&#21487;&#20197;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20026;&#20811;&#26381;RLHF&#30340;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#23398;&#20064;&#24230;&#37327;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#32852;&#37030;&#23398;&#20064;&#24230;&#37327;&#32858;&#21512;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#26377;&#25928;&#31209;&#36866;&#29992;&#20110;&#32852;&#37030;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#26032;&#22411;&#26435;&#37325;&#32858;&#21512;&#26041;&#26696;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#32852;&#37030;&#24179;&#22343;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00257</link><description>&lt;p&gt;
&#21033;&#29992;&#23398;&#20064;&#24230;&#37327;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Learning Metrics for Improved Federated Learning. (arXiv:2309.00257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#32852;&#37030;&#23398;&#20064;&#19982;&#23398;&#20064;&#24230;&#37327;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#32852;&#37030;&#23398;&#20064;&#24230;&#37327;&#32858;&#21512;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#26377;&#25928;&#31209;&#36866;&#29992;&#20110;&#32852;&#37030;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#26032;&#22411;&#26435;&#37325;&#32858;&#21512;&#26041;&#26696;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#30340;&#32852;&#37030;&#24179;&#22343;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#23398;&#20064;&#26041;&#26696;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#30340;&#26032;&#23398;&#20064;&#24230;&#37327;&#65292;&#23588;&#20854;&#26159;&#26377;&#21161;&#20110;&#30830;&#23450;&#27169;&#22411;&#23398;&#20064;&#24773;&#20917;&#30340;&#26032;&#23398;&#20064;&#24230;&#37327;&#12290;&#20854;&#20013;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#24230;&#37327;&#34987;&#31216;&#20026;&#8220;&#26377;&#25928;&#31209;&#8221;&#65288;ER&#65289;&#65292;&#23427;&#34913;&#37327;&#30697;&#38453;&#22855;&#24322;&#20540;&#30340;&#39321;&#20892;&#29109;&#65292;&#20174;&#32780;&#30830;&#23450;&#23618;&#30340;&#26144;&#23556;&#25928;&#26524;&#12290;&#36890;&#36807;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#21644;&#23398;&#20064;&#24230;&#37327;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23558;(1)&#25552;&#20379;&#31532;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24230;&#37327;&#32858;&#21512;&#26041;&#27861;&#65292;(2)&#35777;&#26126;&#26377;&#25928;&#31209;&#36866;&#29992;&#20110;&#32852;&#37030;&#38382;&#39064;&#65292;&#20248;&#20110;&#22522;&#20934;&#30340;&#32852;&#37030;&#24179;&#22343;&#26041;&#27861;&#65292;(3)&#24320;&#21457;&#19968;&#31181;&#20381;&#36182;&#26377;&#25928;&#31209;&#30340;&#26032;&#22411;&#26435;&#37325;&#32858;&#21512;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently in the federated setting, no learning schemes leverage the emerging research of explainable artificial intelligence (XAI) in particular the novel learning metrics that help determine how well a model is learning. One of these novel learning metrics is termed `Effective Rank' (ER) which measures the Shannon Entropy of the singular values of a matrix, thus enabling a metric determining how well a layer is mapping. By joining federated learning and the learning metric, effective rank, this work will \textbf{(1)} give the first federated learning metric aggregation method \textbf{(2)} show that effective rank is well-suited to federated problems by out-performing baseline Federated Averaging \cite{konevcny2016federated} and \textbf{(3)} develop a novel weight-aggregation scheme relying on effective rank.
&lt;/p&gt;</description></item><item><title>SortedNet&#26159;&#19968;&#31181;&#24191;&#20041;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25490;&#24207;&#35757;&#32451;&#21644;&#27010;&#29575;&#26041;&#24335;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21508;&#20010;&#32500;&#24230;&#19978;&#23454;&#29616;&#39640;&#25928;&#21160;&#24577;&#25512;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#22312;&#27169;&#22411;&#25512;&#26029;&#36807;&#31243;&#20013;&#28789;&#27963;&#36866;&#24212;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#23376;&#32593;&#32476;&#30340;&#25968;&#37327;&#25193;&#23637;&#21040;&#25968;&#30334;&#20010;&#12290;</title><link>http://arxiv.org/abs/2309.00255</link><description>&lt;p&gt;
SortedNet&#65292;&#27599;&#20010;&#32593;&#32476;&#37117;&#26377;&#33258;&#24049;&#30340;&#20301;&#32622;&#65306;&#38754;&#21521;&#35757;&#32451;&#22810;&#23545;&#19968;&#31070;&#32463;&#32593;&#32476;&#30340;&#24191;&#20041;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks. (arXiv:2309.00255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00255
&lt;/p&gt;
&lt;p&gt;
SortedNet&#26159;&#19968;&#31181;&#24191;&#20041;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25490;&#24207;&#35757;&#32451;&#21644;&#27010;&#29575;&#26041;&#24335;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21508;&#20010;&#32500;&#24230;&#19978;&#23454;&#29616;&#39640;&#25928;&#21160;&#24577;&#25512;&#26029;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#22312;&#27169;&#22411;&#25512;&#26029;&#36807;&#31243;&#20013;&#28789;&#27963;&#36866;&#24212;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#23376;&#32593;&#32476;&#30340;&#25968;&#37327;&#25193;&#23637;&#21040;&#25968;&#30334;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#22823;&#65292;&#22914;&#20309;&#22312;&#20869;&#23384;&#21644;&#35745;&#31639;&#32422;&#26463;&#19979;&#25214;&#21040;&#26368;&#20248;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#32452;&#25104;&#37096;&#20998;&#36890;&#24120;&#20801;&#35768;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#20351;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#24182;&#19981;&#24847;&#35782;&#21040;&#36825;&#31181;&#27169;&#22359;&#21270;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#32570;&#20047;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#36866;&#24212;&#27169;&#22411;&#35745;&#31639;&#36127;&#36733;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SortedNet&#65292;&#36825;&#26159;&#19968;&#31181;&#24191;&#20041;&#19988;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#32500;&#24230;&#19978;&#30340;&#20869;&#22312;&#27169;&#22359;&#21270;&#29305;&#24615;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#21160;&#24577;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#23884;&#22871;&#32467;&#26500;&#30340;&#23376;&#27169;&#22411;&#21644;&#20027;&#27169;&#22411;&#20849;&#20139;&#21442;&#25968;&#30340;&#26041;&#24335;&#65292;&#24182;&#20197;&#25490;&#24207;&#21644;&#27010;&#29575;&#30340;&#26041;&#24335;&#35757;&#32451;&#23427;&#20204;&#12290;&#36825;&#31181;&#23376;&#32593;&#32476;&#30340;&#25490;&#24207;&#35757;&#32451;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#19968;&#36718;&#35757;&#32451;&#20013;&#23558;&#23376;&#32593;&#32476;&#30340;&#25968;&#37327;&#25193;&#23637;&#21040;&#25968;&#30334;&#20010;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26356;&#26032;&#26041;&#26696;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#23376;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. Although usually the architecture and constituent building blocks of neural networks allow them to be used in a modular way, their training process is not aware of this modularity. Consequently, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. This paper proposes SortedNet, a generalized and scalable solution to harness the inherent modularity of deep neural networks across various dimensions for efficient dynamic inference. Our training considers a nested architecture for the sub-models with shared parameters and trains them together with the main model in a sorted and probabilistic manner. This sorted training of sub-networks enables us to scale the number of sub-networks to hundreds using a single round of training. We utilize a novel updating scheme during 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20960;&#20309;&#35270;&#35282;&#35299;&#37322;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21457;&#29616;&#36890;&#29992;&#23545;&#25239;&#24615;&#35302;&#21457;&#22120;&#21487;&#33021;&#26159;&#23884;&#20837;&#21521;&#37327;&#65292;&#20165;&#20165;&#36817;&#20284;&#20102;&#20854;&#23545;&#25239;&#35757;&#32451;&#21306;&#22495;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.00254</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#36890;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#65306;&#20960;&#20309;&#21487;&#33021;&#26159;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Why do universal adversarial attacks work on large language models?: Geometry might be the answer. (arXiv:2309.00254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20960;&#20309;&#35270;&#35282;&#35299;&#37322;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21457;&#29616;&#36890;&#29992;&#23545;&#25239;&#24615;&#35302;&#21457;&#22120;&#21487;&#33021;&#26159;&#23884;&#20837;&#21521;&#37327;&#65292;&#20165;&#20165;&#36817;&#20284;&#20102;&#20854;&#23545;&#25239;&#35757;&#32451;&#21306;&#22495;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26032;&#21457;&#33021;&#21147;&#65292;&#22312;&#31038;&#20250;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#25239;&#25915;&#20987;&#30340;&#32972;&#26223;&#19979;&#65292;&#29702;&#35299;&#21644;&#35299;&#37322;&#23427;&#20204;&#30340;&#20869;&#37096;&#24037;&#20316;&#20173;&#28982;&#22522;&#26412;&#26410;&#35299;&#20915;&#12290;&#24050;&#32463;&#35777;&#26126;&#22522;&#20110;&#26799;&#24230;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38750;&#24120;&#26377;&#25928;&#65292;&#30001;&#20110;&#23427;&#20204;&#23545;&#36755;&#20837;&#19981;&#25935;&#24863;&#30340;&#29305;&#24615;&#65292;&#21487;&#33021;&#20855;&#26377;&#28508;&#22312;&#30340;&#21361;&#38505;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20960;&#20309;&#35270;&#35282;&#65292;&#35299;&#37322;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#36890;&#36807;&#23545;&#25915;&#20987;117M&#21442;&#25968;&#30340;GPT-2&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#35777;&#25454;&#34920;&#26126;&#36890;&#29992;&#23545;&#25239;&#24615;&#35302;&#21457;&#22120;&#21487;&#33021;&#26159;&#23884;&#20837;&#21521;&#37327;&#65292;&#20165;&#20165;&#36817;&#20284;&#20102;&#20854;&#23545;&#25239;&#35757;&#32451;&#21306;&#22495;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20010;&#20551;&#35774;&#24471;&#21040;&#20102;&#36890;&#36807;&#30333;&#30418;&#27169;&#22411;&#20998;&#26512;&#30340;&#25903;&#25345;&#65292;&#21253;&#25324;&#23545;&#38544;&#34255;&#34920;&#31034;&#30340;&#38477;&#32500;&#21644;&#30456;&#20284;&#24230;&#27979;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#20851;&#20110;&#39537;&#21160;&#36890;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#28508;&#22312;&#26426;&#21046;&#30340;&#26032;&#20960;&#20309;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. However, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. This work presents a novel geometric perspective explaining universal adversarial attacks on large language models. By attacking the 117M parameter GPT-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. This hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. We believe this new geometric perspective on the underlying mechanism driving univer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;ViT&#30340;&#36827;&#23637;&#20197;&#21450;&#23545;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20026;&#21307;&#23398;&#35786;&#26029;&#24212;&#29992;&#25552;&#20379;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.00252</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#30340;&#21487;&#35299;&#37322;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#65306;&#19968;&#20010;&#20851;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Interpretable Medical Imagery Diagnosis with Self-Attentive Transformers: A Review of Explainable AI for Health Care. (arXiv:2309.00252v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;ViT&#30340;&#36827;&#23637;&#20197;&#21450;&#23545;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20026;&#21307;&#23398;&#35786;&#26029;&#24212;&#29992;&#25552;&#20379;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#20419;&#20351;&#20854;&#22312;&#21021;&#32423;&#21307;&#30103;&#26381;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#35299;&#20915;&#20102;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#20379;&#38656;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#20316;&#20026;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20986;&#29616;&#65292;&#21463;&#30410;&#20110;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22797;&#26434;&#19988;&#24120;&#24120;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#8220;&#40657;&#30418;&#23376;&#8221;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#20854;&#36816;&#20316;&#26041;&#24335;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#25351;&#35299;&#37322;&#21644;&#35299;&#35835;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20869;&#37096;&#36816;&#20316;&#26041;&#24335;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36825;&#22312;&#21307;&#23398;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#65292;&#20197;&#25351;&#23548;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#26368;&#36817;ViT&#30340;&#36827;&#23637;&#21644;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20197;&#29702;&#35299;ViT&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#23454;&#29616;&#21307;&#23398;&#35786;&#26029;&#24212;&#29992;&#30340;&#36879;&#26126;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in artificial intelligence (AI) have facilitated its widespread adoption in primary medical services, addressing the demand-supply imbalance in healthcare. Vision Transformers (ViT) have emerged as state-of-the-art computer vision models, benefiting from self-attention modules. However, compared to traditional machine-learning approaches, deep-learning models are complex and are often treated as a "black box" that can cause uncertainty regarding how they operate. Explainable Artificial Intelligence (XAI) refers to methods that explain and interpret machine learning models' inner workings and how they come to decisions, which is especially important in the medical domain to guide the healthcare decision-making process. This review summarises recent ViT advancements and interpretative approaches to understanding the decision-making process of ViT, enabling transparency in medical diagnosis applications.
&lt;/p&gt;</description></item><item><title>NeuroSurgeon&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;Huggingface Transformers&#24211;&#20013;&#21457;&#29616;&#21644;&#25805;&#20316;&#27169;&#22411;&#23376;&#32593;&#32476;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#25512;&#36827;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.00244</link><description>&lt;p&gt;
NeuroSurgeon: &#19968;&#31181;&#29992;&#20110;&#23376;&#32593;&#32476;&#20998;&#26512;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
NeuroSurgeon: A Toolkit for Subnetwork Analysis. (arXiv:2309.00244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00244
&lt;/p&gt;
&lt;p&gt;
NeuroSurgeon&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;Huggingface Transformers&#24211;&#20013;&#21457;&#29616;&#21644;&#25805;&#20316;&#27169;&#22411;&#23376;&#32593;&#32476;&#30340;Python&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#25512;&#36827;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21487;&#35299;&#37322;&#24615;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#34920;&#31034;&#30340;&#31639;&#27861;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23558;&#24050;&#35757;&#32451;&#27169;&#22411;&#20998;&#35299;&#20026;&#21151;&#33021;&#30005;&#36335;&#26469;&#29702;&#35299;&#23427;&#20204;(&#21442;&#32771;Csord\'as&#31561;&#20154;&#30340;&#30740;&#31350;&#65292;2020&#65307;Lepori&#31561;&#20154;&#65292;2023)&#12290;&#20026;&#20102;&#25512;&#36827;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;NeuroSurgeon&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24211;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#21644;&#25805;&#20316;Huggingface Transformers&#24211;&#20013;&#30340;&#27169;&#22411;&#20013;&#30340;&#23376;&#32593;&#32476;(Wolf&#31561;&#20154;&#65292;2019)&#12290;NeuroSurgeon&#21487;&#20197;&#22312;https://github.com/mlepori1/NeuroSurgeon &#20813;&#36153;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances in the field of explainability, much remains unknown about the algorithms that neural networks learn to represent. Recent work has attempted to understand trained models by decomposing them into functional circuits (Csord\'as et al., 2020; Lepori et al., 2023). To advance this research, we developed NeuroSurgeon, a python library that can be used to discover and manipulate subnetworks within models in the Huggingface Transformers library (Wolf et al., 2019). NeuroSurgeon is freely available at https://github.com/mlepori1/NeuroSurgeon.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2309.00236</link><description>&lt;p&gt;
&#22270;&#20687;&#21163;&#25345;&#65306;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Image Hijacking: Adversarial Images can Control Generative Models at Runtime. (arXiv:2309.00236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20813;&#21463;&#24694;&#24847;&#34892;&#20026;&#32773;&#30340;&#25915;&#20987;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#22270;&#20687;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#22270;&#20687;&#21163;&#25345;&#65292;&#21363;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34892;&#20026;&#21305;&#37197;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#65292;&#24182;&#29992;&#23427;&#26469;&#25506;&#32034;&#19977;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#20855;&#20307;&#23383;&#31526;&#20018;&#25915;&#20987;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#34987;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#36755;&#20986;&#65307;&#27844;&#38706;&#19978;&#19979;&#25991;&#25915;&#20987;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#20449;&#24687;&#27844;&#38706;&#21040;&#36755;&#20986;&#20013;&#65307;&#36234;&#29425;&#25915;&#20987;&#21487;&#20197;&#32469;&#36807;&#27169;&#22411;&#30340;&#23433;&#20840;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;CLIP&#21644;LLaMA-2&#30340;&#26368;&#26032;VLM&#27169;&#22411;LLaVA-2&#36827;&#34892;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25152;&#26377;&#30340;&#25915;&#20987;&#31867;&#22411;&#25104;&#21151;&#29575;&#22343;&#22312;90&#65285;&#20197;&#19978;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#21482;&#38656;&#35201;&#23545;&#22270;&#20687;&#36827;&#34892;&#23567;&#30340;&#25200;&#21160;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;&#22914;&#26524;&#22270;&#20687;&#21163;&#25345;&#19982;CIFAR-10&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19968;&#26679;&#38590;&#20197;&#38450;&#24481;&#65292;&#37027;&#20040;&#21487;&#33021;&#38656;&#35201;&#24456;&#22810;&#24180;&#25165;&#33021;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control generative models at runtime. We introduce Behavior Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. Specific string attacks generate arbitrary output of the adversary's choosing. Leak context attacks leak information from the context window into the output. Jailbreak attacks circumvent a model's safety training. We study these attacks against LLaVA-2, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90\% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#30697;&#38453;&#26469;&#38477;&#20302;&#39640;&#32500;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#32500;&#25968;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;&#22522;&#20110;&#8220;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#8221;&#65292;&#25552;&#20986;&#20102;&#27867;&#21270;&#20445;&#35777;&#30340;&#25968;&#25454;&#37327;&#19982;&#24615;&#33021;&#25351;&#26631;&#30340;&#20266;&#32500;&#24230;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2309.00203</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#32447;&#24615;&#35268;&#21010;&#38477;&#32500;&#26041;&#27861;&#65306;&#27867;&#21270;&#30028;&#38480;&#21644;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods. (arXiv:2309.00203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#30697;&#38453;&#26469;&#38477;&#20302;&#39640;&#32500;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#32500;&#25968;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;&#22522;&#20110;&#8220;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#8221;&#65292;&#25552;&#20986;&#20102;&#27867;&#21270;&#20445;&#35777;&#30340;&#25968;&#25454;&#37327;&#19982;&#24615;&#33021;&#25351;&#26631;&#30340;&#20266;&#32500;&#24230;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#22788;&#29702;&#39640;&#32500;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65288;LP&#65289;&#12290;&#32473;&#23450;&#36807;&#21435;&#30340;$n$&#32500;LP&#25968;&#25454;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;$n\times k$&#30340;&#8220;&#25237;&#24433;&#30697;&#38453;&#8221;&#65288;$n &gt; k$&#65289;&#65292;&#23558;&#32500;&#25968;&#20174;$n$&#38477;&#20302;&#21040;$k$&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;$k$&#32500;LP&#38382;&#39064;&#24182;&#36890;&#36807;&#20056;&#20197;&#25237;&#24433;&#30697;&#38453;&#26469;&#24674;&#22797;$n$&#32500;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#26410;&#26469;&#30340;LP&#23454;&#20363;&#12290;&#36825;&#20010;&#24605;&#24819;&#19982;&#20219;&#20309;&#29992;&#25143;&#39318;&#36873;&#30340;LP&#27714;&#35299;&#22120;&#20860;&#23481;&#65292;&#22240;&#27492;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21152;&#36895;LP&#27714;&#35299;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#25165;&#33021;&#30830;&#20445;&#24674;&#22797;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#65311;&#25105;&#20204;&#22522;&#20110;&#8220;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#8221;&#30340;&#24605;&#24819;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#23558;&#36275;&#22815;&#36827;&#34892;&#27867;&#21270;&#20445;&#35777;&#30340;&#25968;&#25454;&#37327;&#19982;&#24615;&#33021;&#25351;&#26631;&#30340;&#8220;&#20266;&#32500;&#24230;&#8221;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20266;&#32500;&#24230;&#30340;$\tilde{\mathrm{O}}(nk^2)$&#19978;&#30028;&#65288;$\tilde{\mathrm{O}}$&#21387;&#32553;&#20102;&#23545;&#25968;&#22240;&#23376;&#65289;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;$\Omega(nk)$&#19979;&#30028;&#26469;&#34917;&#20805;&#23427;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper studies a simple data-driven approach to high-dimensional linear programs (LPs). Given data of past $n$-dimensional LPs, we learn an $n\times k$ \textit{projection matrix} ($n &gt; k$), which reduces the dimensionality from $n$ to $k$. Then, we address future LP instances by solving $k$-dimensional LPs and recovering $n$-dimensional solutions by multiplying the projection matrix. This idea is compatible with any user-preferred LP solvers, hence a versatile approach to faster LP solving. One natural question is: how much data is sufficient to ensure the recovered solutions' quality? We address this question based on the idea of \textit{data-driven algorithm design}, which relates the amount of data sufficient for generalization guarantees to the \textit{pseudo-dimension} of performance metrics. We present an $\tilde{\mathrm{O}}(nk^2)$ upper bound on the pseudo-dimension ($\tilde{\mathrm{O}}$ compresses logarithmic factors) and complement it by an $\Omega(nk)$ lower bound, hence 
&lt;/p&gt;</description></item><item><title>&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#20855;&#26377;&#20027;&#35266;&#24615;&#65292;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#21463;&#27169;&#22411;&#26500;&#24314;&#32773;&#20559;&#22909;&#30340;&#24433;&#21709;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#36873;&#25321;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#38656;&#35201;&#23545;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#36827;&#34892;&#26356;&#21152;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#26631;&#20934;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.00201</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#20027;&#35266;&#24615;
&lt;/p&gt;
&lt;p&gt;
Subjectivity in Unsupervised Machine Learning Model Selection. (arXiv:2309.00201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00201
&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36873;&#25321;&#20855;&#26377;&#20027;&#35266;&#24615;&#65292;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#21463;&#27169;&#22411;&#26500;&#24314;&#32773;&#20559;&#22909;&#30340;&#24433;&#21709;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#36873;&#25321;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#38656;&#35201;&#23545;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#36827;&#34892;&#26356;&#21152;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#26631;&#20934;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36873;&#25321;&#26159;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20013;&#24517;&#35201;&#30340;&#27493;&#39588;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#26631;&#20934;&#21644;&#25351;&#26631;&#65292;&#20294;&#27169;&#22411;&#36873;&#25321;&#20173;&#28982;&#23384;&#22312;&#20027;&#35266;&#24615;&#12290;&#39640;&#24230;&#20027;&#35266;&#24615;&#21487;&#33021;&#20250;&#23545;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#37325;&#22797;&#24615;&#21644;&#21487;&#20877;&#29616;&#24615;&#20135;&#29983;&#30097;&#38382;&#65292;&#24182;&#23545;&#23454;&#38469;&#37096;&#32626;&#30340;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#20135;&#29983;&#24576;&#30097;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#20013;&#27169;&#22411;&#26500;&#24314;&#32773;&#30340;&#20559;&#22909;&#24433;&#21709;&#30340;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#20197;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#20026;&#20363;&#65292;&#35843;&#26597;&#20102;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#20027;&#35266;&#24615;&#12290;&#25105;&#20204;&#36992;&#35831;&#20102;33&#20301;&#21442;&#19982;&#32773;&#21644;&#19977;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19977;&#20010;&#22330;&#26223;&#20013;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26080;&#35770;&#26159;&#21442;&#19982;&#32773;&#36824;&#26159;LLMs&#30340;&#36873;&#25321;&#37117;&#23384;&#22312;&#21464;&#24322;&#24615;&#21644;&#19981;&#19968;&#33268;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#19981;&#21516;&#30340;&#26631;&#20934;&#21644;&#25351;&#26631;&#23384;&#22312;&#20998;&#27495;&#26102;&#12290;&#20027;&#35266;&#24615;&#26469;&#28304;&#21253;&#25324;&#23545;&#19981;&#21516;&#26631;&#20934;&#21644;&#25351;&#26631;&#37325;&#35201;&#24615;&#30340;&#19981;&#21516;&#24847;&#35265;&#65292;&#23545;&#27169;&#22411;&#24212;&#35813;&#26377;&#22810;&#31616;&#27905;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#20197;&#21450;&#23545;&#25968;&#25454;&#35268;&#27169;&#30340;&#22823;&#23567;&#30340;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection is a necessary step in unsupervised machine learning. Despite numerous criteria and metrics, model selection remains subjective. A high degree of subjectivity may lead to questions about repeatability and reproducibility of various machine learning studies and doubts about the robustness of models deployed in the real world. Yet, the impact of modelers' preferences on model selection outcomes remains largely unexplored. This study uses the Hidden Markov Model as an example to investigate the subjectivity involved in model selection. We asked 33 participants and three Large Language Models (LLMs) to make model selections in three scenarios. Results revealed variability and inconsistencies in both the participants' and the LLMs' choices, especially when different criteria and metrics disagree. Sources of subjectivity include varying opinions on the importance of different criteria and metrics, differing views on how parsimonious a model should be, and how the size of a da
&lt;/p&gt;</description></item><item><title /><link>http://arxiv.org/abs/2309.00199</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#39135;&#29289;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model with Clustering-based Conditioning for Food Image Generation. (arXiv:2309.00199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00199
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#33203;&#39135;&#35780;&#20272;&#26159;&#20351;&#29992;&#36827;&#39135;&#22330;&#21512;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;&#35760;&#24405;&#21644;&#20998;&#26512;&#33829;&#20859;&#25668;&#20837;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#36827;&#34892;&#39135;&#29289;&#20998;&#31867;&#12289;&#20998;&#21106;&#21644;&#20221;&#37327;&#20272;&#35745;&#31561;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#20381;&#36182;&#20110;&#22823;&#37327;&#24102;&#26377;&#27880;&#37322;&#30340;&#39135;&#29289;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25968;&#25454;&#20381;&#36182;&#24615;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#26469;&#35828;&#23384;&#22312;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#20026;&#33719;&#21462;&#22823;&#37327;&#20016;&#23500;&#22810;&#26679;&#19988;&#24179;&#34913;&#30340;&#39135;&#29289;&#22270;&#20687;&#38598;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#31181;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#21512;&#25104;&#30340;&#39135;&#29289;&#22270;&#20687;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#32467;&#26500;&#36827;&#34892;&#29983;&#25104;&#65292;&#20294;&#21512;&#25104;&#39135;&#29289;&#22270;&#20687;&#30340;&#36136;&#37327;&#20173;&#28982;&#19981;&#29702;&#24819;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#25193;&#25955;&#24335;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#19968;&#33324;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#29983;&#25104;&#39135;&#29289;&#22270;&#20687;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35201;&#29983;&#25104;&#30495;&#23454;&#30340;&#39135;&#29289;&#22806;&#35266;&#21644;&#32454;&#33410;&#26159;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based dietary assessment serves as an efficient and accurate solution for recording and analyzing nutrition intake using eating occasion images as input. Deep learning-based techniques are commonly used to perform image analysis such as food classification, segmentation, and portion size estimation, which rely on large amounts of food images with annotations for training. However, such data dependency poses significant barriers to real-world applications, because acquiring a substantial, diverse, and balanced set of food images can be challenging. One potential solution is to use synthetic food images for data augmentation. Although existing work has explored the use of generative adversarial networks (GAN) based structures for generation, the quality of synthetic food images still remains subpar. In addition, while diffusion-based generative models have shown promising results for general image generation tasks, the generation of food images can be challenging due to the substan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26089;&#26399;&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20026;&#25972;&#25968;&#21464;&#37327;&#25552;&#20379;&#20540;&#24182;&#26089;&#26399;&#20462;&#22797;&#36825;&#20123;&#21464;&#37327;&#65292;&#20174;&#32780;&#23558;&#21407;&#38382;&#39064;&#31616;&#21270;&#25104;&#20102;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;&#30417;&#30563;&#26041;&#27861;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#22343;&#33021;&#26377;&#25928;&#22320;&#35299;&#20915;&#27668;&#20030;&#27833;&#29983;&#20135;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.00197</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27668;&#20030;&#27833;&#29983;&#20135;&#20248;&#21270;&#30340;&#26089;&#26399;&#20462;&#22797;&#26041;&#27861;&#65306;&#26377;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep-learning-based Early Fixing for Gas-lifted Oil Production Optimization: Supervised and Weakly-supervised Approaches. (arXiv:2309.00197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26089;&#26399;&#20462;&#22797;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20026;&#25972;&#25968;&#21464;&#37327;&#25552;&#20379;&#20540;&#24182;&#26089;&#26399;&#20462;&#22797;&#36825;&#20123;&#21464;&#37327;&#65292;&#20174;&#32780;&#23558;&#21407;&#38382;&#39064;&#31616;&#21270;&#25104;&#20102;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;&#30417;&#30563;&#26041;&#27861;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#22343;&#33021;&#26377;&#25928;&#22320;&#35299;&#20915;&#27668;&#20030;&#27833;&#29983;&#20135;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#29992;&#20110;&#20026;&#25152;&#26377;&#25972;&#25968;&#21464;&#37327;&#25552;&#20379;&#20540;&#65292;&#26089;&#26399;&#20462;&#22797;&#25972;&#25968;&#21464;&#37327;&#24182;&#23558;&#21407;&#38382;&#39064;&#31616;&#21270;&#20026;&#32447;&#24615;&#35268;&#21010;(LP)&#65292;&#20174;&#32780;&#20943;&#23569;&#20381;&#36182;&#26114;&#36149;&#30340;&#31934;&#30830;&#26041;&#27861;&#25110;&#19968;&#33324;&#36817;&#20284;&#26041;&#27861;&#30340;&#25104;&#26412;&#12290;&#25552;&#20986;&#20102;&#20004;&#31181;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#65306;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#26377;&#30417;&#30563;&#26041;&#27861;&#38656;&#35201;&#35757;&#32451;&#38598;&#20013;&#21407;&#38382;&#39064;&#30340;&#22810;&#20010;&#23454;&#20363;&#30340;&#26368;&#20248;&#25972;&#25968;&#20540;&#65292;&#32780;&#24369;&#30417;&#30563;&#26041;&#27861;&#21482;&#38656;&#35201;&#26089;&#26399;&#20462;&#22797;&#30340;&#32447;&#24615;&#38382;&#39064;&#30340;&#35299;&#20197;&#21450;&#25972;&#25968;&#21464;&#37327;&#30340;&#38543;&#26426;&#36171;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximizing oil production from gas-lifted oil wells entails solving Mixed-Integer Linear Programs (MILPs). As the parameters of the wells, such as the basic-sediment-to-water ratio and the gas-oil ratio, are updated, the problems must be repeatedly solved. Instead of relying on costly exact methods or the accuracy of general approximate methods, in this paper, we propose a tailor-made heuristic solution based on deep learning models trained to provide values to all integer variables given varying well parameters, early-fixing the integer variables and, thus, reducing the original problem to a linear program (LP). We propose two approaches for developing the learning-based heuristic: a supervised learning approach, which requires the optimal integer values for several instances of the original problem in the training set, and a weakly-supervised learning approach, which requires only solutions for the early-fixed linear problems with random assignments for the integer variables. Our res
&lt;/p&gt;</description></item><item><title>RepCodec&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#37325;&#26500;&#35821;&#38899;&#34920;&#31034;&#24182;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#35821;&#20041;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;RepCodec&#22312;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00169</link><description>&lt;p&gt;
RepCodec:&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#26631;&#35760;&#30340;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
RepCodec: A Speech Representation Codec for Speech Tokenization. (arXiv:2309.00169v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00169
&lt;/p&gt;
&lt;p&gt;
RepCodec&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#37325;&#26500;&#35821;&#38899;&#34920;&#31034;&#24182;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#35821;&#20041;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;RepCodec&#22312;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#31163;&#25955;&#35821;&#38899;&#26631;&#35760;&#22312;&#23558;&#35821;&#38899;&#27880;&#20837;LLMs&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31163;&#25955;&#21270;&#23548;&#33268;&#20102;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#36825;&#20123;&#31163;&#25955;&#35821;&#38899;&#26631;&#35760;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepCodec&#65292;&#19968;&#31181;&#29992;&#20110;&#35821;&#20041;&#35821;&#38899;&#26631;&#35760;&#30340;&#26032;&#22411;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#22120;&#12290;&#19982;&#37325;&#26032;&#26500;&#24314;&#21407;&#22987;&#38899;&#39057;&#30340;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#19981;&#21516;&#65292;RepCodec&#36890;&#36807;&#20174;&#35821;&#38899;&#32534;&#30721;&#22120;&#65288;&#22914;HuBERT&#25110;data2vec&#65289;&#37325;&#26500;&#35821;&#38899;&#34920;&#31034;&#26469;&#23398;&#20064;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#12290;&#35821;&#38899;&#32534;&#30721;&#22120;&#12289;&#32534;&#35299;&#30721;&#22120;&#21644;&#30690;&#37327;&#37327;&#21270;&#30721;&#20070;&#20849;&#21516;&#26500;&#25104;&#19968;&#20010;&#23558;&#35821;&#38899;&#27874;&#24418;&#36716;&#25442;&#20026;&#35821;&#20041;&#26631;&#35760;&#30340;&#27969;&#27700;&#32447;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30001;&#20110;&#20854;&#22686;&#24378;&#30340;&#20449;&#24687;&#20445;&#30041;&#33021;&#21147;&#65292;RepCodec&#22312;&#35821;&#38899;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;k-means&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#21516;&#20449;&#24687;&#28304;&#36827;&#34892;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#35777;&#25454;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#22312;&#29983;&#20135;&#35780;&#20272;&#36741;&#21161;&#31995;&#32479;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.00157</link><description>&lt;p&gt;
&#20449;&#24687;&#34701;&#21512;&#22312;&#29983;&#20135;&#35780;&#20272;&#36741;&#21161;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Information Fusion for Assistance Systems in Production Assessment. (arXiv:2309.00157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#21516;&#20449;&#24687;&#28304;&#36827;&#34892;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#35777;&#25454;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#22312;&#29983;&#20135;&#35780;&#20272;&#36741;&#21161;&#31995;&#32479;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#25968;&#25454;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23450;&#20041;&#20381;&#36182;&#20449;&#24687;&#34701;&#21512;&#30340;&#36741;&#21161;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#32467;&#21512;&#19981;&#21516;&#30340;&#20449;&#24687;&#28304;&#24182;&#25552;&#20379;&#35780;&#20272;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#35777;&#25454;&#29702;&#35770;&#26469;&#34701;&#21512;n&#20010;&#20449;&#24687;&#28304;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#34701;&#21512;&#25552;&#20379;&#20102;&#26356;&#21487;&#38752;&#30340;&#39044;&#27979;&#21644;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#20449;&#24687;&#34701;&#21512;&#30340;&#26426;&#22120;&#25968;&#25454;&#21644;&#19987;&#23478;&#20013;&#24515;&#27169;&#22411;&#20004;&#20010;&#20027;&#35201;&#26469;&#28304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#19994;&#35774;&#22791;&#30340;&#25968;&#25454;&#28436;&#31034;&#20102;&#20449;&#24687;&#34701;&#21512;&#26041;&#27861;&#65292;&#20174;&#32780;&#23436;&#21892;&#20102;&#26412;&#30740;&#31350;&#30340;&#24212;&#29992;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#29702;&#35770;&#30340;&#26041;&#27861;&#26469;&#26356;&#26032;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#28418;&#31227;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;Benchmark Tennessee Eastman&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#23545;&#27169;&#22411;&#26356;&#26032;&#36827;&#34892;&#20102;&#21106;&#38500;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel methodology to define assistance systems that rely on information fusion to combine different sources of information while providing an assessment. The main contribution of this paper is providing a general framework for the fusion of n number of information sources using the evidence theory. The fusion provides a more robust prediction and an associated uncertainty that can be used to assess the prediction likeliness. Moreover, we provide a methodology for the information fusion of two primary sources: an ensemble classifier based on machine data and an expert-centered model. We demonstrate the information fusion approach using data from an industrial setup, which rounds up the application part of this research. Furthermore, we address the problem of data drift by proposing a methodology to update the data-based models using an evidence theory approach. We validate the approach using the Benchmark Tennessee Eastman while doing an ablation study of the model update p
&lt;/p&gt;</description></item><item><title>TurboGP&#26159;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#28789;&#27963;&#21644;&#20808;&#36827;&#30340;GP&#24211;&#65292;&#20855;&#26377;&#23707;&#21644;&#32454;&#32990;&#20154;&#32676;&#26041;&#26696;&#12289;&#19981;&#21516;&#31867;&#22411;&#30340;&#36951;&#20256;&#25805;&#20316;&#21644;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;GP&#33410;&#28857;&#30340;&#26412;&#22320;&#25903;&#25345;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.00149</link><description>&lt;p&gt;
TurboGP&#65306;&#19968;&#20010;&#28789;&#27963;&#32780;&#20808;&#36827;&#30340;&#22522;&#20110;Python&#30340;GP&#24211;
&lt;/p&gt;
&lt;p&gt;
TurboGP: A flexible and advanced python based GP library. (arXiv:2309.00149v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00149
&lt;/p&gt;
&lt;p&gt;
TurboGP&#26159;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#28789;&#27963;&#21644;&#20808;&#36827;&#30340;GP&#24211;&#65292;&#20855;&#26377;&#23707;&#21644;&#32454;&#32990;&#20154;&#32676;&#26041;&#26696;&#12289;&#19981;&#21516;&#31867;&#22411;&#30340;&#36951;&#20256;&#25805;&#20316;&#21644;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;GP&#33410;&#28857;&#30340;&#26412;&#22320;&#25903;&#25345;&#65292;&#36866;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;TurboGP&#65292;&#19968;&#20010;&#23436;&#20840;&#29992;Python&#32534;&#20889;&#30340;&#36951;&#20256;&#32534;&#31243;&#65288;GP&#65289;&#24211;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;TurboGP&#23454;&#29616;&#20102;&#20854;&#20182;GP&#23454;&#29616;&#20013;&#19981;&#23384;&#22312;&#30340;&#29616;&#20195;&#29305;&#24615;&#65292;&#22914;&#23707;&#21644;&#32454;&#32990;&#20154;&#32676;&#26041;&#26696;&#12289;&#19981;&#21516;&#31867;&#22411;&#30340;&#36951;&#20256;&#25805;&#20316;&#65288;&#36801;&#31227;&#12289;&#21463;&#20445;&#25252;&#30340;&#20132;&#21449;&#31561;&#65289;&#12289;&#22312;&#32447;&#23398;&#20064;&#31561;&#29305;&#24615;&#12290;TurboGP&#26368;&#29420;&#29305;&#30340;&#29305;&#28857;&#26159;&#20854;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;GP&#33410;&#28857;&#30340;&#26412;&#22320;&#25903;&#25345;&#65292;&#20197;&#20801;&#35768;&#19981;&#21516;&#30340;&#25277;&#35937;&#32423;&#21035;&#65292;&#36825;&#20351;&#24471;TurboGP&#22312;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#28304;&#26102;&#29305;&#21035;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce TurboGP, a Genetic Programming (GP) library fully written in Python and specifically designed for machine learning tasks. TurboGP implements modern features not available in other GP implementations, such as island and cellular population schemes, different types of genetic operations (migration, protected crossovers), online learning, among other features. TurboGP's most distinctive characteristic is its native support for different types of GP nodes to allow different abstraction levels, this makes TurboGP particularly useful for processing a wide variety of data sources.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;Agent DeepRL&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;IAB&#32593;&#32476;&#20013;&#32852;&#21512;&#20248;&#21270;&#21151;&#29575;&#21644;&#23376;&#36890;&#36947;&#20998;&#37197;&#65292;&#20197;&#26368;&#22823;&#21270;&#19979;&#34892;&#25968;&#25454;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00144</link><description>&lt;p&gt;
&#22810;Agent DeepRL&#31639;&#27861;&#22312;IAB&#32593;&#32476;&#20013;&#30340;&#32852;&#21512;&#21151;&#29575;&#21644;&#23376;&#36890;&#36947;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Multi Agent DeepRL based Joint Power and Subchannel Allocation in IAB networks. (arXiv:2309.00144v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00144
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;Agent DeepRL&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;IAB&#32593;&#32476;&#20013;&#32852;&#21512;&#20248;&#21270;&#21151;&#29575;&#21644;&#23376;&#36890;&#36947;&#20998;&#37197;&#65292;&#20197;&#26368;&#22823;&#21270;&#19979;&#34892;&#25968;&#25454;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#25509;&#20837;&#21644;&#22238;&#20256;&#65288;IAB&#65289;&#26159;&#28385;&#36275;&#26410;&#26469;&#20960;&#20195;&#23545;&#26356;&#39640;&#25968;&#25454;&#36895;&#29575;&#38656;&#27714;&#30340;&#19968;&#31181;&#21487;&#34892;&#26041;&#27861;&#65292;&#20316;&#20026;&#23494;&#38598;&#20809;&#32420;&#36830;&#25509;&#30340;&#32463;&#27982;&#26377;&#25928;&#26367;&#20195;&#26041;&#26696;&#12290;&#20855;&#26377;&#32422;&#26463;&#26465;&#20214;&#30340;&#36825;&#31181;&#32593;&#32476;&#35774;&#35745;&#36890;&#24120;&#20250;&#23548;&#33268;&#38750;&#20984;&#24615;&#21644;&#32452;&#21512;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#20026;&#32852;&#21512;&#23376;&#36890;&#36947;&#20998;&#37197;&#21644;&#21151;&#29575;&#20998;&#37197;&#65288;SAPA&#65289;&#38382;&#39064;&#33719;&#24471;&#26368;&#20248;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;Agent Deep Reinforcement Learning&#65288;DeepRL&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;IAB&#32593;&#32476;&#20013;&#32852;&#21512;&#20248;&#21270;&#21151;&#29575;&#21644;&#23376;&#36890;&#36947;&#20998;&#37197;&#65292;&#20197;&#26368;&#22823;&#21270;&#19979;&#34892;&#25968;&#25454;&#36895;&#29575;&#12290;&#20351;&#29992;DDQN&#65288;Double Deep Q-Learning Network&#65289;&#30340;SAPA&#21487;&#20197;&#22788;&#29702;&#19982;&#22810;&#20010;&#29992;&#25143;&#21644;&#33410;&#28857;&#30456;&#20851;&#32852;&#30340;&#20855;&#26377;&#24040;&#22823;&#34892;&#21160;&#31354;&#38388;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#21338;&#24328;&#35770;&#65292;&#20998;&#25968;&#35268;&#21010;&#21644;&#20984;&#20248;&#21270;&#65289;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#38656;&#35201;&#36234;&#26469;&#36234;&#31934;&#30830;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrated Access and Backhauling (IAB) is a viable approach for meeting the unprecedented need for higher data rates of future generations, acting as a cost-effective alternative to dense fiber-wired links. The design of such networks with constraints usually results in an optimization problem of non-convex and combinatorial nature. Under those situations, it is challenging to obtain an optimal strategy for the joint Subchannel Allocation and Power Allocation (SAPA) problem. In this paper, we develop a multi-agent Deep Reinforcement Learning (DeepRL) based framework for joint optimization of power and subchannel allocation in an IAB network to maximize the downlink data rate. SAPA using DDQN (Double Deep Q-Learning Network) can handle computationally expensive problems with huge action spaces associated with multiple users and nodes. Unlike the conventional methods such as game theory, fractional programming, and convex optimization, which in practice demand more and more accurate net
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#27169;&#22359;&#36339;&#36807;&#30340;&#27969;conformer&#32534;&#30721;&#22120;&#30340;&#35270;&#35273;&#21551;&#21457;&#20851;&#38190;&#35789;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#38899;&#39057;&#19978;&#21160;&#24577;&#36339;&#36807;&#32593;&#32476;&#27169;&#22359;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#21644;&#23450;&#20301;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#35813;&#26041;&#27861;&#22312;&#21547;&#32972;&#26223;&#22122;&#22768;&#30340;Google&#35821;&#38899;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#21152;&#31361;&#20986;&#65292;&#23545;&#20110;&#22987;&#32456;&#24320;&#21551;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#22120;&#29305;&#21035;&#26377;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.00140</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#27169;&#22359;&#36339;&#36807;&#22312;&#27969;conformer&#32534;&#30721;&#22120;&#20013;&#25913;&#36827;&#20102;&#22522;&#20110;&#35270;&#35273;&#21551;&#21457;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving vision-inspired keyword spotting using dynamic module skipping in streaming conformer encoder. (arXiv:2309.00140v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#27169;&#22359;&#36339;&#36807;&#30340;&#27969;conformer&#32534;&#30721;&#22120;&#30340;&#35270;&#35273;&#21551;&#21457;&#20851;&#38190;&#35789;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#38899;&#39057;&#19978;&#21160;&#24577;&#36339;&#36807;&#32593;&#32476;&#27169;&#22359;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#21644;&#23450;&#20301;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#35813;&#26041;&#27861;&#22312;&#21547;&#32972;&#26223;&#22122;&#22768;&#30340;Google&#35821;&#38899;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#21152;&#31361;&#20986;&#65292;&#23545;&#20110;&#22987;&#32456;&#24320;&#21551;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#22120;&#29305;&#21035;&#26377;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#21551;&#21457;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#36755;&#20837;&#20381;&#36182;&#30340;&#21160;&#24577;&#28145;&#24230;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#22788;&#29702;&#27969;&#24335;&#38899;&#39057;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#20108;&#36827;&#21046;&#38376;&#25193;&#23637;&#20102;&#19968;&#20010;conformer&#32534;&#30721;&#22120;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#26681;&#25454;&#36755;&#20837;&#38899;&#39057;&#21160;&#24577;&#36339;&#36807;&#32593;&#32476;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;Librispeech&#21069;1000&#20010;&#26368;&#24120;&#35265;&#21333;&#35789;&#30340;&#36830;&#32493;&#35821;&#38899;&#19978;&#25552;&#39640;&#20102;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#36739;&#23567;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;&#38376;&#30340;&#21152;&#20837;&#36824;&#20943;&#23569;&#20102;&#24179;&#22343;&#22788;&#29702;&#37327;&#65292;&#32780;&#19981;&#24433;&#21709;&#25972;&#20307;&#24615;&#33021;&#12290;&#23558;&#35895;&#27468;&#35821;&#38899;&#21629;&#20196;&#25968;&#25454;&#38598;&#25918;&#32622;&#22312;&#32972;&#26223;&#22122;&#22768;&#19978;&#65292;&#36825;&#20123;&#20248;&#21183;&#26356;&#21152;&#26174;&#33879;&#65292;&#38750;&#35821;&#38899;&#36755;&#20837;&#19978;&#30340;&#22788;&#29702;&#37327;&#21487;&#20943;&#23569;&#39640;&#36798;97%&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#22987;&#32456;&#24320;&#21551;&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#22120;&#23588;&#20026;&#26377;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using a vision-inspired keyword spotting framework, we propose an architecture with input-dependent dynamic depth capable of processing streaming audio. Specifically, we extend a conformer encoder with trainable binary gates that allow us to dynamically skip network modules according to the input audio. Our approach improves detection and localization accuracy on continuous speech using Librispeech top-1000 most frequent words while maintaining a small memory footprint. The inclusion of gates also reduces the average amount of processing without affecting the overall performance. These benefits are shown to be even more pronounced using the Google speech commands dataset placed over background noise where up to 97% of the processing is skipped on non-speech inputs, therefore making our method particularly interesting for an always-on keyword spotter.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25512;&#25991;&#20013;&#34920;&#36798;&#30340;&#24773;&#32490;&#65292;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#27979;&#20102;&#29305;&#26031;&#25289;&#12289;&#33529;&#26524;&#31561;&#20027;&#35201;&#20844;&#21496;&#32929;&#31080;&#20215;&#26684;&#30340;&#27874;&#21160;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31215;&#26497;&#24615;&#12289;&#28040;&#26497;&#24615;&#21644;&#20027;&#35266;&#24615;&#26159;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#30340;&#20027;&#35201;&#20915;&#23450;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.00136</link><description>&lt;p&gt;
&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#27979;&#37329;&#34701;&#24066;&#22330;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Predicting Financial Market Trends using Time Series Analysis and Natural Language Processing. (arXiv:2309.00136v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#25512;&#25991;&#20013;&#34920;&#36798;&#30340;&#24773;&#32490;&#65292;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#27979;&#20102;&#29305;&#26031;&#25289;&#12289;&#33529;&#26524;&#31561;&#20027;&#35201;&#20844;&#21496;&#32929;&#31080;&#20215;&#26684;&#30340;&#27874;&#21160;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31215;&#26497;&#24615;&#12289;&#28040;&#26497;&#24615;&#21644;&#20027;&#35266;&#24615;&#26159;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#30340;&#20027;&#35201;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39044;&#27979;&#37329;&#34701;&#24066;&#22330;&#36235;&#21183;&#26159;&#19968;&#39033;&#22797;&#26434;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#35768;&#22810;&#21464;&#37327;&#21487;&#20197;&#24433;&#21709;&#32929;&#31080;&#20215;&#26684;&#12290;&#36825;&#20123;&#21464;&#37327;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#32463;&#27982;&#21644;&#25919;&#27835;&#20107;&#20214;&#65292;&#20197;&#21450;&#24403;&#21069;&#30340;&#20844;&#20247;&#24577;&#24230;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#20844;&#20247;&#24773;&#32490;&#30340;&#34920;&#36798;&#65288;&#22914;Twitter&#65289;&#21487;&#33021;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#30830;&#23450;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;Twitter&#24773;&#32490;&#20316;&#20026;&#39044;&#27979;&#29305;&#26031;&#25289;&#12289;&#33529;&#26524;&#31561;&#20027;&#35201;&#20844;&#21496;&#32929;&#31080;&#20215;&#26684;&#30340;&#24037;&#20855;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25512;&#25991;&#20013;&#20256;&#36798;&#30340;&#24773;&#32490;&#19982;&#32929;&#31080;&#20215;&#26684;&#30340;&#27874;&#21160;&#20043;&#38388;&#23384;&#22312;&#24378;&#26377;&#21147;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31215;&#26497;&#24615;&#12289;&#28040;&#26497;&#24615;&#21644;&#20027;&#35266;&#24615;&#26159;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#30340;&#20027;&#35201;&#20915;&#23450;&#22240;&#32032;&#12290;&#25968;&#25454;&#20351;&#29992;&#20102;&#38271;&#30701;&#26399;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#65288;LSTM&#65289;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting financial market trends through time series analysis and natural language processing poses a complex and demanding undertaking, owing to the numerous variables that can influence stock prices. These variables encompass a spectrum of economic and political occurrences, as well as prevailing public attitudes. Recent research has indicated that the expression of public sentiments on social media platforms such as Twitter may have a noteworthy impact on the determination of stock prices. The objective of this study was to assess the viability of Twitter sentiments as a tool for predicting stock prices of major corporations such as Tesla, Apple. Our study has revealed a robust association between the emotions conveyed in tweets and fluctuations in stock prices. Our findings indicate that positivity, negativity, and subjectivity are the primary determinants of fluctuations in stock prices. The data was analyzed utilizing the Long-Short Term Memory neural network (LSTM) model, whi
&lt;/p&gt;</description></item><item><title>FTA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28789;&#27963;&#35302;&#21457;&#22120;&#12289;&#38544;&#34109;&#19988;&#24378;&#20581;&#30340;&#32852;&#37030;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23398;&#20064;&#28789;&#27963;&#35302;&#21457;&#22120;&#27169;&#24335;&#26469;&#25805;&#20316;&#33391;&#24615;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#26631;&#31614;&#30340;&#26368;&#37325;&#35201;&#38544;&#34255;&#29305;&#24449;&#12290;&#36890;&#36807;&#22635;&#20805;&#21487;&#21306;&#20998;&#30340;&#24046;&#24322;&#65292;&#20351;&#25915;&#20987;&#20855;&#26377;&#38544;&#34109;&#24615;&#12290;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00127</link><description>&lt;p&gt;
FTA: &#20855;&#26377;&#28789;&#27963;&#35302;&#21457;&#22120;&#30340;&#38544;&#34109;&#19988;&#24378;&#20581;&#30340;&#32852;&#37030;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
FTA: Stealthy and Robust Backdoor Attack with Flexible Trigger on Federated Learning. (arXiv:2309.00127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00127
&lt;/p&gt;
&lt;p&gt;
FTA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28789;&#27963;&#35302;&#21457;&#22120;&#12289;&#38544;&#34109;&#19988;&#24378;&#20581;&#30340;&#32852;&#37030;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23398;&#20064;&#28789;&#27963;&#35302;&#21457;&#22120;&#27169;&#24335;&#26469;&#25805;&#20316;&#33391;&#24615;&#26679;&#26412;&#65292;&#21516;&#26102;&#20445;&#30041;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#26631;&#31614;&#30340;&#26368;&#37325;&#35201;&#38544;&#34255;&#29305;&#24449;&#12290;&#36890;&#36807;&#22635;&#20805;&#21487;&#21306;&#20998;&#30340;&#24046;&#24322;&#65292;&#20351;&#25915;&#20987;&#20855;&#26377;&#38544;&#34109;&#24615;&#12290;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21518;&#38376;&#25915;&#20987;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#36890;&#29992;&#35302;&#21457;&#22120;&#25110;&#35821;&#20041;&#27169;&#24335;&#65292;&#36825;&#21487;&#20197;&#34987;&#26576;&#20123;&#38450;&#24481;&#26426;&#21046;&#65288;&#22914;&#33539;&#25968;&#21098;&#35009;&#65292;&#27604;&#36739;&#23616;&#37096;&#26356;&#26032;&#20043;&#38388;&#30340;&#21442;&#25968;&#24046;&#24322;&#65289;&#36731;&#26131;&#26816;&#27979;&#21644;&#36807;&#28388;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#34109;&#19988;&#24378;&#20581;&#30340;&#20855;&#26377;&#28789;&#27963;&#35302;&#21457;&#22120;&#30340;&#32852;&#37030;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29983;&#25104;&#35302;&#21457;&#22120;&#20989;&#25968;&#65292;&#21487;&#20197;&#23398;&#20064;&#20351;&#29992;&#19968;&#20010;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#28789;&#27963;&#35302;&#21457;&#22120;&#27169;&#24335;&#26469;&#25805;&#20316;&#33391;&#24615;&#26679;&#26412;&#65292;&#24182;&#21516;&#26102;&#35753;&#35302;&#21457;&#22120;&#27169;&#24335;&#21253;&#21547;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#26631;&#31614;&#30340;&#26368;&#37325;&#35201;&#30340;&#38544;&#34255;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35302;&#21457;&#22120;&#29983;&#25104;&#22120;&#21487;&#20197;&#22312;&#19981;&#21516;&#36718;&#27425;&#20013;&#19981;&#26029;&#23398;&#20064;&#21644;&#36866;&#24212;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#35843;&#25972;&#21040;&#20840;&#23616;&#27169;&#22411;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#22635;&#20805;&#21487;&#21306;&#20998;&#30340;&#24046;&#24322;&#65288;&#35302;&#21457;&#22120;&#27169;&#24335;&#21644;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#30340;&#26144;&#23556;&#65289;&#65292;&#25105;&#20204;&#20351;&#25105;&#20204;&#30340;&#25915;&#20987;&#20855;&#26377;&#33258;&#28982;&#30340;&#38544;&#34109;&#24615;&#12290;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current backdoor attacks against federated learning (FL) strongly rely on universal triggers or semantic patterns, which can be easily detected and filtered by certain defense mechanisms such as norm clipping, comparing parameter divergences among local updates. In this work, we propose a new stealthy and robust backdoor attack with flexible triggers against FL defenses. To achieve this, we build a generative trigger function that can learn to manipulate the benign samples with an imperceptible flexible trigger pattern and simultaneously make the trigger pattern include the most significant hidden features of the attacker-chosen label. Moreover, our trigger generator can keep learning and adapt across different rounds, allowing it to adjust to changes in the global model. By filling the distinguishable difference (the mapping between the trigger pattern and target label), we make our attack naturally stealthy. Extensive experiments on real-world datasets verify the effectiveness and st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#20989;&#25968;&#24615;&#25688;&#35201;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#20998;&#37327;&#25289;&#26222;&#25289;&#26031;&#36807;&#31243;&#23545;&#26080;&#38480;&#32500;&#30340;&#20989;&#25968;&#24615;&#25688;&#35201;&#36827;&#34892;&#25200;&#21160;&#65292;&#25918;&#23485;&#20102;&#23545;&#25968;&#25454;&#36712;&#36857;&#30340;&#20551;&#35774;&#65292;&#24182;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#26377;&#38480;&#32500;&#23376;&#31354;&#38388;&#23884;&#20837;&#26041;&#27861;&#20445;&#30041;&#20102;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.00125</link><description>&lt;p&gt;
&#36890;&#36807;&#29420;&#31435;&#20998;&#37327;&#25289;&#26222;&#25289;&#26031;&#36807;&#31243;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#20989;&#25968;&#24615;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Functional Summaries via the Independent Component Laplace Process. (arXiv:2309.00125v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#20989;&#25968;&#24615;&#25688;&#35201;&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#20998;&#37327;&#25289;&#26222;&#25289;&#26031;&#36807;&#31243;&#23545;&#26080;&#38480;&#32500;&#30340;&#20989;&#25968;&#24615;&#25688;&#35201;&#36827;&#34892;&#25200;&#21160;&#65292;&#25918;&#23485;&#20102;&#23545;&#25968;&#25454;&#36712;&#36857;&#30340;&#20551;&#35774;&#65292;&#24182;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#26377;&#38480;&#32500;&#23376;&#31354;&#38388;&#23884;&#20837;&#26041;&#27861;&#20445;&#30041;&#20102;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29420;&#31435;&#20998;&#37327;&#25289;&#26222;&#25289;&#26031;&#36807;&#31243;&#65288;ICLP&#65289;&#26426;&#21046;&#30340;&#24046;&#20998;&#38544;&#31169;&#20989;&#25968;&#24615;&#25688;&#35201;&#30340;&#26032;&#26426;&#21046;&#12290;&#36890;&#36807;&#23558;&#24863;&#20852;&#36259;&#30340;&#20989;&#25968;&#24615;&#25688;&#35201;&#35270;&#20026;&#30495;&#27491;&#26080;&#38480;&#32500;&#23545;&#35937;&#65292;&#24182;&#20351;&#29992;ICLP&#22122;&#22768;&#26469;&#25200;&#21160;&#23427;&#20204;&#65292;&#35813;&#26032;&#26426;&#21046;&#25918;&#23485;&#20102;&#20851;&#20110;&#25968;&#25454;&#36712;&#36857;&#30340;&#20551;&#35774;&#65292;&#24182;&#30456;&#23545;&#20110;&#25991;&#29486;&#20013;&#30340;&#32463;&#20856;&#26377;&#38480;&#32500;&#23376;&#31354;&#38388;&#23884;&#20837;&#26041;&#27861;&#20445;&#30041;&#20102;&#26356;&#39640;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20989;&#25968;&#31354;&#38388;&#20013;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#32479;&#35745;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36731;&#24494;&#36807;&#24179;&#28369;&#25688;&#35201;&#26469;&#35777;&#26126;&#38544;&#31169;&#25104;&#26412;&#19981;&#20250;&#20027;&#23548;&#32479;&#35745;&#35823;&#24046;&#65292;&#24182;&#19988;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#21487;&#20197;&#24573;&#30053;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a new mechanism for releasing differentially private functional summaries called the Independent Component Laplace Process, or ICLP, mechanism. By treating the functional summaries of interest as truly infinite-dimensional objects and perturbing them with the ICLP noise, this new mechanism relaxes assumptions on data trajectories and preserves higher utility compared to classical finite-dimensional subspace embedding approaches in the literature. We establish the feasibility of the proposed mechanism in multiple function spaces. Several statistical estimation problems are considered, and we demonstrate by slightly over-smoothing the summary, the privacy cost will not dominate the statistical error and is asymptotically negligible. Numerical experiments on synthetic and real datasets demonstrate the efficacy of the proposed mechanism.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#21517;&#20026;Deep SAD&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#22312;&#39640;&#39057;&#37329;&#34701;&#25968;&#25454;&#20013;&#26816;&#27979;&#35784;&#39575;&#30340;&#25928;&#26524;&#12290;&#20351;&#29992;&#20102;&#26469;&#33258;TMX&#20132;&#26131;&#25152;&#30340;&#29420;&#23478;&#38480;&#20215;&#22996;&#25176;&#34180;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#19968;&#23567;&#37096;&#20998;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.00088</link><description>&lt;p&gt;
&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#22312;&#26399;&#36135;&#24066;&#22330;&#35784;&#39575;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Semi-Supervised Anomaly Detection for Finding Fraud in the Futures Market. (arXiv:2309.00088v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#21517;&#20026;Deep SAD&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#22312;&#39640;&#39057;&#37329;&#34701;&#25968;&#25454;&#20013;&#26816;&#27979;&#35784;&#39575;&#30340;&#25928;&#26524;&#12290;&#20351;&#29992;&#20102;&#26469;&#33258;TMX&#20132;&#26131;&#25152;&#30340;&#29420;&#23478;&#38480;&#20215;&#22996;&#25176;&#34180;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;&#19968;&#23567;&#37096;&#20998;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#37329;&#34701;&#30005;&#23376;&#20132;&#26131;&#25152;&#26159;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#19988;&#24555;&#33410;&#22863;&#30340;&#24066;&#22330;&#65292;&#27599;&#22825;&#20132;&#26131;&#39069;&#36798;&#25968;&#21313;&#20159;&#32654;&#20803;&#12290;&#28982;&#32780;&#65292;&#24066;&#22330;&#19978;&#23384;&#22312;&#30528;&#25805;&#32437;&#21644;&#35784;&#39575;&#27963;&#21160;&#12290;&#26816;&#27979;&#27492;&#31867;&#27963;&#21160;&#26159;&#19968;&#20010;&#37325;&#22823;&#20219;&#21153;&#65292;&#36807;&#21435;&#19968;&#30452;&#26159;&#30001;&#20154;&#31867;&#26469;&#23436;&#25104;&#12290;&#26368;&#36817;&#65292;&#26356;&#22810;&#30340;&#30740;&#31350;&#21644;&#36164;&#28304;&#38598;&#20013;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#26469;&#33258;&#21160;&#21270;&#36825;&#20123;&#36807;&#31243;&#19978;&#12290;&#27450;&#35784;&#26816;&#27979;&#20027;&#35201;&#19982;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#26377;&#20851;&#65292;&#36890;&#24120;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#26469;&#36827;&#34892;&#65292;&#22240;&#20026;&#32570;&#23569;&#26377;&#30417;&#30563;&#23398;&#20064;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#36824;&#26377;&#23569;&#37327;&#21487;&#29992;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#19968;&#31181;&#21517;&#20026;Deep SAD&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#22312;&#39640;&#39057;&#37329;&#34701;&#25968;&#25454;&#20013;&#26816;&#27979;&#35784;&#39575;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;&#33945;&#29305;&#21033;&#23572;TMX&#20132;&#26131;&#25152;&#30340;&#29420;&#23478;&#38480;&#20215;&#22996;&#25176;&#34180;&#25968;&#25454;&#65292;&#37197;&#20197;&#23569;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern financial electronic exchanges are an exciting and fast-paced marketplace where billions of dollars change hands every day. They are also rife with manipulation and fraud. Detecting such activity is a major undertaking, which has historically been a job reserved exclusively for humans. Recently, more research and resources have been focused on automating these processes via machine learning and artificial intelligence. Fraud detection is overwhelmingly associated with the greater field of anomaly detection, which is usually performed via unsupervised learning techniques because of the lack of labeled data needed for supervised learning. However, a small quantity of labeled data does often exist. This research article aims to evaluate the efficacy of a deep semi-supervised anomaly detection technique, called Deep SAD, for detecting fraud in high-frequency financial data. We use exclusive proprietary limit order book data from the TMX exchange in Montr\'eal, with a small set of tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RePo&#31639;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21518;&#39564;&#21487;&#39044;&#27979;&#24615;&#30340;&#26041;&#24335;&#65292;&#22686;&#24378;&#20102;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24377;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23545;&#20887;&#20313;&#21644;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#26041;&#27861;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.00082</link><description>&lt;p&gt;
RePo: &#36890;&#36807;&#27491;&#21017;&#21270;&#21518;&#39564;&#21487;&#39044;&#27979;&#24615;&#22686;&#24378;&#24377;&#24615;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability. (arXiv:2309.00082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RePo&#31639;&#27861;&#65292;&#36890;&#36807;&#27491;&#21017;&#21270;&#21518;&#39564;&#21487;&#39044;&#27979;&#24615;&#30340;&#26041;&#24335;&#65292;&#22686;&#24378;&#20102;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24377;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#23545;&#20887;&#20313;&#21644;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#26041;&#27861;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#23558;&#22270;&#20687;&#35266;&#27979;&#32534;&#30721;&#20026;&#20302;&#32500;&#34920;&#31034;&#26041;&#24335;&#65292;&#36825;&#31181;&#26041;&#24335;&#26410;&#33021;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#20266;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#21363;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#32452;&#25104;&#37096;&#20998;&#30340;&#21464;&#21270;&#65292;&#22914;&#32972;&#26223;&#24178;&#25200;&#22240;&#32032;&#25110;&#20809;&#29031;&#26465;&#20214;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#21040;&#20102;&#19968;&#31181;&#23545;&#36825;&#31181;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#30446;&#26631;&#40723;&#21169;&#35813;&#34920;&#31034;&#22312;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26368;&#22823;&#30340;&#39044;&#27979;&#24615;&#65292;&#21516;&#26102;&#38480;&#21046;&#20102;&#35266;&#27979;&#21040;&#28508;&#22312;&#34920;&#31034;&#30340;&#20449;&#24687;&#27969;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19968;&#30446;&#26631;&#26497;&#22823;&#22686;&#24378;&#20102;&#35270;&#35273;&#27169;&#22411;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23545;&#35270;&#35273;&#24178;&#25200;&#30340;&#24377;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#34429;&#28982;&#23398;&#20064;&#21040;&#30340;&#32534;&#30721;&#22120;&#23545;&#20266;&#21464;&#21270;&#20855;&#26377;&#24377;&#24615;&#65292;&#20294;&#22312;&#26174;&#33879;&#20998;&#24067;&#21464;&#21270;&#19979;&#24182;&#27809;&#26377;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22870;&#21169;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual model-based RL methods typically encode image observations into low-dimensional representations in a manner that does not eliminate redundant information. This leaves them susceptible to spurious variations -- changes in task-irrelevant components such as background distractors or lighting conditions. In this paper, we propose a visual model-based RL method that learns a latent representation resilient to such spurious variations. Our training objective encourages the representation to be maximally predictive of dynamics and reward, while constraining the information flow from the observation to the latent representation. We demonstrate that this objective significantly bolsters the resilience of visual model-based RL methods to visual distractors, allowing them to operate in dynamic environments. We then show that while the learned encoder is resilient to spirious variations, it is not invariant under significant distribution shift. To address this, we propose a simple reward-f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00079</link><description>&lt;p&gt;
&#20851;&#20110;Adam&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#21518;&#21521;&#35823;&#24046;&#20998;&#26512;&#34987;&#29992;&#26469;&#25214;&#21040;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#21457;&#29616;&#26377;&#38480;&#27493;&#38271;&#20250;&#38544;&#24335;&#22320;&#35268;&#33539;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20986;&#29616;&#22312;ODE&#20013;&#30340;&#39033;&#20250;&#24809;&#32602;&#25439;&#22833;&#26799;&#24230;&#30340;&#20108;&#33539;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#20013;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#20294;&#28041;&#21450;&#30340;&#8220;&#33539;&#25968;&#8221;&#19981;&#21516;&#65306;&#23545;&#24212;&#30340;ODE&#39033;&#35201;&#20040;&#24809;&#32602;&#65288;&#25200;&#21160;&#30340;&#65289;&#25439;&#22833;&#26799;&#24230;&#30340;&#19968;&#33539;&#25968;&#65292;&#35201;&#20040;&#30456;&#21453;&#22320;&#38459;&#27490;&#20854;&#20943;&#23567;&#65288;&#21518;&#19968;&#31181;&#24773;&#20917;&#26159;&#20856;&#22411;&#30340;&#65289;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#20915;&#22810;&#27493;&#22238;&#24402;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#38543;&#26426;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.00073</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#35299;&#20915;&#22810;&#27493;&#22238;&#24402;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#38543;&#26426;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Diffusion Variational Autoencoder for Tackling Stochasticity in Multi-Step Regression Stock Price Prediction. (arXiv:2309.00073v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#20915;&#22810;&#27493;&#22238;&#24402;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#20013;&#30340;&#38543;&#26426;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20869;&#30340;&#22810;&#27493;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#23545;&#20110;&#39044;&#27979;&#27874;&#21160;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#24471;&#37329;&#34701;&#26426;&#26500;&#33021;&#22815;&#23450;&#20215;&#21644;&#23545;&#20914;&#34893;&#29983;&#21697;&#65292;&#24182;&#35753;&#38134;&#34892;&#37327;&#21270;&#20854;&#20132;&#26131;&#31807;&#20013;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#37329;&#34701;&#30417;&#31649;&#26426;&#26500;&#36824;&#35201;&#27714;&#26426;&#26500;&#25237;&#36164;&#32773;&#26377;&#20960;&#22825;&#30340;&#27969;&#21160;&#24615;&#26399;&#38480;&#20174;&#20854;&#39118;&#38505;&#36164;&#20135;&#20013;&#36864;&#20986;&#65292;&#20197;&#36991;&#20813;&#23545;&#24066;&#22330;&#20215;&#26684;&#20135;&#29983;&#23454;&#36136;&#24615;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32929;&#31080;&#25968;&#25454;&#20855;&#26377;&#39640;&#24230;&#38543;&#26426;&#24615;&#65292;&#22810;&#27493;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#20219;&#21153;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#21333;&#27493;&#22522;&#20110;&#20998;&#31867;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#22312;&#34920;&#24449;&#34920;&#36798;&#21147;&#26041;&#38754;&#26377;&#38480;&#12290;&#38543;&#30528;&#30446;&#26631;&#20215;&#26684;&#24207;&#21015;&#30340;&#24341;&#20837;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#27979;&#35797;&#26102;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#22240;&#20026;&#30446;&#26631;&#20215;&#26684;&#24207;&#21015;&#20013;&#20063;&#21253;&#21547;&#38543;&#26426;&#22122;&#22768;&#65292;&#38477;&#20302;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#28145;&#23618;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#21644;&#25193;&#25955;&#27010;&#29575;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-step stock price prediction over a long-term horizon is crucial for forecasting its volatility, allowing financial institutions to price and hedge derivatives, and banks to quantify the risk in their trading books. Additionally, most financial regulators also require a liquidity horizon of several days for institutional investors to exit their risky assets, in order to not materially affect market prices. However, the task of multi-step stock price prediction is challenging, given the highly stochastic nature of stock data. Current solutions to tackle this problem are mostly designed for single-step, classification-based predictions, and are limited to low representation expressiveness. The problem also gets progressively harder with the introduction of the target price sequence, which also contains stochastic noise and reduces generalizability at test-time. To tackle these issues, we combine a deep hierarchical variational-autoencoder (VAE) and diffusion probabilistic techniques
&lt;/p&gt;</description></item><item><title>YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.00071</link><description>&lt;p&gt;
YaRN: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00071
&lt;/p&gt;
&lt;p&gt;
YaRN&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#32534;&#30721;transformer-based&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36229;&#36807;&#23427;&#20204;&#35757;&#32451;&#30340;&#24207;&#21015;&#38271;&#24230;&#26102;&#26080;&#27861;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;YaRN&#65288;Yet another RoPE extensioN method&#65289;&#65292;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#38656;&#35201;&#30340;tokens&#25968;&#37327;&#21644;&#35757;&#32451;&#27493;&#39588;&#23569;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#30340;10&#20493;&#21644;2.5&#20493;&#12290;&#20351;&#29992;YaRN&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLaMA&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#21644;&#25512;&#26029;&#27604;&#21407;&#22987;&#39044;&#35757;&#32451;&#20801;&#35768;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#26041;&#38754;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;YaRN&#20855;&#26377;&#36229;&#36234;&#24494;&#35843;&#25968;&#25454;&#38598;&#26377;&#38480;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/jquesnelle/yarn&#19978;&#21457;&#24067;&#20102;&#20351;&#29992;64k&#21644;128k&#19978;&#19979;&#25991;&#31383;&#21475;&#36827;&#34892;Fine-tuning&#30340;Llama 2 7B/13B&#30340;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#26694;&#26550;&#23558;&#20248;&#21270;&#30340;&#20840;&#23616;&#32467;&#26500;&#21644;&#23616;&#37096;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#32771;&#34385;&#23616;&#37096;&#30456;&#20284;&#24230;&#21644;&#20840;&#23616;&#30456;&#20284;&#24230;&#65292;&#20197;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.00024</link><description>&lt;p&gt;
&#20445;&#25345;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#39640;&#25928;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-View Graph Clustering with Local and Global Structure Preservation. (arXiv:2309.00024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00024
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#26694;&#26550;&#23558;&#20248;&#21270;&#30340;&#20840;&#23616;&#32467;&#26500;&#21644;&#23616;&#37096;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#32771;&#34385;&#23616;&#37096;&#30456;&#20284;&#24230;&#21644;&#20840;&#23616;&#30456;&#20284;&#24230;&#65292;&#20197;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38170;&#28857;&#30340;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;&#65288;AMVGC&#65289;&#30001;&#20110;&#20854;&#39640;&#25928;&#24615;&#21644;&#25429;&#25417;&#22810;&#20010;&#35270;&#22270;&#20043;&#38388;&#20114;&#34917;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#39640;&#36136;&#37327;&#30340;&#38170;&#22270;&#22312;AMVGC&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;AMVGC&#26041;&#27861;&#21482;&#32771;&#34385;&#21333;&#19968;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#21363;&#23616;&#37096;&#25110;&#20840;&#23616;&#32467;&#26500;&#65292;&#36825;&#20026;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19981;&#36275;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36807;&#20110;&#20998;&#25955;&#30340;&#20840;&#23616;&#32467;&#26500;&#23548;&#33268;&#23398;&#20064;&#21040;&#30340;&#38170;&#28857;&#19981;&#33021;&#24456;&#22909;&#22320;&#25551;&#36848;&#32858;&#31867;&#21010;&#20998;&#12290;&#30456;&#21453;&#65292;&#20855;&#26377;&#19981;&#24688;&#24403;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#23616;&#37096;&#32467;&#26500;&#23548;&#33268;&#28508;&#22312;&#30340;&#19981;&#20934;&#30830;&#30340;&#38170;&#28857;&#20998;&#37197;&#65292;&#26368;&#32456;&#23548;&#33268;&#27425;&#20248;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;&#26694;&#26550;&#65292;&#31216;&#20026;&#20445;&#25345;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#39640;&#25928;&#22810;&#35270;&#22270;&#22270;&#32858;&#31867;&#65288;EMVGC-LG&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20248;&#21270;&#30340;&#20840;&#23616;&#32467;&#26500;&#21644;&#23616;&#37096;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#26694;&#26550;&#36827;&#34892;&#20248;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#23616;&#37096;&#30456;&#20284;&#24230;&#21644;&#20840;&#23616;&#30456;&#20284;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anchor-based multi-view graph clustering (AMVGC) has received abundant attention owing to its high efficiency and the capability to capture complementary structural information across multiple views. Intuitively, a high-quality anchor graph plays an essential role in the success of AMVGC. However, the existing AMVGC methods only consider single-structure information, i.e., local or global structure, which provides insufficient information for the learning task. To be specific, the over-scattered global structure leads to learned anchors failing to depict the cluster partition well. In contrast, the local structure with an improper similarity measure results in potentially inaccurate anchor assignment, ultimately leading to sub-optimal clustering performance. To tackle the issue, we propose a novel anchor-based multi-view graph clustering framework termed Efficient Multi-View Graph Clustering with Local and Global Structure Preservation (EMVGC-LG). Specifically, a unified framework with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;API&#27969;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#26377;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#21644;&#26080;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#26597;&#35810;API&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;&#23558;API&#27969;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#27861;&#33719;&#21462;&#23436;&#25972;&#30340;&#21407;&#22987;&#25968;&#25454;&#12289;&#26410;&#30693;&#30340;&#27169;&#22411;&#21442;&#25968;&#12289;&#24322;&#26500;&#27169;&#22411;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.00023</link><description>&lt;p&gt;
&#20174;API&#27969;&#20013;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning From a Stream of APIs. (arXiv:2309.00023v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;API&#27969;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#26377;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#21644;&#26080;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#26597;&#35810;API&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;&#23558;API&#27969;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#27861;&#33719;&#21462;&#23436;&#25972;&#30340;&#21407;&#22987;&#25968;&#25454;&#12289;&#26410;&#30693;&#30340;&#27169;&#22411;&#21442;&#25968;&#12289;&#24322;&#26500;&#27169;&#22411;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#24536;&#35760;&#20197;&#21069;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#21407;&#22987;&#25968;&#25454;&#65292;&#30001;&#20110;&#29256;&#26435;&#32771;&#34385;&#21644;&#38544;&#31169;&#39118;&#38505;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#19981;&#21487;&#29992;&#12290;&#30456;&#21453;&#65292;&#21033;&#30410;&#30456;&#20851;&#32773;&#36890;&#24120;&#36890;&#36807;API&#37322;&#25918;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;API&#35775;&#38382;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20004;&#31181;&#23454;&#29992;&#20294;&#26032;&#39062;&#30340;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#65306;&#25968;&#25454;&#26377;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;DECL-APIs&#65289;&#21644;&#26080;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#65288;DFCL-APIs&#65289;&#65292;&#36890;&#36807;&#37096;&#20998;&#25110;&#26080;&#21407;&#22987;&#25968;&#25454;&#20174;API&#27969;&#20013;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#12290;&#22312;&#36825;&#20004;&#31181;&#26032;&#35774;&#32622;&#19979;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#38754;&#20020;&#20960;&#20010;&#25361;&#25112;&#65306;&#26080;&#27861;&#33719;&#21462;&#23436;&#25972;&#30340;&#21407;&#22987;&#25968;&#25454;&#65292;&#26410;&#30693;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20219;&#24847;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;&#24322;&#26500;&#27169;&#22411;&#20197;&#21450;&#23545;&#20197;&#21069;API&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#25968;&#25454;&#21512;&#20316;&#25345;&#32493;&#33976;&#39311;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26597;&#35810;API&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;&#23558;API&#27969;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#25345;&#32493;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) aims to learn new tasks without forgetting previous tasks. However, existing CL methods require a large amount of raw data, which is often unavailable due to copyright considerations and privacy risks. Instead, stakeholders usually release pre-trained machine learning models as a service (MLaaS), which users can access via APIs. This paper considers two practical-yet-novel CL settings: data-efficient CL (DECL-APIs) and data-free CL (DFCL-APIs), which achieve CL from a stream of APIs with partial or no raw data. Performing CL under these two new settings faces several challenges: unavailable full raw data, unknown model parameters, heterogeneous models of arbitrary architecture and scale, and catastrophic forgetting of previous APIs. To overcome these issues, we propose a novel data-free cooperative continual distillation learning framework that distills knowledge from a stream of APIs into a CL model by generating pseudo data, just by querying APIs. Specifically
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#28304;&#24863;&#30693;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#21644;&#37096;&#32626;&#33021;&#22815;&#22312;&#24212;&#29992;&#30446;&#26631;&#19982;&#33021;&#28304;&#28040;&#32791;&#20043;&#38388;&#24179;&#34913;&#30340;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.00022</link><description>&lt;p&gt;
&#35774;&#35745;&#36793;&#32536;&#19978;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#33410;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Energy-Aware Approach to Design Self-Adaptive AI-based Applications on the Edge. (arXiv:2309.00022v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#28304;&#24863;&#30693;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#21644;&#37096;&#32626;&#33021;&#22815;&#22312;&#24212;&#29992;&#30446;&#26631;&#19982;&#33021;&#28304;&#28040;&#32791;&#20043;&#38388;&#24179;&#34913;&#30340;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#36793;&#32536;&#35774;&#22791;&#30340;&#20986;&#29616;&#20351;&#24471;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#21644;&#20998;&#31867;&#29289;&#32852;&#32593;&#20013;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#33719;&#21462;&#21040;&#30340;&#25968;&#25454;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#33021;&#22815;&#25191;&#34892;&#12290;&#36825;&#20123;&#24212;&#29992;&#30340;&#26222;&#21450;&#65288;&#20363;&#22914;&#26234;&#33021;&#22478;&#24066;&#20013;&#30340;&#20851;&#38190;&#30417;&#25511;&#65289;&#35201;&#27714;&#25105;&#20204;&#20174;&#33021;&#28304;&#35282;&#24230;&#20063;&#35201;&#32771;&#34385;&#36825;&#20123;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#28304;&#24863;&#30693;&#26041;&#27861;&#65292;&#29992;&#20110;&#35774;&#35745;&#21644;&#37096;&#32626;&#33021;&#22815;&#22312;&#24212;&#29992;&#30446;&#26631;&#65288;&#20363;&#22914;&#29289;&#20307;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#24103;&#22788;&#29702;&#36895;&#29575;&#65289;&#19982;&#33021;&#28304;&#28040;&#32791;&#20043;&#38388;&#24179;&#34913;&#30340;&#33258;&#36866;&#24212;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#20803;&#21551;&#21457;&#24335;&#25628;&#32034;&#36807;&#31243;&#26469;&#35299;&#20915;&#30830;&#23450;&#21487;&#29992;&#20110;&#31995;&#32479;&#33258;&#36866;&#24212;&#30340;&#37197;&#32622;&#38598;&#21512;&#30340;&#38382;&#39064;&#65292;&#35813;&#36807;&#31243;&#20165;&#38656;&#35201;&#23569;&#37327;&#32463;&#39564;&#26679;&#26412;&#12290;&#26368;&#32456;&#30340;&#37197;&#32622;&#38598;&#21512;&#26159;&#36890;&#36807;&#21152;&#26435;&#28784;&#33394;&#20851;&#32852;&#20998;&#26512;&#36873;&#25321;&#30340;&#65292;&#24182;&#26144;&#23556;&#21040;&#33258;&#36866;&#24212;&#31995;&#32479;&#30340;&#25805;&#20316;&#27169;&#24335;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of edge devices dedicated to machine learning tasks enabled the execution of AI-based applications that efficiently process and classify the data acquired by the resource-constrained devices populating the Internet of Things. The proliferation of such applications (e.g., critical monitoring in smart cities) demands new strategies to make these systems also sustainable from an energetic point of view.  In this paper, we present an energy-aware approach for the design and deployment of self-adaptive AI-based applications that can balance application objectives (e.g., accuracy in object detection and frames processing rate) with energy consumption. We address the problem of determining the set of configurations that can be used to self-adapt the system with a meta-heuristic search procedure that only needs a small number of empirical samples. The final set of configurations are selected using weighted gray relational analysis, and mapped to the operation modes of the self-adapt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65288;MAGE&#21644;Ms-IV&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;MAGE&#21487;&#20197;&#21457;&#29616;&#24418;&#25104;&#35821;&#20041;&#21547;&#20041;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#23558;&#20854;&#31216;&#20026;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#20998;&#32452;&#20026;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;Ms-IV&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#21463;&#21040;&#38459;&#26029;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65288;CaOC&#65289;&#20840;&#23616;&#35780;&#20272;&#27169;&#22411;&#26368;&#37325;&#35201;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.00018</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21457;&#29616;&#21487;&#35299;&#37322;&#30340;&#35270;&#35273;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Unsupervised discovery of Interpretable Visual Concepts. (arXiv:2309.00018v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65288;MAGE&#21644;Ms-IV&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;MAGE&#21487;&#20197;&#21457;&#29616;&#24418;&#25104;&#35821;&#20041;&#21547;&#20041;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#23558;&#20854;&#31216;&#20026;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#20998;&#32452;&#20026;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;Ms-IV&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#21463;&#21040;&#38459;&#26029;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65288;CaOC&#65289;&#20840;&#23616;&#35780;&#20272;&#27169;&#22411;&#26368;&#37325;&#35201;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#38750;&#19987;&#23478;&#29992;&#25143;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#26159;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25552;&#20379;&#32473;&#29992;&#25143;&#30340;&#27169;&#22411;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#35832;&#22914;&#38598;&#25104;&#26799;&#24230;&#31561;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#20135;&#29983;&#20102;&#21253;&#21547;&#22823;&#37327;&#20449;&#24687;&#20294;&#38590;&#20197;&#35299;&#37322;&#30340;&#24402;&#22240;&#26144;&#23556;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#26368;&#22823;&#28608;&#27963;&#32452;&#25552;&#21462;&#65288;MAGE&#65289;&#21644;&#22810;&#23610;&#24230;&#21487;&#35299;&#37322;&#24615;&#21487;&#35270;&#21270;&#65288;Ms-IV&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#12290;MAGE&#21487;&#20197;&#25214;&#21040;&#32473;&#23450;CNN&#20013;&#24418;&#25104;&#35821;&#20041;&#21547;&#20041;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#32858;&#31867;&#23558;&#36825;&#20123;&#30456;&#20284;&#29305;&#24449;&#27169;&#24335;&#20998;&#32452;&#20026;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#36890;&#36807;Ms-IV&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#36825;&#19968;&#26041;&#27861;&#21463;&#21040;&#38459;&#26029;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#21551;&#21457;&#65288;&#21253;&#25324;&#22240;&#26524;&#20851;&#31995;&#65289;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;&#65292;&#31216;&#20026;&#31867;&#21035;&#24863;&#30693;&#39034;&#24207;&#30456;&#20851;&#24615;&#65288;CaOC&#65289;&#65292;&#20840;&#23616;&#35780;&#20272;&#26681;&#25454;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#26368;&#37325;&#35201;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing interpretability of deep-learning models to non-experts, while fundamental for a responsible real-world usage, is challenging. Attribution maps from xAI techniques, such as Integrated Gradients, are a typical example of a visualization technique containing a high level of information, but with difficult interpretation. In this paper, we propose two methods, Maximum Activation Groups Extraction (MAGE) and Multiscale Interpretable Visualization (Ms-IV), to explain the model's decision, enhancing global interpretability. MAGE finds, for a given CNN, combinations of features which, globally, form a semantic meaning, that we call concepts. We group these similar feature patterns by clustering in ``concepts'', that we visualize through Ms-IV. This last method is inspired by Occlusion and Sensitivity analysis (incorporating causality), and uses a novel metric, called Class-aware Order Correlation (CaOC), to globally evaluate the most important image regions according to the model's 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#36712;&#36857;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#38632;&#29615;&#22659;&#20013;&#23545;&#32454;&#32990;&#36830;&#25509;&#30340;&#26080;&#20154;&#26426;&#36827;&#34892;&#36712;&#36857;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29289;&#29702;&#27169;&#25311;&#22120;&#32771;&#34385;&#20102;&#35814;&#32454;&#30340;&#29615;&#22659;&#20449;&#24687;&#21644;&#38632;&#27700;&#23545;&#20449;&#21495;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00017</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#32454;&#32990;&#36830;&#25509;&#26080;&#20154;&#26426;&#22312;&#22810;&#38632;&#29615;&#22659;&#20013;&#30340;&#36712;&#36857;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Physics-Based Trajectory Design for Cellular-Connected UAV in Rainy Environments Based on Deep Reinforcement Learning. (arXiv:2309.00017v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#36712;&#36857;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#38632;&#29615;&#22659;&#20013;&#23545;&#32454;&#32990;&#36830;&#25509;&#30340;&#26080;&#20154;&#26426;&#36827;&#34892;&#36712;&#36857;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29289;&#29702;&#27169;&#25311;&#22120;&#32771;&#34385;&#20102;&#35814;&#32454;&#30340;&#29615;&#22659;&#20449;&#24687;&#21644;&#38632;&#27700;&#23545;&#20449;&#21495;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29616;&#26377;&#30340;&#32454;&#32990;&#22522;&#30784;&#35774;&#26045;&#65292;&#32454;&#32990;&#36830;&#25509;&#30340;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#36890;&#36807;&#21487;&#38752;&#30340;&#36890;&#20449;&#22686;&#24378;&#20102;&#20256;&#32479;&#30340;UAV&#33021;&#21147;&#65292;&#22240;&#27492;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23427;&#20204;&#24050;&#32463;&#34987;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#22825;&#27668;&#39044;&#27979;&#21644;&#25628;&#25937;&#34892;&#21160;&#12290;&#28982;&#32780;&#65292;&#22312;&#26497;&#31471;&#22825;&#27668;&#26465;&#20214;&#19979;&#65292;&#22914;&#38632;&#22825;&#65292;&#32454;&#32990;UAV&#30340;&#36712;&#36857;&#35774;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#30001;&#20110;&#22825;&#31354;&#20013;&#30340;&#35206;&#30422;&#21306;&#22495;&#36739;&#24369;&#65292;UAV&#30340;&#39134;&#34892;&#26102;&#38388;&#38480;&#21046;&#20197;&#21450;&#38632;&#28404;&#24341;&#36215;&#30340;&#20449;&#21495;&#34928;&#20943;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#36712;&#36857;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#32454;&#32990;&#36830;&#25509;&#30340;UAV&#22312;&#22810;&#38632;&#29615;&#22659;&#20013;&#12290;&#37319;&#29992;&#22522;&#20110;&#29289;&#29702;&#30340;&#30005;&#30913;&#27169;&#25311;&#22120;&#32771;&#34385;&#20102;&#35814;&#32454;&#30340;&#29615;&#22659;&#20449;&#24687;&#21644;&#38632;&#27700;&#23545;&#26080;&#32447;&#30005;&#27874;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;&#36712;&#36857;&#20248;&#21270;&#38382;&#39064;&#34987;&#35268;&#21010;&#20026;&#21516;&#26102;&#32771;&#34385;&#26080;&#20154;&#26426;&#39134;&#34892;&#26102;&#38388;&#21644;&#20449;&#21495;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular-connected unmanned aerial vehicles (UAVs) have gained increasing attention due to their potential to enhance conventional UAV capabilities by leveraging existing cellular infrastructure for reliable communications between UAVs and base stations. They have been used for various applications, including weather forecasting and search and rescue operations. However, under extreme weather conditions such as rainfall, it is challenging for the trajectory design of cellular UAVs, due to weak coverage regions in the sky, limitations of UAV flying time, and signal attenuation caused by raindrops. To this end, this paper proposes a physics-based trajectory design approach for cellular-connected UAVs in rainy environments. A physics-based electromagnetic simulator is utilized to take into account detailed environment information and the impact of rain on radio wave propagation. The trajectory optimization problem is formulated to jointly consider UAV flying time and signal-to-interferenc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FID&#24471;&#20998;&#21644;&#20854;&#20182;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65292;&#24182;&#33021;&#20197;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.00008</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#29983;&#25104;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Public Data Improves Differentially Private Image Generation Quality. (arXiv:2309.00008v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FID&#24471;&#20998;&#21644;&#20854;&#20182;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65292;&#24182;&#33021;&#20197;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20849;&#25968;&#25454;&#32463;&#24120;&#34987;&#29992;&#26469;&#25913;&#21892;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#30340;&#38544;&#31169;-&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#20294;&#26159;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#20551;&#35774;&#36825;&#20123;&#25968;&#25454;&#19982;&#31169;&#26377;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#36890;&#29992;&#30340;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#26469;&#25552;&#39640;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#20013;&#24046;&#20998;&#38544;&#31169;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#21033;&#29992;&#20844;&#20849;&#25968;&#25454;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#20844;&#20849;&#25968;&#25454;&#20998;&#24067;&#30340;&#25903;&#25345;&#38598;&#21253;&#21547;&#20102;&#31169;&#26377;&#25968;&#25454;&#30340;&#25903;&#25345;&#38598;&#65307;&#19968;&#20010;&#20363;&#23376;&#26159;&#20844;&#20849;&#25968;&#25454;&#26469;&#33258;&#36890;&#29992;&#22411;&#30340;&#20114;&#32852;&#32593;&#35268;&#27169;&#22270;&#20687;&#28304;&#65292;&#32780;&#31169;&#26377;&#25968;&#25454;&#30001;&#29305;&#23450;&#31867;&#22411;&#30340;&#22270;&#20687;&#32452;&#25104;&#12290;&#35814;&#32454;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FID&#24471;&#20998;&#21644;&#20854;&#20182;&#25351;&#26631;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#33021;&#22815;&#20197;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#24335;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public data has been frequently used to improve the privacy-accuracy trade-off of differentially private machine learning, but prior work largely assumes that this data come from the same distribution as the private. In this work, we look at how to use generic large-scale public data to improve the quality of differentially private image generation in Generative Adversarial Networks (GANs), and provide an improved method that uses public data effectively. Our method works under the assumption that the support of the public data distribution contains the support of the private; an example of this is when the public data come from a general-purpose internet-scale image source, while the private data consist of images of a specific type. Detailed evaluations show that our method achieves SOTA in terms of FID score and other metrics compared with existing methods that use public data, and can generate high-quality, photo-realistic images in a differentially private manner.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;Top-k&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#19981;&#21487;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#26041;&#27861;&#65292;&#26082;&#33021;&#22815;&#27450;&#39575;&#20154;&#30524;&#30340;&#35270;&#35273;&#24863;&#30693;&#65292;&#21448;&#33021;&#22815;&#36991;&#24320;&#24230;&#37327;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.00007</link><description>&lt;p&gt;
&#24403;&#24230;&#37327;&#19981;&#21487;&#38752;&#26102;&#65306;&#26397;&#30528;Top-k&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#19981;&#21487;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
When Measures are Unreliable: Imperceptible Adversarial Perturbations toward Top-$k$ Multi-Label Learning. (arXiv:2309.00007v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;Top-k&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#19981;&#21487;&#23519;&#35273;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#26041;&#27861;&#65292;&#26082;&#33021;&#22815;&#27450;&#39575;&#20154;&#30524;&#30340;&#35270;&#35273;&#24863;&#30693;&#65292;&#21448;&#33021;&#22815;&#36991;&#24320;&#24230;&#37327;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#23545;&#25239;&#24615;&#23398;&#20064;&#22312;&#21508;&#31181;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#28085;&#30422;&#20102;&#20174;&#22810;&#31867;&#21035;&#23398;&#20064;&#21040;&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#21482;&#20851;&#27880;&#20256;&#32479;&#30340;&#35270;&#35273;&#19981;&#21487;&#23519;&#35273;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#26469;&#33258;&#24230;&#37327;&#30340;&#26032;&#30340;&#21487;&#24863;&#30693;&#38382;&#39064;&#65292;&#20363;&#22914;Precision@k&#21644;mAP@k&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#19968;&#20010;&#35757;&#32451;&#33391;&#22909;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#22312;&#26576;&#20123;&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#36828;&#20302;&#20110;&#39044;&#26399;&#26102;&#65292;&#21463;&#23475;&#32773;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#24847;&#35782;&#21040;&#36825;&#31181;&#24615;&#33021;&#36864;&#21270;&#28304;&#20110;&#25915;&#20987;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#26412;&#36523;&#12290;&#22240;&#27492;&#65292;&#29702;&#24819;&#30340;&#22810;&#26631;&#31614;&#23545;&#25239;&#24615;&#25915;&#20987;&#24212;&#35813;&#33021;&#22815;&#27450;&#39575;&#35270;&#35273;&#24863;&#30693;&#65292;&#24182;&#19988;&#36991;&#24320;&#24230;&#37327;&#30340;&#30417;&#27979;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#39318;&#20808;&#24341;&#20837;&#20102;&#24230;&#37327;&#19981;&#21487;&#23519;&#35273;&#24615;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#36825;&#31181;&#26082;&#33021;&#22815;&#23454;&#29616;&#35270;&#35273;&#19981;&#21487;&#23519;&#35273;&#24615;&#21448;&#33021;&#22815;&#36991;&#24320;&#24230;&#37327;&#30417;&#27979;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the great success of deep neural networks, adversarial learning has received widespread attention in various studies, ranging from multi-class learning to multi-label learning. However, existing adversarial attacks toward multi-label learning only pursue the traditional visual imperceptibility but ignore the new perceptible problem coming from measures such as Precision@$k$ and mAP@$k$. Specifically, when a well-trained multi-label classifier performs far below the expectation on some samples, the victim can easily realize that this performance degeneration stems from attack, rather than the model itself. Therefore, an ideal multi-labeling adversarial attack should manage to not only deceive visual perception but also evade monitoring of measures. To this end, this paper first proposes the concept of measure imperceptibility. Then, a novel loss function is devised to generate such adversarial perturbations that could achieve both visual and measure imperceptibility. Furthermore, a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#21512;&#25104;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#28304;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#35266;&#27979;&#22330;&#26223;&#25110;&#23545;&#35937;&#30340;&#32508;&#21512;&#12289;&#20934;&#30830;&#21644;&#35814;&#32454;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.00005</link><description>&lt;p&gt;
&#22810;&#28304;&#34701;&#21512;&#39640;&#20809;&#35889;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#21512;&#25104;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
High Spectral Spatial Resolution Synthetic HyperSpectral Dataset form multi-source fusion. (arXiv:2309.00005v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#21512;&#25104;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#34701;&#21512;&#22810;&#28304;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#35266;&#27979;&#22330;&#26223;&#25110;&#23545;&#35937;&#30340;&#32508;&#21512;&#12289;&#20934;&#30830;&#21644;&#35814;&#32454;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21512;&#25104;&#39640;&#20809;&#35889;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23558;&#39640;&#20809;&#35889;&#21644;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#25104;&#20687;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#23545;&#35266;&#27979;&#22330;&#26223;&#25110;&#23545;&#35937;&#30340;&#32508;&#21512;&#12289;&#20934;&#30830;&#21644;&#35814;&#32454;&#30340;&#34920;&#31034;&#12290;&#22312;&#20381;&#36182;&#21333;&#19968;&#30456;&#26426;&#26102;&#65292;&#33719;&#24471;&#36825;&#31181;&#29702;&#24819;&#30340;&#21697;&#36136;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#21033;&#29992;&#19977;&#31181;&#27169;&#24577;&#65288;RGB&#12289;&#25512;&#25195;&#24335;&#21487;&#35265;&#20809;&#39640;&#20809;&#35889;&#30456;&#26426;&#21644;&#24555;&#29031;&#24335;&#32418;&#22806;&#39640;&#20809;&#35889;&#30456;&#26426;&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27599;&#31181;&#27169;&#24577;&#37117;&#20855;&#26377;&#19981;&#21516;&#30340;&#31354;&#38388;&#21644;&#20809;&#35889;&#20998;&#36776;&#29575;&#12290;&#19981;&#21516;&#30340;&#30456;&#26426;&#31995;&#32479;&#20855;&#26377;&#19981;&#21516;&#30340;&#20809;&#24230;&#23398;&#29305;&#24615;&#65292;&#23548;&#33268;&#31354;&#38388;&#21644;&#20809;&#35889;&#20998;&#36776;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;RGB&#30456;&#26426;&#36890;&#24120;&#20855;&#26377;&#39640;&#31354;&#38388;&#20998;&#36776;&#29575;&#20294;&#20809;&#35889;&#20998;&#36776;&#29575;&#26377;&#38480;&#65292;&#32780;&#39640;&#20809;&#35889;&#30456;&#26426;&#22312;&#29306;&#29298;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#39640;&#20809;&#35889;&#20998;&#36776;&#29575;&#12290;&#27492;&#22806;&#65292;&#39640;&#20809;&#35889;&#30456;&#26426;&#26412;&#36523;&#37319;&#29992;&#19981;&#21516;&#30340;&#25429;&#33719;&#25216;&#26415;&#21644;&#20809;&#35889;&#33539;&#22260;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#32508;&#21512;&#33719;&#21462;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper introduces a synthetic hyperspectral dataset that combines high spectral and spatial resolution imaging to achieve a comprehensive, accurate, and detailed representation of observed scenes or objects. Obtaining such desirable qualities is challenging when relying on a single camera. The proposed dataset addresses this limitation by leveraging three modalities: RGB, push-broom visible hyperspectral camera, and snapshot infrared hyperspectral camera, each offering distinct spatial and spectral resolutions. Different camera systems exhibit varying photometric properties, resulting in a trade-off between spatial and spectral resolution. RGB cameras typically offer high spatial resolution but limited spectral resolution, while hyperspectral cameras possess high spectral resolution at the expense of spatial resolution. Moreover, hyperspectral cameras themselves employ different capturing techniques and spectral ranges, further complicating the acquisition of comprehensive
&lt;/p&gt;</description></item><item><title>GNFactor&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20195;&#29702;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#21644;Perceiver Transformer&#27169;&#22359;&#65292;&#20197;&#21450;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#23427;&#36890;&#36807;&#23558;&#35270;&#35273;&#21644;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#26469;&#25552;&#39640;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.16891</link><description>&lt;p&gt;
GNFactor&#65306;&#20855;&#26377;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#30340;&#22810;&#20219;&#21153;&#30495;&#23454;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields. (arXiv:2308.16891v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16891
&lt;/p&gt;
&lt;p&gt;
GNFactor&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20195;&#29702;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#21644;Perceiver Transformer&#27169;&#22359;&#65292;&#20197;&#21450;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#26469;&#23454;&#29616;&#23545;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#23427;&#36890;&#36807;&#23558;&#35270;&#35273;&#21644;&#35821;&#20041;&#20449;&#24687;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#26469;&#25552;&#39640;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32467;&#26500;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#24320;&#21457;&#33021;&#22815;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#20219;&#21153;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#19968;&#30452;&#26159;&#26426;&#22120;&#20154;&#23398;&#20013;&#30340;&#19968;&#20010;&#38271;&#26399;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#20840;&#38754;&#29702;&#35299;&#22330;&#26223;&#30340;&#19977;&#32500;&#32467;&#26500;&#21644;&#35821;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNFactor&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#20219;&#21153;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#21487;&#35270;&#34892;&#20026;&#20811;&#38534;&#20195;&#29702;&#65292;&#23427;&#21033;&#29992;&#21487;&#27867;&#21270;&#31070;&#32463;&#29305;&#24449;&#22330;&#65288;GNF&#65289;&#20316;&#20026;&#37325;&#24314;&#27169;&#22359;&#65292;Perceiver Transformer&#20316;&#20026;&#20915;&#31574;&#27169;&#22359;&#65292;&#20849;&#20139;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#34920;&#31034;&#12290;&#20026;&#20102;&#23558;&#35821;&#20041;&#32435;&#20837;&#19977;&#32500;&#34920;&#31034;&#65292;&#37325;&#24314;&#27169;&#22359;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;&#31283;&#23450;&#25193;&#25955;&#65289;&#23558;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#25552;&#21462;&#21040;&#28145;&#24230;&#19977;&#32500;&#20307;&#32032;&#20013;&#12290;&#25105;&#20204;&#22312;3&#20010;&#30495;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;GNFactor&#65292;&#24182;&#23545;10&#20010;RLBench&#20219;&#21153;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#21482;&#20351;&#29992;&#20102;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot needs to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present $\textbf{GNFactor}$, a visual behavior cloning agent for multi-task robotic manipulation with $\textbf{G}$eneralizable $\textbf{N}$eural feature $\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural field (GNF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module utilizes a vision-language foundation model ($\textit{e.g.}$, Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24046;&#20998;&#21442;&#25968;&#20002;&#24323;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65288;FedDD&#65289;&#12290;&#36825;&#31181;&#26041;&#26696;&#36991;&#20813;&#20102;&#39057;&#32321;&#20132;&#25442;&#27169;&#22411;&#21442;&#25968;&#30340;&#36890;&#20449;&#24310;&#36831;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#20002;&#24323;&#32780;&#19981;&#26159;&#23458;&#25143;&#31471;&#36873;&#25321;&#26469;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16835</link><description>&lt;p&gt;
FedDD: &#36890;&#36807;&#24046;&#20998;&#21442;&#25968;&#20002;&#24323;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout. (arXiv:2308.16835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24046;&#20998;&#21442;&#25968;&#20002;&#24323;&#23454;&#29616;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65288;FedDD&#65289;&#12290;&#36825;&#31181;&#26041;&#26696;&#36991;&#20813;&#20102;&#39057;&#32321;&#20132;&#25442;&#27169;&#22411;&#21442;&#25968;&#30340;&#36890;&#20449;&#24310;&#36831;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27169;&#22411;&#21442;&#25968;&#20002;&#24323;&#32780;&#19981;&#26159;&#23458;&#25143;&#31471;&#36873;&#25321;&#26469;&#20248;&#21270;&#20840;&#23616;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#38656;&#35201;&#39057;&#32321;&#20132;&#25442;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#23548;&#33268;&#20102;&#38271;&#26102;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#23588;&#20854;&#26159;&#24403;&#23458;&#25143;&#31471;&#30340;&#32593;&#32476;&#29615;&#22659;&#24046;&#24322;&#24456;&#22823;&#26102;&#12290;&#27492;&#22806;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#38656;&#35201;&#31561;&#24453;&#26368;&#24930;&#30340;&#23458;&#25143;&#31471;&#65288;&#21363;straggler&#65292;&#21487;&#33021;&#20855;&#26377;&#26368;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#12289;&#26368;&#20302;&#30340;&#35745;&#31639;&#33021;&#21147;&#25110;&#26368;&#24046;&#30340;&#32593;&#32476;&#26465;&#20214;&#65289;&#19978;&#20256;&#21442;&#25968;&#65292;&#36825;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#36890;&#20449;&#25928;&#29575;&#12290;&#24120;&#29992;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#27861;&#65292;&#22914;&#37096;&#20998;&#23458;&#25143;&#31471;&#36873;&#25321;&#65292;&#20250;&#23548;&#33268;&#35745;&#31639;&#36164;&#28304;&#30340;&#28010;&#36153;&#24182;&#21066;&#24369;&#20840;&#23616;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#27169;&#22411;&#21442;&#25968;&#20002;&#24323;&#32780;&#19981;&#26159;&#23458;&#25143;&#31471;&#36873;&#25321;&#65292;&#24182;&#25454;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#21442;&#25968;&#20002;&#24323;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#65288;FedDD&#65289;&#26694;&#26550;&#12290;FedDD&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27169;&#22359;&#65306;&#20002;&#24323;&#29575;&#20998;&#37197;&#21644;&#19978;&#20256;&#21442;&#25968;&#36873;&#25321;&#65292;&#23558;&#20248;&#21270;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) requires frequent exchange of model parameters, which leads to long communication delay, especially when the network environments of clients vary greatly. Moreover, the parameter server needs to wait for the slowest client (i.e., straggler, which may have the largest model size, lowest computing capability or worst network condition) to upload parameters, which may significantly degrade the communication efficiency. Commonly-used client selection methods such as partial client selection would lead to the waste of computing resources and weaken the generalization of the global model. To tackle this problem, along a different line, in this paper, we advocate the approach of model parameter dropout instead of client selection, and accordingly propose a novel framework of Federated learning scheme with Differential parameter Dropout (FedDD). FedDD consists of two key modules: dropout rate allocation and uploaded parameter selection, which will optimize the model par
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#21644;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#39044;&#27979;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16818</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network. (arXiv:2308.16818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#21644;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#39044;&#27979;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#20013;&#21463;&#26234;&#33021;&#20132;&#21449;&#21475;&#25511;&#21046;&#30340;&#20132;&#21449;&#21475;&#30340;&#20132;&#36890;&#27969;&#37327;&#23545;&#20110;&#25552;&#21319;&#20132;&#36890;&#20986;&#34892;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#19981;&#35268;&#21017;&#65292;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#24182;&#19988;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#24322;&#27493;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;2&#65289;&#20132;&#36890;&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;3) &#38656;&#35201;&#39044;&#27979;&#30340;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#24403;&#21069;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;(ASeer)&#26469;&#39044;&#27979;&#26234;&#33021;&#20132;&#21449;&#21475;&#36827;&#20837;&#36710;&#36947;&#30340;&#20132;&#36890;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#20132;&#36890;&#25193;&#25955;&#22270;&#19978;&#36830;&#25509;&#36710;&#36947;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#22270;&#25193;&#25955;&#32593;&#32476;&#26469;&#27169;&#25311;&#36710;&#36947;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate traffic forecasting at intersections governed by intelligent traffic signals is critical for the advancement of an effective intelligent traffic signal control system. However, due to the irregular traffic time series produced by intelligent intersections, the traffic forecasting task becomes much more intractable and imposes three major new challenges: 1) asynchronous spatial dependency, 2) irregular temporal dependency among traffic data, and 3) variable-length sequence to be predicted, which severely impede the performance of current traffic forecasting methods. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic states of the lanes entering intelligent intersections in a future time window. Specifically, by linking lanes via a traffic diffusion graph, we first propose an Asynchronous Graph Diffusion Network to model the asynchronous spatial dependency between the time-misaligned traffic state measurements of la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#32593;&#32476;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;$L_1$-&#33539;&#25968;&#40065;&#26834;&#24615;&#21644;&#20998;&#24067;&#24335;&#27425;&#26799;&#24230;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23450;&#20301;&#38382;&#39064;&#20013;&#30340;&#24322;&#24120;&#25968;&#25454;&#24178;&#25200;&#21644;&#31639;&#27861;&#25910;&#25947;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16737</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#32593;&#32476;&#21270;&#32852;&#37030;&#23398;&#20064;&#22312;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Networked Federated Learning for Localization. (arXiv:2308.16737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#32593;&#32476;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;$L_1$-&#33539;&#25968;&#40065;&#26834;&#24615;&#21644;&#20998;&#24067;&#24335;&#27425;&#26799;&#24230;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23450;&#20301;&#38382;&#39064;&#20013;&#30340;&#24322;&#24120;&#25968;&#25454;&#24178;&#25200;&#21644;&#31639;&#27861;&#25910;&#25947;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#35774;&#22791;&#19978;&#30340;&#32852;&#37030;&#29615;&#22659;&#20013;&#65292;&#26412;&#36136;&#19978;&#26159;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#23450;&#20301;&#38382;&#39064;&#12290;&#30001;&#20110;&#32852;&#37030;&#29615;&#22659;&#30340;&#20998;&#25955;&#24615;&#36136;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#25104;&#20026;&#21487;&#20280;&#32553;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#20851;&#38190;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29615;&#22659;&#32463;&#24120;&#21463;&#21040;&#24322;&#24120;&#25968;&#25454;&#30340;&#24178;&#25200;&#65292;&#20351;&#24471;&#20256;&#32479;&#26041;&#27861;&#22312;&#32500;&#25252;&#20272;&#35745;&#31934;&#24230;&#21644;&#30830;&#20445;&#31639;&#27861;&#25910;&#25947;&#26041;&#38754;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#24067;&#24335;&#27425;&#26799;&#24230;&#26694;&#26550;&#20013;$L_1$-&#33539;&#25968;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#36825;&#20123;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21407;&#22987;&#24418;&#24335;&#35299;&#20915;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#36845;&#20195;&#31616;&#21270;&#25110;&#36817;&#20284;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#20272;&#35745;&#31934;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#65292;&#31361;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of localization, which is inherently non-convex and non-smooth in a federated setting where the data is distributed across a multitude of devices. Due to the decentralized nature of federated environments, distributed learning becomes essential for scalability and adaptability. Moreover, these environments are often plagued by outlier data, which presents substantial challenges to conventional methods, particularly in maintaining estimation accuracy and ensuring algorithm convergence. To mitigate these challenges, we propose a method that adopts an $L_1$-norm robust formulation within a distributed sub-gradient framework, explicitly designed to handle these obstacles. Our approach addresses the problem in its original form, without resorting to iterative simplifications or approximations, resulting in enhanced computational efficiency and improved estimation accuracy. We demonstrate that our method converges to a stationary point, highlighting its effec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26448;&#26009;&#20449;&#24687;&#23398;&#21464;&#21387;&#22120;&#65288;MatInFormer&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#26230;&#20307;&#23398;&#35821;&#27861;&#21644;&#24341;&#20837;MOFs&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#26448;&#26009;&#24615;&#33021;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.16259</link><description>&lt;p&gt;
&#26448;&#26009;&#20449;&#24687;&#23398;&#21464;&#21387;&#22120;&#65306;&#19968;&#31181;&#29992;&#20110;&#21487;&#35299;&#37322;&#26448;&#26009;&#24615;&#33021;&#39044;&#27979;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction. (arXiv:2308.16259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26448;&#26009;&#20449;&#24687;&#23398;&#21464;&#21387;&#22120;&#65288;MatInFormer&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#26230;&#20307;&#23398;&#35821;&#27861;&#21644;&#24341;&#20837;MOFs&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#23545;&#26448;&#26009;&#24615;&#33021;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20998;&#23376;&#24314;&#27169;&#31561;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#25105;&#20204;&#30340;&#27169;&#22411;&#26448;&#26009;&#20449;&#24687;&#23398;&#21464;&#21387;&#22120;(MatInFormer)&#65292;&#23558;LLMs&#30340;&#36825;&#31181;&#33539;&#24335;&#25193;&#23637;&#21040;&#26448;&#26009;&#24615;&#33021;&#39044;&#27979;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#30456;&#20851;&#30340;&#31354;&#38388;&#32676;&#20449;&#24687;&#36827;&#34892;&#26631;&#35760;&#21270;&#65292;&#23398;&#20064;&#20102;&#26230;&#20307;&#23398;&#30340;&#35821;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;MatInFormer&#30340;&#36866;&#24212;&#24615;&#65292;&#36890;&#36807; incorporating task-specific data pertaining to Metal-Organic Frameworks (MOFs)&#12290;&#36890;&#36807;&#27880;&#24847;&#21147;&#21487;&#35270;&#21270;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#27169;&#22411;&#22312;&#23646;&#24615;&#39044;&#27979;&#36807;&#31243;&#20013;&#20248;&#20808;&#32771;&#34385;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#27169;&#22411;&#22312;14&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#20013;&#32463;&#36807;&#23454;&#35777;&#39564;&#35777;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#20854;&#22312;&#36890;&#36807;&#20934;&#30830;&#30340;&#26448;&#26009;&#23646;&#24615;&#39044;&#27979;&#36827;&#34892;&#39640;&#36890;&#37327;&#31579;&#36873;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the remarkable capabilities of large language models (LLMs) have been illustrated across a variety of research domains such as natural language processing, computer vision, and molecular modeling. We extend this paradigm by utilizing LLMs for material property prediction by introducing our model Materials Informatics Transformer (MatInFormer). Specifically, we introduce a novel approach that involves learning the grammar of crystallography through the tokenization of pertinent space group information. We further illustrate the adaptability of MatInFormer by incorporating task-specific data pertaining to Metal-Organic Frameworks (MOFs). Through attention visualization, we uncover the key features that the model prioritizes during property prediction. The effectiveness of our proposed model is empirically validated across 14 distinct datasets, hereby underscoring its potential for high throughput screening through accurate material property prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16245</link><description>&lt;p&gt;
&#22238;&#24402;&#38382;&#39064;&#30340;&#26657;&#20934;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36890;&#24120;&#26159;&#29616;&#20195;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#30340;&#19968;&#37096;&#20998;&#12290;&#22312;&#22522;&#20110;AI&#30340;DSS&#20013;&#20351;&#29992;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#21019;&#24314;&#33021;&#22815;&#21521;&#20154;&#31867;&#29992;&#25143;&#35299;&#37322;&#20854;&#29702;&#30001;&#30340;AI&#31995;&#32479;&#12290;XAI&#20013;&#30340;&#23616;&#37096;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#20010;&#21035;&#39044;&#27979;&#30340;&#21407;&#22240;&#30340;&#20449;&#24687;&#65292;&#21363;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#26159;&#26080;&#27861;&#37327;&#21270;&#19982;&#29305;&#24449;&#37325;&#35201;&#24615;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;Calibrated Explanations&#65288;CE&#65289;&#30340;&#25193;&#23637;&#65292;&#20043;&#21069;&#21482;&#25903;&#25345;&#20998;&#31867;&#65292;&#29616;&#22312;&#25903;&#25345;&#26631;&#20934;&#22238;&#24402;&#21644;&#27010;&#29575;&#22238;&#24402;&#65292;&#21363;&#30446;&#26631;&#36229;&#36807;&#20219;&#24847;&#38408;&#20540;&#30340;&#27010;&#29575;&#12290;&#22238;&#24402;&#38382;&#39064;&#30340;&#25193;&#23637;&#20445;&#30041;&#20102;CE&#30340;&#25152;&#26377;&#20248;&#28857;&#65292;&#20363;&#22914;&#23558;&#24213;&#23618;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is often an integral part of modern decision support systems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidenc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21517;&#20026;ExGNAS&#12290;&#23427;&#21253;&#25324;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#33021;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.15734</link><description>&lt;p&gt;
&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient and Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search. (arXiv:2308.15734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21517;&#20026;ExGNAS&#12290;&#23427;&#21253;&#25324;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#33021;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#22312;&#21508;&#20010;&#39046;&#22495;&#36827;&#34892;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23613;&#31649;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#20351;&#29992;GNNs&#65292;&#20294;&#23545;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26469;&#35828;&#65292;&#22312;&#19981;&#21516;&#30340;&#22270;&#20013;&#35774;&#35745;/&#36873;&#25321;&#26368;&#20339;GNN&#26550;&#26500;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#33410;&#30465;&#20154;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24050;&#32463;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;Graph NAS&#65289;&#26469;&#25628;&#32034;&#32467;&#21512;&#29616;&#26377;&#32452;&#20214;&#30340;&#27425;&#20248;GNN&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#30340;Graph NAS&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#28385;&#36275;&#21487;&#35299;&#37322;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#22810;&#26679;&#21270;&#22270;&#24418;&#30340;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;Graph NAS&#26041;&#27861;&#65292;&#31216;&#20026;ExGNAS&#65292;&#23427;&#21253;&#25324;&#65288;i&#65289;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#65288;ii&#65289;&#19968;&#20010;&#33021;&#22815;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#25628;&#32034;&#31354;&#38388;&#20165;&#21253;&#21547;&#21487;&#20197;&#22788;&#29702;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#30340;&#22522;&#26412;&#20989;&#25968;&#12290;&#25628;&#32034;&#31639;&#27861;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are powerful tools for performing data science tasks in various domains. Although we use GNNs in wide application scenarios, it is a laborious task for researchers and practitioners to design/select optimal GNN rchitectures in diverse graphs. To save human efforts and computational costs, graph neural architecture search (Graph NAS) has been used to search for a sub-optimal GNN architecture that combines existing components. However, there are no existing Graph NAS methods that satisfy explainability, efficiency, and adaptability to various graphs. Therefore, we propose an efficient and explainable Graph NAS method, called ExGNAS, which consists of (i) a simple search space that can adapt to various graphs and (ii) a search algorithm that makes the decision process explainable. The search space includes only fundamental functions that can handle homophilic and heterophilic graphs. The search algorithm efficiently searches for the best GNN architecture via M
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22120;&#29992;&#20110;&#20272;&#35745;&#21333;&#20010;&#29366;&#24577;&#20540;&#65292;&#24182;&#36890;&#36807;&#26681;&#25454;&#22870;&#21169;&#20195;&#26367;&#24120;&#29992;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#24120;&#25968;&#65292;&#25552;&#20379;&#20102;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#25216;&#24039;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.14919</link><description>&lt;p&gt;
&#35770;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22870;&#21169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
On Reward Structures of Markov Decision Processes. (arXiv:2308.14919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22120;&#29992;&#20110;&#20272;&#35745;&#21333;&#20010;&#29366;&#24577;&#20540;&#65292;&#24182;&#36890;&#36807;&#26681;&#25454;&#22870;&#21169;&#20195;&#26367;&#24120;&#29992;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#24120;&#25968;&#65292;&#25552;&#20379;&#20102;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#25216;&#24039;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#36716;&#31227;&#26680;&#19982;&#22870;&#21169;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#36825;&#20004;&#20010;&#22240;&#32032;&#22312;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#27491;&#22914;&#23427;&#20204;&#22312;&#36125;&#23572;&#26364;&#26041;&#31243;&#20013;&#30340;&#23384;&#22312;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#38024;&#23545;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#20851;&#30340;&#21508;&#31181;"&#25104;&#26412;"&#65292;&#22870;&#21169;&#26159;&#29702;&#35299;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#32467;&#26500;&#30340;&#26680;&#24515;&#65292;&#22870;&#21169;&#20013;&#24515;&#27010;&#24565;&#21487;&#20197;&#38416;&#26126;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#27010;&#24565;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31574;&#30053;&#35780;&#20272;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#20854;&#23454;&#20363;&#29305;&#23450;&#30340;&#35823;&#24046;&#30028;&#20026;$\tilde{O}(\sqrt{\frac{\tau_s}{n}})$&#65292;&#29992;&#20110;&#20272;&#35745;&#21333;&#20010;&#29366;&#24577;&#20540;&#12290;&#22312;&#22312;&#32447;&#36951;&#25022;&#26368;&#23567;&#21270;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#36716;&#31227;&#30340;MDP&#24120;&#25968;&#65292;&#30452;&#24452;&#65292;&#25913;&#36827;&#20026;&#22522;&#20110;&#22870;&#21169;&#30340;&#24120;&#25968;&#65292;&#26368;&#22823;&#39044;&#26399;&#21040;&#36798;&#25104;&#26412;&#65292;&#24182;&#36890;&#36807;&#35813;&#24120;&#25968;&#20026;&#19968;&#31181;&#24191;&#20026;&#20154;&#30693;&#30340;&#25216;&#26415;&#65292;&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#24418;&#29366;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Markov decision process can be parameterized by a transition kernel and a reward function. Both play essential roles in the study of reinforcement learning as evidenced by their presence in the Bellman equations. In our inquiry of various kinds of ``costs'' associated with reinforcement learning inspired by the demands in robotic applications, rewards are central to understanding the structure of a Markov decision process and reward-centric notions can elucidate important concepts in reinforcement learning. Specifically, we studied the sample complexity of policy evaluation and developed a novel estimator with an instance-specific error bound of $\tilde{O}(\sqrt{\frac{\tau_s}{n}})$ for estimating a single state value. Under the online regret minimization setting, we refined the transition-based MDP constant, diameter, into a reward-based constant, maximum expected hitting cost, and with it, provided a theoretical explanation for how a well-known technique, potential-based reward shap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#21161;&#22521;&#35757;&#20307;&#21046;&#65292;&#21033;&#29992;&#29420;&#31435;&#23376;&#32593;&#32476;&#30340;&#38598;&#25104;&#21644;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#33258;&#21161;&#30340;&#40065;&#26834;&#24615;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.14705</link><description>&lt;p&gt;
&#33258;&#21161;&#30340;&#40065;&#26834;&#24615;&#34920;&#31034;&#23398;&#20064;&#30340;&#29420;&#31435;&#23376;&#32593;&#32476;&#22810;&#26679;&#21270;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diversified Ensemble of Independent Sub-Networks for Robust Self-Supervised Representation Learning. (arXiv:2308.14705v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#21161;&#22521;&#35757;&#20307;&#21046;&#65292;&#21033;&#29992;&#29420;&#31435;&#23376;&#32593;&#32476;&#30340;&#38598;&#25104;&#21644;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#33258;&#21161;&#30340;&#40065;&#26834;&#24615;&#34920;&#31034;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12289;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#21644;&#25913;&#21892;&#28145;&#24230;&#26377;&#30417;&#30563;&#23398;&#20064;&#40065;&#26834;&#24615;&#30340;&#24191;&#27867;&#25215;&#35748;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#28145;&#23618;&#38598;&#25104;&#36890;&#24120;&#20855;&#26377;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#28145;&#24230;&#38598;&#25104;&#30340;&#25928;&#29575;&#19982;&#38598;&#25104;&#25104;&#21592;&#20043;&#38388;&#30340;&#22810;&#26679;&#24615;&#26377;&#20851;&#65292;&#36825;&#23545;&#20110;&#22823;&#22411;&#30340;&#36807;&#21442;&#25968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#32780;&#19988;&#65292;&#38598;&#25104;&#23398;&#20064;&#23578;&#26410;&#24471;&#21040;&#22914;&#27492;&#24191;&#27867;&#30340;&#37319;&#29992;&#65292;&#24182;&#19988;&#23545;&#20110;&#33258;&#21161;&#30340;&#25110;&#26080;&#30417;&#30563;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#35828;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;&#22312;&#36825;&#20123;&#25361;&#25112;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#33258;&#21161;&#22521;&#35757;&#20307;&#21046;&#65292;&#21033;&#29992;&#29420;&#31435;&#23376;&#32593;&#32476;&#30340;&#38598;&#25104;&#65292;&#36741;&#20197;&#19968;&#20010;&#26088;&#22312;&#40723;&#21169;&#22810;&#26679;&#24615;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#39640;&#22810;&#26679;&#24615;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#23376;&#27169;&#22411;&#38598;&#25104;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#33391;&#22909;&#26657;&#20934;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembling a neural network is a widely recognized approach to enhance model performance, estimate uncertainty, and improve robustness in deep supervised learning. However, deep ensembles often come with high computational costs and memory demands. In addition, the efficiency of a deep ensemble is related to diversity among the ensemble members which is challenging for large, over-parameterized deep neural networks. Moreover, ensemble learning has not yet seen such widespread adoption, and it remains a challenging endeavor for self-supervised or unsupervised representation learning. Motivated by these challenges, we present a novel self-supervised training regime that leverages an ensemble of independent sub-networks, complemented by a new loss function designed to encourage diversity. Our method efficiently builds a sub-model ensemble with high diversity, leading to well-calibrated estimates of model uncertainty, all achieved with minimal computational overhead compared to traditional
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#33258;&#36866;&#24212;&#32852;&#37030;&#20803;&#23398;&#20064;&#65288;FAM&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#20010;&#21035;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;&#21644;&#38544;&#31169;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#39046;&#22495;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2308.13970</link><description>&lt;p&gt;
FAM&#65306;&#24555;&#36895;&#33258;&#36866;&#24212;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FAM: fast adaptive meta-learning. (arXiv:2308.13970v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#33258;&#36866;&#24212;&#32852;&#37030;&#20803;&#23398;&#20064;&#65288;FAM&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#20010;&#21035;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#35299;&#20915;&#20102;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;&#21644;&#38544;&#31169;&#38480;&#21046;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#38656;&#35201;&#22312;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#39046;&#22495;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24555;&#36895;&#33258;&#36866;&#24212;&#32852;&#37030;&#20803;&#23398;&#20064;&#65288;FAM&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#20840;&#23616;&#27169;&#22411;&#65292;&#28982;&#21518;&#21487;&#20197;&#22312;&#20010;&#21035;&#23458;&#25143;&#31471;&#19978;&#20010;&#24615;&#21270;&#12290;&#32852;&#37030;&#23398;&#20064;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#33021;&#22815;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#25968;&#25454;&#12290;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#23458;&#25143;&#31471;&#30001;&#20110;&#25968;&#25454;&#19981;&#36275;&#25110;&#25968;&#25454;&#22810;&#26679;&#24615;&#23548;&#33268;&#23398;&#20064;&#21463;&#21040;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#21457;&#25955;&#26102;&#65292;&#23398;&#20064;&#20250;&#21463;&#21040;&#22256;&#25200;&#12290;&#26377;&#24517;&#35201;&#23398;&#20064;&#19968;&#20010;&#21487;&#20197;&#20351;&#29992;&#23458;&#25143;&#31471;&#29305;&#23450;&#20449;&#24687;&#36827;&#34892;&#33258;&#36866;&#24212;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#22312;&#23458;&#25143;&#31471;&#19978;&#21019;&#24314;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;MRI&#25968;&#25454;&#23384;&#22312;&#36825;&#20010;&#38382;&#39064;&#65292;&#31532;&#19968;&#65292;&#30001;&#20110;&#25968;&#25454;&#37319;&#38598;&#25361;&#25112;&#65292;&#22312;&#26576;&#20010;&#22320;&#28857;&#30340;&#26412;&#22320;&#25968;&#25454;&#36275;&#20197;&#35757;&#32451;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#31532;&#20108;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#26377;&#25968;&#25454;&#20849;&#20139;&#38480;&#21046;&#65292;&#31532;&#19977;&#65292;&#30001;&#20110;&#23458;&#25143;&#31471;&#31449;&#28857;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#21464;&#65292;&#38656;&#35201;&#23545;&#23398;&#20064;&#30340;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a fast adaptive federated meta-learning (FAM) framework for collaboratively learning a single global model, which can then be personalized locally on individual clients. Federated learning enables multiple clients to collaborate to train a model without sharing data. Clients with insufficient data or data diversity participate in federated learning to learn a model with superior performance. Nonetheless, learning suffers when data distributions diverge. There is a need to learn a global model that can be adapted using client's specific information to create personalised models on clients is required. MRI data suffers from this problem, wherein, one, due to data acquisition challenges, local data at a site is sufficient for training an accurate model and two, there is a restriction of data sharing due to privacy concerns and three, there is a need for personalization of a learnt shared global model on account of domain shift across client sites. The global model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13570</link><description>&lt;p&gt;
&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38543;&#26426;&#37197;&#32622;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#65288;IAI&#65289;&#20013;&#65292;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#24314;&#27169;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24378;&#22823;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20197;&#24378;&#35843;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#21644;&#26377;&#20215;&#20540;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;&#19982;&#20855;&#26377;&#20108;&#20540;&#21270;&#23454;&#29616;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30456;&#27604;&#65292;SCMs&#30340;&#27169;&#22411;&#23384;&#20648;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#38500;&#20102;SCM&#23398;&#20064;&#22120;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20316;&#20026;&#26412;&#25991;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;SCMs&#30340;&#23398;&#20064;&#33021;&#21147;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23454;&#39564;&#30740;&#31350;&#20063;&#36827;&#34892;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;(ILP)&#26694;&#26550;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20174;&#23454;&#39564;&#20013;&#23398;&#20064;&#26032;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#21644;&#25351;&#23548;&#23454;&#39564;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2308.12740</link><description>&lt;p&gt;
&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Human Comprehensible Active Learning of Genome-Scale Metabolic Networks. (arXiv:2308.12740v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#32593;&#32476;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;(ILP)&#26694;&#26550;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#20174;&#23454;&#39564;&#20013;&#23398;&#20064;&#26032;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#20197;&#26377;&#25928;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#21644;&#25351;&#23548;&#23454;&#39564;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29983;&#29289;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#26159;&#23558;&#23487;&#20027;&#32454;&#32990;&#31995;&#32479;&#24037;&#31243;&#21270;&#20197;&#20135;&#29983;&#26377;&#29992;&#30340;&#20135;&#21697;&#12290;&#28982;&#32780;&#65292;&#23487;&#20027;&#31995;&#32479;&#35268;&#27169;&#30340;&#22686;&#21152;&#23548;&#33268;&#35774;&#35745;&#31354;&#38388;&#24040;&#22823;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#39640;&#26114;&#30340;&#39564;&#35777;&#35797;&#39564;&#12290;&#20026;&#20102;&#23487;&#20027;&#32454;&#32990;&#31995;&#32479;&#30340;&#35774;&#35745;-&#26500;&#24314;-&#27979;&#35797;-&#23398;&#20064;&#65288;Design-Build-Test-Learn&#65292;DBTL&#65289;&#21608;&#26399;&#65292;&#36843;&#20999;&#38656;&#35201;&#19968;&#31181;&#33021;&#26377;&#25928;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#24182;&#25351;&#23548;&#23454;&#39564;&#35774;&#35745;&#30340;&#21487;&#29702;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;ILP-iML1515&#65292;&#23427;&#36890;&#36807;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#21644;&#20174;&#35757;&#32451;&#23454;&#20363;&#20013;&#31215;&#26497;&#23398;&#20064;&#26469;&#25191;&#34892;&#35828;&#26126;&#24615;&#30340;&#36923;&#36753;&#25512;&#29702;&#12290;&#19982;&#25968;&#20540;&#27169;&#22411;&#19981;&#21516;&#65292;ILP-iML1515&#24314;&#31435;&#22312;&#23545;&#22522;&#22240;&#32452;&#35268;&#27169;&#20195;&#35874;&#27169;&#22411;&#30340;&#21487;&#29702;&#35299;&#30340;&#36923;&#36753;&#34920;&#31034;&#19978;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#20174;&#32570;&#20047;&#33829;&#20859;&#30340;&#31361;&#21464;&#20307;&#35797;&#39564;&#20013;&#23398;&#20064;&#26032;&#30340;&#36923;&#36753;&#32467;&#26500;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;ILP-iML1515&#26694;&#26550;&#20855;&#26377;&#39640;&#36890;&#37327;&#27169;&#25311;&#33021;&#21147;&#65292;&#24182;&#33021;&#20027;&#21160;&#36873;&#25321;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important application of Synthetic Biology is the engineering of the host cell system to yield useful products. However, an increase in the scale of the host system leads to huge design space and requires a large number of validation trials with high experimental costs. A comprehensible machine learning approach that efficiently explores the hypothesis space and guides experimental design is urgently needed for the Design-Build-Test-Learn (DBTL) cycle of the host cell system. We introduce a novel machine learning framework ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive logical reasoning and actively learns from training examples. In contrast to numerical models, ILP-iML1515 is built on comprehensible logical representations of a genome-scale metabolic model and can update the model by learning new logical structures from auxotrophic mutant trials. The ILP-iML1515 framework 1) allows high-throughput simulations and 2) actively selects experiments that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#32467;&#26500;&#31232;&#30095;&#24615;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#27905;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#20248;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#27169;&#22411;&#36827;&#34892;&#31232;&#30095;&#21270;&#65292;&#21487;&#20197;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12289;&#35745;&#31639;&#38656;&#27714;&#21644;&#39044;&#27979;&#26102;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20849;&#20139;&#23618;&#20013;&#20351;&#29992;&#36890;&#36947;&#32423;l1/l2&#32452;&#31232;&#30095;&#65292;&#21516;&#26102;&#28040;&#38500;&#22810;&#20313;&#30340;&#32452;&#65288;&#36890;&#36947;&#65289;&#24182;&#23545;&#26435;&#37325;&#26045;&#21152;&#24809;&#32602;&#65292;&#25552;&#39640;&#20102;&#25152;&#26377;&#20219;&#21153;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12114</link><description>&lt;p&gt;
&#31616;&#32422;&#22810;&#20219;&#21153;&#27169;&#22411;-&#20351;&#29992;&#32467;&#26500;&#31232;&#30095;&#24615;&#23454;&#29616;&#31616;&#27905;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Less is More -- Towards parsimonious multi-task models using structured sparsity. (arXiv:2308.12114v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#32467;&#26500;&#31232;&#30095;&#24615;&#24341;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#27905;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#20248;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#23545;&#27169;&#22411;&#36827;&#34892;&#31232;&#30095;&#21270;&#65292;&#21487;&#20197;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#12289;&#35745;&#31639;&#38656;&#27714;&#21644;&#39044;&#27979;&#26102;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20849;&#20139;&#23618;&#20013;&#20351;&#29992;&#36890;&#36947;&#32423;l1/l2&#32452;&#31232;&#30095;&#65292;&#21516;&#26102;&#28040;&#38500;&#22810;&#20313;&#30340;&#32452;&#65288;&#36890;&#36947;&#65289;&#24182;&#23545;&#26435;&#37325;&#26045;&#21152;&#24809;&#32602;&#65292;&#25552;&#39640;&#20102;&#25152;&#26377;&#20219;&#21153;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#32452;&#31232;&#30095;&#24615;&#40723;&#21169;&#26356;&#31616;&#21333;&#12289;&#26356;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#20855;&#26377;&#36739;&#23569;&#30340;&#27963;&#36291;&#21442;&#25968;&#32452;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#32467;&#26500;&#21270;&#32452;&#31232;&#30095;&#24615;&#32435;&#20837;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#30340;&#20849;&#20139;&#21442;&#25968;&#65292;&#24320;&#21457;&#31616;&#27905;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#65292;&#32780;&#20445;&#25345;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#27169;&#22411;&#36827;&#34892;&#31232;&#30095;&#21270;&#26377;&#21161;&#20110;&#20943;&#23569;&#27169;&#22411;&#30340;&#20869;&#23384;&#21344;&#29992;&#12289;&#35745;&#31639;&#38656;&#27714;&#21644;&#39044;&#27979;&#26102;&#38388;&#12290;&#25105;&#20204;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20849;&#20139;&#23618;&#20013;&#20351;&#29992;&#36890;&#36947;&#32423;l1/l2&#32452;&#31232;&#30095;&#12290;&#27492;&#26041;&#27861;&#19981;&#20165;&#26377;&#21161;&#20110;&#28040;&#38500;&#22810;&#20313;&#30340;&#32452;&#65288;&#36890;&#36947;&#65289;&#65292;&#36824;&#23545;&#26435;&#37325;&#26045;&#21152;&#24809;&#32602;&#65292;&#20174;&#32780;&#22686;&#24378;&#25152;&#26377;&#20219;&#21153;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#25968;&#25454;&#38598;NYU-v2&#21644;CelebAMask-HQ&#19978;&#27604;&#36739;&#20102;&#32452;&#31232;&#30095;&#24615;&#19979;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups. This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model. Sparsifying the model during training helps decrease the model's memory footprint, computation requirements, and prediction time during inference. We use channel-wise l1/l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN). This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks. We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ. We also i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27979;&#35797;&#26102;&#38388;&#23884;&#20837;&#24402;&#19968;&#21270;&#8221;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28909;&#38376;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24402;&#19968;&#21270;&#30340;&#29289;&#21697;&#23884;&#20837;&#26469;&#25511;&#21046;&#23884;&#20837;&#22823;&#23567;&#65292;&#24182;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#30340;&#35282;&#24230;&#30456;&#20284;&#24230;&#21306;&#20998;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#65292;&#20174;&#32780;&#26377;&#25928;&#20943;&#23569;&#20102;&#28909;&#38376;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.11288</link><description>&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#23884;&#20837;&#24402;&#19968;&#21270;&#23545;&#28909;&#38376;&#20559;&#35265;&#30340;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Test Time Embedding Normalization for Popularity Bias Mitigation. (arXiv:2308.11288v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27979;&#35797;&#26102;&#38388;&#23884;&#20837;&#24402;&#19968;&#21270;&#8221;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#28909;&#38376;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#24402;&#19968;&#21270;&#30340;&#29289;&#21697;&#23884;&#20837;&#26469;&#25511;&#21046;&#23884;&#20837;&#22823;&#23567;&#65292;&#24182;&#36890;&#36807;&#19982;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#30340;&#35282;&#24230;&#30456;&#20284;&#24230;&#21306;&#20998;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#65292;&#20174;&#32780;&#26377;&#25928;&#20943;&#23569;&#20102;&#28909;&#38376;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28909;&#38376;&#20559;&#35265;&#26159;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#28909;&#38376;&#29289;&#21697;&#20542;&#21521;&#20110;&#20027;&#23548;&#25512;&#33616;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#27979;&#35797;&#26102;&#38388;&#23884;&#20837;&#24402;&#19968;&#21270;&#8221;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#32531;&#35299;&#28909;&#38376;&#20559;&#35265;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#20102;&#20197;&#24448;&#30340;&#32531;&#35299;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#38454;&#27573;&#21033;&#29992;&#24402;&#19968;&#21270;&#30340;&#29289;&#21697;&#23884;&#20837;&#26469;&#25511;&#21046;&#23884;&#20837;&#30340;&#22823;&#23567;&#65292;&#32780;&#23884;&#20837;&#30340;&#22823;&#23567;&#19982;&#29289;&#21697;&#30340;&#27969;&#34892;&#24230;&#39640;&#24230;&#30456;&#20851;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#37319;&#26679;softmax&#25439;&#22833;&#30456;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#28909;&#38376;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#29992;&#25143;&#21644;&#29289;&#21697;&#23884;&#20837;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#20043;&#38388;&#30340;&#35282;&#24230;&#30456;&#20284;&#24230;&#21487;&#20197;&#21306;&#20998;&#21463;&#27426;&#36814;&#21644;&#19981;&#21463;&#27426;&#36814;&#30340;&#29289;&#21697;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#27969;&#34892;&#31243;&#24230;&#12290;&#36825;&#19968;&#20998;&#26512;&#35299;&#37322;&#20102;&#25105;&#20204;&#26041;&#27861;&#25104;&#21151;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Popularity bias is a widespread problem in the field of recommender systems, where popular items tend to dominate recommendation results. In this work, we propose 'Test Time Embedding Normalization' as a simple yet effective strategy for mitigating popularity bias, which surpasses the performance of the previous mitigation approaches by a significant margin. Our approach utilizes the normalized item embedding during the inference stage to control the influence of embedding magnitude, which is highly correlated with item popularity. Through extensive experiments, we show that our method combined with the sampled softmax loss effectively reduces popularity bias compare to previous approaches for bias mitigation. We further investigate the relationship between user and item embeddings and find that the angular similarity between embeddings distinguishes preferable and non-preferable items regardless of their popularity. The analysis explains the mechanism behind the success of our approac
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#21512;&#23454;&#35777;&#35780;&#20272;&#20102;&#35299;&#20915;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#26041;&#27861;&#23384;&#22312;&#31283;&#23450;&#24615;&#21644;&#27424;&#25311;&#21512;&#38382;&#39064;&#65292;&#20294;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#35757;&#32451;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2308.10328</link><description>&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#30340;&#32508;&#21512;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Empirical Evaluation on Online Continual Learning. (arXiv:2308.10328v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10328
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#21512;&#23454;&#35777;&#35780;&#20272;&#20102;&#35299;&#20915;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#26041;&#27861;&#23384;&#22312;&#31283;&#23450;&#24615;&#21644;&#27424;&#25311;&#21512;&#38382;&#39064;&#65292;&#20294;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#35757;&#32451;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#22312;&#25968;&#25454;&#27969;&#19978;&#30452;&#25509;&#23398;&#20064;&#65292;&#22788;&#29702;&#26102;&#38388;&#21464;&#21270;&#30340;&#20998;&#24067;&#65292;&#24182;&#20165;&#23384;&#20648;&#19968;&#23567;&#37096;&#20998;&#25968;&#25454;&#65292;&#20197;&#26356;&#25509;&#36817;&#23454;&#26102;&#23398;&#20064;&#20307;&#39564;&#12290;&#22312;&#36825;&#20010;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25991;&#29486;&#20013;&#35299;&#20915;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#30340;&#31867;&#22686;&#37327;&#35774;&#32622;&#19979;&#65292;&#20174;&#25968;&#25454;&#27969;&#20013;&#36880;&#27493;&#23398;&#20064;&#26032;&#30340;&#31867;&#21035;&#12290;&#25105;&#20204;&#22312;Split-CIFAR100&#21644;Split-TinyImagenet&#22522;&#20934;&#19978;&#27604;&#36739;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#12289;&#36951;&#24536;&#29575;&#12289;&#31283;&#23450;&#24615;&#21644;&#34920;&#31034;&#36136;&#37327;&#65292;&#20197;&#35780;&#20272;&#31639;&#27861;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#23384;&#22312;&#31283;&#23450;&#24615;&#21644;&#27424;&#25311;&#21512;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#30456;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#19979;&#65292;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#35757;&#32451;&#30456;&#24403;&#12290;&#27809;&#26377;&#26126;&#30830;&#30340;&#20248;&#32988;&#32773;&#20174;&#37325;&#26032;&#35780;&#20272;&#21518;&#30340;&#32467;&#26524;&#20013;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online continual learning aims to get closer to a live learning experience by learning directly on a stream of data with temporally shifting distribution and by storing a minimum amount of data from that stream. In this empirical evaluation, we evaluate various methods from the literature that tackle online continual learning. More specifically, we focus on the class-incremental setting in the context of image classification, where the learner must learn new classes incrementally from a stream of data. We compare these methods on the Split-CIFAR100 and Split-TinyImagenet benchmarks, and measure their average accuracy, forgetting, stability, and quality of the representations, to evaluate various aspects of the algorithm at the end but also during the whole training period. We find that most methods suffer from stability and underfitting issues. However, the learned representations are comparable to i.i.d. training under the same computational budget. No clear winner emerges from the re
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.10248</link><description>&lt;p&gt;
&#28608;&#27963;&#28155;&#21152;: &#26080;&#38656;&#20248;&#21270;&#21363;&#21487;&#25805;&#32437;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#25913;&#21464;&#28608;&#27963;&#26469;&#39044;&#27979;&#24615;&#22320;&#25913;&#21464;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#25104;&#26412;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#22320;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#26377;&#30417;&#30563;&#24494;&#35843;&#12289;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12289;&#25552;&#31034;&#24037;&#31243;&#21644;&#24341;&#23548;&#35299;&#30721;&#12290;&#25105;&#20204;&#30456;&#21453;&#65292;&#30740;&#31350;&#20102;&#28608;&#27963;&#24037;&#31243;&#65306;&#22312;&#25512;&#29702;&#26102;&#20462;&#25913;&#28608;&#27963;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;&#27169;&#22411;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#38544;&#24335;&#25351;&#23450;&#20102;&#19968;&#20010;&#28155;&#21152;&#30340;&#8220;&#23548;&#21521;&#21521;&#37327;&#8221;&#26469;&#20559;&#32622;&#21069;&#21521;&#20256;&#25773;&#12290;&#19982;&#20197;&#21069;&#23398;&#20064;&#36825;&#20123;&#23548;&#21521;&#21521;&#37327;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#28608;&#27963;&#28155;&#21152;&#65288;ActAdd&#65289;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#26469;&#33258;&#25552;&#31034;&#23545;&#30340;&#28608;&#27963;&#24046;&#24322;&#26469;&#35745;&#31639;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;OpenWebText&#21644;ConceptNet&#19978;&#23637;&#31034;&#20102;ActAdd&#22312;GPT-2&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#25512;&#29702;&#26102;&#26041;&#27861;&#25511;&#21046;&#20102;&#36755;&#20986;&#30340;&#39640;&#32423;&#23646;&#24615;&#24182;&#20445;&#25345;&#20102;&#38750;&#30446;&#26631;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23427;&#25152;&#38656;&#30340;&#35745;&#31639;&#21644;&#23454;&#26045;&#24037;&#20316;&#27604;&#24494;&#35843;&#35201;&#23569;&#24471;&#22810;&#65292;&#20801;&#35768;&#29992;&#25143;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#30340;&#35268;&#33539;&#65292;&#24182;&#19988;&#20854;&#24320;&#38144;&#19982;&#27169;&#22411;&#35268;&#27169;&#33258;&#28982;&#22320;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.  Unlike past work which learned these steering vectors, our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with m
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21644;&#32422;&#26463;&#29627;&#23572;&#20857;&#26364;&#26426;&#20316;&#20026;&#35797;&#39564;&#27874;&#20989;&#25968;&#21442;&#25968;&#21270;&#65292;&#30740;&#31350;&#20102;&#20855;&#26377;&#20195;&#25968;&#34928;&#20943;&#38271;&#31243;&#21453;&#38081;&#30913;&#30456;&#20114;&#20316;&#29992;&#30340;&#27178;&#21521;&#22330;&#20234;&#36763;&#38142;&#20013;&#30340;&#37327;&#23376;&#30456;&#21464;&#12290;&#36890;&#36807;&#26377;&#38480;&#22823;&#23567;&#32553;&#25918;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20013;&#24515;&#33655;&#30340;&#20540;&#19982;&#34928;&#21464;&#25351;&#25968;$\alpha_\mathrm{LR}$&#19981;&#21516;&#65292;&#32780;&#20020;&#30028;&#25351;&#25968;&#21017;&#20445;&#25345;&#25509;&#36817;&#30701;&#31243;&#20234;&#36763;&#27169;&#22411;&#30340;&#20540;&#65292;&#36825;&#25903;&#25345;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#20849;&#24418;&#19981;&#21464;&#24615;&#30772;&#32570;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2308.09709</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#30740;&#31350;&#38271;&#31243;&#21453;&#38081;&#30913;&#20234;&#36763;&#38142;
&lt;/p&gt;
&lt;p&gt;
Neural-network quantum state study of the long-range antiferromagnetic Ising chain. (arXiv:2308.09709v2 [cond-mat.stat-mech] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09709
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21644;&#32422;&#26463;&#29627;&#23572;&#20857;&#26364;&#26426;&#20316;&#20026;&#35797;&#39564;&#27874;&#20989;&#25968;&#21442;&#25968;&#21270;&#65292;&#30740;&#31350;&#20102;&#20855;&#26377;&#20195;&#25968;&#34928;&#20943;&#38271;&#31243;&#21453;&#38081;&#30913;&#30456;&#20114;&#20316;&#29992;&#30340;&#27178;&#21521;&#22330;&#20234;&#36763;&#38142;&#20013;&#30340;&#37327;&#23376;&#30456;&#21464;&#12290;&#36890;&#36807;&#26377;&#38480;&#22823;&#23567;&#32553;&#25918;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20013;&#24515;&#33655;&#30340;&#20540;&#19982;&#34928;&#21464;&#25351;&#25968;$\alpha_\mathrm{LR}$&#19981;&#21516;&#65292;&#32780;&#20020;&#30028;&#25351;&#25968;&#21017;&#20445;&#25345;&#25509;&#36817;&#30701;&#31243;&#20234;&#36763;&#27169;&#22411;&#30340;&#20540;&#65292;&#36825;&#25903;&#25345;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#20849;&#24418;&#19981;&#21464;&#24615;&#30772;&#32570;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#30740;&#31350;&#20855;&#26377;&#20195;&#25968;&#34928;&#20943;&#38271;&#31243;&#21453;&#38081;&#30913;&#30456;&#20114;&#20316;&#29992;&#30340;&#27178;&#21521;&#22330;&#20234;&#36763;&#38142;&#20013;&#30340;&#37327;&#23376;&#30456;&#21464;&#65292;&#37319;&#29992;&#32422;&#26463;&#29627;&#23572;&#20857;&#26364;&#26426;&#20316;&#20026;&#35797;&#39564;&#27874;&#20989;&#25968;&#21442;&#25968;&#21270;&#12290;&#22312;&#26377;&#38480;&#22823;&#23567;&#32553;&#25918;&#20998;&#26512;&#20013;&#65292;&#36890;&#36807;&#24207;&#21442;&#25968;&#21644;&#31532;&#20108;&#20010;Renyi&#29109;&#65292;&#25105;&#20204;&#21457;&#29616;&#20013;&#24515;&#33655;&#19982;&#23567;&#30340;&#34928;&#21464;&#25351;&#25968;$\alpha_\mathrm{LR}$&#19981;&#21516;&#65292;&#20559;&#31163;&#20102;1/2&#65292;&#32780;&#20020;&#30028;&#25351;&#25968;&#21017;&#26080;&#35770;&#21463;&#21040;$\alpha_\mathrm{LR}$&#30340;&#24433;&#21709;&#22914;&#20309;&#65292;&#37117;&#20445;&#25345;&#38750;&#24120;&#25509;&#36817;&#30701;&#31243;&#65288;SR&#65289;&#20234;&#36763;&#27169;&#22411;&#30340;&#20540;&#65292;&#25903;&#25345;&#20808;&#21069;&#25552;&#20986;&#30340;&#20849;&#24418;&#19981;&#21464;&#24615;&#30772;&#32570;&#22330;&#26223;&#12290;&#20026;&#20102;&#30830;&#23450;&#20234;&#36763;&#23431;&#23449;&#24615;&#21644;&#20849;&#24418;&#23545;&#31216;&#24615;&#30340;&#20020;&#30028;&#28857;&#65292;&#25105;&#20204;&#23545;&#26222;&#36866;Binder&#27604;&#21644;&#30456;&#20851;&#20989;&#25968;&#30340;&#20849;&#24418;&#22330;&#35770;&#65288;CFT&#65289;&#25551;&#36848;&#36827;&#34892;&#20102;&#20004;&#39033;&#38468;&#21152;&#27979;&#35797;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22312;$\alpha_\mat
&lt;/p&gt;
&lt;p&gt;
We investigate quantum phase transitions in the transverse field Ising chain with algebraically decaying long-range antiferromagnetic interactions by using the variational Monte Carlo method with the restricted Boltzmann machine being employed as a trial wave function ansatz. In the finite-size scaling analysis with the order parameter and the second R\'enyi entropy, we find that the central charge deviates from 1/2 at a small decay exponent $\alpha_\mathrm{LR}$ in contrast to the critical exponents staying very close to the short-range (SR) Ising values regardless of $\alpha_\mathrm{LR}$ examined, supporting the previously proposed scenario of conformal invariance breakdown. To identify the threshold of the Ising universality and the conformal symmetry, we perform two additional tests for the universal Binder ratio and the conformal field theory (CFT) description of the correlation function. It turns out that both indicate a noticeable deviation from the SR Ising class at $\alpha_\mat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35786;&#26029;&#30340;&#27010;&#24565;&#19982;&#28145;&#24230;&#22270;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#31995;&#32479;&#30340;&#24213;&#23618;&#32467;&#26500;&#24182;&#25552;&#20379;&#21160;&#24577;&#35266;&#23519;&#12290;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#31995;&#32479;&#34920;&#31034;&#12289;&#35266;&#23519;&#21644;&#25925;&#38556;&#30340;&#26500;&#24314;&#65292;&#24341;&#20837;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#22312;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#35786;&#26029;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.06961</link><description>&lt;p&gt;
&#22270;&#32467;&#26500;&#27531;&#24046;&#65306;&#19968;&#31181;&#35786;&#26029;&#30340;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Structural Residuals: A Learning Approach to Diagnosis. (arXiv:2308.06961v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35786;&#26029;&#30340;&#27010;&#24565;&#19982;&#28145;&#24230;&#22270;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#25968;&#25454;&#23398;&#20064;&#31995;&#32479;&#30340;&#24213;&#23618;&#32467;&#26500;&#24182;&#25552;&#20379;&#21160;&#24577;&#35266;&#23519;&#12290;&#30740;&#31350;&#36890;&#36807;&#37325;&#26032;&#23450;&#20041;&#31995;&#32479;&#34920;&#31034;&#12289;&#35266;&#23519;&#21644;&#25925;&#38556;&#30340;&#26500;&#24314;&#65292;&#24341;&#20837;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#22312;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#19978;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#35786;&#26029;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35786;&#26029;&#20381;&#36182;&#20110;&#26500;&#24314;&#26126;&#30830;&#30340;&#31995;&#32479;&#27169;&#22411;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#36153;&#26102;&#19988;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#27169;&#22411;&#35786;&#26029;&#30340;&#27010;&#24565;&#19982;&#28145;&#24230;&#22270;&#32467;&#26500;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#21033;&#29992;&#25968;&#25454;&#23398;&#20064;&#31995;&#32479;&#30340;&#24213;&#23618;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#30001;&#20004;&#20010;&#19981;&#21516;&#30340;&#22270;&#37051;&#25509;&#30697;&#38453;&#34920;&#31034;&#30340;&#21160;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#23454;&#29616;&#20102;&#22270;&#32467;&#26500;&#23398;&#20064;&#19982;&#27169;&#22411;&#35786;&#26029;&#30340;&#26080;&#32541;&#38598;&#25104;&#65306;(i)&#37325;&#26032;&#23450;&#20041;&#31995;&#32479;&#34920;&#31034;&#12289;&#35266;&#23519;&#21644;&#25925;&#38556;&#30340;&#26500;&#24314;&#12289;(ii)&#24341;&#20837;&#20004;&#31181;&#19981;&#21516;&#29256;&#26412;&#30340;&#33258;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;&#12289;(iii)&#36890;&#36807;&#32806;&#21512;&#25391;&#33633;&#22120;&#31995;&#32479;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#25968;&#25454;&#39537;&#21160;&#30340;&#35786;&#26029;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.
&lt;/p&gt;</description></item><item><title>Tango&#26159;&#19968;&#20010;&#37325;&#26032;&#24605;&#32771;&#22312;GPU&#19978;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#37327;&#21270;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35268;&#21017;&#26469;&#20445;&#25345;&#20934;&#30830;&#24230;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#37327;&#21270;&#24863;&#30693;&#30340;&#22522;&#26412;&#25805;&#20316;&#21644;&#22522;&#26412;&#25805;&#20316;&#20043;&#38388;&#30340;&#20248;&#21270;&#65292;&#20197;&#21152;&#36895;GNN&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2308.00890</link><description>&lt;p&gt;
Tango: &#37325;&#26032;&#24605;&#32771;&#22312;GPU&#19978;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tango: rethinking quantization for graph neural network training on GPUs. (arXiv:2308.00890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00890
&lt;/p&gt;
&lt;p&gt;
Tango&#26159;&#19968;&#20010;&#37325;&#26032;&#24605;&#32771;&#22312;GPU&#19978;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#37327;&#21270;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35268;&#21017;&#26469;&#20445;&#25345;&#20934;&#30830;&#24230;&#65292;&#35774;&#35745;&#24182;&#23454;&#29616;&#37327;&#21270;&#24863;&#30693;&#30340;&#22522;&#26412;&#25805;&#20316;&#21644;&#22522;&#26412;&#25805;&#20316;&#20043;&#38388;&#30340;&#20248;&#21270;&#65292;&#20197;&#21152;&#36895;GNN&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22240;&#20854;&#22312;&#20851;&#38190;&#30340;&#22270;&#30456;&#20851;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#37327;&#21270;&#34987;&#24191;&#27867;&#29992;&#20110;&#21152;&#36895;GNN&#35745;&#31639;&#65292;&#20294;&#37327;&#21270;&#35757;&#32451;&#38754;&#20020;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#37327;&#21270;GNN&#35757;&#32451;&#31995;&#32479;&#24448;&#24448;&#27604;&#20854;&#20840;&#31934;&#24230;&#23545;&#24212;&#29289;&#26377;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#21407;&#22240;&#26377;&#20108;&#65306;&#65288;&#19968;&#65289;&#35299;&#20915;&#20934;&#30830;&#24230;&#38382;&#39064;&#23548;&#33268;&#20102;&#36807;&#22810;&#30340;&#24320;&#38144;&#65292;&#65288;&#20108;&#65289;&#27809;&#26377;&#20805;&#20998;&#21457;&#25381;&#37327;&#21270;&#25152;&#23637;&#31034;&#30340;&#20248;&#21270;&#28508;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Tango&#65292;&#23427;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;GPU&#19978;&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#37327;&#21270;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26377;&#25928;&#30340;&#35268;&#21017;&#26469;&#22312;&#37327;&#21270;GNN&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#20934;&#30830;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23454;&#29616;&#20102;&#37327;&#21270;&#24863;&#30693;&#30340;&#22522;&#26412;&#25805;&#20316;&#21644;&#22522;&#26412;&#25805;&#20316;&#20043;&#38388;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#21152;&#36895;GNN&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;Tango&#19982;&#27969;&#34892;&#30340;Deep Graph Library&#65288;DGL&#65289;&#31995;&#32479;&#38598;&#25104;&#65292;&#24182;&#23637;&#31034;&#20854;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are becoming increasingly popular due to their superior performance in critical graph-related tasks. While quantization is widely used to accelerate GNN computation, quantized training faces unprecedented challenges. Current quantized GNN training systems often have longer training times than their full-precision counterparts for two reasons: (i) addressing the accuracy challenge leads to excessive overhead, and (ii) the optimization potential exposed by quantization is not adequately leveraged. This paper introduces Tango which re-thinks quantization challenges and opportunities for graph neural network training on GPUs with three contributions: Firstly, we introduce efficient rules to maintain accuracy during quantized GNN training. Secondly, we design and implement quantization-aware primitives and inter-primitive optimizations that can speed up GNN training. Finally, we integrate Tango with the popular Deep Graph Library (DGL) system and demonstrate its
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#29702;&#26680;&#30340;&#22797;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22797;&#20540;&#20989;&#25968;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#24182;&#32467;&#21512;&#20302;&#38454;&#26377;&#29702;&#20989;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#25554;&#20540;&#65292;&#35299;&#20915;&#20102;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25311;&#21512;&#36807;&#31243;&#20013;&#26631;&#20934;&#26680;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.13484</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#29702;&#26680;&#30340;&#22797;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Rational kernel-based interpolation for complex-valued frequency response functions. (arXiv:2307.13484v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#29702;&#26680;&#30340;&#22797;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25554;&#20540;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#22797;&#20540;&#20989;&#25968;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#24182;&#32467;&#21512;&#20302;&#38454;&#26377;&#29702;&#20989;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#25554;&#20540;&#65292;&#35299;&#20915;&#20102;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#25311;&#21512;&#36807;&#31243;&#20013;&#26631;&#20934;&#26680;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26680;&#30340;&#36924;&#36817;&#26041;&#27861;&#22312;&#22797;&#20540;&#20989;&#25968;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#39057;&#22495;&#20013;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#39057;&#29575;&#21709;&#24212;&#20989;&#25968;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#26680;&#26041;&#27861;&#36234;&#26469;&#36234;&#24120;&#29992;&#65292;&#28982;&#32780;&#26631;&#20934;&#30340;&#26680;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#22312;&#22797;&#20540;&#24773;&#20917;&#19979;&#65292;&#24213;&#23618;&#26680;&#23545;&#30340;&#25968;&#23398;&#21547;&#20041;&#21644;&#25968;&#23398;&#25512;&#23548;&#23578;&#24453;&#35299;&#20915;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22797;&#20540;&#20989;&#25968;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65292;&#24182;&#23558;&#24102;&#26377;&#26680;&#23545;&#30340;&#22797;&#20540;&#25554;&#20540;&#38382;&#39064;&#36716;&#21270;&#20026;&#36825;&#20123;&#31354;&#38388;&#20013;&#30340;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25554;&#20540;&#22120;&#19982;&#20302;&#38454;&#26377;&#29702;&#20989;&#25968;&#32467;&#21512;&#65292;&#20854;&#20013;&#38454;&#25968;&#26681;&#25454;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#33258;&#36866;&#24212;&#36873;&#25321;&#12290;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#21253;&#25324;&#30005;&#30913;&#23398;&#21644;&#22768;&#23398;&#65289;&#30340;&#20363;&#23376;&#30340;&#25968;&#20540;&#32467;&#26524;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is concerned with the kernel-based approximation of a complex-valued function from data, where the frequency response function of a partial differential equation in the frequency domain is of particular interest. In this setting, kernel methods are employed more and more frequently, however, standard kernels do not perform well. Moreover, the role and mathematical implications of the underlying pair of kernels, which arises naturally in the complex-valued case, remain to be addressed. We introduce new reproducing kernel Hilbert spaces of complex-valued functions, and formulate the problem of complex-valued interpolation with a kernel pair as minimum norm interpolation in these spaces. Moreover, we combine the interpolant with a low-order rational function, where the order is adaptively selected based on a new model selection criterion. Numerical results on examples from different fields, including electromagnetics and acoustic examples, illustrate the performance of the metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#24863;&#30693;&#23545;&#40784;&#26799;&#24230;&#65288;PAG&#65289;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#35270;&#35273;-&#35821;&#35328;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#23545; CLIP &#36827;&#34892;&#40065;&#26834;&#24615;&#35843;&#25972;&#65292;&#23637;&#31034;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#38598;&#25104; CLIPAG &#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26080;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.16805</link><description>&lt;p&gt;
CLIPAG: &#36208;&#21521;&#26080;&#38656;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CLIPAG: Towards Generator-Free Text-to-Image Generation. (arXiv:2306.16805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#24863;&#30693;&#23545;&#40784;&#26799;&#24230;&#65288;PAG&#65289;&#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#35270;&#35273;-&#35821;&#35328;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#23545; CLIP &#36827;&#34892;&#40065;&#26834;&#24615;&#35843;&#25972;&#65292;&#23637;&#31034;&#20102;&#22312;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#38598;&#25104; CLIPAG &#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26080;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#23545;&#40784;&#26799;&#24230; (Perceptually Aligned Gradients, PAG) &#26159;&#22312;&#20581;&#22766;&#30340;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#19968;&#31181;&#26377;&#36259;&#23646;&#24615;&#65292;&#20854;&#20013;&#23427;&#20204;&#30340;&#36755;&#20837;&#28176;&#21464;&#19982;&#20154;&#31867;&#24863;&#30693;&#23545;&#40784;&#24182;&#20855;&#26377;&#35821;&#20041;&#24847;&#20041;&#12290;&#34429;&#28982;&#36825;&#19968;&#29616;&#35937;&#24341;&#36215;&#20102;&#26174;&#30528;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#20294;&#20165;&#20165;&#22312;&#21333;&#27169;&#24577;&#32431;&#35270;&#35273;&#26550;&#26500;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558; PAG &#30340;&#30740;&#31350;&#25193;&#23637;&#21040;&#35270;&#35273;-&#35821;&#35328;&#26550;&#26500;&#65292;&#36825;&#26159;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20219;&#21153;&#21644;&#24212;&#29992;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#23545; CLIP &#36827;&#34892;&#23545;&#25239;&#24615;&#40065;&#26834;&#24494;&#35843;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#40065;&#26834;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20854;&#22522;&#20934;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102; PAG&#12290;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102; CLIPAG &#22312;&#20960;&#31181;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26080;&#32541;&#38598;&#25104; CLIPAG &#30340; "&#21363;&#25554;&#21363;&#29992;" &#26041;&#24335;&#26174;&#33879;&#25913;&#36827;&#20102;&#35270;&#35273;-&#35821;&#35328;&#29983;&#25104;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#20854; PAG &#23646;&#24615;&#65292;CLIPAG &#23454;&#29616;&#20102;&#26080;&#29983;&#25104;&#22120;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceptually Aligned Gradients (PAG) refer to an intriguing property observed in robust image classification models, wherein their input gradients align with human perception and pose semantic meanings. While this phenomenon has gained significant research attention, it was solely studied in the context of unimodal vision-only architectures. In this work, we extend the study of PAG to Vision-Language architectures, which form the foundations for diverse image-text tasks and applications. Through an adversarial robustification finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit PAG in contrast to their vanilla counterparts. This work reveals the merits of CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we show that seamlessly integrating CLIPAG in a "plug-n-play" manner leads to substantial improvements in vision-language generative applications. Furthermore, leveraging its PAG property, CLIPAG enables text-to-image generation witho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPbRL&#30340;&#26032;&#30340;&#20844;&#24179;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24191;&#20041;Gini&#31119;&#21033;&#20989;&#25968;&#26368;&#22823;&#21270;&#31574;&#30053;&#23398;&#20064;&#26469;&#23454;&#29616;&#22810;&#30446;&#26631;&#20248;&#21270;&#24182;&#22788;&#29702;&#27599;&#20010;&#30446;&#26631;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09995</link><description>&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness in Preference-based Reinforcement Learning. (arXiv:2306.09995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPbRL&#30340;&#26032;&#30340;&#20844;&#24179;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#24191;&#20041;Gini&#31119;&#21033;&#20989;&#25968;&#26368;&#22823;&#21270;&#31574;&#30053;&#23398;&#20064;&#26469;&#23454;&#29616;&#22810;&#30446;&#26631;&#20248;&#21270;&#24182;&#22788;&#29702;&#27599;&#20010;&#30446;&#26631;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#30446;&#26631;&#24773;&#20917;&#19979;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;(PbRL)&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#35774;&#35745;&#25511;&#21046;&#31574;&#30053;&#65292;&#26082;&#33021;&#22815;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#65292;&#21448;&#33021;&#22815;&#20844;&#24179;&#22320;&#22788;&#29702;&#27599;&#20010;&#30446;&#26631;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;(FPbRL)&#26041;&#27861;&#12290;FPbRL&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#26032;&#30340;&#31119;&#21033;&#20559;&#22909;&#32780;&#19981;&#26159;PbRL&#20013;&#30340;&#22522;&#20110;&#22870;&#21169;&#30340;&#20559;&#22909;&#26469;&#23398;&#20064;&#19982;&#22810;&#30446;&#26631;&#20851;&#32852;&#30340;&#21521;&#37327;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#24191;&#20041;Gini&#31119;&#21033;&#20989;&#25968;&#36827;&#34892;&#31574;&#30053;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#29615;&#22659;&#19978;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;FPbRL&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#21644;&#20844;&#24179;&#30340;&#25511;&#21046;&#31574;&#30053;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the issue of fairness in preference-based reinforcement learning (PbRL) in the presence of multiple objectives. The main objective is to design control policies that can optimize multiple objectives while treating each objective fairly. Toward this objective, we design a new fairness-induced preference-based reinforcement learning or FPbRL. The main idea of FPbRL is to learn vector reward functions associated with multiple objectives via new welfare-based preferences rather than reward-based preference in PbRL, coupled with policy learning via maximizing a generalized Gini welfare function. Finally, we provide experiment studies on three different environments to show that the proposed FPbRL approach can achieve both efficiency and equity for learning effective and fair policies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#21487;&#20197;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#25110;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.04866</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#20840;&#36523;&#25805;&#20316;&#30340;&#22240;&#26524;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Causal Policy Gradient for Whole-Body Mobile Manipulation. (arXiv:2305.04866v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#21487;&#20197;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#25110;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19979;&#19968;&#20195;&#23478;&#24237;&#26426;&#22120;&#20154;&#21161;&#25163;&#38656;&#35201;&#32467;&#21512;&#26426;&#21160;&#21644;&#20132;&#20114;&#33021;&#21147;&#65292;&#21363;&#36890;&#24120;&#25152;&#35828;&#30340;&#31227;&#21160;&#25805;&#20316;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#21644;&#20219;&#21153;&#24120;&#35265;&#30340;&#22810;&#30446;&#26631;&#24615;&#36136;&#65292;&#20363;&#22914;&#33021;&#22815;&#26377;&#25928;&#22320;&#36798;&#21040;&#30446;&#26631;&#19988;&#36991;&#20813;&#38556;&#30861;&#65292;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#24456;&#38590;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#26681;&#25454;&#20154;&#24037;&#21305;&#37197;&#21160;&#20316;&#31354;&#38388;&#30340;&#37096;&#20998;&#21040;&#31227;&#21160;&#25805;&#20316;&#23376;&#30446;&#26631;&#65288;&#20363;&#22914;&#29992;&#20110;&#31227;&#21160;&#30446;&#26631;&#30340;&#22522;&#30784;&#21160;&#20316;&#21644;&#29992;&#20110;&#25805;&#20316;&#30340;&#25163;&#33218;&#21160;&#20316;&#65289;&#23558;&#20219;&#21153;&#20998;&#20026;&#19981;&#24102;&#25805;&#20316;&#30340;&#23548;&#33322;&#21644;&#19981;&#24102;&#26426;&#21160;&#30340;&#22266;&#23450;&#25805;&#20316;&#12290;&#27492;&#35299;&#20915;&#26041;&#26696;&#38450;&#27490;&#20102;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#30340;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#24182;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#35813;&#26694;&#26550;&#29992;&#20110;&#35757;&#32451;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing the next generation of household robot helpers requires combining locomotion and interaction capabilities, which is generally referred to as mobile manipulation (MoMa). MoMa tasks are difficult due to the large action space of the robot and the common multi-objective nature of the task, e.g., efficiently reaching a goal while avoiding obstacles. Current approaches often segregate tasks into navigation without manipulation and stationary manipulation without locomotion by manually matching parts of the action space to MoMa sub-objectives (e.g. base actions for locomotion objectives and arm actions for manipulation). This solution prevents simultaneous combinations of locomotion and interaction degrees of freedom and requires human domain knowledge for both partitioning the action space and matching the action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a new framework to train policies for typical MoMa tasks that makes use of the most favorable subsp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#29992;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#23398;&#29983;&#22312;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;LightGBM&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.05257</link><description>&lt;p&gt;
&#22810;&#31890;&#24230;&#26102;&#38388;&#21464;&#25442;&#22120;&#29992;&#20110;&#30693;&#35782;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Multi-granulariy Time-based Transformer for Knowledge Tracing. (arXiv:2304.05257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#29992;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#23398;&#29983;&#22312;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#20351;&#29992;&#20102;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#19982;LightGBM&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20934;&#30830;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#26631;&#20934;&#21270;&#27979;&#35797;&#20013;&#23398;&#29983;&#30340;&#34920;&#29616;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#23398;&#29983;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#21253;&#25324;&#20182;&#20204;&#20197;&#24448;&#30340;&#32771;&#35797;&#25104;&#32489;&#12289;&#23398;&#20064;&#20064;&#24815;&#21644;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#65292;&#20026;&#27599;&#20010;&#23398;&#29983;&#21019;&#24314;&#19968;&#20010;&#20010;&#24615;&#21270;&#30340;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#39044;&#27979;&#23398;&#29983;&#22312;&#32473;&#23450;&#27979;&#35797;&#20013;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;RIIID&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#22810;&#20010;&#26102;&#38388;&#29305;&#24449;&#31890;&#24230;&#20316;&#20026;&#35299;&#30721;&#22120;&#36755;&#20837;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;LightGBM&#26041;&#27861;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;AI&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20280;&#32553;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#23398;&#29983;&#25104;&#26524;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a transformer architecture for predicting student performance on standardized tests. Specifically, we leverage students historical data, including their past test scores, study habits, and other relevant information, to create a personalized model for each student. We then use these models to predict their future performance on a given test. Applying this model to the RIIID dataset, we demonstrate that using multiple granularities for temporal features as the decoder input significantly improve model performance. Our results also show the effectiveness of our approach, with substantial improvements over the LightGBM method. Our work contributes to the growing field of AI in education, providing a scalable and accurate tool for predicting student outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27431;&#25289;&#29305;&#24449;&#25216;&#26415;&#22312;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#28857;&#36816;&#31639;&#27431;&#25289;&#29305;&#24449;&#24471;&#21040;&#27431;&#25289;&#29305;&#24449;&#36718;&#24275;&#65292;&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#27431;&#25289;&#36718;&#24275;&#21450;&#20854;&#28151;&#21512;&#21464;&#25442;&#25429;&#25417;&#20449;&#24687;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14040</link><description>&lt;p&gt;
&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#27431;&#25289;&#29305;&#24449;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Euler Characteristic Tools For Topological Data Analysis. (arXiv:2303.14040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27431;&#25289;&#29305;&#24449;&#25216;&#26415;&#22312;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#28857;&#36816;&#31639;&#27431;&#25289;&#29305;&#24449;&#24471;&#21040;&#27431;&#25289;&#29305;&#24449;&#36718;&#24275;&#65292;&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#27431;&#25289;&#36718;&#24275;&#21450;&#20854;&#28151;&#21512;&#21464;&#25442;&#25429;&#25417;&#20449;&#24687;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#27431;&#25289;&#29305;&#24449;&#25216;&#26415;&#12290;&#20174;&#25968;&#25454;&#26500;&#24314;&#30340;&#19968;&#26063;&#21333;&#32431;&#22797;&#21512;&#20307;&#30340;&#28857;&#36816;&#31639;&#27431;&#25289;&#29305;&#24449;&#65292;&#24471;&#21040;&#25152;&#35859;&#30340;&#27431;&#25289;&#29305;&#24449;&#36718;&#24275;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31616;&#21333;&#25551;&#36848;&#31526;&#20197;&#26497;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#22312;&#30417;&#30563;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#21463;&#20449;&#21495;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35745;&#31639;&#27431;&#25289;&#29305;&#24449;&#36718;&#24275;&#30340;&#28151;&#21512;&#21464;&#25442;&#12290;&#36825;&#20123;&#31215;&#20998;&#21464;&#25442;&#23558;&#27431;&#25289;&#29305;&#24449;&#25216;&#26415;&#19982;&#21202;&#36125;&#26684;&#31215;&#20998;&#28151;&#21512;&#65292;&#25552;&#20379;&#39640;&#25928;&#21387;&#32553;&#25299;&#25169;&#20449;&#21495;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#22312;&#23450;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27431;&#25289;&#36718;&#24275;&#21450;&#20854;&#28151;&#21512;&#21464;&#25442;&#25152;&#25429;&#25417;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#30340;&#20247;&#22810;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#25551;&#36848;&#31526;&#30340;&#31283;&#23450;&#24615;&#32467;&#26524;&#20197;&#21450;&#22312;&#38543;&#26426;&#35774;&#32622;&#19979;&#30340;&#28176;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we study Euler characteristic techniques in topological data analysis. Pointwise computing the Euler characteristic of a family of simplicial complexes built from data gives rise to the so-called Euler characteristic profile. We show that this simple descriptor achieve state-of-the-art performance in supervised tasks at a very low computational cost. Inspired by signal analysis, we compute hybrid transforms of Euler characteristic profiles. These integral transforms mix Euler characteristic techniques with Lebesgue integration to provide highly efficient compressors of topological signals. As a consequence, they show remarkable performances in unsupervised settings. On the qualitative side, we provide numerous heuristics on the topological and geometric information captured by Euler profiles and their hybrid transforms. Finally, we prove stability results for these descriptors as well as asymptotic guarantees in random settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#27169;&#22411;&#25340;&#25509;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#27604;&#36739;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#19981;&#21516;&#24418;&#29366;&#23618;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#23567;ResNets&#20013;&#20351;&#29992;&#22522;&#20110;&#21367;&#31215;&#30340;&#25340;&#25509;&#65292;&#22312;&#31532;&#19968;&#20010;&#32593;&#32476;&#20013;&#36739;&#26202;&#20986;&#29616;&#30340;&#23618;&#19982;&#31532;&#20108;&#20010;&#32593;&#32476;&#20013;&#23545;&#24212;&#23618;&#36317;&#31163;&#36739;&#36828;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.11277</link><description>&lt;p&gt;
&#27169;&#22411;&#25340;&#25509;&#65306;&#23547;&#25214;&#34920;&#31034;&#38388;&#30340;&#21151;&#33021;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Model Stitching: Looking For Functional Similarity Between Representations. (arXiv:2303.11277v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#27169;&#22411;&#25340;&#25509;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#27604;&#36739;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#19981;&#21516;&#24418;&#29366;&#23618;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#23567;ResNets&#20013;&#20351;&#29992;&#22522;&#20110;&#21367;&#31215;&#30340;&#25340;&#25509;&#65292;&#22312;&#31532;&#19968;&#20010;&#32593;&#32476;&#20013;&#36739;&#26202;&#20986;&#29616;&#30340;&#23618;&#19982;&#31532;&#20108;&#20010;&#32593;&#32476;&#20013;&#23545;&#24212;&#23618;&#36317;&#31163;&#36739;&#36828;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#25340;&#25509;&#26159;&#19968;&#31181;&#27604;&#36739;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#25105;&#20204;&#34913;&#37327;&#23427;&#20204;&#21487;&#20197;&#20114;&#25442;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;Bansal, Nakkiran &amp; Barak&#30340;&#20808;&#21069;&#24037;&#20316;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#35813;&#24037;&#20316;&#20351;&#29992;&#27169;&#22411;&#25340;&#25509;&#26469;&#27604;&#36739;&#19981;&#21516;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21040;&#30340;&#30456;&#21516;&#24418;&#29366;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#20351;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#19981;&#21516;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#19981;&#21516;&#24418;&#29366;&#23618;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#38543;&#21518;&#25581;&#31034;&#20102;&#27169;&#22411;&#25340;&#25509;&#30340;&#24847;&#22806;&#34892;&#20026;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#21367;&#31215;&#30340;&#25340;&#25509;&#23545;&#20110;&#23567;ResNets&#26469;&#35828;&#65292;&#22914;&#26524;&#36825;&#20123;&#23618;&#22312;&#31532;&#19968;&#20010;&#65288;&#21457;&#36865;&#32773;&#65289;&#32593;&#32476;&#20013;&#20986;&#29616;&#30340;&#20301;&#32622;&#26202;&#20110;&#31532;&#20108;&#20010;&#65288;&#25509;&#25910;&#32773;&#65289;&#32593;&#32476;&#20013;&#30340;&#20301;&#32622;&#65292;&#21363;&#20351;&#36825;&#20123;&#23618;&#30456;&#36317;&#24456;&#36828;&#65292;&#20063;&#21487;&#20197;&#36798;&#21040;&#24456;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model stitching (Lenc &amp; Vedaldi 2015) is a compelling methodology to compare different neural network representations, because it allows us to measure to what degree they may be interchanged. We expand on a previous work from Bansal, Nakkiran &amp; Barak which used model stitching to compare representations of the same shapes learned by differently seeded and/or trained neural networks of the same architecture. Our contribution enables us to compare the representations learned by layers with different shapes from neural networks with different architectures. We subsequently reveal unexpected behavior of model stitching. Namely, we find that stitching, based on convolutions, for small ResNets, can reach high accuracy if those layers come later in the first (sender) network than in the second (receiver), even if those layers are far apart.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#20010;&#39044;&#35757;&#32451;&#30340;&#22402;&#30452;&#20108;&#32500;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#19977;&#32500;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23558;&#19977;&#32500;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#20026;&#22312;&#19981;&#21516;&#26041;&#21521;&#19978;&#20999;&#29255;&#30340;&#20108;&#32500;&#20998;&#24067;&#30340;&#20056;&#31215;&#65292;&#35299;&#20915;&#20102;&#32500;&#25968;&#30340;&#28798;&#38590;&#24615;&#38382;&#39064;&#65292;&#24182;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.08440</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22402;&#30452;&#20108;&#32500;&#25193;&#25955;&#27169;&#22411;&#25913;&#36827;&#19977;&#32500;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models. (arXiv:2303.08440v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20004;&#20010;&#39044;&#35757;&#32451;&#30340;&#22402;&#30452;&#20108;&#32500;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#19977;&#32500;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23558;&#19977;&#32500;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#20026;&#22312;&#19981;&#21516;&#26041;&#21521;&#19978;&#20999;&#29255;&#30340;&#20108;&#32500;&#20998;&#24067;&#30340;&#20056;&#31215;&#65292;&#35299;&#20915;&#20102;&#32500;&#25968;&#30340;&#28798;&#38590;&#24615;&#38382;&#39064;&#65292;&#24182;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20247;&#22810;&#30340;&#20248;&#28857;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#21644;&#37325;&#26500;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290; &#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#25193;&#25955;&#30340;&#36870;&#38382;&#39064;&#27714;&#35299;&#26041;&#27861;&#20165;&#22788;&#29702;&#20108;&#32500;&#22270;&#20687;&#65292;&#21363;&#20351;&#26159;&#26368;&#36817;&#21457;&#24067;&#30340;&#19977;&#32500;&#26041;&#27861;&#20063;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#19977;&#32500;&#20808;&#39564;&#20998;&#24067;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#20004;&#20010;&#30456;&#20114;&#22402;&#30452;&#30340;&#39044;&#35757;&#32451;&#20108;&#32500;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#19977;&#32500;&#36870;&#38382;&#39064;&#12290; &#36890;&#36807;&#23558;&#19977;&#32500;&#25968;&#25454;&#20998;&#24067;&#24314;&#27169;&#20026;&#22312;&#19981;&#21516;&#26041;&#21521;&#19978;&#20999;&#29255;&#30340;&#20108;&#32500;&#20998;&#24067;&#30340;&#20056;&#31215;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#32500;&#25968;&#30340;&#28798;&#38590;&#24615;&#38382;&#39064;&#12290; &#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#65292;&#21253;&#25324;MRI Z&#36724;&#36229;&#20998;&#36776;&#29575;&#65292;&#21387;&#32553;&#24863;&#30693;MRI&#21644;&#31232;&#30095;&#35270;&#22270;CT&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#36866;&#29992;&#20110;&#21307;&#23398;&#24212;&#29992;&#30340;&#39640;&#36136;&#37327;&#20307;&#32032;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have become a popular approach for image generation and reconstruction due to their numerous advantages. However, most diffusion-based inverse problem-solving methods only deal with 2D images, and even recently published 3D methods do not fully exploit the 3D distribution prior. To address this, we propose a novel approach using two perpendicular pre-trained 2D diffusion models to solve the 3D inverse problem. By modeling the 3D data distribution as a product of 2D distributions sliced in different directions, our method effectively addresses the curse of dimensionality. Our experimental results demonstrate that our method is highly effective for 3D medical image reconstruction tasks, including MRI Z-axis super-resolution, compressed sensing MRI, and sparse-view CT. Our method can generate high-quality voxel volumes suitable for medical applications.
&lt;/p&gt;</description></item><item><title>STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06261</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;
&lt;/p&gt;
&lt;p&gt;
Interpretable Outlier Summarization. (arXiv:2303.06261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06261
&lt;/p&gt;
&lt;p&gt;
STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
STAIR proposes an interpretable outlier summarization method by learning a compact set of human understandable rules to summarize and explain the anomaly detection results, which has strong interpretability to accurately summarize the detection results.
&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20540;&#26816;&#27979;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#38450;&#27490;&#37329;&#34701;&#27450;&#35784;&#12289;&#38450;&#24481;&#32593;&#32476;&#20837;&#20405;&#25110;&#26816;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#35774;&#22791;&#25925;&#38556;&#12290;&#20026;&#20102;&#20943;&#23569;&#20154;&#21147;&#35780;&#20272;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#26377;&#25928;&#22320;&#23558;&#24322;&#24120;&#20540;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#65292;&#29992;&#25143;&#36890;&#24120;&#24076;&#26395;&#31995;&#32479;&#33258;&#21160;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#23376;&#32452;&#30340;&#27719;&#24635;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#31995;&#32479;&#23384;&#22312;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAIR&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#19981;&#20351;&#29992;&#32463;&#20856;&#30340;&#20915;&#31574;&#26641;&#31639;&#27861;&#26469;&#20135;&#29983;&#36825;&#20123;&#35268;&#21017;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#20135;&#29983;&#23569;&#37327;&#35268;&#21017;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#30340;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#20998;&#21106;&#22823;&#35268;&#21017;&#26469;&#20135;&#29983;&#35268;&#21017;&#38598;&#65292;&#24182;&#22312;&#27599;&#20010;i&#20013;&#26368;&#22823;&#21270;&#36825;&#20010;&#30446;&#26631;&#65292;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier detection is critical in real applications to prevent financial fraud, defend network intrusions, or detecting imminent device failures. To reduce the human effort in evaluating outlier detection results and effectively turn the outliers into actionable insights, the users often expect a system to automatically produce interpretable summarizations of subgroups of outlier detection results. Unfortunately, to date no such systems exist. To fill this gap, we propose STAIR which learns a compact set of human understandable rules to summarize and explain the anomaly detection results. Rather than use the classical decision tree algorithms to produce these rules, STAIR proposes a new optimization objective to produce a small number of rules with least complexity, hence strong interpretability, to accurately summarize the detection results. The learning algorithm of STAIR produces a rule set by iteratively splitting the large rules and is optimal in maximizing this objective in each i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Task Aware Dreamer&#65288;TAD&#65289;&#30340;&#26041;&#27861;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#27867;&#21270;&#12290;&#36890;&#36807;&#37327;&#21270;&#20219;&#21153;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#65292;TAD&#33021;&#22815;&#23558;&#21382;&#21490;&#20449;&#24687;&#32534;&#30721;&#21040;&#31574;&#30053;&#20013;&#65292;&#20197;&#20415;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#65292;&#24182;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05092</link><description>&lt;p&gt;
Task Aware Dreamer&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Task Aware Dreamer for Task Generalization in Reinforcement Learning. (arXiv:2303.05092v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Task Aware Dreamer&#65288;TAD&#65289;&#30340;&#26041;&#27861;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#27867;&#21270;&#12290;&#36890;&#36807;&#37327;&#21270;&#20219;&#21153;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#65292;TAD&#33021;&#22815;&#23558;&#21382;&#21490;&#20449;&#24687;&#32534;&#30721;&#21040;&#31574;&#30053;&#20013;&#65292;&#20197;&#20415;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#65292;&#24182;&#22312;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#26102;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#26159;&#33719;&#24471;&#33021;&#22815;&#22312;&#35757;&#32451;&#20219;&#21153;&#19978;&#23398;&#20064;&#24182;&#19988;&#22312;&#19981;&#21516;&#22870;&#21169;&#20989;&#25968;&#19979;&#21487;&#20197;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#19968;&#20010;&#36890;&#29992;&#30340;&#25361;&#25112;&#26159;&#23450;&#37327;&#22320;&#34913;&#37327;&#36825;&#20123;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#36825;&#23545;&#20110;&#20998;&#26512;&#20219;&#21153;&#20998;&#24067;&#24182;&#36827;&#19968;&#27493;&#35774;&#35745;&#20855;&#26377;&#26356;&#24378;&#27867;&#21270;&#33021;&#21147;&#30340;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#20219;&#21153;&#20998;&#24067;&#30456;&#20851;&#24615;&#65288;TDR&#65289;&#65292;&#36890;&#36807;&#19981;&#21516;&#20219;&#21153;&#30340;&#26368;&#20248;Q&#20989;&#25968;&#26469;&#37327;&#21270;&#20219;&#21153;&#20998;&#24067;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#20855;&#26377;&#39640;TDR&#30340;&#20219;&#21153;&#24773;&#20917;&#19979;&#65292;&#21363;&#20219;&#21153;&#20043;&#38388;&#26174;&#33879;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#26080;&#27861;&#21306;&#20998;&#23427;&#20204;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#32534;&#30721;&#21040;&#31574;&#30053;&#20013;&#20197;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;Task Aware Dreamer&#65288;TAD&#65289;&#65292;&#23427;&#23558;&#19990;&#30028;&#27169;&#22411;&#25193;&#23637;&#20026;&#25105;&#20204;&#30340;&#22870;&#21169;&#24863;&#30693;&#19990;&#30028;&#27169;&#22411;&#20197;&#25429;&#25417;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A long-standing goal of reinforcement learning is to acquire agents that can learn on training tasks and generalize well on unseen tasks that may share a similar dynamic but with different reward functions. A general challenge is to quantitatively measure the similarities between these different tasks, which is vital for analyzing the task distribution and further designing algorithms with stronger generalization. To address this, we present a novel metric named Task Distribution Relevance (TDR) via optimal Q functions of different tasks to capture the relevance of the task distribution quantitatively. In the case of tasks with a high TDR, i.e., the tasks differ significantly, we show that the Markovian policies cannot differentiate them, leading to poor performance. Based on this insight, we encode all historical information into policies for distinguishing different tasks and propose Task Aware Dreamer (TAD), which extends world models into our reward-informed world models to capture
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.00848</link><description>&lt;p&gt;
&#20197;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#29702;&#35299;&#25193;&#25955;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#24182;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#29486;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#29992;&#19981;&#21516;&#30340;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#19988;&#36825;&#20123;&#30446;&#26631;&#37117;&#26159;&#21152;&#26435;&#25439;&#22833;&#30340;&#29305;&#20363;&#65292;&#20854;&#20013;&#21152;&#26435;&#20989;&#25968;&#25351;&#23450;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#30340;&#26435;&#37325;&#12290;&#22343;&#21248;&#21152;&#26435;&#23545;&#24212;&#20110;&#26368;&#22823;&#20284;&#28982;&#30340;&#21407;&#21017;&#24615;&#36817;&#20284;ELBO&#30340;&#26368;&#22823;&#21270;&#12290;&#20294;&#26159;&#23454;&#38469;&#19978;&#65292;&#30001;&#20110;&#26356;&#22909;&#30340;&#26679;&#26412;&#36136;&#37327;&#65292;&#30446;&#21069;&#30340;&#25193;&#25955;&#27169;&#22411;&#20351;&#29992;&#38750;&#22343;&#21248;&#21152;&#26435;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#65288;&#24102;&#26377;&#20219;&#20309;&#21152;&#26435;&#65289;&#21644;ELBO&#30446;&#26631;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#26435;&#25439;&#22833;&#21487;&#20197;&#34987;&#20889;&#25104;&#19968;&#31181;ELBOs&#30340;&#21152;&#26435;&#31215;&#20998;&#24418;&#24335;&#65292;&#20854;&#20013;&#27599;&#20010;&#22122;&#22768;&#32423;&#21035;&#37117;&#26377;&#19968;&#20010;ELBO&#12290;&#22914;&#26524;&#26435;&#37325;&#20989;&#25968;&#26159;&#21333;&#35843;&#30340;&#65292;&#37027;&#20040;&#21152;&#26435;&#25439;&#22833;&#26159;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#30446;&#26631;&#65306;&#23427;&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#19979;&#65288;&#21363;&#39640;&#26031;&#22122;&#22768;&#25200;&#21160;&#65289;&#19979;&#26368;&#22823;&#21270;ELBO&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20102;&#25193;&#25955;&#30446;&#26631;&#65292;&#20294;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20123;&#27604;&#36739;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#26435;&#37325;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#31639;&#27861;&#22312;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#23398;&#20064;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#24230;&#37327;&#31216;&#20026;&#36291;&#36801;&#65292;&#35777;&#26126;&#20102;&#22312;&#39640;&#26031;&#21508;&#21521;&#21516;&#24615;&#25968;&#25454;&#21644;&#20108;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#20989;&#25968;&#25903;&#25345;&#30340;&#21160;&#24577;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.11055</link><description>&lt;p&gt;
SGD&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;: &#36291;&#36801;&#22797;&#26434;&#24230;&#19982;&#38797;&#21040;&#38797;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics. (arXiv:2302.11055v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#31639;&#27861;&#22312;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#23398;&#20064;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#24230;&#37327;&#31216;&#20026;&#36291;&#36801;&#65292;&#35777;&#26126;&#20102;&#22312;&#39640;&#26031;&#21508;&#21521;&#21516;&#24615;&#25968;&#25454;&#21644;&#20108;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#20989;&#25968;&#25903;&#25345;&#30340;&#21160;&#24577;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#23398;&#20064;&#20855;&#26377;&#21508;&#21521;&#21516;&#24615;&#25968;&#25454;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22797;&#26434;&#24230;&#24230;&#37327;&#8212;&#8212;&#36291;&#36801;&#65292;&#29992;&#26469;&#24230;&#37327;&#30446;&#26631;&#20989;&#25968;&#30340;"&#23618;&#32423;"&#31243;&#24230;&#12290;&#23545;&#20110;$d$&#32500;&#22343;&#21248;&#24067;&#23572;&#25110;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#29468;&#24819;&#26159;&#23398;&#20064;&#19968;&#20010;&#20302;&#32500;&#25903;&#25345;&#20989;&#25968;$f$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$\tilde\Theta (d^{\max(\mathrm{Leap}(f),2)})$&#12290;&#25105;&#20204;&#22312;&#39069;&#22806;&#23545;SGD&#30340;&#25216;&#26415;&#20551;&#35774;&#19979;&#65292;&#35777;&#26126;&#20102;&#36825;&#20010;&#29468;&#24819;&#22312;&#39640;&#26031;&#21508;&#21521;&#21516;&#24615;&#25968;&#25454;&#21644;&#20108;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#19968;&#20010;&#29256;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#35757;&#32451;&#36807;&#31243;&#20197;&#38797;&#28857;&#21040;&#38797;&#28857;&#30340;&#21160;&#24577;&#26041;&#24335;&#36880;&#28176;&#23398;&#20064;&#20102;&#20989;&#25968;&#30340;&#25903;&#25345;&#12290;&#19982;[Abbe et al. 2022]&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36229;&#36234;&#20102;&#36291;&#36801;1(&#21512;&#24182;&#38454;&#26799;&#20989;&#25968;)&#30340;&#38480;&#21046;&#65292;&#24182;&#36229;&#36234;&#20102;&#22343;&#22330;&#21644;&#26799;&#24230;&#27969;&#36817;&#20284;&#65292;&#22312;&#36825;&#37324;&#21487;&#20197;&#33719;&#24471;&#23436;&#20840;&#30340;&#22797;&#26434;&#24230;&#25511;&#21046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#36825;&#32473;&#20986;&#20102;&#23436;&#25972;&#35757;&#32451;&#30340;SGD&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the time complexity of SGD learning on fully-connected neural networks with isotropic data. We put forward a complexity measure -- the leap -- which measures how "hierarchical" target functions are. For $d$-dimensional uniform Boolean or isotropic Gaussian data, our main conjecture states that the time complexity to learn a function $f$ with low-dimensional support is $\tilde\Theta (d^{\max(\mathrm{Leap}(f),2)})$. We prove a version of this conjecture for a class of functions on Gaussian isotropic data and 2-layer neural networks, under additional technical assumptions on how SGD is run. We show that the training sequentially learns the function support with a saddle-to-saddle dynamic. Our result departs from [Abbe et al. 2022] by going beyond leap 1 (merged-staircase functions), and by going beyond the mean-field and gradient flow approximations that prohibit the full complexity control obtained here. Finally, we note that this gives an SGD complexity for the full train
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25191;&#34892;&#32467;&#26524;&#26469;&#39564;&#35777;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#31616;&#21333;&#26041;&#27861;LEVER&#65292;&#36890;&#36807;&#35757;&#32451;&#39564;&#35777;&#22120;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#12289;&#31243;&#24207;&#26412;&#36523;&#21644;&#25191;&#34892;&#32467;&#26524;&#26469;&#30830;&#23450;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2302.08468</link><description>&lt;p&gt;
LEVER: &#20351;&#29992;&#25191;&#34892;&#36827;&#34892;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#23398;&#20064;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
LEVER: Learning to Verify Language-to-Code Generation with Execution. (arXiv:2302.08468v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08468
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25191;&#34892;&#32467;&#26524;&#26469;&#39564;&#35777;&#29983;&#25104;&#30340;&#31243;&#24207;&#30340;&#31616;&#21333;&#26041;&#27861;LEVER&#65292;&#36890;&#36807;&#35757;&#32451;&#39564;&#35777;&#22120;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#12289;&#31243;&#24207;&#26412;&#36523;&#21644;&#25191;&#34892;&#32467;&#26524;&#26469;&#30830;&#23450;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22312;&#20195;&#30721;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;code LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#22312;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#27492;&#39046;&#22495;&#30340;&#26368;&#26032;&#26041;&#27861;&#23558;LLM&#35299;&#30721;&#19982;&#20351;&#29992;&#27979;&#35797;&#29992;&#20363;&#25110;&#22522;&#20110;&#25191;&#34892;&#32467;&#26524;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26679;&#26412;&#20462;&#21098;&#21644;&#37325;&#26032;&#25490;&#24207;&#30456;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#21040;&#20195;&#30721;&#24212;&#29992;&#26469;&#35828;&#65292;&#33719;&#21462;&#27979;&#35797;&#29992;&#20363;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#21551;&#21457;&#24335;&#26041;&#27861;&#19981;&#33021;&#24456;&#22909;&#22320;&#25429;&#25417;&#25191;&#34892;&#32467;&#26524;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#27604;&#22914;&#25968;&#25454;&#31867;&#22411;&#21644;&#20540;&#33539;&#22260;&#65292;&#36825;&#24448;&#24448;&#34920;&#26126;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEVER&#65292;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#20351;&#29992;&#25191;&#34892;&#32467;&#26524;&#26469;&#39564;&#35777;&#29983;&#25104;&#30340;&#31243;&#24207;&#65292;&#20174;&#32780;&#25913;&#36827;&#35821;&#35328;&#21040;&#20195;&#30721;&#29983;&#25104;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35757;&#32451;&#39564;&#35777;&#22120;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#12289;&#31243;&#24207;&#26412;&#36523;&#21644;&#25191;&#34892;&#32467;&#26524;&#26469;&#30830;&#23450;&#20174;LLM&#20013;&#25277;&#26679;&#30340;&#31243;&#24207;&#26159;&#21542;&#27491;&#30830;&#12290;&#36890;&#36807;&#23558;&#39564;&#35777;&#20998;&#25968;&#19982;LLM&#29983;&#25104;&#20998;&#25968;&#30456;&#32467;&#21512;&#65292;&#23545;&#25277;&#26679;&#30340;&#31243;&#24207;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#33976;&#39311;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;SD-DARTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36229;&#32593;&#30340;&#20808;&#21069;&#27493;&#39588;&#20013;&#33976;&#39311;&#30693;&#35782;&#26469;&#25351;&#23548;&#20854;&#35757;&#32451;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#36229;&#32593;&#25439;&#22833;&#30340;&#23574;&#38160;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#31163;&#25955;&#21270;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2302.05629</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#33976;&#39311;&#25913;&#36827;&#21487;&#24494;&#20998;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Differentiable Architecture Search via Self-Distillation. (arXiv:2302.05629v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#33976;&#39311;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;SD-DARTS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36229;&#32593;&#30340;&#20808;&#21069;&#27493;&#39588;&#20013;&#33976;&#39311;&#30693;&#35782;&#26469;&#25351;&#23548;&#20854;&#35757;&#32451;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#36229;&#32593;&#25439;&#22833;&#30340;&#23574;&#38160;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#31163;&#25955;&#21270;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#26550;&#26500;&#25628;&#32034;&#65288;DARTS&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26041;&#27861;&#12290;DARTS&#22312;&#25628;&#32034;&#38454;&#27573;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26550;&#26500;&#21442;&#25968;&#21644;&#32593;&#32476;&#21442;&#25968;&#26469;&#35757;&#32451;&#36229;&#32593;&#12290;&#22312;&#35780;&#20272;&#38454;&#27573;&#65292;DARTS&#23558;&#36229;&#32593;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#24471;&#21040;&#22522;&#20110;&#26550;&#26500;&#21442;&#25968;&#30340;&#26368;&#20248;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36229;&#32593;&#24448;&#24448;&#20250;&#25910;&#25947;&#21040;&#23574;&#38160;&#30340;&#26497;&#23567;&#20540;&#28857;&#32780;&#19981;&#26159;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#12290;&#36825;&#20307;&#29616;&#22312;&#36229;&#32593;&#25439;&#22833;&#26354;&#38754;&#30340;&#23574;&#38160;&#31243;&#24230;&#36739;&#39640;&#65292;&#26368;&#32456;&#23548;&#33268;&#36229;&#32593;&#19982;&#26368;&#20248;&#26550;&#26500;&#20043;&#38388;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#33976;&#39311;&#21487;&#24494;&#20998;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;SD-DARTS&#65289;&#26469;&#32531;&#35299;&#31163;&#25955;&#21270;&#24046;&#36317;&#12290;&#25105;&#20204;&#21033;&#29992;&#33258;&#25105;&#33976;&#39311;&#20174;&#36229;&#32593;&#30340;&#20808;&#21069;&#27493;&#39588;&#20013;&#33976;&#39311;&#30693;&#35782;&#65292;&#24341;&#23548;&#20854;&#22312;&#24403;&#21069;&#27493;&#39588;&#20013;&#30340;&#35757;&#32451;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#36229;&#32593;&#25439;&#22833;&#30340;&#23574;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable Architecture Search (DARTS) is a simple yet efficient Neural Architecture Search (NAS) method. During the search stage, DARTS trains a supernet by jointly optimizing architecture parameters and network parameters. During the evaluation stage, DARTS discretizes the supernet to derive the optimal architecture based on architecture parameters. However, recent research has shown that during the training process, the supernet tends to converge towards sharp minima rather than flat minima. This is evidenced by the higher sharpness of the loss landscape of the supernet, which ultimately leads to a performance gap between the supernet and the optimal architecture. In this paper, we propose Self-Distillation Differentiable Neural Architecture Search (SD-DARTS) to alleviate the discretization gap. We utilize self-distillation to distill knowledge from previous steps of the supernet to guide its training in the current step, effectively reducing the sharpness of the supernet's loss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25209;&#22788;&#29702;&#31574;&#30053;&#65292;&#37319;&#29992;&#22522;&#20110;GPU&#30340;&#25209;&#22788;&#29702;&#26381;&#21153;&#38431;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#26368;&#23567;&#21270;&#24179;&#22343;&#21709;&#24212;&#26102;&#38388;&#21644;&#21151;&#32791;&#12290;</title><link>http://arxiv.org/abs/2301.12865</link><description>&lt;p&gt;
&#22522;&#20110;SMDP&#30340;GPU&#21160;&#24577;&#25209;&#22788;&#29702;&#20248;&#21270;&#25512;&#26029;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
SMDP-Based Dynamic Batching for Efficient Inference on GPU-Based Platforms. (arXiv:2301.12865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25209;&#22788;&#29702;&#31574;&#30053;&#65292;&#37319;&#29992;&#22522;&#20110;GPU&#30340;&#25209;&#22788;&#29702;&#26381;&#21153;&#38431;&#21015;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#27861;&#26368;&#23567;&#21270;&#24179;&#22343;&#21709;&#24212;&#26102;&#38388;&#21644;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#35745;&#31639;&#25110;&#36793;&#32536;&#35745;&#31639;&#24179;&#21488;&#19978;&#65292;&#25209;&#22788;&#29702;&#26159;&#25552;&#20379;&#39640;&#25928;&#21644;&#32463;&#27982;&#26381;&#21153;&#30340;&#37325;&#35201;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25209;&#22788;&#29702;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#25928;&#29575;&#21644;&#24310;&#36831;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#23558;&#22522;&#20110;GPU&#30340;&#25512;&#26029;&#26381;&#21153;&#24314;&#27169;&#20026;&#25209;&#22788;&#29702;&#26381;&#21153;&#38431;&#21015;&#65292;&#24182;&#23558;&#20854;&#35774;&#35745;&#20026;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#24179;&#22343;&#25104;&#26412;&#38382;&#39064;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;&#21322;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;SMDP&#65289;&#65292;&#24182;&#20197;&#26368;&#23567;&#21270;&#24179;&#22343;&#21709;&#24212;&#26102;&#38388;&#21644;&#24179;&#22343;&#21151;&#32791;&#20043;&#21644;&#20026;&#30446;&#26631;&#12290;&#26368;&#20248;&#31574;&#30053;&#36890;&#36807;&#35299;&#20915;&#30456;&#20851;&#30340;&#31163;&#25955;&#26102;&#38388;&#36125;&#23572;&#26364;&#26041;&#31243;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In up-to-date machine learning (ML) applications on cloud or edge computing platforms, batching is an important technique for providing efficient and economical services at scale. In particular, parallel computing resources on the platforms, such as graphics processing units (GPUs), have higher computational and energy efficiency with larger batch sizes. However, larger batch sizes may also result in longer response time, and thus it requires a judicious design. This paper aims to provide a dynamic batching policy that strikes a balance between efficiency and latency. The GPU-based inference service is modeled as a batch service queue with batch-size dependent processing time. Then, the design of dynamic batching is a continuous-time average-cost problem, and is formulated as a semi-Markov decision process (SMDP) with the objective of minimizing the weighted sum of average response time and average power consumption. The optimal policy is acquired by solving an associated discrete-time
&lt;/p&gt;</description></item><item><title>MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11259</link><description>&lt;p&gt;
&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#29983;&#25104;&#19982;&#33258;&#25105;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11259
&lt;/p&gt;
&lt;p&gt;
MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#29983;&#25104;&#24050;&#32463;&#21463;&#21040;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#20854;&#38761;&#26032;&#20102;&#31185;&#23398;&#23478;&#35774;&#35745;&#20998;&#23376;&#32467;&#26500;&#30340;&#26041;&#24335;&#65292;&#24182;&#20026;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#20998;&#23376;&#29983;&#25104;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#29983;&#25104;&#35821;&#27861;&#25110;&#21270;&#23398;&#23384;&#22312;&#32570;&#38519;&#30340;&#20998;&#23376;&#65292;&#29421;&#31364;&#30340;&#39046;&#22495;&#19987;&#27880;&#20197;&#21450;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#25110;&#22806;&#37096;&#20998;&#23376;&#25968;&#25454;&#24211;&#32780;&#38480;&#21046;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MolGen&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#12290;MolGen&#36890;&#36807;&#37325;&#26500;&#19968;&#20159;&#22810;&#20010;&#20998;&#23376;SELFIES&#33719;&#24471;&#20102;&#22266;&#26377;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21453;&#39304;&#33539;&#24335;&#65292;&#21551;&#21457;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#26368;&#32456;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#26356;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MolGen&#22312;&#21270;&#23398;&#26377;&#25928;&#24615;&#65292;&#22810;&#26679;&#24615;&#65292;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#20986;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01168</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning. (arXiv:2212.01168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#21704;&#23494;&#39039;&#27969;&#24418;&#20013;&#35782;&#21035;&#20986;&#26222;&#36941;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#29289;&#29702;&#31995;&#32479;&#30340;&#24555;&#36895;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29289;&#29702;&#23398;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#38598;&#20013;&#22312;&#36890;&#36807;&#23558;&#29289;&#29702;&#20808;&#39564;&#25110;&#24402;&#32435;&#20559;&#35265;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#26469;&#21457;&#29616;&#30446;&#26631;&#31995;&#32479;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#29305;&#23450;&#20110;&#31995;&#32479;&#65292;&#19981;&#20801;&#35768;&#36731;&#26494;&#36866;&#24212;&#30001;&#19981;&#21516;&#29289;&#29702;&#27861;&#21017;&#39537;&#21160;&#30340;&#26032;&#29289;&#29702;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#35757;&#32451;&#20110;&#36136;&#28857;&#24377;&#31783;&#31995;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#21452;&#20307;&#31995;&#32479;&#25110;&#20219;&#20309;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#27861;&#21017;&#30340;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#25105;&#20204;&#30340;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#20803;&#23398;&#20064;&#31639;&#27861;&#20351;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#31215;&#32047;&#32463;&#39564;&#65292;&#24182;&#20351;&#20854;&#36866;&#24212;&#26032;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#36328;&#21508;&#31181;&#21704;&#23494;&#39039;&#27969;&#24418;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#36825;&#26159;&#21704;&#23494;&#39039;&#31995;&#32479;&#25968;&#25454;&#20998;&#24067;&#30340;&#20849;&#21516;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;&#19981;&#21516;&#31995;&#32479;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#27169;&#22411;&#65292;&#27599;&#20010;&#31995;&#32479;&#37117;&#26377;&#20854;&#33258;&#36523;&#22266;&#26377;&#30340;&#21160;&#21147;&#23398;&#65292;&#24182;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. However, these approaches are system-specific and do not allow for easy adaptation to new physical systems governed by different laws. For example, a neural network trained on a mass-spring system cannot accurately predict the behavior of a two-body system or any other system with different governing physics. In this work, we model our system with a graph neural network and employ a meta-learning algorithm to enable the model to gain experience over a distribution of tasks and make it adapt to new physics. Our approach aims to learn a general representation across the various Hamiltonian manifolds, which is a common feature of the data distribution of Hamiltonian systems. We train our model using a dataset of different physical systems, each governed by its own inherent dynamics, and evaluate its 
&lt;/p&gt;</description></item><item><title>C3&#26159;&#19968;&#31181;&#24341;&#20837;&#20102;&#20132;&#21449;&#23454;&#20363;&#20851;&#31995;&#30340;&#23545;&#27604;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#22686;&#21152;&#20102;&#27491;&#23545;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.07136</link><description>&lt;p&gt;
C3: &#36328;&#23454;&#20363;&#24341;&#23548;&#23545;&#27604;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
C3: Cross-instance guided Contrastive Clustering. (arXiv:2211.07136v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07136
&lt;/p&gt;
&lt;p&gt;
C3&#26159;&#19968;&#31181;&#24341;&#20837;&#20102;&#20132;&#21449;&#23454;&#20363;&#20851;&#31995;&#30340;&#23545;&#27604;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#22686;&#21152;&#20102;&#27491;&#23545;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#23558;&#30456;&#20284;&#30340;&#25968;&#25454;&#26679;&#26412;&#32858;&#38598;&#21040;&#26080;&#38656;&#39044;&#23450;&#20041;&#26631;&#31614;&#30340;&#31751;&#20013;&#30340;&#20219;&#21153;&#12290;&#23427;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#37325;&#26032;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#35813;&#39046;&#22495;&#30340;&#20852;&#36259;&#12290;&#23545;&#27604;&#32858;&#31867;&#65288;CC&#65289;&#27169;&#22411;&#26159;&#28145;&#24230;&#32858;&#31867;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#30340;&#27491;&#36127;&#23545;&#12290;CC&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#23558;&#27491;&#23545;&#23454;&#20363;&#30340;&#23454;&#20363;&#32423;&#21644;&#32858;&#31867;&#32423;&#34920;&#31034;&#20998;&#32452;&#22312;&#19968;&#36215;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#23613;&#31649;&#25552;&#39640;&#20102;SOTA&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#24573;&#30053;&#20102;&#25658;&#24102;&#26377;&#25913;&#36827;&#32858;&#31867;&#24615;&#33021;&#30340;&#20132;&#21449;&#23454;&#20363;&#27169;&#24335;&#12290;&#36825;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#38169;&#35823;&#36127;&#23545;&#25968;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#23427;&#30340;&#30495;&#27491;&#27491;&#23545;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#32858;&#31867;&#26041;&#27861;&#65292;Cross-instance guided Contrastive Clustering (C3)&#65292;&#32771;&#34385;&#20102;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#22686;&#21152;&#27491;&#23545;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering is the task of gathering similar data samples into clusters without using any predefined labels. It has been widely studied in machine learning literature, and recent advancements in deep learning have revived interest in this field. Contrastive clustering (CC) models are a staple of deep clustering in which positive and negative pairs of each data instance are generated through data augmentation. CC models aim to learn a feature space where instance-level and cluster-level representations of positive pairs are grouped together. Despite improving the SOTA, these algorithms ignore the cross-instance patterns, which carry essential information for improving clustering performance. This increases the false-negative-pair rate of the model while decreasing its true-positive-pair rate. In this paper, we propose a novel contrastive clustering method, Cross-instance guided Contrastive Clustering (C3), that considers the cross-sample relationships to increase the number of positive p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#20132;&#26367;&#26799;&#24230;&#27861;&#35299;&#20915;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#38544;&#31169;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.04088</link><description>&lt;p&gt;
&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#20132;&#26367;&#26799;&#24230;&#27861;&#29992;&#20110;&#21452;&#23618;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
A Decentralized Alternating Gradient Method for Communication-Efficient Bilevel Programming. (arXiv:2211.04088v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#20132;&#26367;&#26799;&#24230;&#27861;&#35299;&#20915;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#38544;&#31169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#35268;&#21010;&#36817;&#26399;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#21333;&#26426;&#25110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#24182;&#23384;&#22312;&#36890;&#20449;&#25104;&#26412;&#39640;&#21644;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#20989;&#25968;&#30340;&#20998;&#25955;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#31867;&#20248;&#21270;&#38382;&#39064;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20102;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel programming has recently received attention in the literature, due to a wide range of applications, including reinforcement learning and hyper-parameter optimization. However, it is widely assumed that the underlying bilevel optimization problem is solved either by a single machine or in the case of multiple machines connected in a star-shaped network, i.e., federated learning setting. The latter approach suffers from a high communication cost on the central node (e.g., parameter server) and exhibits privacy vulnerabilities. Hence, it is of interest to develop methods that solve bilevel optimization problems in a communication-efficient decentralized manner. To that end, this paper introduces a penalty function based decentralized algorithm with theoretical guarantees for this class of optimization problems. Specifically, a distributed alternating gradient-type algorithm for solving consensus bilevel programming over a decentralized network is developed. A key feature of the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;KalmenNet&#36741;&#21161;&#30340;Bollinger&#24102;&#37197;&#23545;&#20132;&#26131;&#31574;&#30053;&#65288;KBPT&#65289;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#20102;KF-BB&#20132;&#26131;&#25805;&#20316;&#12290;KBPT&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#25193;&#23637;&#30340;&#31354;&#38388;&#29366;&#24577;&#27169;&#22411;&#26469;&#36817;&#20284;&#37197;&#23545;&#20132;&#26131;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;&#20132;&#26131;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.15448</link><description>&lt;p&gt;
&#20351;&#29992;Bollinger&#24102;&#22686;&#24378;&#30340;&#31070;&#32463;&#22686;&#24378;&#21345;&#23572;&#26364;&#28388;&#27874;&#36827;&#34892;&#37197;&#23545;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
Neural Augmented Kalman Filtering with Bollinger Bands for Pairs Trading. (arXiv:2210.15448v2 [q-fin.TR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;KalmenNet&#36741;&#21161;&#30340;Bollinger&#24102;&#37197;&#23545;&#20132;&#26131;&#31574;&#30053;&#65288;KBPT&#65289;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#20102;KF-BB&#20132;&#26131;&#25805;&#20316;&#12290;KBPT&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#25193;&#23637;&#30340;&#31354;&#38388;&#29366;&#24577;&#27169;&#22411;&#26469;&#36817;&#20284;&#37197;&#23545;&#20132;&#26131;&#30340;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;&#20132;&#26131;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37197;&#23545;&#20132;&#26131;&#26159;&#19968;&#31867;&#20132;&#26131;&#25216;&#26415;&#65292;&#20854;&#31574;&#30053;&#26159;&#22522;&#20110;&#30417;&#27979;&#36164;&#20135;&#23545;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#37197;&#23545;&#20132;&#26131;&#26041;&#27861;&#26159;&#23558;&#36164;&#20135;&#23545;&#20043;&#38388;&#30340;&#20851;&#31995;&#25551;&#36848;&#20026;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;&#32447;&#24615;&#31354;&#38388;&#29366;&#24577;&#27169;&#22411;&#12290;&#36825;&#31181;&#34920;&#31034;&#26041;&#27861;&#21033;&#29992;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#25552;&#21462;&#20855;&#26377;&#20302;&#22797;&#26434;&#24615;&#21644;&#24310;&#36831;&#30340;&#37329;&#34701;&#25351;&#26631;&#65292;&#28982;&#21518;&#20351;&#29992;&#32463;&#20856;&#31574;&#30053;&#65288;&#22914;Bollinger&#24102;&#65289;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31354;&#38388;&#29366;&#24577;&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#36817;&#20284;&#21644;&#19981;&#21305;&#37197;&#30340;&#65292;&#32463;&#24120;&#20250;&#38477;&#20302;&#25910;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KalmenNet&#36741;&#21161;&#30340;Bollinger&#24102;&#37197;&#23545;&#20132;&#26131;&#65288;KBPT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20511;&#21161;&#28145;&#24230;&#23398;&#20064;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#24378;KF-BB&#20132;&#26131;&#30340;&#25805;&#20316;&#12290;KBPT&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#25193;&#23637;&#30340;&#31354;&#38388;&#29366;&#24577;&#27169;&#22411;&#26469;&#36817;&#20284;&#37197;&#23545;&#20132;&#26131;&#30340;&#20851;&#31995;&#65292;&#23558;&#20854;&#35270;&#20026;&#25345;&#26377;&#37096;&#20998;&#21327;&#25972;&#12290;&#36825;&#20010;&#31354;&#38388;&#29366;&#24577;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#19968;&#20010;&#20132;&#26131;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#21033;&#29992;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;KF-BB&#20132;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pairs trading is a family of trading techniques that determine their policies based on monitoring the relationships between pairs of assets. A common pairs trading approach relies on describing the pair-wise relationship as a linear Space State (SS) model with Gaussian noise. This representation facilitates extracting financial indicators with low complexity and latency using a Kalman Filter (KF), that are then processed using classic policies such as Bollinger Bands (BB). However, such SS models are inherently approximated and mismatched, often degrading the revenue. In this work, we propose KalmenNet-aided Bollinger bands Pairs Trading (KBPT), a deep learning aided policy that augments the operation of KF-aided BB trading. KBPT is designed by formulating an extended SS model for pairs trading that approximates their relationship as holding partial co-integration. This SS model is utilized by a trading policy that augments KF-BB trading with a dedicated neural network based on the Kal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#27604;&#20363;&#22810;&#26657;&#20934;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#20844;&#24179;&#26657;&#20934;&#65292;&#22312;&#38480;&#21046;&#26657;&#20934;&#35823;&#24046;&#30340;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#26657;&#20934;&#12290;&#36890;&#36807;&#27604;&#20363;&#22810;&#26657;&#20934;&#21487;&#20197;&#36991;&#20813;&#27169;&#22411;&#23545;&#29305;&#23450;&#32676;&#20307;&#30340;&#39044;&#27979;&#20449;&#20219;&#25110;&#19981;&#20449;&#20219;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.14613</link><description>&lt;p&gt;
&#20855;&#26377;&#27604;&#20363;&#22810;&#26657;&#20934;&#30340;&#20844;&#24179;&#20837;&#23398;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fair admission risk prediction with proportional multicalibration. (arXiv:2209.14613v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14613
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#27604;&#20363;&#22810;&#26657;&#20934;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#20844;&#24179;&#26657;&#20934;&#65292;&#22312;&#38480;&#21046;&#26657;&#20934;&#35823;&#24046;&#30340;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#26657;&#20934;&#12290;&#36890;&#36807;&#27604;&#20363;&#22810;&#26657;&#20934;&#21487;&#20197;&#36991;&#20813;&#27169;&#22411;&#23545;&#29305;&#23450;&#32676;&#20307;&#30340;&#39044;&#27979;&#20449;&#20219;&#25110;&#19981;&#20449;&#20219;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#26657;&#20934;&#26159;&#39118;&#38505;&#39044;&#27979;&#29615;&#22659;&#20013;&#24191;&#27867;&#38656;&#35201;&#30340;&#20844;&#24179;&#26631;&#20934;&#20043;&#19968;&#12290;&#23454;&#29616;&#20844;&#24179;&#26657;&#20934;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#37319;&#29992;&#22810;&#26657;&#20934;&#12290;&#22810;&#26657;&#20934;&#36890;&#36807;&#22312;&#28789;&#27963;&#23450;&#20041;&#30340;&#23376;&#32676;&#20307;&#20043;&#38388;&#38480;&#21046;&#26657;&#20934;&#35823;&#24046;&#26469;&#23454;&#29616;&#25972;&#20307;&#26657;&#20934;&#12290;&#28982;&#32780;&#65292;&#22810;&#26657;&#20934;&#27169;&#22411;&#22312;&#20302;&#22522;&#30784;&#29575;&#32676;&#20307;&#20013;&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#26657;&#20934;&#35823;&#24046;&#30334;&#20998;&#27604;&#65292;&#32780;&#22312;&#39640;&#22522;&#30784;&#29575;&#32676;&#20307;&#20013;&#21017;&#36739;&#20302;&#12290;&#22240;&#27492;&#65292;&#20915;&#31574;&#32773;&#21487;&#33021;&#20250;&#23398;&#20064;&#20449;&#20219;&#25110;&#19981;&#20449;&#20219;&#29305;&#23450;&#32676;&#20307;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#27604;&#20363;&#22810;&#26657;&#20934;&#8221;&#26631;&#20934;&#65292;&#35813;&#26631;&#20934;&#38480;&#21046;&#20102;&#32676;&#20307;&#21644;&#39044;&#27979;&#21306;&#38388;&#20869;&#30340;&#26657;&#20934;&#35823;&#24046;&#30334;&#20998;&#27604;&#12290;&#25105;&#20204;&#35777;&#26126;&#28385;&#36275;&#27604;&#20363;&#22810;&#26657;&#20934;&#19981;&#20165;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#22810;&#26657;&#20934;&#65292;&#32780;&#19988;&#36824;&#38480;&#21046;&#20102;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#30452;&#25509;&#34913;&#37327;&#27169;&#22411;&#36817;&#20284;&#20805;&#20998;&#24615;&#30340;&#20844;&#24179;&#26631;&#20934;&#30340;&#8220;&#24046;&#20998;&#26657;&#20934;&#8221;&#12290;&#22240;&#27492;&#65292;&#27604;&#20363;&#26657;&#20934;&#27169;&#22411;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fair calibration is a widely desirable fairness criteria in risk prediction contexts. One way to measure and achieve fair calibration is with multicalibration. Multicalibration constrains calibration error among flexibly-defined subpopulations while maintaining overall calibration. However, multicalibrated models can exhibit a higher percent calibration error among groups with lower base rates than groups with higher base rates. As a result, it is possible for a decision-maker to learn to trust or distrust model predictions for specific groups. To alleviate this, we propose \emph{proportional multicalibration}, a criteria that constrains the percent calibration error among groups and within prediction bins. We prove that satisfying proportional multicalibration bounds a model's multicalibration as well its \emph{differential calibration}, a fairness criteria that directly measures how closely a model approximates sufficiency. Therefore, proportionally calibrated models limit the abilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#26500;&#24314;&#20855;&#26377;&#23567;Lipschitz&#24120;&#25968;&#30340;&#39640;&#25928;&#36890;&#29992;Lipschitz&#32593;&#32476;&#12290;&#35813;&#25216;&#26415;&#20855;&#26377;&#24418;&#24335;&#21270;&#20445;&#35777;&#12289;&#26131;&#20110;&#23454;&#29616;&#21644;&#39640;&#25928;&#36816;&#34892;&#30340;&#20248;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#35757;&#32451;&#30446;&#26631;&#21644;&#20248;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#37325;&#26032;&#32553;&#25918;&#30340;&#26435;&#37325;&#30697;&#38453;&#21442;&#25968;&#21270;&#65292;&#30830;&#20445;&#27599;&#20010;&#32593;&#32476;&#23618;&#30340;Lipschitz&#24120;&#25968;&#26368;&#22823;&#20026;1&#65292;&#24182;&#19988;&#23548;&#33268;&#23398;&#20064;&#30340;&#26435;&#37325;&#30697;&#38453;&#25509;&#36817;&#27491;&#20132;&#12290;</title><link>http://arxiv.org/abs/2208.03160</link><description>&lt;p&gt;
&#39640;&#25928;&#36890;&#29992;&#30340;Lipschitz&#32593;&#32476;&#30340;&#36817;&#27491;&#20132;&#23618;
&lt;/p&gt;
&lt;p&gt;
Almost-Orthogonal Layers for Efficient General-Purpose Lipschitz Networks. (arXiv:2208.03160v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#26500;&#24314;&#20855;&#26377;&#23567;Lipschitz&#24120;&#25968;&#30340;&#39640;&#25928;&#36890;&#29992;Lipschitz&#32593;&#32476;&#12290;&#35813;&#25216;&#26415;&#20855;&#26377;&#24418;&#24335;&#21270;&#20445;&#35777;&#12289;&#26131;&#20110;&#23454;&#29616;&#21644;&#39640;&#25928;&#36816;&#34892;&#30340;&#20248;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#35757;&#32451;&#30446;&#26631;&#21644;&#20248;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#37325;&#26032;&#32553;&#25918;&#30340;&#26435;&#37325;&#30697;&#38453;&#21442;&#25968;&#21270;&#65292;&#30830;&#20445;&#27599;&#20010;&#32593;&#32476;&#23618;&#30340;Lipschitz&#24120;&#25968;&#26368;&#22823;&#20026;1&#65292;&#24182;&#19988;&#23548;&#33268;&#23398;&#20064;&#30340;&#26435;&#37325;&#30697;&#38453;&#25509;&#36817;&#27491;&#20132;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#32593;&#32476;&#26469;&#35828;&#65292;&#33021;&#22815;&#23545;&#23567;&#30340;&#36755;&#20837;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#26159;&#38750;&#24120;&#29702;&#24819;&#30340;&#23646;&#24615;&#12290;&#23454;&#29616;&#36825;&#20010;&#23646;&#24615;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#35774;&#35745;&#20855;&#26377;&#36739;&#23567;&#30340;Lipschitz&#24120;&#25968;&#30340;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#36825;&#26679;&#30340;Lipschitz&#32593;&#32476;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#20855;&#26377;&#22810;&#20010;&#20248;&#28857;&#65306;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32447;&#24615;&#32593;&#32476;&#23618;&#65288;&#20840;&#36830;&#25509;&#25110;&#21367;&#31215;&#65289;&#65292;&#23545;Lipschitz&#24120;&#25968;&#25552;&#20379;&#24418;&#24335;&#21270;&#30340;&#20445;&#35777;&#65292;&#26131;&#20110;&#23454;&#29616;&#21644;&#39640;&#25928;&#36816;&#34892;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#35757;&#32451;&#30446;&#26631;&#21644;&#20248;&#21270;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#25991;&#29486;&#20013;&#31532;&#19968;&#20010;&#21516;&#26102;&#23454;&#29616;&#25152;&#26377;&#36825;&#20123;&#23646;&#24615;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#37325;&#26032;&#32553;&#25918;&#30340;&#26435;&#37325;&#30697;&#38453;&#21442;&#25968;&#21270;&#65292;&#20445;&#35777;&#27599;&#20010;&#32593;&#32476;&#23618;&#30340;Lipschitz&#24120;&#25968;&#26368;&#22823;&#20026;1&#65292;&#24182;&#19988;&#23548;&#33268;&#23398;&#20064;&#30340;&#26435;&#37325;&#30697;&#38453;&#25509;&#36817;&#27491;&#20132;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#23618;&#31216;&#20026;&#20960;&#20046;&#27491;&#20132;Lipschitz&#65288;AOL&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#21644;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
It is a highly desirable property for deep networks to be robust against small input changes. One popular way to achieve this property is by designing networks with a small Lipschitz constant. In this work, we propose a new technique for constructing such Lipschitz networks that has a number of desirable properties: it can be applied to any linear network layer (fully-connected or convolutional), it provides formal guarantees on the Lipschitz constant, it is easy to implement and efficient to run, and it can be combined with any training objective and optimization method. In fact, our technique is the first one in the literature that achieves all of these properties simultaneously. Our main contribution is a rescaling-based weight matrix parametrization that guarantees each network layer to have a Lipschitz constant of at most 1 and results in the learned weight matrices to be close to orthogonal. Hence we call such layers almost-orthogonal Lipschitz (AOL). Experiments and ablation stu
&lt;/p&gt;</description></item><item><title>SeHGNN&#26159;&#19968;&#20010;&#31616;&#21333;&#39640;&#25928;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#22343;&#20540;&#32858;&#21512;&#22120;&#39044;&#20808;&#35745;&#31639;&#37051;&#23621;&#32858;&#21512;&#26469;&#25429;&#25417;&#32467;&#26500;&#20449;&#24687;&#65292;&#37319;&#29992;&#21333;&#23618;&#32467;&#26500;&#21644;&#38271;&#27983;&#35272;&#36335;&#24452;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2207.02547</link><description>&lt;p&gt;
&#31616;&#21333;&#39640;&#25928;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Simple and Efficient Heterogeneous Graph Neural Network. (arXiv:2207.02547v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02547
&lt;/p&gt;
&lt;p&gt;
SeHGNN&#26159;&#19968;&#20010;&#31616;&#21333;&#39640;&#25928;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#22343;&#20540;&#32858;&#21512;&#22120;&#39044;&#20808;&#35745;&#31639;&#37051;&#23621;&#32858;&#21512;&#26469;&#25429;&#25417;&#32467;&#26500;&#20449;&#24687;&#65292;&#37319;&#29992;&#21333;&#23618;&#32467;&#26500;&#21644;&#38271;&#27983;&#35272;&#36335;&#24452;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#33021;&#22815;&#23558;&#24322;&#26500;&#22270;&#30340;&#20016;&#23500;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#23884;&#20837;&#33410;&#28857;&#34920;&#31034;&#20013;&#12290;&#29616;&#26377;&#30340;HGNNs&#20174;&#21516;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#32487;&#25215;&#20102;&#35768;&#22810;&#26426;&#21046;&#65292;&#23588;&#20854;&#26159;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#23618;&#32467;&#26500;&#12290;&#28982;&#32780;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#36825;&#20123;&#26426;&#21046;&#26159;&#21542;&#22312;&#24322;&#26500;&#22270;&#19978;&#30495;&#27491;&#26377;&#25928;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#28145;&#20837;&#32780;&#35814;&#32454;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#39640;&#25928;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SeHGNN&#65289;&#12290;&#20026;&#20102;&#36731;&#26494;&#25429;&#25417;&#32467;&#26500;&#20449;&#24687;&#65292;SeHGNN&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#22343;&#20540;&#32858;&#21512;&#22120;&#39044;&#20808;&#35745;&#31639;&#37051;&#23621;&#32858;&#21512;&#65292;&#36890;&#36807;&#21435;&#38500;&#36807;&#24230;&#20351;&#29992;&#30340;&#37051;&#23621;&#27880;&#24847;&#21147;&#21644;&#36991;&#20813;&#22312;&#27599;&#20010;&#35757;&#32451;&#21608;&#26399;&#20013;&#37325;&#22797;&#36827;&#34892;&#37051;&#23621;&#32858;&#21512;&#26469;&#38477;&#20302;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#65292;SeHGNN&#37319;&#29992;&#20855;&#26377;&#38271;&#27983;&#35272;&#36335;&#24452;&#30340;&#21333;&#23618;&#32467;&#26500;&#26469;&#25193;&#23637;&#24863;&#21463;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graph neural networks (HGNNs) have powerful capability to embed rich structural and semantic information of a heterogeneous graph into node representations. Existing HGNNs inherit many mechanisms from graph neural networks (GNNs) over homogeneous graphs, especially the attention mechanism and the multi-layer structure. These mechanisms bring excessive complexity, but seldom work studies whether they are really effective on heterogeneous graphs. This paper conducts an in-depth and detailed study of these mechanisms and proposes Simple and Efficient Heterogeneous Graph Neural Network (SeHGNN). To easily capture structural information, SeHGNN pre-computes the neighbor aggregation using a light-weight mean aggregator, which reduces complexity by removing overused neighbor attention and avoiding repeated neighbor aggregation in every training epoch. To better utilize semantic information, SeHGNN adopts the single-layer structure with long metapaths to extend the receptive fiel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaserMix&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#28857;&#20113;&#30340;&#24378;&#31354;&#38388;&#32447;&#32034;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#32479;&#35745;&#22522;&#30784;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;SemanticKITTI&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.00026</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;LiDAR&#35821;&#20041;&#20998;&#21106;&#30340;LaserMix&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LaserMix for Semi-Supervised LiDAR Semantic Segmentation. (arXiv:2207.00026v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaserMix&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;LiDAR&#28857;&#20113;&#30340;&#24378;&#31354;&#38388;&#32447;&#32034;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#32479;&#35745;&#22522;&#30784;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;SemanticKITTI&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26631;&#27880;LiDAR&#28857;&#20113;&#32791;&#36153;&#24040;&#22823;&#65292;&#38480;&#21046;&#20102;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LiDAR&#20998;&#21106;&#20013;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24819;&#27861;&#26159;&#21033;&#29992;LiDAR&#28857;&#20113;&#30340;&#24378;&#31354;&#38388;&#32447;&#32034;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LaserMix&#26041;&#27861;&#65292;&#23558;&#26469;&#33258;&#19981;&#21516;LiDAR&#25195;&#25551;&#30340;&#28608;&#20809;&#26463;&#28151;&#21512;&#65292;&#28982;&#21518;&#20419;&#20351;&#27169;&#22411;&#22312;&#28151;&#21512;&#21069;&#21518;&#20570;&#20986;&#19968;&#33268;&#19988;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#19977;&#20010;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#65306;1&#65289;&#36890;&#29992;&#24615;&#65306;LaserMix&#19982;LiDAR&#34920;&#31034;&#65288;&#20363;&#22914;&#65292;&#35270;&#35282;&#21644;&#20307;&#32032;&#65289;&#26080;&#20851;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#21322;&#30417;&#30563;&#26694;&#26550;&#21487;&#20197;&#26222;&#36941;&#24212;&#29992;&#12290;2&#65289;&#32479;&#35745;&#22522;&#30784;&#65306;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#65292;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;3&#65289;&#26377;&#25928;&#24615;&#65306;&#23545;&#27969;&#34892;&#30340;LiDAR&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;nuScenes&#65292;SemanticKITTI&#21644;ScribbleKITTI&#65289;&#30340;&#20840;&#38754;&#23454;&#39564;&#20998;&#26512;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;SemanticKITTI&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Densely annotating LiDAR point clouds is costly, which restrains the scalability of fully-supervised learning methods. In this work, we study the underexplored semi-supervised learning (SSL) in LiDAR segmentation. Our core idea is to leverage the strong spatial cues of LiDAR point clouds to better exploit unlabeled data. We propose LaserMix to mix laser beams from different LiDAR scans, and then encourage the model to make consistent and confident predictions before and after mixing. Our framework has three appealing properties: 1) Generic: LaserMix is agnostic to LiDAR representations (e.g., range view and voxel), and hence our SSL framework can be universally applied. 2) Statistically grounded: We provide a detailed analysis to theoretically explain the applicability of the proposed framework. 3) Effective: Comprehensive experimental analysis on popular LiDAR segmentation datasets (nuScenes, SemanticKITTI, and ScribbleKITTI) demonstrates our effectiveness and superiority. Notably, we
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#21472;&#21152;(FES)&#12290;FES&#36890;&#36807;&#21472;&#21152;&#22810;&#20010;&#20027;&#24178;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#21033;&#29992;&#24322;&#26500;&#39044;&#35757;&#32451;&#30340;&#20027;&#24178;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#32500;&#25252;&#19968;&#20010;&#38656;&#35201;&#37325;&#26032;&#35745;&#31639;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.05831</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21472;&#21152;
&lt;/p&gt;
&lt;p&gt;
Feature Extractor Stacking for Cross-domain Few-shot Meta-learning. (arXiv:2205.05831v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05831
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#21472;&#21152;(FES)&#12290;FES&#36890;&#36807;&#21472;&#21152;&#22810;&#20010;&#20027;&#24178;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#21033;&#29992;&#24322;&#26500;&#39044;&#35757;&#32451;&#30340;&#20027;&#24178;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#32500;&#25252;&#19968;&#20010;&#38656;&#35201;&#37325;&#26032;&#35745;&#31639;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#23569;&#26679;&#26412;&#20803;&#23398;&#20064;(CDFSML)&#35299;&#20915;&#20102;&#38656;&#35201;&#23558;&#30693;&#35782;&#20174;&#22810;&#20010;&#28304;&#39046;&#22495;&#36716;&#31227;&#21040;&#19968;&#20010;&#23454;&#20363;&#31232;&#32570;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#32780;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;CDFSML&#26041;&#27861;&#36890;&#24120;&#26500;&#24314;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#23558;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#21512;&#24182;&#21040;&#19968;&#20010;&#20027;&#24178;&#29305;&#24449;&#25552;&#21462;&#22120;&#20013;&#12290;&#36825;&#26679;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#25512;&#29702;&#65292;&#20294;&#38656;&#35201;&#22312;&#28155;&#21152;&#26032;&#30340;&#28304;&#39046;&#22495;&#26102;&#37325;&#26032;&#35745;&#31639;&#20027;&#24178;&#12290;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#36824;&#19982;&#24322;&#26500;&#28304;&#39046;&#22495;&#20027;&#24178;&#26550;&#26500;&#19981;&#20860;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#24449;&#25552;&#21462;&#22120;&#21472;&#21152;(FES)&#65292;&#19968;&#31181;&#23558;&#26469;&#33258;&#19968;&#32452;&#20027;&#24178;&#30340;&#20449;&#24687;&#36827;&#34892;&#32452;&#21512;&#30340;&#26032;CDFSML&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#24322;&#26500;&#39044;&#35757;&#32451;&#30340;&#20027;&#24178;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#32500;&#25252;&#19968;&#20010;&#38656;&#35201;&#22312;&#20027;&#24178;&#38598;&#21512;&#26356;&#26032;&#26102;&#37325;&#26032;&#35745;&#31639;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#26412;&#30340;FES&#31639;&#27861;&#65292;&#23427;&#21463;&#32463;&#20856;&#21472;&#21152;&#26041;&#27861;&#20803;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain few-shot meta-learning (CDFSML) addresses learning problems where knowledge needs to be transferred from several source domains into an instance-scarce target domain with an explicitly different distribution. Recently published CDFSML methods generally construct a universal model that combines knowledge of multiple source domains into one backbone feature extractor. This enables efficient inference but necessitates re-computation of the backbone whenever a new source domain is added. Some of these methods are also incompatible with heterogeneous source domain backbone architectures. We propose feature extractor stacking (FES), a new CDFSML method for combining information from a collection of backbones, which can utilise heterogeneous pretrained backbones out of the box, and does not maintain a universal model that needs to be re-computed when its backbone collection is updated. We present the basic FES algorithm, which is inspired by the classic stacking approach to meta-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32452;&#32455;&#24418;&#24577;&#23398;&#34920;&#22411;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#35760;&#12289;&#26410;&#27880;&#37322;&#30340;&#30149;&#29702;&#20999;&#29255;&#19978;&#32472;&#21046;&#20986;&#30284;&#30151;&#32452;&#32455;&#30340;&#22320;&#22270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20986;&#19981;&#21516;&#34920;&#22411;&#38388;&#30340;&#29305;&#24449;&#21644;&#36716;&#21464;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2205.01931</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26410;&#26631;&#35760;&#12289;&#26410;&#27880;&#37322;&#30340;&#30149;&#29702;&#20999;&#29255;&#19978;&#32472;&#21046;&#32452;&#32455;&#24418;&#24577;&#23398;&#30284;&#30151;&#34920;&#22411;&#30340;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Mapping the landscape of histomorphological cancer phenotypes using self-supervised learning on unlabeled, unannotated pathology slides. (arXiv:2205.01931v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32452;&#32455;&#24418;&#24577;&#23398;&#34920;&#22411;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#35760;&#12289;&#26410;&#27880;&#37322;&#30340;&#30149;&#29702;&#20999;&#29255;&#19978;&#32472;&#21046;&#20986;&#30284;&#30151;&#32452;&#32455;&#30340;&#22320;&#22270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20986;&#19981;&#21516;&#34920;&#22411;&#38388;&#30340;&#29305;&#24449;&#21644;&#36716;&#21464;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#30340;&#30830;&#35786;&#21644;&#27835;&#30103;&#20381;&#36182;&#20110;&#30149;&#29702;&#23398;&#23478;&#20174;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#36825;&#20123;&#22270;&#20687;&#21253;&#21547;&#22797;&#26434;&#30340;&#20449;&#24687;&#65292;&#38656;&#35201;&#32791;&#26102;&#30340;&#19987;&#23478;&#20154;&#24037;&#35299;&#35835;&#65292;&#23384;&#22312;&#20154;&#20026;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#21463;&#21040;&#27880;&#37322;&#25104;&#26412;&#21644;&#36136;&#37327;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#30417;&#30563;&#26041;&#27861;&#30340;&#36825;&#31181;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32452;&#32455;&#24418;&#24577;&#23398;&#34920;&#22411;&#23398;&#20064; (Histomorphological Phenotype Learning, HPL) &#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#19987;&#23478;&#26631;&#31614;&#25110;&#27880;&#37322;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#23567;&#22270;&#20687;&#22359;&#20013;&#30340;&#21306;&#20998;&#24615;&#22270;&#20687;&#29305;&#24449;&#26469;&#36827;&#34892;&#25805;&#20316;&#12290;&#22270;&#20687;&#22359;&#34987;&#20998;&#32452;&#25104;&#24418;&#24577;&#30456;&#20284;&#30340;&#32858;&#31867;&#65292;&#26500;&#25104;&#32452;&#32455;&#24418;&#24577;&#23398;&#34920;&#22411;&#30340;&#24211;&#65292;&#25581;&#31034;&#20102;&#20174;&#33391;&#24615;&#21040;&#24694;&#24615;&#32452;&#32455;&#32463;&#36807;&#28814;&#30151;&#21644;&#21453;&#24212;&#24615;&#34920;&#22411;&#30340;&#36712;&#36857;&#12290;&#36825;&#20123;&#32858;&#31867;&#20855;&#26377;&#29420;&#29305;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#34987;&#35782;&#21035;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Definitive cancer diagnosis and management depend upon the extraction of information from microscopy images by pathologists. These images contain complex information requiring time-consuming expert human interpretation that is prone to human bias. Supervised deep learning approaches have proven powerful for classification tasks, but they are inherently limited by the cost and quality of annotations used for training these models. To address this limitation of supervised methods, we developed Histomorphological Phenotype Learning (HPL), a fully blue{self-}supervised methodology that requires no expert labels or annotations and operates via the automatic discovery of discriminatory image features in small image tiles. Tiles are grouped into morphologically similar clusters which constitute a library of histomorphological phenotypes, revealing trajectories from benign to malignant tissue via inflammatory and reactive phenotypes. These clusters have distinct features which can be identifie
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;GAN&#29983;&#25104;&#22810;&#26631;&#31614;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;EHR&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#30340;&#32597;&#35265;&#30142;&#30149;&#12290;&#29983;&#25104;&#22120;&#20351;&#29992;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#24179;&#28369;&#26465;&#20214;&#30697;&#38453;&#29983;&#25104;&#24207;&#21015;&#21644;&#32597;&#35265;&#30142;&#30149;&#65292;&#35780;&#35770;&#23478;&#20351;&#29992;Wasserstein&#36317;&#31163;&#35780;&#20998;&#26469;&#21306;&#20998;&#30495;&#23454;&#26679;&#26412;&#21644;&#21512;&#25104;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2204.04797</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;GAN&#29983;&#25104;&#22810;&#26631;&#31614;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Multi-Label Clinical Time-Series Generation via Conditional GAN. (arXiv:2204.04797v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;GAN&#29983;&#25104;&#22810;&#26631;&#31614;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;EHR&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#30340;&#32597;&#35265;&#30142;&#30149;&#12290;&#29983;&#25104;&#22120;&#20351;&#29992;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#21644;&#24179;&#28369;&#26465;&#20214;&#30697;&#38453;&#29983;&#25104;&#24207;&#21015;&#21644;&#32597;&#35265;&#30142;&#30149;&#65292;&#35780;&#35770;&#23478;&#20351;&#29992;Wasserstein&#36317;&#31163;&#35780;&#20998;&#26469;&#21306;&#20998;&#30495;&#23454;&#26679;&#26412;&#21644;&#21512;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#19982;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30456;&#20851;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22914;&#34920;&#31034;&#23398;&#20064;&#21644;&#20020;&#24202;&#20107;&#20214;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#32422;&#26463;&#65292;&#23545;EHR&#30340;&#26377;&#38480;&#35775;&#38382;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#34987;&#25104;&#21151;&#29992;&#20110;&#29983;&#25104;EHR&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#39640;&#36136;&#37327;EHR&#29983;&#25104;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#21253;&#25324;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;EHR&#25968;&#25454;&#21644;&#19981;&#24179;&#34913;&#30340;&#32597;&#35265;&#30142;&#30149;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26631;&#31614;&#26102;&#38388;&#24207;&#21015;GAN&#65288;MTGAN&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;EHR&#24182;&#21516;&#26102;&#25552;&#39640;&#32597;&#35265;&#30142;&#30149;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;MTGAN&#30340;&#29983;&#25104;&#22120;&#20351;&#29992;&#24102;&#26377;&#24179;&#28369;&#26465;&#20214;&#30697;&#38453;&#30340;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#29983;&#25104;&#24207;&#21015;&#21644;&#32597;&#35265;&#30142;&#30149;&#12290;&#35780;&#35770;&#23478;&#20351;&#29992;Wasserstein&#36317;&#31163;&#32473;&#20986;&#24471;&#20998;&#65292;&#36890;&#36807;&#32771;&#34385;&#25968;&#25454;&#21644;&#26102;&#38388;&#29305;&#24449;&#26469;&#35782;&#21035;&#30495;&#23454;&#26679;&#26412;&#21644;&#21512;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has been successfully adopted in a wide range of applications related to electronic health records (EHRs) such as representation learning and clinical event prediction. However, due to privacy constraints, limited access to EHR becomes a bottleneck for deep learning research. To mitigate these concerns, generative adversarial networks (GANs) have been successfully used for generating EHR data. However, there are still challenges in high-quality EHR generation, including generating time-series EHR data and imbalanced uncommon diseases. In this work, we propose a Multi-label Time-series GAN (MTGAN) to generate EHR and simultaneously improve the quality of uncommon disease generation. The generator of MTGAN uses a gated recurrent unit (GRU) with a smooth conditional matrix to generate sequences and uncommon diseases. The critic gives scores using Wasserstein distance to recognize real samples from synthetic samples by considering both data and temporal featu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#37325;&#20998;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#35835;&#25991;&#26412;&#20013;&#38544;&#34255;&#30340;&#22810;&#37325;&#20998;&#24418;&#23646;&#24615;&#65292;&#24182;&#32467;&#21512;&#25552;&#20986;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20013;&#23454;&#29616;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36755;&#12290;</title><link>http://arxiv.org/abs/2111.13861</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#22810;&#37325;&#20998;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
A New Multifractal-based Deep Learning Model for Text Mining. (arXiv:2111.13861v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.13861
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#37325;&#20998;&#24418;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#35835;&#25991;&#26412;&#20013;&#38544;&#34255;&#30340;&#22810;&#37325;&#20998;&#24418;&#23646;&#24615;&#65292;&#24182;&#32467;&#21512;&#25552;&#20986;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20013;&#23454;&#29616;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#20805;&#28385;&#19981;&#30830;&#23450;&#24615;&#30340;&#19990;&#30028;&#20013;&#65292;&#23384;&#22312;&#30340;&#32441;&#29702;&#32534;&#32455;&#20986;&#22797;&#26434;&#30340;&#27169;&#24335;&#65292;&#22810;&#37325;&#20998;&#24418;&#25104;&#20026;&#27934;&#23519;&#21147;&#30340;&#26631;&#24535;&#65292;&#29031;&#20142;&#23427;&#20204;&#12290;&#24403;&#25105;&#20204;&#28145;&#20837;&#25506;&#32034;&#26500;&#25104;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#21644;&#26234;&#33021;&#26381;&#21153;&#30340;&#25991;&#26412;&#25366;&#25496;&#39046;&#22495;&#26102;&#65292;&#25105;&#20204;&#24847;&#35782;&#21040;&#22312;&#25991;&#26412;&#30340;&#38754;&#32433;&#21518;&#38754;&#38544;&#34255;&#30528;&#20154;&#31867;&#24605;&#24819;&#21644;&#35748;&#30693;&#30340;&#34920;&#29616;&#65292;&#19982;&#22797;&#26434;&#24615;&#32039;&#23494;&#30456;&#20114;&#20132;&#32455;&#12290;&#22312;&#23558;&#25991;&#26412;&#35270;&#20026;&#22797;&#26434;&#31995;&#32479;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#25581;&#31034;&#20854;&#20013;&#38544;&#34255;&#30340;&#23453;&#34255;&#65292;&#20511;&#21161;&#25552;&#20986;&#30340;&#22810;&#37325;&#20998;&#24418;&#26041;&#27861;&#35299;&#35835;&#23884;&#20837;&#22312;&#25991;&#26412;&#26223;&#35266;&#20013;&#30340;&#22810;&#37325;&#20998;&#24418;&#23646;&#24615;&#12290;&#36825;&#19968;&#21162;&#21147;&#26368;&#32456;&#23381;&#32946;&#20986;&#25105;&#20204;&#30340;&#26032;&#39062;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36824;&#21033;&#29992;&#20102;&#25552;&#20986;&#30340;&#28608;&#27963;&#20989;&#25968;&#30340;&#21147;&#37327;&#65292;&#22312;&#20854;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20013;&#23454;&#29616;&#38750;&#32447;&#24615;&#20449;&#24687;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this world full of uncertainty, where the fabric of existence weaves patterns of complexity, multifractal emerges as beacons of insight, illuminating them. As we delve into the realm of text mining that underpins various natural language processing applications and powers a range of intelligent services, we recognize that behind the veil of text lies a manifestation of human thought and cognition, intricately intertwined with the complexities. Building upon the foundation of perceiving text as a complex system, this study embarks on a journey to unravel the hidden treasures within, armed with the proposed multifractal method that deciphers the multifractal attributes embedded within the text landscape. This endeavor culminates in the birth of our novel model, which also harnesses the power of the proposed activation function to facilitate nonlinear information transmission within its neural network architecture. The success on experiments anchored in real-world technical reports cov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65292;&#21457;&#29616;&#37027;&#20123;&#22312;&#19981;&#20351;&#29992;SSL&#26102;&#20855;&#26377;&#26356;&#39640;&#22522;&#20934;&#20934;&#30830;&#24615;&#30340;&#23376;&#26063;&#32676;&#26356;&#23481;&#26131;&#20174;SSL&#20013;&#33719;&#30410;&#65292;&#32780;&#37027;&#20123;&#22522;&#20934;&#20934;&#30830;&#24615;&#36739;&#20302;&#30340;&#23376;&#26063;&#32676;&#22312;&#28155;&#21152;SSL&#27169;&#22359;&#21518;&#21487;&#33021;&#20250;&#35266;&#23519;&#21040;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2110.06282</link><description>&lt;p&gt;
&#23500;&#32773;&#24840;&#23500;: &#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Rich Get Richer: Disparate Impact of Semi-Supervised Learning. (arXiv:2110.06282v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65292;&#21457;&#29616;&#37027;&#20123;&#22312;&#19981;&#20351;&#29992;SSL&#26102;&#20855;&#26377;&#26356;&#39640;&#22522;&#20934;&#20934;&#30830;&#24615;&#30340;&#23376;&#26063;&#32676;&#26356;&#23481;&#26131;&#20174;SSL&#20013;&#33719;&#30410;&#65292;&#32780;&#37027;&#20123;&#22522;&#20934;&#20934;&#30830;&#24615;&#36739;&#20302;&#30340;&#23376;&#26063;&#32676;&#22312;&#28155;&#21152;SSL&#27169;&#22359;&#21518;&#21487;&#33021;&#20250;&#35266;&#23519;&#21040;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#39640;&#36136;&#37327;&#30417;&#30563;&#25968;&#25454;&#20005;&#37325;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24050;&#32463;&#23637;&#31034;&#20102;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#24448;&#24448;&#21487;&#20197;&#30830;&#31435;&#25972;&#20010;&#25968;&#25454;&#26063;&#32676;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#24471;&#21040;&#25913;&#21892;&#65292;&#20294;SSL&#22312;&#19981;&#21516;&#23376;&#26063;&#32676;&#20013;&#30340;&#34920;&#29616;&#23578;&#19981;&#28165;&#26970;&#12290;&#24403;&#25105;&#20204;&#24076;&#26395;&#20844;&#24179;&#23545;&#24453;&#30340;&#20154;&#21475;&#32676;&#20307;&#30001;&#20154;&#21475;&#32479;&#35745;&#23398;&#20998;&#32452;&#23450;&#20041;&#26102;&#65292;&#29702;&#35299;&#19978;&#36848;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#30340;&#20844;&#24179;&#24615;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#37096;&#32626;SSL&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65306;&#37027;&#20123;&#22312;&#19981;&#20351;&#29992;SSL&#26102;&#20855;&#26377;&#26356;&#39640;&#22522;&#20934;&#20934;&#30830;&#24615;&#30340;&#23376;&#26063;&#32676;&#65288;"&#23500;&#35029;"&#23376;&#26063;&#32676;&#65289;&#24448;&#24448;&#20174;SSL&#20013;&#33719;&#30410;&#26356;&#22810;&#65307;&#32780;&#37027;&#20123;&#22312;&#22522;&#20934;&#20934;&#30830;&#24615;&#36739;&#20302;&#65288;"&#36139;&#31351;"&#65289;&#30340;&#23376;&#26063;&#32676;&#22312;&#28155;&#21152;SSL&#27169;&#22359;&#21518;&#29978;&#33267;&#21487;&#33021;&#35266;&#23519;&#21040;&#24615;&#33021;&#19979;&#38477;&#12290;&#25105;&#20204;&#22312;&#19968;&#31867;&#24191;&#27867;&#30340;SSL&#31639;&#27861;&#19978;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#35777;&#23454;&#20102;&#19978;&#36848;&#35266;&#23519;&#65292;&#36825;&#20123;&#31639;&#27861;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#20351;&#29992;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) has demonstrated its potential to improve the model accuracy for a variety of learning tasks when the high-quality supervised data is severely limited. Although it is often established that the average accuracy for the entire population of data is improved, it is unclear how SSL fares with different sub-populations. Understanding the above question has substantial fairness implications when different sub-populations are defined by the demographic groups that we aim to treat fairly. In this paper, we reveal the disparate impacts of deploying SSL: the sub-population who has a higher baseline accuracy without using SSL (the "rich" one) tends to benefit more from SSL; while the sub-population who suffers from a low baseline accuracy (the "poor" one) might even observe a performance drop after adding the SSL module. We theoretically and empirically establish the above observation for a broad family of SSL algorithms, which either explicitly or implicitly use a
&lt;/p&gt;</description></item><item><title>&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#39564;&#35777;&#20102;&#22122;&#22768;&#30340;&#23384;&#22312;&#30830;&#23454;&#20250;&#25913;&#21464;&#27880;&#37322;&#26631;&#31614;&#65292;&#24182;&#23637;&#31034;&#20102;&#24573;&#35270;&#36825;&#19968;&#30693;&#35782;&#20250;&#23545;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#35780;&#20272;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2104.08806</link><description>&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#22686;&#24378;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#20197;&#25552;&#39640;&#21487;&#37096;&#32626;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Best Practices for Noise-Based Augmentation to Improve the Performance of Deployable Speech-Based Emotion Recognition Systems. (arXiv:2104.08806v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.08806
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#39564;&#35777;&#20102;&#22122;&#22768;&#30340;&#23384;&#22312;&#30830;&#23454;&#20250;&#25913;&#21464;&#27880;&#37322;&#26631;&#31614;&#65292;&#24182;&#23637;&#31034;&#20102;&#24573;&#35270;&#36825;&#19968;&#30693;&#35782;&#20250;&#23545;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#35780;&#20272;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26159;&#20219;&#20309;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20294;&#19968;&#20010;&#20154;&#20135;&#29983;&#21644;&#24863;&#30693;&#30340;&#35821;&#38899;&#29305;&#24449;&#21487;&#33021;&#21463;&#21040;&#22810;&#31181;&#21407;&#22240;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#21487;&#21462;&#30340;&#24773;&#24863;&#21644;&#19981;&#21487;&#21462;&#30340;&#22122;&#22768;&#12290;&#20026;&#20102;&#35757;&#32451;&#31283;&#20581;&#30340;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#22823;&#32780;&#30495;&#23454;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#20294;&#24773;&#24863;&#25968;&#25454;&#38598;&#24448;&#24448;&#24456;&#23567;&#65292;&#22240;&#27492;&#38656;&#35201;&#22686;&#21152;&#22122;&#22768;&#12290;&#36890;&#24120;&#65292;&#22122;&#22768;&#22686;&#24378;&#20250;&#20570;&#20986;&#19968;&#20010;&#37325;&#35201;&#30340;&#20551;&#35774;&#65292;&#21363;&#22312;&#26377;&#25110;&#26080;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#26631;&#31614;&#24212;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#23545;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26469;&#35828;&#26159;&#27491;&#30830;&#30340;&#65292;&#20294;&#23545;&#20110;&#22522;&#20110;&#24863;&#30693;&#30340;&#20219;&#21153;&#26469;&#35828;&#21364;&#26410;&#24517;&#27491;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#19977;&#20010;&#21019;&#26032;&#36129;&#29486;&#12290;&#25105;&#20204;&#36890;&#36807;&#20247;&#21253;&#39564;&#35777;&#20102;&#22122;&#22768;&#30340;&#23384;&#22312;&#30830;&#23454;&#20250;&#25913;&#21464;&#27880;&#37322;&#26631;&#31614;&#65292;&#20174;&#32780;&#21487;&#33021;&#25913;&#21464;&#21407;&#22987;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24573;&#35270;&#36825;&#19968;&#30693;&#35782;&#24182;&#20551;&#35774;&#30495;&#23454;&#26631;&#31614;&#30340;&#19968;&#33268;&#24615;&#20250;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19979;&#28216;&#35780;&#20272;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition is an important component of any human centered system. But speech characteristics produced and perceived by a person can be influenced by a multitude of reasons, both desirable such as emotion, and undesirable such as noise. To train robust emotion recognition models, we need a large, yet realistic data distribution, but emotion datasets are often small and hence are augmented with noise. Often noise augmentation makes one important assumption, that the prediction label should remain the same in presence or absence of noise, which is true for automatic speech recognition but not necessarily true for perception based tasks. In this paper we make three novel contributions. We validate through crowdsourcing that the presence of noise does change the annotation label and hence may alter the original ground truth label. We then show how disregarding this knowledge and assuming consistency in ground truth labels propagates to downstream evaluation of ML models, bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#27604;&#36739;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#21435;&#20559;&#20272;&#35745;&#22120;&#22312;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#21435;&#20559;&#20272;&#35745;&#22120;&#19982;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#19968;&#26679;&#22909;&#65292;&#22312;&#26679;&#26412;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#26041;&#27861;&#26356;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2103.11749</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#19982;&#21435;&#20559;&#20272;&#35745;&#22120;&#22312;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#20013;&#30340;&#27169;&#25311;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Simulation comparisons between Bayesian and de-biased estimators in low-rank matrix completion. (arXiv:2103.11749v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.11749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27169;&#25311;&#27604;&#36739;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#21435;&#20559;&#20272;&#35745;&#22120;&#22312;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#21435;&#20559;&#20272;&#35745;&#22120;&#19982;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#19968;&#26679;&#22909;&#65292;&#22312;&#26679;&#26412;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#26041;&#27861;&#26356;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31867;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#26088;&#22312;&#39044;&#27979;&#37096;&#20998;&#35266;&#23519;&#30697;&#38453;&#20013;&#30340;&#32570;&#22833;&#26465;&#30446;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#21327;&#21516;&#36807;&#28388;&#12289;&#22270;&#20687;&#22788;&#29702;&#21644;&#22522;&#22240;&#22411;&#25554;&#34917;&#31561;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24212;&#29992;&#20013;&#20986;&#29616;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#21644;&#26368;&#36817;&#24341;&#20837;&#30340;&#21435;&#20559;&#20272;&#35745;&#22120;&#65292;&#35813;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#29992;&#30340;&#26041;&#24335;&#26469;&#26500;&#24314;&#24863;&#20852;&#36259;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#21435;&#20559;&#20272;&#35745;&#22120;&#20855;&#26377;&#23574;&#38160;&#30340;&#26497;&#23567;&#26368;&#20248;&#20272;&#35745;&#35823;&#24046;&#36895;&#29575;&#65292;&#32780;&#36125;&#21494;&#26031;&#26041;&#27861;&#36890;&#36807;&#39069;&#22806;&#30340;&#23545;&#25968;&#22240;&#23376;&#36798;&#21040;&#20102;&#36825;&#19968;&#36895;&#29575;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#30740;&#31350;&#26174;&#31034;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#65292;&#21363;&#21435;&#20559;&#20272;&#35745;&#22120;&#19982;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#19968;&#26679;&#22909;&#12290;&#27492;&#22806;&#65292;&#36125;&#21494;&#26031;&#26041;&#27861;&#26356;&#21152;&#31283;&#23450;&#65292;&#22312;&#26679;&#26412;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#20248;&#20110;&#21435;&#20559;&#20272;&#35745;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#32622;&#20449;&#21306;&#38388;&#30340;&#32463;&#39564;&#35206;&#30422;&#29575;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the low-rank matrix completion problem, a class of machine learning problems, that aims at the prediction of missing entries in a partially observed matrix. Such problems appear in several challenging applications such as collaborative filtering, image processing, and genotype imputation. We compare the Bayesian approaches and a recently introduced de-biased estimator which provides a useful way to build confidence intervals of interest. From a theoretical viewpoint, the de-biased estimator comes with a sharp minimax-optimal rate of estimation error whereas the Bayesian approach reaches this rate with an additional logarithmic factor. Our simulation studies show originally interesting results that the de-biased estimator is just as good as the Bayesian estimators. Moreover, Bayesian approaches are much more stable and can outperform the de-biased estimator in the case of small samples. In addition, we also find that the empirical coverage rate of the confidence 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#24352;&#37327;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20803;&#22270;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#35299;&#37322;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#26144;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;&#30340;&#25299;&#25169;&#24863;&#30693;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2101.01078</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#30340;&#24352;&#37327;&#20998;&#35299;&#29992;&#20110;&#20803;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Topology-aware Tensor Decomposition for Meta-graph Learning. (arXiv:2101.01078v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.01078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#24352;&#37327;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20803;&#22270;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#35299;&#37322;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#26144;&#26377;&#21521;&#26080;&#29615;&#22270;&#32467;&#26500;&#30340;&#25299;&#25169;&#24863;&#30693;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#36890;&#24120;&#25351;&#20855;&#26377;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#30340;&#22270;&#12290;&#20174;&#24322;&#26500;&#22270;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#20803;&#22270;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#19982;&#24322;&#26500;&#22270;&#20855;&#26377;&#30456;&#21516;&#33410;&#28857;&#21644;&#36793;&#31867;&#22411;&#30340;&#29305;&#27530;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#21512;&#36866;&#30340;&#20803;&#22270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26368;&#36817;&#65292;&#26377;&#35768;&#22810;&#20851;&#20110;&#20174;&#24322;&#26500;&#22270;&#20013;&#23398;&#20064;&#36866;&#24403;&#30340;&#20803;&#22270;&#30340;&#30740;&#31350;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24341;&#20837;&#29420;&#31435;&#20110;&#24444;&#27492;&#30340;&#36793;&#30340;&#36830;&#32493;&#26435;&#37325;&#65292;&#24573;&#30053;&#20102;&#20803;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#21487;&#33021;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24352;&#37327;&#30340;&#26032;&#35270;&#35282;&#26469;&#23398;&#20064;&#20803;&#22270;&#12290;&#36825;&#20010;&#35270;&#35282;&#19981;&#20165;&#26377;&#21161;&#20110;&#35299;&#37322;&#29616;&#26377;&#24037;&#20316;&#20013;CANDECOMP/PARAFAC&#65288;CP&#65289;&#20998;&#35299;&#30340;&#23616;&#38480;&#24615;&#65292;&#32780;&#19988;&#28608;&#21457;&#20102;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#26144;DAG&#32467;&#26500;&#30340;&#25299;&#25169;&#24863;&#30693;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#31216;&#20026;TENSUS&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graphs generally refers to graphs with different types of nodes and edges. A common approach for extracting useful information from heterogeneous graphs is to use meta-graphs, which can be seen as a special kind of directed acyclic graph (DAG) with same node and edge types as the heterogeneous graph. However, how to design proper meta-graphs is challenging. Recently, there have been many works on learning suitable meta-graphs from a heterogeneous graph. Existing methods generally introduce continuous weights for edges that are independent of each other, which ignores the topological stucture of meta-graphs and can be ineffective. To address this issue, we propose a new viewpoint from tensor on learning meta-graphs. Such a viewpoint not only helps interpret the limitation of existing works by CANDECOMP/PARAFAC (CP) decomposition, but also inspires us to propose a topology-aware tensor decomposition, called TENSUS, that reflects the structure of DAGs. The proposed topology-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#21098;&#36753;&#24314;&#27169;&#20026;&#19968;&#20010;&#26102;&#31354;&#20307;&#65292;&#22312;&#19968;&#20010;&#38454;&#27573;&#20869;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#21644;&#36319;&#36394;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22810;&#38454;&#27573;&#31649;&#36947;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26102;&#31354;&#23884;&#20837;&#30340;&#27010;&#24565;&#65292;&#24182;&#20351;&#29992;&#22686;&#24378;&#29305;&#24449;&#34920;&#31034;&#30340;&#28151;&#21512;&#20989;&#25968;&#26469;&#32858;&#31867;&#20687;&#32032;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#23454;&#20363;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2003.08429</link><description>&lt;p&gt;
STEm-Seg: &#29992;&#20110;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#30340;&#26102;&#31354;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos. (arXiv:2003.08429v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.08429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#21098;&#36753;&#24314;&#27169;&#20026;&#19968;&#20010;&#26102;&#31354;&#20307;&#65292;&#22312;&#19968;&#20010;&#38454;&#27573;&#20869;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#21644;&#36319;&#36394;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#22810;&#38454;&#27573;&#31649;&#36947;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26102;&#31354;&#23884;&#20837;&#30340;&#27010;&#24565;&#65292;&#24182;&#20351;&#29992;&#22686;&#24378;&#29305;&#24449;&#34920;&#31034;&#30340;&#28151;&#21512;&#20989;&#25968;&#26469;&#32858;&#31867;&#20687;&#32032;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#23454;&#20363;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#22810;&#38454;&#27573;&#31649;&#36947;&#65292;&#36981;&#24490;&#36319;&#36394;-&#26816;&#27979;&#33539;&#24335;&#65292;&#24182;&#23558;&#35270;&#39057;&#21098;&#36753;&#24314;&#27169;&#20026;&#22270;&#20687;&#24207;&#21015;&#12290;&#22810;&#20010;&#32593;&#32476;&#29992;&#20110;&#26816;&#27979;&#21333;&#20010;&#24103;&#20013;&#30340;&#23545;&#35937;&#65292;&#28982;&#21518;&#22312;&#26102;&#38388;&#19978;&#20851;&#32852;&#36825;&#20123;&#26816;&#27979;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#19981;&#26159;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#39640;&#24230;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#28041;&#21450;&#35270;&#39057;&#20013;&#30340;&#23454;&#20363;&#20998;&#21106;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;&#35270;&#39057;&#21098;&#36753;&#24314;&#27169;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;3D&#26102;&#31354;&#20307;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;&#19968;&#20010;&#38454;&#27573;&#20869;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#36827;&#34892;&#23454;&#20363;&#20998;&#21106;&#21644;&#36319;&#36394;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#24418;&#24335;&#21270;&#22260;&#32469;&#26102;&#31354;&#23884;&#20837;&#30340;&#24605;&#24819;&#65292;&#36825;&#20123;&#23884;&#20837;&#34987;&#35757;&#32451;&#20026;&#22312;&#25972;&#20010;&#35270;&#39057;&#21098;&#36753;&#20013;&#32858;&#31867;&#23646;&#20110;&#29305;&#23450;&#23545;&#35937;&#23454;&#20363;&#30340;&#20687;&#32032;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22686;&#24378;&#26102;&#31354;&#29305;&#24449;&#34920;&#31034;&#30340;&#26032;&#22411;&#28151;&#21512;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods for instance segmentation in videos typically involve multi-stage pipelines that follow the tracking-by-detection paradigm and model a video clip as a sequence of images. Multiple networks are used to detect objects in individual frames, and then associate these detections over time. Hence, these methods are often non-end-to-end trainable and highly tailored to specific tasks. In this paper, we propose a different approach that is well-suited to a variety of tasks involving instance segmentation in videos. In particular, we model a video clip as a single 3D spatio-temporal volume, and propose a novel approach that segments and tracks instances across space and time in a single stage. Our problem formulation is centered around the idea of spatio-temporal embeddings which are trained to cluster pixels belonging to a specific object instance over an entire video clip. To this end, we introduce (i) novel mixing functions that enhance the feature representation of spatio-te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;&#31995;&#32479;&#32423;&#21644;&#31639;&#27861;&#32423;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2003.06307</link><description>&lt;p&gt;
&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Distributed Deep Learning: A Comprehensive Survey. (arXiv:2003.06307v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.06307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;&#31995;&#32479;&#32423;&#21644;&#31639;&#27861;&#32423;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#22810;&#20010;&#35745;&#31639;&#35774;&#22791;&#65288;&#22914;GPU/TPU&#65289;&#36827;&#34892;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#30340;&#19968;&#31181;&#26222;&#36941;&#26041;&#27861;&#65292;&#36825;&#26159;&#22240;&#20026;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#36234;&#26469;&#36234;&#22823;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36890;&#20449;&#25104;&#20026;&#24615;&#33021;&#29942;&#39048;&#65292;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#35299;&#20915;&#36825;&#20010;&#36890;&#20449;&#38382;&#39064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#31639;&#27861;&#30340;&#32508;&#36848;&#65292;&#37325;&#28857;&#20851;&#27880;&#31995;&#32479;&#32423;&#21644;&#31639;&#27861;&#32423;&#30340;&#20248;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#36890;&#20449;&#21516;&#27493;&#12289;&#31995;&#32479;&#26550;&#26500;&#12289;&#21387;&#32553;&#25216;&#26415;&#21644;&#36890;&#20449;&#21644;&#35745;&#31639;&#20219;&#21153;&#24182;&#34892;&#21270;&#31561;&#22235;&#20010;&#20027;&#35201;&#32500;&#24230;&#32435;&#20837;&#30340;&#25968;&#25454;&#24182;&#34892;&#20998;&#24067;&#24335;&#35757;&#32451;&#31639;&#27861;&#20998;&#31867;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#22235;&#20010;&#26041;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#20197;&#20102;&#35299;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#27604;&#20102;&#19981;&#21516;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#20197;&#20102;&#35299;&#20854;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed deep learning (DL) has become prevalent in recent years to reduce training time by leveraging multiple computing devices (e.g., GPUs/TPUs) due to larger models and datasets. However, system scalability is limited by communication becoming the performance bottleneck. Addressing this communication issue has become a prominent research topic. In this paper, we provide a comprehensive survey of the communication-efficient distributed training algorithms, focusing on both system-level and algorithmic-level optimizations. We first propose a taxonomy of data-parallel distributed training algorithms that incorporates four primary dimensions: communication synchronization, system architectures, compression techniques, and parallelism of communication and computing tasks. We then investigate state-of-the-art studies that address problems in these four dimensions. We also compare the convergence rates of different algorithms to understand their convergence speed. Additionally, we cond
&lt;/p&gt;</description></item></channel></rss>