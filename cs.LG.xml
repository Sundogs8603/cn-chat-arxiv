<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>DreamLLM&#26159;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;&#29983;&#25104;&#35821;&#35328;&#21644;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;DreamLLM&#33021;&#22815;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#20132;&#32455;&#20869;&#23481;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11499</link><description>&lt;p&gt;
DreamLLM&#65306;&#21327;&#21516;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
DreamLLM: Synergistic Multimodal Comprehension and Creation. (arXiv:2309.11499v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11499
&lt;/p&gt;
&lt;p&gt;
DreamLLM&#26159;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;&#29983;&#25104;&#35821;&#35328;&#21644;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;DreamLLM&#33021;&#22815;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#20132;&#32455;&#20869;&#23481;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DreamLLM&#65292;&#19968;&#31181;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#39318;&#27425;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#21033;&#29992;&#20102;&#22810;&#27169;&#24577;&#29702;&#35299;&#19982;&#21019;&#20316;&#20043;&#38388;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#21327;&#21516;&#25928;&#24212;&#12290;DreamLLM&#36981;&#24490;&#20004;&#20010;&#22522;&#26412;&#21407;&#21017;&#12290;&#31532;&#19968;&#20010;&#21407;&#21017;&#19987;&#27880;&#20110;&#36890;&#36807;&#22312;&#21407;&#22987;&#22810;&#27169;&#24577;&#31354;&#38388;&#20013;&#36827;&#34892;&#30452;&#25509;&#37319;&#26679;&#26469;&#29983;&#25104;&#35821;&#35328;&#21644;&#22270;&#20687;&#21518;&#39564;&#30340;&#29983;&#25104;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#20687;CLIP&#36825;&#26679;&#30340;&#22806;&#37096;&#29305;&#24449;&#25552;&#21462;&#22120;&#25152;&#22266;&#26377;&#30340;&#38480;&#21046;&#21644;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#33719;&#24471;&#20102;&#26356;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;&#20854;&#27425;&#65292;DreamLLM&#20419;&#36827;&#20102;&#21407;&#22987;&#30340;&#12289;&#20132;&#32455;&#30340;&#25991;&#20214;&#29983;&#25104;&#65292;&#23545;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#20197;&#21450;&#38750;&#32467;&#26500;&#21270;&#24067;&#23616;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;DreamLLM&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#25152;&#26377;&#26465;&#20214;&#12289;&#36793;&#32536;&#21644;&#32852;&#21512;&#22810;&#27169;&#24577;&#20998;&#24067;&#12290;&#20316;&#20026;&#32467;&#26524;&#65292;DreamLLM&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#33258;&#30001;&#24418;&#24335;&#20132;&#32455;&#20869;&#23481;&#30340;MLLM&#12290;&#32508;&#21512;&#23454;&#39564;&#31361;&#26174;&#20102;DreamLLM&#20316;&#20026;&#38646;&#26679;&#26412;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal
&lt;/p&gt;</description></item><item><title>Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11489</link><description>&lt;p&gt;
Text2Reward&#65306;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11489
&lt;/p&gt;
&lt;p&gt;
Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#65307;&#23427;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#25110;&#39046;&#22495;&#25968;&#25454;&#65292;&#23548;&#33268;&#24320;&#21457;&#25104;&#26412;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text2Reward&#65292;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21487;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#30446;&#26631;&#65292;Text2Reward&#29983;&#25104;&#20316;&#20026;&#29615;&#22659;&#32039;&#20945;&#34920;&#31034;&#30340;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#36817;&#20351;&#29992;LLM&#32534;&#20889;&#31232;&#30095;&#22870;&#21169;&#20195;&#30721;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;Text2Reward&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20195;&#30721;&#65292;&#21487;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#29616;&#26377;&#36719;&#20214;&#21253;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;ManiSkill2&#65292;MetaWorld&#65289;&#21644;&#20004;&#20010;MuJoCo&#30340;&#36816;&#21160;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;Text2Reward&#12290;&#22312;17&#20010;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;13&#20010;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#22870;&#21169;&#20195;&#30721;&#35757;&#32451;&#30340;&#25919;&#31574;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#26497;&#28857;&#26469;&#36991;&#20813;&#26681;&#26597;&#25214;&#21644;&#20248;&#21270;&#20013;&#19981;&#38656;&#35201;&#30340;&#28857;&#65292;&#26041;&#27861;&#26159;&#23558;&#20195;&#20215;&#20989;&#25968;&#38500;&#20197;&#21040;&#30446;&#26631;&#28857;&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#36866;&#24403;&#24130;&#12290;</title><link>http://arxiv.org/abs/2309.11475</link><description>&lt;p&gt;
&#36991;&#20813;&#22312;&#26681;&#26597;&#25214;&#21644;&#20248;&#21270;&#20013;&#20986;&#29616;&#19981;&#38656;&#35201;&#30340;&#28857;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#22686;&#21152;&#26497;&#28857;
&lt;/p&gt;
&lt;p&gt;
Multiplying poles to avoid unwanted points in root finding and optimization. (arXiv:2309.11475v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11475
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#26497;&#28857;&#26469;&#36991;&#20813;&#26681;&#26597;&#25214;&#21644;&#20248;&#21270;&#20013;&#19981;&#38656;&#35201;&#30340;&#28857;&#65292;&#26041;&#27861;&#26159;&#23558;&#20195;&#20215;&#20989;&#25968;&#38500;&#20197;&#21040;&#30446;&#26631;&#28857;&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#36866;&#24403;&#24130;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26681;&#26597;&#25214;&#21644;&#20248;&#21270;&#20013;&#65292;&#23384;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#33021;&#26080;&#27861;&#20445;&#35777;&#33258;&#24049;&#36873;&#25321;&#30340;&#26041;&#27861;&#26500;&#36896;&#30340;&#24207;&#21015;&#25910;&#25947;&#20110;&#19968;&#20010;&#38381;&#38598;&#21512;A&#65288;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24182;&#19981;&#20551;&#35774;A&#26377;&#20854;&#20182;&#38468;&#21152;&#23646;&#24615;&#65292;&#22914;&#20984;&#24615;&#25110;&#36830;&#36890;&#24615;&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24076;&#26395;&#26377;&#19968;&#20010;&#26426;&#21046;&#26469;&#36991;&#20813;&#22312;&#31639;&#27861;&#30340;&#19979;&#19968;&#27425;&#36816;&#34892;&#20013;&#20877;&#27425;&#36935;&#21040;&#36825;&#20010;&#28857;z*&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;&#25105;&#20204;&#23558;&#20195;&#20215;&#20989;&#25968;&#38500;&#20197;&#21040;A&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#36866;&#24403;&#24130;&#12290;&#36825;&#20010;&#24819;&#27861;&#21463;&#21040;&#20102;&#22312;&#19968;&#32500;&#20989;&#25968;&#20013;&#23581;&#35797;&#25214;&#21040;&#25152;&#26377;&#26681;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#37322;&#20102;&#22312;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#24688;&#22909;&#20026;0&#30340;&#24773;&#20917;&#19979;&#36825;&#31181;&#26041;&#27861;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#28982;&#21518;&#35299;&#37322;&#20102;&#22914;&#26524;&#26368;&#23567;&#20540;&#19981;&#20026;&#38646;&#35813;&#22914;&#20309;&#36827;&#34892;&#65288;&#21516;&#26102;&#20801;&#35768;&#27491;&#30340;&#26368;&#23567;&#20540;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In root finding and optimization, there are many cases where there is a closed set $A$ one does not the sequence constructed by one's favourite method will converge to A (here, we do not assume extra properties on $A$ such as being convex or connected). For example, if one wants to find roots, and one chooses initial points in the basin of attraction for 1 root $x^*$ (a fact which one may not know before hand), then one will always end up in that root. In this case, one would like to have a mechanism to avoid this point $z^*$ in the next runs of one's algorithm.  In this paper, we propose a new method aiming to achieve this: we divide the cost function by an appropriate power of the distance function to $A$. This idea is inspired by how one would try to find all roots of a function in 1 variable. We first explain the heuristic for this method in the case where the minimum of the cost function is exactly 0, and then explain how to proceed if the minimum is non-zero (allowing both positi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#37096;&#20998;&#35266;&#27979;&#29366;&#24577;&#26469;&#25511;&#21046;&#20855;&#26377;&#20004;&#33218;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20648;&#23618;&#35745;&#31639;&#21644;&#38543;&#26426;&#36755;&#20837;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#22810;&#31181;&#21608;&#26399;&#24615;&#21644;&#28151;&#27788;&#20449;&#21495;&#30340;&#36319;&#36394;&#25511;&#21046;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11470</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#36319;&#36394;&#25511;&#21046;&#19982;&#26426;&#22120;&#23398;&#20064;&#22312;&#22797;&#26434;&#21160;&#24577;&#36712;&#36857;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Model-free tracking control of complex dynamical trajectories with machine learning. (arXiv:2309.11470v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#37096;&#20998;&#35266;&#27979;&#29366;&#24577;&#26469;&#25511;&#21046;&#20855;&#26377;&#20004;&#33218;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20648;&#23618;&#35745;&#31639;&#21644;&#38543;&#26426;&#36755;&#20837;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#22810;&#31181;&#21608;&#26399;&#24615;&#21644;&#28151;&#27788;&#20449;&#21495;&#30340;&#36319;&#36394;&#25511;&#21046;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#36319;&#36394;&#25511;&#21046;&#26159;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#22522;&#30784;&#38382;&#39064;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#27665;&#29992;&#21644;&#22269;&#38450;&#39046;&#22495;&#12290;&#20256;&#32479;&#30340;&#36319;&#36394;&#25511;&#21046;&#26041;&#27861;&#38656;&#35201;&#23436;&#20840;&#20102;&#35299;&#31995;&#32479;&#27169;&#22411;&#21644;&#26041;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#37096;&#20998;&#35266;&#27979;&#29366;&#24577;&#26469;&#25511;&#21046;&#20855;&#26377;&#20004;&#33218;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#12290;&#25511;&#21046;&#22120;&#37319;&#29992;&#20102;&#20648;&#23618;&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#38543;&#26426;&#36755;&#20837;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#36755;&#20837;&#21253;&#25324;&#35266;&#27979;&#21040;&#30340;&#37096;&#20998;&#29366;&#24577;&#21521;&#37327;&#21644;&#35813;&#21521;&#37327;&#30340;&#31435;&#21363;&#26410;&#26469;&#29366;&#24577;&#21521;&#37327;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#23558;&#21518;&#32773;&#35270;&#20026;&#21069;&#32773;&#30340;&#26410;&#26469;&#29366;&#24577;&#12290;&#22312;&#27979;&#35797;&#21644;&#23454;&#38469;&#24212;&#29992;&#38454;&#27573;&#65292;&#31435;&#21363;&#26410;&#26469;&#29366;&#24577;&#21521;&#37327;&#34987;&#26367;&#25442;&#20026;&#21442;&#32771;&#36712;&#36857;&#20013;&#30340;&#26399;&#26395;&#35266;&#27979;&#21521;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#21608;&#26399;&#24615;&#21644;&#28151;&#27788;&#20449;&#21495;&#26469;&#39564;&#35777;&#35813;&#25511;&#21046;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#23545;&#22810;&#31181;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear tracking control enabling a dynamical system to track a desired trajectory is fundamental to robotics, serving a wide range of civil and defense applications. In control engineering, designing tracking control requires complete knowledge of the system model and equations. We develop a model-free, machine-learning framework to control a two-arm robotic manipulator using only partially observed states, where the controller is realized by reservoir computing. Stochastic input is exploited for training, which consists of the observed partial state vector as the first and its immediate future as the second component so that the neural machine regards the latter as the future state of the former. In the testing (deployment) phase, the immediate-future component is replaced by the desired observational vector from the reference trajectory. We demonstrate the effectiveness of the control framework using a variety of periodic and chaotic signals, and establish its robustness against m
&lt;/p&gt;</description></item><item><title>AudioFool&#26159;&#19968;&#31181;&#39640;&#36895;&#12289;&#36890;&#29992;&#21644;&#26080;&#38656;&#21516;&#27493;&#30340;&#36328;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#25915;&#20987;&#65292;&#33021;&#22815;&#36890;&#36807;&#21453;&#21521;&#20613;&#37324;&#21494;&#21464;&#25442;&#26500;&#36896;&#23545;&#21516;&#27493;&#30340;&#19981;&#21464;&#24615;&#21644;&#23545;&#28388;&#27874;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#29616;&#23545;ASR&#31995;&#32479;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.11462</link><description>&lt;p&gt;
AudioFool: &#39640;&#36895;&#12289;&#36890;&#29992;&#21644;&#26080;&#38656;&#21516;&#27493;&#30340;&#36328;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AudioFool: Fast, Universal and synchronization-free Cross-Domain Attack on Speech Recognition. (arXiv:2309.11462v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11462
&lt;/p&gt;
&lt;p&gt;
AudioFool&#26159;&#19968;&#31181;&#39640;&#36895;&#12289;&#36890;&#29992;&#21644;&#26080;&#38656;&#21516;&#27493;&#30340;&#36328;&#39046;&#22495;&#35821;&#38899;&#35782;&#21035;&#25915;&#20987;&#65292;&#33021;&#22815;&#36890;&#36807;&#21453;&#21521;&#20613;&#37324;&#21494;&#21464;&#25442;&#26500;&#36896;&#23545;&#21516;&#27493;&#30340;&#19981;&#21464;&#24615;&#21644;&#23545;&#28388;&#27874;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#29616;&#23545;ASR&#31995;&#32479;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24050;&#32463;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#35774;&#22791;&#19978;&#21629;&#20196;&#30340;&#24694;&#24847;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#25506;&#35752;&#22914;&#20309;&#21019;&#24314;&#36825;&#31181;&#25915;&#20987;&#65292;&#28982;&#32780;&#19968;&#20123;&#19982;&#31354;&#20013;&#25509;&#21475; (OTA) &#25915;&#20987;&#30456;&#20851;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#22949;&#21892;&#35299;&#20915;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;OTA&#27169;&#22411;&#20860;&#23481;&#30340;&#24378;&#25915;&#20987;&#25152;&#38656;&#30340;&#23646;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#20219;&#24847;&#25152;&#38656;&#23646;&#24615;&#30340;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21363;&#23545;&#21516;&#27493;&#30340;&#19981;&#21464;&#24615;&#21644;&#23545;&#28388;&#27874;&#30340;&#40065;&#26834;&#24615;&#65306;&#36825;&#20801;&#35768;&#23545;ASR&#31995;&#32479;&#36827;&#34892;&#25298;&#32477;&#26381;&#21153;&#65288;DoS&#65289;&#25915;&#20987;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#25913;&#36827;&#30340;&#39057;&#22495;&#25915;&#20987;&#65292;&#24182;&#36890;&#36807;&#36870;&#20613;&#37324;&#21494;&#21464;&#25442;&#23454;&#29616;&#20102;&#36825;&#20123;&#29305;&#24615;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#20851;&#38190;&#35789;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;OTA&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#20998;&#26512;&#20102;&#36328;&#39046;&#22495;&#25915;&#20987;&#30340;&#23646;&#24615;&#65292;&#20197;&#35299;&#37322;&#35813;&#26041;&#27861;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition systems have been shown to be vulnerable to adversarial attacks that manipulate the command executed on the device. Recent research has focused on exploring methods to create such attacks, however, some issues relating to Over-The-Air (OTA) attacks have not been properly addressed. In our work, we examine the needed properties of robust attacks compatible with the OTA model, and we design a method of generating attacks with arbitrary such desired properties, namely the invariance to synchronization, and the robustness to filtering: this allows a Denial-of-Service (DoS) attack against ASR systems. We achieve these characteristics by constructing attacks in a modified frequency domain through an inverse Fourier transform. We evaluate our method on standard keyword classification tasks and analyze it in OTA, and we analyze the properties of the cross-domain attacks to explain the efficiency of the approach.
&lt;/p&gt;</description></item><item><title>&#25968;&#23383;&#23402;&#29983;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#25968;&#23383;&#23402;&#29983;&#21487;&#20197;&#29983;&#25104;&#31995;&#32479;&#28436;&#21270;&#12289;&#39044;&#27979;&#28798;&#38590;&#24615;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#26089;&#26399;&#35686;&#21578;&#12290;&#25968;&#23383;&#23402;&#29983;&#21487;&#20197;&#29992;&#20110;&#23454;&#26102;&#31995;&#32479;&#30417;&#27979;&#21644;&#39044;&#27979;&#24615;&#38382;&#39064;&#35299;&#20915;&#12290;&#20004;&#31181;&#26041;&#27861;&#26500;&#24314;&#25968;&#23383;&#23402;&#29983;&#65306;&#31232;&#30095;&#20248;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.11461</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#25968;&#23383;&#23402;&#29983;&#65306;&#19968;&#31181;&#35270;&#35282;&#12290;&#65288;arXiv&#65306;2309.11461v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Digital twins of nonlinear dynamical systems: A perspective. (arXiv:2309.11461v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11461
&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#25968;&#23383;&#23402;&#29983;&#21487;&#20197;&#29983;&#25104;&#31995;&#32479;&#28436;&#21270;&#12289;&#39044;&#27979;&#28798;&#38590;&#24615;&#34892;&#20026;&#65292;&#24182;&#25552;&#20379;&#26089;&#26399;&#35686;&#21578;&#12290;&#25968;&#23383;&#23402;&#29983;&#21487;&#20197;&#29992;&#20110;&#23454;&#26102;&#31995;&#32479;&#30417;&#27979;&#21644;&#39044;&#27979;&#24615;&#38382;&#39064;&#35299;&#20915;&#12290;&#20004;&#31181;&#26041;&#27861;&#26500;&#24314;&#25968;&#23383;&#23402;&#29983;&#65306;&#31232;&#30095;&#20248;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#36817;&#24180;&#26469;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#25968;&#23383;&#23402;&#29983;&#30340;&#22522;&#26412;&#35201;&#27714;&#26159;&#33021;&#22815;&#29983;&#25104;&#31995;&#32479;&#28436;&#21270;&#24182;&#39044;&#27979;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#31361;&#21457;&#34892;&#20026;&#65292;&#20197;&#25552;&#20379;&#21450;&#26089;&#30340;&#35686;&#21578;&#12290;&#25968;&#23383;&#23402;&#29983;&#21487;&#20197;&#29992;&#20110;&#23454;&#26102;&#30340;&#31995;&#32479;&#8220;&#20581;&#24247;&#8221;&#30417;&#27979;&#21644;&#39044;&#27979;&#24615;&#38382;&#39064;&#35299;&#20915;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;&#25968;&#23383;&#23402;&#29983;&#39044;&#27979;&#26410;&#26469;&#21487;&#33021;&#21457;&#29983;&#31995;&#32479;&#23849;&#28291;&#65292;&#36825;&#26159;&#30001;&#29615;&#22659;&#21464;&#21270;&#25110;&#25200;&#21160;&#24341;&#36215;&#30340;&#21442;&#25968;&#28418;&#31227;&#23548;&#33268;&#30340;&#65292;&#21487;&#20197;&#21046;&#23450;&#24182;&#25191;&#34892;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20316;&#20026;&#26089;&#26399;&#24178;&#39044;&#65292;&#20197;&#38450;&#27490;&#31995;&#32479;&#23849;&#28291;&#12290;&#26500;&#24314;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30340;&#25968;&#23383;&#23402;&#29983;&#23384;&#22312;&#20004;&#31181;&#26041;&#27861;&#65306;&#31232;&#30095;&#20248;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#27880;&#24847;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital twins have attracted a great deal of recent attention from a wide range of fields. A basic requirement for digital twins of nonlinear dynamical systems is the ability to generate the system evolution and predict potentially catastrophic emergent behaviors so as to providing early warnings. The digital twin can then be used for system "health" monitoring in real time and for predictive problem solving. In particular, if the digital twin forecasts a possible system collapse in the future due to parameter drifting as caused by environmental changes or perturbations, an optimal control strategy can be devised and executed as early intervention to prevent the collapse. Two approaches exist for constructing digital twins of nonlinear dynamical systems: sparse optimization and machine learning. The basics of these two approaches are described and their advantages and caveats are discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;&#39044;&#27979;&#35270;&#37326;&#26469;&#20943;&#23569;&#38663;&#33633;&#30340;&#22810;&#27493;&#39588;&#27169;&#22411;&#39044;&#27979;&#23433;&#20840;&#28388;&#27874;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38271;&#26399;&#36755;&#20837;&#20462;&#27491;&#32771;&#34385;&#22312;&#20869;&#65292;&#23545;&#26377;&#30028;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#36882;&#24402;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#35777;&#26126;&#65292;&#24182;&#22312;&#20223;&#30495;&#21644;&#22235;&#26059;&#32764;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.11453</link><description>&lt;p&gt;
&#22810;&#27493;&#39588;&#27169;&#22411;&#39044;&#27979;&#23433;&#20840;&#28388;&#27874;&#22120;&#65306;&#36890;&#36807;&#22686;&#21152;&#39044;&#27979;&#35270;&#37326;&#20943;&#23569;&#38663;&#33633;
&lt;/p&gt;
&lt;p&gt;
Multi-Step Model Predictive Safety Filters: Reducing Chattering by Increasing the Prediction Horizon. (arXiv:2309.11453v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22686;&#21152;&#39044;&#27979;&#35270;&#37326;&#26469;&#20943;&#23569;&#38663;&#33633;&#30340;&#22810;&#27493;&#39588;&#27169;&#22411;&#39044;&#27979;&#23433;&#20840;&#28388;&#27874;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38271;&#26399;&#36755;&#20837;&#20462;&#27491;&#32771;&#34385;&#22312;&#20869;&#65292;&#23545;&#26377;&#30028;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#36882;&#24402;&#21487;&#34892;&#24615;&#36827;&#34892;&#20102;&#35777;&#26126;&#65292;&#24182;&#22312;&#20223;&#30495;&#21644;&#22235;&#26059;&#32764;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#34920;&#29616;&#20986;&#27604;&#32463;&#20856;&#25511;&#21046;&#22120;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25552;&#20379;&#23433;&#20840;&#20445;&#35777;&#24182;&#19981;&#23481;&#26131;&#12290;&#23433;&#20840;&#65292;&#21363;&#28385;&#36275;&#29366;&#24577;&#21644;&#36755;&#20837;&#32422;&#26463;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#31574;&#30053;&#19982;&#23433;&#20840;&#28388;&#27874;&#22120;&#30456;&#32467;&#21512;&#26469;&#20445;&#35777;&#12290;&#27169;&#22411;&#39044;&#27979;&#23433;&#20840;&#28388;&#27874;&#22120;&#65288;MPSFs&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#30340;&#24120;&#35265;&#23433;&#20840;&#28388;&#27874;&#26041;&#27861;&#12290;MPSFs&#26088;&#22312;&#22312;&#31435;&#21363;&#30340;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#20445;&#35777;&#23433;&#20840;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#25552;&#20986;&#30340;&#36755;&#20837;&#19982;&#24212;&#29992;&#30340;&#36755;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#31181;&#26377;&#38480;&#30340;&#39044;&#35265;&#24615;&#21487;&#33021;&#23548;&#33268;&#22312;&#32422;&#26463;&#36793;&#30028;&#38468;&#36817;&#20986;&#29616;&#39076;&#21160;&#36816;&#21160;&#21644;&#19981;&#24076;&#26395;&#30340;&#25391;&#33633;&#65292;&#21363;&#25152;&#35859;&#30340;&#38663;&#33633;&#29616;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#26356;&#38271;&#30340;&#21069;&#30651;&#24615;&#36755;&#20837;&#20462;&#27491;&#26469;&#20943;&#23569;&#38663;&#33633;&#12290;&#22312;&#26377;&#30028;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#40065;&#26834;MPC&#30340;&#25216;&#26415;&#35777;&#26126;&#20102;&#36882;&#24402;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#20223;&#30495;&#21644;&#22235;&#26059;&#32764;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based controllers have demonstrated superior performance compared to classical controllers in various tasks. However, providing safety guarantees is not trivial. Safety, the satisfaction of state and input constraints, can be guaranteed by augmenting the learned control policy with a safety filter. Model predictive safety filters (MPSFs) are a common safety filtering approach based on model predictive control (MPC). MPSFs seek to guarantee safety while minimizing the difference between the proposed and applied inputs in the immediate next time step. This limited foresight can lead to jerky motions and undesired oscillations close to constraint boundaries, known as chattering. In this paper, we reduce chattering by considering input corrections over a longer horizon. Under the assumption of bounded model uncertainties, we prove recursive feasibility using techniques from robust MPC. We verified the proposed approach in both extensive simulation and quadrotor experiments. In exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;Isolation Forest&#26041;&#27861;&#20013;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#65292;&#20998;&#21035;&#26159;&#23545;&#35780;&#20998;&#20989;&#25968;&#30340;&#20449;&#24687;&#29702;&#35770;&#25512;&#24191;&#21644;&#22312;&#20010;&#20307;&#26641;&#20272;&#35745;&#22120;&#23618;&#38754;&#19978;&#20351;&#29992;&#22522;&#20110;&#36229;&#20307;&#31215;&#30340;&#35780;&#20998;&#20989;&#25968;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22343;&#23545;&#19968;&#20123;&#25968;&#25454;&#38598;&#30456;&#23545;&#20110;&#26631;&#20934;Isolation Forest&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#19988;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#34920;&#29616;&#20986;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11450</link><description>&lt;p&gt;
Isolation Forest&#30340;&#20998;&#24067;&#21644;&#23481;&#37327;&#22522;&#20934;&#35780;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distribution and volume based scoring for Isolation Forests. (arXiv:2309.11450v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;Isolation Forest&#26041;&#27861;&#20013;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#65292;&#20998;&#21035;&#26159;&#23545;&#35780;&#20998;&#20989;&#25968;&#30340;&#20449;&#24687;&#29702;&#35770;&#25512;&#24191;&#21644;&#22312;&#20010;&#20307;&#26641;&#20272;&#35745;&#22120;&#23618;&#38754;&#19978;&#20351;&#29992;&#22522;&#20110;&#36229;&#20307;&#31215;&#30340;&#35780;&#20998;&#20989;&#25968;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22343;&#23545;&#19968;&#20123;&#25968;&#25454;&#38598;&#30456;&#23545;&#20110;&#26631;&#20934;Isolation Forest&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#19988;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#34920;&#29616;&#20986;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#24322;&#24120;&#21644;&#31163;&#32676;&#20540;&#26816;&#27979;&#26041;&#27861;Isolation Forest&#20570;&#20986;&#20102;&#20004;&#39033;&#36129;&#29486;&#12290;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#23545;&#29992;&#20110;&#32858;&#21512;&#38543;&#26426;&#26641;&#20272;&#35745;&#22120;&#30340;&#24471;&#20998;&#20989;&#25968;&#36827;&#34892;&#20102;&#20449;&#24687;&#29702;&#35770;&#19978;&#30340;&#25512;&#24191;&#12290;&#36825;&#20010;&#25512;&#24191;&#20801;&#35768;&#32771;&#34385;&#25972;&#20010;&#20998;&#24067;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26641;&#30340;&#38598;&#25104;&#24179;&#22343;&#20540;&#12290;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#22312;&#21333;&#20010;&#26641;&#20272;&#35745;&#22120;&#23618;&#38754;&#19978;&#26367;&#25442;&#20102;Isolation Forest&#22522;&#20110;&#28145;&#24230;&#30340;&#24471;&#20998;&#26041;&#27861;&#65292;&#25913;&#20026;&#22522;&#20110;&#38548;&#31163;&#26641;&#21494;&#33410;&#28857;&#30340;&#36229;&#20307;&#31215;&#30340;&#24471;&#20998;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#30340;&#25968;&#25454;&#23545;&#36825;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#22312;&#26469;&#33258;&#26368;&#36817;&#21644;&#35814;&#23613;&#30340;&#8220;ADBench&#8221;&#22522;&#20934;&#30340;34&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#20004;&#31181;&#21464;&#20307;&#30456;&#23545;&#20110;&#26631;&#20934;&#30340;Isolation Forest&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#24182;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#23545;&#20854;&#20013;&#19968;&#31181;&#21464;&#20307;&#24179;&#22343;&#26174;&#31034;&#20986;&#25913;&#36827;&#12290;&#20195;&#30721;&#21487;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We make two contributions to the Isolation Forest method for anomaly and outlier detection. The first contribution is an information-theoretically motivated generalisation of the score function that is used to aggregate the scores across random tree estimators. This generalisation allows one to take into account not just the ensemble average across trees but instead the whole distribution. The second contribution is an alternative scoring function at the level of the individual tree estimator, in which we replace the depth-based scoring of the Isolation Forest with one based on hyper-volumes associated to an isolation tree's leaf nodes.  We motivate the use of both of these methods on generated data and also evaluate them on 34 datasets from the recent and exhaustive ``ADBench'' benchmark, finding significant improvement over the standard isolation forest for both variants on some datasets and improvement on average across all datasets for one of the two variants. The code to reproduce
&lt;/p&gt;</description></item><item><title>&#21152;&#26435;&#24179;&#22343;&#25216;&#26415;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#30693;&#35782;&#33976;&#39311;&#20013;&#20063;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#27809;&#26377;&#39564;&#35777;&#25968;&#25454;&#30340;&#31616;&#21333;&#21152;&#26435;&#24179;&#22343;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.11446</link><description>&lt;p&gt;
&#21152;&#26435;&#24179;&#22343;&#25913;&#21892;&#20102;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Weight Averaging Improves Knowledge Distillation under Domain Shift. (arXiv:2309.11446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11446
&lt;/p&gt;
&lt;p&gt;
&#21152;&#26435;&#24179;&#22343;&#25216;&#26415;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#30693;&#35782;&#33976;&#39311;&#20013;&#20063;&#33021;&#25552;&#39640;&#24615;&#33021;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#27809;&#26377;&#39564;&#35777;&#25968;&#25454;&#30340;&#31616;&#21333;&#21152;&#26435;&#24179;&#22343;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#24378;&#22823;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#23427;&#19987;&#27880;&#20110;&#35757;&#32451;&#19968;&#20010;&#23567;&#22411;&#30340;&#23398;&#29983;&#32593;&#32476;&#26469;&#27169;&#20223;&#19968;&#20010;&#36739;&#22823;&#30340;&#25945;&#24072;&#32593;&#32476;&#12290;&#23613;&#31649;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30693;&#35782;&#33976;&#39311;&#21487;&#20197;&#22312;i.i.d&#30340;&#24773;&#20917;&#19979;&#25913;&#21892;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#23398;&#29983;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#21363;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#65292;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#30693;&#35782;&#33976;&#39311;&#21644;&#39046;&#22495;&#27867;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#20043;&#38388;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#65292;&#39046;&#22495;&#27867;&#21270;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#21152;&#26435;&#24179;&#22343;&#25216;&#26415;&#65292;&#22914;SWAD&#21644;SMA&#65292;&#20063;&#21487;&#20197;&#25552;&#39640;&#30693;&#35782;&#33976;&#39311;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21152;&#26435;&#24179;&#22343;&#31574;&#30053;&#65292;&#19981;&#38656;&#35201;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#39564;&#35777;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#24403;&#24212;&#29992;&#20110;&#30693;&#35782;&#33976;&#39311;&#26102;&#65292;&#23427;&#19982;SWAD&#21644;SMA&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;f
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is a powerful model compression technique broadly used in practical deep learning applications. It is focused on training a small student network to mimic a larger teacher network. While it is widely known that KD can offer an improvement to student generalization in i.i.d setting, its performance under domain shift, i.e. the performance of student networks on data from domains unseen during training, has received little attention in the literature. In this paper we make a step towards bridging the research fields of knowledge distillation and domain generalization. We show that weight averaging techniques proposed in domain generalization literature, such as SWAD and SMA, also improve the performance of knowledge distillation under domain shift. In addition, we propose a simplistic weight averaging strategy that does not require evaluation on validation data during training and show that it performs on par with SWAD and SMA when applied to KD. We name our f
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31614;&#21517;&#28608;&#27963;&#30340;&#26174;&#33879;&#24615;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#20449;&#21495;&#35266;&#28857;&#29983;&#25104;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#25972;&#20307;&#21644;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#20020;&#24202;&#30149;&#21464;&#26816;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11443</link><description>&lt;p&gt;
&#31614;&#21517;&#28608;&#27963;: &#19968;&#31181;&#29992;&#20110;&#25972;&#20307;&#26174;&#33879;&#24615;&#30340;&#31232;&#30095;&#20449;&#21495;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Signature Activation: A Sparse Signal View for Holistic Saliency. (arXiv:2309.11443v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11443
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31614;&#21517;&#28608;&#27963;&#30340;&#26174;&#33879;&#24615;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#20449;&#21495;&#35266;&#28857;&#29983;&#25104;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#30340;&#25972;&#20307;&#21644;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#21644;&#20020;&#24202;&#30149;&#21464;&#26816;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#38656;&#35201;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#31614;&#21517;&#28608;&#27963;&#30340;&#26174;&#33879;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36755;&#20986;&#30340;&#25972;&#20307;&#21644;&#31867;&#21035;&#19981;&#21487;&#30693;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26576;&#20123;&#31867;&#22411;&#30340;&#21307;&#23398;&#22270;&#20687;&#65288;&#22914;&#34880;&#31649;&#36896;&#24433;&#22270;&#20687;&#65289;&#20855;&#26377;&#28165;&#26224;&#30340;&#21069;&#26223;&#21644;&#32972;&#26223;&#23545;&#35937;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#29702;&#35770;&#35299;&#37322;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20896;&#29366;&#21160;&#33033;&#36896;&#24433;&#22270;&#20687;&#20013;&#36741;&#21161;&#30149;&#21464;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of machine learning in healthcare calls for model transparency and explainability. In this work, we introduce Signature Activation, a saliency method that generates holistic and class-agnostic explanations for Convolutional Neural Network (CNN) outputs. Our method exploits the fact that certain kinds of medical images, such as angiograms, have clear foreground and background objects. We give theoretical explanation to justify our methods. We show the potential use of our method in clinical settings through evaluating its efficacy for aiding the detection of lesions in coronary angiograms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TRACE-GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#19994;&#20013;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#23884;&#20837;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#26469;&#39044;&#35757;&#32451;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24322;&#24120;&#24207;&#21015;&#21644;&#27491;&#24120;&#24207;&#21015;&#30340;&#20998;&#31867;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11427</link><description>&lt;p&gt;
&#21322;&#23548;&#20307;&#21046;&#36896;&#19994;&#20013;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-Training of Time-Series Data for Unsupervised Fault Detection in Semiconductor Manufacturing. (arXiv:2309.11427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TRACE-GPT&#27169;&#22411;&#65292;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#19994;&#20013;&#26080;&#30417;&#30563;&#25925;&#38556;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#23884;&#20837;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#26469;&#39044;&#35757;&#32451;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24322;&#24120;&#24207;&#21015;&#21644;&#27491;&#24120;&#24207;&#21015;&#30340;&#20998;&#31867;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TRACE-GPT&#65288;Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers&#65289;&#65292;&#23427;&#26159;&#29992;&#20110;&#21322;&#23548;&#20307;&#21046;&#36896;&#20013;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#26816;&#27979;&#25925;&#38556;&#30340;&#27169;&#22411;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#21367;&#31215;&#23884;&#20837;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#65288;GPT&#65289;&#26469;&#25552;&#21462;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#23545;&#24322;&#24120;&#24207;&#21015;&#21644;&#27491;&#24120;&#24207;&#21015;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces TRACE-GPT, which stands for Time-seRies Anomaly-detection with Convolutional Embedding and Generative Pre-trained Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor data and detect faults on unlabeled datasets in semiconductor manufacturing. In semiconductor industry, classifying abnormal time-series sensor data from normal data is important because it is directly related to wafer defect. However, small, unlabeled, and even mixed training data without enough anomalies make classification tasks difficult. In this research, we capture features of time-series data with temporal convolutional embedding and Generative Pre-trained Transformer (GPT) to classify abnormal sequences from normal sequences using cross entropy loss. We prove that our model shows better performance than previous unsupervised models with both an open dataset, the University of California Riverside (UCR) time-series classification archive, and the process log of our Ch
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20316;&#20026;&#38477;&#22122;&#31639;&#27861;&#22312;&#39640;&#32500;&#22270;&#24418;&#27169;&#22411;&#20013;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#65292;&#20026;&#29983;&#25104;&#24314;&#27169;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#36924;&#36817;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11420</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#38477;&#22122;&#31639;&#27861;&#65306;&#39640;&#32500;&#22270;&#24418;&#27169;&#22411;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Networks as Denoising Algorithms: Sample-Efficient Learning of Diffusion Models in High-Dimensional Graphical Models. (arXiv:2309.11420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11420
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20316;&#20026;&#38477;&#22122;&#31639;&#27861;&#22312;&#39640;&#32500;&#22270;&#24418;&#27169;&#22411;&#20013;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;&#65292;&#20026;&#29983;&#25104;&#24314;&#27169;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#36924;&#36817;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24314;&#27169;&#20013;&#36890;&#36807;&#35780;&#20998;&#20989;&#25968;&#30340;&#36924;&#36817;&#25928;&#29575;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#36817;&#20284;&#29702;&#35770;&#21033;&#29992;&#20102;&#35780;&#20998;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#26412;&#36136;&#19978;&#39640;&#32500;&#25968;&#25454;&#20013;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#22256;&#25200;&#12290;&#36825;&#31181;&#38480;&#21046;&#22312;&#22270;&#24418;&#27169;&#22411;&#65288;&#22914;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#65289;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#36825;&#26159;&#22270;&#20687;&#20998;&#24067;&#24120;&#35265;&#30340;&#31867;&#22411;&#65292;&#35780;&#20998;&#20989;&#25968;&#30340;&#36817;&#20284;&#25928;&#29575;&#23578;&#26410;&#30830;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35780;&#20998;&#20989;&#25968;&#22312;&#22270;&#24418;&#27169;&#22411;&#20013;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#38477;&#22122;&#31639;&#27861;&#36827;&#34892;&#36739;&#22909;&#30340;&#36924;&#36817;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31639;&#27861;&#36866;&#29992;&#20110;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#22270;&#24418;&#27169;&#22411;&#30340;&#20363;&#23376;&#20013;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#21253;&#25324;&#20234;&#36763;&#27169;&#22411;&#12289;&#26465;&#20214;&#20234;&#36763;&#27169;&#22411;&#12289;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#21644;&#31232;&#30095;&#32534;&#30721;&#27169;&#22411;&#12290;&#32467;&#21512;&#22522;&#20110;&#25193;&#25955;&#37319;&#26679;&#30340;&#29616;&#25104;&#31163;&#25955;&#21270;&#35823;&#24046;&#30028;&#38480;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26679;&#26412;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the approximation efficiency of score functions by deep neural networks in diffusion-based generative modeling. While existing approximation theories utilize the smoothness of score functions, they suffer from the curse of dimensionality for intrinsically high-dimensional data. This limitation is pronounced in graphical models such as Markov random fields, common for image distributions, where the approximation efficiency of score functions remains unestablished.  To address this, we observe score functions can often be well-approximated in graphical models through variational inference denoising algorithms. Furthermore, these algorithms are amenable to efficient neural network representation. We demonstrate this in examples of graphical models, including Ising models, conditional Ising models, restricted Boltzmann machines, and sparse encoding models. Combined with off-the-shelf discretization error bounds for diffusion-based sampling, we provide an efficient sample com
&lt;/p&gt;</description></item><item><title>EDMP&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36816;&#21160;&#35268;&#21010;&#20248;&#21183;&#30340;&#22522;&#20110;&#25104;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#30340;&#36816;&#21160;&#23398;&#26377;&#25928;&#36712;&#36857;&#36827;&#34892;&#35757;&#32451;&#65292;&#26469;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.11414</link><description>&lt;p&gt;
EDMP: &#22522;&#20110;&#25104;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
EDMP: Ensemble-of-costs-guided Diffusion for Motion Planning. (arXiv:2309.11414v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11414
&lt;/p&gt;
&lt;p&gt;
EDMP&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#36816;&#21160;&#35268;&#21010;&#20248;&#21183;&#30340;&#22522;&#20110;&#25104;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#30340;&#36816;&#21160;&#23398;&#26377;&#25928;&#36712;&#36857;&#36827;&#34892;&#35757;&#32451;&#65292;&#26469;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#25805;&#32437;&#36816;&#21160;&#35268;&#21010;&#21253;&#25324;&#19968;&#32452;&#36890;&#29992;&#31639;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#25191;&#34892;&#32473;&#23450;&#35745;&#21010;&#30340;&#29305;&#23450;&#20110;&#22330;&#26223;&#30340;&#25104;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#20110;&#20219;&#20309;&#26032;&#22330;&#26223;&#65292;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#23545;&#22810;&#26679;&#26377;&#25928;&#36712;&#36857;&#26377;&#20808;&#39564;&#20102;&#35299;&#65292;&#24182;&#19988;&#27809;&#26377;&#38024;&#23545;&#32473;&#23450;&#22330;&#26223;&#35774;&#35745;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#21151;&#29575;&#24448;&#24448;&#36739;&#20302;&#12290;&#34429;&#28982;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#65292;&#20294;&#27809;&#26377;&#19987;&#38376;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#24456;&#38590;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EDMP&#65292;&#19968;&#31181;&#22522;&#20110;&#25104;&#26412;&#24341;&#23548;&#30340;&#25193;&#25955;&#36816;&#21160;&#35268;&#21010;&#38598;&#25104;&#65292;&#26088;&#22312;&#32467;&#21512;&#20256;&#32479;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36816;&#21160;&#35268;&#21010;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#25193;&#25955;&#32593;&#32476;&#22312;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36816;&#21160;&#23398;&#26377;&#25928;&#36712;&#36857;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#19982;&#20256;&#32479;&#35268;&#21010;&#19968;&#26679;&#65292;&#22312;&#25512;&#26029;&#26102;&#23545;&#20110;&#20219;&#20309;&#26032;&#22330;&#26223;&#65292;&#25105;&#20204;&#35745;&#31639;&#29305;&#23450;&#20110;&#22330;&#26223;&#30340;&#25104;&#26412;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical motion planning for robotic manipulation includes a set of general algorithms that aim to minimize a scene-specific cost of executing a given plan. This approach offers remarkable adaptability, as they can be directly used off-the-shelf for any new scene without needing specific training datasets. However, without a prior understanding of what diverse valid trajectories are and without specially designed cost functions for a given scene, the overall solutions tend to have low success rates. While deep-learning-based algorithms tremendously improve success rates, they are much harder to adopt without specialized training datasets. We propose EDMP, an Ensemble-of-costs-guided Diffusion for Motion Planning that aims to combine the strengths of classical and deep-learning-based motion planning. Our diffusion-based network is trained on a set of diverse kinematically valid trajectories. Like classical planning, for any new scene at the time of inference, we compute scene-specific 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;LSTM&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;LSTM-based&#27169;&#22411;DLSTM&#12290;</title><link>http://arxiv.org/abs/2309.11400</link><description>&lt;p&gt;
Transformers&#23545;&#30005;&#23376;&#20132;&#26131;&#30340;&#27604;&#36739;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Transformers versus LSTMs for electronic trading. (arXiv:2309.11400v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22522;&#20110;LSTM&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;LSTM-based&#27169;&#22411;DLSTM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#38271;&#30701;&#26399;&#35760;&#24518;(LSTM)&#20316;&#20026;&#19968;&#31181;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#19982;RNN&#30456;&#20284;&#65292;Transformer&#34987;&#35774;&#35745;&#29992;&#26469;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#12290;&#30001;&#20110;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#30740;&#31350;&#32773;&#24320;&#22987;&#20851;&#27880;Transformer&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;Transformer&#30340;&#38271;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26102;&#65292;LSTM&#20173;&#28982;&#26159;&#20027;&#27969;&#30340;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#24819;&#35201;&#22238;&#31572;&#30340;&#38382;&#39064;&#26159;&#65306;Transformer&#22522;&#20110;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#24182;&#20987;&#36133;LSTM&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#22312;&#22522;&#20110;&#39640;&#39057;&#38480;&#20215;&#22996;&#25176;&#31807;&#25968;&#25454;&#30340;&#22810;&#20010;&#37329;&#34701;&#39044;&#27979;&#20219;&#21153;&#19978;&#27604;&#36739;&#20102;&#22810;&#31181;&#22522;&#20110;LSTM&#21644;Transformer&#30340;&#27169;&#22411;&#12290;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;DLSTM&#30340;&#26032;&#22411;LSTM-based&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of artificial intelligence, long short term memory (LSTM), one kind of recurrent neural network (RNN), has been widely applied in time series prediction.  Like RNN, Transformer is designed to handle the sequential data. As Transformer achieved great success in Natural Language Processing (NLP), researchers got interested in Transformer's performance on time series prediction, and plenty of Transformer-based solutions on long time series forecasting have come out recently. However, when it comes to financial time series prediction, LSTM is still a dominant architecture. Therefore, the question this study wants to answer is: whether the Transformer-based model can be applied in financial time series prediction and beat LSTM.  To answer this question, various LSTM-based and Transformer-based models are compared on multiple financial prediction tasks based on high-frequency limit order book data. A new LSTM-based model called DLSTM is built and new architecture f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#20010;&#33258;&#36866;&#24212;&#26694;&#26550;&#30340;&#26032;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65306;PreFed&#21644;PreFedOp&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#26469;&#23454;&#29616;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;i.i.d.&#21644;&#38750;i.i.d.&#35774;&#32622;&#19979;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11378</link><description>&lt;p&gt;
&#39044;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Preconditioned Federated Learning. (arXiv:2309.11378v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#20010;&#33258;&#36866;&#24212;&#26694;&#26550;&#30340;&#26032;&#30340;&#36890;&#20449;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65306;PreFed&#21644;PreFedOp&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#26469;&#23454;&#29616;&#36866;&#24212;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;i.i.d.&#21644;&#38750;i.i.d.&#35774;&#32622;&#19979;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#39640;&#25928;&#30340;&#36890;&#20449;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;FL&#20013;&#30340;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;&#26159;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#65292;&#23427;&#22312;&#36890;&#20449;&#36718;&#20043;&#38388;&#25191;&#34892;&#22810;&#20010;&#26412;&#22320;SGD&#27493;&#39588;&#12290;&#19982;&#29616;&#20195;&#30340;&#19968;&#38454;&#33258;&#36866;&#24212;&#20248;&#21270;&#30456;&#27604;&#65292;FedAvg&#34987;&#35748;&#20026;&#32570;&#20047;&#31639;&#27861;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#20010;&#33258;&#36866;&#24212;&#26694;&#26550;&#65288;&#26412;&#22320;&#36866;&#24212;&#24615;&#21644;&#26381;&#21153;&#22120;&#31471;&#36866;&#24212;&#24615;&#65289;&#30340;&#26032;&#30340;&#39640;&#25928;&#36890;&#20449;FL&#31639;&#27861;&#65306;PreFed&#21644;PreFedOp&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#39044;&#22788;&#29702;&#22120;&#26469;&#23454;&#29616;&#36866;&#24212;&#24615;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;i.i.d.&#21644;&#38750;i.i.d.&#35774;&#32622;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a distributed machine learning approach that enables model training in communication efficient and privacy-preserving manner. The standard optimization method in FL is Federated Averaging (FedAvg), which performs multiple local SGD steps between communication rounds. FedAvg has been considered to lack algorithm adaptivity compared to modern first-order adaptive optimizations. In this paper, we propose new communication-efficient FL algortithms based on two adaptive frameworks: local adaptivity (PreFed) and server-side adaptivity (PreFedOp). Proposed methods adopt adaptivity by using a novel covariance matrix preconditioner. Theoretically, we provide convergence guarantees for our algorithms. The empirical experiments show our methods achieve state-of-the-art performances on both i.i.d. and non-i.i.d. settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#39044;&#27979;&#24739;&#32773;&#38745;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#20165;&#21407;&#22987;&#25968;&#25454;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20063;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#21508;&#31181;&#38745;&#24577;&#20449;&#24687;&#65292;&#21253;&#25324;&#29983;&#29289;&#23398;&#24615;&#21035;&#12289;&#20108;&#20540;&#21270;&#24180;&#40836;&#21644;&#33258;&#25253;&#31181;&#26063;&#65292;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11373</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#23398;&#20064;&#24739;&#32773;&#38745;&#24577;&#20449;&#24687;&#21450;&#20445;&#25252;&#38544;&#31169;&#21644;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Patient Static Information from Time-series EHR and an Approach for Safeguarding Privacy and Fairness. (arXiv:2309.11373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#39044;&#27979;&#24739;&#32773;&#38745;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#20165;&#21407;&#22987;&#25968;&#25454;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20063;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#21508;&#31181;&#38745;&#24577;&#20449;&#24687;&#65292;&#21253;&#25324;&#29983;&#29289;&#23398;&#24615;&#21035;&#12289;&#20108;&#20540;&#21270;&#24180;&#40836;&#21644;&#33258;&#25253;&#31181;&#26063;&#65292;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21307;&#30103;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#24739;&#32773;&#38544;&#31169;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#21307;&#30103;&#25968;&#25454;&#20013;&#27809;&#26377;&#26126;&#30830;&#21253;&#21547;&#31181;&#26063;&#20449;&#24687;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#21307;&#30103;&#25968;&#25454;&#39044;&#27979;&#24739;&#32773;&#33258;&#25253;&#30340;&#31181;&#26063;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#35782;&#21035;&#30340;&#31243;&#24230;&#27809;&#26377;&#20102;&#35299;&#65292;&#20063;&#32570;&#20047;&#24320;&#21457;&#27169;&#22411;&#20197;&#26368;&#23567;&#31243;&#24230;&#21463;&#21040;&#36825;&#20123;&#20449;&#24687;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#39044;&#27979;&#24739;&#32773;&#38745;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#20165;&#21407;&#22987;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#36824;&#26377;&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;&#37117;&#21487;&#20197;&#34987;&#35757;&#32451;&#29992;&#26469;&#39044;&#27979;&#21508;&#31181;&#38745;&#24577;&#20449;&#24687;&#65292;&#20854;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#21487;&#36798;&#21040;0.851&#65288;&#23545;&#29983;&#29289;&#23398;&#24615;&#21035;&#65289;&#12289;0.869&#65288;&#23545;&#20108;&#20540;&#21270;&#24180;&#40836;&#65289;&#21644;0.810&#65288;&#23545;&#33258;&#25253;&#31181;&#26063;&#65289;&#12290;&#36825;&#31181;&#39640;&#39044;&#27979;&#24615;&#33021;&#21487;&#20197;&#25193;&#23637;&#21040;&#24191;&#27867;&#30340;&#20849;&#30149;&#22240;&#32032;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#27169;&#22411;&#34987;&#35299;&#37322;&#26102;&#20063;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in machine learning for healthcare has raised concerns about patient privacy and algorithmic fairness. For example, previous work has shown that patient self-reported race can be predicted from medical data that does not explicitly contain racial information. However, the extent of data identification is unknown, and we lack ways to develop models whose outcomes are minimally affected by such information. Here we systematically investigated the ability of time-series electronic health record data to predict patient static information. We found that not only the raw time-series data, but also learned representations from machine learning models, can be trained to predict a variety of static information with area under the receiver operating characteristic curve as high as 0.851 for biological sex, 0.869 for binarized age and 0.810 for self-reported race. Such high predictive performance can be extended to a wide range of comorbidity factors and exists even when the model was
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;3D&#20154;&#33080;&#37325;&#24314;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#20174;&#25972;&#24418;&#25163;&#26415;&#21040;&#23089;&#20048;&#34892;&#19994;&#65292;&#20294;&#22312;&#27861;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#19968;&#20123;&#38480;&#21046;&#21644;&#38556;&#30861;&#12290;&#23545;&#20110;&#23558;3D&#20154;&#33080;&#37325;&#24314;&#20316;&#20026;&#27861;&#21307;&#23398;&#35777;&#25454;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#25105;&#20204;&#20173;&#38656;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.11357</link><description>&lt;p&gt;
3D&#20154;&#33080;&#37325;&#24314;&#65306;&#36890;&#24448;&#27861;&#21307;&#23398;&#30340;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;
3D Face Reconstruction: the Road to Forensics. (arXiv:2309.11357v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;3D&#20154;&#33080;&#37325;&#24314;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#20174;&#25972;&#24418;&#25163;&#26415;&#21040;&#23089;&#20048;&#34892;&#19994;&#65292;&#20294;&#22312;&#27861;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#19968;&#20123;&#38480;&#21046;&#21644;&#38556;&#30861;&#12290;&#23545;&#20110;&#23558;3D&#20154;&#33080;&#37325;&#24314;&#20316;&#20026;&#27861;&#21307;&#23398;&#35777;&#25454;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#25105;&#20204;&#20173;&#38656;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#20154;&#33080;&#37325;&#24314;&#31639;&#27861;&#36890;&#36807;&#22270;&#20687;&#21644;&#35270;&#39057;&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#65292;&#20174;&#25972;&#24418;&#25163;&#26415;&#21040;&#23089;&#20048;&#34892;&#19994;&#65292;&#24471;&#30410;&#20110;&#20854;&#20248;&#21183;&#29305;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#27861;&#21307;&#24212;&#29992;&#26102;&#65292;3D&#20154;&#33080;&#37325;&#24314;&#24517;&#39035;&#36981;&#23432;&#20005;&#26684;&#30340;&#35201;&#27714;&#65292;&#36825;&#20173;&#28982;&#20351;&#20854;&#22312;&#25552;&#20379;&#35785;&#35772;&#35777;&#25454;&#26041;&#38754;&#30340;&#28508;&#22312;&#20316;&#29992;&#19981;&#26126;&#30830;&#12290;&#23545;&#20854;&#22312;&#27861;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#32422;&#26463;&#12289;&#28508;&#21147;&#21644;&#38480;&#21046;&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#20173;&#28982;&#32570;&#22833;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#38416;&#26126;&#27861;&#21307;&#24212;&#29992;&#21644;&#29983;&#29289;&#29305;&#24449;&#35782;&#21035;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#33080;&#35782;&#21035;&#12290;&#22240;&#27492;&#65292;&#23427;&#23545;&#26469;&#33258;&#30417;&#25511;&#35270;&#39057;&#21644;&#23244;&#30097;&#29359;&#29031;&#29255;&#30340;3D&#20154;&#33080;&#37325;&#24314;&#31639;&#27861;&#30340;&#25104;&#26524;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#30446;&#21069;&#38459;&#30861;3D&#20154;&#33080;&#37325;&#24314;&#22312;&#27861;&#21307;&#24212;&#29992;&#20013;&#21457;&#25381;&#31215;&#26497;&#20316;&#29992;&#30340;&#38556;&#30861;&#12290;&#26368;&#21518;&#65292;&#23427;&#36824;&#23545;&#24213;&#23618;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#26816;&#26597;&#65292;&#21253;&#25324;&#20854;&#20248;&#28857;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D face reconstruction algorithms from images and videos are applied to many fields, from plastic surgery to the entertainment sector, thanks to their advantageous features. However, when looking at forensic applications, 3D face reconstruction must observe strict requirements that still make its possible role in bringing evidence to a lawsuit unclear. An extensive investigation of the constraints, potential, and limits of its application in forensics is still missing. Shedding some light on this matter is the goal of the present survey, which starts by clarifying the relation between forensic applications and biometrics, with a focus on face recognition. Therefore, it provides an analysis of the achievements of 3D face reconstruction algorithms from surveillance videos and mugshot images and discusses the current obstacles that separate 3D face reconstruction from an active role in forensic applications. Finally, it examines the underlying data sets, with their advantages and limitati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20262;&#25958;&#30340;&#22478;&#24066;&#21464;&#21270;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;1500&#19975;&#24352;&#34903;&#26223;&#22270;&#20687;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#20102;&#20303;&#25151;&#20379;&#24212;&#30340;&#21464;&#21270;&#65292;&#24182;&#21306;&#20998;&#20102;&#20027;&#35201;&#21644;&#27425;&#35201;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.11354</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#25581;&#31034;&#20102;&#34903;&#26223;&#22270;&#20687;&#20013;&#22478;&#24066;&#20303;&#25151;&#30340;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning unveils change in urban housing from street-level images. (arXiv:2309.11354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20262;&#25958;&#30340;&#22478;&#24066;&#21464;&#21270;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;1500&#19975;&#24352;&#34903;&#26223;&#22270;&#20687;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20986;&#20102;&#20303;&#25151;&#20379;&#24212;&#30340;&#21464;&#21270;&#65292;&#24182;&#21306;&#20998;&#20102;&#20027;&#35201;&#21644;&#27425;&#35201;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#21508;&#22320;&#30340;&#22478;&#24066;&#37117;&#38754;&#20020;&#30528;&#21487;&#36127;&#25285;&#21644;&#20307;&#38754;&#20303;&#25151;&#20005;&#37325;&#30701;&#32570;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#23545;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#25105;&#20204;&#22312;&#26377;&#25928;&#30417;&#27979;&#21644;&#36861;&#36394;&#22478;&#24066;&#20303;&#25151;&#36827;&#23637;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#24212;&#29992;&#20110;&#34903;&#26223;&#22270;&#20687;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#22312;&#27979;&#37327;&#31038;&#20250;&#32463;&#27982;&#21644;&#29615;&#22659;&#19981;&#24179;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#26102;&#21464;&#26631;&#31614;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#23427;&#20204;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#26102;&#38388;&#22270;&#20687;&#26469;&#36319;&#36394;&#22478;&#24066;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#22312;2008&#24180;&#33267;2021&#24180;&#20043;&#38388;&#20351;&#29992;1500&#19975;&#24352;&#20262;&#25958;&#34903;&#26223;&#22270;&#20687;&#26469;&#27979;&#37327;&#21464;&#21270;&#12290;&#25105;&#20204;&#23545;Barlow Twins&#30340;&#26032;&#39062;&#25913;&#36827;Street2Vec&#65292;&#22312;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#23884;&#20837;&#20102;&#22478;&#24066;&#32467;&#26500;&#65292;&#24182;&#23545;&#23395;&#33410;&#24615;&#21644;&#26085;&#24120;&#21464;&#21270;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#23427;&#20248;&#20110;&#36890;&#29992;&#23884;&#20837;&#65292;&#25104;&#21151;&#22320;&#20174;&#34903;&#26223;&#22270;&#20687;&#20013;&#35782;&#21035;&#20102;&#20262;&#25958;&#20303;&#25151;&#20379;&#24212;&#30340;&#28857;&#32423;&#21464;&#21270;&#65292;&#24182;&#21306;&#20998;&#20102;&#20027;&#35201;&#21644;&#27425;&#35201;&#21464;&#21270;&#12290;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#20026;&#22478;&#24066;&#35268;&#21010;&#25552;&#20379;&#21450;&#26102;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cities around the world face a critical shortage of affordable and decent housing. Despite its critical importance for policy, our ability to effectively monitor and track progress in urban housing is limited. Deep learning-based computer vision methods applied to street-level images have been successful in the measurement of socioeconomic and environmental inequalities but did not fully utilize temporal images to track urban change as time-varying labels are often unavailable. We used self-supervised methods to measure change in London using 15 million street images taken between 2008 and 2021. Our novel adaptation of Barlow Twins, Street2Vec, embeds urban structure while being invariant to seasonal and daily changes without manual annotations. It outperformed generic embeddings, successfully identified point-level change in London's housing supply from street-level images, and distinguished between major and minor change. This capability can provide timely information for urban plann
&lt;/p&gt;</description></item><item><title>C&#8901;ASE&#26159;&#19968;&#20010;&#23398;&#20064;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#30340;&#26465;&#20214;&#23545;&#25239;&#25216;&#33021;&#23884;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24322;&#26500;&#30340;&#25216;&#33021;&#21160;&#20316;&#21010;&#20998;&#20026;&#19981;&#21516;&#23376;&#38598;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;&#28966;&#28857;&#25216;&#33021;&#37319;&#26679;&#12289;&#39592;&#39612;&#27531;&#20313;&#21147;&#21644;&#36880;&#20803;&#32032;&#29305;&#24449;&#23631;&#34109;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#25216;&#33021;&#65292;&#24182;&#25429;&#25417;&#26356;&#19968;&#33324;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.11351</link><description>&lt;p&gt;
C&#8901;ASE&#65306;&#23398;&#20064;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#30340;&#26465;&#20214;&#23545;&#25239;&#25216;&#33021;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
C$\cdot$ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters. (arXiv:2309.11351v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11351
&lt;/p&gt;
&lt;p&gt;
C&#8901;ASE&#26159;&#19968;&#20010;&#23398;&#20064;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#30340;&#26465;&#20214;&#23545;&#25239;&#25216;&#33021;&#23884;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#24322;&#26500;&#30340;&#25216;&#33021;&#21160;&#20316;&#21010;&#20998;&#20026;&#19981;&#21516;&#23376;&#38598;&#65292;&#20197;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;&#28966;&#28857;&#25216;&#33021;&#37319;&#26679;&#12289;&#39592;&#39612;&#27531;&#20313;&#21147;&#21644;&#36880;&#20803;&#32032;&#29305;&#24449;&#23631;&#34109;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#25216;&#33021;&#65292;&#24182;&#25429;&#25417;&#26356;&#19968;&#33324;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;C&#8901;ASE&#65292;&#19968;&#20010;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#30340;&#26465;&#20214;&#23545;&#25239;&#25216;&#33021;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#29289;&#29702;&#27169;&#25311;&#35282;&#33394;&#21487;&#20197;&#23398;&#20064;&#21508;&#31181;&#25216;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20197;&#30452;&#25509;&#25805;&#32437;&#25191;&#34892;&#30340;&#25216;&#33021;&#30340;&#21487;&#25511;&#24615;&#12290;C&#8901;ASE&#23558;&#24322;&#26500;&#30340;&#25216;&#33021;&#21160;&#20316;&#21010;&#20998;&#20026;&#21253;&#21547;&#21516;&#36136;&#26679;&#26412;&#30340;&#19981;&#21516;&#23376;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#20302;&#32423;&#26465;&#20214;&#27169;&#22411;&#26469;&#23398;&#20064;&#26465;&#20214;&#34892;&#20026;&#20998;&#24067;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#20102;&#28966;&#28857;&#25216;&#33021;&#37319;&#26679;&#12289;&#39592;&#39612;&#27531;&#20313;&#21147;&#21644;&#36880;&#20803;&#32032;&#29305;&#24449;&#23631;&#34109;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#22797;&#26434;&#24615;&#30340;&#21508;&#31181;&#25216;&#33021;&#65292;&#20943;&#36731;&#21160;&#21147;&#23398;&#19981;&#21305;&#37197;&#20197;&#25484;&#25569;&#25935;&#25463;&#21160;&#20316;&#65292;&#24182;&#25429;&#25417;&#26356;&#22810;&#30340;&#19968;&#33324;&#34892;&#20026;&#29305;&#24449;&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#26465;&#20214;&#27169;&#22411;&#23601;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present C$\cdot$ASE, an efficient and effective framework that learns conditional Adversarial Skill Embeddings for physics-based characters. Our physically simulated character can learn a diverse repertoire of skills while providing controllability in the form of direct manipulation of the skills to be performed. C$\cdot$ASE divides the heterogeneous skill motions into distinct subsets containing homogeneous samples for training a low-level conditional model to learn conditional behavior distribution. The skill-conditioned imitation learning naturally offers explicit control over the character's skills after training. The training course incorporates the focal skill sampling, skeletal residual forces, and element-wise feature masking to balance diverse skills of varying complexities, mitigate dynamics mismatch to master agile motions and capture more general behavior characteristics, respectively. Once trained, the conditional model can produce highly diverse and realistic skills, o
&lt;/p&gt;</description></item><item><title>GECTurk&#26159;&#19968;&#20010;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#21644;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#23427;&#37319;&#29992;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#35206;&#30422;&#20102;20&#22810;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#35821;&#27861;&#21644;&#25340;&#20889;&#35268;&#21017;&#65292;&#24182;&#19988;&#36890;&#36807;&#25163;&#21160;&#27880;&#37322;&#30005;&#24433;&#35780;&#35770;&#21019;&#36896;&#20102;&#26356;&#30495;&#23454;&#30340;&#27979;&#35797;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.11346</link><description>&lt;p&gt;
GECTurk&#65306;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#21644;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GECTurk: Grammatical Error Correction and Detection Dataset for Turkish. (arXiv:2309.11346v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11346
&lt;/p&gt;
&lt;p&gt;
GECTurk&#26159;&#19968;&#20010;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#21644;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#23427;&#37319;&#29992;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#35206;&#30422;&#20102;20&#22810;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#35821;&#27861;&#21644;&#25340;&#20889;&#35268;&#21017;&#65292;&#24182;&#19988;&#36890;&#36807;&#25163;&#21160;&#27880;&#37322;&#30005;&#24433;&#35780;&#35770;&#21019;&#36896;&#20102;&#26356;&#30495;&#23454;&#30340;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#65288;GEC&#65289;&#24037;&#20855;&#24050;&#32463;&#34987;&#35777;&#26126;&#23545;&#20110;&#27597;&#35821;&#20351;&#29992;&#32773;&#21644;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#38750;&#24120;&#26377;&#29992;&#12290;&#24320;&#21457;&#36825;&#26679;&#30340;&#24037;&#20855;&#38656;&#35201;&#22823;&#37327;&#24179;&#34892;&#30340;&#12289;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#20294;&#26159;&#23545;&#20110;&#22823;&#22810;&#25968;&#35821;&#35328;&#26469;&#35828;&#65292;&#36825;&#31181;&#25968;&#25454;&#26159;&#19981;&#21487;&#24471;&#21040;&#30340;&#12290;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26159;&#20811;&#26381;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#30340;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22303;&#32819;&#20854;&#35821;&#36825;&#26679;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#26469;&#35828;&#65292;&#24182;&#19981;&#30452;&#25509;&#65292;&#22240;&#20026;&#22797;&#26434;&#30340;&#20889;&#20316;&#35268;&#21017;&#38656;&#35201;&#38899;&#38901;&#12289;&#24418;&#24577;&#21644;&#21477;&#27861;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#22303;&#32819;&#20854;&#35821;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#27969;&#27700;&#32447;&#65292;&#28085;&#30422;&#20102;20&#22810;&#20010;&#19987;&#23478;&#31574;&#21010;&#30340;&#35821;&#27861;&#21644;&#25340;&#20889;&#35268;&#21017;&#65288;&#21363;&#20889;&#20316;&#35268;&#21017;&#65289;&#65292;&#36890;&#36807;&#22797;&#26434;&#30340;&#36716;&#25442;&#20989;&#25968;&#23454;&#29616;&#12290;&#20351;&#29992;&#36825;&#20010;&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#20174;&#19987;&#19994;&#32534;&#36753;&#30340;&#25991;&#31456;&#20013;&#27966;&#29983;&#20986;&#20102;13&#19975;&#26465;&#39640;&#36136;&#37327;&#30340;&#24179;&#34892;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25163;&#21160;&#27880;&#37322;&#19968;&#32452;&#30005;&#24433;&#35780;&#35770;&#26469;&#21019;&#24314;&#19968;&#20010;&#26356;&#30495;&#23454;&#30340;&#27979;&#35797;&#38598;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19977;&#20010;&#22522;&#20934;&#32447;&#65292;&#21046;&#23450;&#20102;&#20219;&#21153;&#30340;
&lt;/p&gt;
&lt;p&gt;
Grammatical Error Detection and Correction (GEC) tools have proven useful for native speakers and second language learners. Developing such tools requires a large amount of parallel, annotated data, which is unavailable for most languages. Synthetic data generation is a common practice to overcome the scarcity of such data. However, it is not straightforward for morphologically rich languages like Turkish due to complex writing rules that require phonological, morphological, and syntactic information. In this work, we present a flexible and extensible synthetic data generation pipeline for Turkish covering more than 20 expert-curated grammar and spelling rules (a.k.a., writing rules) implemented through complex transformation functions. Using this pipeline, we derive 130,000 high-quality parallel sentences from professionally edited articles. Additionally, we create a more realistic test set by manually annotating a set of movie reviews. We implement three baselines formulating the tas
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#23646;&#24615;&#24341;&#23548;&#26041;&#27861;&#26469;&#25506;&#32034;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#20989;&#25968;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#25552;&#20379;&#20102;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#20989;&#25968;&#25104;&#23545;&#26102;&#23646;&#24615;&#25913;&#21464;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#20915;&#31574;&#19982;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#21644;&#32422;&#26463;&#38590;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11343</link><description>&lt;p&gt;
&#20351;&#29992;&#23646;&#24615;&#24341;&#23548;&#26041;&#27861;&#26469;&#29702;&#35299;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Using Property Elicitation to Understand the Impacts of Fairness Constraints. (arXiv:2309.11343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11343
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#23646;&#24615;&#24341;&#23548;&#26041;&#27861;&#26469;&#25506;&#32034;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#20989;&#25968;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#25552;&#20379;&#20102;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#20989;&#25968;&#25104;&#23545;&#26102;&#23646;&#24615;&#25913;&#21464;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#20915;&#31574;&#19982;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#21644;&#32422;&#26463;&#38590;&#24230;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#20248;&#21270;&#26576;&#20010;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#28155;&#21152;&#27491;&#21017;&#21270;&#20989;&#25968;&#26469;&#26045;&#21152;&#36829;&#21453;&#32422;&#26463;&#30340;&#24809;&#32602;&#12290;&#39044;&#26399;&#22320;&#65292;&#28155;&#21152;&#36825;&#26679;&#30340;&#27491;&#21017;&#21270;&#20989;&#25968;&#21487;&#20197;&#25913;&#21464;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#20540;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#21738;&#20123;&#27491;&#21017;&#21270;&#20989;&#25968;&#20250;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#20540;&#65292;&#20197;&#21450;&#24403;&#26368;&#23567;&#21270;&#20540;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#23427;&#20250;&#22914;&#20309;&#21464;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#23646;&#24615;&#24341;&#23548;&#26041;&#27861;&#26469;&#21021;&#27493;&#20102;&#35299;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#20989;&#25968;&#19982;&#32473;&#23450;&#38382;&#39064;&#23454;&#20363;&#30340;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#32852;&#21512;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#25439;&#22833;&#20989;&#25968;&#21644;&#27491;&#21017;&#21270;&#20989;&#25968;&#25104;&#23545;&#26102;&#65292;&#23646;&#24615;&#25913;&#21464;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#20123;&#28385;&#36275;&#36825;&#20010;&#26465;&#20214;&#30340;&#27491;&#21017;&#21270;&#20989;&#25968;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#30340;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#20915;&#31574;&#22914;&#20309;&#38543;&#30528;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#21644;&#32422;&#26463;&#30340;&#38590;&#24230;&#32780;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive algorithms are often trained by optimizing some loss function, to which regularization functions are added to impose a penalty for violating constraints. As expected, the addition of such regularization functions can change the minimizer of the objective. It is not well-understood which regularizers change the minimizer of the loss, and, when the minimizer does change, how it changes. We use property elicitation to take first steps towards understanding the joint relationship between the loss and regularization functions and the optimal decision for a given problem instance. In particular, we give a necessary and sufficient condition on loss and regularizer pairs for when a property changes with the addition of the regularizer, and examine some regularizers satisfying this condition standard in the fair machine learning literature. We empirically demonstrate how algorithmic decision-making changes as a function of both data distribution changes and hardness of the constraint
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#25991;&#31456;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#39640;&#38454;&#35821;&#20041;&#30340;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11341</link><description>&lt;p&gt;
&#29992;&#36793;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#25991;&#31456;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Article Classification with Edge-Heterogeneous Graph Neural Networks. (arXiv:2309.11341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36793;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#25991;&#31456;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#39640;&#38454;&#35821;&#20041;&#30340;&#33410;&#28857;&#29305;&#24449;&#29983;&#25104;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29616;&#26377;&#21644;&#26032;&#21457;&#24067;&#30340;&#25991;&#31456;&#25968;&#37327;&#24222;&#22823;&#65292;&#23558;&#30740;&#31350;&#25104;&#26524;&#20998;&#31867;&#21040;&#29305;&#23450;&#19978;&#19979;&#25991;&#26631;&#31614;&#20307;&#31995;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36793;&#24322;&#26500;&#22270;&#34920;&#31034;&#26469;&#20016;&#23500;&#31616;&#21333;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27969;&#27700;&#32447;&#65292;&#20197;&#25552;&#39640;&#25991;&#31456;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;SciBERT&#26469;&#29983;&#25104;&#33410;&#28857;&#29305;&#24449;&#65292;&#20197;&#25429;&#25417;&#25991;&#31456;&#30340;&#25991;&#26412;&#20803;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;Open Graph Benchmark&#65288;OGB&#65289;ogbn-arxiv&#25968;&#25454;&#38598;&#21644;PubMed&#31958;&#23615;&#30149;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23436;&#20840;&#30417;&#30563;&#30340;&#20256;&#23548;&#24335;&#33410;&#28857;&#20998;&#31867;&#23454;&#39564;&#65292;&#20998;&#21035;&#36890;&#36807;Microsoft Academic Graph&#65288;MAG&#65289;&#21644;PubMed Central&#28155;&#21152;&#20102;&#38468;&#21152;&#20803;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36793;&#24322;&#26500;&#22270;&#30456;&#23545;&#20110;&#36793;&#21516;&#26500;&#22270;&#65292;&#33021;&#22815;&#22987;&#32456;&#25552;&#39640;&#25152;&#26377;GNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#20351;&#31616;&#21333;&#19988;&#27973;&#23618;&#30340;GNN&#27969;&#27700;&#32447;&#33021;&#22815;&#19982;&#26356;&#22797;&#26434;&#30340;&#26550;&#26500;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifying research output into context-specific label taxonomies is a challenging and relevant downstream task, given the volume of existing and newly published articles. We propose a method to enhance the performance of article classification by enriching simple Graph Neural Networks (GNN) pipelines with edge-heterogeneous graph representations. SciBERT is used for node feature generation to capture higher-order semantics within the articles' textual metadata. Fully supervised transductive node classification experiments are conducted on the Open Graph Benchmark (OGB) ogbn-arxiv dataset and the PubMed diabetes dataset, augmented with additional metadata from Microsoft Academic Graph (MAG) and PubMed Central, respectively. The results demonstrate that edge-heterogeneous graphs consistently improve the performance of all GNN models compared to the edge-homogeneous graphs. The transformed data enable simple and shallow GNN pipelines to achieve results on par with more complex architect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#25968;&#25454;&#20197;&#21450;&#25506;&#32034;&#20999;&#25442;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31361;&#23612;&#26031;&#26041;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26469;&#28040;&#38500;&#25340;&#20889;&#19981;&#21512;&#36866;&#30340;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2309.11327</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#25910;&#38598;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20999;&#25442;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition. (arXiv:2309.11327v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;&#21644;&#26631;&#27880;&#25968;&#25454;&#20197;&#21450;&#25506;&#32034;&#20999;&#25442;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31361;&#23612;&#26031;&#26041;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26469;&#28040;&#38500;&#25340;&#20889;&#19981;&#21512;&#36866;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#26041;&#35328;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#35201;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#36824;&#35201;&#22788;&#29702;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#31361;&#23612;&#26031;&#26041;&#35328;&#65292;&#35299;&#20915;&#20102;&#19978;&#36848;ASR&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25910;&#38598;&#20102;&#25991;&#26412;&#21644;&#38899;&#39057;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25506;&#32034;&#33258;&#25105;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#23569;&#26679;&#26412;&#20999;&#25442;&#26041;&#27861;&#65292;&#20197;&#22312;&#19981;&#21516;&#31361;&#23612;&#26031;&#27979;&#35797;&#38598;&#19978;&#25512;&#21160;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65307;&#28085;&#30422;&#19981;&#21516;&#30340;&#22768;&#23398;&#12289;&#35821;&#35328;&#21644;&#38901;&#24459;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#37492;&#20110;&#24120;&#35268;&#25340;&#20889;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#23545;&#36716;&#24405;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#65292;&#20197;&#36991;&#20813;&#27979;&#35797;&#21442;&#32771;&#20013;&#30340;&#25340;&#20889;&#19981;&#21512;&#36866;&#25152;&#24102;&#26469;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36716;&#24405;&#31361;&#23612;&#26031;&#38463;&#25289;&#20271;&#35821;&#12289;&#33521;&#35821;&#21644;&#27861;&#35821;&#28151;&#21512;&#35821;&#35328;&#30340;&#38899;&#39057;&#26679;&#26412;&#65292;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;&#25152;&#26377;&#35757;&#32451;&#21644;&#27979;&#35797;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#20379;&#20844;&#20247;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crafting an effective Automatic Speech Recognition (ASR) solution for dialects demands innovative approaches that not only address the data scarcity issue but also navigate the intricacies of linguistic diversity. In this paper, we address the aforementioned ASR challenge, focusing on the Tunisian dialect. First, textual and audio data is collected and in some cases annotated. Second, we explore self-supervision, semi-supervision and few-shot code-switching approaches to push the state-of-the-art on different Tunisian test sets; covering different acoustic, linguistic and prosodic conditions. Finally, and given the absence of conventional spelling, we produce a human evaluation of our transcripts to avoid the noise coming from spelling inadequacies in our testing references. Our models, allowing to transcribe audio samples in a linguistic mix involving Tunisian Arabic, English and French, and all the data used during training and testing are released for public use and further improvem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#21608;&#26399;&#24615;&#30340;Wavelet-Fourier&#21464;&#25442;&#32593;&#32476;&#65288;WFTNet&#65289;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#22312;&#21033;&#29992;&#20613;&#37324;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#25552;&#21462;&#26102;&#39057;&#20449;&#24687;&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#21608;&#26399;&#37325;&#35201;&#24615;&#21152;&#26435;&#31995;&#25968;&#65288;PWC&#65289;&#26469;&#24179;&#34913;&#20840;&#23616;&#21644;&#23616;&#37096;&#39057;&#29575;&#27169;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;WFTNet&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.11319</link><description>&lt;p&gt;
WFTNet&#65306;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#21608;&#26399;&#24615;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series Forecasting. (arXiv:2309.11319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#21608;&#26399;&#24615;&#30340;Wavelet-Fourier&#21464;&#25442;&#32593;&#32476;&#65288;WFTNet&#65289;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#22312;&#21033;&#29992;&#20613;&#37324;&#21494;&#21644;&#23567;&#27874;&#21464;&#25442;&#25552;&#21462;&#26102;&#39057;&#20449;&#24687;&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#21608;&#26399;&#37325;&#35201;&#24615;&#21152;&#26435;&#31995;&#25968;&#65288;PWC&#65289;&#26469;&#24179;&#34913;&#20840;&#23616;&#21644;&#23616;&#37096;&#39057;&#29575;&#27169;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;WFTNet&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#27169;&#22411;&#23581;&#35797;&#21033;&#29992;&#39057;&#29575;&#21644;&#21608;&#26399;&#24615;&#20449;&#24687;&#36827;&#34892;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#37117;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#32454;&#31890;&#24230;&#21644;&#23616;&#37096;&#39057;&#29575;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Wavelet-Fourier Transform Network&#65288;WFTNet&#65289;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;WFTNet&#21033;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#23567;&#27874;&#21464;&#25442;&#20174;&#20449;&#21495;&#20013;&#25552;&#21462;&#20840;&#38754;&#30340;&#26102;&#39057;&#20449;&#24687;&#65292;&#20854;&#20013;&#20613;&#37324;&#21494;&#21464;&#25442;&#25429;&#25417;&#20840;&#23616;&#21608;&#26399;&#27169;&#24335;&#65292;&#32780;&#23567;&#27874;&#21464;&#25442;&#25429;&#25417;&#23616;&#37096;&#21608;&#26399;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21608;&#26399;&#37325;&#35201;&#24615;&#21152;&#26435;&#31995;&#25968;&#65288;PWC&#65289;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#24179;&#34913;&#20840;&#23616;&#21644;&#23616;&#37096;&#39057;&#29575;&#27169;&#24335;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;WFTNet&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent CNN and Transformer-based models tried to utilize frequency and periodicity information for long-term time series forecasting. However, most existing work is based on Fourier transform, which cannot capture fine-grained and local frequency structure. In this paper, we propose a Wavelet-Fourier Transform Network (WFTNet) for long-term time series forecasting. WFTNet utilizes both Fourier and wavelet transforms to extract comprehensive temporal-frequency information from the signal, where Fourier transform captures the global periodic patterns and wavelet transform captures the local ones. Furthermore, we introduce a Periodicity-Weighted Coefficient (PWC) to adaptively balance the importance of global and local frequency patterns. Extensive experiments on various time series datasets show that WFTNet consistently outperforms other state-of-the-art baseline.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;C&amp;F&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21069;&#20026;&#27599;&#20010;&#20219;&#21153;&#26500;&#24314;&#19968;&#20010;&#24179;&#22374;&#30340;&#35757;&#32451;&#31354;&#38388;&#65292;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#22312;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#26102;&#33258;&#36866;&#24212;&#22320;&#21019;&#24314;&#24179;&#22374;&#21306;&#22495;&#65292;&#24182;&#26681;&#25454;&#21442;&#25968;&#30340;&#24179;&#22374;&#31243;&#24230;&#25214;&#21040;&#20854;&#23545;&#24403;&#21069;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36866;&#24212;&#26032;&#20219;&#21153;&#26102;&#65292;&#21033;&#29992;&#24179;&#22374;&#24230;&#24212;&#29992;&#32422;&#26463;&#24182;&#20026;&#26032;&#20219;&#21153;&#20934;&#22791;&#24179;&#22374;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.11305</link><description>&lt;p&gt;
&#21019;&#24314;&#21644;&#21457;&#29616;&#24179;&#38754;&#65306;&#20026;&#36830;&#32493;&#23398;&#20064;&#26500;&#24314;&#24179;&#22374;&#35757;&#32451;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Create and Find Flatness: Building Flat Training Spaces in Advance for Continual Learning. (arXiv:2309.11305v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11305
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;C&amp;F&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#21069;&#20026;&#27599;&#20010;&#20219;&#21153;&#26500;&#24314;&#19968;&#20010;&#24179;&#22374;&#30340;&#35757;&#32451;&#31354;&#38388;&#65292;&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#22312;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#26102;&#33258;&#36866;&#24212;&#22320;&#21019;&#24314;&#24179;&#22374;&#21306;&#22495;&#65292;&#24182;&#26681;&#25454;&#21442;&#25968;&#30340;&#24179;&#22374;&#31243;&#24230;&#25214;&#21040;&#20854;&#23545;&#24403;&#21069;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36866;&#24212;&#26032;&#20219;&#21153;&#26102;&#65292;&#21033;&#29992;&#24179;&#22374;&#24230;&#24212;&#29992;&#32422;&#26463;&#24182;&#20026;&#26032;&#20219;&#21153;&#20934;&#22791;&#24179;&#22374;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#25361;&#25112;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#21560;&#25910;&#26032;&#20449;&#24687;&#26102;&#24456;&#38590;&#20445;&#30041;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#21482;&#27880;&#37325;&#22312;&#36935;&#21040;&#26032;&#20219;&#21153;&#26102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#24573;&#35270;&#20102;&#20219;&#21153;&#21069;&#38454;&#27573;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#31227;&#21040;&#24403;&#21069;&#20219;&#21153;&#23398;&#20064;&#38454;&#27573;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;C&amp;F&#65288;&#21019;&#24314;&#21644;&#21457;&#29616;&#24179;&#38754;&#65289;&#65292;&#35813;&#26694;&#26550;&#20026;&#27599;&#20010;&#20219;&#21153;&#25552;&#21069;&#24314;&#31435;&#19968;&#20010;&#24179;&#22374;&#30340;&#35757;&#32451;&#31354;&#38388;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33258;&#36866;&#24212;&#22320;&#21019;&#24314;&#20102;&#19968;&#20010;&#22312;&#25439;&#22833;&#31354;&#38388;&#20013;&#26368;&#23567;&#20540;&#21608;&#22260;&#30340;&#24179;&#22374;&#21306;&#22495;&#12290;&#38543;&#21518;&#65292;&#23427;&#26681;&#25454;&#21442;&#25968;&#30340;&#24179;&#22374;&#24230;&#25214;&#21040;&#23427;&#20204;&#23545;&#24403;&#21069;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36866;&#24212;&#27169;&#22411;&#21040;&#26032;&#20219;&#21153;&#26102;&#65292;&#26681;&#25454;&#24179;&#22374;&#24230;&#24212;&#29992;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#20934;&#22791;&#19968;&#20010;&#24179;&#22374;&#31354;&#38388;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#21019;&#24314;&#21644;&#21457;&#29616;&#24179;&#22374;&#24230;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Catastrophic forgetting remains a critical challenge in the field of continual learning, where neural networks struggle to retain prior knowledge while assimilating new information. Most existing studies emphasize mitigating this issue only when encountering new tasks, overlooking the significance of the pre-task phase. Therefore, we shift the attention to the current task learning stage, presenting a novel framework, C&amp;F (Create and Find Flatness), which builds a flat training space for each task in advance. Specifically, during the learning of the current task, our framework adaptively creates a flat region around the minimum in the loss landscape. Subsequently, it finds the parameters' importance to the current task based on their flatness degrees. When adapting the model to a new task, constraints are applied according to the flatness and a flat space is simultaneously prepared for the impending task. We theoretically demonstrate the consistency between the created and found flatne
&lt;/p&gt;</description></item><item><title>CPLLM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#26469;&#39044;&#27979;&#30446;&#26631;&#30142;&#30149;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CPLLM&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11295</link><description>&lt;p&gt;
CPLLM: &#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
CPLLM: Clinical Prediction with Large Language Models. (arXiv:2309.11295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11295
&lt;/p&gt;
&lt;p&gt;
CPLLM&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#24739;&#32773;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#26469;&#39044;&#27979;&#30446;&#26631;&#30142;&#30149;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CPLLM&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#36827;&#34892;&#20020;&#24202;&#30142;&#30149;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#21033;&#29992;&#37327;&#21270;&#21644;&#25552;&#31034;&#26469;&#24494;&#35843;LLM&#65292;&#20219;&#21153;&#26159;&#39044;&#27979;&#24739;&#32773;&#22312;&#19979;&#19968;&#27425;&#23601;&#35786;&#25110;&#38543;&#21518;&#30340;&#35786;&#26029;&#20013;&#26159;&#21542;&#20250;&#34987;&#35786;&#26029;&#20026;&#30446;&#26631;&#30142;&#30149;&#65292;&#24182;&#21033;&#29992;&#20182;&#20204;&#30340;&#21382;&#21490;&#35786;&#26029;&#35760;&#24405;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#36923;&#36753;&#22238;&#24402;&#12289;RETAIN&#21644;Med-BERT&#65292;&#21518;&#32773;&#26159;&#20351;&#29992;&#32467;&#26500;&#21270;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#30142;&#30149;&#39044;&#27979;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CPLLM&#22312;PR-AUC&#21644;ROC-AUC&#25351;&#26631;&#19978;&#22343;&#36229;&#36807;&#20102;&#25152;&#26377;&#27979;&#35797;&#27169;&#22411;&#65292;&#30456;&#27604;&#22522;&#32447;&#27169;&#22411;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Clinical Prediction with Large Language Models (CPLLM), a method that involves fine-tuning a pre-trained Large Language Model (LLM) for clinical disease prediction. We utilized quantization and fine-tuned the LLM using prompts, with the task of predicting whether patients will be diagnosed with a target disease during their next visit or in the subsequent diagnosis, leveraging their historical diagnosis records. We compared our results versus various baselines, including Logistic Regression, RETAIN, and Med-BERT, which is the current state-of-the-art model for disease prediction using structured EHR data. Our experiments have shown that CPLLM surpasses all the tested models in terms of both PR-AUC and ROC-AUC metrics, displaying noteworthy enhancements compared to the baseline models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#23884;&#20837;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;t-SNE&#30340;&#37051;&#22495;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#34920;&#31034;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.11294</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#65306;&#34913;&#37327;&#23884;&#20837;&#30340;&#34920;&#31034;&#33021;&#21147;&#20197;&#20445;&#30041;&#32467;&#26500;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy: Measuring Representation Capacity of Embeddings to Preserve Structural and Contextual Information. (arXiv:2309.11294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#23884;&#20837;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;t-SNE&#30340;&#37051;&#22495;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#34920;&#31034;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#26377;&#25928;&#22320;&#34920;&#31034;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#25429;&#25417;&#21040;&#20102;&#25968;&#25454;&#30340;&#24213;&#23618;&#32467;&#26500;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#23884;&#20837;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#25454;&#34920;&#31034;&#25216;&#26415;&#65292;&#20294;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#20445;&#30041;&#32467;&#26500;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#34913;&#37327;&#23884;&#20837;&#30340;&#8220;&#34920;&#31034;&#33021;&#21147;&#8221;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#21160;&#26426;&#28304;&#20110;&#29702;&#35299;&#23884;&#20837;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#33021;&#22815;&#22312;&#36873;&#25321;&#36866;&#21512;&#20854;&#29305;&#23450;&#24212;&#29992;&#30340;&#23884;&#20837;&#27169;&#22411;&#26102;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#23558;&#20998;&#31867;&#21644;&#32858;&#31867;&#31561;&#22806;&#37096;&#35780;&#20272;&#26041;&#27861;&#19982;&#22522;&#20110;t-SNE&#30340;&#37051;&#22495;&#20998;&#26512;&#26041;&#27861;&#65288;&#22914;&#37051;&#22495;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24230;&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#23545;&#34920;&#31034;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20248;&#21270;&#25216;&#26415;&#30340;&#26041;&#27861;&#33021;&#22686;&#24378;&#34920;&#31034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective representation of data is crucial in various machine learning tasks, as it captures the underlying structure and context of the data. Embeddings have emerged as a powerful technique for data representation, but evaluating their quality and capacity to preserve structural and contextual information remains a challenge. In this paper, we address this need by proposing a method to measure the \textit{representation capacity} of embeddings. The motivation behind this work stems from the importance of understanding the strengths and limitations of embeddings, enabling researchers and practitioners to make informed decisions in selecting appropriate embedding models for their specific applications. By combining extrinsic evaluation methods, such as classification and clustering, with t-SNE-based neighborhood analysis, such as neighborhood agreement and trustworthiness, we provide a comprehensive assessment of the representation capacity. Additionally, the use of optimization techni
&lt;/p&gt;</description></item><item><title>AuTexTification&#26159;IberLEF 2023&#30740;&#35752;&#20250;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#24402;&#23646;&#22810;&#39046;&#22495;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;160,000&#22810;&#26465;&#25991;&#26412;&#65292;&#28085;&#30422;&#20102;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20197;&#21450;&#25512;&#25991;&#12289;&#35780;&#35770;&#12289;&#26032;&#38395;&#12289;&#27861;&#24459;&#21644;&#25805;&#20316;&#25351;&#21335;&#31561;&#20116;&#20010;&#39046;&#22495;&#12290;&#20849;&#26377;114&#20010;&#22242;&#38431;&#21442;&#19982;&#65292;&#25552;&#20132;&#20102;175&#27425;&#36816;&#34892;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11285</link><description>&lt;p&gt;
&#12298;IberLEF 2023&#30340;AuTexTification&#27010;&#36848;&#65306;&#22810;&#39046;&#22495;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#21644;&#24402;&#23646;&#12299;
&lt;/p&gt;
&lt;p&gt;
Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains. (arXiv:2309.11285v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11285
&lt;/p&gt;
&lt;p&gt;
AuTexTification&#26159;IberLEF 2023&#30740;&#35752;&#20250;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#26088;&#22312;&#26816;&#27979;&#21644;&#24402;&#23646;&#22810;&#39046;&#22495;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;160,000&#22810;&#26465;&#25991;&#26412;&#65292;&#28085;&#30422;&#20102;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20197;&#21450;&#25512;&#25991;&#12289;&#35780;&#35770;&#12289;&#26032;&#38395;&#12289;&#27861;&#24459;&#21644;&#25805;&#20316;&#25351;&#21335;&#31561;&#20116;&#20010;&#39046;&#22495;&#12290;&#20849;&#26377;114&#20010;&#22242;&#38431;&#21442;&#19982;&#65292;&#25552;&#20132;&#20102;175&#27425;&#36816;&#34892;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20316;&#20026;IberLEF 2023&#30740;&#35752;&#20250;&#19968;&#37096;&#20998;&#30340;AuTexTification&#20849;&#20139;&#20219;&#21153;&#30340;&#27010;&#36848;&#65292;&#35813;&#30740;&#35752;&#20250;&#26159;&#22312;SEPLN 2023&#20250;&#35758;&#26694;&#26550;&#20869;&#30340;&#20234;&#27604;&#21033;&#20122;&#35821;&#35328;&#35780;&#20272;&#35770;&#22363;&#20013;&#36827;&#34892;&#30340;&#12290;AuTexTification&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#22312;&#23376;&#20219;&#21153;1&#20013;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#30830;&#23450;&#19968;&#27573;&#25991;&#26412;&#26159;&#20154;&#24037;&#25776;&#20889;&#36824;&#26159;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#12290;&#22312;&#23376;&#20219;&#21153;2&#20013;&#65292;&#21442;&#19982;&#32773;&#38656;&#35201;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#24402;&#23646;&#20110;&#20845;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;AuTexTification 2023&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20004;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65289;&#21644;&#20116;&#20010;&#39046;&#22495;&#65288;&#25512;&#25991;&#12289;&#35780;&#35770;&#12289;&#26032;&#38395;&#12289;&#27861;&#24459;&#21644;&#25805;&#20316;&#25351;&#21335;&#65289;&#65292;&#20849;&#21253;&#21547;&#36229;&#36807;160,000&#26465;&#25991;&#26412;&#12290;&#20849;&#26377;114&#20010;&#22242;&#38431;&#25253;&#21517;&#21442;&#19982;&#65292;&#20854;&#20013;36&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;175&#27425;&#36816;&#34892;&#32467;&#26524;&#65292;&#20854;&#20013;20&#20010;&#22242;&#38431;&#36824;&#25552;&#20132;&#20102;&#24037;&#20316;&#31508;&#35760;&#12290;&#22312;&#36825;&#20010;&#27010;&#36848;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AuTexTification&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#65292;&#20197;&#21450;&#21442;&#19982;&#31995;&#32479;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the overview of the AuTexTification shared task as part of the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the framework of the SEPLN 2023 conference. AuTexTification consists of two subtasks: for Subtask 1, participants had to determine whether a text is human-authored or has been generated by a large language model. For Subtask 2, participants had to attribute a machine-generated text to one of six different text generation models. Our AuTexTification 2023 dataset contains more than 160.000 texts across two languages (English and Spanish) and five domains (tweets, reviews, news, legal, and how-to articles). A total of 114 teams signed up to participate, of which 36 sent 175 runs, and 20 of them sent their working notes. In this overview, we present the AuTexTification dataset and task, the submitted participating systems, and the results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#35010;&#32541;&#20998;&#21106;&#21644;&#30417;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#30340;&#35299;&#37322;&#20013;&#23548;&#20986;&#20998;&#21106;&#32467;&#26524;&#65292;&#26080;&#38656;&#20687;&#32032;&#32423;&#27880;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#36827;&#34892;&#35010;&#32541;&#20998;&#21106;&#21644;&#29983;&#38271;&#30417;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.11267</link><description>&lt;p&gt;
&#20174;&#20998;&#31867;&#21040;&#20998;&#21106;&#19982;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65306;&#20851;&#20110;&#35010;&#32441;&#26816;&#27979;&#21644;&#29983;&#38271;&#30417;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Classification to Segmentation with Explainable AI: A Study on Crack Detection and Growth Monitoring. (arXiv:2309.11267v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#35010;&#32541;&#20998;&#21106;&#21644;&#30417;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#20998;&#31867;&#22120;&#30340;&#35299;&#37322;&#20013;&#23548;&#20986;&#20998;&#21106;&#32467;&#26524;&#65292;&#26080;&#38656;&#20687;&#32032;&#32423;&#27880;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#26377;&#25928;&#36827;&#34892;&#35010;&#32541;&#20998;&#21106;&#21644;&#29983;&#38271;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#22522;&#30784;&#35774;&#26045;&#30340;&#34920;&#38754;&#35010;&#32541;&#23545;&#20110;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#21160;&#35270;&#35273;&#26816;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29305;&#21035;&#26159;&#22312;&#38590;&#20197;&#21040;&#36798;&#30340;&#21306;&#22495;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#19968;&#26086;&#26816;&#27979;&#21040;&#35010;&#32541;&#65292;&#30417;&#27979;&#20854;&#20005;&#37325;&#31243;&#24230;&#36890;&#24120;&#38656;&#35201;&#23545;&#25439;&#23475;&#36827;&#34892;&#31934;&#30830;&#30340;&#20998;&#21106;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20998;&#21106;&#32780;&#35328;&#65292;&#22270;&#20687;&#30340;&#20687;&#32032;&#32423;&#27880;&#37322;&#26159;&#19968;&#39033;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#65292;&#21487;&#20197;&#21033;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20174;&#20998;&#31867;&#22120;&#30340;&#35299;&#37322;&#20013;&#23548;&#20986;&#20998;&#21106;&#32467;&#26524;&#65292;&#20165;&#38656;&#35201;&#24369;&#22270;&#20687;&#32423;&#21035;&#30340;&#30417;&#30563;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20998;&#21106;&#21644;&#30417;&#27979;&#34920;&#38754;&#35010;&#32541;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;XAI&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#20419;&#36827;&#20005;&#37325;&#31243;&#24230;&#37327;&#21270;&#21644;&#29983;&#38271;&#30417;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#24471;&#21040;&#30340;&#20998;&#21106;&#25513;&#27169;&#21487;&#33021;&#36739;&#20302;&#36136;&#37327;&#65292;&#20294;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#20173;&#33021;&#26377;&#25928;&#22320;&#36827;&#34892;&#35010;&#32441;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring surface cracks in infrastructure is crucial for structural health monitoring. Automatic visual inspection offers an effective solution, especially in hard-to-reach areas. Machine learning approaches have proven their effectiveness but typically require large annotated datasets for supervised training. Once a crack is detected, monitoring its severity often demands precise segmentation of the damage. However, pixel-level annotation of images for segmentation is labor-intensive. To mitigate this cost, one can leverage explainable artificial intelligence (XAI) to derive segmentations from the explanations of a classifier, requiring only weak image-level supervision. This paper proposes applying this methodology to segment and monitor surface cracks. We evaluate the performance of various XAI methods and examine how this approach facilitates severity quantification and growth monitoring. Results reveal that while the resulting segmentation masks may exhibit lower quality than th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;BART&#12289;T5&#21644;BERT2BERT-style&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11259</link><description>&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-Sequence Spanish Pre-trained Language Models. (arXiv:2309.11259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;BART&#12289;T5&#21644;BERT2BERT-style&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#22823;&#36827;&#23637;&#20026;&#35768;&#22810;&#38750;&#33521;&#35821;&#35821;&#35328;&#29256;&#26412;&#30340;&#24320;&#21457;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#20102;&#20165;&#32534;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#30340;&#26550;&#26500;&#12290;&#34429;&#28982;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21253;&#25324;BERT&#12289;RoBERTa&#21644;GPT&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20248;&#21183;&#65292;&#20294;&#22312;&#28041;&#21450;&#36755;&#20837;&#36755;&#20986;&#23545;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#32570;&#20047;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23454;&#26045;&#21644;&#35780;&#20272;&#33879;&#21517;&#30340;&#20165;&#22312;&#35199;&#29677;&#29273;&#35821;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24320;&#21019;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BART&#12289;T5&#21644;BERT2BERT&#39118;&#26684;&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#35821;&#29256;&#26412;&#65292;&#24182;&#23545;&#23427;&#20204;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#37325;&#36848;&#21644;&#29983;&#25104;&#24335;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#20854;&#20013;BART&#21644;T5&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, substantial advancements in pre-trained language models have paved the way for the development of numerous non-English language versions, with a particular focus on encoder-only and decoder-only architectures. While Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited prowess in natural language understanding and generation, there remains a scarcity of encoder-decoder models designed for sequence-to-sequence tasks involving input-output pairs. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder-decoder architectures, exclusively pre-trained on Spanish corpora. Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across a diverse range of sequence-to-sequence tasks, spanning summarization, rephrasing, and generative question answering. Our findings underscore the competitive performance of all models, with BART and T5 emerging a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#31354;&#20013;&#25112;&#26007;&#26426;&#21160;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20915;&#31574;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#30340;&#25277;&#35937;&#26469;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35757;&#32451;&#20302;&#23618;&#31574;&#30053;&#23454;&#29616;&#20934;&#30830;&#30340;&#21333;&#20301;&#25112;&#26007;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.11247</link><description>&lt;p&gt;
&#38024;&#23545;&#31354;&#20013;&#25112;&#26007;&#26426;&#21160;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Agent Reinforcement Learning for Air Combat Maneuvering. (arXiv:2309.11247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11247
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#31354;&#20013;&#25112;&#26007;&#26426;&#21160;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20915;&#31574;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#30340;&#25277;&#35937;&#26469;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#35757;&#32451;&#20302;&#23618;&#31574;&#30053;&#23454;&#29616;&#20934;&#30830;&#30340;&#21333;&#20301;&#25112;&#26007;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#27169;&#25311;&#31354;&#20013;&#23545;&#31354;&#26684;&#26007;&#22330;&#26223;&#27491;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#39640;&#32500;&#29366;&#24577;&#21644;&#34892;&#21160;&#31354;&#38388;&#12289;&#24773;&#20917;&#20449;&#24687;&#30340;&#39640;&#22797;&#26434;&#24615;&#65288;&#22914;&#19981;&#23436;&#32654;&#21644;&#31579;&#36873;&#20449;&#24687;&#12289;&#38543;&#26426;&#24615;&#12289;&#20851;&#20110;&#20219;&#21153;&#30446;&#26631;&#30340;&#19981;&#23436;&#20840;&#30693;&#35782;&#65289;&#20197;&#21450;&#38750;&#32447;&#24615;&#39134;&#34892;&#21160;&#21147;&#23398;&#32473;&#20934;&#30830;&#30340;&#31354;&#25112;&#20915;&#31574;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#24403;&#28041;&#21450;&#22810;&#20010;&#24322;&#26500;&#26234;&#33021;&#20307;&#26102;&#65292;&#36825;&#20123;&#25361;&#25112;&#21464;&#24471;&#26356;&#21152;&#20005;&#23803;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#20010;&#24322;&#26500;&#26234;&#33021;&#20307;&#30340;&#31354;&#23545;&#31354;&#26684;&#26007;&#30340;&#20998;&#23618;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#20915;&#31574;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#23618;&#27425;&#30340;&#25277;&#35937;&#65292;&#24322;&#26500;&#30340;&#20302;&#23618;&#31574;&#30053;&#25511;&#21046;&#21333;&#20010;&#21333;&#20301;&#30340;&#34892;&#21160;&#65292;&#24182;&#19988;&#22312;&#25972;&#20307;&#20219;&#21153;&#30446;&#26631;&#19979;&#65292;&#39640;&#23618;&#25351;&#25381;&#31574;&#30053;&#21457;&#20986;&#23439;&#35266;&#21629;&#20196;&#12290;&#23545;&#20110;&#20934;&#30830;&#30340;&#21333;&#20301;&#25112;&#26007;&#25511;&#21046;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20302;&#23618;&#31574;&#30053;&#12290;&#20182;&#20204;&#30340;&#35757;&#32451;&#26159;&#20197;&#19968;&#31181;&#23398;&#20064;&#26041;&#24335;&#32452;&#32455;&#36215;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of artificial intelligence to simulate air-to-air combat scenarios is attracting increasing attention. To date the high-dimensional state and action spaces, the high complexity of situation information (such as imperfect and filtered information, stochasticity, incomplete knowledge about mission targets) and the nonlinear flight dynamics pose significant challenges for accurate air combat decision-making. These challenges are exacerbated when multiple heterogeneous agents are involved. We propose a hierarchical multi-agent reinforcement learning framework for air-to-air combat with multiple heterogeneous agents. In our framework, the decision-making process is divided into two stages of abstraction, where heterogeneous low-level policies control the action of individual units, and a high-level commander policy issues macro commands given the overall mission targets. Low-level policies are trained for accurate unit combat control. Their training is organized in a learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33609;&#26681;&#25805;&#20316;&#25628;&#32034;&#30340;&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#25628;&#32034;&#39640;&#25928;&#30340;&#25805;&#20316;&#26367;&#25442;&#26469;&#36866;&#24212;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#32473;&#23450;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25968;&#23398;&#25351;&#20196;&#20316;&#20026;&#22522;&#30784;&#65292;&#36873;&#25321;&#39640;&#25928;&#26367;&#25442;&#25805;&#20316;&#20197;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11246</link><description>&lt;p&gt;
&#22522;&#20110;&#33609;&#26681;&#25805;&#20316;&#25628;&#32034;&#30340;&#27169;&#22411;&#36793;&#32536;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Grassroots Operator Search for Model Edge Adaptation. (arXiv:2309.11246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#33609;&#26681;&#25805;&#20316;&#25628;&#32034;&#30340;&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#25628;&#32034;&#39640;&#25928;&#30340;&#25805;&#20316;&#26367;&#25442;&#26469;&#36866;&#24212;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#32473;&#23450;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#25968;&#23398;&#25351;&#20196;&#20316;&#20026;&#22522;&#30784;&#65292;&#36873;&#25321;&#39640;&#25928;&#26367;&#25442;&#25805;&#20316;&#20197;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30828;&#20214;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;HW-NAS&#65289;&#22312;&#35774;&#35745;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#19978;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#19968;&#20010;&#39640;&#25928;&#19988;&#28789;&#27963;&#30340;&#25628;&#32034;&#31354;&#38388;&#23545;&#20110;HW-NAS&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20391;&#37325;&#20110;&#35774;&#35745;&#19968;&#20010;&#23439;&#26550;&#26500;&#65292;&#24182;&#22522;&#20110;&#19968;&#32452;&#21487;&#33021;&#20540;&#26469;&#25628;&#32034;&#26550;&#26500;&#30340;&#36229;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#21463;&#28145;&#24230;&#23398;&#20064;&#24037;&#31243;&#24072;&#21644;&#26631;&#20934;&#24314;&#27169;&#26041;&#27861;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33609;&#26681;&#25805;&#20316;&#25628;&#32034;&#65288;GOS&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;HW-NAS&#36890;&#36807;&#25628;&#32034;&#39640;&#25928;&#30340;&#25805;&#20316;&#26367;&#25442;&#65292;&#26469;&#36866;&#24212;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#32473;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#25805;&#20316;&#34920;&#31034;&#20026;&#19968;&#32452;&#33021;&#25429;&#25417;&#20854;&#34892;&#20026;&#30340;&#25968;&#23398;&#25351;&#20196;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#25968;&#23398;&#25351;&#20196;&#34987;&#29992;&#20316;&#25628;&#32034;&#21644;&#36873;&#25321;&#39640;&#25928;&#26367;&#25442;&#25805;&#20316;&#30340;&#22522;&#30784;&#65292;&#20197;&#20445;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20197;&#33609;&#26681;&#20026;&#22522;&#30784;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#25968;&#23398;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hardware-aware Neural Architecture Search (HW-NAS) is increasingly being used to design efficient deep learning architectures. An efficient and flexible search space is crucial to the success of HW-NAS. Current approaches focus on designing a macro-architecture and searching for the architecture's hyperparameters based on a set of possible values. This approach is biased by the expertise of deep learning (DL) engineers and standard modeling approaches. In this paper, we present a Grassroots Operator Search (GOS) methodology. Our HW-NAS adapts a given model for edge devices by searching for efficient operator replacement. We express each operator as a set of mathematical instructions that capture its behavior. The mathematical instructions are then used as the basis for searching and selecting efficient replacement operators that maintain the accuracy of the original model while reducing computational complexity. Our approach is grassroots since it relies on the mathematical foundations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#36827;&#34892;&#39044;&#27979;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20840;&#21442;&#25968;&#26102;&#38388;&#22797;&#26434;&#24230;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#21457;&#29616;&#35757;&#32451;&#26102;&#38388;&#30340;&#39044;&#27979;&#19982;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.11226</link><description>&lt;p&gt;
&#20026;&#25903;&#25345;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#24320;&#21457;&#32780;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26102;&#38388;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards a Prediction of Machine Learning Training Time to Support Continuous Learning Systems Development. (arXiv:2309.11226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#36827;&#34892;&#39044;&#27979;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20840;&#21442;&#25968;&#26102;&#38388;&#22797;&#26434;&#24230;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#21457;&#29616;&#35757;&#32451;&#26102;&#38388;&#30340;&#39044;&#27979;&#19982;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30028;&#20013;&#65292;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#26102;&#38388;&#30340;&#38382;&#39064;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#33021;&#22815;&#39044;&#20808;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#26102;&#38388;&#23558;&#20351;&#24471;&#33021;&#22815;&#33258;&#21160;&#36873;&#25321;&#26368;&#20339;&#27169;&#22411;&#65292;&#21253;&#25324;&#22312;&#33021;&#28304;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#65292;&#20363;&#22914;&#22312;MLOps&#26550;&#26500;&#30340;&#32972;&#26223;&#19979;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#37073;&#31561;&#20154;&#25552;&#20986;&#30340;&#20840;&#21442;&#25968;&#26102;&#38388;&#22797;&#26434;&#24230;&#65288;FPTC&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#36825;&#26159;&#25105;&#20204;&#30446;&#21069;&#25152;&#30693;&#21807;&#19968;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#26102;&#38388;&#24418;&#24335;&#21270;&#20026;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#30340;&#20844;&#24335;&#65292;&#24182;&#31361;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#21644;&#24369;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20174;&#25152;&#36827;&#34892;&#30340;&#30740;&#31350;&#20013;&#65292;&#35757;&#32451;&#26102;&#38388;&#30340;&#39044;&#27979;&#19982;...
&lt;/p&gt;
&lt;p&gt;
The problem of predicting the training time of machine learning (ML) models has become extremely relevant in the scientific community. Being able to predict a priori the training time of an ML model would enable the automatic selection of the best model both in terms of energy efficiency and in terms of performance in the context of, for instance, MLOps architectures. In this paper, we present the work we are conducting towards this direction. In particular, we present an extensive empirical study of the Full Parameter Time Complexity (FPTC) approach by Zheng et al., which is, to the best of our knowledge, the only approach formalizing the training time of ML models as a function of both dataset's and model's parameters. We study the formulations proposed for the Logistic Regression and Random Forest classifiers, and we highlight the main strengths and weaknesses of the approach. Finally, we observe how, from the conducted study, the prediction of training time is strictly related to t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#21306;&#22359;&#38142;&#24212;&#29992;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#23450;&#37197;&#32622;&#21442;&#25968;&#39044;&#27979;&#24615;&#33021;&#24182;&#25506;&#32034;&#26368;&#20339;&#37197;&#32622;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#21306;&#22359;&#38142;&#24212;&#29992;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11205</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#21306;&#22359;&#38142;&#24212;&#29992;&#24615;&#33021;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Model-Based Machine Learning Approach for Assessing the Performance of Blockchain Applications. (arXiv:2309.11205v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#21306;&#22359;&#38142;&#24212;&#29992;&#24615;&#33021;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#23450;&#37197;&#32622;&#21442;&#25968;&#39044;&#27979;&#24615;&#33021;&#24182;&#25506;&#32034;&#26368;&#20339;&#37197;&#32622;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#21306;&#22359;&#38142;&#24212;&#29992;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#24041;&#22266;&#20102;&#20854;&#20316;&#20026;&#21508;&#20010;&#39046;&#22495;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#30340;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#30784;&#35774;&#26045;&#30340;&#22797;&#26434;&#24615;&#21644;&#20998;&#24067;&#24335;&#29305;&#24615;&#65292;&#35780;&#20272;&#21306;&#22359;&#38142;&#24212;&#29992;&#30340;&#24615;&#33021;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#21487;&#38752;&#30340;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#39640;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#24212;&#29992;&#30340;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#34429;&#28982;&#24050;&#32463;&#36827;&#34892;&#20102;&#22522;&#20110;&#27169;&#25311;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#30740;&#31350;&#65292;&#20294;&#24456;&#23569;&#35752;&#35770;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#19982;&#35780;&#20272;&#21306;&#22359;&#38142;&#24212;&#29992;&#24615;&#33021;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#20004;&#31181;&#22522;&#20110;ML&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;k&#26368;&#36817;&#37051;&#65288;kNN&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26469;&#39044;&#27979;&#20351;&#29992;&#39044;&#23450;&#37197;&#32622;&#21442;&#25968;&#30340;&#21306;&#22359;&#38142;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;salp swarm optimization&#65288;SO&#65289;ML&#27169;&#22411;&#65292;&#21487;&#20197;&#25506;&#32034;&#23454;&#29616;&#25152;&#38656;&#24615;&#33021;&#27700;&#24179;&#30340;&#26368;&#20339;&#21306;&#22359;&#38142;&#37197;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#31895;&#31961;&#38598;&#26041;&#27861;&#36827;&#34892;&#20102;&#25968;&#25454;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#21644;&#39564;&#35777;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancement of Blockchain technology consolidates its status as a viable alternative for various domains. However, evaluating the performance of blockchain applications can be challenging due to the underlying infrastructure's complexity and distributed nature. Therefore, a reliable modelling approach is needed to boost Blockchain-based applications' development and evaluation. While simulation-based solutions have been researched, machine learning (ML) model-based techniques are rarely discussed in conjunction with evaluating blockchain application performance. Our novel research makes use of two ML model-based methods. Firstly, we train a $k$ nearest neighbour ($k$NN) and support vector machine (SVM) to predict blockchain performance using predetermined configuration parameters. Secondly, we employ the salp swarm optimization (SO) ML model which enables the investigation of optimal blockchain configurations for achieving the required performance level. We use rough set the
&lt;/p&gt;</description></item><item><title>Languini Kitchen&#26159;&#19968;&#20010;&#30740;&#31350;&#38598;&#20307;&#21644;&#20195;&#30721;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#31561;&#25928;&#35745;&#31639;&#26469;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#35201;&#28857;&#65306;&#23454;&#39564;&#21327;&#35758;&#12289;&#27169;&#22411;&#27604;&#36739;&#12289;&#31561;&#25928;&#35745;&#31639;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.11197</link><description>&lt;p&gt;
Languini Kitchen: &#22312;&#19981;&#21516;&#35745;&#31639;&#35268;&#27169;&#19978;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Languini Kitchen: Enabling Language Modelling Research at Different Scales of Compute. (arXiv:2309.11197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11197
&lt;/p&gt;
&lt;p&gt;
Languini Kitchen&#26159;&#19968;&#20010;&#30740;&#31350;&#38598;&#20307;&#21644;&#20195;&#30721;&#24211;&#65292;&#26088;&#22312;&#36890;&#36807;&#31561;&#25928;&#35745;&#31639;&#26469;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#26032;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#35201;&#28857;&#65306;&#23454;&#39564;&#21327;&#35758;&#12289;&#27169;&#22411;&#27604;&#36739;&#12289;&#31561;&#25928;&#35745;&#31639;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Languini Kitchen&#26082;&#26159;&#19968;&#20010;&#30740;&#31350;&#38598;&#20307;&#65292;&#21448;&#26159;&#19968;&#20010;&#20195;&#30721;&#24211;&#65292;&#26088;&#22312;&#36171;&#20104;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#20570;&#20986;&#26377;&#24847;&#20041;&#36129;&#29486;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#39564;&#21327;&#35758;&#65292;&#20351;&#24471;&#21487;&#20197;&#22522;&#20110;&#31561;&#25928;&#35745;&#31639;&#65288;&#20197;&#21152;&#36895;&#22120;&#23567;&#26102;&#35745;&#37327;&#65289;&#26469;&#36827;&#34892;&#27169;&#22411;&#27604;&#36739;&#12290;&#27169;&#22411;&#35757;&#32451;&#30340;&#20196;&#29260;&#25968;&#37327;&#30001;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#21644;&#36873;&#25321;&#30340;&#35745;&#31639;&#31867;&#21035;&#26469;&#23450;&#20041;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#23545;&#24433;&#21709;&#24635;&#21442;&#25968;&#25110;&#28014;&#28857;&#25805;&#20316;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#39044;&#22788;&#29702;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#19988;&#39640;&#36136;&#37327;&#30340;&#22270;&#20070;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#25991;&#26723;&#38271;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#23398;&#26415;&#22522;&#20934;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#19981;&#21516;&#35745;&#31639;&#27700;&#24179;&#19978;&#30340;&#23454;&#39564;&#26469;&#20272;&#35745;&#26041;&#27861;&#30340;&#32463;&#39564;&#24615;&#25193;&#23637;&#36235;&#21183;&#12290;&#36825;&#39033;&#24037;&#20316;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#22522;&#20934;&#27169;&#22411;&#65306;&#20174;GPT-2&#26550;&#26500;&#25512;&#23548;&#20986;&#30340;&#21069;&#39304;&#27169;&#22411;&#21450;...
&lt;/p&gt;
&lt;p&gt;
The Languini Kitchen serves as both a research collective and codebase designed to empower researchers with limited computational resources to contribute meaningfully to the field of language modelling. We introduce an experimental protocol that enables model comparisons based on equivalent compute, measured in accelerator hours. The number of tokens on which a model is trained is defined by the model's throughput and the chosen compute class. Notably, this approach avoids constraints on critical hyperparameters which affect total parameters or floating-point operations. For evaluation, we pre-process an existing large, diverse, and high-quality dataset of books that surpasses existing academic benchmarks in quality, diversity, and document length. On it, we compare methods based on their empirical scaling trends which are estimated through experiments at various levels of compute. This work also provides two baseline models: a feed-forward model derived from the GPT-2 architecture and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#35748;&#35777;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32508;&#36848;&#20102;&#24050;&#32463;&#24320;&#21457;&#30340;&#29992;&#20110;&#30830;&#20445;AI&#20915;&#31574;&#23433;&#20840;&#30340;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.11196</link><description>&lt;p&gt;
&#20309;&#26102;&#20449;&#20219;&#20154;&#24037;&#26234;&#33021;&#65306;&#31070;&#32463;&#32593;&#32476;&#35748;&#35777;&#30340;&#36827;&#23637;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When to Trust AI: Advances and Challenges for Certification of Neural Networks. (arXiv:2309.11196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#35748;&#35777;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32508;&#36848;&#20102;&#24050;&#32463;&#24320;&#21457;&#30340;&#29992;&#20110;&#30830;&#20445;AI&#20915;&#31574;&#23433;&#20840;&#30340;&#25216;&#26415;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#19988;&#27491;&#20934;&#22791;&#22312;&#33258;&#20027;&#31995;&#32479;&#12289;&#21307;&#23398;&#35786;&#26029;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#36827;&#34892;&#37096;&#32626;&#12290;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#26469;&#35828;&#65292;&#26089;&#26399;&#37319;&#29992;AI&#25216;&#26415;&#24182;&#19981;&#26159;&#27809;&#26377;&#38382;&#39064;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#65292;&#20854;&#21487;&#33021;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;&#20174;&#38271;&#36828;&#26469;&#30475;&#65292;&#38656;&#35201;&#24320;&#21457;&#36866;&#24403;&#30340;&#23433;&#20840;&#20445;&#35777;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#21487;&#36991;&#20813;&#30340;&#31995;&#32479;&#25925;&#38556;&#24102;&#26469;&#30340;&#28508;&#22312;&#21361;&#23475;&#65292;&#24182;&#30830;&#20445;&#20854;&#21487;&#20449;&#24615;&#12290;&#26412;&#25991;&#20197;&#35748;&#35777;&#21644;&#21487;&#35299;&#37322;&#24615;&#20026;&#37325;&#28857;&#65292;&#32508;&#36848;&#20102;&#24050;&#32463;&#24320;&#21457;&#30340;&#29992;&#20110;&#30830;&#20445;AI&#20915;&#31574;&#23433;&#20840;&#30340;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has been advancing at a fast pace and it is now poised for deployment in a wide range of applications, such as autonomous systems, medical diagnosis and natural language processing. Early adoption of AI technology for real-world applications has not been without problems, particularly for neural networks, which may be unstable and susceptible to adversarial examples. In the longer term, appropriate safety assurance techniques need to be developed to reduce potential harm due to avoidable system failures and ensure trustworthiness. Focusing on certification and explainability, this paper provides an overview of techniques that have been developed to ensure safety of AI decisions and discusses future challenges.
&lt;/p&gt;</description></item><item><title>RHALE&#26041;&#27861;&#26159;&#19968;&#31181;&#24378;&#20581;&#19988;&#32771;&#34385;&#24322;&#36136;&#24615;&#30340;&#32047;&#31215;&#23616;&#37096;&#25928;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#24322;&#36136;&#24615;&#21644;&#33258;&#21160;&#30830;&#23450;&#26368;&#20339;&#30340;&#21306;&#38388;&#21010;&#20998;&#65292;&#35299;&#20915;&#20102;&#32047;&#31215;&#23616;&#37096;&#25928;&#24212;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.11193</link><description>&lt;p&gt;
Robust and Heterogeneity-aware Accumulated Local Effects (RHALE): &#24378;&#20581;&#19988;&#32771;&#34385;&#24322;&#36136;&#24615;&#30340;&#32047;&#31215;&#23616;&#37096;&#25928;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RHALE: Robust and Heterogeneity-aware Accumulated Local Effects. (arXiv:2309.11193v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11193
&lt;/p&gt;
&lt;p&gt;
RHALE&#26041;&#27861;&#26159;&#19968;&#31181;&#24378;&#20581;&#19988;&#32771;&#34385;&#24322;&#36136;&#24615;&#30340;&#32047;&#31215;&#23616;&#37096;&#25928;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#21270;&#24322;&#36136;&#24615;&#21644;&#33258;&#21160;&#30830;&#23450;&#26368;&#20339;&#30340;&#21306;&#38388;&#21010;&#20998;&#65292;&#35299;&#20915;&#20102;&#32047;&#31215;&#23616;&#37096;&#25928;&#24212;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32047;&#31215;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#31163;&#29305;&#24449;&#23545;&#36755;&#20986;&#30340;&#24179;&#22343;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#30456;&#20851;&#29305;&#24449;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#23427;&#26377;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#19981;&#33021;&#37327;&#21270;&#23454;&#20363;&#32423;&#65288;&#23616;&#37096;&#65289;&#25928;&#24212;&#19982;&#24179;&#22343;&#65288;&#20840;&#23616;&#65289;&#25928;&#24212;&#20043;&#38388;&#30340;&#20559;&#24046;&#65292;&#21363;&#24322;&#36136;&#24615;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#20272;&#35745;&#24179;&#22343;&#25928;&#24212;&#65292;&#23427;&#23558;&#29305;&#24449;&#22495;&#21010;&#20998;&#20026;&#29992;&#25143;&#23450;&#20041;&#30340;&#22266;&#23450;&#22823;&#23567;&#30340;&#21306;&#38388;&#65292;&#19981;&#21516;&#30340;&#21306;&#38388;&#22823;&#23567;&#20250;&#23548;&#33268;&#19981;&#19968;&#33268;&#30340;ALE&#20272;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24378;&#20581;&#19988;&#32771;&#34385;&#24322;&#36136;&#24615;&#30340;ALE&#65288;RHALE&#65289;&#26041;&#27861;&#12290;RHALE&#36890;&#36807;&#32771;&#34385;&#23616;&#37096;&#25928;&#24212;&#30340;&#26631;&#20934;&#24046;&#26469;&#37327;&#21270;&#24322;&#36136;&#24615;&#65292;&#24182;&#33258;&#21160;&#30830;&#23450;&#26368;&#20339;&#30340;&#21487;&#21464;&#22823;&#23567;&#21306;&#38388;&#21010;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20026;&#20102;&#22312;&#27599;&#20010;&#21306;&#38388;&#20869;&#33719;&#24471;&#23616;&#37096;&#25928;&#24212;&#30340;&#26080;&#20559;&#20272;&#35745;&#30340;&#26631;&#20934;&#24046;&#65292;&#21306;&#38388;&#21010;&#20998;&#24517;&#39035;&#36981;&#24490;&#19968;&#32452;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accumulated Local Effects (ALE) is a widely-used explainability method for isolating the average effect of a feature on the output, because it handles cases with correlated features well. However, it has two limitations. First, it does not quantify the deviation of instance-level (local) effects from the average (global) effect, known as heterogeneity. Second, for estimating the average effect, it partitions the feature domain into user-defined, fixed-sized bins, where different bin sizes may lead to inconsistent ALE estimations. To address these limitations, we propose Robust and Heterogeneity-aware ALE (RHALE). RHALE quantifies the heterogeneity by considering the standard deviation of the local effects and automatically determines an optimal variable-size bin-splitting. In this paper, we prove that to achieve an unbiased approximation of the standard deviation of local effects within each bin, bin splitting must follow a set of sufficient conditions. Based on these conditions, we pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#25193;&#25955;&#22120;&#21644;&#20004;&#31181;&#24050;&#26377;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#26356;&#23481;&#26131;&#23398;&#20064;&#33410;&#22863;&#38899;&#20048;&#32467;&#26500;&#32780;&#38750;&#26059;&#24459;&#12290;</title><link>http://arxiv.org/abs/2309.11140</link><description>&lt;p&gt;
&#25506;&#31350;&#20010;&#24615;&#21270;&#26041;&#27861;&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Investigating Personalization Methods in Text to Music Generation. (arXiv:2309.11140v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#25193;&#25955;&#22120;&#21644;&#20004;&#31181;&#24050;&#26377;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#26356;&#23481;&#26131;&#23398;&#20064;&#33410;&#22863;&#38899;&#20048;&#32467;&#26500;&#32780;&#38750;&#26059;&#24459;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#38899;&#20048;&#25193;&#25955;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#20102;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#25193;&#25955;&#22120;&#19982;&#20004;&#31181;&#24050;&#24314;&#31435;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#38899;&#39057;&#29305;&#23450;&#25968;&#25454;&#22686;&#24378;&#23545;&#25972;&#20010;&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#20026;&#20102;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25552;&#31034;&#21644;&#38899;&#20048;&#29255;&#27573;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#23884;&#20837;&#21644;&#38899;&#20048;&#29305;&#23450;&#30340;&#25351;&#26631;&#36827;&#34892;&#23450;&#37327;&#35780;&#20272;&#65292;&#20197;&#21450;&#29992;&#25143;&#30740;&#31350;&#36827;&#34892;&#23450;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#30456;&#20284;&#24230;&#25351;&#26631;&#19982;&#29992;&#25143;&#20559;&#22909;&#19968;&#33268;&#65292;&#24182;&#19988;&#24403;&#21069;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#26356;&#23481;&#26131;&#23398;&#20064;&#21040;&#33410;&#22863;&#38899;&#20048;&#32467;&#26500;&#32780;&#38750;&#26059;&#24459;&#12290;&#26412;&#30740;&#31350;&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#31034;&#20363;&#36164;&#26009;&#23545;&#30740;&#31350;&#31038;&#21306;&#24320;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the personalization of text-to-music diffusion models in a few-shot setting. Motivated by recent advances in the computer vision domain, we are the first to explore the combination of pre-trained text-to-audio diffusers with two established personalization methods. We experiment with the effect of audio-specific data augmentation on the overall system performance and assess different training strategies. For evaluation, we construct a novel dataset with prompts and music clips. We consider both embedding-based and music-specific metrics for quantitative evaluation, as well as a user study for qualitative evaluation. Our analysis shows that similarity metrics are in accordance with user preferences and that current personalization approaches tend to learn rhythmic music constructs more easily than melody. The code, dataset, and example material of this study are open to the research community.
&lt;/p&gt;</description></item><item><title>Ano-SuPs&#26159;&#19968;&#31181;&#36890;&#36807;&#35782;&#21035;&#21487;&#30097;&#21306;&#22359;&#26469;&#36827;&#34892;&#21046;&#36896;&#20135;&#21697;&#30340;&#22810;&#23610;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#26041;&#27861;&#12290;&#23427;&#21487;&#20197;&#35299;&#20915;&#22270;&#20687;&#32972;&#26223;&#22797;&#26434;&#24615;&#21644;&#24322;&#24120;&#27169;&#24335;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11120</link><description>&lt;p&gt;
Ano-SuPs: &#36890;&#36807;&#35782;&#21035;&#21487;&#30097;&#30340;&#21306;&#22359;&#36827;&#34892;&#21046;&#36896;&#20135;&#21697;&#30340;&#22810;&#23610;&#24230;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ano-SuPs: Multi-size anomaly detection for manufactured products by identifying suspected patches. (arXiv:2309.11120v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11120
&lt;/p&gt;
&lt;p&gt;
Ano-SuPs&#26159;&#19968;&#31181;&#36890;&#36807;&#35782;&#21035;&#21487;&#30097;&#21306;&#22359;&#26469;&#36827;&#34892;&#21046;&#36896;&#20135;&#21697;&#30340;&#22810;&#23610;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#26041;&#27861;&#12290;&#23427;&#21487;&#20197;&#35299;&#20915;&#22270;&#20687;&#32972;&#26223;&#22797;&#26434;&#24615;&#21644;&#24322;&#24120;&#27169;&#24335;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#31995;&#32479;&#22240;&#20854;&#25552;&#20379;&#20016;&#23500;&#30340;&#21046;&#36896;&#29366;&#24577;&#20449;&#24687;&#12289;&#20302;&#23454;&#26045;&#25104;&#26412;&#21644;&#39640;&#37319;&#38598;&#36895;&#24230;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#32972;&#26223;&#30340;&#22797;&#26434;&#24615;&#21644;&#21508;&#31181;&#24322;&#24120;&#27169;&#24335;&#32473;&#29616;&#26377;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36275;&#20197;&#28385;&#36275;&#24314;&#27169;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#24322;&#24120;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#23548;&#33268;&#24322;&#24120;&#30340;&#27745;&#26579;&#38382;&#39064;&#65292;&#20351;&#24471;&#35774;&#35745;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#23545;&#22806;&#37096;&#24178;&#25200;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35782;&#21035;&#21487;&#30097;&#21306;&#22359;&#65288;Ano-SuPs&#65289;&#26469;&#26816;&#27979;&#24322;&#24120;&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#27425;&#37325;&#24314;&#36755;&#20837;&#22270;&#20687;&#26469;&#26816;&#27979;&#24102;&#26377;&#24322;&#24120;&#30340;&#21306;&#22359;&#30340;&#26041;&#27861;&#65306;&#31532;&#19968;&#27493;&#26159;&#36890;&#36807;&#21435;&#38500;&#37027;&#20123;&#21487;&#30097;&#21306;&#22359;&#26469;&#33719;&#24471;&#19968;&#32452;&#27491;&#24120;&#21306;&#22359;&#65292;&#31532;&#20108;&#27493;&#26159;&#20351;&#29992;&#36825;&#20123;&#27491;&#24120;&#21306;&#22359;&#26469;&#20248;&#21270;&#23545;&#24102;&#26377;&#24322;&#24120;&#21306;&#22359;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based systems have gained popularity owing to their capacity to provide rich manufacturing status information, low implementation costs and high acquisition rates. However, the complexity of the image background and various anomaly patterns pose new challenges to existing matrix decomposition methods, which are inadequate for modeling requirements. Moreover, the uncertainty of the anomaly can cause anomaly contamination problems, making the designed model and method highly susceptible to external disturbances. To address these challenges, we propose a two-stage strategy anomaly detection method that detects anomalies by identifying suspected patches (Ano-SuPs). Specifically, we propose to detect the patches with anomalies by reconstructing the input image twice: the first step is to obtain a set of normal patches by removing those suspected patches, and the second step is to use those normal patches to refine the identification of the patches with anomalies. To demonstrate its ef
&lt;/p&gt;</description></item><item><title>&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20915;&#23450;&#21738;&#20123;&#21442;&#25968;&#24212;&#35813;&#26412;&#22320;&#21270;&#25110;&#20849;&#20139;&#12290;&#20256;&#32479;&#26041;&#27861;&#20013;&#36890;&#24120;&#20010;&#24615;&#21270;&#19982;&#38750;IID&#25968;&#25454;&#25935;&#24863;&#30340;&#25152;&#26377;&#23618;&#30340;&#21442;&#25968;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#36807;&#20110;&#20445;&#23432;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#21512;&#20316;&#20013;&#35201;&#32771;&#34385;&#19982;&#20854;&#20182;&#23458;&#25143;&#31471;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#21363;&#20351;&#21442;&#25968;&#23481;&#26131;&#21463;&#38750;IID&#25968;&#25454;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#20173;&#21487;&#20197;&#36890;&#36807;&#20849;&#20139;&#36825;&#20123;&#21442;&#25968;&#26469;&#33719;&#30410;&#12290;</title><link>http://arxiv.org/abs/2309.11103</link><description>&lt;p&gt;
&#22823;&#32966;&#32780;&#35880;&#24910;&#65306;&#36890;&#36807;&#35880;&#24910;&#32780;&#31215;&#26497;&#30340;&#21512;&#20316;&#37322;&#25918;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Bold but Cautious: Unlocking the Potential of Personalized Federated Learning through Cautiously Aggressive Collaboration. (arXiv:2309.11103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11103
&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20915;&#23450;&#21738;&#20123;&#21442;&#25968;&#24212;&#35813;&#26412;&#22320;&#21270;&#25110;&#20849;&#20139;&#12290;&#20256;&#32479;&#26041;&#27861;&#20013;&#36890;&#24120;&#20010;&#24615;&#21270;&#19982;&#38750;IID&#25968;&#25454;&#25935;&#24863;&#30340;&#25152;&#26377;&#23618;&#30340;&#21442;&#25968;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#36807;&#20110;&#20445;&#23432;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#21512;&#20316;&#20013;&#35201;&#32771;&#34385;&#19982;&#20854;&#20182;&#23458;&#25143;&#31471;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#21363;&#20351;&#21442;&#25968;&#23481;&#26131;&#21463;&#38750;IID&#25968;&#25454;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#20173;&#21487;&#20197;&#36890;&#36807;&#20849;&#20139;&#36825;&#20123;&#21442;&#25968;&#26469;&#33719;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#36890;&#36807;&#20801;&#35768;&#27599;&#20010;&#23458;&#25143;&#31471;&#22312;&#19982;&#20854;&#20182;&#20154;&#21512;&#20316;&#26102;&#35757;&#32451;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-IID&#65289;&#25968;&#25454;&#23545;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#12290;PFL&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20915;&#23450;&#23458;&#25143;&#31471;&#30340;&#21738;&#20123;&#21442;&#25968;&#24212;&#35813;&#26412;&#22320;&#21270;&#25110;&#19982;&#20854;&#20182;&#20154;&#20849;&#20139;&#12290;&#22312;&#24403;&#21069;&#20027;&#27969;&#26041;&#27861;&#20013;&#65292;&#36890;&#24120;&#20250;&#20010;&#24615;&#21270;&#19982;&#38750;IID&#25968;&#25454;&#25935;&#24863;&#30340;&#25152;&#26377;&#23618;&#65288;&#22914;&#20998;&#31867;&#22120;&#23618;&#65289;&#30340;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#29702;&#30001;&#26159;&#21487;&#20197;&#29702;&#35299;&#30340;&#65292;&#22240;&#20026;&#26412;&#22320;&#21270;&#26131;&#21463;&#38750;IID&#25968;&#25454;&#24433;&#21709;&#30340;&#21442;&#25968;&#21487;&#20197;&#38450;&#27490;&#21512;&#20316;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#21512;&#20316;&#26469;&#35828;&#36807;&#20110;&#20445;&#23432;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#26576;&#20010;&#23458;&#25143;&#31471;&#65292;&#21363;&#20351;&#20854;&#21442;&#25968;&#23481;&#26131;&#21463;&#38750;IID&#25968;&#25454;&#24433;&#21709;&#65292;&#19982;&#20855;&#26377;&#30456;&#20284;&#25968;&#25454;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#20849;&#20139;&#36825;&#20123;&#21442;&#25968;&#20173;&#28982;&#21487;&#20197;&#24102;&#26469;&#25910;&#30410;&#12290;&#36825;&#19968;&#35266;&#23519;&#24378;&#35843;&#20102;&#19981;&#20165;&#35201;&#32771;&#34385;&#23545;&#38750;IID&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#65292;&#32780;&#19988;&#35201;&#32771;&#34385;&#19982;&#20854;&#20182;&#23458;&#25143;&#31471;&#36827;&#34892;&#21512;&#20316;&#25152;&#24102;&#26469;&#30340;&#28508;&#22312;&#22909;&#22788;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized federated learning (PFL) reduces the impact of non-independent and identically distributed (non-IID) data among clients by allowing each client to train a personalized model when collaborating with others. A key question in PFL is to decide which parameters of a client should be localized or shared with others. In current mainstream approaches, all layers that are sensitive to non-IID data (such as classifier layers) are generally personalized. The reasoning behind this approach is understandable, as localizing parameters that are easily influenced by non-IID data can prevent the potential negative effect of collaboration. However, we believe that this approach is too conservative for collaboration. For example, for a certain client, even if its parameters are easily influenced by non-IID data, it can still benefit by sharing these parameters with clients having similar data distribution. This observation emphasizes the importance of considering not only the sensitivity to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;TT-rules&#65292;&#22312;&#21307;&#30103;&#20915;&#31574;&#20013;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#25903;&#25345;&#20108;&#20998;&#31867;&#12289;&#22810;&#26631;&#31614;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#19988;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.11101</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#21017;&#27169;&#22411;&#29992;&#20110;&#21307;&#30103;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
A New Interpretable Neural Network-Based Rule Model for Healthcare Decision Making. (arXiv:2309.11101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#31216;&#20026;TT-rules&#65292;&#22312;&#21307;&#30103;&#20915;&#31574;&#20013;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#25903;&#25345;&#20108;&#20998;&#31867;&#12289;&#22810;&#26631;&#31614;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65292;&#19988;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#65292;&#29702;&#35299;&#26426;&#22120;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;&#30495;&#20540;&#34920;&#35268;&#21017;&#8221;&#65288;TT-rules&#65289;&#65292;&#23427;&#23558;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#30340;&#20840;&#23616;&#21644;&#31934;&#30830;&#21487;&#35299;&#37322;&#24615;&#24615;&#36136;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#24615;&#33021;&#30456;&#32467;&#21512;&#12290;TT-rules&#22522;&#20110;&#8220;&#30495;&#20540;&#34920;&#32593;&#32476;&#8221;&#65288;TTnet&#65289;&#26500;&#24314;&#65292;&#36825;&#26159;&#19968;&#26063;&#26368;&#21021;&#29992;&#20110;&#24418;&#24335;&#39564;&#35777;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#20174;&#35757;&#32451;&#22909;&#30340;TTnet&#27169;&#22411;&#20013;&#25552;&#21462;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#35268;&#21017;$\mathcal{R}$&#26469;&#20135;&#29983;&#19982;TTnet&#30456;&#21516;&#36755;&#20986;&#30340;&#35268;&#21017;&#65288;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#65289;&#65292;TT-rules&#26377;&#25928;&#22320;&#23558;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#12290;&#36825;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#25903;&#25345;&#23567;&#21040;&#22823;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#20108;&#20998;&#31867;&#12289;&#22810;&#26631;&#31614;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;&#22312;&#27010;&#36848;&#20102;&#26694;&#26550;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;TT-rules&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#35268;&#21017;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In healthcare applications, understanding how machine/deep learning models make decisions is crucial. In this study, we introduce a neural network framework, $\textit{Truth Table rules}$ (TT-rules), that combines the global and exact interpretability properties of rule-based models with the high performance of deep neural networks. TT-rules is built upon $\textit{Truth Table nets}$ (TTnet), a family of deep neural networks initially developed for formal verification. By extracting the necessary and sufficient rules $\mathcal{R}$ from the trained TTnet model (global interpretability) to yield the same output as the TTnet (exact interpretability), TT-rules effectively transforms the neural network into a rule-based model. This rule-based model supports binary classification, multi-label classification, and regression tasks for small to large tabular datasets. After outlining the framework, we evaluate TT-rules' performance on healthcare applications and compare it to state-of-the-art rul
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24310;&#36831;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#20195;&#29702;&#35266;&#27979;&#24310;&#36831;&#21644;&#25191;&#34892;&#34892;&#21160;&#24310;&#36831;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#24310;&#36831;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11096</link><description>&lt;p&gt;
&#24310;&#36831;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Delays in Reinforcement Learning. (arXiv:2309.11096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11096
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#24310;&#36831;&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23545;&#20195;&#29702;&#35266;&#27979;&#24310;&#36831;&#21644;&#25191;&#34892;&#34892;&#21160;&#24310;&#36831;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#24310;&#36831;&#23545;&#31995;&#32479;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24310;&#36831;&#26159;&#22823;&#22810;&#25968;&#21160;&#24577;&#31995;&#32479;&#22266;&#26377;&#30340;&#29305;&#24615;&#12290;&#38500;&#20102;&#23558;&#36807;&#31243;&#25512;&#21518;&#19968;&#27573;&#26102;&#38388;&#22806;&#65292;&#24310;&#36831;&#36824;&#20250;&#26174;&#33879;&#24433;&#21709;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#24310;&#36831;&#24182;&#21152;&#20197;&#32771;&#34385;&#36890;&#24120;&#26159;&#24456;&#26377;&#20215;&#20540;&#30340;&#12290;&#30001;&#20110;&#23427;&#20204;&#26159;&#21160;&#24577;&#31995;&#32479;&#65292;&#25152;&#20197;&#24310;&#36831;&#20063;&#20250;&#24433;&#21709;&#21040;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#31561;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#12290;MDP&#26159;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;RL&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#19982;&#29615;&#22659;&#20114;&#21160;&#23398;&#20064;&#20197;&#26368;&#22823;&#21270;&#25928;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#21019;&#24314;&#12290;&#23613;&#31649;RL&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#23545;&#20110;&#24310;&#36831;&#30340;&#26174;&#24335;&#32771;&#34385;&#21364;&#24456;&#23569;&#35265;&#12290;&#23545;&#20110;MDP&#30340;&#24310;&#36831;&#24433;&#21709;&#30340;&#29702;&#35299;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#30740;&#31350;&#20195;&#29702;&#23545;&#29615;&#22659;&#29366;&#24577;&#30340;&#35266;&#27979;&#25110;&#20195;&#29702;&#25191;&#34892;&#34892;&#21160;&#30340;&#24310;&#36831;&#12290;&#25105;&#20204;&#23558;&#19981;&#26029;&#25913;&#21464;&#23545;&#38382;&#39064;&#30340;&#35266;&#28857;&#20197;&#25581;&#31034;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Delays are inherent to most dynamical systems. Besides shifting the process in time, they can significantly affect their performance. For this reason, it is usually valuable to study the delay and account for it. Because they are dynamical systems, it is of no surprise that sequential decision-making problems such as Markov decision processes (MDP) can also be affected by delays. These processes are the foundational framework of reinforcement learning (RL), a paradigm whose goal is to create artificial agents capable of learning to maximise their utility by interacting with their environment.  RL has achieved strong, sometimes astonishing, empirical results, but delays are seldom explicitly accounted for. The understanding of the impact of delay on the MDP is limited. In this dissertation, we propose to study the delay in the agent's observation of the state of the environment or in the execution of the agent's actions. We will repeatedly change our point of view on the problem to reve
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11093</link><description>&lt;p&gt;
K-pop&#27468;&#35789;&#32763;&#35793;&#65306;&#25968;&#25454;&#38598;&#12289;&#20998;&#26512;&#19982;&#31070;&#32463;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11093
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;K-pop&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#35789;&#32763;&#35793;&#20316;&#20026;&#19968;&#20010;&#30740;&#31350;&#20102;&#19968;&#20010;&#19990;&#32426;&#30340;&#39046;&#22495;&#65292;&#22914;&#20170;&#21560;&#24341;&#30528;&#35745;&#31639;&#35821;&#35328;&#23398;&#30740;&#31350;&#32773;&#30340;&#27880;&#24847;&#12290;&#25105;&#20204;&#22312;&#20197;&#24448;&#30740;&#31350;&#20013;&#21457;&#29616;&#20102;&#20004;&#20010;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22312;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#23613;&#31649;K-pop&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#35199;&#26041;&#27969;&#27966;&#21644;&#35821;&#35328;&#65292;&#27809;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;K-pop&#19978;&#12290;&#20854;&#27425;&#65292;&#27468;&#35789;&#32763;&#35793;&#39046;&#22495;&#32570;&#20047;&#21487;&#20844;&#24320;&#33719;&#24471;&#30340;&#25968;&#25454;&#38598;&#65307;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#23578;&#26080;&#27492;&#31867;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25299;&#23485;&#27468;&#35789;&#32763;&#35793;&#30740;&#31350;&#30340;&#27969;&#27966;&#21644;&#35821;&#35328;&#33539;&#22260;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21809;&#27468;&#35789;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#32422;89%&#20026;K-pop&#27468;&#35789;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#36880;&#34892;&#21644;&#36880;&#33410;&#23545;&#40784;&#20102;&#38889;&#35821;&#21644;&#33521;&#35821;&#27468;&#35789;&#12290;&#25105;&#20204;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;K-pop&#27468;&#35789;&#32763;&#35793;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#19982;&#20854;&#20182;&#24191;&#27867;&#30740;&#31350;&#30340;&#27969;&#27966;&#21306;&#20998;&#24320;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#31070;&#32463;&#27468;&#35789;&#32763;&#35793;&#27169;&#22411;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19987;&#29992;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#27010;&#29575;&#27169;&#22411;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;dropout&#19981;&#30830;&#23450;&#24615;&#21644;&#36712;&#36857;&#37319;&#26679;&#65292;&#31283;&#23450;&#22320;&#39044;&#27979;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#65292;&#32416;&#27491;&#31070;&#32463;&#32593;&#32476;&#30340;&#25311;&#21512;&#35823;&#24046;&#65292;&#36807;&#28388;aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25511;&#21046;&#20219;&#21153;&#21644;&#23454;&#38469;&#26426;&#26800;&#33218;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11089</link><description>&lt;p&gt;
&#32467;&#21512;Dropout&#19981;&#30830;&#23450;&#24615;&#19982;&#36712;&#36857;&#37319;&#26679;&#30340;&#23454;&#29992;&#27010;&#29575;&#27169;&#22411;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling. (arXiv:2309.11089v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#27010;&#29575;&#27169;&#22411;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;dropout&#19981;&#30830;&#23450;&#24615;&#21644;&#36712;&#36857;&#37319;&#26679;&#65292;&#31283;&#23450;&#22320;&#39044;&#27979;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#65292;&#32416;&#27491;&#31070;&#32463;&#32593;&#32476;&#30340;&#25311;&#21512;&#35823;&#24046;&#65292;&#36807;&#28388;aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25511;&#21046;&#20219;&#21153;&#21644;&#23454;&#38469;&#26426;&#26800;&#33218;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#24314;&#31435;&#22312;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#27010;&#29575;&#27169;&#22411;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#30340;&#39044;&#27979;&#31283;&#23450;&#24615;&#12289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25511;&#21046;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;dropout&#30340;&#27010;&#29575;&#38598;&#25104;&#21644;&#36712;&#36857;&#37319;&#26679;&#65288;DPETS&#65289;&#65292;&#22312;&#19968;&#20010;&#26694;&#26550;&#20013;&#36890;&#36807;Monte-Carlo dropout&#21644;&#36712;&#36857;&#37319;&#26679;&#26469;&#31283;&#23450;&#22320;&#39044;&#27979;&#31995;&#32479;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#29992;&#20110;&#32416;&#27491;&#31070;&#32463;&#32593;&#32476;&#30340;&#25311;&#21512;&#35823;&#24046;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#27010;&#29575;&#27169;&#22411;&#12290;&#20854;&#31574;&#30053;&#20013;&#30340;&#29366;&#24577;&#20256;&#25773;&#34987;&#25193;&#23637;&#29992;&#20110;&#28388;&#38500;aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#20960;&#20010;Mujoco&#22522;&#20934;&#25511;&#21046;&#20219;&#21153;&#21644;&#19968;&#20010;&#23454;&#38469;&#26426;&#26800;&#33218;&#25805;&#20316;&#20219;&#21153;&#19979;&#30340;&#35780;&#20272;&#65292;DPETS&#22312;&#24179;&#22343;&#22238;&#25253;&#21644;&#25910;&#25947;&#36895;&#24230;&#19978;&#20248;&#20110;&#30456;&#20851;MBRL&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#26174;&#33879;&#30340;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#65292;&#32988;&#36807;&#30693;&#21517;&#30340;&#26080;&#27169;&#22411;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the prediction stability, prediction accuracy and control capability of the current probabilistic model-based reinforcement learning (MBRL) built on neural networks. A novel approach dropout-based probabilistic ensembles with trajectory sampling (DPETS) is proposed where the system uncertainty is stably predicted by combining the Monte-Carlo dropout and trajectory sampling in one framework. Its loss function is designed to correct the fitting error of neural networks for more accurate prediction of probabilistic models. The state propagation in its policy is extended to filter the aleatoric uncertainty for superior control capability. Evaluated by several Mujoco benchmark control tasks under additional disturbances and one practical robot arm manipulation task, DPETS outperforms related MBRL approaches in both average return and convergence velocity while achieving superior performance than well-known model-free baselines with significant sample efficiency. The ope
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#27880;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#22686;&#24378;&#25216;&#26415;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#22312;&#35270;&#35273;&#38169;&#35823;&#26816;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#38598;&#25193;&#22823;&#21644;&#33258;&#20027;/&#20132;&#20114;&#24369;&#30417;&#30563;&#65292;&#23637;&#31034;&#20102;&#22312;&#24191;&#38420;&#30340;&#28216;&#25103;&#19990;&#30028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11077</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#22312;&#26631;&#27880;&#39640;&#25928;&#21487;&#35270;&#21270;&#38169;&#35823;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weak Supervision for Label Efficient Visual Bug Detection. (arXiv:2309.11077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11077
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#27880;&#25968;&#25454;&#21644;&#39046;&#22495;&#29305;&#23450;&#22686;&#24378;&#25216;&#26415;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#22312;&#35270;&#35273;&#38169;&#35823;&#26816;&#27979;&#20013;&#23454;&#29616;&#20102;&#25968;&#25454;&#38598;&#25193;&#22823;&#21644;&#33258;&#20027;/&#20132;&#20114;&#24369;&#30417;&#30563;&#65292;&#23637;&#31034;&#20102;&#22312;&#24191;&#38420;&#30340;&#28216;&#25103;&#19990;&#30028;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35270;&#39057;&#28216;&#25103;&#36827;&#21270;&#20026;&#24191;&#38420;&#12289;&#32454;&#33268;&#30340;&#19990;&#30028;&#65292;&#35270;&#35273;&#36136;&#37327;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#26085;&#30410;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;&#27979;&#35797;&#26041;&#27861;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#65292;&#38590;&#20197;&#24212;&#23545;&#22823;&#37327;&#28508;&#22312;&#30340;&#38169;&#35823;&#12290;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#65292;&#23545;&#22823;&#22411;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20005;&#37325;&#20381;&#36182;&#20173;&#28982;&#26159;&#19968;&#20010;&#21046;&#32422;&#22240;&#32032;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#28216;&#25103;&#29609;&#27861;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#22686;&#24378;&#25216;&#26415;&#29983;&#25104;&#29992;&#20110;&#39044;&#35757;&#32451;&#25110;&#22810;&#20219;&#21153;&#35774;&#32622;&#30340;&#25968;&#25454;&#38598;&#21644;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#29992;&#20110;&#21518;&#32493;&#30340;&#35270;&#35273;&#38169;&#35823;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#25193;&#22823;&#37327;&#34920;&#21644;&#20419;&#36827;&#33258;&#20027;&#21644;&#20132;&#20114;&#24335;&#24369;&#30417;&#30563;&#65292;&#21253;&#25324;&#22522;&#20110;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;/&#25110;&#22522;&#20110;&#25991;&#26412;&#21644;&#20960;&#20309;&#25552;&#31034;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#24191;&#38420;&#30340;Giantmap&#28216;&#25103;&#19990;&#30028;&#20013;&#23637;&#31034;&#20102;&#23545;&#19968;&#20154;&#31216;&#29609;&#23478;&#21098;&#36753;/&#30896;&#25758;&#38169;&#35823;&#65288;FPPC&#65289;&#30340;&#26816;&#27979;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
As video games evolve into expansive, detailed worlds, visual quality becomes essential, yet increasingly challenging. Traditional testing methods, limited by resources, face difficulties in addressing the plethora of potential bugs. Machine learning offers scalable solutions; however, heavy reliance on large labeled datasets remains a constraint. Addressing this challenge, we propose a novel method, utilizing unlabeled gameplay and domain-specific augmentations to generate datasets &amp; self-supervised objectives used during pre-training or multi-task settings for downstream visual bug detection. Our methodology uses weak-supervision to scale datasets for the crafted objectives and facilitates both autonomous and interactive weak-supervision, incorporating unsupervised clustering and/or an interactive approach based on text and geometric prompts. We demonstrate on first-person player clipping/collision bugs (FPPC) within the expansive Giantmap game world, that our approach is very effect
&lt;/p&gt;</description></item><item><title>GPSINDy&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#19982;SINDy&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#27169;&#22411;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#31995;&#32479;&#21160;&#24577;&#21644;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#26041;&#38754;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11076</link><description>&lt;p&gt;
GPSINDy: &#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#26041;&#31243;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GPSINDy: Data-Driven Discovery of Equations of Motion. (arXiv:2309.11076v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11076
&lt;/p&gt;
&lt;p&gt;
GPSINDy&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#19982;SINDy&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#27169;&#22411;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#31995;&#32479;&#21160;&#24577;&#21644;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#26041;&#38754;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32771;&#34385;&#20174;&#26377;&#22122;&#22768;&#25968;&#25454;&#20013;&#21457;&#29616;&#21160;&#21147;&#23398;&#31995;&#32479;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#24050;&#30693;&#22122;&#22768;&#23384;&#22312;&#23545;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65288;&#19968;&#31181;&#38750;&#21442;&#25968;&#23398;&#20064;&#26041;&#27861;&#65289;&#19982;SINDy&#65288;&#19968;&#31181;&#21442;&#25968;&#23398;&#20064;&#26041;&#27861;&#65289;&#30456;&#32467;&#21512;&#65292;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#19982;SINDy&#30456;&#27604;&#22312;&#26377;&#22122;&#22768;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#22312;Lotka-Volterra&#27169;&#22411;&#21644;&#20223;&#30495;&#20013;&#30340;&#21333;&#36718;&#36710;&#21160;&#24577;&#27169;&#22411;&#19978;&#20197;&#21450;&#22312;&#20351;&#29992;&#30828;&#20214;&#25968;&#25454;&#30340;NVIDIA JetRacer&#31995;&#32479;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;SINDy&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21457;&#29616;&#31995;&#32479;&#21160;&#24577;&#21644;&#39044;&#27979;&#26410;&#26469;&#36712;&#36857;&#26041;&#38754;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of discovering dynamical system models from noisy data. The presence of noise is known to be a significant problem for symbolic regression algorithms. We combine Gaussian process regression, a nonparametric learning method, with SINDy, a parametric learning approach, to identify nonlinear dynamical systems from data. The key advantages of our proposed approach are its simplicity coupled with the fact that it demonstrates improved robustness properties with noisy data over SINDy. We demonstrate our proposed approach on a Lotka-Volterra model and a unicycle dynamic model in simulation and on an NVIDIA JetRacer system using hardware data. We demonstrate improved performance over SINDy for discovering the system dynamics and predicting future trajectories.
&lt;/p&gt;</description></item><item><title>InkStream&#26159;&#19968;&#31181;&#22312;&#27969;&#24335;&#22270;&#19978;&#36827;&#34892;&#23454;&#26102;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#37327;&#26356;&#26032;&#33410;&#28857;&#23884;&#20837;&#26469;&#35299;&#20915;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#24335;&#22270;&#19978;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.11071</link><description>&lt;p&gt;
InkStream: &#36890;&#36807;&#22686;&#37327;&#26356;&#26032;&#22312;&#27969;&#24335;&#22270;&#19978;&#36827;&#34892;&#23454;&#26102;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
InkStream: Real-time GNN Inference on Streaming Graphs via Incremental Update. (arXiv:2309.11071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11071
&lt;/p&gt;
&lt;p&gt;
InkStream&#26159;&#19968;&#31181;&#22312;&#27969;&#24335;&#22270;&#19978;&#36827;&#34892;&#23454;&#26102;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#37327;&#26356;&#26032;&#33410;&#28857;&#23884;&#20837;&#26469;&#35299;&#20915;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#24335;&#22270;&#19978;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#26041;&#27861;&#36866;&#29992;&#20110;&#38745;&#24577;&#22270;&#65292;&#32780;&#23545;&#20110;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#27969;&#24335;&#22270;&#21017;&#19981;&#21512;&#36866;&#12290;&#27969;&#24335;&#22270;&#30340;&#21160;&#24577;&#24615;&#38656;&#35201;&#36827;&#34892;&#25345;&#32493;&#30340;&#26356;&#26032;&#65292;&#23545;GPU&#21152;&#36895;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#22522;&#20110;&#20004;&#20010;&#20851;&#38190;&#35266;&#28857;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22312;k-hop&#37051;&#22495;&#20869;&#65292;&#24403;&#27169;&#22411;&#20351;&#29992;&#26368;&#23567;&#25110;&#26368;&#22823;&#32858;&#21512;&#20989;&#25968;&#26102;&#65292;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#33410;&#28857;&#21463;&#21040;&#20462;&#25913;&#36793;&#30340;&#24433;&#21709;&#65307;&#65288;2&#65289;&#24403;&#27169;&#22411;&#26435;&#37325;&#20445;&#25345;&#38745;&#24577;&#32780;&#22270;&#32467;&#26500;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#33410;&#28857;&#23884;&#20837;&#21487;&#20197;&#36890;&#36807;&#20165;&#35745;&#31639;&#37051;&#22495;&#30340;&#21463;&#24433;&#21709;&#37096;&#20998;&#26469;&#36880;&#27493;&#28436;&#21270;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;InkStream&#65292;&#26088;&#22312;&#23454;&#29616;&#23454;&#26102;&#25512;&#29702;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#35775;&#38382;&#21644;&#35745;&#31639;&#65292;&#24182;&#30830;&#20445;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;InkStream&#30340;&#25805;&#20316;&#21407;&#21017;&#26159;&#20165;&#22312;&#24517;&#35201;&#26102;&#20256;&#25773;&#21644;&#33719;&#21462;&#25968;&#25454;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#20107;&#20214;&#30340;&#31995;&#32479;&#26469;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classic Graph Neural Network (GNN) inference approaches, designed for static graphs, are ill-suited for streaming graphs that evolve with time. The dynamism intrinsic to streaming graphs necessitates constant updates, posing unique challenges to acceleration on GPU. We address these challenges based on two key insights: (1) Inside the $k$-hop neighborhood, a significant fraction of the nodes is not impacted by the modified edges when the model uses min or max as aggregation function; (2) When the model weights remain static while the graph structure changes, node embeddings can incrementally evolve over time by computing only the impacted part of the neighborhood. With these insights, we propose a novel method, InkStream, designed for real-time inference with minimal memory access and computation, while ensuring an identical output to conventional methods. InkStream operates on the principle of propagating and fetching data only when necessary. It uses an event-based system to control 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22312;&#22825;&#28982;&#30005;&#32593;&#35268;&#21010;&#20013;&#36873;&#25321;&#26497;&#31471;&#24773;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#21151;&#33021;&#28145;&#24230;&#24230;&#37327;&#26469;&#31579;&#36873;&#20986;&#26368;&#37325;&#35201;&#30340;&#24773;&#26223;&#20197;&#20943;&#36731;&#36816;&#33829;&#39118;&#38505;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;Texas-7k&#30005;&#32593;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11067</link><description>&lt;p&gt;
&#22825;&#28982;&#30005;&#32593;&#36816;&#33829;&#35745;&#21010;&#20013;&#30340;&#26497;&#31471;&#24773;&#26223;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Extreme Scenario Selection in Day-Ahead Power Grid Operational Planning. (arXiv:2309.11067v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22312;&#22825;&#28982;&#30005;&#32593;&#35268;&#21010;&#20013;&#36873;&#25321;&#26497;&#31471;&#24773;&#26223;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#21151;&#33021;&#28145;&#24230;&#24230;&#37327;&#26469;&#31579;&#36873;&#20986;&#26368;&#37325;&#35201;&#30340;&#24773;&#26223;&#20197;&#20943;&#36731;&#36816;&#33829;&#39118;&#38505;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;Texas-7k&#30005;&#32593;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#22312;&#25552;&#21069;&#19968;&#22825;&#30340;&#30005;&#32593;&#35268;&#21010;&#20013;&#24212;&#29992;&#32479;&#35745;&#21151;&#33021;&#28145;&#24230;&#24230;&#37327;&#26469;&#36873;&#25321;&#26497;&#31471;&#24773;&#26223;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#31579;&#36873;&#38024;&#23545;&#23454;&#38469;&#36127;&#36733;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#29983;&#25104;&#30340;&#27010;&#29575;&#24773;&#26223;&#65292;&#20197;&#35782;&#21035;&#23545;&#36816;&#33829;&#39118;&#38505;&#32531;&#35299;&#26368;&#37325;&#35201;&#30340;&#24773;&#26223;&#12290;&#20026;&#20102;&#22788;&#29702;&#36164;&#20135;&#31867;&#21035;&#21644;&#26085;&#20869;&#26102;&#27573;&#30340;&#22330;&#26223;&#39640;&#32500;&#24230;&#24773;&#20917;&#65292;&#25105;&#20204;&#20351;&#29992;&#21151;&#33021;&#28145;&#24230;&#24230;&#37327;&#26469;&#23376;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#23545;&#30005;&#32593;&#36816;&#33829;&#39118;&#38505;&#26368;&#39640;&#30340;&#24322;&#24120;&#24773;&#26223;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#21151;&#33021;&#28145;&#24230;&#24230;&#37327;&#20197;&#21450;&#19968;&#31995;&#21015;&#36816;&#33829;&#39118;&#38505;&#65292;&#21253;&#25324;&#36127;&#33655;&#21066;&#20943;&#12289;&#36816;&#33829;&#25104;&#26412;&#12289;&#22791;&#29992;&#19981;&#36275;&#21644;&#21487;&#21464;&#21487;&#20877;&#29983;&#33021;&#28304;&#21066;&#20943;&#12290;&#36890;&#36807;&#23545;&#29616;&#23454;&#30340;Texas-7k&#30005;&#32593;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31579;&#36873;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and analyze the application of statistical functional depth metrics for the selection of extreme scenarios in day-ahead grid planning. Our primary motivation is screening of probabilistic scenarios for realized load and renewable generation, in order to identify scenarios most relevant for operational risk mitigation. To handle the high-dimensionality of the scenarios across asset classes and intra-day periods, we employ functional measures of depth to sub-select outlying scenarios that are most likely to be the riskiest for the grid operation. We investigate a range of functional depth measures, as well as a range of operational risks, including load shedding, operational costs, reserves shortfall and variable renewable energy curtailment. The effectiveness of the proposed screening approach is demonstrated through a case study on the realistic Texas-7k grid.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#24605;&#36335;&#38142;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#36890;&#24120;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#26356;&#21152;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#20855;&#26377;&#26356;&#22823;&#22810;&#26679;&#24615;&#19988;&#24615;&#33021;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#36739;&#22909;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#24605;&#36335;&#38142;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2309.11054</link><description>&lt;p&gt;
&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#24605;&#36335;&#38142;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Design of Chain-of-Thought in Math Problem Solving. (arXiv:2309.11054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#24605;&#36335;&#38142;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#23545;&#27604;&#20102;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#36890;&#24120;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#26356;&#21152;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#20855;&#26377;&#26356;&#22823;&#22810;&#26679;&#24615;&#19988;&#24615;&#33021;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#36739;&#22909;&#36873;&#25321;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#24605;&#36335;&#38142;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#36335;&#38142;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#23545;&#35774;&#35745;&#24605;&#36335;&#38142;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32771;&#23519;&#65292;&#27604;&#36739;&#20102;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#24605;&#36335;&#38142;&#21644;&#21508;&#31181;&#31243;&#24207;&#24605;&#36335;&#38142;&#65292;&#21253;&#25324;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#12289;&#27880;&#37322;&#25551;&#36848;&#31243;&#24207;&#21644;&#38750;&#25551;&#36848;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#32534;&#31243;&#35821;&#35328;&#23545;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#24433;&#21709;&#65292;&#27604;&#36739;&#20102;Python&#21644;Wolfram&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;GSM8K&#12289;MATHQA&#21644;SVAMP&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#31243;&#24207;&#24605;&#36335;&#38142;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#36890;&#24120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20855;&#26377;30B&#21442;&#25968;&#30340;&#26368;&#20339;&#32452;&#21512;&#26126;&#26174;&#36229;&#36807;&#20102;GPT-3.5-turbo&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#25105;&#25551;&#36848;&#31243;&#24207;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#22810;&#26679;&#24615;&#65292;&#22240;&#27492;&#36890;&#24120;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;Python&#26159;&#31243;&#24207;&#24605;&#36335;&#38142;&#30340;&#26356;&#22909;&#36873;&#25321;&#27604;Wolfram&#35821;&#35328;&#12290;&#23454;&#39564;&#32467;&#26524;&#20026;&#26410;&#26469;&#32771;&#34385;&#22240;&#32032;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) plays a crucial role in reasoning for math problem solving. We conduct a comprehensive examination of methods for designing CoT, comparing conventional natural language CoT with various program CoTs, including the self-describing program, the comment-describing program, and the non-describing program. Furthermore, we investigate the impact of programming language on program CoTs, comparing Python and Wolfram Language. Through extensive experiments on GSM8K, MATHQA, and SVAMP, we find that program CoTs often have superior effectiveness in math problem solving. Notably, the best performing combination with 30B parameters beats GPT-3.5-turbo by a significant margin. The results show that self-describing program offers greater diversity and thus can generally achieve higher performance. We also find that Python is a better choice of language than Wolfram for program CoTs. The experimental results provide a valuable guideline for future CoT designs that take into acco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#24179;&#21488;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#35782;&#21035;&#20551;&#26032;&#38395;&#65292;&#21516;&#26102;&#25552;&#20379;&#23454;&#26102;&#20998;&#26512;&#21644;&#39564;&#35777;&#26032;&#38395;&#25991;&#31456;&#30495;&#23454;&#24615;&#30340;&#29992;&#25143;&#21451;&#22909;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2309.11052</link><description>&lt;p&gt;
Fake News BR: &#19968;&#31181;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Fake News BR: A Fake News Detection Platform for Brazilian Portuguese. (arXiv:2309.11052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#24179;&#21488;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#35782;&#21035;&#20551;&#26032;&#38395;&#65292;&#21516;&#26102;&#25552;&#20379;&#23454;&#26102;&#20998;&#26512;&#21644;&#39564;&#35777;&#26032;&#38395;&#25991;&#31456;&#30495;&#23454;&#24615;&#30340;&#29992;&#25143;&#21451;&#22909;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20551;&#26032;&#38395;&#20256;&#25773;&#35823;&#23548;&#20844;&#20247;&#33286;&#35770;&#30340;&#28508;&#21147;&#65292;&#20854;&#20256;&#25773;&#24050;&#25104;&#20026;&#36817;&#26399;&#20851;&#27880;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#23545;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#26032;&#38395;&#31867;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#21253;&#25324;TF-IDF&#21644;Word2Vec&#65292;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#20998;&#31867;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#22914;&#36923;&#36753;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;AdaBoost&#21644;LightGBM&#65292;&#20351;&#29992;&#21253;&#21547;&#30495;&#23454;&#21644;&#20551;&#26032;&#38395;&#25991;&#31456;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;F1&#24471;&#20998;&#19978;&#37117;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#65292;&#35777;&#26126;&#20102;&#20854;&#35782;&#21035;&#20551;&#26032;&#38395;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#32593;&#31449;&#24179;&#21488;FAKENEWSBR.COM&#65292;&#20197;&#20415;&#39564;&#35777;&#26032;&#38395;&#25991;&#31456;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#25552;&#20379;&#23454;&#26102;&#20998;&#26512;&#65292;&#20801;&#35768;&#29992;&#25143;&#26816;&#26597;&#26032;&#38395;&#25991;&#31456;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of fake news has become a significant concern in recent times due to its potential to spread misinformation and manipulate public opinion. In this paper, we present a comprehensive study on the detection of fake news in Brazilian Portuguese, focusing on journalistic-type news. We propose a machine learning-based approach that leverages natural language processing techniques, including TF-IDF and Word2Vec, to extract features from textual data. We evaluate the performance of various classification algorithms, such as logistic regression, support vector machine, random forest, AdaBoost, and LightGBM, on a dataset containing both true and fake news articles. The proposed approach achieves a high level of accuracy and F1-Score, demonstrating its effectiveness in identifying fake news. Additionally, we develop a user-friendly web platform, FAKENEWSBR.COM, to facilitate the verification of news articles' veracity. Our platform provides real-time analysis, allowing users to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36793;&#32536;&#36827;&#34892;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#39057;&#22495;&#21387;&#32553;&#21644;&#20869;&#23384;&#30456;&#20114;&#34701;&#21512;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#38754;&#31215;&#25928;&#29575;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.11048</link><description>&lt;p&gt;
&#22312;&#21327;&#20316;&#30340;&#20869;&#23384;&#35745;&#31639;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#39057;&#22495;&#21387;&#32553;&#26469;&#25511;&#21046;&#36793;&#32536;&#30340;&#27169;&#25311;&#25968;&#25454;&#27946;&#27700;
&lt;/p&gt;
&lt;p&gt;
Containing Analog Data Deluge at Edge through Frequency-Domain Compression in Collaborative Compute-in-Memory Networks. (arXiv:2309.11048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36793;&#32536;&#36827;&#34892;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#39057;&#22495;&#21387;&#32553;&#21644;&#20869;&#23384;&#30456;&#20114;&#34701;&#21512;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#38754;&#31215;&#25928;&#29575;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;&#26159;&#22788;&#29702;&#26469;&#33258;&#20256;&#24863;&#22120;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#39640;&#32500;&#12289;&#22810;&#20809;&#35889;&#27169;&#25311;&#25968;&#25454;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#33258;&#20027;&#26080;&#20154;&#26426;&#31561;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#35774;&#22791;&#30340;&#26377;&#38480;&#23384;&#20648;&#21644;&#35745;&#31639;&#36164;&#28304;&#20351;&#24471;&#22312;&#36793;&#32536;&#36827;&#34892;&#22797;&#26434;&#30340;&#39044;&#27979;&#24314;&#27169;&#25104;&#20026;&#19968;&#39033;&#25361;&#25112;&#12290;&#20869;&#23384;&#35745;&#31639;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20027;&#35201;&#33539;&#24335;&#65292;&#29992;&#20110;&#22312;&#36793;&#32536;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#29702;&#20013;&#26368;&#23567;&#21270;&#33021;&#37327;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#38598;&#25104;&#23384;&#20648;&#21644;&#22788;&#29702;&#22312;&#23384;&#20648;&#21333;&#20803;&#21644;/&#25110;&#23384;&#20648;&#22806;&#35774;&#19978;&#21464;&#24471;&#22797;&#26434;&#65292;&#20174;&#26681;&#26412;&#19978;&#22312;&#38754;&#31215;&#25928;&#29575;&#21644;&#33021;&#37327;&#25928;&#29575;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20219;&#21153;&#20013;&#38754;&#31215;&#25928;&#29575;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#20004;&#31181;&#20851;&#38190;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#39057;&#22495;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20108;&#20540;&#21270;&#30340;Walsh-Hadamard&#21464;&#25442;&#65292;&#20943;&#23569;&#20102;DNN&#30340;&#24517;&#35201;&#21442;&#25968;&#65288;&#22312;MobileNetV2&#20013;&#20943;&#23569;&#20102;87%&#65289;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;SRAM&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21033;&#29992;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24182;&#34892;&#24615;&#12290;&#20854;&#27425;&#65292;&#19968;&#31181;&#20869;&#23384;&#30456;&#20114;&#34701;&#21512;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge computing is a promising solution for handling high-dimensional, multispectral analog data from sensors and IoT devices for applications such as autonomous drones. However, edge devices' limited storage and computing resources make it challenging to perform complex predictive modeling at the edge. Compute-in-memory (CiM) has emerged as a principal paradigm to minimize energy for deep learning-based inference at the edge. Nevertheless, integrating storage and processing complicates memory cells and/or memory peripherals, essentially trading off area efficiency for energy efficiency. This paper proposes a novel solution to improve area efficiency in deep learning inference tasks. The proposed method employs two key strategies. Firstly, a Frequency domain learning approach uses binarized Walsh-Hadamard Transforms, reducing the necessary parameters for DNN (by 87% in MobileNetV2) and enabling compute-in-SRAM, which better utilizes parallelism during inference. Secondly, a memory-immer
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#30340;&#20013;&#38388;&#20840;&#23616;&#27169;&#22411;Clustered FedStack&#12290;&#36890;&#36807;&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#39044;&#27979;&#21644;&#36755;&#20986;&#23618;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26426;&#21046;&#23545;&#26412;&#22320;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2309.11044</link><description>&lt;p&gt;
Clustered FedStack&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#30340;&#20013;&#38388;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustered FedStack: Intermediate Global Models with Bayesian Information Criterion. (arXiv:2309.11044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11044
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#30340;&#20013;&#38388;&#20840;&#23616;&#27169;&#22411;Clustered FedStack&#12290;&#36890;&#36807;&#23558;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#39044;&#27979;&#21644;&#36755;&#20986;&#23618;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#26426;&#21046;&#23545;&#26412;&#22320;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#30446;&#21069;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26368;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#22240;&#20854;&#21327;&#20316;&#23398;&#20064;&#21644;&#20445;&#25252;&#23458;&#25143;&#38544;&#31169;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#38750;&#29420;&#31435;&#21644;&#38750;&#29420;&#31435;&#20998;&#24067;&#65288;&#38750;IID&#65289;&#20197;&#21450;&#26412;&#22320;&#23458;&#25143;&#20043;&#38388;&#26631;&#31614;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#30740;&#31350;&#22242;&#38431;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;&#20351;&#29992;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#12289;&#32852;&#37030;&#29983;&#25104;&#23545;&#25239;&#23398;&#20064;&#21644;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24050;&#21457;&#34920;&#30340;Stacked Federated Learning&#65288;FedStack&#65289;&#26694;&#26550;&#30340;&#26032;&#39062;Clustered FedStack&#26694;&#26550;&#12290;&#26412;&#22320;&#23458;&#25143;&#31471;&#23558;&#20854;&#27169;&#22411;&#39044;&#27979;&#21644;&#36755;&#20986;&#23618;&#26435;&#37325;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#28982;&#21518;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#36825;&#20010;&#20840;&#23616;&#27169;&#22411;&#20351;&#29992;&#32858;&#31867;&#26426;&#21046;&#22522;&#20110;&#20854;&#36755;&#20986;&#23618;&#26435;&#37325;&#23545;&#26412;&#22320;&#23458;&#25143;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#32858;&#31867;&#26426;&#21046;&#65292;&#20998;&#21035;&#26159;K-Means&#12289;Agglomerative&#12289;DBSCAN&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is currently one of the most popular technologies in the field of Artificial Intelligence (AI) due to its collaborative learning and ability to preserve client privacy. However, it faces challenges such as non-identically and non-independently distributed (non-IID) and data with imbalanced labels among local clients. To address these limitations, the research community has explored various approaches such as using local model parameters, federated generative adversarial learning, and federated representation learning. In our study, we propose a novel Clustered FedStack framework based on the previously published Stacked Federated Learning (FedStack) framework. The local clients send their model predictions and output layer weights to a server, which then builds a robust global model. This global model clusters the local clients based on their output layer weights using a clustering mechanism. We adopt three clustering mechanisms, namely K-Means, Agglomerative, a
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#21644;&#26131;&#25193;&#23637;&#24615;&#30340;&#26032;&#33539;&#24335;&#65292;&#20026;&#35299;&#20915;&#21160;&#24577;&#36710;&#36742;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.11039</link><description>&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#65306;&#26368;&#36817;&#30340;&#24212;&#29992;&#21644;&#24320;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning in Intelligent Transportation Systems: Recent Applications and Open Problems. (arXiv:2309.11039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11039
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#21644;&#26131;&#25193;&#23637;&#24615;&#30340;&#26032;&#33539;&#24335;&#65292;&#20026;&#35299;&#20915;&#21160;&#24577;&#36710;&#36742;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#21463;&#21040;&#36890;&#20449;&#25216;&#26415;&#12289;&#20256;&#24863;&#22120;&#25216;&#26415;&#21644;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#30340;&#36805;&#29467;&#21457;&#23637;&#30340;&#25512;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36710;&#36742;&#32593;&#32476;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#21450;&#26102;&#20934;&#30830;&#22320;&#20915;&#31574;&#36710;&#36742;&#34892;&#20026;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#31227;&#21160;&#26080;&#32447;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#65292;&#36710;&#36742;&#20449;&#24687;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#38754;&#20020;&#30528;&#25345;&#32493;&#30340;&#39118;&#38505;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#24613;&#38656;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#26469;&#24212;&#23545;&#21160;&#24577;&#36710;&#36742;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22240;&#20854;&#20986;&#33394;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#33021;&#21644;&#26131;&#25193;&#23637;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#23545;ITS&#20013;FL&#26368;&#26032;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#30740;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;ITS&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#20174;&#21508;&#20010;&#35282;&#24230;&#38416;&#26126;&#24212;&#29992;FL&#30340;&#21160;&#26426;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;FL&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Intelligent transportation systems (ITSs) have been fueled by the rapid development of communication technologies, sensor technologies, and the Internet of Things (IoT). Nonetheless, due to the dynamic characteristics of the vehicle networks, it is rather challenging to make timely and accurate decisions of vehicle behaviors. Moreover, in the presence of mobile wireless communications, the privacy and security of vehicle information are at constant risk. In this context, a new paradigm is urgently needed for various applications in dynamic vehicle environments. As a distributed machine learning technology, federated learning (FL) has received extensive attention due to its outstanding privacy protection properties and easy scalability. We conduct a comprehensive survey of the latest developments in FL for ITS. Specifically, we initially research the prevalent challenges in ITS and elucidate the motivations for applying FL from various perspectives. Subsequently, we review existing depl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#31867;&#30340;&#26080;&#23548;&#25968;&#20248;&#21270;&#31639;&#27861;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25910;&#32553;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;RACE-CARS&#8221;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#21306;&#22495;&#25910;&#32553;&#30340;&#21152;&#36895;&#24615;&#36136;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;"RACE-CARS"&#30340;&#39640;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32463;&#39564;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.11036</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#25910;&#32553;&#30340;&#20998;&#31867;&#20248;&#21270;&#31639;&#27861;&#30340;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
A Region-Shrinking-Based Acceleration for Classification-Based Derivative-Free Optimization. (arXiv:2309.11036v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#31867;&#30340;&#26080;&#23548;&#25968;&#20248;&#21270;&#31639;&#27861;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21306;&#22495;&#25910;&#32553;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;RACE-CARS&#8221;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#21306;&#22495;&#25910;&#32553;&#30340;&#21152;&#36895;&#24615;&#36136;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;"RACE-CARS"&#30340;&#39640;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#32463;&#39564;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#23548;&#25968;&#20248;&#21270;&#31639;&#27861;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#26080;&#27861;&#33719;&#21462;&#23548;&#25968;&#20449;&#24687;&#26102;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#31867;&#30340;&#26080;&#23548;&#25968;&#20248;&#21270;&#31639;&#27861;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;&#20551;&#35774;-&#30446;&#26631;&#30772;&#35010;&#29575;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#35813;&#31867;&#22411;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#19978;&#30028;&#12290;&#21463;&#37325;&#26032;&#23457;&#35270;&#30340;&#19978;&#30028;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;RACE-CARS&#8221;&#30340;&#31639;&#27861;&#65292;&#19982;&#8220;SRACOS&#8221;&#65288;Hu et al., 2017&#65289;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#28155;&#21152;&#20102;&#19968;&#20010;&#38543;&#26426;&#21306;&#22495;&#25910;&#32553;&#27493;&#39588;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#21306;&#22495;&#25910;&#32553;&#30340;&#21152;&#36895;&#24615;&#36136;&#12290;&#38024;&#23545;&#21512;&#25104;&#20989;&#25968;&#20197;&#21450;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#40657;&#30418;&#35843;&#20248;&#30340;&#23454;&#39564;&#22312;&#32463;&#39564;&#35777;&#26126;&#20102;&#8220;RACE-CARS&#8221;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20851;&#20110;&#24341;&#20837;&#36229;&#21442;&#25968;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#8220;RACE-CARS&#8221;&#30340;&#24037;&#20316;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#32463;&#39564;&#30340;&#36229;&#21442;&#25968;&#35843;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Derivative-free optimization algorithms play an important role in scientific and engineering design optimization problems, especially when derivative information is not accessible. In this paper, we study the framework of classification-based derivative-free optimization algorithms. By introducing a concept called hypothesis-target shattering rate, we revisit the computational complexity upper bound of this type of algorithms. Inspired by the revisited upper bound, we propose an algorithm named "RACE-CARS", which adds a random region-shrinking step compared with "SRACOS" (Hu et al., 2017).. We further establish a theorem showing the acceleration of region-shrinking. Experiments on the synthetic functions as well as black-box tuning for language-model-as-a-service demonstrate empirically the efficiency of "RACE-CARS". An ablation experiment on the introduced hyperparameters is also conducted, revealing the mechanism of "RACE-CARS" and putting forward an empirical hyperparameter-tuning g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20174;&#20960;&#20309;&#32467;&#26500;&#21040;&#25299;&#25169;&#32467;&#26500;&#30340;&#25277;&#35937;&#27493;&#39588;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#34920;&#24449;&#30456;&#20284;&#20998;&#26512;&#26041;&#27861;&#65288;tRSA&#65289;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#22320;&#29702;&#25299;&#25169;&#25688;&#35201;&#32479;&#35745;&#37327;&#23545;&#22823;&#33041;&#34920;&#24449;&#36827;&#34892;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.11028</link><description>&lt;p&gt;
&#31070;&#32463;&#34920;&#24449;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Topology and Geometry of Neural Representations. (arXiv:2309.11028v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20174;&#20960;&#20309;&#32467;&#26500;&#21040;&#25299;&#25169;&#32467;&#26500;&#30340;&#25277;&#35937;&#27493;&#39588;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#34920;&#24449;&#30456;&#20284;&#20998;&#26512;&#26041;&#27861;&#65288;tRSA&#65289;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#22320;&#29702;&#25299;&#25169;&#25688;&#35201;&#32479;&#35745;&#37327;&#23545;&#22823;&#33041;&#34920;&#24449;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31185;&#23398;&#25152;&#20851;&#24515;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#34920;&#24449;&#24863;&#30693;&#21644;&#35748;&#30693;&#20869;&#23481;&#30340;&#22823;&#33041;&#34920;&#24449;&#12290;&#19968;&#20010;&#29702;&#24819;&#30340;&#34920;&#24449;&#24212;&#35813;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#21151;&#33021;&#21306;&#22495;&#65292;&#24182;&#19988;&#23545;&#22122;&#22768;&#21644;&#20010;&#20307;&#22823;&#33041;&#30340;&#29305;&#24322;&#24615;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#32780;&#19981;&#20250;&#19982;&#35745;&#31639;&#24046;&#24322;&#30456;&#23545;&#24212;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#34920;&#24449;&#20960;&#20309;&#32467;&#26500;&#26469;&#34920;&#24449;&#22823;&#33041;&#34920;&#24449;&#65292;&#20960;&#20309;&#32467;&#26500;&#30001;&#34920;&#24449;&#19981;&#30456;&#20284;&#30697;&#38453;&#65288;RDM&#65289;&#23450;&#20041;&#65292;RDM&#26159;&#19968;&#20010;&#25688;&#35201;&#32479;&#35745;&#37327;&#65292;&#25688;&#35201;&#20102;&#20010;&#20307;&#31070;&#32463;&#20803;&#65288;&#25110;&#21709;&#24212;&#36890;&#36947;&#65289;&#30340;&#20316;&#29992;&#65292;&#24182;&#34920;&#24449;&#20102;&#21050;&#28608;&#30340;&#21487;&#36776;&#21035;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20174;&#20960;&#20309;&#32467;&#26500;&#21040;&#22823;&#33041;&#34920;&#24449;&#25299;&#25169;&#30340;&#25277;&#35937;&#27493;&#39588;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25299;&#25169;&#34920;&#24449;&#30456;&#20284;&#20998;&#26512;&#65288;tRSA&#65289;&#65292;&#23427;&#26159;&#34920;&#24449;&#30456;&#20284;&#20998;&#26512;&#65288;RSA&#65289;&#30340;&#19968;&#31181;&#25193;&#23637;&#65292;&#20351;&#29992;&#20102;&#19968;&#31995;&#21015;&#22320;&#29702;&#25299;&#25169;&#25688;&#35201;&#32479;&#35745;&#37327;&#65292;&#23558;RDM&#36827;&#34892;&#27867;&#21270;&#20197;&#34920;&#24449;&#25299;&#25169;&#32467;&#26500;&#24182;&#20943;&#24369;&#20960;&#20309;&#32467;&#26500;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central question for neuroscience is how to characterize brain representations of perceptual and cognitive content. An ideal characterization should distinguish different functional regions with robustness to noise and idiosyncrasies of individual brains that do not correspond to computational differences. Previous studies have characterized brain representations by their representational geometry, which is defined by the representational dissimilarity matrix (RDM), a summary statistic that abstracts from the roles of individual neurons (or responses channels) and characterizes the discriminability of stimuli. Here we explore a further step of abstraction: from the geometry to the topology of brain representations. We propose topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics that generalizes the RDM to characterize the topology while de-emphasizing the geometry. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36827;&#34892;&#25968;&#25454;&#26356;&#26032;&#26102;&#21487;&#33021;&#21457;&#29983;&#30340;&#20449;&#24687;&#27844;&#38706;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#27979;&#32622;&#20449;&#24230;&#24046;&#24322;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#20004;&#20010;&#27169;&#22411;&#24555;&#29031;&#30456;&#23545;&#20110;&#20165;&#35775;&#38382;&#26356;&#26032;&#25968;&#25454;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#20449;&#24687;&#27844;&#38706;&#12290;</title><link>http://arxiv.org/abs/2309.11022</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#25968;&#25454;&#26356;&#26032;&#30340;&#20449;&#24687;&#27844;&#38706;
&lt;/p&gt;
&lt;p&gt;
Information Leakage from Data Updates in Machine Learning Models. (arXiv:2309.11022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36827;&#34892;&#25968;&#25454;&#26356;&#26032;&#26102;&#21487;&#33021;&#21457;&#29983;&#30340;&#20449;&#24687;&#27844;&#38706;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#27979;&#32622;&#20449;&#24230;&#24046;&#24322;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#20004;&#20010;&#27169;&#22411;&#24555;&#29031;&#30456;&#23545;&#20110;&#20165;&#35775;&#38382;&#26356;&#26032;&#25968;&#25454;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#20449;&#24687;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#26356;&#26032;&#25968;&#25454;&#38598;&#26102;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24773;&#26223;&#65292;&#20197;&#20415;&#23558;&#26368;&#26032;&#20449;&#24687;&#32435;&#20837;&#27169;&#22411;&#25110;&#21453;&#26144;&#20998;&#24067;&#21464;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#25512;&#26029;&#20986;&#20851;&#20110;&#36825;&#20123;&#26356;&#26032;&#30340;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#35760;&#24405;&#23646;&#24615;&#20540;&#30340;&#26356;&#25913;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#25968;&#25454;&#38598;&#21464;&#21270;&#21069;&#21518;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#29031;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#30456;&#21453;&#65292;&#25105;&#20204;&#20551;&#35774;&#19968;&#20010;&#25110;&#22810;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#23646;&#24615;&#21457;&#29983;&#25913;&#21464;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#25968;&#25454;&#35760;&#24405;&#34987;&#21024;&#38500;&#25110;&#28155;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22987;&#27169;&#22411;&#21644;&#26356;&#26032;&#27169;&#22411;&#39044;&#27979;&#32622;&#20449;&#24230;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#21644;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#20004;&#20010;&#24555;&#29031;&#19982;&#20165;&#35775;&#38382;&#26356;&#26032;&#25968;&#25454;&#30456;&#27604;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#20449;&#24687;&#27844;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider the setting where machine learning models are retrained on updated datasets in order to incorporate the most up-to-date information or reflect distribution shifts. We investigate whether one can infer information about these updates in the training data (e.g., changes to attribute values of records). Here, the adversary has access to snapshots of the machine learning model before and after the change in the dataset occurs. Contrary to the existing literature, we assume that an attribute of a single or multiple training data points are changed rather than entire data records are removed or added. We propose attacks based on the difference in the prediction confidence of the original model and the updated model. We evaluate our attack methods on two public datasets along with multi-layer perceptron and logistic regression models. We validate that two snapshots of the model can result in higher information leakage in comparison to having access to only the update
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#24418;&#21270;&#30340;&#36731;&#37327;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#23558;&#20849;&#24418;&#39044;&#27979;&#19982;&#28145;&#24230;&#23398;&#20064;&#22238;&#24402;&#22120;&#32467;&#21512;&#65292;&#22312;&#22810;&#27169;&#24577;&#24773;&#20917;&#19979;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#36793;&#30028;&#65292;&#24182;&#22522;&#20110;&#20809;&#27969;&#25512;&#29702;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11018</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#22238;&#24402;&#19982;&#25512;&#29702;&#30340;&#20849;&#24418;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformalized Multimodal Uncertainty Regression and Reasoning. (arXiv:2309.11018v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#24418;&#21270;&#30340;&#36731;&#37327;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#23558;&#20849;&#24418;&#39044;&#27979;&#19982;&#28145;&#24230;&#23398;&#20064;&#22238;&#24402;&#22120;&#32467;&#21512;&#65292;&#22312;&#22810;&#27169;&#24577;&#24773;&#20917;&#19979;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#36793;&#30028;&#65292;&#24182;&#22522;&#20110;&#20809;&#27969;&#25512;&#29702;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#23558;&#20849;&#24418;&#39044;&#27979;&#19982;&#28145;&#24230;&#23398;&#20064;&#22238;&#24402;&#22120;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#39044;&#27979;&#22810;&#27169;&#24577;&#65288;&#19981;&#37325;&#21472;&#65289;&#30340;&#19981;&#30830;&#23450;&#24615;&#36793;&#30028;&#12290;&#25105;&#20204;&#29305;&#21035;&#35752;&#35770;&#20102;&#22312;&#35270;&#35273;&#37324;&#31243;&#35745;&#65288;VO&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#29615;&#22659;&#29305;&#24449;&#65288;&#20363;&#22914;&#39134;&#34892;&#22495;&#23545;&#31216;&#24615;&#65289;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#22312;&#27169;&#31946;&#21644;&#36974;&#25377;&#19979;&#20250;&#23548;&#33268;&#22810;&#27169;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#20197;&#26681;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25805;&#20316;&#26465;&#20214;&#65288;&#22914;&#22122;&#22768;&#12289;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#39044;&#27979;&#27169;&#22411;&#21442;&#25968;&#30340;&#26377;&#38480;&#22823;&#23567;&#65289;&#36827;&#34892;&#26679;&#26412;&#36866;&#24212;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#25512;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#36825;&#20123;&#31283;&#20581;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#20809;&#27969;&#30340;&#25512;&#29702;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#36866;&#24403;&#32771;&#34385;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#26469;&#38381;&#29615;&#20272;&#35745;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#33021;&#22815;&#25552;&#20379;&#31283;&#23450;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a lightweight uncertainty estimator capable of predicting multimodal (disjoint) uncertainty bounds by integrating conformal prediction with a deep-learning regressor. We specifically discuss its application for visual odometry (VO), where environmental features such as flying domain symmetries and sensor measurements under ambiguities and occlusion can result in multimodal uncertainties. Our simulation results show that uncertainty estimates in our framework adapt sample-wise against challenging operating conditions such as pronounced noise, limited training data, and limited parametric size of the prediction model. We also develop a reasoning framework that leverages these robust uncertainty estimates and incorporates optical flow-based reasoning to improve prediction prediction accuracy. Thus, by appropriately accounting for predictive uncertainties of data-driven learning and closing their estimation loop via rule-based reasoning, our methodology consistently s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D-U-SAM&#32593;&#32476;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;CBCT&#22270;&#20687;&#30340;&#29273;&#40831;&#20998;&#21106;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;SAM&#21644;&#21367;&#31215;&#36924;&#36817;&#26041;&#27861;&#65292;&#20197;&#21450;&#36339;&#36291;&#36830;&#25509;&#34701;&#21512;&#29305;&#24449;&#65292;&#26412;&#26041;&#27861;&#22312;&#35299;&#20915;&#23567;&#26679;&#26412;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11015</link><description>&lt;p&gt;
&#36890;&#36807;3D-U-SAM&#32593;&#32476;&#36827;&#34892;&#23569;&#26679;&#26412;CBCT&#22270;&#20687;&#30340;&#29273;&#40831;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images. (arXiv:2309.11015v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D-U-SAM&#32593;&#32476;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;CBCT&#22270;&#20687;&#30340;&#29273;&#40831;&#20998;&#21106;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;SAM&#21644;&#21367;&#31215;&#36924;&#36817;&#26041;&#27861;&#65292;&#20197;&#21450;&#36339;&#36291;&#36830;&#25509;&#34701;&#21512;&#29305;&#24449;&#65292;&#26412;&#26041;&#27861;&#22312;&#35299;&#20915;&#23567;&#26679;&#26412;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29273;&#40831;&#20301;&#32622;&#30340;&#20934;&#30830;&#34920;&#31034;&#22312;&#27835;&#30103;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;3D&#29273;&#40831;&#22270;&#20687;&#20998;&#21106;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#28982;&#32780;&#26631;&#27880;&#30340;3D&#29273;&#40831;&#25968;&#25454;&#38598;&#26159;&#31232;&#32570;&#30340;&#36164;&#28304;&#65292;&#36825;&#23548;&#33268;&#20102;&#36825;&#20010;&#20219;&#21153;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#38754;&#20020;&#23567;&#26679;&#26412;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;SAM&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D-U-SAM&#32593;&#32476;&#29992;&#20110;3D&#29273;&#40831;&#22270;&#20687;&#20998;&#21106;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#35299;&#20915;&#22312;3D&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;2D&#39044;&#35757;&#32451;&#26435;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21367;&#31215;&#36924;&#36817;&#26041;&#27861;&#65307;&#20026;&#20102;&#20445;&#30041;&#26356;&#22810;&#32454;&#33410;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36339;&#36291;&#36830;&#25509;&#65292;&#20197;&#21442;&#32771;U-Net&#22312;&#25152;&#26377;&#23618;&#32423;&#19978;&#34701;&#21512;&#29305;&#24449;&#12290;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#12289;&#23545;&#27604;&#23454;&#39564;&#21644;&#26679;&#26412;&#22823;&#23567;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate representation of tooth position is extremely important in treatment. 3D dental image segmentation is a widely used method, however labelled 3D dental datasets are a scarce resource, leading to the problem of small samples that this task faces in many cases. To this end, we address this problem with a pretrained SAM and propose a novel 3D-U-SAM network for 3D dental image segmentation. Specifically, in order to solve the problem of using 2D pre-trained weights on 3D datasets, we adopted a convolution approximation method; in order to retain more details, we designed skip connections to fuse features at all levels with reference to U-Net. The effectiveness of the proposed method is demonstrated in ablation experiments, comparison experiments, and sample size experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#27169;&#22411;&#21151;&#33021;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ModelGiF&#12290;&#36890;&#36807;&#22312;&#24322;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#21516;&#36136;&#30340;&#34920;&#31034;&#65292;ModelGiF&#21487;&#36890;&#36807;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#34913;&#37327;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#30456;&#20851;&#24615;&#20272;&#35745;&#12289;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#21644;&#27169;&#22411;&#36951;&#24536;&#39564;&#35777;&#31561;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11013</link><description>&lt;p&gt;
ModelGiF: &#27169;&#22411;&#21151;&#33021;&#36317;&#31163;&#30340;&#26799;&#24230;&#22330;
&lt;/p&gt;
&lt;p&gt;
ModelGiF: Gradient Fields for Model Functional Distance. (arXiv:2309.11013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#27169;&#22411;&#21151;&#33021;&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;ModelGiF&#12290;&#36890;&#36807;&#22312;&#24322;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#21516;&#36136;&#30340;&#34920;&#31034;&#65292;ModelGiF&#21487;&#36890;&#36807;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#34913;&#37327;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#30456;&#20851;&#24615;&#20272;&#35745;&#12289;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#21644;&#27169;&#22411;&#36951;&#24536;&#39564;&#35777;&#31561;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#35265;&#35777;&#20102;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#21644;&#20844;&#24320;&#21457;&#24067;&#30340;&#35757;&#32451;&#27169;&#22411;&#30340;&#28608;&#22686;&#65292;&#36825;&#23601;&#38656;&#35201;&#23545;&#21508;&#31181;&#30446;&#30340;&#30340;&#27169;&#22411;&#21151;&#33021;&#36317;&#31163;&#36827;&#34892;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20869;&#37096;&#24037;&#20316;&#30340;&#19981;&#36879;&#26126;&#24615;&#21644;&#20307;&#31995;&#32467;&#26500;&#25110;&#20219;&#21153;&#30340;&#24322;&#36136;&#24615;&#65292;&#37327;&#21270;&#27169;&#22411;&#21151;&#33021;&#36317;&#31163;&#22987;&#32456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21463;&#29289;&#29702;&#23398;&#20013;&#8220;&#22330;&#8221;&#27010;&#24565;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#27169;&#22411;&#26799;&#24230;&#22330;&#65288;&#31616;&#31216;ModelGiF&#65289;&#65292;&#20174;&#24322;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#21516;&#36136;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20551;&#35774;&#26159;&#65292;&#27599;&#20010;&#39044;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#22312;&#36755;&#20837;&#31354;&#38388;&#19978;&#21807;&#19968;&#30830;&#23450;&#19968;&#20010;ModelGiF&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#20043;&#38388;&#30340;&#36317;&#31163;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;ModelGiF&#30340;&#30456;&#20284;&#24615;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;ModelGiF&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20219;&#21153;&#30456;&#20851;&#24615;&#20272;&#35745;&#12289;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#21644;&#27169;&#22411;&#36951;&#24536;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last decade has witnessed the success of deep learning and the surge of publicly released trained models, which necessitates the quantification of the model functional distance for various purposes. However, quantifying the model functional distance is always challenging due to the opacity in inner workings and the heterogeneity in architectures or tasks. Inspired by the concept of "field" in physics, in this work we introduce Model Gradient Field (abbr. ModelGiF) to extract homogeneous representations from the heterogeneous pre-trained models. Our main assumption underlying ModelGiF is that each pre-trained deep model uniquely determines a ModelGiF over the input space. The distance between models can thus be measured by the similarity between their ModelGiFs. We validate the effectiveness of the proposed ModelGiF with a suite of testbeds, including task relatedness estimation, intellectual property protection, and model unlearning verification. Experimental results demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#35299;&#26500;&#24230;&#37327;&#26469;&#25552;&#39640;&#35748;&#35777;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#35748;&#35777;&#26426;&#21046;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#25968;&#25454;&#38598;&#26080;&#20851;&#21644;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#24230;&#37327;&#37117;&#26377;&#25928;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23558;&#21487;&#23454;&#29616;&#30340;&#35748;&#35777;&#21322;&#24452;&#25552;&#39640;&#19968;&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2309.11005</link><description>&lt;p&gt;
Simplex&#65281;&#36890;&#36807;&#35299;&#26500;&#24230;&#37327;&#26469;&#25552;&#39640;&#35748;&#35777;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
It's Simplex! Disaggregating Measures to Improve Certified Robustness. (arXiv:2309.11005v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#35299;&#26500;&#24230;&#37327;&#26469;&#25552;&#39640;&#35748;&#35777;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#35748;&#35777;&#26426;&#21046;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#25968;&#25454;&#38598;&#26080;&#20851;&#21644;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#24230;&#37327;&#37117;&#26377;&#25928;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#23558;&#21487;&#23454;&#29616;&#30340;&#35748;&#35777;&#21322;&#24452;&#25552;&#39640;&#19968;&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32473;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#25552;&#20379;&#25915;&#20987;&#22823;&#23567;&#19978;&#19981;&#21464;&#30340;&#20445;&#35777;&#65292;&#35748;&#35777;&#30340;&#40065;&#26834;&#24615;&#32469;&#36807;&#20102;&#23545;&#25239;&#25915;&#20987;&#20013;&#38450;&#24481;&#30340;&#33030;&#24369;&#24615;&#12290;&#23613;&#31649;&#36825;&#20123;&#35748;&#35777;&#20855;&#26377;&#20215;&#20540;&#65292;&#20294;&#25105;&#20204;&#35780;&#20272;&#23427;&#20204;&#24615;&#33021;&#30340;&#25216;&#26415;&#27809;&#26377;&#23545;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#36827;&#34892;&#36866;&#24403;&#30340;&#32771;&#34385;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#20998;&#26512;&#22312;&#32771;&#34385;&#32858;&#21512;&#24230;&#37327;&#32780;&#19981;&#26159;&#21333;&#20010;&#26679;&#26412;&#30340;&#24615;&#33021;&#26041;&#38754;&#36827;&#34892;&#20102;&#35268;&#36991;&#12290;&#36890;&#36807;&#32771;&#34385;&#35748;&#35777;&#27169;&#22411;&#30340;&#28508;&#22312;&#36755;&#20986;&#31354;&#38388;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#35748;&#35777;&#26426;&#21046;&#30340;&#20998;&#26512;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20801;&#35768;&#36827;&#34892;&#25968;&#25454;&#38598;&#26080;&#20851;&#21644;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#35748;&#35777;&#24615;&#33021;&#24230;&#37327;&#12290;&#37319;&#32435;&#36825;&#31181;&#35270;&#35282;&#25581;&#31034;&#20102;&#26032;&#30340;&#35748;&#35777;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#24403;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#36825;&#20123;&#26041;&#27861;&#26377;&#28508;&#21147;&#23558;&#21487;&#23454;&#29616;&#30340;&#35748;&#35777;&#21322;&#24452;&#25552;&#39640;&#19968;&#20493;&#20197;&#19978;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#21487;&#20197;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Certified robustness circumvents the fragility of defences against adversarial attacks, by endowing model predictions with guarantees of class invariance for attacks up to a calculated size. While there is value in these certifications, the techniques through which we assess their performance do not present a proper accounting of their strengths and weaknesses, as their analysis has eschewed consideration of performance over individual samples in favour of aggregated measures. By considering the potential output space of certified models, this work presents two distinct approaches to improve the analysis of certification mechanisms, that allow for both dataset-independent and dataset-dependent measures of certification performance. Embracing such a perspective uncovers new certification approaches, which have the potential to more than double the achievable radius of certification, relative to current state-of-the-art. Empirical evaluation verifies that our new approach can certify $9\
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;</title><link>http://arxiv.org/abs/2309.10980</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning. (arXiv:2309.10980v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10980
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#37096;&#32626;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#38024;&#23545;&#19981;&#21516;&#30340;&#29983;&#29702;&#29305;&#24449;&#36827;&#34892;&#30417;&#27979;&#65292;&#24182;&#26681;&#25454;&#32039;&#24613;&#31243;&#24230;&#39044;&#35686;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#24739;&#32773;&#30417;&#27979;&#23545;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#21307;&#30103;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#30417;&#27979;&#31995;&#32479;&#24448;&#24448;&#38590;&#20197;&#22788;&#29702;&#22797;&#26434;&#12289;&#21160;&#24577;&#30340;&#29615;&#22659;&#21644;&#27874;&#21160;&#30340;&#29983;&#21629;&#20307;&#24449;&#65292;&#23548;&#33268;&#24310;&#36831;&#21457;&#29616;&#21361;&#24613;&#24773;&#20917;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;AI&#39537;&#21160;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37096;&#32626;&#20102;&#22810;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#19987;&#38376;&#36127;&#36131;&#30417;&#27979;&#29305;&#23450;&#30340;&#29983;&#29702;&#29305;&#24449;&#65292;&#22914;&#24515;&#29575;&#12289;&#21628;&#21560;&#21644;&#20307;&#28201;&#12290;&#36825;&#20123;&#26234;&#33021;&#20307;&#19982;&#36890;&#29992;&#30340;&#21307;&#30103;&#30417;&#27979;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#23398;&#20064;&#24739;&#32773;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#32039;&#24613;&#31243;&#24230;&#20570;&#20986;&#36890;&#30693;&#30456;&#24212;&#21307;&#30103;&#32039;&#24613;&#22242;&#38431;&#65288;MET&#65289;&#30340;&#20915;&#31574;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;PPG-DaLiA&#21644;WESAD&#65289;&#30340;&#30495;&#23454;&#29983;&#29702;&#21644;&#36816;&#21160;&#25968;&#25454;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#22810;&#26234;&#33021;&#20307;DRL&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#24182;&#23637;&#26395;&#20102;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22312;&#22270;&#25968;&#25454;&#39046;&#22495;&#30340;&#24403;&#21069;&#21162;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65306;&#25968;&#25454;&#20013;&#24515;&#21270;&#22270;&#26426;&#22120;&#23398;&#20064;&#65288;DC-GML&#65289;&#65292;&#24182;&#20171;&#32461;&#20102;&#35299;&#20915;&#22270;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#36136;&#37327;&#21644;&#26500;&#24314;&#22270;MLOps&#31995;&#32479;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10979</link><description>&lt;p&gt;
&#36208;&#21521;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#65306;&#22238;&#39038;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Towards Data-centric Graph Machine Learning: Review and Outlook. (arXiv:2309.10979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#24182;&#23637;&#26395;&#20102;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22312;&#22270;&#25968;&#25454;&#39046;&#22495;&#30340;&#24403;&#21069;&#21162;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65306;&#25968;&#25454;&#20013;&#24515;&#21270;&#22270;&#26426;&#22120;&#23398;&#20064;&#65288;DC-GML&#65289;&#65292;&#24182;&#20171;&#32461;&#20102;&#35299;&#20915;&#22270;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#36136;&#37327;&#21644;&#26500;&#24314;&#22270;MLOps&#31995;&#32479;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20197;&#20854;&#23545;&#25968;&#25454;&#30340;&#25910;&#38598;&#12289;&#31649;&#29702;&#21644;&#21033;&#29992;&#20026;&#20027;&#35201;&#28966;&#28857;&#65292;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#25552;&#20379;&#20102;&#23545;&#19982;&#22270;&#25968;&#25454;&#30456;&#20851;&#30340;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;AI&#24403;&#21069;&#24037;&#20316;&#30340;&#21069;&#30651;&#24615;&#23637;&#26395;&#65292;&#36825;&#26159;&#34920;&#31034;&#21644;&#25429;&#25417;&#24222;&#22823;&#19988;&#22810;&#26679;&#30340;&#29616;&#23454;&#23454;&#20307;&#20043;&#38388;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#22522;&#26412;&#25968;&#25454;&#32467;&#26500;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#25968;&#25454;&#20013;&#24515;&#21270;&#22270;&#26426;&#22120;&#23398;&#20064;&#65288;DC-GML&#65289;&#65292;&#28085;&#30422;&#20102;&#22270;&#25968;&#25454;&#29983;&#21629;&#21608;&#26399;&#30340;&#25152;&#26377;&#38454;&#27573;&#65292;&#21253;&#25324;&#22270;&#25968;&#25454;&#25910;&#38598;&#12289;&#25506;&#32034;&#12289;&#25913;&#36827;&#12289;&#21033;&#29992;&#21644;&#32500;&#25252;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#20998;&#31867;&#27861;&#22238;&#31572;&#20102;&#19977;&#20010;&#20851;&#38190;&#30340;&#22270;&#20013;&#24515;&#21270;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22914;&#20309;&#25552;&#39640;&#22270;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#36136;&#37327;&#65307;&#65288;2&#65289;&#22914;&#20309;&#20174;&#21487;&#29992;&#24615;&#26377;&#38480;&#21644;&#36136;&#37327;&#36739;&#20302;&#30340;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#65307;&#65288;3&#65289;&#22914;&#20309;&#20174;&#22270;&#25968;&#25454;&#26500;&#24314;&#22270;MLOps&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-centric AI, with its primary focus on the collection, management, and utilization of data to drive AI models and applications, has attracted increasing attention in recent years. In this article, we conduct an in-depth and comprehensive review, offering a forward-looking outlook on the current efforts in data-centric AI pertaining to graph data-the fundamental data structure for representing and capturing intricate dependencies among massive and diverse real-life entities. We introduce a systematic framework, Data-centric Graph Machine Learning (DC-GML), that encompasses all stages of the graph data lifecycle, including graph data collection, exploration, improvement, exploitation, and maintenance. A thorough taxonomy of each stage is presented to answer three critical graph-centric questions: (1) how to enhance graph data availability and quality; (2) how to learn from graph data with limited-availability and low-quality; (3) how to build graph MLOps systems from the graph data-c
&lt;/p&gt;</description></item><item><title>PAGER&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#25925;&#38556;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#20998;&#25968;&#65292;&#23545;&#26679;&#26412;&#36827;&#34892;&#20998;&#32452;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.10977</link><description>&lt;p&gt;
PAGER: &#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#25925;&#38556;&#20998;&#26512;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PAGER: A Framework for Failure Analysis of Deep Regression Models. (arXiv:2309.10977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10977
&lt;/p&gt;
&lt;p&gt;
PAGER&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#25925;&#38556;&#20998;&#26512;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#19968;&#33268;&#20998;&#25968;&#65292;&#23545;&#26679;&#26412;&#36827;&#34892;&#20998;&#32452;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#37096;&#32626;AI&#27169;&#22411;&#38656;&#35201;&#20027;&#21160;&#26816;&#27979;&#28508;&#22312;&#30340;&#39044;&#27979;&#25925;&#38556;&#65292;&#20197;&#38450;&#27490;&#26114;&#36149;&#30340;&#38169;&#35823;&#12290;&#23613;&#31649;&#20998;&#31867;&#38382;&#39064;&#30340;&#25925;&#38556;&#26816;&#27979;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#34920;&#24449;&#25925;&#38556;&#27169;&#24335;&#26356;&#21152;&#22797;&#26434;&#19988;&#36739;&#23569;&#30740;&#31350;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#25110;&#19982;&#35757;&#32451;&#20998;&#24067;&#30340;&#29305;&#24449;&#19981;&#19968;&#33268;&#26469;&#34920;&#24449;&#27169;&#22411;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#20165;&#38752;&#19981;&#30830;&#23450;&#24615;&#26080;&#27861;&#20934;&#30830;&#34920;&#24449;&#25925;&#38556;&#65292;&#36825;&#26159;&#30001;&#20110;&#21508;&#31181;&#35823;&#24046;&#28304;&#30340;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PAGER&#65288;&#22238;&#24402;&#22120;&#30340;&#21407;&#21017;&#24615;&#27867;&#21270;&#38169;&#35823;&#20998;&#26512;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#26816;&#27979;&#21644;&#34920;&#24449;&#28145;&#24230;&#22238;&#24402;&#27169;&#22411;&#25925;&#38556;&#30340;&#26694;&#26550;&#12290;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#28145;&#24230;&#27169;&#22411;&#38170;&#23450;&#24605;&#24819;&#65292;PAGER&#23558;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#21644;&#26032;&#39062;&#30340;&#12289;&#20114;&#34917;&#30340;&#19981;&#19968;&#33268;&#20998;&#25968;&#32479;&#19968;&#36215;&#26469;&#65292;&#23558;&#26679;&#26412;&#32452;&#32455;&#25104;&#19981;&#21516;&#30340;&#39118;&#38505;&#21306;&#22495;&#65292;&#20174;&#32780;&#25552;&#20379;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe deployment of AI models requires proactive detection of potential prediction failures to prevent costly errors. While failure detection in classification problems has received significant attention, characterizing failure modes in regression tasks is more complicated and less explored. Existing approaches rely on epistemic uncertainties or feature inconsistency with the training distribution to characterize model risk. However, we show that uncertainties are necessary but insufficient to accurately characterize failure, owing to the various sources of error. In this paper, we propose PAGER (Principled Analysis of Generalization Errors in Regressors), a framework to systematically detect and characterize failures in deep regression models. Built upon the recently proposed idea of anchoring in deep models, PAGER unifies both epistemic uncertainties and novel, complementary non-conformity scores to organize samples into different risk regimes, thereby providing a comprehensive analys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#19988;&#21487;&#25193;&#23637;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25193;&#23637;&#38543;&#26426;&#23621;&#20013;&#26694;&#26550;&#65292;&#24182;&#25903;&#25345;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#37096;&#20998;&#38543;&#26426;&#24615;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#26657;&#20934;&#30340;&#20449;&#24515;&#25351;&#26631;&#65288;CI&#65289;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.10976</link><description>&lt;p&gt;
&#20934;&#30830;&#19988;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks. (arXiv:2309.10976v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#19988;&#21487;&#25193;&#23637;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25193;&#23637;&#38543;&#26426;&#23621;&#20013;&#26694;&#26550;&#65292;&#24182;&#25903;&#25345;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#37096;&#20998;&#38543;&#26426;&#24615;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#26657;&#20934;&#30340;&#20449;&#24515;&#25351;&#26631;&#65288;CI&#65289;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#23433;&#20840;&#37096;&#32626;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#38656;&#35201;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#30340;&#20449;&#24515;&#25351;&#26631;&#65288;CI&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24050;&#32463;&#30693;&#36947;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;CI&#30340;&#36136;&#37327;&#20250;&#38477;&#20302;&#65292;&#20294;&#23545;&#20110;GNNs&#26469;&#35828;&#36825;&#20010;&#34892;&#20026;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21463;&#25511;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;CI&#26657;&#20934;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#34920;&#36798;&#33021;&#21147;&#25110;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#19981;&#24635;&#26159;&#33021;&#25552;&#39640;CI&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20513;&#20351;&#29992;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#26041;&#27861;&#26469;&#35843;&#33410;CI&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;G-$\Delta$UQ&#65292;&#19968;&#31181;&#26032;&#30340;&#21333;&#27169;&#22411;UQ&#26041;&#27861;&#65292;&#23427;&#25193;&#23637;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#38543;&#26426;&#23621;&#20013;&#26694;&#26550;&#65292;&#20197;&#25903;&#25345;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#37096;&#20998;&#38543;&#26426;&#24615;&#12290;&#22312;&#21327;&#21464;&#37327;&#12289;&#27010;&#24565;&#21644;&#22270;&#22823;&#23567;&#36716;&#31227;&#26041;&#38754;&#30340;&#35780;&#20272;&#20013;&#65292;G-$\Delta$UQ&#19981;&#20165;&#22312;&#33719;&#24471;&#26657;&#20934;&#30340;CI&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20960;&#31181;&#27969;&#34892;&#30340;UQ&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;CI&#38656;&#35201;&#35774;&#35745;&#20026;&#31232;&#32570;&#36164;&#28304;&#26102;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safe deployment of graph neural networks (GNNs) under distribution shift requires models to provide accurate confidence indicators (CI). However, while it is well-known in computer vision that CI quality diminishes under distribution shift, this behavior remains understudied for GNNs. Hence, we begin with a case study on CI calibration under controlled structural and feature distribution shifts and demonstrate that increased expressivity or model size do not always lead to improved CI performance. Consequently, we instead advocate for the use of epistemic uncertainty quantification (UQ) methods to modulate CIs. To this end, we propose G-$\Delta$UQ, a new single model UQ method that extends the recently proposed stochastic centering framework to support structured data and partial stochasticity. Evaluated across covariate, concept, and graph size shifts, G-$\Delta$UQ not only outperforms several popular UQ methods in obtaining calibrated CIs, but also outperforms alternatives when CIs a
&lt;/p&gt;</description></item><item><title>SPFQ&#26159;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#30340;&#24555;&#36895;&#38543;&#26426;&#31639;&#27861;&#65292;&#36890;&#36807;&#36138;&#23146;&#30340;&#36335;&#24452;&#36319;&#36394;&#21644;&#38543;&#26426;&#37327;&#21270;&#22120;&#26377;&#25928;&#22320;&#20943;&#23569;&#32593;&#32476;&#20013;&#30340;&#20887;&#20313;&#24182;&#25552;&#39640;&#37327;&#21270;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#24314;&#31435;&#20102;&#20840;&#32593;&#32476;&#30340;&#35823;&#24046;&#30028;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20110;&#20855;&#26377;&#39640;&#26031;&#26435;&#37325;&#30340;&#22810;&#23618;&#32593;&#32476;&#30340;&#37327;&#21270;&#35777;&#26126;&#20102;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10975</link><description>&lt;p&gt;
SPFQ:&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#30340;&#38543;&#26426;&#31639;&#27861;&#21450;&#20854;&#35823;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
SPFQ: A Stochastic Algorithm and Its Error Analysis for Neural Network Quantization. (arXiv:2309.10975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10975
&lt;/p&gt;
&lt;p&gt;
SPFQ&#26159;&#19968;&#31181;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#30340;&#24555;&#36895;&#38543;&#26426;&#31639;&#27861;&#65292;&#36890;&#36807;&#36138;&#23146;&#30340;&#36335;&#24452;&#36319;&#36394;&#21644;&#38543;&#26426;&#37327;&#21270;&#22120;&#26377;&#25928;&#22320;&#20943;&#23569;&#32593;&#32476;&#20013;&#30340;&#20887;&#20313;&#24182;&#25552;&#39640;&#37327;&#21270;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#24314;&#31435;&#20102;&#20840;&#32593;&#32476;&#30340;&#35823;&#24046;&#30028;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#20110;&#20855;&#26377;&#39640;&#26031;&#26435;&#37325;&#30340;&#22810;&#23618;&#32593;&#32476;&#30340;&#37327;&#21270;&#35777;&#26126;&#20102;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20887;&#20313;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#25216;&#26415;&#24448;&#24448;&#32570;&#20047;&#20840;&#38754;&#30340;&#35823;&#24046;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#37327;&#21270;&#23436;&#20840;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#36138;&#23146;&#30340;&#36335;&#24452;&#36319;&#36394;&#26426;&#21046;&#32467;&#21512;&#38543;&#26426;&#37327;&#21270;&#22120;&#12290;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#21482;&#19982;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#25968;&#37327;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22823;&#22411;&#32593;&#32476;&#30340;&#39640;&#25928;&#37327;&#21270;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#26080;&#31351;&#23383;&#27597;&#26465;&#20214;&#21644;&#26368;&#23567;&#30340;&#26435;&#37325;&#21644;&#36755;&#20837;&#25968;&#25454;&#20551;&#35774;&#19979;&#24314;&#31435;&#20102;&#20840;&#32593;&#32476;&#30340;&#35823;&#24046;&#30028;&#12290;&#20316;&#20026;&#36825;&#19968;&#32467;&#26524;&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#37327;&#21270;&#20855;&#26377;&#39640;&#26031;&#26435;&#37325;&#30340;&#22810;&#23618;&#32593;&#32476;&#26102;&#65292;&#30456;&#23545;&#24179;&#26041;&#37327;&#21270;&#35823;&#24046;e&#30340;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;
Quantization is a widely used compression method that effectively reduces redundancies in over-parameterized neural networks. However, existing quantization techniques for deep neural networks often lack a comprehensive error analysis due to the presence of non-convex loss functions and nonlinear activations. In this paper, we propose a fast stochastic algorithm for quantizing the weights of fully trained neural networks. Our approach leverages a greedy path-following mechanism in combination with a stochastic quantizer. Its computational complexity scales only linearly with the number of weights in the network, thereby enabling the efficient quantization of large networks. Importantly, we establish, for the first time, full-network error bounds, under an infinite alphabet condition and minimal assumptions on the weights and input data. As an application of this result, we prove that when quantizing a multi-layer network having Gaussian weights, the relative square quantization error e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMPART&#30340;&#33258;&#30417;&#30563;&#22810;&#20998;&#36776;&#29575;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#25512;&#26029;&#22270;&#20687;&#30340;&#31895;&#32454;&#21452;&#20998;&#21106;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#22270;&#24418;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#36827;&#34892;&#36793;&#30028;&#32454;&#33410;&#20445;&#30041;&#65292;&#25104;&#21151;&#23558;&#31895;&#31961;&#25513;&#30721;&#35821;&#20041;&#25552;&#21462;&#20026;&#32454;&#31890;&#24230;&#25513;&#30721;&#65292;&#22312;&#26174;&#33879;&#23545;&#35937;&#26816;&#27979;&#21644;&#21333;&#20010;&#23545;&#35937;&#23450;&#20301;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.10972</link><description>&lt;p&gt;
SEMPART: &#33258;&#30417;&#30563;&#22810;&#20998;&#36776;&#29575;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SEMPART: Self-supervised Multi-resolution Partitioning of Image Semantics. (arXiv:2309.10972v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMPART&#30340;&#33258;&#30417;&#30563;&#22810;&#20998;&#36776;&#29575;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#25512;&#26029;&#22270;&#20687;&#30340;&#31895;&#32454;&#21452;&#20998;&#21106;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#22270;&#24418;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#36827;&#34892;&#36793;&#30028;&#32454;&#33410;&#20445;&#30041;&#65292;&#25104;&#21151;&#23558;&#31895;&#31961;&#25513;&#30721;&#35821;&#20041;&#25552;&#21462;&#20026;&#32454;&#31890;&#24230;&#25513;&#30721;&#65292;&#22312;&#26174;&#33879;&#23545;&#35937;&#26816;&#27979;&#21644;&#21333;&#20010;&#23545;&#35937;&#23450;&#20301;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#20934;&#30830;&#30830;&#23450;&#22270;&#20687;&#30340;&#26174;&#33879;&#21306;&#22495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22522;&#20110;DINO&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#26368;&#36817;&#21033;&#29992;&#20102;&#36890;&#36807;&#22522;&#20110;&#34917;&#19969;&#30340;&#29305;&#24449;&#25429;&#25417;&#30340;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;&#35821;&#20041;&#26469;&#23450;&#20301;&#21069;&#26223;&#23545;&#35937;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36824;&#34701;&#21512;&#20102;&#30452;&#35266;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26080;&#30417;&#30563;&#23545;&#35937;&#20998;&#21106;&#26041;&#27861;&#20013;&#20855;&#26377;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SEMPART&#65292;&#23427;&#22312;&#22522;&#20110;DINO&#30340;&#35821;&#20041;&#22270;&#20013;&#32852;&#21512;&#25512;&#26029;&#22270;&#20687;&#30340;&#31895;&#32454;&#21452;&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;SEMPART&#21033;&#29992;&#22522;&#20110;&#22270;&#24418;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#20445;&#30041;&#32454;&#33410;&#36793;&#30028;&#65292;&#24182;&#25104;&#21151;&#23558;&#31895;&#31961;&#25513;&#30721;&#35821;&#20041;&#25552;&#21462;&#20026;&#32454;&#31890;&#24230;&#25513;&#30721;&#12290;&#25105;&#20204;&#30340;&#26174;&#33879;&#23545;&#35937;&#26816;&#27979;&#21644;&#21333;&#20010;&#23545;&#35937;&#23450;&#20301;&#32467;&#26524;&#34920;&#26126;&#65292;SEMPART&#33021;&#22815;&#24555;&#36895;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25513;&#30721;&#65292;&#24182;&#19988;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#31895;&#20998;&#25903;&#21644;&#32454;&#20998;&#25903;&#32780;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately determining salient regions of an image is challenging when labeled data is scarce. DINO-based self-supervised approaches have recently leveraged meaningful image semantics captured by patch-wise features for locating foreground objects. Recent methods have also incorporated intuitive priors and demonstrated value in unsupervised methods for object partitioning. In this paper, we propose SEMPART, which jointly infers coarse and fine bi-partitions over an image's DINO-based semantic graph. Furthermore, SEMPART preserves fine boundary details using graph-driven regularization and successfully distills the coarse mask semantics into the fine mask. Our salient object detection and single object localization findings suggest that SEMPART produces high-quality masks rapidly without additional post-processing and benefits from co-optimizing the coarse and fine branches.
&lt;/p&gt;</description></item><item><title>DPpack&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;R&#21253;&#65292;&#25552;&#20379;&#20102;&#24046;&#20998;&#38544;&#31169;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20855;&#38598;&#21512;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#20445;&#25252;&#26426;&#21046;&#21644;&#25551;&#36848;&#24615;&#32479;&#35745;&#20989;&#25968;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#25252;&#29256;&#26412;&#30340;&#36923;&#36753;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#32447;&#24615;&#22238;&#24402;&#30340;&#23454;&#29616;&#65292;&#20197;&#21450;&#23545;&#27599;&#20010;&#27169;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2309.10965</link><description>&lt;p&gt;
DPpack&#65306;&#19968;&#20010;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;R&#21253;
&lt;/p&gt;
&lt;p&gt;
DPpack: An R Package for Differentially Private Statistical Analysis and Machine Learning. (arXiv:2309.10965v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10965
&lt;/p&gt;
&lt;p&gt;
DPpack&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;R&#21253;&#65292;&#25552;&#20379;&#20102;&#24046;&#20998;&#38544;&#31169;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20855;&#38598;&#21512;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#20445;&#25252;&#26426;&#21046;&#21644;&#25551;&#36848;&#24615;&#32479;&#35745;&#20989;&#25968;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#25252;&#29256;&#26412;&#30340;&#36923;&#36753;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#32447;&#24615;&#22238;&#24402;&#30340;&#23454;&#29616;&#65292;&#20197;&#21450;&#23545;&#27599;&#20010;&#27169;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26159;&#30446;&#21069;&#29992;&#20110;&#20445;&#25252;&#20010;&#20154;&#38544;&#31169;&#30340;&#26368;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#21457;&#24067;&#32858;&#21512;&#32479;&#35745;&#25968;&#25454;&#25110;&#20174;&#25968;&#25454;&#20013;&#26500;&#24314;&#32479;&#35745;/&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#24320;&#28304;&#30340;R&#21253;DPpack&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#24046;&#20998;&#38544;&#31169;&#20998;&#26512;&#24037;&#20855;&#12290;&#24403;&#21069;&#29256;&#26412;&#30340;DPpack&#23454;&#29616;&#20102;&#19977;&#31181;&#24120;&#29992;&#30340;DP&#20445;&#25252;&#26426;&#21046;&#65306;&#25289;&#26222;&#25289;&#26031;&#65292;&#39640;&#26031;&#21644;&#25351;&#25968;&#12290;&#27492;&#22806;&#65292;DPpack&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#38544;&#31169;&#20445;&#25252;&#25551;&#36848;&#24615;&#32479;&#35745;&#20989;&#25968;&#24037;&#20855;&#21253;&#12290;&#20854;&#20013;&#21253;&#25324;&#22343;&#20540;&#12289;&#26041;&#24046;&#12289;&#21327;&#26041;&#24046;&#21644;&#20998;&#20301;&#25968;&#65292;&#20197;&#21450;&#30452;&#26041;&#22270;&#21644;&#21015;&#32852;&#34920;&#12290;&#26368;&#21518;&#65292;DPpack&#25552;&#20379;&#20102;&#22522;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#36923;&#36753;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#32447;&#24615;&#22238;&#24402;&#30340;&#29992;&#25143;&#21451;&#22909;&#23454;&#29616;&#65292;&#20197;&#21450;&#38024;&#23545;&#27599;&#20010;&#27169;&#22411;&#30340;&#24046;&#20998;&#38544;&#31169;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#36825;&#20010;&#22823;&#37327;&#23454;&#29616;&#30340;&#24046;&#20998;&#38544;&#31169;&#32479;&#35745;&#21644;&#27169;&#22411;&#38598;&#21512;&#20801;&#35768;&#26080;&#40635;&#28902;&#22320;&#21033;&#29992;&#24046;&#24322;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy (DP) is the state-of-the-art framework for guaranteeing privacy for individuals when releasing aggregated statistics or building statistical/machine learning models from data. We develop the open-source R package DPpack that provides a large toolkit of differentially private analysis. The current version of DPpack implements three popular mechanisms for ensuring DP: Laplace, Gaussian, and exponential. Beyond that, DPpack provides a large toolkit of easily accessible privacy-preserving descriptive statistics functions. These include mean, variance, covariance, and quantiles, as well as histograms and contingency tables. Finally, DPpack provides user-friendly implementation of privacy-preserving versions of logistic regression, SVM, and linear regression, as well as differentially private hyperparameter tuning for each of these models. This extensive collection of implemented differentially private statistics and models permits hassle-free utilization of differential
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#24847;&#22270;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#24494;&#35843;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#23545;&#20110;&#26377;&#25928;&#21033;&#29992;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.10954</link><description>&lt;p&gt;
&#24102;&#26377;&#22810;&#20010;&#26631;&#31614;&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning for Text Classification with Many Labels. (arXiv:2309.10954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#24847;&#22270;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#24494;&#35843;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#23545;&#20110;&#26377;&#25928;&#21033;&#29992;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20855;&#26377;&#35768;&#22810;&#26631;&#31614;&#30340;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#20351;&#24471;&#22312;&#25552;&#31034;&#20013;&#38590;&#20197;&#36866;&#24212;&#36275;&#22815;&#25968;&#37327;&#30340;&#31034;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#32469;&#36807;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#27599;&#27425;&#25512;&#29702;&#35843;&#29992;&#21482;&#32473;&#27169;&#22411;&#25552;&#20379;&#20102;&#23545;&#23436;&#25972;&#26631;&#31614;&#31354;&#38388;&#30340;&#37096;&#20998;&#35270;&#22270;&#12290;&#22312;&#26368;&#36817;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;(OPT, LLaMA)&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;&#24120;&#35265;&#30340;&#24847;&#22270;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;&#26080;&#38656;&#24494;&#35843;&#21363;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#36229;&#36234;&#20102;&#24494;&#35843;&#24615;&#33021;&#22312;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#25968;&#37327;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20197;&#21450;&#19981;&#21516;&#27169;&#22411;&#35268;&#27169;&#19979;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#23545;&#20110;&#26377;&#25928;&#32780;&#19968;&#33268;&#22320;&#21033;&#29992;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#12290;&#36890;&#36807;&#36816;&#34892;&#20960;&#20010;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27169;&#22411;&#23545;&#20197;&#19979;&#20869;&#23481;&#30340;&#20351;&#29992;&#65306;a)&#19978;&#19979;&#25991;&#31034;&#20363;&#19982;&#24403;&#21069;&#36755;&#20837;&#30340;&#30456;&#20284;&#24230;, b) &#21363;&#26102;&#26597;&#35810;&#35821;&#21477;&#30340;&#30456;&#20284;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) using large language models for tasks with many labels is challenging due to the limited context window, which makes it difficult to fit a sufficient number of examples in the prompt. In this paper, we use a pre-trained dense retrieval model to bypass this limitation, giving the model only a partial view of the full label space for each inference call. Testing with recent open-source LLMs (OPT, LLaMA), we set new state of the art performance in few-shot settings for three common intent classification datasets, with no finetuning. We also surpass fine-tuned performance on fine-grained sentiment classification in certain cases. We analyze the performance across number of in-context examples and different model scales, showing that larger models are necessary to effectively and consistently make use of larger context lengths for ICL. By running several ablations, we analyze the model's use of: a) the similarity of the in-context examples to the current input, b) 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#33539;&#24335;&#19982;&#22343;&#22330;&#20998;&#24067;&#34920;&#31034;&#37197;&#23545;&#65292;&#26469;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#22343;&#22330;&#21338;&#24328;&#21644;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#20174;&#20998;&#24067;&#20013;&#33719;&#21462;&#26679;&#26412;&#12290;&#35813;&#31639;&#27861;&#22312;&#28176;&#36817;&#26080;&#38480;&#26102;&#22495;&#26694;&#26550;&#19979;&#20351;&#29992;&#32447;&#24615;&#20108;&#27425;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.10953</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36830;&#32493;&#31354;&#38388;&#26080;&#38480;&#26102;&#22495;&#22343;&#22330;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Infinite Horizon Mean Field Problems in Continuous Spaces. (arXiv:2309.10953v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10953
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#33539;&#24335;&#19982;&#22343;&#22330;&#20998;&#24067;&#34920;&#31034;&#37197;&#23545;&#65292;&#26469;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#22343;&#22330;&#21338;&#24328;&#21644;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#20174;&#20998;&#24067;&#20013;&#33719;&#21462;&#26679;&#26412;&#12290;&#35813;&#31639;&#27861;&#22312;&#28176;&#36817;&#26080;&#38480;&#26102;&#22495;&#26694;&#26550;&#19979;&#20351;&#29992;&#32447;&#24615;&#20108;&#27425;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#21644;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#21457;&#23637;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;AC&#65289;&#33539;&#24335;&#19982;&#36890;&#36807;&#21442;&#25968;&#21270;&#35780;&#20998;&#20989;&#25968;&#34920;&#31034;&#30340;&#22343;&#22330;&#20998;&#24067;&#37197;&#23545;&#65292;&#21487;&#20197;&#20197;&#22312;&#32447;&#26041;&#24335;&#26377;&#25928;&#22320;&#26356;&#26032;&#65292;&#24182;&#20351;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#20174;&#24471;&#21040;&#30340;&#20998;&#24067;&#20013;&#33719;&#24471;&#26679;&#26412;&#12290;AC&#20195;&#29702;&#21644;&#35780;&#20998;&#20989;&#25968;&#25353;&#36845;&#20195;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#65292;&#20197;&#25910;&#25947;&#21040;&#32473;&#23450;&#22343;&#22330;&#38382;&#39064;&#30340;MFG&#24179;&#34913;&#25110;MFC&#26368;&#20248;&#35299;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#23398;&#20064;&#29575;&#30340;&#36873;&#25321;&#12290;&#31639;&#27861;&#30340;&#31616;&#21333;&#20462;&#25913;&#20351;&#25105;&#20204;&#33021;&#22815;&#35299;&#20915;&#28151;&#21512;&#22343;&#22330;&#25511;&#21046;&#21338;&#24328;&#65288;MFCG&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#28176;&#36817;&#26080;&#38480;&#26102;&#22495;&#26694;&#26550;&#20013;&#30340;&#32447;&#24615;&#20108;&#27425;&#22522;&#20934;&#35780;&#20272;&#25105;&#20204;&#30340;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the development and analysis of a reinforcement learning (RL) algorithm designed to solve continuous-space mean field game (MFG) and mean field control (MFC) problems in a unified manner. The proposed approach pairs the actor-critic (AC) paradigm with a representation of the mean field distribution via a parameterized score function, which can be efficiently updated in an online fashion, and uses Langevin dynamics to obtain samples from the resulting distribution. The AC agent and the score function are updated iteratively to converge, either to the MFG equilibrium or the MFC optimum for a given mean field problem, depending on the choice of learning rates. A straightforward modification of the algorithm allows us to solve mixed mean field control games (MFCGs). The performance of our algorithm is evaluated using linear-quadratic benchmarks in the asymptotic infinite horizon framework.
&lt;/p&gt;</description></item><item><title>LMDX&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#24067;&#23616;&#32534;&#30721;&#21644;&#31572;&#26696;&#34394;&#26500;&#30340;&#22256;&#38590;&#65292;&#33021;&#22815;&#22312;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.10952</link><description>&lt;p&gt;
LMDX&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
LMDX: Language Model-based Document Information Extraction and Localization. (arXiv:2309.10952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10952
&lt;/p&gt;
&lt;p&gt;
LMDX&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#24067;&#23616;&#32534;&#30721;&#21644;&#31572;&#26696;&#34394;&#26500;&#30340;&#22256;&#38590;&#65292;&#33021;&#22815;&#22312;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;&#35768;&#22810;&#29616;&#26377;&#20219;&#21153;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#20852;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLM&#23578;&#26410;&#25104;&#21151;&#24212;&#29992;&#20110;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#65292;&#36825;&#26159;&#35768;&#22810;&#25991;&#26723;&#22788;&#29702;&#24037;&#20316;&#27969;&#30340;&#26680;&#24515;&#65292;&#21253;&#25324;&#20174;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#65288;VRD&#65289;&#20013;&#25552;&#21462;&#20851;&#38190;&#23454;&#20307;&#65292;&#32473;&#23450;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#27169;&#24335;&#12290;LLM&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;LLM&#20013;&#32570;&#20047;&#24067;&#23616;&#32534;&#30721;&#65292;&#36825;&#23545;&#20110;&#39640;&#36136;&#37327;&#30340;&#25552;&#21462;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21450;&#32570;&#20047;&#19968;&#20010;&#22522;&#20110;&#29702;&#35770;&#30340;&#26426;&#21046;&#65292;&#30830;&#20445;&#31572;&#26696;&#19981;&#26159;&#34394;&#26500;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#19982;&#23450;&#20301;&#65288;LMDX&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#20219;&#24847;LLM&#36866;&#24212;&#25991;&#26723;&#20449;&#24687;&#25552;&#21462;&#12290;LMDX&#21487;&#20197;&#25552;&#21462;&#21333;&#19968;&#12289;&#37325;&#22797;&#21644;&#23618;&#27425;&#32467;&#26500;&#23454;&#20307;&#65292;&#26080;&#35770;&#26159;&#21542;&#26377;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#22522;&#20110;&#29702;&#35770;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) have revolutionized Natural Language Processing (NLP), improving state-of-the-art on many existing tasks and exhibiting emergent capabilities. However, LLMs have not yet been successfully applied on semi-structured document information extraction, which is at the core of many document processing workflows and consists of extracting key entities from a visually rich document (VRD) given a predefined target schema. The main obstacles to LLM adoption in that task have been the absence of layout encoding within LLMs, critical for a high quality extraction, and the lack of a grounding mechanism ensuring the answer is not hallucinated. In this paper, we introduce Language Model-based Document Information Extraction and Localization (LMDX), a methodology to adapt arbitrary LLMs for document information extraction. LMDX can do extraction of singular, repeated, and hierarchical entities, both with and without training data, while providing grounding guarantees and lo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#36895;&#24230;&#21521;&#37327;&#22330;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#36712;&#36857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36895;&#24230;&#21521;&#37327;&#22330;&#30340;&#21152;&#20837;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10948</link><description>&lt;p&gt;
&#20351;&#29992;&#36895;&#24230;&#21521;&#37327;&#22330;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#30340;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Novel Deep Neural Network for Trajectory Prediction in Automated Vehicles Using Velocity Vector Field. (arXiv:2309.10948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#36895;&#24230;&#21521;&#37327;&#22330;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#39044;&#27979;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#36712;&#36857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36895;&#24230;&#21521;&#37327;&#22330;&#30340;&#21152;&#20837;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20854;&#20182;&#36947;&#36335;&#29992;&#25143;&#30340;&#36816;&#21160;&#36827;&#34892;&#39044;&#27979;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#23454;&#29616;&#23433;&#20840;&#21644;&#26126;&#26234;&#30340;&#19979;&#28216;&#20915;&#31574;&#21644;&#36816;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;&#23398;&#20064;&#30340;&#36816;&#21160;&#39044;&#27979;&#26041;&#27861;&#22312;&#39044;&#27979;&#33539;&#22260;&#22686;&#21152;&#25110;&#35266;&#27979;&#31383;&#21475;&#20943;&#23567;&#26102;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36712;&#36857;&#39044;&#27979;&#25216;&#26415;&#65292;&#23427;&#23558;&#20197;&#27969;&#20307;&#27969;&#21160;&#21160;&#21147;&#23398;&#20026;&#28789;&#24863;&#30340;&#36895;&#24230;&#21521;&#37327;&#22330;&#19982;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#23558;&#21521;&#37327;&#22330;&#20316;&#20026;&#21367;&#31215;-&#24490;&#29615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39069;&#22806;&#36755;&#20837;&#65292;&#24110;&#21161;&#39044;&#27979;&#32473;&#23450;&#19968;&#31995;&#21015;&#40479;&#30640;&#22330;&#26223;&#34920;&#31034;&#30340;&#26368;&#21487;&#33021;&#30340;&#26410;&#26469;&#36712;&#36857;&#12290;&#35813;&#27169;&#22411;&#22312;HighD&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#36895;&#24230;&#21521;&#37327;&#22330;&#30340;&#21152;&#20837;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#26080;&#35770;&#26159;&#30701;&#36824;&#26159;&#38271;&#30340;&#39044;&#27979;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipating the motion of other road users is crucial for automated driving systems (ADS), as it enables safe and informed downstream decision-making and motion planning. Unfortunately, contemporary learning-based approaches for motion prediction exhibit significant performance degradation as the prediction horizon increases or the observation window decreases. This paper proposes a novel technique for trajectory prediction that combines a data-driven learning-based method with a velocity vector field (VVF) generated from a nature-inspired concept, i.e., fluid flow dynamics. In this work, the vector field is incorporated as an additional input to a convolutional-recurrent deep neural network to help predict the most likely future trajectories given a sequence of bird's eye view scene representations. The performance of the proposed model is compared with state-of-the-art methods on the HighD dataset demonstrating that the VVF inclusion improves the prediction accuracy for both short a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#22312;&#35821;&#38899;&#24212;&#29992;&#20013;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;BitFit&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#21482;&#32771;&#34385;&#20559;&#32622;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.10930</link><description>&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#29992;&#20110;&#35821;&#38899;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Test-Time Training for Speech. (arXiv:2309.10930v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#22312;&#35821;&#38899;&#24212;&#29992;&#20013;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;BitFit&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#21482;&#32771;&#34385;&#20559;&#32622;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#65292;&#35299;&#20915;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#65288;TTT&#65289;&#22312;&#22788;&#29702;&#35821;&#38899;&#24212;&#29992;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;&#20998;&#24067;&#20559;&#31227;&#24341;&#20837;&#21040;&#26631;&#20934;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#65292;&#20363;&#22914;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#24773;&#32490;&#26816;&#27979;&#65292;&#24182;&#25506;&#35752;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#22914;&#20309;&#24110;&#21161;&#35843;&#25972;&#21040;&#36825;&#20123;&#20998;&#24067;&#20559;&#31227;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#30001;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#30340;&#33258;&#28982;&#21464;&#21270;&#65288;&#22914;&#24615;&#21035;&#21644;&#24180;&#40836;&#65289;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#30340;&#19968;&#20123;&#20851;&#38190;&#25361;&#25112;&#65292;&#21253;&#25324;&#23545;&#20248;&#21270;&#36229;&#21442;&#25968;&#65288;&#20363;&#22914;&#20248;&#21270;&#27493;&#39588;&#30340;&#25968;&#37327;&#21644;&#36873;&#25321;&#29992;&#20110;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#30340;&#21442;&#25968;&#23376;&#38598;&#65289;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#21487;&#25193;&#23637;&#24615;&#65288;&#20363;&#22914;&#65292;&#30001;&#20110;&#27599;&#20010;&#26679;&#26412;&#37117;&#26377;&#33258;&#24049;&#30340;&#19968;&#32452;&#21442;&#25968;&#65292;&#27979;&#35797;&#26102;&#38388;&#35757;&#32451;&#19981;&#21487;&#25193;&#23637;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;BitFit&#8212;&#8212;&#19968;&#31181;&#20165;&#32771;&#34385;&#20559;&#32622;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#25991;&#26412;&#24212;&#29992;&#20013;&#25552;&#20986;&#30340;&#24494;&#35843;&#31639;&#27861;&#8212;&#8212;&#20316;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the application of Test-Time Training (TTT) as a solution to handling distribution shifts in speech applications. In particular, we introduce distribution-shifts to the test datasets of standard speech-classification tasks -- for example, speaker-identification and emotion-detection -- and explore how Test-Time Training (TTT) can help adjust to the distribution-shift. In our experiments that include distribution shifts due to background noise and natural variations in speech such as gender and age, we identify some key-challenges with TTT including sensitivity to optimization hyperparameters (e.g., number of optimization steps and subset of parameters chosen for TTT) and scalability (e.g., as each example gets its own set of parameters, TTT is not scalable). Finally, we propose using BitFit -- a parameter-efficient fine-tuning algorithm proposed for text applications that only considers the bias parameters for fine-tuning -- as a solution to the aforementioned c
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;&#24179;&#21488;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36229;&#23548;&#20307;&#23454;&#39564;&#25968;&#25454;&#12290;&#35813;&#24179;&#21488;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#31243;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26356;&#26032;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#12290;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#35813;&#20998;&#21306;&#24179;&#21488;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#31649;&#29702;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10923</link><description>&lt;p&gt;
&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;: &#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#39640;&#36136;&#37327;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Semi-automatic staging area for high-quality structured data extraction from scientific literature. (arXiv:2309.10923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10923
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;&#24179;&#21488;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#36229;&#23548;&#20307;&#23454;&#39564;&#25968;&#25454;&#12290;&#35813;&#24179;&#21488;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#31243;&#30340;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26356;&#26032;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#25968;&#25454;&#36136;&#37327;&#12290;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#35813;&#20998;&#21306;&#24179;&#21488;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#31649;&#29702;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20174;&#31185;&#23398;&#25991;&#31456;&#20013;&#37319;&#38598;&#36229;&#23548;&#20307;&#23454;&#39564;&#25968;&#25454;&#30340; SuperCon &#25968;&#25454;&#24211;&#30340;&#20998;&#21306;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#26356;&#26032; SuperCon &#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#31243;&#32452;&#25104;&#30340;&#24037;&#20316;&#27969;&#39537;&#21160;&#30340;&#21322;&#33258;&#21160;&#21270;&#20998;&#21306;&#24179;&#21488;&#65292;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#25968;&#25454;&#24211;&#20013;&#23545;&#25968;&#25454;&#36827;&#34892;&#26657;&#39564;&#21644;&#32416;&#38169;&#12290;&#24322;&#24120;&#26816;&#27979;&#33258;&#21160;&#36807;&#31243;&#29992;&#20110;&#39044;&#20808;&#31579;&#36873;&#37319;&#38598;&#21040;&#30340;&#25968;&#25454;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#23450;&#21046;&#30340;&#29992;&#25143;&#30028;&#38754;&#22312;&#21407;&#22987; PDF &#25991;&#26723;&#19978;&#36827;&#34892;&#25968;&#25454;&#39564;&#35777;&#21644;&#32416;&#38169;&#12290;&#27492;&#22806;&#65292;&#24403;&#35760;&#24405;&#34987;&#32416;&#38169;&#26102;&#65292;&#20854;&#21407;&#22987;&#25968;&#25454;&#34987;&#25910;&#38598;&#24182;&#29992;&#20110;&#25913;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#20998;&#21306;&#24179;&#21488;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#31649;&#29702;&#36136;&#37327;&#12290;&#25105;&#20204;&#23558;&#30028;&#38754;&#19982;&#20256;&#32479;&#30340;&#25163;&#21160;&#38405;&#35835; PDF &#25991;&#26723;&#24182;&#22312; Excel &#25991;&#26723;&#20013;&#35760;&#24405;&#20449;&#24687;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a staging area for ingesting new superconductors' experimental data in SuperCon that is machine-collected from scientific articles. Our objective is to enhance the efficiency of updating SuperCon while maintaining or enhancing the data quality. We present a semi-automatic staging area driven by a workflow combining automatic and manual processes on the extracted database. An anomaly detection automatic process aims to pre-screen the collected data. Users can then manually correct any errors through a user interface tailored to simplify the data verification on the original PDF documents. Additionally, when a record is corrected, its raw data is collected and utilised to improve machine learning models as training data. Evaluation experiments demonstrate that our staging area significantly improves curation quality. We compare the interface with the traditional manual approach of reading PDF documents and recording information in an Excel document. Using the in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23450;&#20041;&#22312;&#32039;&#33268;Riemannian&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;Matern&#39640;&#26031;&#36807;&#31243;&#21644;&#22806;&#22312;&#36807;&#31243;&#20043;&#38388;&#30340;&#25910;&#32553;&#36895;&#29575;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#36895;&#29575;&#22312;&#36866;&#24403;&#21305;&#37197;&#24179;&#28369;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26159;&#30456;&#31561;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.10918</link><description>&lt;p&gt;
Riemannian&#27969;&#24418;&#19978;Matern&#39640;&#26031;&#36807;&#31243;&#30340;&#21518;&#39564;&#25910;&#32553;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Posterior Contraction Rates for Mat\'ern Gaussian Processes on Riemannian Manifolds. (arXiv:2309.10918v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23450;&#20041;&#22312;&#32039;&#33268;Riemannian&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;Matern&#39640;&#26031;&#36807;&#31243;&#21644;&#22806;&#22312;&#36807;&#31243;&#20043;&#38388;&#30340;&#25910;&#32553;&#36895;&#29575;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#36895;&#29575;&#22312;&#36866;&#24403;&#21305;&#37197;&#24179;&#28369;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#26159;&#30456;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22312;&#35768;&#22810;&#20381;&#36182;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#34987;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#22312;&#20960;&#20309;&#35774;&#32622;&#19979;&#22788;&#29702;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#24037;&#20855;&#65292;&#20363;&#22914;&#65292;&#24403;&#36755;&#20837;&#20301;&#20110;Riemannian&#27969;&#24418;&#19978;&#26102;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#36825;&#20123;&#20869;&#22312;&#27169;&#22411;&#22312;&#29702;&#35770;&#19978;&#26159;&#21542;&#21487;&#20197;&#35777;&#26126;&#30456;&#27604;&#20110;&#23558;&#25152;&#26377;&#30456;&#20851;&#37327;&#23884;&#20837;&#21040;$\mathbb{R}^d$&#24182;&#20351;&#29992;&#26222;&#36890;&#27431;&#20960;&#37324;&#24503;&#39640;&#26031;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23450;&#20041;&#22312;&#32039;&#33268;Riemannian&#27969;&#24418;&#19978;&#30340;&#20869;&#22312;Matern&#39640;&#26031;&#36807;&#31243;&#30340;&#26368;&#20248;&#25910;&#32553;&#36895;&#29575;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#27969;&#24418;&#21644;&#29615;&#22659;Sobolev&#31354;&#38388;&#20043;&#38388;&#30340;&#36857;&#21644;&#25193;&#23637;&#23450;&#29702;&#35777;&#26126;&#20102;&#22806;&#22312;&#36807;&#31243;&#30340;&#31867;&#20284;&#36895;&#29575;&#65306;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25152;&#24471;&#21040;&#30340;&#36895;&#29575;&#19982;&#20869;&#22312;&#36807;&#31243;&#30340;&#36895;&#29575;&#30456;&#31526;&#65292;&#21069;&#25552;&#26159;&#23427;&#20204;&#30340;&#24179;&#28369;&#21442;&#25968;&#36866;&#24403;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#19968;&#20123;&#23454;&#35777;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23545;&#36825;&#20123;&#36895;&#29575;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are used in many machine learning applications that rely on uncertainty quantification. Recently, computational tools for working with these models in geometric settings, such as when inputs lie on a Riemannian manifold, have been developed. This raises the question: can these intrinsic models be shown theoretically to lead to better performance, compared to simply embedding all relevant quantities into $\mathbb{R}^d$ and using the restriction of an ordinary Euclidean Gaussian process? To study this, we prove optimal contraction rates for intrinsic Mat\'ern Gaussian processes defined on compact Riemannian manifolds. We also prove analogous rates for extrinsic processes using trace and extension theorems between manifold and ambient Sobolev spaces: somewhat surprisingly, the rates obtained turn out to coincide with those of the intrinsic processes, provided that their smoothness parameters are matched appropriately. We illustrate these rates empirically on a number of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#25552;&#20379;&#38899;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#19978;&#19979;&#25991;&#65292;&#20351;&#31995;&#32479;&#38544;&#21547;&#22320;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2309.10917</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#19978;&#19979;&#25991;&#21270;
&lt;/p&gt;
&lt;p&gt;
End-to-End Speech Recognition Contextualization with Large Language Models. (arXiv:2309.10917v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#25552;&#20379;&#38899;&#39057;&#29305;&#24449;&#21644;&#25991;&#26412;&#19978;&#19979;&#25991;&#65292;&#20351;&#31995;&#32479;&#38544;&#21547;&#22320;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#32780;&#21463;&#21040;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#34701;&#20837;LLMs&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35821;&#38899;&#35782;&#21035;&#35270;&#20026;&#22522;&#20110;&#39044;&#35757;&#32451;LLM&#30340;&#28151;&#21512;&#27169;&#24577;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#38899;&#39057;&#29305;&#24449;&#20197;&#21450;&#21487;&#36873;&#30340;&#25991;&#26412;&#26631;&#35760;&#26469;&#35757;&#32451;&#31995;&#32479;&#20197;&#35299;&#30721;&#26041;&#24335;&#23436;&#25104;&#36716;&#24405;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#31995;&#32479;&#20250;&#38544;&#21547;&#22320;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#38750;&#32467;&#26500;&#21270;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65292;&#24403;&#25552;&#20379;&#39069;&#22806;&#30340;&#25991;&#26412;&#19978;&#19979;&#25991;&#26102;&#65292;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#20102;6%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31454;&#20105;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#22312;&#25972;&#20307;&#19978;&#23558;WER&#25552;&#39640;&#20102;7.5%&#65292;&#23545;&#20110;&#32597;&#35265;&#35789;&#35821;&#30340;WER&#25552;&#39640;&#20102;17%&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#19978;&#19979;&#25991;&#21270;RNN-T&#31995;&#32479;&#30340;&#35757;&#32451;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Large Language Models (LLMs) have garnered significant attention from the research community due to their exceptional performance and generalization capabilities. In this paper, we introduce a novel method for contextualizing speech recognition models incorporating LLMs. Our approach casts speech recognition as a mixed-modal language modeling task based on a pretrained LLM. We provide audio features, along with optional text tokens for context, to train the system to complete transcriptions in a decoder-only fashion. As a result, the system is implicitly incentivized to learn how to leverage unstructured contextual information during training. Our empirical results demonstrate a significant improvement in performance, with a 6% WER reduction when additional textual context is provided. Moreover, we find that our method performs competitively and improve by 7.5% WER overall and 17% WER on rare words against a baseline contextualized RNN-T system that has been trained on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#25216;&#26415;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.10916</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#21644;&#24433;&#21709;&#20989;&#25968;&#65292;&#25105;&#20204;&#33021;&#20174;&#23545;&#25239;&#26679;&#26412;&#20013;&#33719;&#24471;&#20160;&#20040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#25216;&#26415;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#26159;&#36890;&#36807;&#24494;&#23567;&#25200;&#21160;&#26469;&#27450;&#39575;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#65292;&#36215;&#21021;&#22312;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#30740;&#31350;&#65292;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20063;&#24320;&#22987;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26816;&#27979;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36755;&#20837;&#25200;&#21160;&#30340;&#25628;&#32034;&#65292;&#20294;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#24050;&#32463;&#21457;&#23637;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#34920;&#24449;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#12290;&#26412;&#25991;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#65292;&#19968;&#31181;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#12290;&#29305;&#21035;&#26159;&#21069;&#32773;&#30456;&#27604;&#20960;&#20010;&#24378;&#22522;&#20934;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65307;&#27492;&#22806;&#65292;&#23545;&#24433;&#21709;&#20989;&#25968;&#30340;&#26032;&#39062;&#20351;&#29992;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#26681;&#25454;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples, deliberately crafted using small perturbations to fool deep neural networks, were first studied in image processing and more recently in NLP. While approaches to detecting adversarial examples in NLP have largely relied on search over input perturbations, image processing has seen a range of techniques that aim to characterise adversarial subspaces over the learned representations.  In this paper, we adapt two such approaches to NLP, one based on nearest neighbors and influence functions and one on Mahalanobis distances. The former in particular produces a state-of-the-art detector when compared against several strong baselines; moreover, the novel use of influence functions provides insight into how the nature of adversarial example subspaces in NLP relate to those in image processing, and also how they differ depending on the kind of NLP task.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#36328;&#25968;&#25454;&#38598;&#36801;&#31227;&#23398;&#20064;&#26469;&#25918;&#22823;&#33041;&#30005;&#20449;&#21495;&#36890;&#36335;&#20013;&#30340;&#30149;&#29702;&#26816;&#27979;&#65292;&#20197;&#25552;&#39640;&#30149;&#29702;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#35299;&#20915;&#20102;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#20302;&#25104;&#26412;&#25307;&#21215;&#30495;&#23454;&#24739;&#32773;&#38431;&#21015;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10910</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#25968;&#25454;&#38598;&#36801;&#31227;&#23398;&#20064;&#26469;&#25918;&#22823;&#33041;&#30005;&#20449;&#21495;&#36890;&#36335;&#20013;&#30340;&#30149;&#29702;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Amplifying Pathological Detection in EEG Signaling Pathways through Cross-Dataset Transfer Learning. (arXiv:2309.10910v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10910
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#36328;&#25968;&#25454;&#38598;&#36801;&#31227;&#23398;&#20064;&#26469;&#25918;&#22823;&#33041;&#30005;&#20449;&#21495;&#36890;&#36335;&#20013;&#30340;&#30149;&#29702;&#26816;&#27979;&#65292;&#20197;&#25552;&#39640;&#30149;&#29702;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#35299;&#20915;&#20102;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#20302;&#25104;&#26412;&#25307;&#21215;&#30495;&#23454;&#24739;&#32773;&#38431;&#21015;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#21644;&#33041;&#27963;&#21160;&#35299;&#30721;&#30340;&#30149;&#29702;&#35786;&#26029;&#22312;&#29702;&#35299;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20934;&#30830;&#30340;&#25968;&#25454;&#39537;&#21160;&#35786;&#26029;&#21644;&#26377;&#25928;&#30340;&#27835;&#30103;&#30340;&#28508;&#21147;&#26174;&#33879;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#22312;&#22810;&#20010;&#23618;&#38754;&#19978;&#37117;&#23384;&#22312;&#21508;&#31181;&#25361;&#25112;&#12290;&#26631;&#27880;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#33539;&#22260;&#24773;&#26223;&#19979;&#65292;&#30001;&#20110;&#39640;&#26114;&#30340;&#25307;&#21215;&#25104;&#26412;&#23548;&#33268;&#30495;&#23454;&#24739;&#32773;&#38431;&#21015;&#26377;&#38480;&#65292;&#20984;&#26174;&#20102;&#25193;&#23637;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;&#30149;&#29702;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#20984;&#26174;&#25968;&#25454;&#21644;&#27169;&#22411;&#25193;&#23637;&#20197;&#21450;&#36328;&#25968;&#25454;&#38598;&#30693;&#35782;&#36801;&#31227;&#30340;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36890;&#36807;&#25968;&#25454;&#25193;&#23637;&#21487;&#20197;&#33719;&#24471;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#34920;&#26126;&#38656;&#35201;&#36827;&#34892;&#20180;&#32454;&#35780;&#20272;&#21644;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pathology diagnosis based on EEG signals and decoding brain activity holds immense importance in understanding neurological disorders. With the advancement of artificial intelligence methods and machine learning techniques, the potential for accurate data-driven diagnoses and effective treatments has grown significantly. However, applying machine learning algorithms to real-world datasets presents diverse challenges at multiple levels. The scarcity of labelled data, especially in low regime scenarios with limited availability of real patient cohorts due to high costs of recruitment, underscores the vital deployment of scaling and transfer learning techniques. In this study, we explore a real-world pathology classification task to highlight the effectiveness of data and model scaling and cross-dataset knowledge transfer. As such, we observe varying performance improvements through data scaling, indicating the need for careful evaluation and labelling. Additionally, we identify the chall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SALT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20195;&#30721;&#20999;&#25442;&#21644;&#23884;&#20837;&#28151;&#21512;&#19982;&#33258;&#25105;&#22686;&#24378;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10891</link><description>&lt;p&gt;
&#33258;&#25105;&#22686;&#24378;&#25913;&#36827;&#20102;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Self-Augmentation Improves Zero-Shot Cross-Lingual Transfer. (arXiv:2309.10891v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SALT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20195;&#30721;&#20999;&#25442;&#21644;&#23884;&#20837;&#28151;&#21512;&#19982;&#33258;&#25105;&#22686;&#24378;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#26159;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26680;&#24515;&#20219;&#21153;&#65292;&#20801;&#35768;&#22312;&#20855;&#26377;&#26356;&#20805;&#36275;&#35757;&#32451;&#36164;&#28304;&#30340;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#25512;&#24191;&#21040;&#20854;&#20182;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#20808;&#21069;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#21162;&#21147;&#20351;&#29992;&#24179;&#34892;&#35821;&#26009;&#24211;&#12289;&#21452;&#35821;&#35789;&#20856;&#25110;&#20854;&#20182;&#26631;&#27880;&#23545;&#40784;&#25968;&#25454;&#26469;&#25552;&#39640;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#65292;&#36825;&#20123;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#33719;&#21462;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;SALT&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#65292;&#32780;&#19981;&#38656;&#35201;&#36825;&#20123;&#22806;&#37096;&#25968;&#25454;&#30340;&#24110;&#21161;&#12290;&#36890;&#36807;&#32467;&#21512;&#20195;&#30721;&#20999;&#25442;&#21644;&#23884;&#20837;&#28151;&#21512;&#19982;&#33258;&#25105;&#22686;&#24378;&#65292;SALT&#26377;&#25928;&#22320;&#33976;&#39311;&#20102;&#22810;&#35821;&#35328;PLM&#30340;&#36328;&#35821;&#35328;&#30693;&#35782;&#65292;&#24182;&#22686;&#24378;&#20102;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;&#22312;XNLI&#21644;PAWS-X&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#22806;&#37096;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/luka-group/SALT&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-lingual transfer is a central task in multilingual NLP, allowing models trained in languages with more sufficient training resources to generalize to other low-resource languages. Earlier efforts on this task use parallel corpora, bilingual dictionaries, or other annotated alignment data to improve cross-lingual transferability, which are typically expensive to obtain. In this paper, we propose a simple yet effective method, SALT, to improve the zero-shot cross-lingual transfer of the multilingual pretrained language models without the help of such external data. By incorporating code-switching and embedding mixup with self-augmentation, SALT effectively distills cross-lingual knowledge from the multilingual PLM and enhances its transferability on downstream tasks. Experimental results on XNLI and PAWS-X show that our method is able to improve zero-shot cross-lingual transferability without external data. Our code is available at https://github.com/luka-group/SALT.
&lt;/p&gt;</description></item><item><title>Crypto'Graph&#26159;&#19968;&#31181;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#22270;&#24418;&#38142;&#36335;&#39044;&#27979;&#21327;&#35758;&#65292;&#22312;&#19981;&#36879;&#38706;&#21508;&#26041;&#31169;&#26377;&#22270;&#24418;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#20801;&#35768;&#21442;&#19982;&#26041;&#25512;&#26029;&#26410;&#26469;&#26032;&#38142;&#36335;&#30340;&#24418;&#25104;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10890</link><description>&lt;p&gt;
Crypto'Graph&#65306;&#21033;&#29992;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#38142;&#36335;&#39044;&#27979;&#23454;&#29616;&#24378;&#22823;&#30340;&#22270;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Crypto'Graph: Leveraging Privacy-Preserving Distributed Link Prediction for Robust Graph Learning. (arXiv:2309.10890v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10890
&lt;/p&gt;
&lt;p&gt;
Crypto'Graph&#26159;&#19968;&#31181;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#22270;&#24418;&#38142;&#36335;&#39044;&#27979;&#21327;&#35758;&#65292;&#22312;&#19981;&#36879;&#38706;&#21508;&#26041;&#31169;&#26377;&#22270;&#24418;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#20801;&#35768;&#21442;&#19982;&#26041;&#25512;&#26029;&#26410;&#26469;&#26032;&#38142;&#36335;&#30340;&#24418;&#25104;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25910;&#38598;&#21644;&#20998;&#26512;&#20851;&#31995;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#22270;&#24418;&#32467;&#26500;&#20998;&#24067;&#22312;&#22810;&#20010;&#21442;&#19982;&#26041;&#20043;&#38388;&#26102;&#65292;&#20854;&#20998;&#26512;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29305;&#21035;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#65292;&#27599;&#20010;&#21442;&#19982;&#26041;&#21487;&#33021;&#24076;&#26395;&#20445;&#25345;&#20854;&#23545;&#22270;&#24418;&#30340;&#37096;&#20998;&#30693;&#35782;&#31169;&#26377;&#65292;&#21516;&#26102;&#20173;&#24895;&#24847;&#19982;&#20854;&#20182;&#21442;&#19982;&#26041;&#21512;&#20316;&#36827;&#34892;&#30456;&#20114;&#21033;&#30410;&#30340;&#20219;&#21153;&#65292;&#22914;&#25968;&#25454;&#25972;&#29702;&#25110;&#27602;&#25968;&#25454;&#21024;&#38500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Crypto'Graph&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#22270;&#24418;&#38142;&#36335;&#39044;&#27979;&#30340;&#39640;&#25928;&#21327;&#35758;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#23427;&#20801;&#35768;&#37096;&#20998;&#20849;&#20139;&#20855;&#26377;&#20998;&#24067;&#24335;&#38142;&#36335;&#30340;&#22270;&#24418;&#30340;&#21442;&#19982;&#26041;&#25512;&#26029;&#26410;&#26469;&#26032;&#38142;&#36335;&#30340;&#24418;&#25104;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21152;&#23494;&#21407;&#35821;&#65292;Crypto'Graph&#33021;&#22815;&#22312;&#19981;&#36879;&#38706;&#27599;&#20010;&#21442;&#19982;&#26041;&#30340;&#31169;&#26377;&#20010;&#20154;&#22270;&#24418;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#32852;&#21512;&#32593;&#32476;&#19978;&#35745;&#31639;&#36825;&#20123;&#26032;&#38142;&#36335;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#20182;&#20204;&#30693;&#36947;&#21442;&#19982;&#26041;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs are a widely used data structure for collecting and analyzing relational data. However, when the graph structure is distributed across several parties, its analysis is particularly challenging. In particular, due to the sensitivity of the data each party might want to keep their partial knowledge of the graph private, while still willing to collaborate with the other parties for tasks of mutual benefit, such as data curation or the removal of poisoned data. To address this challenge, we propose Crypto'Graph, an efficient protocol for privacy-preserving link prediction on distributed graphs. More precisely, it allows parties partially sharing a graph with distributed links to infer the likelihood of formation of new links in the future. Through the use of cryptographic primitives, Crypto'Graph is able to compute the likelihood of these new links on the joint network without revealing the structure of the private individual graph of each party, even though they know the number of 
&lt;/p&gt;</description></item><item><title>DeepliteRT&#26159;&#19968;&#20010;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36229;&#20302;&#20301;&#37327;&#21270;&#25216;&#26415;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#36816;&#31639;&#65292;&#24182;&#36890;&#36807;&#32534;&#35793;&#22120;&#36716;&#25442;&#24037;&#20316;&#31616;&#21270;&#20102;&#37327;&#21270;&#27169;&#22411;&#30340;&#37096;&#32626;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.10878</link><description>&lt;p&gt;
DeepliteRT&#65306;&#36793;&#32536;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DeepliteRT: Computer Vision at the Edge. (arXiv:2309.10878v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10878
&lt;/p&gt;
&lt;p&gt;
DeepliteRT&#26159;&#19968;&#20010;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36229;&#20302;&#20301;&#37327;&#21270;&#25216;&#26415;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#36816;&#31639;&#65292;&#24182;&#36890;&#36807;&#32534;&#35793;&#22120;&#36716;&#25442;&#24037;&#20316;&#31616;&#21270;&#20102;&#37327;&#21270;&#27169;&#22411;&#30340;&#37096;&#32626;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35774;&#22791;&#30340;&#26222;&#21450;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37096;&#32626;&#24320;&#36767;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#30005;&#21147;&#12289;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#32780;&#36825;&#20123;&#36164;&#28304;&#36890;&#24120;&#22312;&#36793;&#32536;&#24179;&#21488;&#19978;&#19981;&#21487;&#29992;&#12290;&#36229;&#20302;&#20301;&#37327;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#26435;&#37325;&#21644;&#28608;&#27963;&#20174;32&#20301;&#32553;&#20943;&#21040;&#23567;&#20110;8&#20301;&#26469;&#38477;&#20302;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;ARM&#26550;&#26500;&#30340;&#30446;&#26631;&#19978;&#23454;&#29616;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;&#36229;&#20302;&#20301;&#21367;&#31215;&#36816;&#31639;&#31526;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#26368;&#22810;4.34&#20493;&#12290;&#25105;&#20204;&#30340;&#36816;&#31639;&#31526;&#26159;&#22312;Deeplite Runtime&#65288;DeepliteRT&#65289;&#20013;&#23454;&#29616;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;ARM&#35774;&#22791;&#19978;&#32534;&#35793;&#12289;&#35843;&#20248;&#21644;&#25512;&#26029;&#36229;&#20302;&#20301;&#27169;&#22411;&#12290;&#22312;DeepliteRT&#20013;&#30340;&#32534;&#35793;&#22120;&#36716;&#25442;&#24037;&#20316;&#20250;&#23558;&#19968;&#20010;&#22522;&#20110;&#20266;&#37327;&#21270;&#30340;&#20840;&#31934;&#24230;&#27169;&#22411;&#33258;&#21160;&#36716;&#25442;&#20026;&#32039;&#20945;&#30340;&#36229;&#20302;&#20301;&#34920;&#31034;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#22312;&#21830;&#29992;&#30828;&#20214;&#19978;&#37096;&#32626;&#37327;&#21270;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;DeepliteRT&#22312;&#26576;&#20123;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of edge devices has unlocked unprecedented opportunities for deep learning model deployment in computer vision applications. However, these complex models require considerable power, memory and compute resources that are typically not available on edge platforms. Ultra low-bit quantization presents an attractive solution to this problem by scaling down the model weights and activations from 32-bit to less than 8-bit. We implement highly optimized ultra low-bit convolution operators for ARM-based targets that outperform existing methods by up to 4.34x. Our operator is implemented within Deeplite Runtime (DeepliteRT), an end-to-end solution for the compilation, tuning, and inference of ultra low-bit models on ARM devices. Compiler passes in DeepliteRT automatically convert a fake-quantized model in full precision to a compact ultra low-bit representation, easing the process of quantized model deployment on commodity hardware. We analyze the performance of DeepliteRT on 
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20840;&#29699;&#28145;&#24230;&#23398;&#20064;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#19968;&#31181;&#21517;&#20026;Pangu-weather&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#21160;&#21147;&#23398;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#28155;&#21152;&#24658;&#23450;&#30340;&#28909;&#28304;&#21644;&#23616;&#37096;&#25200;&#21160;&#26102;&#20135;&#29983;&#20102;&#32463;&#20856;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#21709;&#24212;&#21644;&#20449;&#21495;&#20256;&#25773;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2309.10867</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#30340;&#21160;&#21147;&#23398;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Dynamical Tests of a Deep-Learning Weather Prediction Model. (arXiv:2309.10867v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10867
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20840;&#29699;&#28145;&#24230;&#23398;&#20064;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#19968;&#31181;&#21517;&#20026;Pangu-weather&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#21160;&#21147;&#23398;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#28155;&#21152;&#24658;&#23450;&#30340;&#28909;&#28304;&#21644;&#23616;&#37096;&#25200;&#21160;&#26102;&#20135;&#29983;&#20102;&#32463;&#20856;&#30340;&#22823;&#27668;&#21160;&#21147;&#23398;&#21709;&#24212;&#21644;&#20449;&#21495;&#20256;&#25773;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#20840;&#29699;&#28145;&#24230;&#23398;&#20064;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#19982;&#25805;&#20316;&#20013;&#24515;&#20351;&#29992;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#39044;&#27979;&#12290;&#30446;&#21069;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#32534;&#30721;&#20102;&#22823;&#27668;&#21160;&#21147;&#23398;&#65292;&#36824;&#26159;&#21482;&#26159;&#36827;&#34892;&#20102;&#27169;&#24335;&#21305;&#37197;&#20197;&#33719;&#24471;&#26368;&#23567;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#23545;&#20110;&#30830;&#23450;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#31185;&#23398;&#24037;&#20855;&#30340;&#23454;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#21517;&#20026;Pangu-weather&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#22235;&#20010;&#32463;&#20856;&#30340;&#21160;&#21147;&#23398;&#23454;&#39564;&#65292;&#36825;&#20123;&#23454;&#39564;&#19982;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#19981;&#30456;&#20284;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#36755;&#20986;&#21644;&#21021;&#22987;&#26465;&#20214;&#26045;&#21152;&#23616;&#37096;&#25200;&#21160;&#65292;&#24182;&#21152;&#20837;&#24658;&#23450;&#30340;&#26102;&#38388;&#24179;&#22343;&#26465;&#20214;&#65292;&#20197;&#35780;&#20272;&#20449;&#21495;&#33258;&#23616;&#37096;&#28304;&#28857;&#20256;&#25773;&#30340;&#36895;&#24230;&#21644;&#32467;&#26500;&#28436;&#21270;&#12290;&#36890;&#36807;&#28155;&#21152;&#24658;&#23450;&#30340;&#28909;&#28304;&#26469;&#25200;&#21160;&#27169;&#22411;&#29289;&#29702;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#21152;&#28909;&#21306;&#22495;&#38468;&#36817;&#26377;&#32463;&#20856;&#30340;&#26494;&#37326;-&#21513;&#23572;&#21709;&#24212;&#65292;&#24182;&#26377;&#21521;&#22806;&#23618;&#28909;&#24102;&#25918;&#23556;&#30340;&#34892;&#26143;&#27874;&#12290;&#22312;&#20908;&#23395;&#22343;&#20540;&#19978;&#26045;&#21152;&#23616;&#37096;&#25200;&#21160;&#23558;&#20135;&#29983;&#29699;&#23545;&#31216;&#27874;&#21644;&#22320;&#36716;&#27874;&#31561;&#38745;&#27490;&#27874;&#65292;-&#31616;&#31216;MJO&#12290;
&lt;/p&gt;
&lt;p&gt;
Global deep-learning weather prediction models have recently been shown to produce forecasts that rival those from physics-based models run at operational centers. It is unclear whether these models have encoded atmospheric dynamics, or simply pattern matching that produces the smallest forecast error. Answering this question is crucial to establishing the utility of these models as tools for basic science. Here we subject one such model, Pangu-weather, to a set of four classical dynamical experiments that do not resemble the model training data. Localized perturbations to the model output and the initial conditions are added to steady time-averaged conditions, to assess the propagation speed and structural evolution of signals away from the local source. Perturbing the model physics by adding a steady tropical heat source results in a classical Matsuno--Gill response near the heating, and planetary waves that radiate into the extratropics. A localized disturbance on the winter-average
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#26377;&#20851;&#30340;&#36951;&#20256;&#21464;&#24322;&#21644;&#34892;&#20026;&#29305;&#24449;&#65292;&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#39564;&#35774;&#35745;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#36951;&#20256;&#21644;&#36801;&#31227;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#39118;&#38505;&#20272;&#35745;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10837</link><description>&lt;p&gt;
&#36890;&#36807;&#34892;&#20026;&#21644;&#36951;&#20256;&#29305;&#24449;&#25972;&#21512;&#25913;&#36827;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#39118;&#38505;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Improving Opioid Use Disorder Risk Modelling through Behavioral and Genetic Feature Integration. (arXiv:2309.10837v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10837
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#26377;&#20851;&#30340;&#36951;&#20256;&#21464;&#24322;&#21644;&#34892;&#20026;&#29305;&#24449;&#65292;&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#23454;&#39564;&#35774;&#35745;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#30340;&#39118;&#38505;&#12290;&#32467;&#26524;&#26174;&#31034;&#25972;&#21512;&#36951;&#20256;&#21644;&#36801;&#31227;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#39118;&#38505;&#20272;&#35745;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#29255;&#31867;&#33647;&#29289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24613;&#24615;&#21644;&#24930;&#24615;&#30140;&#30171;&#27490;&#30171;&#33647;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#30270;&#39118;&#38505;&#65292;&#23548;&#33268;&#32654;&#22269;&#27599;&#24180;&#25968;&#30334;&#19975;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#65288;OUD&#65289;&#30149;&#20363;&#21644;&#25968;&#19975;&#20154;&#30340;&#36807;&#26089;&#27515;&#20129;&#12290;&#22312;&#22788;&#26041;&#20043;&#21069;&#20272;&#35745;OUD&#39118;&#38505;&#21487;&#20197;&#25913;&#36827;&#27835;&#30103;&#26041;&#26696;&#30340;&#25928;&#26524;&#12289;&#30417;&#27979;&#35745;&#21010;&#21644;&#24178;&#39044;&#31574;&#30053;&#65292;&#20294;&#39118;&#38505;&#20272;&#35745;&#36890;&#24120;&#22522;&#20110;&#33258;&#25105;&#25253;&#21578;&#30340;&#25968;&#25454;&#25110;&#35843;&#26597;&#38382;&#21367;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#23454;&#39564;&#35774;&#35745;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#23558;&#19982;OUD&#30456;&#20851;&#30340;&#36951;&#20256;&#21464;&#24322;&#19982;&#20174;GPS&#21644;Wi-Fi&#26102;&#31354;&#22352;&#26631;&#20013;&#25552;&#21462;&#30340;&#34892;&#20026;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#35780;&#20272;OUD&#39118;&#38505;&#12290;&#30001;&#20110;OUD&#36801;&#31227;&#21644;&#36951;&#20256;&#25968;&#25454;&#19981;&#23384;&#22312;&#20110;&#21516;&#19968;&#20010;&#32676;&#20307;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31639;&#27861;&#26469;(1)&#26681;&#25454;&#32463;&#39564;&#20998;&#24067;&#29983;&#25104;&#36801;&#31227;&#29305;&#24449;&#21644;(2)&#21512;&#25104;&#36801;&#31227;&#21644;&#36951;&#20256;&#26679;&#26412;&#65292;&#20551;&#35774;&#23384;&#22312;&#26576;&#31181;&#20849;&#30149;&#21644;&#30456;&#23545;&#39118;&#38505;&#27700;&#24179;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25972;&#21512;&#36951;&#20256;&#21644;&#36801;&#31227;&#27169;&#24335;&#21487;&#20197;&#25913;&#21892;&#39118;&#38505;&#20272;&#35745;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opioids are an effective analgesic for acute and chronic pain, but also carry a considerable risk of addiction leading to millions of opioid use disorder (OUD) cases and tens of thousands of premature deaths in the United States yearly. Estimating OUD risk prior to prescription could improve the efficacy of treatment regimens, monitoring programs, and intervention strategies, but risk estimation is typically based on self-reported data or questionnaires. We develop an experimental design and computational methods that combines genetic variants associated with OUD with behavioral features extracted from GPS and Wi-Fi spatiotemporal coordinates to assess OUD risk. Since both OUD mobility and genetic data do not exist for the same cohort, we develop algorithms to (1) generate mobility features from empirical distributions and (2) synthesize mobility and genetic samples assuming a level of comorbidity and relative risks. We show that integrating genetic and mobility modalities improves ris
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#31181;&#26063;&#21644;&#29983;&#29289;&#24615;&#21035;&#20122;&#32452;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#21644;&#29305;&#24449;&#26816;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#33041;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#23384;&#22312;&#20559;&#35265;&#12290;&#25105;&#20204;&#20351;&#29992;Kruskal-Wallis&#21644;Conover-Iman&#26816;&#39564;&#21457;&#29616;&#20102;&#31181;&#26063;&#21644;&#24615;&#21035;&#20043;&#38388;&#30340;&#20559;&#35265;&#65292;&#20351;&#29992;PCA&#21644;Kolmogorov-Smirnov&#26816;&#39564;&#26816;&#27979;&#21040;&#20102;&#29983;&#25104;&#29305;&#24449;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.10835</link><description>&lt;p&gt;
&#20998;&#26512;&#22823;&#33041;&#24180;&#40836;&#39044;&#27979;&#20013;&#30340;&#31181;&#26063;&#21644;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Analysing race and sex bias in brain age prediction. (arXiv:2309.10835v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#31181;&#26063;&#21644;&#29983;&#29289;&#24615;&#21035;&#20122;&#32452;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#21644;&#29305;&#24449;&#26816;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#33041;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#23384;&#22312;&#20559;&#35265;&#12290;&#25105;&#20204;&#20351;&#29992;Kruskal-Wallis&#21644;Conover-Iman&#26816;&#39564;&#21457;&#29616;&#20102;&#31181;&#26063;&#21644;&#24615;&#21035;&#20043;&#38388;&#30340;&#20559;&#35265;&#65292;&#20351;&#29992;PCA&#21644;Kolmogorov-Smirnov&#26816;&#39564;&#26816;&#27979;&#21040;&#20102;&#29983;&#25104;&#29305;&#24449;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;MRI&#20013;&#39044;&#27979;&#22823;&#33041;&#24180;&#40836;&#24050;&#25104;&#20026;&#19982;&#22810;&#31181;&#31070;&#32463;&#30149;&#29702;&#23398;&#30456;&#20851;&#30340;&#27969;&#34892;&#25104;&#20687;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#22312;&#20154;&#21475;&#32479;&#35745;&#19978;&#20559;&#26012;&#21644;&#19981;&#24179;&#34913;&#65292;&#36825;&#21487;&#33021;&#20351;&#22823;&#33041;&#24180;&#40836;&#39044;&#27979;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#20840;&#38754;&#30340;&#20122;&#32452;&#24615;&#33021;&#20998;&#26512;&#21644;&#29305;&#24449;&#26816;&#26597;&#26469;&#20998;&#26512;&#24120;&#29992;&#30340;ResNet-34&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#26469;&#33258;Cam-CAN&#21644;IXI&#30340;1,215&#20010;T1&#21152;&#26435;MRI&#25195;&#25551;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;UK Biobank&#65288;n=42,786&#65289;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#20998;&#20026;&#20845;&#20010;&#31181;&#26063;&#21644;&#29983;&#29289;&#24615;&#21035;&#20122;&#32452;&#12290;&#20026;&#20102;&#27604;&#36739;&#20122;&#32452;&#20043;&#38388;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#32477;&#23545;&#39044;&#27979;&#35823;&#24046;&#26469;&#34913;&#37327;&#65292;&#25105;&#20204;&#20351;&#29992;Kruskal-Wallis&#26816;&#39564;&#65292;&#21518;&#36319;&#20004;&#20010;&#21518;&#32493;&#30340;Conover-Iman&#26816;&#39564;&#26469;&#26816;&#26597;&#31181;&#26063;&#21644;&#29983;&#29289;&#24615;&#21035;&#20043;&#38388;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#26816;&#26597;&#29983;&#25104;&#30340;&#29305;&#24449;&#20013;&#30340;&#20559;&#35265;&#65292;&#25105;&#20204;&#20351;&#29992;PCA&#36827;&#34892;&#32500;&#24230;&#32422;&#31616;&#65292;&#24182;&#20351;&#29992;&#20004;&#26679;&#26412;Kolmogorov-Smirnov&#26816;&#39564;&#26469;&#35782;&#21035;&#20122;&#32452;&#20043;&#38388;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain age prediction from MRI has become a popular imaging biomarker associated with a wide range of neuropathologies. The datasets used for training, however, are often skewed and imbalanced regarding demographics, potentially making brain age prediction models susceptible to bias. We analyse the commonly used ResNet-34 model by conducting a comprehensive subgroup performance analysis and feature inspection. The model is trained on 1,215 T1-weighted MRI scans from Cam-CAN and IXI, and tested on UK Biobank (n=42,786), split into six racial and biological sex subgroups. With the objective of comparing the performance between subgroups, measured by the absolute prediction error, we use a Kruskal-Wallis test followed by two post-hoc Conover-Iman tests to inspect bias across race and biological sex. To examine biases in the generated features, we use PCA for dimensionality reduction and employ two-sample Kolmogorov-Smirnov tests to identify distribution shifts among subgroups. Our results 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#26041;&#27861;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#32593;&#32476;&#65292;&#36890;&#36807;&#20248;&#21270;&#20108;&#36827;&#21046;&#25513;&#30721;&#26469;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#22312;&#26412;&#22320;&#30446;&#26631;&#20013;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#26469;&#20419;&#36827;&#31232;&#30095;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#36890;&#20449;&#21644;&#20869;&#23384;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.10834</link><description>&lt;p&gt;
&#23384;&#22312;&#31232;&#30095;&#30340;&#38543;&#26426;&#32593;&#32476;&#65306;&#36890;&#36807;&#27491;&#21017;&#21270;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparser Random Networks Exist: Enforcing Communication-Efficient Federated Learning via Regularization. (arXiv:2309.10834v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27491;&#21017;&#21270;&#26041;&#27861;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#30340;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#32593;&#32476;&#65292;&#36890;&#36807;&#20248;&#21270;&#20108;&#36827;&#21046;&#25513;&#30721;&#26469;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#22312;&#26412;&#22320;&#30446;&#26631;&#20013;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#26469;&#20419;&#36827;&#31232;&#30095;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#36890;&#20449;&#21644;&#20869;&#23384;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#38543;&#26426;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#32593;&#32476;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#20248;&#21270;&#30340;&#26159;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#32780;&#19981;&#26159;&#22266;&#23450;&#30340;&#27169;&#22411;&#26435;&#37325;&#12290;&#35813;&#25513;&#30721;&#34920;&#24449;&#20102;&#19968;&#20010;&#33021;&#22815;&#21644;&#36739;&#23567;&#30340;&#30446;&#26631;&#32593;&#32476;&#19968;&#26679;&#22909;&#22320;&#27867;&#21270;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#20256;&#32479;&#32852;&#21512;&#23398;&#20064;&#20013;&#65292;&#20132;&#25442;&#30340;&#26159;&#31232;&#30095;&#30340;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#32780;&#19981;&#26159;&#28014;&#28857;&#26435;&#37325;&#65292;&#36825;&#26679;&#21487;&#20197;&#23558;&#36890;&#20449;&#25104;&#26412;&#38477;&#20302;&#21040;&#27599;&#20010;&#21442;&#25968;&#26368;&#22810;1&#20010;&#27604;&#29305;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#38543;&#26426;&#26041;&#27861;&#26080;&#27861;&#25214;&#21040;&#33021;&#22815;&#36890;&#36807;&#19968;&#33268;&#30340;&#25439;&#22833;&#30446;&#26631;&#20943;&#23569;&#36890;&#20449;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#31232;&#30095;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#26412;&#22320;&#30446;&#26631;&#20013;&#28155;&#21152;&#27491;&#21017;&#21270;&#39033;&#65292;&#36890;&#36807;&#28040;&#38500;&#23376;&#32593;&#32476;&#20043;&#38388;&#30340;&#20887;&#20313;&#29305;&#24449;&#26469;&#20419;&#36827;&#26356;&#31232;&#30095;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36890;&#20449;&#21644;&#20869;&#23384;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#39640;&#36798;&#20116;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a new method for enhancing communication efficiency in stochastic Federated Learning that trains over-parameterized random networks. In this setting, a binary mask is optimized instead of the model weights, which are kept fixed. The mask characterizes a sparse sub-network that is able to generalize as good as a smaller target network. Importantly, sparse binary masks are exchanged rather than the floating point weights in traditional federated learning, reducing communication cost to at most 1 bit per parameter. We show that previous state of the art stochastic methods fail to find the sparse networks that can reduce the communication and storage overhead using consistent loss objectives. To address this, we propose adding a regularization term to local objectives that encourages sparser solutions by eliminating redundant features across sub-networks. Extensive experiments demonstrate significant improvements in communication and memory efficiency of up to five magni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#65292;&#29983;&#25104;&#30340;&#25511;&#21046;&#22120;&#33021;&#22815;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.10831</link><description>&lt;p&gt;
&#27963;&#21160;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#31181;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Actively Learning Reinforcement Learning: A Stochastic Optimal Control Approach. (arXiv:2309.10831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#65292;&#29983;&#25104;&#30340;&#25511;&#21046;&#22120;&#33021;&#22815;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#21644;&#23454;&#26102;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#24212;&#23545;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;i&#65289;&#24378;&#21270;&#23398;&#20064;&#22312;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#22240;&#20026;&#21463;&#25511;&#23454;&#39564;&#23460;/&#20223;&#30495;&#21644;&#23454;&#38469;&#26465;&#20214;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#30340;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#38543;&#26426;&#21160;&#24577;&#35268;&#21010;&#26041;&#31243;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#23545;&#20110;&#20960;&#31181;&#31867;&#22411;&#30340;&#32422;&#26463;&#26465;&#20214;&#26159;&#23433;&#20840;&#30340;&#65292;&#24182;&#19988;&#23427;&#21487;&#20197;&#20027;&#21160;&#23398;&#20064;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#25506;&#32034;&#21644;&#21033;&#29992;&#19981;&#21516;&#65292;&#25506;&#27979;&#21644;&#23433;&#20840;&#24615;&#30001;&#25511;&#21046;&#22120;&#33258;&#36523;&#33258;&#21160;&#23454;&#29616;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#23398;&#20064;&#12290;&#19968;&#20010;&#20223;&#30495;&#31034;&#20363;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide framework to cope with two problems: (i) the fragility of reinforcement learning due to modeling uncertainties because of the mismatch between controlled laboratory/simulation and real-world conditions and (ii) the prohibitive computational cost of stochastic optimal control. We approach both problems by using reinforcement learning to solve the stochastic dynamic programming equation. The resulting reinforcement learning controller is safe with respect to several types of constraints constraints and it can actively learn about the modeling uncertainties. Unlike exploration and exploitation, probing and safety are employed automatically by the controller itself, resulting real-time learning. A simulation example demonstrates the efficacy of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#32593;&#26684;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#28508;&#22312;&#35299;&#32544;&#26500;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#33041;&#39045;&#32508;&#21512;&#30151;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#36741;&#21161;&#25163;&#26415;&#35268;&#21010;&#65292;&#24182;&#20801;&#35768;&#23458;&#35266;&#35780;&#20272;&#25163;&#26415;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10825</link><description>&lt;p&gt;
&#32593;&#26684;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#28508;&#22312;&#35299;&#32544;&#26500;&#26550;&#22312;&#33041;&#39045;&#32508;&#21512;&#30151;&#30340;&#35786;&#26029;&#21644;&#25163;&#26415;&#35268;&#21010;&#20013;&#26377;&#25152;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Latent Disentanglement in Mesh Variational Autoencoders Improves the Diagnosis of Craniofacial Syndromes and Aids Surgical Planning. (arXiv:2309.10825v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25913;&#36827;&#20102;&#32593;&#26684;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#28508;&#22312;&#35299;&#32544;&#26500;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#33041;&#39045;&#32508;&#21512;&#30151;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#36741;&#21161;&#25163;&#26415;&#35268;&#21010;&#65292;&#24182;&#20801;&#35768;&#23458;&#35266;&#35780;&#20272;&#25163;&#26415;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#20154;&#31867;&#22836;&#37096;&#30340;&#24418;&#29366;&#36827;&#34892;&#20998;&#26512;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#20840;&#23616;&#21644;&#23616;&#37096;&#27700;&#24179;&#19978;&#20934;&#30830;&#24314;&#27169;&#19968;&#30452;&#23384;&#22312;&#19968;&#20123;&#38556;&#30861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;&#20132;&#25442;&#35299;&#32544;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;SD-VAE&#65289;&#22312;Crouzon&#12289;Apert&#21644;Muenke&#32508;&#21512;&#24449;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;&#32508;&#21512;&#30151;&#20998;&#31867;&#26159;&#22312;&#25972;&#20010;&#32593;&#26684;&#19978;&#36827;&#34892;&#30340;&#65292;&#20294;&#39318;&#27425;&#21487;&#20197;&#20998;&#26512;&#22836;&#37096;&#27599;&#20010;&#21306;&#22495;&#23545;&#32508;&#21512;&#30151;&#34920;&#22411;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#25805;&#32437;&#29983;&#25104;&#27169;&#22411;&#30340;&#29305;&#23450;&#21442;&#25968;&#65292;&#24182;&#20135;&#29983;&#29305;&#23450;&#25163;&#26415;&#30340;&#26032;&#24418;&#29366;&#65292;&#36824;&#21487;&#20197;&#27169;&#25311;&#19968;&#31995;&#21015;&#39045;&#39068;&#38754;&#25163;&#26415;&#30340;&#32467;&#26524;&#12290;&#36825;&#25171;&#24320;&#20102;&#25512;&#21160;&#35786;&#26029;&#36827;&#23637;&#12289;&#24110;&#21161;&#25163;&#26415;&#35268;&#21010;&#21644;&#23454;&#26045;&#25163;&#26415;&#32467;&#26524;&#23458;&#35266;&#35780;&#20272;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of deep learning to undertake shape analysis of the complexities of the human head holds great promise. However, there have traditionally been a number of barriers to accurate modelling, especially when operating on both a global and local level. In this work, we will discuss the application of the Swap Disentangled Variational Autoencoder (SD-VAE) with relevance to Crouzon, Apert and Muenke syndromes. Although syndrome classification is performed on the entire mesh, it is also possible, for the first time, to analyse the influence of each region of the head on the syndromic phenotype. By manipulating specific parameters of the generative model, and producing procedure-specific new shapes, it is also possible to simulate the outcome of a range of craniofacial surgical procedures. This opens new avenues to advance diagnosis, aids surgical planning and allows for the objective evaluation of surgical outcomes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#22522;&#30784;&#27169;&#22411;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#35752;&#35770;&#20102;&#20351;&#29992;transformers&#12289;&#29289;&#29702;&#30693;&#35782;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20840;&#29699;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;AI&#27169;&#22411;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2309.10808</link><description>&lt;p&gt;
&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;AI&#22522;&#30784;&#27169;&#22411;&#65306;&#24212;&#29992;&#12289;&#35774;&#35745;&#21644;&#23454;&#26045;
&lt;/p&gt;
&lt;p&gt;
AI Foundation Models for Weather and Climate: Applications, Design, and Implementation. (arXiv:2309.10808v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10808
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#22522;&#30784;&#27169;&#22411;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#35201;&#35752;&#35770;&#20102;&#20351;&#29992;transformers&#12289;&#29289;&#29702;&#30693;&#35782;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20840;&#29699;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;AI&#27169;&#22411;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35299;&#22823;&#27668;&#30340;&#28151;&#27788;&#34892;&#20026;&#21644;&#25512;&#36827;&#22825;&#27668;&#39044;&#25253;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#32034;&#12290;&#31185;&#25216;&#20844;&#21496;&#12289;&#25919;&#24220;&#26426;&#26500;&#21644;&#27668;&#35937;&#26426;&#26500;&#23545;&#24314;&#31435;&#22320;&#29699;&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#34920;&#29616;&#20986;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#26368;&#36817;&#20351;&#29992;transformers&#12289;&#21463;&#29289;&#29702;&#30693;&#35782;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#22312;&#30456;&#23545;&#29421;&#31364;&#30340;&#26102;&#31354;&#33539;&#22260;&#21644;&#29305;&#23450;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;transformers&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#21644;&#35270;&#35273;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#25105;&#20204;&#27491;&#22312;&#26397;&#30528;&#21487;&#25512;&#24191;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36808;&#36827;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#27491;&#22312;&#35265;&#35777;&#33021;&#22815;&#22312;&#22810;&#20010;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#19978;&#26377;&#31454;&#20105;&#21147;&#30340;AI&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20840;&#29699;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#21487;&#25512;&#24191;AI&#27169;&#22411;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning and deep learning methods have been widely explored in understanding the chaotic behavior of the atmosphere and furthering weather forecasting. There has been increasing interest from technology companies, government institutions, and meteorological agencies in building digital twins of the Earth. Recent approaches using transformers, physics-informed machine learning, and graph neural networks have demonstrated state-of-the-art performance on relatively narrow spatiotemporal scales and specific tasks. With the recent success of generative artificial intelligence (AI) using pre-trained transformers for language modeling and vision with prompt engineering and fine-tuning, we are now moving towards generalizable AI. In particular, we are witnessing the rise of AI foundation models that can perform competitively on multiple domain-specific downstream tasks. Despite this progress, we are still in the nascent stages of a generalizable AI model for global Earth system models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#27979;&#30417;&#25511;&#31995;&#32479;&#65288;PDRL&#65289;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30417;&#27979;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#31574;&#30053;&#26469;&#23398;&#20064;&#29616;&#26377;&#30693;&#35782;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;</title><link>http://arxiv.org/abs/2309.10576</link><description>&lt;p&gt;
PDRL&#65306;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#27979;&#30417;&#25511;
&lt;/p&gt;
&lt;p&gt;
PDRL: Multi-Agent based Reinforcement Learning for Predictive Monitoring. (arXiv:2309.10576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39044;&#27979;&#30417;&#25511;&#31995;&#32479;&#65288;PDRL&#65289;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#30417;&#27979;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#31574;&#30053;&#26469;&#23398;&#20064;&#29616;&#26377;&#30693;&#35782;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22240;&#20854;&#33021;&#22815;&#20174;&#20197;&#24448;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#20570;&#20986;&#33258;&#36866;&#24212;&#20915;&#31574;&#30340;&#33021;&#21147;&#65292;&#22312;&#30417;&#25511;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#34987;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20581;&#24247;&#30417;&#25511;&#24212;&#29992;&#22823;&#22810;&#26159;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#35757;&#32451;&#26631;&#31614;&#25968;&#25454;&#65292;&#26080;&#27861;&#22312;&#19981;&#30830;&#23450;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#20570;&#20986;&#33258;&#36866;&#24212;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#31995;&#32479;&#65292;&#21363;&#20855;&#26377;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#39044;&#27979;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;PDRL&#65289;&#65292;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#29615;&#22659;&#12290;&#35813;&#25552;&#20986;&#30340;&#36890;&#29992;&#26694;&#26550;&#21487;&#20197;&#23481;&#32435;&#34394;&#25311;&#28145;&#24230; Q &#32593;&#32476;&#65288;DQN&#65289;&#26234;&#33021;&#20307;&#65292;&#20197;&#30417;&#27979;&#22797;&#26434;&#29615;&#22659;&#30340;&#39044;&#27979;&#26410;&#26469;&#29366;&#24577;&#65292;&#24182;&#26681;&#25454;&#26126;&#30830;&#23450;&#20041;&#30340;&#22870;&#21169;&#31574;&#30053;&#20351;&#26234;&#33021;&#20307;&#22312;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#21516;&#26102;&#23398;&#20064;&#29616;&#26377;&#30693;&#35782;&#12290;&#22312;&#35780;&#20272;&#35813;&#26694;&#26550;&#30340;&#36807;&#31243;&#20013;&#65292;&#37096;&#32626;&#20102;&#19977;&#20010;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20197;&#30417;&#27979;&#36890;&#36807; BiLSTM &#27169;&#22411;&#39044;&#27979;&#30340;&#21463;&#35797;&#32773;&#26410;&#26469;&#30340;&#24515;&#29575;&#12289;&#21628;&#21560;&#29575;&#21644;&#20307;&#28201;&#12290;&#38543;&#30528;&#27599;&#27425;&#36845;&#20195;&#65292;&#26234;&#33021;&#20307;&#26681;&#25454;&#23454;&#38469;&#21453;&#39304;&#35843;&#25972;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning has been increasingly applied in monitoring applications because of its ability to learn from previous experiences and can make adaptive decisions. However, existing machine learning-based health monitoring applications are mostly supervised learning algorithms, trained on labels and they cannot make adaptive decisions in an uncertain complex environment. This study proposes a novel and generic system, predictive deep reinforcement learning (PDRL) with multiple RL agents in a time series forecasting environment. The proposed generic framework accommodates virtual Deep Q Network (DQN) agents to monitor predicted future states of a complex environment with a well-defined reward policy so that the agent learns existing knowledge while maximizing their rewards. In the evaluation process of the proposed framework, three DRL agents were deployed to monitor a subject's future heart rate, respiration, and temperature predicted using a BiLSTM model. With each iteration, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2309.10400</link><description>&lt;p&gt;
PoSE: &#36890;&#36807;&#20301;&#32622;&#36339;&#36291;&#24335;&#35757;&#32451;&#25552;&#39640;LLMs&#23545;&#20110;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#26377;&#25928;&#25299;&#23637;
&lt;/p&gt;
&lt;p&gt;
PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. (arXiv:2309.10400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PoSE&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#36866;&#24212;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#65292;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Positional Skip-wise (PoSE)&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20110;&#26497;&#38271;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;PoSE&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22266;&#23450;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#21644;&#25805;&#32437;&#20301;&#32622;&#32034;&#24341;&#26469;&#27169;&#25311;&#38271;&#36755;&#20837;&#65292;&#23558;&#35757;&#32451;&#38271;&#24230;&#19982;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#20998;&#31163;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#38271;&#36755;&#20837;&#24207;&#21015;&#20013;&#36873;&#25321;&#33509;&#24178;&#30701;&#22359;&#65292;&#24182;&#24341;&#20837;&#19981;&#21516;&#30340;&#36339;&#36291;&#20559;&#32622;&#39033;&#26469;&#20462;&#25913;&#27599;&#20010;&#22359;&#30340;&#20301;&#32622;&#32034;&#24341;&#12290;&#36825;&#20123;&#36339;&#36291;&#20559;&#32622;&#39033;&#20197;&#21450;&#27599;&#20010;&#22359;&#30340;&#38271;&#24230;&#22312;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#20013;&#37117;&#20250;&#21464;&#21270;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#25152;&#26377;&#20301;&#32622;&#65292;&#32780;&#26080;&#38656;&#23545;&#23436;&#25972;&#38271;&#24230;&#30340;&#36755;&#20837;&#36827;&#34892;&#35757;&#32451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#23545;&#23436;&#25972;&#38271;&#24230;&#36827;&#34892;&#24494;&#35843;&#30456;&#27604;&#65292;PoSE&#22823;&#22823;&#20943;&#23567;&#20102;&#20869;&#23384;&#21644;&#26102;&#38388;&#24320;&#38144;&#65292;&#23545;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#12290;&#21033;&#29992;&#36825;&#19968;&#20248;&#21183;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;LLaMA&#27169;&#22411;&#25193;&#23637;&#21040;&#20102;128k&#20010;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#23454;&#65292;PoSE&#19982;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Positional Skip-wisE (PoSE) training for efficient adaptation of large language models~(LLMs) to extremely long context windows. PoSE decouples train length from target context window size by simulating long inputs using a fixed context window with manipulated position indices during training. Concretely, we select several short chunks from a long input sequence, and introduce distinct skipping bias terms to modify the position indices of each chunk. These bias terms, along with the length of each chunk, are altered for each training example, allowing the model to adapt to all positions within the target context window without training on full length inputs. Experiments show that, compared with fine-tuning on the full length, PoSE greatly reduces memory and time overhead with minimal impact on performance. Leveraging this advantage, we have successfully extended the LLaMA model to 128k tokens. Furthermore, we empirically confirm that PoSE is compatible with 
&lt;/p&gt;</description></item><item><title>DQAS&#26159;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;NISQ&#26102;&#20195;&#33258;&#21160;&#35774;&#35745;&#37327;&#23376;&#30005;&#36335;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;DQAS&#22312;&#35299;&#20915;&#37327;&#23376;&#28145;&#24230;Q-learning&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.10392</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentiable Quantum Architecture Search for Quantum Reinforcement Learning. (arXiv:2309.10392v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10392
&lt;/p&gt;
&lt;p&gt;
DQAS&#26159;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;NISQ&#26102;&#20195;&#33258;&#21160;&#35774;&#35745;&#37327;&#23376;&#30005;&#36335;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;DQAS&#22312;&#35299;&#20915;&#37327;&#23376;&#28145;&#24230;Q-learning&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;DQAS&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;NISQ&#26102;&#20195;&#33258;&#21160;&#35774;&#35745;&#37327;&#23376;&#30005;&#36335;&#12290;&#23427;&#30340;&#21160;&#26426;&#26159;&#37327;&#23376;&#30828;&#20214;&#30340;&#20302;&#20445;&#30495;&#24230;&#65292;&#30005;&#36335;&#26550;&#26500;&#30340;&#20302;&#28789;&#27963;&#24615;&#65292;&#30005;&#36335;&#35774;&#35745;&#25104;&#26412;&#39640;&#65292;&#24179;&#22374;&#30340;&#33618;&#21407;&#38382;&#39064;&#21644;&#26435;&#37325;&#30340;&#21608;&#26399;&#24615;&#12290;&#20154;&#20204;&#20351;&#29992;&#23427;&#26469;&#35299;&#20915;&#22522;&#20110;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#35823;&#24046;&#32531;&#35299;&#12289;&#37193;&#20998;&#35299;&#21644;&#37327;&#23376;&#36924;&#36817;&#20248;&#21270;&#38382;&#39064;&#12290;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#37096;&#20998;&#65292;&#36890;&#24120;&#26377;&#21508;&#31181;&#25968;&#25454;&#12290;QRL&#36890;&#24120;&#20351;&#29992;&#25163;&#21160;&#35774;&#35745;&#30340;&#30005;&#36335;&#12290;&#28982;&#32780;&#65292;&#39044;&#23450;&#20041;&#30340;&#30005;&#36335;&#38656;&#35201;&#26356;&#22810;&#30340;&#28789;&#27963;&#24615;&#26469;&#24212;&#23545;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;&#30005;&#36335;&#35774;&#35745;&#21487;&#33021;&#21464;&#24471;&#26840;&#25163;&#65292;&#29305;&#21035;&#26159;&#22312;&#30005;&#36335;&#35268;&#27169;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#12290;DQAS&#33021;&#21542;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#37327;&#23376;&#28145;&#24230;Q-learning&#38382;&#39064;&#20173;&#26410;&#35299;&#20915;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21457;&#29616;DQAS&#22312;&#35299;&#20915;&#37327;&#23376;&#28145;&#24230;Q-learning&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable quantum architecture search (DQAS) is a gradient-based framework to design quantum circuits automatically in the NISQ era. It was motivated by such as low fidelity of quantum hardware, low flexibility of circuit architecture, high circuit design cost, barren plateau (BP) problem, and periodicity of weights. People used it to address error mitigation, unitary decomposition, and quantum approximation optimization problems based on fixed datasets. Quantum reinforcement learning (QRL) is a part of quantum machine learning and often has various data. QRL usually uses a manually designed circuit. However, the pre-defined circuit needs more flexibility for different tasks, and the circuit design based on various datasets could become intractable in the case of a large circuit. The problem of whether DQAS can be applied to quantum deep Q-learning with various datasets is still open. The main target of this work is to discover the capability of DQAS to solve quantum deep Q-learni
&lt;/p&gt;</description></item><item><title>TensorCodec&#26159;&#19968;&#31181;&#32039;&#20945;&#30340;&#26377;&#25439;&#24352;&#37327;&#21387;&#32553;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#24378;&#25968;&#25454;&#20551;&#35774;&#30340;&#19968;&#33324;&#24352;&#37327;&#12290;&#23427;&#37319;&#29992;&#31070;&#32463;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#12289;&#25240;&#21472;&#36755;&#20837;&#24352;&#37327;&#21644;&#37325;&#26032;&#25490;&#24207;&#27169;&#24335;&#32034;&#24341;&#31561;&#20851;&#38190;&#28857;&#26469;&#25552;&#39640;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10310</link><description>&lt;p&gt;
TensorCodec: &#26080;&#24378;&#25968;&#25454;&#20551;&#35774;&#30340;&#32039;&#20945;&#26377;&#25439;&#24352;&#37327;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
TensorCodec: Compact Lossy Compression of Tensors without Strong Data Assumptions. (arXiv:2309.10310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10310
&lt;/p&gt;
&lt;p&gt;
TensorCodec&#26159;&#19968;&#31181;&#32039;&#20945;&#30340;&#26377;&#25439;&#24352;&#37327;&#21387;&#32553;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#24378;&#25968;&#25454;&#20551;&#35774;&#30340;&#19968;&#33324;&#24352;&#37327;&#12290;&#23427;&#37319;&#29992;&#31070;&#32463;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#12289;&#25240;&#21472;&#36755;&#20837;&#24352;&#37327;&#21644;&#37325;&#26032;&#25490;&#24207;&#27169;&#24335;&#32034;&#24341;&#31561;&#20851;&#38190;&#28857;&#26469;&#25552;&#39640;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#37117;&#26159;&#20197;&#24352;&#37327;&#30340;&#24418;&#24335;&#34920;&#31034;&#30340;&#65292;&#21363;&#22810;&#32500;&#25968;&#20540;&#25968;&#32452;&#12290;&#22914;&#26524;&#19981;&#36827;&#34892;&#21387;&#32553;&#65292;&#23384;&#20648;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#31354;&#38388;&#65292;&#32780;&#19988;&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#24352;&#37327;&#21387;&#32553;&#31639;&#27861;&#21487;&#29992;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#20381;&#36182;&#20110;&#20851;&#20110;&#25968;&#25454;&#32500;&#24230;&#12289;&#31232;&#30095;&#24615;&#12289;&#31209;&#21644;&#24179;&#28369;&#24615;&#30340;&#24378;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TENSORCODEC&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#19968;&#33324;&#24352;&#37327;&#30340;&#26377;&#25439;&#21387;&#32553;&#31639;&#27861;&#65292;&#19981;&#38656;&#35201;&#31526;&#21512;&#24378;&#20551;&#35774;&#30340;&#36755;&#20837;&#25968;&#25454;&#12290;TENSORCODEC&#21253;&#21547;&#20102;&#19977;&#20010;&#20851;&#38190;&#28857;&#12290;&#31532;&#19968;&#20010;&#20851;&#38190;&#28857;&#26159;&#31070;&#32463;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#65288;NTTD&#65289;&#65292;&#25105;&#20204;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#21040;&#24352;&#37327;&#21015;&#36710;&#20998;&#35299;&#20013;&#65292;&#20197;&#22686;&#24378;&#20854;&#34920;&#36798;&#33021;&#21147;&#24182;&#20943;&#36731;&#30001;&#20302;&#31209;&#20551;&#35774;&#25152;&#24102;&#26469;&#30340;&#38480;&#21046;&#12290;&#21478;&#19968;&#20010;&#20851;&#38190;&#28857;&#26159;&#23558;&#36755;&#20837;&#24352;&#37327;&#25240;&#21472;&#25104;&#26356;&#39640;&#38454;&#30340;&#24352;&#37327;&#65292;&#20197;&#20943;&#23569;NTTD&#25152;&#38656;&#30340;&#31354;&#38388;&#12290;&#26368;&#21518;&#65292;&#23545;&#36755;&#20837;&#24352;&#37327;&#36827;&#34892;&#27169;&#24335;&#32034;&#24341;&#30340;&#37325;&#26032;&#25490;&#24207;&#65292;&#20197;&#25581;&#31034;&#21487;&#20197;&#34987;&#21033;&#29992;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world datasets are represented as tensors, i.e., multi-dimensional arrays of numerical values. Storing them without compression often requires substantial space, which grows exponentially with the order. While many tensor compression algorithms are available, many of them rely on strong data assumptions regarding its order, sparsity, rank, and smoothness. In this work, we propose TENSORCODEC, a lossy compression algorithm for general tensors that do not necessarily adhere to strong input data assumptions. TENSORCODEC incorporates three key ideas. The first idea is Neural Tensor-Train Decomposition (NTTD) where we integrate a recurrent neural network into Tensor-Train Decomposition to enhance its expressive power and alleviate the limitations imposed by the low-rank assumption. Another idea is to fold the input tensor into a higher-order tensor to reduce the space required by NTTD. Finally, the mode indices of the input tensor are reordered to reveal patterns that can be explo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#20351;&#29992;&#20102;$\ell_0$-&#33539;&#25968;&#32422;&#26463;&#65292;&#21487;&#20197;&#36731;&#26494;&#25511;&#21046;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#25968;&#37327;&#30340;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.10152</link><description>&lt;p&gt;
Primal-Dual $\ell_0$-&#32422;&#26463;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
Primal-Dual $\ell_0$-Constrained Sparse Index Tracking. (arXiv:2309.10152v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#20351;&#29992;&#20102;$\ell_0$-&#33539;&#25968;&#32422;&#26463;&#65292;&#21487;&#20197;&#36731;&#26494;&#25511;&#21046;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#25968;&#37327;&#30340;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#34987;&#21160;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#26469;&#36319;&#36394;&#37329;&#34701;&#25351;&#25968;&#12290;&#30456;&#27604;&#20110;&#20840;&#20179;&#25237;&#36164;&#32452;&#21512;&#65292;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#22312;&#38477;&#20302;&#20132;&#26131;&#25104;&#26412;&#21644;&#36991;&#20813;&#19981;&#27969;&#21160;&#36164;&#20135;&#26041;&#38754;&#26356;&#20855;&#20248;&#21183;&#12290;&#20026;&#20102;&#24378;&#21046;&#25237;&#36164;&#32452;&#21512;&#30340;&#31232;&#30095;&#24615;&#65292;&#20256;&#32479;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;$\ell_p$-&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#20844;&#24335;&#65292;&#20316;&#20026;$\ell_0$-&#33539;&#25968;&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#26367;&#20195;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#20844;&#24335;&#21487;&#20197;&#29992;&#26469;&#26500;&#24314;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#65292;&#20294;&#22312;&#23454;&#38469;&#25237;&#36164;&#20013;&#21364;&#19981;&#26131;&#20351;&#29992;&#65292;&#22240;&#20026;&#32454;&#33268;&#30340;&#21442;&#25968;&#35843;&#25972;&#26469;&#25351;&#23450;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#25968;&#37327;&#30340;&#19978;&#38480;&#26159;&#33392;&#38590;&#19988;&#32791;&#26102;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#65292;&#20351;&#29992;&#20102;$\ell_0$-&#33539;&#25968;&#32422;&#26463;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#25511;&#21046;&#25237;&#36164;&#32452;&#21512;&#20013;&#36164;&#20135;&#25968;&#37327;&#30340;&#19978;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#20801;&#35768;&#22312;&#25237;&#36164;&#32452;&#21512;&#31232;&#30095;&#24615;&#21644;&#25442;&#25163;&#29575;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse index tracking is one of the prominent passive portfolio management strategies that construct a sparse portfolio to track a financial index. A sparse portfolio is desirable over a full portfolio in terms of transaction cost reduction and avoiding illiquid assets. To enforce the sparsity of the portfolio, conventional studies have proposed formulations based on $\ell_p$-norm regularizations as a continuous surrogate of the $\ell_0$-norm regularization. Although such formulations can be used to construct sparse portfolios, they are not easy to use in actual investments because parameter tuning to specify the exact upper bound on the number of assets in the portfolio is delicate and time-consuming. In this paper, we propose a new problem formulation of sparse index tracking using an $\ell_0$-norm constraint that enables easy control of the upper bound on the number of assets in the portfolio. In addition, our formulation allows the choice between portfolio sparsity and turnover spa
&lt;/p&gt;</description></item><item><title>Q-Transformer&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#26469;&#34920;&#31034;Q&#20989;&#25968;&#24182;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2309.10150</link><description>&lt;p&gt;
Q-Transformer&#65306;&#36890;&#36807;&#33258;&#22238;&#24402;Q-&#20989;&#25968;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions. (arXiv:2309.10150v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10150
&lt;/p&gt;
&lt;p&gt;
Q-Transformer&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Transformer&#26469;&#34920;&#31034;Q&#20989;&#25968;&#24182;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#20197;&#21033;&#29992;&#20154;&#31867;&#28436;&#31034;&#21644;&#33258;&#20027;&#37319;&#38598;&#25968;&#25454;&#30340;&#22823;&#22411;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#22810;&#20219;&#21153;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;Transformer&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;Q&#20989;&#25968;&#34920;&#31034;&#65292;&#36890;&#36807;&#31163;&#32447;&#26102;&#24046;&#22791;&#20221;&#36827;&#34892;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#31216;&#20026;Q-Transformer&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#21160;&#20316;&#32500;&#24230;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#24182;&#23558;&#27599;&#20010;&#21160;&#20316;&#32500;&#24230;&#30340;Q&#20540;&#34920;&#31034;&#20026;&#21333;&#29420;&#30340;&#26631;&#35760;&#65292;&#25105;&#20204;&#21487;&#20197;&#24212;&#29992;&#39640;&#23481;&#37327;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#36827;&#34892;Q&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#35774;&#35745;&#20915;&#31574;&#65292;&#20351;&#20854;&#22312;&#31163;&#32447;RL&#35757;&#32451;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;Q-Transformer&#22312;&#22823;&#35268;&#27169;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#22871;&#20214;&#19978;&#20248;&#20110;&#20197;&#24448;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#21644;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#12290;&#35813;&#39033;&#30446;&#30340;&#32593;&#31449;&#21644;&#35270;&#39057;&#21487;&#20197;&#22312;https://q-transformer.github.io&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite. The project's website and videos can be found at https://q-transformer.github.io
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#29702;&#35770;&#19982;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#27880;&#24847;&#21147;&#25918;&#22312;&#19978;&#19979;&#25991;-&#26410;&#26631;&#35760;&#26679;&#26412;&#19978;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39046;&#22495;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.09888</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#8776;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context $\approx$ Environment. (arXiv:2309.09888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09888
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#36890;&#36807;&#29702;&#35770;&#19982;&#23454;&#39564;&#35777;&#26126;&#20102;&#23558;&#27880;&#24847;&#21147;&#25918;&#22312;&#19978;&#19979;&#25991;-&#26410;&#26631;&#35760;&#26679;&#26412;&#19978;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39046;&#22495;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#30740;&#31350;&#30340;&#20013;&#24515;&#22312;&#20110;&#20004;&#20010;&#26041;&#38754;&#12290;&#19968;&#26041;&#38754;&#65292;&#31038;&#21306;&#27491;&#22312;&#21162;&#21147;&#26500;&#24314;&#33021;&#22815;&#20002;&#24323;&#34394;&#20551;&#30456;&#20851;&#24615;&#24182;&#22312;&#26032;&#39062;&#30340;&#27979;&#35797;&#29615;&#22659;&#20013;&#26356;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#30340;&#27169;&#22411;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#20219;&#20309;&#25552;&#26696;&#33021;&#22815;&#20196;&#20154;&#20449;&#26381;&#22320;&#36229;&#36234;&#31616;&#21333;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22522;&#32447;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#12289;&#26681;&#25454;&#29992;&#25143;&#36890;&#36807;&#25552;&#31034;&#26045;&#21152;&#30340;&#22810;&#31181;&#19978;&#19979;&#25991;&#32972;&#26223;&#28789;&#27963;&#27867;&#21270;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#35748;&#20026;&#19978;&#19979;&#25991;&#8776;&#29615;&#22659;&#65292;&#24182;&#20551;&#35774;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#38544;&#34255;&#30528;&#26356;&#22909;&#30340;&#39046;&#22495;&#27867;&#21270;&#20043;&#38053;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29702;&#35770;&#19982;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27880;&#24847;&#19978;&#19979;&#25991;-&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#31181;&#27880;&#24847;&#21487;&#20197;&#20351;&#25105;&#20204;&#25552;&#20986;&#30340;In-Context Risk Minimization (ICRM)&#31639;&#27861;&#32858;&#28966;&#20110;&#27979;&#35797;&#29615;&#22659;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two lines of work are taking the central stage in AI research. On the one hand, the community is making increasing efforts to build models that discard spurious correlations and generalize better in novel test environments. Unfortunately, the bitter lesson so far is that no proposal convincingly outperforms a simple empirical risk minimization baseline. On the other hand, large language models (LLMs) have erupted as algorithms able to learn in-context, generalizing on-the-fly to the eclectic contextual circumstances that users enforce by means of prompting. In this paper, we argue that context $\approx$ environment, and posit that in-context learning holds the key to better domain generalization. Via extensive theory and experiments, we show that paying attention to context$\unicode{x2013}\unicode{x2013}$unlabeled examples as they arrive$\unicode{x2013}\unicode{x2013}$allows our proposed In-Context Risk Minimization (ICRM) algorithm to zoom-in on the test environment risk minimizer, le
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#27604;&#21021;&#22987;&#29366;&#24577;&#32531;&#20914;&#21306;&#30340;&#27010;&#24565;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#30340;&#29366;&#24577;&#26469;&#21021;&#22987;&#21270;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#20197;&#24341;&#23548;&#20854;&#36827;&#20837;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#29366;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.09752</link><description>&lt;p&gt;
&#23545;&#27604;&#21021;&#22987;&#29366;&#24577;&#32531;&#20914;&#21306;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contrastive Initial State Buffer for Reinforcement Learning. (arXiv:2309.09752v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#27604;&#21021;&#22987;&#29366;&#24577;&#32531;&#20914;&#21306;&#30340;&#27010;&#24565;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#30340;&#29366;&#24577;&#26469;&#21021;&#22987;&#21270;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#20197;&#24341;&#23548;&#20854;&#36827;&#20837;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#29366;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#39640;&#30340;&#20219;&#21153;&#24615;&#33021;&#24182;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21208;&#25506;&#19982;&#21033;&#29992;&#20043;&#38388;&#30340;&#24179;&#34913;&#32473;&#20174;&#26377;&#38480;&#30340;&#26679;&#26412;&#20013;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#24102;&#26469;&#20102;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#24037;&#20316;&#22312;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#36827;&#34892;&#31574;&#30053;&#26356;&#26032;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#20102;&#37325;&#26032;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#30340;&#28508;&#21147;&#12290;&#29420;&#31435;&#20110;&#22522;&#26412;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#21021;&#22987;&#29366;&#24577;&#32531;&#20914;&#21306;&#30340;&#27010;&#24565;&#65292;&#23427;&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#36873;&#25321;&#29366;&#24577;&#65292;&#24182;&#29992;&#36825;&#20123;&#29366;&#24577;&#21021;&#22987;&#21270;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#20197;&#24341;&#23548;&#23427;&#36208;&#21521;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#20381;&#36182;&#20219;&#20309;&#20851;&#20110;&#29615;&#22659;&#30340;&#20808;&#39564;&#20449;&#24687;&#65306;&#65288;i&#65289;&#22235;&#36275;&#26426;&#22120;&#20154;&#31359;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#21644;&#65288;ii&#65289;&#22235;&#26059;&#32764;&#26080;&#20154;&#26426;&#22312;&#36187;&#36947;&#19978;&#39134;&#34892;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21021;&#22987;&#29366;&#24577;&#32531;&#20914;&#21306;&#22312;&#20219;&#21153;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#21516;&#26102;&#36824;&#21152;&#36895;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning, the trade-off between exploration and exploitation poses a complex challenge for achieving efficient learning from limited samples. While recent works have been effective in leveraging past experiences for policy updates, they often overlook the potential of reusing past experiences for data collection. Independent of the underlying RL algorithm, we introduce the concept of a Contrastive Initial State Buffer, which strategically selects states from past experiences and uses them to initialize the agent in the environment in order to guide it toward more informative states. We validate our approach on two complex robotic tasks without relying on any prior information about the environment: (i) locomotion of a quadruped robot traversing challenging terrains and (ii) a quadcopter drone racing through a track. The experimental results show that our initial state buffer achieves higher task performance than the nominal baseline while also speeding up training conv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#31070;&#32463;&#35745;&#31639;&#30340;&#26234;&#33021;&#26426;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29615;&#22659;&#20449;&#24687;&#30340;&#20027;&#35201;&#29305;&#24449;&#24182;&#24212;&#29992;&#30456;&#24212;&#30340;&#32534;&#30721;&#21050;&#28608;&#21040;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#22788;&#29702;&#26080;&#32467;&#26500;&#29615;&#22659;&#20449;&#24687;&#30340;&#31867;&#20154;&#33021;&#21147;&#65292;&#24182;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#29289;&#20307;&#25235;&#21462;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.08835</link><description>&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#31070;&#32463;&#35745;&#31639;&#65292;&#26234;&#33021;&#26426;&#22120;&#22312;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#24037;&#20316;
&lt;/p&gt;
&lt;p&gt;
Intelligent machines work in unstructured environments by differential neural computing. (arXiv:2309.08835v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#31070;&#32463;&#35745;&#31639;&#30340;&#26234;&#33021;&#26426;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29615;&#22659;&#20449;&#24687;&#30340;&#20027;&#35201;&#29305;&#24449;&#24182;&#24212;&#29992;&#30456;&#24212;&#30340;&#32534;&#30721;&#21050;&#28608;&#21040;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#22788;&#29702;&#26080;&#32467;&#26500;&#29615;&#22659;&#20449;&#24687;&#30340;&#31867;&#20154;&#33021;&#21147;&#65292;&#24182;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#29289;&#20307;&#25235;&#21462;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24076;&#26395;&#26234;&#33021;&#26426;&#22120;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#39640;&#25928;&#22320;&#24037;&#20316;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#22320;&#29702;&#35299;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#26080;&#32467;&#26500;&#20449;&#24687;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#23601;&#20687;&#20154;&#31867;&#19968;&#26679;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#38459;&#24615;&#31070;&#32463;&#35745;&#31639;&#30340;&#24863;&#30693;&#20449;&#21495;&#24046;&#20998;&#22788;&#29702;&#21644;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29615;&#22659;&#20449;&#24687;&#30340;&#20027;&#35201;&#29305;&#24449;&#24182;&#24212;&#29992;&#30456;&#20851;&#32534;&#30721;&#21050;&#28608;&#21040;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#33719;&#24471;&#20102;&#22788;&#29702;&#26080;&#32467;&#26500;&#29615;&#22659;&#20449;&#24687;&#30340;&#31867;&#20154;&#33021;&#21147;&#65292;&#22914;&#26426;&#26800;&#21050;&#28608;&#30340;&#25918;&#22823;&#65288;&gt;720%&#65289;&#21644;&#36866;&#24212;&#65288;&lt;50%&#65289;&#12290;&#35813;&#26041;&#27861;&#36824;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#22312;&#26234;&#33021;&#26426;&#22120;&#30340;&#20004;&#20010;&#20856;&#22411;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65306;&#29289;&#20307;&#25235;&#21462;&#21644;&#33258;&#21160;&#39550;&#39542;&#12290;&#22312;&#29289;&#20307;&#25235;&#21462;&#26041;&#38754;&#65292;&#36890;&#36807;&#22312;1&#27627;&#31186;&#20869;&#20351;&#29992;&#21333;&#20010;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#23398;&#20064;&#26410;&#30693;&#29289;&#20307;&#29305;&#24449;&#65288;&#20363;&#22914;&#23574;&#38160;&#30340;&#35282;&#21644;&#20809;&#28369;&#30340;&#34920;&#38754;&#65289;&#65292;&#19968;&#20010;&#26426;&#22120;&#25163;&#23454;&#29616;&#20102;&#23433;&#20840;&#31283;&#23450;&#30340;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expecting intelligent machines to efficiently work in real world requires a new method to understand unstructured information in unknown environments with good accuracy, scalability and generalization, like human. Here, a memristive neural computing based perceptual signal differential processing and learning method for intelligent machines is presented, via extracting main features of environmental information and applying associated encoded stimuli to memristors, we successfully obtain human-like ability in processing unstructured environmental information, such as amplification (&gt;720%) and adaptation (&lt;50%) of mechanical stimuli. The method also exhibits good scalability and generalization, validated in two typical applications of intelligent machines: object grasping and autonomous driving. In the former, a robot hand experimentally realizes safe and stable grasping, through learning unknown object features (e.g., sharp corner and smooth surface) with a single memristor in 1 ms. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cluster-Bootstrap PLS&#65288;CLUB-PLS&#65289;&#30340;&#22522;&#20110;&#20559;&#26368;&#23567;&#20108;&#20056;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24433;&#20687;&#36951;&#20256;&#23398;&#20013;&#30340;&#32500;&#24230;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#20004;&#20010;&#39046;&#22495;&#30340;&#22823;&#37327;&#36755;&#20837;&#32500;&#24230;&#21644;&#22823;&#26679;&#26412;&#37327;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#33258;&#21161;&#27861;&#25552;&#20379;&#31283;&#20581;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.07352</link><description>&lt;p&gt;
&#20351;&#29992;CLUB-PLS&#35299;&#20915;&#24433;&#20687;&#36951;&#20256;&#23398;&#20013;&#30340;&#32500;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling the dimensions in imaging genetics with CLUB-PLS. (arXiv:2309.07352v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cluster-Bootstrap PLS&#65288;CLUB-PLS&#65289;&#30340;&#22522;&#20110;&#20559;&#26368;&#23567;&#20108;&#20056;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24433;&#20687;&#36951;&#20256;&#23398;&#20013;&#30340;&#32500;&#24230;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#20004;&#20010;&#39046;&#22495;&#30340;&#22823;&#37327;&#36755;&#20837;&#32500;&#24230;&#21644;&#22823;&#26679;&#26412;&#37327;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#33258;&#21161;&#27861;&#25552;&#20379;&#31283;&#20581;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#20687;&#36951;&#20256;&#23398;&#21644;&#31867;&#20284;&#39046;&#22495;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#23558;&#19968;&#20010;&#39046;&#22495;&#65288;&#22914;&#36951;&#20256;&#25968;&#25454;&#65289;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#19982;&#21478;&#19968;&#20010;&#39046;&#22495;&#65288;&#22914;&#33041;&#25104;&#20687;&#25968;&#25454;&#65289;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#36827;&#34892;&#20851;&#32852;&#12290;&#35813;&#39046;&#22495;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#36890;&#36807;&#36951;&#20256;&#22240;&#32032;&#21644;&#25104;&#20687;&#34920;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#21333;&#21464;&#37327;&#20998;&#26512;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#27599;&#20010;&#39044;&#23450;&#20041;&#30340;&#25104;&#20687;&#27979;&#37327;&#25191;&#34892;&#19968;&#27425;&#20840;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;&#65288;GWAS&#65289;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#23427;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#34920;&#22411;&#24517;&#39035;&#20107;&#20808;&#23450;&#20041;&#22909;&#12290;&#22240;&#27492;&#65292;&#19981;&#34987;&#38480;&#21046;&#22312;&#39044;&#36873;&#24863;&#20852;&#36259;&#21306;&#22495;&#20869;&#25110;&#21453;&#26144;&#26356;&#22823;&#30340;&#25972;&#20010;&#22823;&#33041;&#27169;&#24335;&#30340;&#25928;&#24212;&#24456;&#23481;&#26131;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Cluster-Bootstrap PLS&#65288;CLUB-PLS&#65289;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#20004;&#20010;&#39046;&#22495;&#30340;&#22823;&#37327;&#36755;&#20837;&#32500;&#24230;&#20197;&#21450;&#22823;&#26679;&#26412;&#37327;&#12290;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#20351;&#29992;&#32858;&#31867;&#33258;&#21161;&#27861;&#20026;&#21333;&#20010;&#36755;&#20837;&#29305;&#24449;&#25552;&#20379;&#31283;&#20581;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in imaging genetics and similar fields is to link high-dimensional data in one domain, e.g., genetic data, to high dimensional data in a second domain, e.g., brain imaging data. The standard approach in the area are mass univariate analyses across genetic factors and imaging phenotypes. That entails executing one genome-wide association study (GWAS) for each pre-defined imaging measure. Although this approach has been tremendously successful, one shortcoming is that phenotypes must be pre-defined. Consequently, effects that are not confined to pre-selected regions of interest or that reflect larger brain-wide patterns can easily be missed. In this work we introduce a Partial Least Squares (PLS)-based framework, which we term Cluster-Bootstrap PLS (CLUB-PLS), that can work with large input dimensions in both domains as well as with large sample sizes. One key factor of the framework is to use cluster bootstrap to provide robust statistics for single input features in b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28508;&#22312;&#20960;&#20309;&#25628;&#32034;(NLGS)&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#36317;&#31163;&#26469;&#33258;&#21160;&#35782;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#26368;&#20339;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04810</link><description>&lt;p&gt;
&#31070;&#32463;&#28508;&#22312;&#20960;&#20309;&#25628;&#32034;&#65306;&#36890;&#36807;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#20449;&#24687;&#39537;&#21160;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#36827;&#34892;&#20056;&#31215;&#27969;&#24418;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Neural Latent Geometry Search: Product Manifold Inference via Gromov-Hausdorff-Informed Bayesian Optimization. (arXiv:2309.04810v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28508;&#22312;&#20960;&#20309;&#25628;&#32034;(NLGS)&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#36890;&#36807;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#36317;&#31163;&#26469;&#33258;&#21160;&#35782;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#26368;&#20339;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#19982;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#40784;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20855;&#26377;&#24658;&#23450;&#26354;&#29575;&#30340;&#21452;&#26354;&#21644;&#29699;&#24418;&#31354;&#38388;&#65292;&#25110;&#32773;&#23427;&#20204;&#30340;&#32452;&#21512;&#65292;&#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#28508;&#22312;&#31354;&#38388;&#24182;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#33258;&#21160;&#35782;&#21035;&#19979;&#28216;&#20219;&#21153;&#30340;&#26368;&#20339;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#38382;&#39064;&#36824;&#27809;&#26377;&#32473;&#20104;&#36275;&#22815;&#20851;&#27880;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#19978;&#23450;&#20041;&#20102;&#36825;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#31216;&#20026;&#31070;&#32463;&#28508;&#22312;&#20960;&#20309;&#25628;&#32034;(NLGS)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#36317;&#31163;&#30340;&#20505;&#36873;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#20043;&#38388;&#30340;&#26032;&#27010;&#24565;&#36317;&#31163;&#65292;&#20197;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#20026;&#20102;&#35745;&#31639;&#26684;&#32599;&#33707;&#22827;-&#35946;&#26031;&#22810;&#22827;&#36317;&#31163;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#26597;&#35810;&#35780;&#20272;&#25628;&#32034;&#30001;&#24658;&#23450;&#26354;&#29575;&#27169;&#22411;&#31354;&#38388;&#20056;&#31215;&#32452;&#25104;&#30340;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#30340;&#21407;&#21017;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research indicates that the performance of machine learning models can be improved by aligning the geometry of the latent space with the underlying data structure. Rather than relying solely on Euclidean space, researchers have proposed using hyperbolic and spherical spaces with constant curvature, or combinations thereof, to better model the latent space and enhance model performance. However, little attention has been given to the problem of automatically identifying the optimal latent geometry for the downstream task. We mathematically define this novel formulation and coin it as neural latent geometry search (NLGS). More specifically, we introduce a principled method that searches for a latent geometry composed of a product of constant curvature model spaces with minimal query evaluations. To accomplish this, we propose a novel notion of distance between candidate latent geometries based on the Gromov-Hausdorff distance from metric geometry. In order to compute the Gromov-Ha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#29616;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#33021;&#37327;&#20989;&#25968;&#19982;&#26377;&#25928;&#20234;&#36763;&#33258;&#26059;&#21704;&#23494;&#39039;&#37327;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#26029;&#22797;&#26434;&#25968;&#25454;&#20013;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.02292</link><description>&lt;p&gt;
&#20351;&#29992;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#25512;&#26029;&#26377;&#25928;&#32806;&#21512;
&lt;/p&gt;
&lt;p&gt;
Inferring effective couplings with Restricted Boltzmann Machines. (arXiv:2309.02292v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#29616;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#33021;&#37327;&#20989;&#25968;&#19982;&#26377;&#25928;&#20234;&#36763;&#33258;&#26059;&#21704;&#23494;&#39039;&#37327;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#26029;&#22797;&#26434;&#25968;&#25454;&#20013;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#25509;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#26088;&#22312;&#20197;&#27169;&#22411;&#30340;&#29627;&#23572;&#20857;&#26364;&#26435;&#37325;&#30340;&#27700;&#24179;&#20934;&#30830;&#37325;&#29616;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#25152;&#26377;&#32479;&#35745;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#25361;&#25112;&#26159;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#29289;&#29702;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#21463;&#38480;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#33021;&#37327;&#20989;&#25968;&#19982;&#21253;&#21547;&#33258;&#26059;&#20043;&#38388;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#26377;&#25928;&#20234;&#36763;&#33258;&#26059;&#21704;&#23494;&#39039;&#37327;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26144;&#23556;&#21253;&#25324;&#25152;&#26377;&#21487;&#33021;&#38454;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36229;&#36234;&#20102;&#36870;&#20234;&#36763;&#26041;&#27861;&#20013;&#36890;&#24120;&#32771;&#34385;&#30340;&#20256;&#32479;&#20108;&#27425;&#30456;&#20114;&#20316;&#29992;&#65292;&#20351;&#20854;&#33021;&#22815;&#25551;&#36848;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#35797;&#22270;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#26144;&#23556;&#27809;&#26377;&#27491;&#30830;&#22788;&#29702;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#27809;&#26377;&#20855;&#20307;&#30340;&#23454;&#38469;&#24212;&#29992;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models offer a direct way to model complex data. Among them, energy-based models provide us with a neural network model that aims to accurately reproduce all statistical correlations observed in the data at the level of the Boltzmann weight of the model. However, one challenge is to understand the physical interpretation of such models. In this study, we propose a simple solution by implementing a direct mapping between the energy function of the Restricted Boltzmann Machine and an effective Ising spin Hamiltonian that includes high-order interactions between spins. This mapping includes interactions of all possible orders, going beyond the conventional pairwise interactions typically considered in the inverse Ising approach, and allowing the description of complex datasets. Earlier works attempted to achieve this goal, but the proposed mappings did not do properly treat the complexity of the problem or did not contain direct prescriptions for practical application. To valid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.15363</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#36171;&#33021;&#25991;&#26412;&#21040;SQL&#30340;&#30740;&#31350;&#65306;&#19968;&#20010;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. (arXiv:2308.15363v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#33021;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#22522;&#20110;&#23454;&#39564;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#24182;&#23454;&#29616;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#24378;&#35843;&#20102;&#22312;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#20197;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#27492;&#22806;&#36824;&#23545;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#24212;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#36827;&#34892;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#30340;&#19968;&#31181;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#22522;&#20934;&#38459;&#30861;&#20102;&#35774;&#35745;&#26377;&#25928;&#12289;&#39640;&#25928;&#21644;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#21644;&#24191;&#27867;&#30340;&#27604;&#36739;&#65292;&#21253;&#25324;&#38382;&#39064;&#34920;&#31034;&#12289;&#31034;&#20363;&#36873;&#25321;&#21644;&#31034;&#20363;&#32452;&#32455;&#65292;&#24182;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#35814;&#32454;&#38416;&#36848;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;DAIL-SQL&#65292;&#21047;&#26032;&#20102;Spider&#27036;&#21333;&#65292;&#36798;&#21040;&#20102;86.6%&#30340;&#25191;&#34892;&#20934;&#30830;&#29575;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#26438;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#32463;&#27982;&#30340;LLM-based&#25991;&#26412;&#21040;SQL&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#24378;&#35843;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#35789;&#27719;&#25928;&#29575;&#65292;&#24182;&#22312;&#27492;&#24230;&#37327;&#19979;&#27604;&#36739;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24320;&#28304;LLMs&#65292;&#24182;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#30417;&#30563;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborates their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. Towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. Additionally, we investigate open-source LLMs in in-context learning, and further enhance their performance with task-specific superv
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32500;Bellman&#31639;&#23376;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.13049</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Exploration Networks. (arXiv:2308.13049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13049
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32500;Bellman&#31639;&#23376;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#20026;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#39034;&#24207;&#20915;&#31574;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#21644;&#20248;&#38597;&#30340;&#26041;&#27861;&#12290;&#26368;&#26174;&#33879;&#30340;&#26159;&#65292;&#36125;&#21494;&#26031;&#20195;&#29702;&#19981;&#20250;&#38754;&#20020;&#39057;&#29575;&#26041;&#27861;&#30340;&#25506;&#32034;/&#24320;&#21457;&#22256;&#22659;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#38382;&#39064;&#12290;&#36125;&#21494;&#26031;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23398;&#20064;&#36125;&#21494;&#26031;&#26368;&#20248;&#31574;&#30053;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#36825;&#22312;&#29609;&#20855;&#39046;&#22495;&#20013;&#26159;&#21487;&#35745;&#31639;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#19968;&#32500;Bellman&#31639;&#23376;&#20013;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#32780;&#19981;&#26159;&#22312;&#39640;&#32500;&#29366;&#24577;&#36716;&#31227;&#20998;&#24067;&#20013;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#25581;&#31034;&#20102;&#29616;&#26377;&#30340;&#26080;&#27169;&#22411;&#26041;&#27861;&#35201;&#20040;&#19981;&#36890;&#36807;MDP&#20256;&#25773;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#35201;&#20040;&#22312;&#19968;&#32452;&#35821;&#22659;&#31574;&#30053;&#20013;&#20248;&#21270;&#32780;&#19981;&#26159;&#25152;&#26377;&#21382;&#21490;&#26465;&#20214;&#31574;&#30053;&#12290;&#36825;&#20004;&#20010;&#36817;&#20284;&#24471;&#21040;&#30340;&#31574;&#30053;&#21487;&#33021;&#26159;&#20219;&#24847;&#36125;&#21494;&#26031;&#27425;&#20248;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36125;&#21494;&#26031;&#25506;&#32034;&#32593;&#32476;&#65288;Bayesian exploration network&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian reinforcement learning (RL) offers a principled and elegant approach for sequential decision making under uncertainty. Most notably, Bayesian agents do not face an exploration/exploitation dilemma, a major pathology of frequentist methods. A key challenge for Bayesian RL is the computational complexity of learning Bayes-optimal policies, which is only tractable in toy domains. In this paper we propose a novel model-free approach to address this challenge. Rather than modelling uncertainty in high-dimensional state transition distributions as model-based approaches do, we model uncertainty in a one-dimensional Bellman operator. Our theoretical analysis reveals that existing model-free approaches either do not propagate epistemic uncertainty through the MDP or optimise over a set of contextual policies instead of all history-conditioned policies. Both approximations yield policies that can be arbitrarily Bayes-suboptimal. To overcome these issues, we introduce the Bayesian explo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#19978;&#19979;&#25991;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#19982;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#65292;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#24178;&#39044;&#25514;&#26045;&#12290;&#35813;&#31639;&#27861;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07843</link><description>&lt;p&gt;
Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG]) &#35813;&#35770;&#25991;&#26631;&#39064;&#24050;&#32763;&#35793;&#65306;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#19978;&#19979;&#25991;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#19982;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#65292;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#24178;&#39044;&#25514;&#26045;&#12290;&#35813;&#31639;&#27861;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#21307;&#30103;&#26088;&#22312;&#36890;&#36807;&#22312;&#20010;&#20154;&#26085;&#24120;&#29983;&#27963;&#20013;&#25552;&#20379;&#24178;&#39044;&#26469;&#25552;&#39640;&#20581;&#24247;&#32467;&#26524;&#12290;&#29031;&#39038;&#20276;&#20387;&#21644;&#31038;&#20250;&#25903;&#25345;&#32593;&#32476;&#30340;&#21442;&#19982;&#32463;&#24120;&#22312;&#24110;&#21161;&#20010;&#20154;&#31649;&#29702;&#32321;&#37325;&#30340;&#21307;&#30103;&#26465;&#20214;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20026;&#31227;&#21160;&#21307;&#30103;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#35774;&#35745;&#38024;&#23545;&#20108;&#20803;&#20851;&#31995;&#8212;&#8212;&#30446;&#26631;&#20154;&#21644;&#20854;&#29031;&#39038;&#20276;&#20387;&#20043;&#38388;&#20851;&#31995;&#8212;&#8212;&#20197;&#25552;&#39640;&#31038;&#20250;&#25903;&#25345;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;Dyadic RL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#21450;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#20010;&#24615;&#21270;&#24178;&#39044;&#25514;&#26045;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#22810;&#32452;&#24178;&#39044;&#25514;&#26045;&#24433;&#21709;&#30528;&#20108;&#20803;&#20851;&#31995;&#22312;&#22810;&#20010;&#26102;&#38388;&#38388;&#38548;&#20869;&#12290;&#24320;&#21457;&#30340;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#12290;&#25105;&#20204;&#27491;&#24335;&#20171;&#32461;&#20102;&#38382;&#39064;&#35774;&#23450;&#65292;&#24320;&#21457;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#24182;&#30830;&#23450;&#20102;&#36951;&#25022;&#36793;&#30028;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation st
&lt;/p&gt;</description></item><item><title>REFORMS&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25253;&#21578;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#26377;&#25928;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#22833;&#36133;&#38382;&#39064;&#12290;&#36825;&#20010;&#26631;&#20934;&#30001;32&#20010;&#38382;&#39064;&#21644;&#19968;&#22871;&#25351;&#23548;&#26041;&#38024;&#32452;&#25104;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#21644;&#23454;&#26045;&#31185;&#30740;&#26102;&#30340;&#21442;&#32771;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2308.07832</link><description>&lt;p&gt;
REFORMS: &#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25253;&#21578;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
REFORMS: Reporting Standards for Machine Learning Based Science. (arXiv:2308.07832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07832
&lt;/p&gt;
&lt;p&gt;
REFORMS&#26159;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25253;&#21578;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#26377;&#25928;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#22833;&#36133;&#38382;&#39064;&#12290;&#36825;&#20010;&#26631;&#20934;&#30001;32&#20010;&#38382;&#39064;&#21644;&#19968;&#22871;&#25351;&#23548;&#26041;&#38024;&#32452;&#25104;&#65292;&#21487;&#20316;&#20026;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#21644;&#23454;&#26045;&#31185;&#30740;&#26102;&#30340;&#21442;&#32771;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#37319;&#29992;&#20063;&#20276;&#38543;&#30528;&#26377;&#25928;&#24615;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#30340;&#22833;&#36133;&#12290;&#36825;&#20123;&#22833;&#36133;&#21487;&#33021;&#20250;&#38459;&#30861;&#31185;&#23398;&#36827;&#23637;&#65292;&#23548;&#33268;&#23545;&#26080;&#25928;&#32467;&#35770;&#30340;&#38169;&#35823;&#20849;&#35782;&#65292;&#24182;&#21066;&#24369;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#30340;&#21487;&#20449;&#24230;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#19981;&#21516;&#23398;&#31185;&#20013;&#24120;&#24120;&#20197;&#30456;&#20284;&#30340;&#26041;&#24335;&#24212;&#29992;&#19988;&#22833;&#36133;&#12290;&#20986;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31185;&#23398;&#25552;&#20379;&#28165;&#26224;&#30340;&#25253;&#21578;&#26631;&#20934;&#12290;&#22522;&#20110;&#23545;&#36807;&#21435;&#25991;&#29486;&#30340;&#24191;&#27867;&#35780;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REFORMS&#26816;&#26597;&#34920;&#65288;$\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience&#65289;&#12290;&#23427;&#30001;32&#20010;&#38382;&#39064;&#21644;&#19968;&#22871;&#37197;&#22871;&#30340;&#25351;&#23548;&#26041;&#38024;&#32452;&#25104;&#12290;REFORMS&#26159;&#22522;&#20110;19&#20301;&#30740;&#31350;&#20154;&#21592;&#30340;&#20849;&#35782;&#24320;&#21457;&#30340;&#65292;&#36825;&#20123;&#20154;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#25968;&#25454;&#31185;&#23398;&#12289;&#25968;&#23398;&#12289;&#31038;&#20250;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#39046;&#22495;&#12290;REFORMS&#21487;&#20197;&#20026;&#30740;&#31350;&#20154;&#21592;&#22312;&#35774;&#35745;&#21644;&#23454;&#26045;&#31185;&#30740;&#26102;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) methods are proliferating in scientific research. However, the adoption of these methods has been accompanied by failures of validity, reproducibility, and generalizability. These failures can hinder scientific progress, lead to false consensus around invalid claims, and undermine the credibility of ML-based science. ML methods are often applied and fail in similar ways across disciplines. Motivated by this observation, our goal is to provide clear reporting standards for ML-based science. Drawing from an extensive review of past literature, we present the REFORMS checklist ($\textbf{Re}$porting Standards $\textbf{For}$ $\textbf{M}$achine Learning Based $\textbf{S}$cience). It consists of 32 questions and a paired set of guidelines. REFORMS was developed based on a consensus of 19 researchers across computer science, data science, mathematics, social sciences, and biomedical sciences. REFORMS can serve as a resource for researchers when designing and implementing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06534</link><description>&lt;p&gt;
&#35299;&#20915;&#21307;&#23398;&#24433;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23567;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#38382;&#39064;&#65306;&#23545;&#27604;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dealing with Small Annotated Datasets for Deep Learning in Medical Imaging: An Evaluation of Self-Supervised Pre-Training on CT Scans Comparing Contrastive and Masked Autoencoder Methods for Convolutional Models. (arXiv:2308.06534v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#27604;&#36739;&#20102;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#21644;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#26377;&#28508;&#21147;&#20943;&#23569;&#35786;&#26029;&#38169;&#35823;&#30340;&#39118;&#38505;&#12289;&#20943;&#36731;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#36127;&#25285;&#24182;&#21152;&#36895;&#30830;&#35786;&#12290;&#35757;&#32451;&#36825;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#22411;&#19988;&#20934;&#30830;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25552;&#20379;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#65292;&#30001;&#20110;&#27880;&#37322;&#30340;&#39640;&#22797;&#26434;&#24615;&#12289;&#21463;&#38480;&#30340;&#33719;&#21462;&#26041;&#24335;&#25110;&#30142;&#30149;&#30340;&#32597;&#35265;&#24615;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#36890;&#24120;&#24456;&#23567;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#22823;&#22411;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#23567;&#22411;&#30340;&#24050;&#27880;&#37322;&#25968;&#25454;&#38598;&#23601;&#36275;&#20197;&#23545;&#27169;&#22411;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#19979;&#28216;&#20219;&#21153;&#8221;&#12290;&#21307;&#23398;&#24433;&#20687;&#20013;&#26368;&#27969;&#34892;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#22522;&#20110;&#20849;&#21516;&#23545;&#27604;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#33258;&#28982;&#22270;&#20687;&#22788;&#29702;&#30740;&#31350;&#34920;&#26126;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20108;&#32773;&#22312;CT&#25195;&#25551;&#21367;&#31215;&#27169;&#22411;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning in medical imaging has the potential to minimize the risk of diagnostic errors, reduce radiologist workload, and accelerate diagnosis. Training such deep learning models requires large and accurate datasets, with annotations for all training samples. However, in the medical imaging domain, annotated datasets for specific tasks are often small due to the high complexity of annotations, limited access, or the rarity of diseases. To address this challenge, deep learning models can be pre-trained on large image datasets without annotations using methods from the field of self-supervised learning. After pre-training, small annotated datasets are sufficient to fine-tune the models for a specific task, the so-called ``downstream task". The most popular self-supervised pre-training approaches in medical imaging are based on contrastive learning. However, recent studies in natural image processing indicate a strong potential for masked autoencoder approaches. Our work compares sta
&lt;/p&gt;</description></item><item><title>Bandit&#21453;&#39304;&#19979;&#30340;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#30340;&#20851;&#38190;&#22312;&#20110;Bandit Littlestone&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#65292;&#26080;&#35770;&#26631;&#31614;&#31354;&#38388;&#26159;&#21542;&#26080;&#30028;&#12290;</title><link>http://arxiv.org/abs/2308.04620</link><description>&lt;p&gt;
&#22810;&#31867;&#22312;&#32447;&#23398;&#20064;&#22312;Bandit&#21453;&#39304;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multiclass Online Learnability under Bandit Feedback. (arXiv:2308.04620v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04620
&lt;/p&gt;
&lt;p&gt;
Bandit&#21453;&#39304;&#19979;&#30340;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#30340;&#20851;&#38190;&#22312;&#20110;Bandit Littlestone&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#65292;&#26080;&#35770;&#26631;&#31614;&#31354;&#38388;&#26159;&#21542;&#26080;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Bandit&#21453;&#39304;&#19979;&#30340;&#22810;&#31867;&#22312;&#32447;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;(daniely2013price)&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#23637;&#31034;Bandit Littlestone&#32500;&#24230;&#30340;&#26377;&#38480;&#24615;&#26159;&#22810;&#31867;&#22312;&#32447;&#23398;&#20064;&#30340;&#24517;&#35201;&#19988;&#20805;&#20998;&#26465;&#20214;&#65292;&#21363;&#20351;&#26631;&#31614;&#31354;&#38388;&#26159;&#26080;&#30028;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34917;&#20805;&#20102;(hanneke2023multiclass)&#30340;&#26368;&#36817;&#24037;&#20316;&#65292;&#20182;&#20204;&#22312;&#26631;&#31614;&#31354;&#38388;&#26080;&#30028;&#30340;&#20840;&#20449;&#24687;&#35774;&#32622;&#20013;&#65292;&#23637;&#31034;&#20102;Littlestone&#32500;&#24230;&#21051;&#30011;&#20102;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online multiclass classification under bandit feedback. We extend the results of (daniely2013price) by showing that the finiteness of the Bandit Littlestone dimension is necessary and sufficient for bandit online multiclass learnability even when the label space is unbounded. Our result complements the recent work by (hanneke2023multiclass) who show that the Littlestone dimension characterizes online multiclass learnability in the full-information setting when the label space is unbounded.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20934;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LOB&#25968;&#25454;&#30340;&#32929;&#31080;&#20215;&#26684;&#36235;&#21183;&#39044;&#27979;&#30340;15&#31181;&#26368;&#26032;DL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#38754;&#23545;&#26032;&#25968;&#25454;&#26102;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#65292;&#23545;&#20854;&#22312;&#23454;&#38469;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;&#24615;&#25552;&#20986;&#20102;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01915</link><description>&lt;p&gt;
&#22522;&#20110;LOB&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#36235;&#21183;&#39044;&#27979;&#65306;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LOB-Based Deep Learning Models for Stock Price Trend Prediction: A Benchmark Study. (arXiv:2308.01915v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20934;&#30740;&#31350;&#25506;&#35752;&#20102;&#22522;&#20110;LOB&#25968;&#25454;&#30340;&#32929;&#31080;&#20215;&#26684;&#36235;&#21183;&#39044;&#27979;&#30340;15&#31181;&#26368;&#26032;DL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#22312;&#38754;&#23545;&#26032;&#25968;&#25454;&#26102;&#24615;&#33021;&#26126;&#26174;&#19979;&#38477;&#65292;&#23545;&#20854;&#22312;&#23454;&#38469;&#24066;&#22330;&#20013;&#30340;&#24212;&#29992;&#24615;&#25552;&#20986;&#20102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30740;&#31350;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#38480;&#20215;&#35746;&#21333;&#31807;&#65288;LOB&#65289;&#25968;&#25454;&#30340;&#32929;&#31080;&#20215;&#26684;&#36235;&#21183;&#39044;&#27979;&#65288;SPTP&#65289;&#30340;&#21313;&#20116;&#31181;&#26368;&#26032; DL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;LOBCAST&#65292;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;DL &#27169;&#22411;&#35757;&#32451;&#12289;&#35780;&#20272;&#21644;&#21033;&#28070;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#26377;&#27169;&#22411;&#22312;&#38754;&#23545;&#26032;&#25968;&#25454;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#20174;&#32780;&#23545;&#23427;&#20204;&#22312;&#23454;&#38469;&#24066;&#22330;&#20013;&#30340;&#36866;&#29992;&#24615;&#25552;&#20986;&#20102;&#30097;&#38382;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20316;&#20026;&#19968;&#20010;&#22522;&#20934;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in Deep Learning (DL) research have notably influenced the finance sector. We examine the robustness and generalizability of fifteen state-of-the-art DL models focusing on Stock Price Trend Prediction (SPTP) based on Limit Order Book (LOB) data. To carry out this study, we developed LOBCAST, an open-source framework that incorporates data preprocessing, DL model training, evaluation and profit analysis. Our extensive experiments reveal that all models exhibit a significant performance drop when exposed to new data, thereby raising questions about their real-world market applicability. Our work serves as a benchmark, illuminating the potential and the limitations of current approaches and providing insight for innovative solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#23545;&#31216;&#24615;&#20462;&#25913;&#26435;&#37325;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#30340;&#29305;&#28857;&#65292;&#24076;&#26395;&#23558;&#23545;&#31216;&#24615;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#25512;&#24191;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;&#24212;&#29992;&#20110;ConvNet&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2308.01621</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#26032;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Novel Convolutional Neural Network Architecture with a Continuous Symmetry. (arXiv:2308.01621v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#23545;&#31216;&#24615;&#20462;&#25913;&#26435;&#37325;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#30340;&#29305;&#28857;&#65292;&#24076;&#26395;&#23558;&#23545;&#31216;&#24615;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#25512;&#24191;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35270;&#35282;&#24212;&#29992;&#20110;ConvNet&#30340;&#20998;&#26512;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(ConvNet)&#26550;&#26500;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#19968;&#31867;&#31216;&#20026;&#25311;&#32447;&#24615;&#21452;&#26354;&#31995;&#32479;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#23427;&#20801;&#35768;&#36890;&#36807;&#36830;&#32493;&#30340;&#23545;&#31216;&#24615;&#20462;&#25913;&#26435;&#37325;&#12290;&#36825;&#26159;&#19982;&#20256;&#32479;&#27169;&#22411;&#20013;&#22522;&#26412;&#22266;&#23450;&#30340;&#26550;&#26500;&#21644;&#26435;&#37325;&#30456;&#27604;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;&#25105;&#20204;&#24076;&#26395;&#23558;(&#20869;&#37096;)&#23545;&#31216;&#24615;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26399;&#26395;&#29305;&#24615;&#65292;&#24182;&#22312;&#26356;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#21560;&#24341;&#23545;PDE&#35270;&#35282;&#20998;&#26512;&#21644;&#35299;&#37322;ConvNet&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new Convolutional Neural Network (ConvNet) architecture inspired by a class of partial differential equations (PDEs) called quasi-linear hyperbolic systems. With comparable performance on image classification task, it allows for the modification of the weights via a continuous group of symmetry. This is a significant shift from traditional models where the architecture and weights are essentially fixed. We wish to promote the (internal) symmetry as a new desirable property for a neural network, and to draw attention to the PDE perspective in analyzing and interpreting ConvNets in the broader Deep Learning community.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#22256;&#38590;&#23454;&#20363;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;&#12290;&#36825;&#26041;&#27861;&#36866;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#21644;&#36817;&#20284;k.</title><link>http://arxiv.org/abs/2307.07604</link><description>&lt;p&gt;
&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#23545;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;
&lt;/p&gt;
&lt;p&gt;
Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes. (arXiv:2307.07604v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22635;&#20805;&#21644;&#32622;&#25442;&#25351;&#32441;&#32534;&#30721;&#30340;&#26041;&#27861;&#26469;&#20135;&#29983;&#22256;&#38590;&#23454;&#20363;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#25552;&#20379;&#24179;&#28369;&#19979;&#30028;&#12290;&#36825;&#26041;&#27861;&#36866;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#21644;&#36817;&#20284;k.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#32441;&#32534;&#30721;&#26041;&#27861;&#26159;&#26368;&#24191;&#27867;&#29992;&#20110;&#30830;&#23450;&#32422;&#26463;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25110;&#38169;&#35823;&#29575;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24046;&#20998;&#38544;&#31169;&#38382;&#39064;&#65292;&#25105;&#20204;&#24182;&#19981;&#30693;&#36947;&#36866;&#24403;&#30340;&#19979;&#30028;&#65292;&#24182;&#19988;&#21363;&#20351;&#23545;&#20110;&#25105;&#20204;&#30693;&#36947;&#30340;&#38382;&#39064;&#65292;&#19979;&#30028;&#20063;&#19981;&#24179;&#28369;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#35823;&#24046;&#22823;&#20110;&#26576;&#20010;&#38408;&#20540;&#26102;&#21464;&#24471;&#26080;&#24847;&#20041;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22635;&#20805;&#21644;&#32622;&#25442;&#36716;&#25442;&#24212;&#29992;&#20110;&#25351;&#32441;&#32534;&#30721;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#22256;&#38590;&#23454;&#20363;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#25552;&#20379;&#26032;&#30340;&#19979;&#30028;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65306;1. &#20302;&#20934;&#30830;&#24230;&#24773;&#26223;&#19979;&#24046;&#20998;&#38544;&#31169;&#24179;&#22343;&#38382;&#39064;&#30340;&#32039;&#23494;&#19979;&#30028;&#65292;&#36825;&#23588;&#20854;&#24847;&#21619;&#30528;&#26032;&#30340;&#31169;&#26377;1&#31751;&#38382;&#39064;&#30340;&#19979;&#30028; 2. &#36817;&#20284;k
&lt;/p&gt;
&lt;p&gt;
Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC 2014), are the most widely used method for establishing lower bounds on the sample complexity or error of approximately differentially private (DP) algorithms. Still, there are many problems in differential privacy for which we don't know suitable lower bounds, and even for problems that we do, the lower bounds are not smooth, and usually become vacuous when the error is larger than some threshold.  In this work, we present a simple method to generate hard instances by applying a padding-and-permuting transformation to a fingerprinting code. We illustrate the applicability of this method by providing new lower bounds in various settings:  1. A tight lower bound for DP averaging in the low-accuracy regime, which in particular implies a new lower bound for the private 1-cluster problem introduced by Nissim, Stemmer, and Vadhan (PODS 2016).  2. A lower bound on the additive error of DP algorithms for approximate k
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#31471;&#28857;&#35774;&#22791;&#19978;&#39044;&#27979;&#23567;&#20998;&#23376;&#28342;&#35299;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38745;&#24577;&#32593;&#31449;&#36816;&#34892;&#65292;&#21516;&#26102;&#20855;&#22791;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.05318</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#31471;&#28857;&#35774;&#22791;&#19978;&#39044;&#27979;&#23567;&#20998;&#23376;&#30340;&#28342;&#35299;&#24230;
&lt;/p&gt;
&lt;p&gt;
Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks. (arXiv:2307.05318v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05318
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#22312;&#31471;&#28857;&#35774;&#22791;&#19978;&#39044;&#27979;&#23567;&#20998;&#23376;&#28342;&#35299;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38745;&#24577;&#32593;&#31449;&#36816;&#34892;&#65292;&#21516;&#26102;&#20855;&#22791;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#28342;&#35299;&#24230;&#26159;&#19968;&#31181;&#26377;&#20215;&#20540;&#20294;&#38590;&#20197;&#39044;&#27979;&#30340;&#24615;&#36136;&#12290;&#20351;&#29992;&#19968;&#32423;&#21407;&#29702;&#26041;&#27861;&#35745;&#31639;&#28342;&#35299;&#24230;&#38656;&#35201;&#32771;&#34385;&#29109;&#21644;&#28947;&#30340;&#31454;&#20105;&#25928;&#24212;&#65292;&#23548;&#33268;&#35745;&#31639;&#26102;&#38388;&#36739;&#38271;&#19988;&#20934;&#30830;&#24615;&#30456;&#23545;&#36739;&#24046;&#12290;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#22914;&#28145;&#24230;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#27492;&#22806;&#65292;&#20219;&#20309;&#35745;&#31639;&#25216;&#26415;&#30340;&#26131;&#29992;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#23548;&#33268;&#32676;&#20307;&#36129;&#29486;&#26041;&#27861;&#30340;&#25345;&#32493;&#27969;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#20855;&#26377;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#22312;&#38745;&#24577;&#32593;&#31449;&#19978;&#36816;&#34892;&#65288;&#26080;&#38656;&#26381;&#21153;&#22120;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#35745;&#31639;&#38656;&#27714;&#36716;&#31227;&#21040;&#32593;&#31449;&#35775;&#38382;&#32773;&#36523;&#19978;&#65292;&#32780;&#19981;&#38656;&#35201;&#23433;&#35013;&#65292;&#28040;&#38500;&#20102;&#25903;&#20184;&#21644;&#32500;&#25252;&#26381;&#21153;&#22120;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28342;&#35299;&#24230;&#39044;&#27979;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21019;&#24314;&#24179;&#34913;&#28342;&#35299;&#24230;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aqueous solubility is a valuable yet challenging property to predict. Computing solubility using first-principles methods requires accounting for the competing effects of entropy and enthalpy, resulting in long computations for relatively poor accuracy. Data-driven approaches, such as deep learning, offer improved accuracy and computational efficiency but typically lack uncertainty quantification. Additionally, ease of use remains a concern for any computational technique, resulting in the sustained popularity of group-based contribution methods. In this work, we addressed these problems with a deep learning model with predictive uncertainty that runs on a static website (without a server). This approach moves computing needs onto the website visitor without requiring installation, removing the need to pay for and maintain servers. Our model achieves satisfactory results in solubility prediction. Furthermore, we demonstrate how to create molecular property prediction models that balanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20174;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#21644;&#27010;&#24565;&#22270;&#12290;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.17089</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Concept-Oriented Deep Learning with Large Language Models. (arXiv:2306.17089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20174;&#25991;&#26412;&#21644;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#21644;&#27010;&#24565;&#22270;&#12290;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#36798;&#20154;&#31867;&#30693;&#35782;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#25991;&#26412;&#29983;&#25104;&#21644;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#23427;&#20204;&#20063;&#26159;&#27010;&#24565;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#65288;CODL&#65289;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#21069;&#25552;&#26159;LLMs&#35201;&#29702;&#35299;&#27010;&#24565;&#24182;&#30830;&#20445;&#27010;&#24565;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#20197;&#21450;LLMs&#22312;CODL&#20013;&#30340;&#20027;&#35201;&#29992;&#36884;&#65292;&#21253;&#25324;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27010;&#24565;&#12289;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27010;&#24565;&#22270;&#21644;&#27010;&#24565;&#23398;&#20064;&#12290;&#20154;&#31867;&#30693;&#35782;&#21253;&#25324;&#31526;&#21495;&#65288;&#27010;&#24565;&#24615;&#65289;&#30693;&#35782;&#21644;&#20855;&#20307;&#65288;&#24863;&#24615;&#65289;&#30693;&#35782;&#12290;&#32780;&#20165;&#25991;&#26412;&#30340;LLMs&#21482;&#33021;&#34920;&#31034;&#31526;&#21495;&#65288;&#27010;&#24565;&#24615;&#65289;&#30693;&#35782;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22810;&#27169;&#24577;LLMs&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#30693;&#35782;&#30340;&#23436;&#25972;&#33539;&#22260;&#65288;&#27010;&#24565;&#24615;&#21644;&#24863;&#24615;&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#35270;&#35273;-&#35821;&#35328;LLMs&#20013;&#30340;&#27010;&#24565;&#29702;&#35299;&#65292;&#36825;&#26159;&#26368;&#37325;&#35201;&#30340;&#22810;&#27169;&#24577;LLMs&#65292;&#24182;&#20171;&#32461;&#20102;&#23427;&#20204;&#22312;CODL&#20013;&#30340;&#20027;&#35201;&#29992;&#36884;&#65292;&#21253;&#25324;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#12289;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#27010;&#24565;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been successfully used in many natural-language tasks and applications including text generation and AI chatbots. They also are a promising new technology for concept-oriented deep learning (CODL). However, the prerequisite is that LLMs understand concepts and ensure conceptual consistency. We discuss these in this paper, as well as major uses of LLMs for CODL including concept extraction from text, concept graph extraction from text, and concept learning. Human knowledge consists of both symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal LLMs, on the other hand, are capable of representing the full range (conceptual and sensory) of human knowledge. We discuss conceptual understanding in visual-language LLMs, the most important multimodal LLMs, and major uses of them for CODL including concept extraction from image, concept graph extraction from image
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.16740</link><description>&lt;p&gt;
&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Principles and Guidelines for Evaluating Social Robot Navigation Algorithms. (arXiv:2306.16740v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#31639;&#27861;&#30340;&#21407;&#21017;&#19982;&#25351;&#21335;&#65292;&#20026;&#35299;&#20915;&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#21644;&#21487;&#27604;&#36739;&#30340;&#22522;&#20934;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#23621;&#20303;&#29615;&#22659;&#20013;&#23548;&#33322;&#26159;&#37096;&#32626;&#26426;&#22120;&#20154;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#12290;&#34429;&#28982;&#31038;&#20132;&#23548;&#33322;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#35780;&#20272;&#35299;&#20915;&#31038;&#20132;&#23548;&#33322;&#30340;&#31639;&#27861;&#20173;&#28982;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#28041;&#21450;&#26426;&#22120;&#20154;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#31227;&#21160;&#65292;&#36824;&#28041;&#21450;&#21040;&#21160;&#24577;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#21450;&#20854;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#24863;&#30693;&#36866;&#24212;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#28165;&#26224;&#12289;&#21487;&#37325;&#22797;&#12289;&#26131;&#20110;&#33719;&#24471;&#30340;&#22522;&#20934;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20256;&#32479;&#26426;&#22120;&#20154;&#23548;&#33322;&#31561;&#39046;&#22495;&#21152;&#36895;&#20102;&#36827;&#23637;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20844;&#24179;&#27604;&#36739;&#31639;&#27861;&#65292;&#25581;&#31034;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21576;&#29616;&#26377;&#21069;&#36884;&#30340;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#30456;&#20449;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#21161;&#20110;&#31038;&#20132;&#23548;&#33322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#35780;&#20272;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#24314;&#31435;&#20102;&#20849;&#21516;&#12289;&#24191;&#27867;&#21487;&#29992;&#19988;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#26631;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#24049;&#30340;&#21019;&#26032;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge to deploying robots widely is navigation in human-populated environments, commonly referred to as social robot navigation. While the field of social navigation has advanced tremendously in recent years, the fair evaluation of algorithms that tackle social navigation remains hard because it involves not just robotic agents moving in static environments but also dynamic human agents and their perceptions of the appropriateness of robot behavior. In contrast, clear, repeatable, and accessible benchmarks have accelerated progress in fields like computer vision, natural language processing and traditional robot navigation by enabling researchers to fairly compare algorithms, revealing limitations of existing solutions and illuminating promising new directions. We believe the same approach can benefit social navigation. In this paper, we pave the road towards common, widely accessible, and repeatable benchmarking criteria to evaluate social robot navigation. Our contributio
&lt;/p&gt;</description></item><item><title>PEAR&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26041;&#27861;&#65292;&#29992;&#20110;Boosting&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#23545;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26469;&#29983;&#25104;&#39640;&#25928;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#26469;&#35757;&#32451;&#20998;&#23618;&#20195;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PEAR&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06394</link><description>&lt;p&gt;
PEAR: &#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#29992;&#20110;Boosting&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning. (arXiv:2306.06394v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06394
&lt;/p&gt;
&lt;p&gt;
PEAR&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26041;&#27861;&#65292;&#29992;&#20110;Boosting&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#23545;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#26469;&#29983;&#25104;&#39640;&#25928;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#26469;&#35757;&#32451;&#20998;&#23618;&#20195;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PEAR&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#21033;&#29992;&#26102;&#38388;&#25277;&#35937;&#21644;&#22686;&#21152;&#30340;&#25506;&#32034;&#24615;&#33021;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#26399;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#38750;&#38745;&#24577;&#24615;&#65292;&#20998;&#23618;&#20195;&#29702;&#38590;&#20197;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22987;&#25805;&#20316;&#30340;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#65288;PEAR&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#23569;&#37327;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#33258;&#36866;&#24212;&#37325;&#26631;&#35760;&#65292;&#20135;&#29983;&#39640;&#25928;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;&#65292;&#28982;&#21518;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#32852;&#21512;&#20248;&#21270;HRL&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#26469;$(i)$&#38480;&#21046;&#25105;&#20204;&#26041;&#27861;&#30340;&#27425;&#20248;&#24615;&#65292;&#21644;$(ii)$&#25512;&#23548;&#20986;&#20351;&#29992;RL&#21644;IL&#30340;&#24191;&#20041;&#21363;&#25554;&#21363;&#29992;&#30340;&#26694;&#26550;&#36827;&#34892;&#32852;&#21512;&#20248;&#21270;&#12290;PEAR&#20351;&#29992;&#19968;&#20123;&#19987;&#23478;&#28436;&#31034;&#65292;&#24182;&#23545;&#20219;&#21153;&#32467;&#26500;&#36827;&#34892;&#26368;&#23567;&#30340;&#38480;&#21046;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#19982;&#20856;&#22411;&#30340;&#27169;&#22411;&#33258;&#30001;RL&#31639;&#27861;&#38598;&#25104;&#65292;&#20135;&#29983;&#19968;&#20010;&#23454;&#29992;&#30340;HRL&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning (HRL) has the potential to solve complex long horizon tasks using temporal abstraction and increased exploration. However, hierarchical agents are difficult to train due to inherent non-stationarity. We present primitive enabled adaptive relabeling (PEAR), a two-phase approach where we first perform adaptive relabeling on a few expert demonstrations to generate efficient subgoal supervision, and then jointly optimize HRL agents by employing reinforcement learning (RL) and imitation learning (IL). We perform theoretical analysis to $(i)$ bound the sub-optimality of our approach, and $(ii)$ derive a generalized plug-and-play framework for joint optimization using RL and IL. PEAR uses a handful of expert demonstrations and makes minimal limiting assumptions on the task structure. Additionally, it can be easily integrated with typical model free RL algorithms to produce a practical HRL algorithm. We perform experiments on challenging robotic environments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04502</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#22522;&#20110;&#26799;&#24230;&#30340;&#24322;&#24120;&#20540;&#21435;&#38500;&#30340;&#22024;&#26434;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal. (arXiv:2306.04502v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AGRA&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#21487;&#38752;&#21644;&#39640;&#24615;&#33021;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#21363;&#20415;&#26159;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20063;&#20250;&#21253;&#21547;&#38169;&#35823;&#65292;&#26356;&#19981;&#29992;&#35828;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20102;&#12290;&#29616;&#26377;&#30340;&#19968;&#20123;&#25968;&#25454;&#21435;&#22122;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#26816;&#27979;&#24322;&#24120;&#20540;&#24182;&#36827;&#34892;&#27704;&#20037;&#24615;&#21435;&#38500;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23481;&#26131;&#36807;&#24230;&#25110;&#32773;&#27424;&#24230;&#36807;&#28388;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#24322;&#24120;&#20540;&#21435;&#38500;&#26041;&#27861;&#65288;AGRA&#65289;&#65292;&#19981;&#21516;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#28165;&#27927;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#27604;&#36739;&#19968;&#32452;&#26679;&#26412;&#30340;&#32047;&#31215;&#26799;&#24230;&#21644;&#21333;&#20010;&#26679;&#26412;&#30340;&#26799;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20915;&#23450;&#26159;&#21542;&#22312;&#24403;&#21069;&#26356;&#26032;&#26102;&#20445;&#30041;&#23545;&#24212;&#30340;&#26679;&#26412;&#65292;&#20197;&#27492;&#26469;&#30830;&#23450;&#23427;&#26159;&#21542;&#26377;&#21161;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;AGRA&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#20840;&#38754;&#30340;&#32467;&#26524;&#20998;&#26512;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
An accurate and substantial dataset is necessary to train a reliable and well-performing model. However, even manually labeled datasets contain errors, not to mention automatically labeled ones. The problem of data denoising was addressed in different existing research, most of which focuses on the detection of outliers and their permanent removal - a process that is likely to over- or underfilter the dataset. In this work, we propose AGRA: a new method for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior to model training, the dataset is adjusted during the training process. By comparing the aggregated gradient of a batch of samples and an individual example gradient, our method dynamically decides whether a corresponding example is helpful for the model at this point or is counter-productive and should be left out for the current update. Extensive evaluation on several datasets demonstrates the AGRA effectiveness, while comprehensive results analysis sup
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#23616;&#37096;-&#36828;&#31243;&#36965;&#25805;&#20316;&#20013;&#30340;&#26102;&#24310;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;PD&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#35843;&#25972;&#25511;&#21046;&#22120;&#21442;&#25968;&#65292;&#25913;&#21892;&#21516;&#27493;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#20811;&#26381;&#20102;&#38543;&#26426;&#24310;&#36831;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.16979</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;PD&#25511;&#21046;&#22312;&#20855;&#26377;&#38543;&#26426;&#26102;&#24310;&#30340;&#23616;&#37096;-&#36828;&#31243;&#36965;&#25805;&#20316;&#20013;
&lt;/p&gt;
&lt;p&gt;
Adaptive PD Control using Deep Reinforcement Learning for Local-Remote Teleoperation with Stochastic Time Delays. (arXiv:2305.16979v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#23616;&#37096;-&#36828;&#31243;&#36965;&#25805;&#20316;&#20013;&#30340;&#26102;&#24310;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;PD&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#35843;&#25972;&#25511;&#21046;&#22120;&#21442;&#25968;&#65292;&#25913;&#21892;&#21516;&#27493;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#20811;&#26381;&#20102;&#38543;&#26426;&#24310;&#36831;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;-&#36828;&#31243;&#31995;&#32479;&#20801;&#35768;&#26426;&#22120;&#20154;&#22312;&#21361;&#38505;&#29615;&#22659;&#20013;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#65292;&#22914;&#22826;&#31354;&#21644;&#26680;&#30005;&#31449;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#33021;&#23041;&#32961;&#21040;&#31995;&#32479;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#30340;&#26102;&#24310;&#65292;&#30830;&#31435;&#23616;&#37096;&#21644;&#36828;&#31243;&#35774;&#22791;&#20043;&#38388;&#30340;&#20934;&#30830;&#20301;&#32622;&#26144;&#23556;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#22686;&#24378;&#23616;&#37096;-&#36828;&#31243;&#31995;&#32479;&#30340;&#21516;&#27493;&#24615;&#21644;&#31283;&#23450;&#24615;&#23545;&#20110;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26356;&#36828;&#36317;&#31163;&#21644;&#39640;&#24230;&#25361;&#25112;&#30340;&#32593;&#32476;&#29615;&#22659;&#20013;&#19982;&#29615;&#22659;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#26102;&#24310;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#26102;&#24310;&#25511;&#21046;&#38382;&#39064;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#26102;&#35843;&#25972;&#25511;&#21046;&#22120;&#21442;&#25968;&#65292;&#36825;&#31181;&#33258;&#36866;&#24212;&#25511;&#21046;&#22120;&#21487;&#20197;&#34917;&#20607;&#38543;&#26426;&#24310;&#36831;&#24182;&#25913;&#21892;&#23616;&#37096;&#21644;&#36828;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#22120;&#20043;&#38388;&#30340;&#21516;&#27493;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#33258;&#36866;&#24212;PD&#25511;&#21046;&#22120;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#23558;&#22810;&#27493;&#24310;&#36831;&#32435;&#20837;&#21040;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local-remote systems allow robots to execute complex tasks in hazardous environments such as space and nuclear power stations. However, establishing accurate positional mapping between local and remote devices can be difficult due to time delays that can compromise system performance and stability. Enhancing the synchronicity and stability of local-remote systems is vital for enabling robots to interact with environments at greater distances and under highly challenging network conditions, including time delays. We introduce an adaptive control method employing reinforcement learning to tackle the time-delayed control problem. By adjusting controller parameters in real-time, this adaptive controller compensates for stochastic delays and improves synchronicity between local and remote robotic manipulators. To improve the adaptive PD controller's performance, we devise a model-based reinforcement learning approach that effectively incorporates multi-step delays into the learning framewor
&lt;/p&gt;</description></item><item><title>Marsellus&#26159;&#19968;&#27454;&#20855;&#26377;2&#33267;8&#20301;DNN&#21152;&#36895;&#21644;30%&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#24322;&#26500;RISC-V AI-IoT&#26411;&#31471;SoC&#65292;&#36866;&#29992;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#20197;&#21450;&#39640;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.08415</link><description>&lt;p&gt;
Marsellus: &#19968;&#27454;&#20855;&#26377;2&#33267;8&#20301;DNN&#21152;&#36895;&#21644;30%&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#24322;&#26500;RISC-V AI-IoT&#26411;&#31471;SoC
&lt;/p&gt;
&lt;p&gt;
Marsellus: A Heterogeneous RISC-V AI-IoT End-Node SoC with 2-to-8b DNN Acceleration and 30%-Boost Adaptive Body Biasing. (arXiv:2305.08415v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08415
&lt;/p&gt;
&lt;p&gt;
Marsellus&#26159;&#19968;&#27454;&#20855;&#26377;2&#33267;8&#20301;DNN&#21152;&#36895;&#21644;30%&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#24322;&#26500;RISC-V AI-IoT&#26411;&#31471;SoC&#65292;&#36866;&#29992;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#20197;&#21450;&#39640;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#20114;&#32852;&#29289;&#32852;&#32593;&#65288;AI-IoT&#65289;&#31995;&#32479;&#32423;&#33455;&#29255;&#65288;SoC&#65289;&#38656;&#35201;&#22312;&#33539;&#22260;&#24191;&#27867;&#30340;&#24037;&#20316;&#26465;&#20214;&#19979;&#65292;&#22312;&#20960;&#21313;&#27627;&#29926;&#30340;&#21151;&#32791;&#38480;&#21046;&#19979;&#36816;&#34892;&#35768;&#22810;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#23494;&#38598;&#22411;&#20294;&#24378;&#37327;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#29702;&#20197;&#21450;&#38656;&#35201;&#39640;&#31934;&#24230;&#28014;&#28857;&#36816;&#31639;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Marsellus&#65292;&#19968;&#20010;&#22312;GlobalFoundries 22nm FDX&#19978;&#21046;&#36896;&#30340;&#20840;&#25968;&#23383;&#24322;&#26500;SoC&#65292;&#29992;&#20110;AI-IoT&#26411;&#31471;&#33410;&#28857;&#65292;&#23427;&#32467;&#21512;&#20102;&#65306;1&#65289;&#19968;&#20010;16&#20010;RISC-V&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#65288;DSP&#65289;&#26680;&#24515;&#30340;&#36890;&#29992;&#38598;&#32676;&#65292;&#29992;&#20110;&#25191;&#34892;&#21508;&#31181;&#25903;&#25345;4&#20301;&#21644;2&#20301;&#31639;&#26415;&#25193;&#23637;&#65288;XpulpNN&#65289;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#21516;&#26102;&#32467;&#21512;&#20102;&#34701;&#21512;&#30340;MAC&#21644;LOAD&#25805;&#20316;&#21644;&#28014;&#28857;&#25903;&#25345;&#65307;2&#65289;&#19968;&#20010;2-8&#20301;&#21487;&#37325;&#26500;&#20108;&#36827;&#21046;&#24341;&#25806;&#65288;RBE&#65289;&#65292;&#29992;&#20110;&#21152;&#36895;DNN&#20013;&#30340;3x3&#21644;1x1&#65288;&#36880;&#28857;&#65289;&#21367;&#31215;&#65307;3&#65289;&#19968;&#32452;&#36830;&#25509;&#21040;&#33258;&#36866;&#24212;&#20307;&#20559;&#21270;&#30340;&#29255;&#19978;&#30417;&#35270;&#65288;OCM&#65289;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging Artificial Intelligence-enabled Internet-of-Things (AI-IoT) System-on-a-Chip (SoC) for augmented reality, personalized healthcare, and nano-robotics need to run many diverse tasks within a power envelope of a few tens of mW over a wide range of operating conditions: compute-intensive but strongly quantized Deep Neural Network (DNN) inference, as well as signal processing and control requiring high-precision floating-point. We present Marsellus, an all-digital heterogeneous SoC for AI-IoT end-nodes fabricated in GlobalFoundries 22nm FDX that combines 1) a general-purpose cluster of 16 RISC-V Digital Signal Processing (DSP) cores attuned for the execution of a diverse range of workloads exploiting 4-bit and 2-bit arithmetic extensions (XpulpNN), combined with fused MAC&amp;LOAD operations and floating-point support; 2) a 2-8bit Reconfigurable Binary Engine (RBE) to accelerate 3x3 and 1x1 (pointwise) convolutions in DNNs; 3) a set of On-Chip Monitoring (OCM) blocks connected to an Ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ACTC&#30340;&#26041;&#27861;&#65292;&#22312;&#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26102;&#36827;&#34892;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#20803;&#32452;&#26469;&#25214;&#21040;&#27599;&#20010;&#20851;&#31995;&#30340;&#26368;&#20339;&#38408;&#20540;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#26410;&#26631;&#35760;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.06395</link><description>&lt;p&gt;
ACTC: &#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion. (arXiv:2305.06395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ACTC&#30340;&#26041;&#27861;&#65292;&#22312;&#20919;&#21551;&#21160;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26102;&#36827;&#34892;&#20027;&#21160;&#38408;&#20540;&#26657;&#20934;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#20803;&#32452;&#26469;&#25214;&#21040;&#27599;&#20010;&#20851;&#31995;&#30340;&#26368;&#20339;&#38408;&#20540;&#65292;&#21516;&#26102;&#20063;&#32467;&#21512;&#26410;&#26631;&#35760;&#20803;&#32452;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;(KGC)&#20381;&#36182;&#20110;&#20272;&#35745;&#24471;&#20998;&#27169;&#22411;(&#23454;&#20307;&#65292;&#20851;&#31995;&#65292;&#23454;&#20307;)-&#20803;&#32452;&#65292;&#20363;&#22914;&#65292;&#36890;&#36807;&#23884;&#20837;&#21021;&#22987;&#30693;&#35782;&#22270;&#12290;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#38408;&#20540;(&#20351;&#29992;&#25163;&#21160;&#27880;&#37322;&#30340;&#31034;&#20363;)&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#27979;&#36136;&#37327;&#12290;&#26412;&#25991;&#23581;&#35797;&#39318;&#27425;&#38024;&#23545;KGC&#36827;&#34892;&#20919;&#21551;&#21160;&#26657;&#20934;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#21021;&#22987;&#27809;&#26377;&#27880;&#37322;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#21482;&#33021;&#36873;&#25321;&#26377;&#38480;&#25968;&#37327;&#30340;&#20803;&#32452;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;ACTC&#22522;&#20110;&#26377;&#38480;&#30340;&#27880;&#37322;&#20803;&#32452;&#26377;&#25928;&#22320;&#25214;&#21040;&#22909;&#30340;&#27599;&#20010;&#20851;&#31995;&#30340;&#38408;&#20540;&#12290;&#38500;&#20102;&#19968;&#20123;&#27880;&#37322;&#30340;&#20803;&#32452;&#22806;&#65292;ACTC&#36824;&#21033;&#29992;Logistic&#22238;&#24402;&#25110;&#39640;&#26031;&#36807;&#31243;&#20998;&#31867;&#22120;&#20272;&#35745;&#30340;&#26410;&#26631;&#35760;&#20803;&#32452;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23494;&#24230;&#21644;&#38543;&#26426;&#36873;&#25321;&#31561;&#19981;&#21516;&#26041;&#27861;&#36873;&#25321;&#20505;&#36873;&#20803;&#32452;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;&#35780;&#20998;&#27169;&#22411;&#21644;&#19968;&#20010;oracle&#27880;&#37322;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised knowledge-graph completion (KGC) relies on estimating a scoring model over (entity, relation, entity)-tuples, for example, by embedding an initial knowledge graph. Prediction quality can be improved by calibrating the scoring model, typically by adjusting the prediction thresholds using manually annotated examples. In this paper, we attempt for the first time cold-start calibration for KGC, where no annotated examples exist initially for calibration, and only a limited number of tuples can be selected for annotation. Our new method ACTC finds good per-relation thresholds efficiently based on a limited set of annotated tuples. Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers. We also experiment with different methods for selecting candidate tuples for annotation: density-based and random selection. Experiments with five scoring models and an oracle annotat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25945;&#31243;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#24110;&#21161;&#25552;&#21319;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36827;&#32780;&#20419;&#36827;&#20854;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#32972;&#26223;&#19979;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22312;&#24037;&#31243;&#35774;&#35745;&#21644;&#39044;&#27979;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.04933</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#24037;&#31243;&#35774;&#35745;&#19982;&#20581;&#24247;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31687;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification in Machine Learning for Engineering Design and Health Prognostics: A Tutorial. (arXiv:2305.04933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25945;&#31243;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#24110;&#21161;&#25552;&#21319;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36827;&#32780;&#20419;&#36827;&#20854;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#32972;&#26223;&#19979;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22312;&#24037;&#31243;&#35774;&#35745;&#21644;&#39044;&#27979;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#20316;&#20026;&#23433;&#20840;&#20445;&#38556;&#30340;&#22522;&#26412;&#23618;&#65292;&#21487;&#20197;&#36890;&#36807;&#21551;&#29992;&#21512;&#29702;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#26469;&#20419;&#36827;&#26356;&#21152;&#21407;&#21017;&#24615;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;UQ&#20351;&#24471;ML&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#24471;&#21040;&#25913;&#21892;&#65292;&#26377;&#28508;&#21147;&#26174;&#33879;&#20419;&#36827;&#39640;&#39118;&#38505;&#20915;&#31574;&#32972;&#26223;&#19979;&#30340;ML&#35299;&#20915;&#26041;&#26696;&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#21046;&#36896;&#19994;&#21644;&#33322;&#31354;&#31561;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20840;&#26041;&#20301;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26032;&#20852;UQ&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21450;&#36825;&#20123;UQ&#26041;&#27861;&#22312;&#35299;&#20915;&#24037;&#31243;&#35774;&#35745;&#21644;&#39044;&#27979;&#20581;&#24247;&#31649;&#29702;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#28041;&#21450;ML&#27169;&#22411;UQ&#30340;&#19981;&#30830;&#23450;&#24615;&#31867;&#22411;&#65292;&#26469;&#28304;&#21644;&#21407;&#22240;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;UQ&#26041;&#27861;&#30340;&#25945;&#31243;&#24335;&#25551;&#36848;&#65306;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
On top of machine learning models, uncertainty quantification (UQ) functions as an essential layer of safety assurance that could lead to more principled decision making by enabling sound risk assessment and management. The safety and reliability improvement of ML models empowered by UQ has the potential to significantly facilitate the broad adoption of ML solutions in high-stakes decision settings, such as healthcare, manufacturing, and aviation, to name a few. In this tutorial, we aim to provide a holistic lens on emerging UQ methods for ML models with a particular focus on neural networks and the applications of these UQ methods in tackling engineering design as well as prognostics and health management problems. Toward this goal, we start with a comprehensive classification of uncertainty types, sources, and causes pertaining to UQ of ML models. Next, we provide a tutorial-style description of several state-of-the-art UQ methods: Gaussian process regression, Bayesian neural network
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#22823;&#33041;&#36830;&#25509;&#32452;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19981;&#21516;&#30340;&#22270;&#21367;&#31215;&#22836;&#37096;&#65292;&#21253;&#25324;&#36793;&#32536;&#21644;&#33410;&#28857;&#65292;&#20840;&#38754;&#25429;&#25417;&#36755;&#20837;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24615;&#21035;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#33021;&#20174;&#36830;&#25509;&#32452;&#25968;&#25454;&#20013;&#25552;&#21462;&#20114;&#34917;&#21644;&#20195;&#34920;&#24615;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.02199</link><description>&lt;p&gt;
&#22810;&#22836;&#22270;&#21367;&#31215;&#32593;&#32476;&#22312;&#32467;&#26500;&#36830;&#25509;&#32452;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Head Graph Convolutional Network for Structural Connectome Classification. (arXiv:2305.02199v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#22836;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#22823;&#33041;&#36830;&#25509;&#32452;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#19981;&#21516;&#30340;&#22270;&#21367;&#31215;&#22836;&#37096;&#65292;&#21253;&#25324;&#36793;&#32536;&#21644;&#33410;&#28857;&#65292;&#20840;&#38754;&#25429;&#25417;&#36755;&#20837;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24615;&#21035;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#24182;&#33021;&#20174;&#36830;&#25509;&#32452;&#25968;&#25454;&#20013;&#25552;&#21462;&#20114;&#34917;&#21644;&#20195;&#34920;&#24615;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#22522;&#20110;&#25193;&#25955;&#30913;&#20849;&#25391;&#22270;&#20687;&#25552;&#21462;&#30340;&#22823;&#33041;&#36830;&#25509;&#25968;&#25454;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#22810;&#22836;&#24182;&#34892;GCN&#26426;&#21046;&#65292;&#20998;&#21035;&#23545;&#22823;&#33041;&#36830;&#25509;&#36755;&#20837;&#22270;&#36827;&#34892;&#22788;&#29702;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#35774;&#35745;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#22836;&#37096;&#65292;&#28041;&#21450;&#36793;&#32536;&#21644;&#33410;&#28857;&#30340;&#22270;&#21367;&#31215;&#65292;&#20805;&#20998;&#25429;&#25417;&#36755;&#20837;&#25968;&#25454;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#27169;&#22411;&#20174;&#22823;&#33041;&#36830;&#25509;&#25968;&#25454;&#20013;&#25552;&#21462;&#20114;&#34917;&#21644;&#20195;&#34920;&#24615;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#24615;&#21035;&#20998;&#31867;&#20219;&#21153;&#12290;&#36825;&#34920;&#24449;&#20102;&#36830;&#25509;&#32452;&#22312;&#24615;&#21035;&#26041;&#38754;&#30340;&#21464;&#21270;&#31243;&#24230;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#25105;&#20204;&#23545;&#20004;&#24615;&#20581;&#24247;&#21644;&#30142;&#30149;&#29702;&#35299;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;PREVENT-AD&#65288;347&#20010;&#21463;&#35797;&#32773;&#65289;&#21644;OASIS3&#65288;771&#20010;&#21463;&#35797;&#32773;&#65289;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle classification based on brain connectivity derived from diffusion magnetic resonance images. We propose a machine-learning model inspired by graph convolutional networks (GCNs), which takes a brain connectivity input graph and processes the data separately through a parallel GCN mechanism with multiple heads. The proposed network is a simple design that employs different heads involving graph convolutions focused on edges and nodes, capturing representations from the input data thoroughly. To test the ability of our model to extract complementary and representative features from brain connectivity data, we chose the task of sex classification. This quantifies the degree to which the connectome varies depending on the sex, which is important for improving our understanding of health and disease in both sexes. We show experiments on two publicly available datasets: PREVENT-AD (347 subjects) and OASIS3 (771 subjects). The proposed model demonstrates the highest performance compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeepAqua&#65292;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26469;&#20174;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#20013;&#20998;&#21106;&#27700;&#22495;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#30417;&#27979;&#28287;&#22320;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.01698</link><description>&lt;p&gt;
DeepAqua:&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#20174;SAR&#22270;&#20687;&#33258;&#25105;&#30417;&#30563;&#20998;&#21106;&#28287;&#22320;&#30340;&#35821;&#20041;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
DeepAqua: Self-Supervised Semantic Segmentation of Wetlands from SAR Images using Knowledge Distillation. (arXiv:2305.01698v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeepAqua&#65292;&#19968;&#31181;&#21033;&#29992;&#33258;&#25105;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#26469;&#20174;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#22270;&#20687;&#20013;&#20998;&#21106;&#27700;&#22495;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#30417;&#27979;&#28287;&#22320;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#25216;&#26415;&#36890;&#36807;&#23558;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#24212;&#29992;&#20110;&#21355;&#26143;&#22270;&#20687;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#20102;&#27700;&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#65292;&#35821;&#20041;&#20998;&#21106;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#22312;&#28287;&#22320;&#26816;&#27979;&#26041;&#38754;&#23588;&#20854;&#22256;&#38590;&#65292;&#22240;&#20026;&#27700;&#30340;&#33539;&#22260;&#38543;&#26102;&#38388;&#21644;&#31354;&#38388;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#65292;&#38656;&#35201;&#23545;&#21516;&#19968;&#21306;&#22495;&#36827;&#34892;&#22810;&#27425;&#27880;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeepAqua&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#28040;&#38500;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;DeepAqua&#21033;&#29992;&#24402;&#19968;&#21270;&#24046;&#24322;&#27700;&#25351;&#25968;&#65288;NDWI&#65289;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#65292;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20197;&#20998;&#21106;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#22270;&#20687;&#20013;&#30340;&#27700;&#12290;&#20026;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;&#20809;&#23398;&#21644;&#38647;&#36798;&#27700;&#36136;&#25513;&#34109;&#30456;&#37325;&#21472;&#30340;&#24773;&#20917;&#65292;&#23454;&#29616;&#20102;&#24320;&#25918;&#21450;&#26377;&#26893;&#34987;&#27700;&#38754;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#28287;&#22320;&#26816;&#27979;&#39046;&#22495;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#36827;&#27493;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;&#30417;&#27979;&#28287;&#22320;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Remote sensing has significantly advanced water detection by applying semantic segmentation techniques to satellite imagery. However, semantic segmentation remains challenging due to the substantial amount of annotated data required. This is particularly problematic in wetland detection, where water extent varies over time and space, necessitating multiple annotations for the same area. In this paper, we present DeepAqua, a self-supervised deep learning model that leverages knowledge distillation to eliminate the need for manual annotations during the training phase. DeepAqua utilizes the Normalized Difference Water Index (NDWI) as a teacher model to train a Convolutional Neural Network (CNN) for segmenting water from Synthetic Aperture Radar (SAR) images. To train the student model, we exploit cases where optical- and radar-based water masks coincide, enabling the detection of both open and vegetated water surfaces. Our model represents a significant advancement in computer vision tec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28304;&#20195;&#30721;&#35299;&#26512;&#25104;&#27969;&#22686;&#24378;&#25277;&#35937;&#35821;&#27861;&#26641;&#22270;&#24418;&#24335;&#65292;&#26500;&#36896;&#21508;&#31181;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#22270;&#23884;&#20837;&#65292;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#39640;&#25928;&#21033;&#29992;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2304.13032</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#27880;&#37322;&#22270;&#24418;&#25968;&#25454;&#24182;&#24212;&#29992;&#20110;&#36719;&#20214;&#20195;&#30721;&#24615;&#33021;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Unified Active Learning Framework for Annotating Graph Data with Application to Software Source Code Performance Prediction. (arXiv:2304.13032v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13032
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28304;&#20195;&#30721;&#35299;&#26512;&#25104;&#27969;&#22686;&#24378;&#25277;&#35937;&#35821;&#27861;&#26641;&#22270;&#24418;&#24335;&#65292;&#26500;&#36896;&#21508;&#31181;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#22270;&#23884;&#20837;&#65292;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#39640;&#25928;&#21033;&#29992;&#65292;&#24182;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#31243;&#24207;&#65292;&#21253;&#25324;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#24037;&#31243;&#65292;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#21644;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20123;&#21487;&#33021;&#20107;&#20808;&#24182;&#19981;&#21487;&#29992;&#12290;&#33719;&#21462;&#27880;&#37322;&#36890;&#24120;&#38656;&#35201;&#26174;&#30528;&#30340;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20351;&#24471;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#19987;&#38376;&#38024;&#23545;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;&#23558;&#28304;&#20195;&#30721;&#35299;&#26512;&#25104;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#24320;&#22987;&#65292;&#28982;&#21518;&#29992;&#25968;&#25454;&#21644;&#25511;&#21046;&#27969;&#36793;&#26469;&#22686;&#24378;&#23427;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#28304;&#20195;&#30721;&#30340;&#26641;&#24418;&#34920;&#31034;&#36716;&#25442;&#20026;&#27969;&#22686;&#24378;&#25277;&#35937;&#35821;&#27861;&#26641;&#22270;&#65288;FA-AST&#65289;&#34920;&#31034;&#27861;&#12290;&#22522;&#20110;&#22270;&#24418;&#34920;&#31034;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#22270;&#23884;&#20837;&#65288;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#65289;&#26500;&#36896;&#25104;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#12290;&#37492;&#20110;&#36825;&#26679;&#30340;&#23884;&#20837;&#65292;&#35813;&#26694;&#26550;&#21464;&#24471;&#20219;&#21153;&#19981;&#21487;&#30693;&#65292;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#22238;&#24402;&#26041;&#27861;&#21644;&#36866;&#29992;&#20110;&#22238;&#24402;&#30340;&#26597;&#35810;&#31574;&#30053;&#26469;&#25191;&#34892;&#20027;&#21160;&#23398;&#20064;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#29992;&#20110;&#36719;&#20214;&#24615;&#33021;&#39044;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26597;&#35810;&#31574;&#30053;&#21644;&#22238;&#24402;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning and data analytics applications, including performance engineering in software systems, require a large number of annotations and labelled data, which might not be available in advance. Acquiring annotations often requires significant time, effort, and computational resources, making it challenging. We develop a unified active learning framework, specializing in software performance prediction, to address this task. We begin by parsing the source code to an Abstract Syntax Tree (AST) and augmenting it with data and control flow edges. Then, we convert the tree representation of the source code to a Flow Augmented-AST graph (FA-AST) representation. Based on the graph representation, we construct various graph embeddings (unsupervised and supervised) into a latent space. Given such an embedding, the framework becomes task agnostic since active learning can be performed using any regression method and query strategy suited for regression. Within this framework, we in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;INDEED&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#19981;&#21516;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#30340;&#25805;&#20316;&#31526;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#19988;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.07993</link><description>&lt;p&gt;
&#20869;&#22312;&#19978;&#19979;&#25991;&#31639;&#23376;&#23398;&#20064;&#29992;&#20110;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
In-Context Operator Learning for Differential Equation Problems. (arXiv:2304.07993v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;INDEED&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#19981;&#21516;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#30340;&#25805;&#20316;&#31526;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#65292;&#19988;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#8212;&#8212;IN-context Differential Equation Encoder-Decoder&#65288;INDEED&#65289;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#21516;&#26102;&#23398;&#20064;&#25805;&#20316;&#31526;&#24182;&#22312;&#25512;&#29702;&#38454;&#27573;&#23558;&#20854;&#24212;&#29992;&#20110;&#26032;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#26435;&#37325;&#26356;&#26032;&#12290;&#29616;&#26377;&#26041;&#27861;&#23616;&#38480;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#29305;&#23450;&#30340;&#26041;&#31243;&#35299;&#25110;&#29305;&#23450;&#30340;&#25805;&#20316;&#31526;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#26041;&#31243;&#30340;&#26032;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#25805;&#20316;&#31526;&#23398;&#20064;&#22120;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#25670;&#33073;&#20026;&#26032;&#38382;&#39064;&#37325;&#26032;&#35757;&#32451;&#65288;&#29978;&#33267;&#24494;&#35843;&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#22256;&#25200;&#65292;&#36824;&#21487;&#20197;&#21033;&#29992;&#25805;&#20316;&#31526;&#20043;&#38388;&#20849;&#20139;&#30340;&#20849;&#21516;&#28857;&#65292;&#36825;&#26679;&#22312;&#23398;&#20064;&#26032;&#30340;&#25805;&#20316;&#31526;&#26102;&#21482;&#38656;&#35201;&#26497;&#23569;&#30340;&#28436;&#31034;&#21363;&#21487;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#33021;&#21147;&#65292;&#29992;&#20110;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#65292;&#21253;&#25324;ODE&#21644;PDE&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#65292;&#21516;&#26102;&#26174;&#31034;&#23427;&#21487;&#20197;&#25512;&#24191;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new neural-network-based approach, namely IN-context Differential Equation Encoder-Decoder (INDEED), to simultaneously learn operators from data and apply it to new questions during the inference stage, without any weight update. Existing methods are limited to using a neural network to approximate a specific equation solution or a specific operator, requiring retraining when switching to a new problem with different equations. By training a single neural network as an operator learner, we can not only get rid of retraining (even fine-tuning) the neural network for new problems, but also leverage the commonalities shared across operators so that only a few demos are needed when learning a new operator. Our numerical results show the neural network's capability as a few-shot operator learner for a diversified type of differential equation problems, including forward and inverse problems of ODEs and PDEs, and also show that it can generalize its learning capabilit
&lt;/p&gt;</description></item><item><title>BoundaryCAM&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#30340;&#24369;&#30417;&#30563;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#65292;&#23454;&#29616;&#31934;&#32454;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#25513;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.07853</link><description>&lt;p&gt;
BoundaryCAM&#65306;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#30340;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BoundaryCAM: A Boundary-based Refinement Framework for Weakly Supervised Semantic Segmentation of Medical Images. (arXiv:2303.07853v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07853
&lt;/p&gt;
&lt;p&gt;
BoundaryCAM&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#30028;&#30340;&#24369;&#30417;&#30563;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#65292;&#23454;&#29616;&#31934;&#32454;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#25513;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#21033;&#29992;&#22270;&#20687;&#32423;&#21035;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#26159;&#35299;&#20915;&#20998;&#21106;&#32593;&#32476;&#38656;&#27714;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22312;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22823;&#37327;&#20687;&#32032;&#32423;&#25513;&#27169;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#32423;WSSS&#25216;&#26415;&#32570;&#20047;&#23545;&#22270;&#20687;&#20013;&#21253;&#21547;&#30340;&#20960;&#20309;&#29305;&#24449;&#30340;&#29702;&#35299;&#65292;&#22240;&#20026;&#32593;&#32476;&#26080;&#27861;&#20174;&#20165;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#20013;&#23548;&#20986;&#20219;&#20309;&#23545;&#35937;&#36793;&#30028;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26032;&#22411;BoundaryCAM&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#31867;&#28608;&#27963;&#22270;&#32467;&#21512;&#21508;&#31181;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#31934;&#32454;&#30340;&#39640;&#31934;&#24230;&#20998;&#21106;&#25513;&#27169;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21487;&#29992;&#20110;&#26500;&#24314;&#36793;&#30028;&#22270;&#65292;&#20197;&#20351;BoundaryCAM&#33021;&#22815;&#39640;&#31934;&#24230;&#39044;&#27979;&#23545;&#35937;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision is a promising approach to deal with the need for Segmentation networks, especially for generating a large number of pixel-wise masks in a given dataset. However, most state-of-the-art image-level WSSS techniques lack an understanding of the geometric features embedded in the images since the network cannot derive any object boundary information from just image-level labels. We define a boundary here as the line separating an object and its background, or two different objects. To address this drawback, we propose our novel BoundaryCAM framework, which deploys state-of-the-art class activation maps combined with various post-processing techniques in order to achieve fine-grained higher-accuracy segmentation masks. To achieve this, we investigate a state-of-the-art unsupervised semantic segmentation network that can be used to construct a boundary map, which enables BoundaryCAM to predict object locations w
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#22312;&#22788;&#29702;&#24102;&#26377;&#22122;&#38899;&#26631;&#31614;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#26102;&#38754;&#20020;&#30528;&#22256;&#38590;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#29616;&#29366;&#21644;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#35270;&#35273;Transformer&#21644;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#26631;&#35760;&#26679;&#26412;&#36873;&#25321;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.11075</link><description>&lt;p&gt;
&#26631;&#39064;&#65306;&#26631;&#31614;&#22122;&#38899;&#23384;&#22312;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Active Learning in the Presence of Label Noise: A Survey. (arXiv:2302.11075v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11075
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#22312;&#22788;&#29702;&#24102;&#26377;&#22122;&#38899;&#26631;&#31614;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#26102;&#38754;&#20020;&#30528;&#22256;&#38590;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#29616;&#29366;&#21644;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#35270;&#35273;Transformer&#21644;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#26631;&#35760;&#26679;&#26412;&#36873;&#25321;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22312;&#39044;&#23450;&#20041;&#26631;&#31614;&#39044;&#31639;&#20869;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#22312;&#31163;&#32447;&#35774;&#32622;&#19979;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#21253;&#21547;&#22122;&#38899;&#26631;&#31614;&#30340;&#20998;&#31867;&#25968;&#25454;&#38598;&#26102;&#65292;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38754;&#20020;&#37325;&#22823;&#38382;&#39064;&#12290;&#22312;&#36825;&#31687;&#25991;&#29486;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26631;&#31614;&#22122;&#38899;&#23384;&#22312;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#29616;&#29366;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#29420;&#29305;&#30340;&#26041;&#27861;&#20197;&#21450;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#12290;&#38543;&#30528;&#35270;&#35273;Transformer&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#35201;&#30340;&#27010;&#36848;&#65292;&#24182;&#32771;&#34385;&#22914;&#20309;&#20351;&#29992;Transformer&#23618;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22686;&#24378;&#22810;&#26679;&#24615;&#12289;&#37325;&#35201;&#24615;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#26597;&#35810;&#36873;&#25321;&#65292;&#20197;&#21457;&#36865;&#32473;&#26631;&#31614;&#39044;&#27979;&#24072;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#25506;&#32034;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24471;&#21040;&#33021;&#22815;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;&#20013;&#36873;&#25321;&#39640;&#20215;&#20540;&#26679;&#26412;&#30340;&#33391;&#22909;&#22270;&#20687;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#35299;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep active learning has emerged as a powerful tool for training deep learning models within a predefined labeling budget. These models have achieved performances comparable to those trained in an offline setting. However, deep active learning faces substantial issues when dealing with classification datasets containing noisy labels. In this literature review, we discuss the current state of deep active learning in the presence of label noise, highlighting unique approaches, their strengths, and weaknesses. With the recent success of vision transformers in image classification tasks, we provide a brief overview and consider how the transformer layers and attention mechanisms can be used to enhance diversity, importance, and uncertainty-based selection in queries sent to an oracle for labeling. We further propose exploring contrastive learning methods to derive good image representations that can aid in selecting high-value samples for labeling in an active learning setting. We also hig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22810;&#26657;&#20934;&#38382;&#39064;&#25918;&#32622;&#22312;&#22810;&#30446;&#26631;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#24182;&#21033;&#29992;&#28216;&#25103;&#21160;&#21147;&#23398;&#30340;&#36830;&#25509;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#22810;&#26657;&#20934;&#23398;&#20064;&#38382;&#39064;&#30340;&#26368;&#26032;&#20445;&#35777;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29616;&#26377;&#20445;&#35777;&#24182;&#31616;&#21270;&#20102;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2302.10863</link><description>&lt;p&gt;
&#22810;&#26657;&#20934;&#30340;&#32479;&#19968;&#35270;&#35282;: &#22810;&#30446;&#26631;&#23398;&#20064;&#30340;&#28216;&#25103;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Unifying Perspective on Multi-Calibration: Game Dynamics for Multi-Objective Learning. (arXiv:2302.10863v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22810;&#26657;&#20934;&#38382;&#39064;&#25918;&#32622;&#22312;&#22810;&#30446;&#26631;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#24182;&#21033;&#29992;&#28216;&#25103;&#21160;&#21147;&#23398;&#30340;&#36830;&#25509;&#65292;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#22810;&#26657;&#20934;&#23398;&#20064;&#38382;&#39064;&#30340;&#26368;&#26032;&#20445;&#35777;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29616;&#26377;&#20445;&#35777;&#24182;&#31616;&#21270;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#21644;&#20998;&#26512;&#22810;&#26657;&#20934;&#30340;&#39044;&#27979;&#22120;&#12290;&#23558;&#22810;&#26657;&#20934;&#38382;&#39064;&#25918;&#32622;&#22312;&#22810;&#30446;&#26631;&#23398;&#20064;&#30340;&#19968;&#33324;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#19982;&#28216;&#25103;&#21160;&#21147;&#23398;&#30340;&#32852;&#31995;&#65292;&#22312;&#22810;&#26679;&#30340;&#22810;&#26657;&#20934;&#23398;&#20064;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20445;&#35777;&#12290;&#38500;&#20102;&#38416;&#26126;&#29616;&#26377;&#22810;&#26657;&#20934;&#20445;&#35777;&#24182;&#26497;&#22823;&#31616;&#21270;&#23427;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#20363;&#22914;&#33719;&#24471;&#38543;&#30528;&#32676;&#20307;&#35268;&#27169;&#30340;&#24179;&#26041;&#26681;&#32553;&#25918;&#30340;&#24378;&#22810;&#26657;&#20934;&#26465;&#20214;&#65292;&#24182;&#23558;$ k $-&#31867;&#22810;&#26657;&#20934;&#30340;&#22797;&#26434;&#24615;&#25552;&#39640;&#20102;$ k $&#30340;&#25351;&#25968;&#22240;&#23376;&#12290;&#38500;&#20102;&#22810;&#26657;&#20934;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#36825;&#20123;&#28216;&#25103;&#21160;&#21147;&#23398;&#26469;&#35299;&#20915;&#22312;&#32676;&#20307;&#20844;&#24179;&#21644;&#22810;&#20998;&#24067;&#23398;&#20064;&#30740;&#31350;&#20013;&#20986;&#29616;&#30340;&#32771;&#34385;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a unifying framework for the design and analysis of multicalibrated predictors. By placing the multicalibration problem in the general setting of multi-objective learning -- where learning guarantees must hold simultaneously over a set of distributions and loss functions -- we exploit connections to game dynamics to achieve state-of-the-art guarantees for a diverse set of multicalibration learning problems. In addition to shedding light on existing multicalibration guarantees and greatly simplifying their analysis, our approach also yields improved guarantees, such as obtaining stronger multicalibration conditions that scale with the square-root of group size and improving the complexity of $k$-class multicalibration by an exponential factor of $k$. Beyond multicalibration, we use these game dynamics to address emerging considerations in the study of group fairness and multi-distribution learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19981;&#21516;&#30340;&#20998;&#26512;&#31649;&#36947;&#23545;DNN&#22833;&#25928;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#26368;&#20339;&#31649;&#36947;&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;DBSCAN&#21644;UMAP&#65292;&#24182;&#20960;&#20046;&#21482;&#20135;&#29983;&#20102;&#19968;&#20010;&#31751;&#32676;&#12290;</title><link>http://arxiv.org/abs/2301.13506</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#25903;&#25345;&#22270;&#20687;&#22788;&#29702;DNN&#30340;&#23433;&#20840;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Supporting Safety Analysis of Image-processing DNNs through Clustering-based Approaches. (arXiv:2301.13506v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19981;&#21516;&#30340;&#20998;&#26512;&#31649;&#36947;&#23545;DNN&#22833;&#25928;&#30340;&#26681;&#26412;&#21407;&#22240;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#26368;&#20339;&#31649;&#36947;&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;DBSCAN&#21644;UMAP&#65292;&#24182;&#20960;&#20046;&#21482;&#20135;&#29983;&#20102;&#19968;&#20010;&#31751;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#25163;&#27573;&#35299;&#37322;&#20854;&#32467;&#26524;&#65288;&#23588;&#20854;&#26159;&#24403;&#32467;&#26524;&#38169;&#35823;&#26102;&#65289;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24448;&#24448;&#26080;&#27861;&#24212;&#29992;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30333;&#30418;&#26041;&#27861;&#65288;HUDD&#65289;&#21644;&#19968;&#31181;&#40657;&#30418;&#26041;&#27861;&#65288;SAFE&#65289;&#26469;&#33258;&#21160;&#21270;&#34920;&#24449;DNN&#30340;&#22833;&#25928;&#24773;&#20917;&#12290;&#23427;&#20204;&#37117;&#21487;&#20197;&#20174;&#19968;&#32452;&#28508;&#22312;&#24222;&#22823;&#30340;&#23548;&#33268;DNN&#22833;&#25928;&#30340;&#22270;&#20687;&#20013;&#35782;&#21035;&#30456;&#20284;&#30340;&#22270;&#20687;&#38598;&#32676;&#12290;&#28982;&#32780;&#65292;HUDD&#21644;SAFE&#30340;&#20998;&#26512;&#31649;&#36947;&#26159;&#25353;&#29031;&#24120;&#35268;&#23454;&#36341;&#23454;&#20363;&#21270;&#30340;&#65292;&#23558;&#20854;&#20182;&#31649;&#36947;&#30340;&#20998;&#26512;&#25512;&#36831;&#21040;&#23558;&#26469;&#30340;&#24037;&#20316;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;99&#31181;&#19981;&#21516;&#30340;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;DNN&#22833;&#25928;&#30340;&#31649;&#36947;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#12290;&#23427;&#20204;&#32467;&#21512;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#31070;&#32463;&#20803;&#30456;&#20851;&#24615;&#30340;&#28909;&#22270;&#65292;&#38477;&#32500;&#25216;&#26415;&#21644;&#19981;&#21516;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;&#30340;&#31649;&#36947;&#26159;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#65292;DBSCAN&#21644;UMAP&#65292;&#20960;&#20046;&#21482;&#20135;&#29983;&#20102;&#31751;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of deep neural networks (DNNs) in safety-critical contexts is often prevented by the lack of effective means to explain their results, especially when they are erroneous. In our previous work, we proposed a white-box approach (HUDD) and a black-box approach (SAFE) to automatically characterize DNN failures. They both identify clusters of similar images from a potentially large set of images leading to DNN failures. However, the analysis pipelines for HUDD and SAFE were instantiated in specific ways according to common practices, deferring the analysis of other pipelines to future work. In this paper, we report on an empirical evaluation of 99 different pipelines for root cause analysis of DNN failures. They combine transfer learning, autoencoders, heatmaps of neuron relevance, dimensionality reduction techniques, and different clustering algorithms. Our results show that the best pipeline combines transfer learning, DBSCAN, and UMAP. It leads to clusters almost exclusively
&lt;/p&gt;</description></item><item><title>RouteNet-Fermi&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#23450;&#20041;&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#30340;&#25490;&#38431;&#29702;&#35770;&#30456;&#27604;&#65292;&#22312;&#23384;&#22312;&#30495;&#23454;&#27969;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#30340;&#24310;&#36831;&#12289;&#25238;&#21160;&#21644;&#20002;&#21253;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.12070</link><description>&lt;p&gt;
RouteNet-Fermi: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32593;&#32476;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
RouteNet-Fermi: Network Modeling with Graph Neural Networks. (arXiv:2212.12070v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12070
&lt;/p&gt;
&lt;p&gt;
RouteNet-Fermi&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#23450;&#20041;&#27169;&#22411;&#65292;&#19982;&#20256;&#32479;&#30340;&#25490;&#38431;&#29702;&#35770;&#30456;&#27604;&#65292;&#22312;&#23384;&#22312;&#30495;&#23454;&#27969;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#23427;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#30340;&#24310;&#36831;&#12289;&#25238;&#21160;&#21644;&#20002;&#21253;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#27169;&#22411;&#26159;&#29616;&#20195;&#32593;&#32476;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#24191;&#27867;&#29992;&#20110;&#32593;&#32476;&#35268;&#21010;&#21644;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32593;&#32476;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#19968;&#20123;&#27169;&#22411;&#23384;&#22312;&#38480;&#21046;&#65292;&#22914;&#25490;&#38431;&#29702;&#35770;&#27169;&#22411;&#20013;&#23545;&#39532;&#23572;&#21487;&#22827;&#27969;&#37327;&#30340;&#20551;&#35774;&#65292;&#20197;&#21450;&#32593;&#32476;&#27169;&#25311;&#22120;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#27491;&#22312;&#25512;&#21160;&#19968;&#20195;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RouteNet-Fermi&#30340;&#33258;&#23450;&#20041;GNN&#27169;&#22411;&#65292;&#23427;&#19982;&#25490;&#38431;&#29702;&#35770;&#20855;&#26377;&#30456;&#21516;&#30340;&#30446;&#26631;&#65292;&#24182;&#19988;&#22312;&#23384;&#22312;&#30495;&#23454;&#27969;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#32593;&#32476;&#30340;&#24310;&#36831;&#12289;&#25238;&#21160;&#21644;&#20002;&#21253;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#32593;&#32476;&#35268;&#27169;&#65288;&#26368;&#22823;&#36798;&#21040;300&#20010;&#33410;&#28857;&#65289;&#21644;&#21253;&#25324;&#20855;&#26377;&#28151;&#21512;&#27969;&#37327;&#29305;&#24615;&#30340;&#26679;&#26412;&#65288;&#22914;&#22797;&#26434;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65289;&#20013;&#27979;&#35797;&#20102;RouteNet-Fermi&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network models are an essential block of modern networks. For example, they are widely used in network planning and optimization. However, as networks increase in scale and complexity, some models present limitations, such as the assumption of Markovian traffic in queuing theory models, or the high computational cost of network simulators. Recent advances in machine learning, such as Graph Neural Networks (GNN), are enabling a new generation of network models that are data-driven and can learn complex non-linear behaviors. In this paper, we present RouteNet-Fermi, a custom GNN model that shares the same goals as Queuing Theory, while being considerably more accurate in the presence of realistic traffic models. The proposed model predicts accurately the delay, jitter, and packet loss of a network. We have tested RouteNet-Fermi in networks of increasing size (up to 300 nodes), including samples with mixed traffic profiles -- e.g., with complex non-Markovian models -- and arbitrary routin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32479;&#35745;&#21644;&#35745;&#31639;&#36793;&#30028;&#26469;&#20445;&#35777;&#24433;&#21709;&#35786;&#26029;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;&#36870;-Hessian-&#21521;&#37327;&#20056;&#31215;&#23454;&#29616;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#20934;&#30830;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#28857;&#25110;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2212.04014</link><description>&lt;p&gt;
&#32479;&#35745;&#21644;&#35745;&#31639;&#20445;&#35777;&#20102;&#24433;&#21709;&#35786;&#26029;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Statistical and Computational Guarantees for Influence Diagnostics. (arXiv:2212.04014v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32479;&#35745;&#21644;&#35745;&#31639;&#36793;&#30028;&#26469;&#20445;&#35777;&#24433;&#21709;&#35786;&#26029;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;&#36870;-Hessian-&#21521;&#37327;&#20056;&#31215;&#23454;&#29616;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#20934;&#30830;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#28857;&#25110;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#35786;&#26029;&#65292;&#22914;&#24433;&#21709;&#20989;&#25968;&#21644;&#36817;&#20284;&#26368;&#22823;&#24433;&#21709;&#25200;&#21160;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290;&#24433;&#21709;&#35786;&#26029;&#26159;&#29992;&#20110;&#35782;&#21035;&#26377;&#24433;&#21709;&#21147;&#30340;&#25968;&#25454;&#28857;&#25110;&#25968;&#25454;&#23376;&#38598;&#30340;&#24378;&#22823;&#32479;&#35745;&#24037;&#20855;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#25928;&#30340;&#36870;-Hessian-&#21521;&#37327;&#20056;&#31215;&#23454;&#29616;&#24314;&#31435;&#20102;&#24433;&#21709;&#20989;&#25968;&#21644;&#36817;&#20284;&#26368;&#22823;&#24433;&#21709;&#25200;&#21160;&#30340;&#26377;&#38480;&#26679;&#26412;&#32479;&#35745;&#30028;&#38480;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#22522;&#20110;&#22823;&#35268;&#27169;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influence diagnostics such as influence functions and approximate maximum influence perturbations are popular in machine learning and in AI domain applications. Influence diagnostics are powerful statistical tools to identify influential datapoints or subsets of datapoints. We establish finite-sample statistical bounds, as well as computational complexity bounds, for influence functions and approximate maximum influence perturbations using efficient inverse-Hessian-vector product implementations. We illustrate our results with generalized linear models and large attention based models on synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32508;&#36848;&#20102;&#36793;&#32536;&#35270;&#39057;&#20998;&#26512;&#65288;EVA&#65289;&#30340;&#24212;&#29992;&#12289;&#31995;&#32479;&#21644;&#25903;&#25345;&#25216;&#26415;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#21644;&#20114;&#32852;&#35774;&#22791;&#30340;&#26222;&#21450;&#65292;EVA&#20316;&#20026;&#19968;&#31181;&#22312;&#32593;&#32476;&#36793;&#32536;&#22788;&#29702;&#35270;&#39057;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2211.15751</link><description>&lt;p&gt;
&#36793;&#32536;&#35270;&#39057;&#20998;&#26512;&#65306;&#24212;&#29992;&#12289;&#31995;&#32479;&#21644;&#25903;&#25345;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Edge Video Analytics: A Survey on Applications, Systems and Enabling Techniques. (arXiv:2211.15751v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15751
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32508;&#36848;&#20102;&#36793;&#32536;&#35270;&#39057;&#20998;&#26512;&#65288;EVA&#65289;&#30340;&#24212;&#29992;&#12289;&#31995;&#32479;&#21644;&#25903;&#25345;&#25216;&#26415;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#21644;&#20114;&#32852;&#35774;&#22791;&#30340;&#26222;&#21450;&#65292;EVA&#20316;&#20026;&#19968;&#31181;&#22312;&#32593;&#32476;&#36793;&#32536;&#22788;&#29702;&#35270;&#39057;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20316;&#20026;&#20840;&#29699;&#25968;&#23383;&#20449;&#24687;&#29190;&#28856;&#30340;&#20851;&#38190;&#39537;&#21160;&#21147;&#65292;&#21487;&#20197;&#20026;&#20154;&#31867;&#31038;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#22909;&#22788;&#12290;&#25919;&#24220;&#21644;&#20225;&#19994;&#27491;&#22312;&#37096;&#32626;&#26080;&#25968;&#25668;&#20687;&#22836;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#25191;&#27861;&#12289;&#24212;&#24613;&#31649;&#29702;&#12289;&#20132;&#36890;&#25511;&#21046;&#21644;&#23433;&#20840;&#30417;&#25511;&#65292;&#36825;&#20123;&#37117;&#26159;&#30001;&#35270;&#39057;&#20998;&#26512;&#65288;VA&#65289;&#25152;&#25903;&#25345;&#12290;&#28145;&#24230;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#23545;&#35937;&#20998;&#31867;&#12289;&#26816;&#27979;&#21644;&#36319;&#36394;&#30340;&#27169;&#22411;&#26356;&#21152;&#31934;&#30830;&#12290;&#21516;&#26102;&#65292;&#38543;&#30528;&#20114;&#32852;&#35774;&#22791;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#27599;&#22825;&#37117;&#20250;&#20135;&#29983;&#28023;&#37327;&#25968;&#25454;&#65292;&#36229;&#36234;&#20102;&#20113;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#36793;&#32536;&#35745;&#31639;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33539;&#24335;&#65292;&#23427;&#23558;&#24037;&#20316;&#36127;&#36733;&#21644;&#26381;&#21153;&#20174;&#32593;&#32476;&#26680;&#24515;&#31227;&#21160;&#21040;&#32593;&#32476;&#36793;&#32536;&#65292;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26032;&#20132;&#21449;&#39046;&#22495;&#65292;&#21363;&#36793;&#32536;&#35270;&#39057;&#20998;&#26512;&#65288;EVA&#65289;&#65292;&#24320;&#22987;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#21482;&#26377;&#23569;&#25968;&#26494;&#25955;&#30456;&#20851;&#30340;&#32508;&#36848;&#23384;&#22312;&#12290;EVA&#30340;&#22522;&#26412;&#27010;&#24565;&#65288;&#20363;&#22914;
&lt;/p&gt;
&lt;p&gt;
Video, as a key driver in the global explosion of digital information, can create tremendous benefits for human society. Governments and enterprises are deploying innumerable cameras for a variety of applications, e.g., law enforcement, emergency management, traffic control, and security surveillance, all facilitated by video analytics (VA). This trend is spurred by the rapid advancement of deep learning (DL), which enables more precise models for object classification, detection, and tracking. Meanwhile, with the proliferation of Internet-connected devices, massive amounts of data are generated daily, overwhelming the cloud. Edge computing, an emerging paradigm that moves workloads and services from the network core to the network edge, has been widely recognized as a promising solution. The resulting new intersection, edge video analytics (EVA), begins to attract widespread attention. Nevertheless, only a few loosely-related surveys exist on this topic. The basic concepts of EVA (e.g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#33258;&#36866;&#24212;&#36864;&#28779;&#30340;&#26041;&#27861;&#65292;&#23558;&#33258;&#23398;&#20064;Monte Carlo&#65288;SLMC&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#22810;&#23792;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;Monte Carlo&#26356;&#26032;&#65292;&#24182;&#35299;&#20915;&#20102;&#24314;&#35758;&#27169;&#22411;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.14024</link><description>&lt;p&gt;
&#36808;&#21521;&#26080;&#38480;&#33258;&#23398;&#20064;&#30340;&#24182;&#34892;&#33258;&#36866;&#24212;&#36864;&#28779;&#30340;MCMC&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Toward Unlimited Self-Learning MCMC with Parallel Adaptive Annealing. (arXiv:2211.14024v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14024
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24182;&#34892;&#33258;&#36866;&#24212;&#36864;&#28779;&#30340;&#26041;&#27861;&#65292;&#23558;&#33258;&#23398;&#20064;Monte Carlo&#65288;SLMC&#65289;&#26041;&#27861;&#24212;&#29992;&#20110;&#22810;&#23792;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;Monte Carlo&#26356;&#26032;&#65292;&#24182;&#35299;&#20915;&#20102;&#24314;&#35758;&#27169;&#22411;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#23398;&#20064;Monte Carlo&#65288;SLMC&#65289;&#26041;&#27861;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21152;&#36895;Markov&#38142;Monte Carlo&#65288;MCMC&#65289;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#28508;&#22312;&#29983;&#25104;&#27169;&#22411;&#65292;SLMC&#26041;&#27861;&#23454;&#29616;&#20102;&#20855;&#26377;&#36739;&#23569;&#33258;&#30456;&#20851;&#24615;&#30340;&#39640;&#25928;Monte Carlo&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;SLMC&#26041;&#27861;&#38590;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#22810;&#23792;&#20998;&#24067;&#65292;&#20854;&#20013;&#38590;&#20197;&#33719;&#24471;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24182;&#34892;&#33258;&#36866;&#24212;&#36864;&#28779;&#26041;&#27861;&#65292;&#23427;&#20351;SLMC&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#20855;&#26377;&#36880;&#28176;&#35757;&#32451;&#30340;&#24314;&#35758;&#20998;&#24067;&#30340;&#22810;&#23792;&#20998;&#24067;&#12290;&#24182;&#34892;&#33258;&#36866;&#24212;&#36864;&#28779;&#22522;&#20110;&#65288;i&#65289;&#24102;&#26377;&#36864;&#28779;&#30340;&#39034;&#24207;&#23398;&#20064;&#65292;&#20197;&#32487;&#25215;&#21644;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#65292;&#65288;ii&#65289;&#33258;&#36866;&#24212;&#36864;&#28779;&#65292;&#20197;&#33258;&#21160;&#26816;&#27979;&#27424;&#23398;&#20064;&#65292;&#65288;iii&#65289;&#24182;&#34892;&#36864;&#28779;&#65292;&#20197;&#20943;&#36731;&#24314;&#35758;&#27169;&#22411;&#30340;&#27169;&#24335;&#23849;&#28291;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#20316;&#20026;SLMC&#24314;&#35758;&#30340;VAE-SLMC&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#24182;&#34892;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-learning Monte Carlo (SLMC) methods are recently proposed to accelerate Markov chain Monte Carlo (MCMC) methods using a machine learning model. With latent generative models, SLMC methods realize efficient Monte Carlo updates with less autocorrelation. However, SLMC methods are difficult to directly apply to multimodal distributions for which training data are difficult to obtain. To solve the limitation, we propose parallel adaptive annealing, which makes SLMC methods directly apply to multimodal distributions with a gradually trained proposal while annealing target distribution. Parallel adaptive annealing is based on (i) sequential learning with annealing to inherit and update the model parameters, (ii) adaptive annealing to automatically detect under-learning, and (iii) parallel annealing to mitigate mode collapse of proposal models. We also propose VAE-SLMC method which utilizes a variational autoencoder (VAE) as a proposal of SLMC to make efficient parallel proposals indepen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38598;&#25104;&#36816;&#21160;&#21644;&#36830;&#32493;&#24615;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#29289;&#20307;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#20013;&#20135;&#29983;&#26356;&#20248;&#30340;&#29289;&#20307;&#32534;&#30721;&#65292;&#24182;&#22312;&#29289;&#20307;&#21457;&#29616;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#24635;&#20307;&#28508;&#22312;&#29289;&#20307;&#34920;&#31034;&#31561;&#26041;&#38754;&#21462;&#24471;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.09771</link><description>&lt;p&gt;
&#36890;&#36807;&#36816;&#21160;&#21644;&#29289;&#20307;&#36830;&#32493;&#24615;&#25552;&#21319;&#29289;&#20307;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Object Representation Learning via Motion and Object Continuity. (arXiv:2211.09771v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09771
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#36816;&#21160;&#21644;&#36830;&#32493;&#24615;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#21319;&#29289;&#20307;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#20013;&#20135;&#29983;&#26356;&#20248;&#30340;&#29289;&#20307;&#32534;&#30721;&#65292;&#24182;&#22312;&#29289;&#20307;&#21457;&#29616;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#24635;&#20307;&#28508;&#22312;&#29289;&#20307;&#34920;&#31034;&#31561;&#26041;&#38754;&#21462;&#24471;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#22312;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25913;&#36827;&#65292;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#26032;&#39062;&#30340;&#26550;&#26500;&#20559;&#35265;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#21487;&#33021;&#20026;&#19979;&#28216;&#20219;&#21153;&#20135;&#29983;&#27425;&#20248;&#30340;&#29289;&#20307;&#32534;&#30721;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#29289;&#20307;&#36816;&#21160;&#21644;&#36830;&#32493;&#24615;&#65292;&#21363;&#29289;&#20307;&#19981;&#33021;&#31361;&#28982;&#20986;&#29616;&#21644;&#28040;&#22833;&#12290;&#36825;&#36890;&#36807;&#20004;&#20010;&#26426;&#21046;&#23454;&#29616;&#65306;&#65288;i&#65289;&#36890;&#36807;&#38598;&#25104;&#20809;&#27969;&#25552;&#20379;&#29289;&#20307;&#20301;&#32622;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#65288;ii&#65289;&#23545;&#36830;&#32493;&#22270;&#20687;&#24103;&#20043;&#38388;&#30340;&#23545;&#27604;&#29289;&#20307;&#36830;&#32493;&#24615;&#25439;&#22833;&#36827;&#34892;&#24314;&#27169;&#12290;&#19982;&#24320;&#21457;&#26174;&#24335;&#28145;&#24230;&#26550;&#26500;&#19981;&#21516;&#65292;&#24471;&#21040;&#30340;&#36816;&#21160;&#21644;&#29289;&#20307;&#36830;&#32493;&#24615;&#65288;MOC&#65289;&#26041;&#26696;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#22522;&#32447;&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29609;Atari&#28216;&#25103;&#32780;&#35328;&#65292;SOTA&#27169;&#22411;&#22312;&#29289;&#20307;&#21457;&#29616;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#24635;&#20307;&#28508;&#22312;&#29289;&#20307;&#34920;&#31034;&#26041;&#38754;&#37117;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25972;&#21512;&#36816;&#21160;&#21644;&#36830;&#32493;&#24615;&#30340;&#26126;&#26174;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent unsupervised multi-object detection models have shown impressive performance improvements, largely attributed to novel architectural inductive biases. Unfortunately, they may produce suboptimal object encodings for downstream tasks. To overcome this, we propose to exploit object motion and continuity, i.e., objects do not pop in and out of existence. This is accomplished through two mechanisms: (i) providing priors on the location of objects through integration of optical flow, and (ii) a contrastive object continuity loss across consecutive image frames. Rather than developing an explicit deep architecture, the resulting Motion and Object Continuity (MOC) scheme can be instantiated using any baseline object detection model. Our results show large improvements in the performances of a SOTA model in terms of object discovery, convergence speed and overall latent object representations, particularly for playing Atari games. Overall, we show clear benefits of integrating motion and
&lt;/p&gt;</description></item><item><title>DyG2Vec&#26159;&#19968;&#20010;&#20855;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#30340;&#21160;&#24577;&#22270;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#21644;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21462;&#20016;&#23500;&#30340;&#26102;&#38388;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.16906</link><description>&lt;p&gt;
DyG2Vec: &#24102;&#26377;&#33258;&#30417;&#30563;&#30340;&#21160;&#24577;&#22270;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DyG2Vec: Representation Learning for Dynamic Graphs with Self-Supervision. (arXiv:2210.16906v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16906
&lt;/p&gt;
&lt;p&gt;
DyG2Vec&#26159;&#19968;&#20010;&#20855;&#26377;&#33258;&#30417;&#30563;&#23398;&#20064;&#33021;&#21147;&#30340;&#21160;&#24577;&#22270;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;&#21644;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#21462;&#20016;&#23500;&#30340;&#26102;&#38388;&#23884;&#20837;&#34920;&#31034;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#36890;&#36807;&#33258;&#21160;&#25552;&#21462;&#26102;&#38388;&#27169;&#24335;&#26469;&#23398;&#20064;&#24402;&#32435;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#24076;&#26395;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#24120;&#24120;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#35760;&#24518;&#27169;&#22359;&#25110;&#20302;&#25928;&#30340;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#26469;&#26500;&#24314;&#26102;&#38388;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#21160;&#24577;&#22270;&#32534;&#30721;&#22120;&#19981;&#23481;&#26131;&#36866;&#24212;&#33258;&#30417;&#30563;&#33539;&#24335;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#26102;&#38388;&#36793;&#32534;&#30721;&#21644;&#22522;&#20110;&#31383;&#21475;&#30340;&#23376;&#22270;&#37319;&#26679;&#26469;&#29983;&#25104;&#20219;&#21153;&#26080;&#20851;&#30340;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#27604;SSL&#30340;&#32852;&#21512;&#23884;&#20837;&#26550;&#26500;&#65292;&#20197;&#23398;&#20064;&#20016;&#23500;&#30340;&#26102;&#38388;&#23884;&#20837;&#32780;&#19981;&#38656;&#35201;&#26631;&#31614;&#12290;&#22312;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20256;&#23548;&#35774;&#32622;&#21644;&#24402;&#32435;&#35774;&#32622;&#30340;&#26410;&#26469;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#24179;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;SoTA&#22522;&#32447;4.23&#65285;&#21644;3.30&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graph neural networks have shown promising results in learning inductive representations by automatically extracting temporal patterns. However, previous works often rely on complex memory modules or inefficient random walk methods to construct temporal representations. In addition, the existing dynamic graph encoders are non-trivial to adapt to self-supervised paradigms, which prevents them from utilizing unlabeled data. To address these limitations, we present an efficient yet effective attention-based encoder that leverages temporal edge encodings and window-based subgraph sampling to generate task-agnostic embeddings. Moreover, we propose a joint-embedding architecture using non-contrastive SSL to learn rich temporal embeddings without labels. Experimental results on 7 benchmark datasets indicate that on average, our model outperforms SoTA baselines on the future link prediction task by 4.23% for the transductive setting and 3.30% for the inductive setting while only requi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#25968;&#25454;&#24314;&#27169;&#30340;&#26032;&#22411;&#27169;&#31946;&#31995;&#32479;&#65292;&#21517;&#20026;&#22270;&#27169;&#31946;&#31995;&#32479;&#65288;GFS&#65289;&#65292;&#36890;&#36807;&#23450;&#20041;&#30456;&#20851;&#27010;&#24565;&#12289;&#26500;&#24314;&#27169;&#22411;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22788;&#29702;&#20855;&#26377;&#38750;&#27431;&#20960;&#37324;&#24471;&#32467;&#26500;&#30340;&#22270;&#25968;&#25454;&#26102;&#20445;&#30041;&#27169;&#31946;&#31995;&#32479;&#20248;&#21183;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2210.16730</link><description>&lt;p&gt;
&#22270;&#27169;&#31946;&#31995;&#32479;: &#27010;&#24565;&#12289;&#27169;&#22411;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Fuzzy System: Concepts, Models and Algorithms. (arXiv:2210.16730v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#25968;&#25454;&#24314;&#27169;&#30340;&#26032;&#22411;&#27169;&#31946;&#31995;&#32479;&#65292;&#21517;&#20026;&#22270;&#27169;&#31946;&#31995;&#32479;&#65288;GFS&#65289;&#65292;&#36890;&#36807;&#23450;&#20041;&#30456;&#20851;&#27010;&#24565;&#12289;&#26500;&#24314;&#27169;&#22411;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22788;&#29702;&#20855;&#26377;&#38750;&#27431;&#20960;&#37324;&#24471;&#32467;&#26500;&#30340;&#22270;&#25968;&#25454;&#26102;&#20445;&#30041;&#27169;&#31946;&#31995;&#32479;&#20248;&#21183;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#31995;&#32479;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#27169;&#24335;&#35782;&#21035;&#12289;&#26234;&#33021;&#25511;&#21046;&#12289;&#25968;&#25454;&#25366;&#25496;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#65292;&#36825;&#24402;&#21151;&#20110;&#20854;&#24378;&#22823;&#30340;&#35299;&#37322;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#20256;&#32479;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#27169;&#31946;&#31995;&#32479;&#20027;&#35201;&#29992;&#20110;&#24314;&#27169;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#25968;&#25454;&#65292;&#26080;&#27861;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24471;&#32467;&#26500;&#30340;&#22270;&#25968;&#25454;&#65292;&#27604;&#22914;&#31038;&#20132;&#32593;&#32476;&#21644;&#20132;&#36890;&#36335;&#32447;&#22270;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#36866;&#29992;&#20110;&#22270;&#25968;&#25454;&#24182;&#33021;&#20445;&#30041;&#20256;&#32479;&#27169;&#31946;&#31995;&#32479;&#20248;&#21183;&#30340;&#24314;&#27169;&#26041;&#27861;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22270;&#25968;&#25454;&#24314;&#27169;&#30340;&#26032;&#22411;&#27169;&#31946;&#31995;&#32479;&#65292;&#31216;&#20026;&#22270;&#27169;&#31946;&#31995;&#32479;&#65288;GFS&#65289;&#65292;&#24182;&#31995;&#32479;&#22320;&#24320;&#21457;&#20102;&#30456;&#20851;&#27010;&#24565;&#12289;&#24314;&#27169;&#26694;&#26550;&#21644;&#26500;&#24314;&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#23450;&#20041;&#20102;GFS&#30456;&#20851;&#27010;&#24565;&#65292;&#21253;&#25324;&#22270;&#27169;&#31946;&#35268;&#21017;&#24211;&#12289;&#22270;&#27169;&#31946;&#38598;&#21644;&#22270;&#21518;&#20214;&#22788;&#29702;&#21333;&#20803;&#65288;GCPU&#65289;&#12290;&#28982;&#21518;&#26500;&#24314;&#20102;&#19968;&#20010;GFS&#24314;&#27169;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzy systems (FSs) have enjoyed wide applications in various fields, including pattern recognition, intelligent control, data mining and bioinformatics, which is attributed to the strong interpretation and learning ability. In traditional application scenarios, FSs are mainly applied to model Euclidean space data and cannot be used to handle graph data of non-Euclidean structure in nature, such as social networks and traffic route maps. Therefore, development of FS modeling method that is suitable for graph data and can retain the advantages of traditional FSs is an important research. To meet this challenge, a new type of FS for graph data modeling called Graph Fuzzy System (GFS) is proposed in this paper, where the concepts, modeling framework and construction algorithms are systematically developed. First, GFS related concepts, including graph fuzzy rule base, graph fuzzy sets and graph consequent processing unit (GCPU), are defined. A GFS modeling framework is then constructed and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#24863;&#20852;&#36259;&#28857;&#30340;&#36873;&#25321;&#21644;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#31283;&#23450;&#21487;&#38752;&#30340;&#31232;&#30095;&#36924;&#36817;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.07893</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Cover Trees&#30340;&#26368;&#23567;&#38388;&#38548;&#23454;&#29616;&#25968;&#20540;&#31283;&#23450;&#30340;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees. (arXiv:2210.07893v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#24863;&#20852;&#36259;&#28857;&#30340;&#36873;&#25321;&#21644;&#35745;&#31639;&#65292;&#25552;&#20379;&#20102;&#31283;&#23450;&#21487;&#38752;&#30340;&#31232;&#30095;&#36924;&#36817;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#24120;&#29992;&#20110;&#36739;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20915;&#31574;&#31995;&#32479;&#20013;&#65292;&#20363;&#22914;&#22320;&#29702;&#31354;&#38388;&#24314;&#27169;&#12289;&#36125;&#21494;&#26031;&#20248;&#21270;&#25110;&#28508;&#22312;&#39640;&#26031;&#27169;&#22411;&#20013;&#12290;&#22312;&#19968;&#20010;&#31995;&#32479;&#20013;&#65292;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#38656;&#35201;&#20197;&#31283;&#23450;&#21487;&#38752;&#30340;&#26041;&#24335;&#36816;&#34892;&#65292;&#20197;&#30830;&#20445;&#19982;&#31995;&#32479;&#30340;&#20854;&#20182;&#37096;&#20998;&#27491;&#30830;&#20132;&#20114;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24863;&#20852;&#36259;&#28857;&#30340;&#21487;&#25193;&#23637;&#31232;&#30095;&#36924;&#36817;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#24182;&#38416;&#36848;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#21487;&#33021;&#19981;&#31283;&#23450;&#30340;&#20856;&#22411;&#24773;&#20917;&#12290;&#22312;&#25554;&#20540;&#25991;&#29486;&#20013;&#21407;&#22987;&#24320;&#21457;&#30340;&#31283;&#23450;&#24615;&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23545;&#24863;&#20852;&#36259;&#28857;&#36827;&#34892;&#35745;&#31639;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#21644;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#23545;&#20110;&#22320;&#29702;&#31354;&#38388;&#24314;&#27169;&#31561;&#20302;&#32500;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35745;&#31639;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#30340;&#24863;&#20852;&#36259;&#28857;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes are frequently deployed as part of larger machine learning and decision-making systems, for instance in geospatial modeling, Bayesian optimization, or in latent Gaussian models. Within a system, the Gaussian process model needs to perform in a stable and reliable manner to ensure it interacts correctly with other parts of the system. In this work, we study the numerical stability of scalable sparse approximations based on inducing points. To do so, we first review numerical stability, and illustrate typical situations in which Gaussian process models can be unstable. Building on stability theory originally developed in the interpolation literature, we derive sufficient and in certain cases necessary conditions on the inducing points for the computations performed to be numerically stable. For low-dimensional tasks such as geospatial modeling, we propose an automated method for computing inducing points satisfying these conditions. This is done via a modification of t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Conformal Prediction&#26041;&#27861;&#23545;&#20110;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25214;&#20986;&#20102;&#26500;&#24314;&#21487;&#20197;&#27491;&#30830;&#35206;&#30422;&#26080;&#22122;&#22768;&#30495;&#23454;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#30830;&#25511;&#21046;&#30340;&#35201;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23545;&#25239;&#24615;&#26696;&#20363;&#20043;&#22806;&#65292;&#20351;&#29992;Conformal Prediction&#21644;&#39118;&#38505;&#25511;&#21046;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#23545;&#24178;&#20928;&#30495;&#23454;&#26631;&#31614;&#30340;&#20445;&#23432;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#30028;&#23610;&#23544;&#22122;&#22768;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#27491;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2209.14295</link><description>&lt;p&gt;
Conformal Prediction&#23545;&#20998;&#25955;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction is Robust to Dispersive Label Noise. (arXiv:2209.14295v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Conformal Prediction&#26041;&#27861;&#23545;&#20110;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25214;&#20986;&#20102;&#26500;&#24314;&#21487;&#20197;&#27491;&#30830;&#35206;&#30422;&#26080;&#22122;&#22768;&#30495;&#23454;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#30830;&#25511;&#21046;&#30340;&#35201;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23545;&#25239;&#24615;&#26696;&#20363;&#20043;&#22806;&#65292;&#20351;&#29992;Conformal Prediction&#21644;&#39118;&#38505;&#25511;&#21046;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#23545;&#24178;&#20928;&#30495;&#23454;&#26631;&#31614;&#30340;&#20445;&#23432;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#30028;&#23610;&#23544;&#22122;&#22768;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#27491;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;Conformal Prediction&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#65292;&#23545;&#20110;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#27491;&#30830;&#35206;&#30422;&#26410;&#35266;&#23519;&#21040;&#30340;&#26080;&#22122;&#22768;&#30495;&#23454;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#36827;&#34892;&#20102;&#30028;&#23450;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#27491;&#30830;&#25511;&#21046;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914;&#20551;&#38452;&#24615;&#27604;&#20363;&#65289;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;Conformal Prediction&#21644;&#39118;&#38505;&#25511;&#21046;&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#23545;&#24178;&#20928;&#30495;&#23454;&#26631;&#31614;&#30340;&#20445;&#23432;&#39118;&#38505;&#65292;&#38500;&#20102;&#22312;&#23545;&#25239;&#24615;&#26696;&#20363;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#36890;&#36807;&#23545;Conformal Prediction&#31639;&#27861;&#36827;&#34892;&#26377;&#30028;&#23610;&#23544;&#30340;&#22122;&#22768;&#20462;&#27491;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#27491;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#39118;&#38505;&#65292;&#32780;&#26080;&#38656;&#32771;&#34385;&#20998;&#25968;&#25110;&#25968;&#25454;&#30340;&#35268;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the robustness of conformal prediction, a powerful tool for uncertainty quantification, to label noise. Our analysis tackles both regression and classification problems, characterizing when and how it is possible to construct uncertainty sets that correctly cover the unobserved noiseless ground truth labels. We further extend our theory and formulate the requirements for correctly controlling a general loss function, such as the false negative proportion, with noisy labels. Our theory and experiments suggest that conformal prediction and risk-controlling techniques with noisy labels attain conservative risk over the clean ground truth labels except in adversarial cases. In such cases, we can also correct for noise of bounded size in the conformal prediction algorithm in order to ensure achieving the correct risk of the ground truth labels without score or data regularity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#21253;&#21547;47&#31181;&#19981;&#21516;&#23646;&#24615;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;Deepfake&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#26088;&#22312;&#30740;&#31350;&#20844;&#20849;Deepfake&#25968;&#25454;&#38598;&#21487;&#33021;&#24102;&#26469;&#30340;AI&#20559;&#24046;&#38382;&#39064;</title><link>http://arxiv.org/abs/2208.05845</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#27880;&#37322;&#25968;&#25454;&#24211;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979; AI &#20559;&#24046;&#30340;&#20840;&#38754;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated Databases. (arXiv:2208.05845v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#21253;&#21547;47&#31181;&#19981;&#21516;&#23646;&#24615;&#27880;&#37322;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;Deepfake&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#26088;&#22312;&#30740;&#31350;&#20844;&#20849;Deepfake&#25968;&#25454;&#38598;&#21487;&#33021;&#24102;&#26469;&#30340;AI&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Deepfake &#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#31713;&#25913;&#24050;&#32463;&#25104;&#20026;&#23433;&#20840;&#21644;&#31038;&#20250;&#30340;&#20005;&#37325;&#20851;&#27880;&#28857;&#12290;&#35768;&#22810;&#26816;&#27979;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979; Deepfake &#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#33021;&#23384;&#22312;&#20559;&#24046;&#65292;&#20174;&#32780;&#23548;&#33268; Deepfake &#26816;&#27979;&#22120;&#22833;&#25928;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#20116;&#20010;&#27969;&#34892;&#30340; Deepfake &#25968;&#25454;&#38598;&#20013; 47 &#31181;&#19981;&#21516;&#23646;&#24615;&#30340;&#22823;&#35268;&#27169;&#20154;&#21475;&#32479;&#35745;&#21644;&#38750;&#20154;&#21475;&#32479;&#35745;&#23646;&#24615;&#27880;&#37322;&#65292;&#24182;&#20840;&#38754;&#20998;&#26512;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340; Deepfake &#26816;&#27979;&#27169;&#22411;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340; AI &#20559;&#24046;&#38382;&#39064;&#65292;&#35843;&#26597;&#30740;&#31350;&#20102;&#36229;&#36807; 6500 &#19975;&#20010;&#26631;&#31614;&#30340;&#35768;&#22810;&#19981;&#21516;&#23646;&#24615;&#65288;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#65288;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#31181;&#26063;&#65289;&#21644;&#38750;&#20154;&#21475;&#32479;&#35745;&#23398;&#65288;&#22836;&#21457;&#12289;&#30382;&#32932;&#12289;&#37197;&#39280;&#31561;&#65289;&#20449;&#24687;&#23545;&#26816;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35843;&#26597;&#30340;&#25968;&#25454;&#24211;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268; AI &#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, image and video manipulations with Deepfake have become a severe concern for security and society. Many detection models and datasets have been proposed to detect Deepfake data reliably. However, there is an increased concern that these models and training databases might be biased and, thus, cause Deepfake detectors to fail. In this work, we investigate the bias issue caused by public Deepfake datasets by (a) providing large-scale demographic and non-demographic attribute annotations of 47 different attributes for five popular Deepfake datasets and (b) comprehensively analysing AI-bias of three state-of-the-art Deepfake detection backbone models on these datasets. The investigation analyses the influence of a large variety of distinctive attributes (from over 65M labels) on the detection performance, including demographic (age, gender, ethnicity) and non-demographic (hair, skin, accessories, etc.) information. The results indicate that investigated databases lack dive
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;NeurVec&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#27491;&#22120;&#65292;&#23427;&#33021;&#22815;&#34917;&#20607;&#38598;&#25104;&#35823;&#24046;&#24182;&#22312;&#27169;&#25311;&#20013;&#23454;&#29616;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;NeurVec&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#20256;&#32479;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.03680</link><description>&lt;p&gt;
&#36890;&#36807;NeurVec&#21152;&#36895;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec. (arXiv:2208.03680v2 [cs.CE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;NeurVec&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#27491;&#22120;&#65292;&#23427;&#33021;&#22815;&#34917;&#20607;&#38598;&#25104;&#35823;&#24046;&#24182;&#22312;&#27169;&#25311;&#20013;&#23454;&#29616;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;NeurVec&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21152;&#36895;&#20102;&#20256;&#32479;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#22823;&#35268;&#27169;&#21160;&#24577;&#31995;&#32479;&#30340;&#27169;&#25311;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#22312;&#20272;&#35745;&#31215;&#20998;&#26102;&#30001;&#20110;&#27493;&#38271;&#36873;&#25321;&#30340;&#38480;&#21046;&#65292;&#23384;&#22312;&#30528;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20462;&#27491;&#22120;&#65292;&#31216;&#20026;NeurVec&#65292;&#23427;&#21487;&#20197;&#34917;&#20607;&#38598;&#25104;&#35823;&#24046;&#24182;&#22312;&#27169;&#25311;&#20013;&#23454;&#29616;&#26356;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#20351;&#29992;&#26377;&#38480;&#21644;&#31163;&#25955;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;NeurVec&#22312;&#36830;&#32493;&#30456;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;NeurVec&#26174;&#33879;&#21152;&#36895;&#20102;&#20256;&#32479;&#27714;&#35299;&#22120;&#65292;&#23454;&#29616;&#20102;&#20960;&#21313;&#21040;&#20960;&#30334;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;NeurVec&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35774;&#35745;&#32467;&#21512;&#20102;&#26131;&#20110;&#23454;&#29616;&#30340;&#29305;&#28857;&#65292;&#26377;&#28508;&#21147;&#24314;&#31435;&#36215;&#19968;&#20010;&#26032;&#30340;&#27714;&#35299;&#22120;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large-scale simulation of dynamical systems is critical in numerous scientific and engineering disciplines. However, traditional numerical solvers are limited by the choice of step sizes when estimating integration, resulting in a trade-off between accuracy and computational efficiency. To address this challenge, we introduce a deep learning-based corrector called Neural Vector (NeurVec), which can compensate for integration errors and enable larger time step sizes in simulations. Our extensive experiments on a variety of complex dynamical system benchmarks demonstrate that NeurVec exhibits remarkable generalization capability on a continuous phase space, even when trained using limited and discrete data. NeurVec significantly accelerates traditional solvers, achieving speeds tens to hundreds of times faster while maintaining high levels of accuracy and stability. Moreover, NeurVec's simple-yet-effective design, combined with its ease of implementation, has the potential to establi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#25945;&#23398;&#20551;&#35774;$h^\mathcal{T}$&#65292;&#36890;&#36807;&#24341;&#20837;&#26356;&#32039;&#23494;&#30340;&#26494;&#24347;&#39033;&#65292;&#21487;&#20197;&#25913;&#36827;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20551;&#35774;&#20462;&#21098;&#26041;&#27861;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#36825;&#20010;&#25945;&#23398;&#20551;&#35774;&#30340;&#25351;&#23548;&#19979;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#36798;&#21040;&#27604;&#26080;&#25351;&#23548;&#30340;&#23398;&#20064;&#32773;&#26356;&#22909;&#30340;&#27867;&#21270;&#35823;&#24046;&#21644;&#26631;&#35760;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2206.15205</link><description>&lt;p&gt;
&#26426;&#22120;&#25945;&#23398;&#30340;&#40657;&#30418;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Black-box Generalization of Machine Teaching. (arXiv:2206.15205v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#25945;&#23398;&#20551;&#35774;$h^\mathcal{T}$&#65292;&#36890;&#36807;&#24341;&#20837;&#26356;&#32039;&#23494;&#30340;&#26494;&#24347;&#39033;&#65292;&#21487;&#20197;&#25913;&#36827;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20551;&#35774;&#20462;&#21098;&#26041;&#27861;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#36825;&#20010;&#25945;&#23398;&#20551;&#35774;&#30340;&#25351;&#23548;&#19979;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#36798;&#21040;&#27604;&#26080;&#25351;&#23548;&#30340;&#23398;&#20064;&#32773;&#26356;&#22909;&#30340;&#27867;&#21270;&#35823;&#24046;&#21644;&#26631;&#35760;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#20462;&#21098;&#26368;&#22823;&#21270;&#29992;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#20551;&#35774;&#26356;&#26032;&#65292;&#20197;&#25214;&#21040;&#26399;&#26395;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#20854;&#22266;&#26377;&#30340;&#20551;&#35774;&#26159;&#65292;&#36825;&#31181;&#23398;&#20064;&#26041;&#24335;&#21487;&#20197;&#23558;&#36825;&#20123;&#26356;&#26032;&#23548;&#20986;&#20026;&#26368;&#20248;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#36882;&#22686;&#30340;&#26356;&#26032;&#26159;&#36127;&#30340;&#21644;&#26080;&#24207;&#30340;&#65292;&#23427;&#30340;&#25910;&#25947;&#24615;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#37319;&#29992;&#26356;&#32039;&#23494;&#30340;&#26494;&#24347;&#39033;$\left(1+\mathcal{F}^{\mathcal{T}}(\widehat{h}_t)\right)\Delta_t$&#26469;&#26367;&#20195;&#20462;&#21098;&#30340;&#20856;&#22411;$2\Delta_t$&#30340;&#40657;&#30418;&#25945;&#23398;&#20551;&#35774;$h^\mathcal{T}$&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#20010;&#25945;&#23398;&#20551;&#35774;&#30340;&#25351;&#23548;&#19979;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#25910;&#25947;&#21040;&#27604;&#37027;&#20123;&#27809;&#26377;&#20174;&#25945;&#24072;&#37027;&#37324;&#24471;&#21040;&#20219;&#20309;&#25351;&#23548;&#30340;&#38750;&#25945;&#32946;&#23398;&#20064;&#32773;&#26356;&#32039;&#23494;&#30340;&#27867;&#21270;&#35823;&#24046;&#21644;&#26631;&#35760;&#22797;&#26434;&#24230;&#30028;&#38480;&#65306;1&#65289;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#21487;&#20197;&#20174;$R(h^*)+4\Delta_{T-1}$&#20943;&#23569;&#21040;&#22823;&#32422;$R(h^{\mathcal{T}})+2\Delta_{T-1}$&#65292;2&#65289;&#26631;&#35760;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#21487;&#20197;&#20174;$4 \theta
&lt;/p&gt;
&lt;p&gt;
Hypothesis-pruning maximizes the hypothesis updates for active learning to find those desired unlabeled data. An inherent assumption is that this learning manner can derive those updates into the optimal hypothesis. However, its convergence may not be guaranteed well if those incremental updates are negative and disordered. In this paper, we introduce a black-box teaching hypothesis $h^\mathcal{T}$ employing a tighter slack term $\left(1+\mathcal{F}^{\mathcal{T}}(\widehat{h}_t)\right)\Delta_t$ to replace the typical $2\Delta_t$ for pruning. Theoretically, we prove that, under the guidance of this teaching hypothesis, the learner can converge into a tighter generalization error and label complexity bound than those non-educated learners who do not receive any guidance from a teacher:1) the generalization error upper bound can be reduced from $R(h^*)+4\Delta_{T-1}$ to approximately $R(h^{\mathcal{T}})+2\Delta_{T-1}$, and 2) the label complexity upper bound can be decreased from $4 \theta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20998;&#26512;&#30005;&#32593;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;&#65292;&#29983;&#25104;&#20102;&#26032;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;GNN&#22312;&#20165;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#38750;&#32447;&#24615;&#30446;&#26631;&#65292;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#30005;&#32593;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2206.06369</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#30005;&#32593;&#25299;&#25169;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Toward Dynamic Stability Assessment of Power Grid Topologies using Graph Neural Networks. (arXiv:2206.06369v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20998;&#26512;&#30005;&#32593;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;&#65292;&#29983;&#25104;&#20102;&#26032;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;GNN&#22312;&#20165;&#21033;&#29992;&#25299;&#25169;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#38750;&#32447;&#24615;&#30446;&#26631;&#65292;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#30005;&#32593;&#20013;&#30340;&#33030;&#24369;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#65292;&#21487;&#20877;&#29983;&#33021;&#28304;&#22312;&#30005;&#21147;&#29983;&#20135;&#20013;&#30340;&#20221;&#39069;&#38656;&#35201;&#22686;&#21152;&#12290;&#21487;&#20877;&#29983;&#33021;&#28304;&#24341;&#20837;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#21040;&#30005;&#32593;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;&#65292;&#21253;&#25324;&#20998;&#25955;&#21270;&#12289;&#38477;&#20302;&#30340;&#24815;&#24615;&#21644;&#29983;&#20135;&#30340;&#27874;&#21160;&#24615;&#12290;&#30001;&#20110;&#23545;&#20110;&#22823;&#22411;&#30005;&#32593;&#32780;&#35328;&#65292;&#21160;&#24577;&#31283;&#23450;&#24615;&#27169;&#25311;&#26159;&#26840;&#25163;&#19988;&#26497;&#20854;&#26114;&#36149;&#30340;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#20998;&#26512;&#30005;&#32593;&#21160;&#24577;&#31283;&#23450;&#24615;&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#12290;&#20316;&#20026;GNN&#27169;&#22411;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#26032;&#30340;&#12289;&#22823;&#22411;&#30340;&#21512;&#25104;&#30005;&#32593;&#21160;&#24577;&#31283;&#23450;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#24320;&#28304;&#36164;&#28304;&#25552;&#20379;&#32473;&#30740;&#31350;&#30028;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#20973;&#25299;&#25169;&#20449;&#24687;&#65292;GNN&#33021;&#22815;&#20986;&#22855;&#22320;&#26377;&#25928;&#22320;&#39044;&#27979;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#30446;&#26631;&#12290;&#39318;&#27425;&#23454;&#29616;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20934;&#30830;&#35782;&#21035;&#30005;&#32593;&#20013;&#29305;&#23450;&#33030;&#24369;&#33410;&#28857;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate climate change, the share of renewable energies in power production needs to be increased. Renewables introduce new challenges to power grids regarding the dynamic stability due to decentralization, reduced inertia, and volatility in production. Since dynamic stability simulations are intractable and exceedingly expensive for large grids, graph neural networks (GNNs) are a promising method to reduce the computational effort of analyzing the dynamic stability of power grids. As a testbed for GNN models, we generate new, large datasets of dynamic stability of synthetic power grids, and provide them as an open-source resource to the research community. We find that GNNs are surprisingly effective at predicting the highly non-linear targets from topological information only. For the first time, performance that is suitable for practical use cases is achieved. Furthermore, we demonstrate the ability of these models to accurately identify particular vulnerable nodes in power grid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32534;&#30721;&#29702;&#35770;&#35774;&#35745;&#29305;&#23450;&#36335;&#24452;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#36879;&#26126;&#24230;&#22686;&#24378;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26500;&#24314;&#12290;</title><link>http://arxiv.org/abs/2202.05343</link><description>&lt;p&gt;
&#23454;&#29616;&#36890;&#36807;&#32534;&#30721;ResNeXt&#26469;&#20998;&#31163;&#20449;&#24687;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Towards Disentangling Information Paths with Coded ResNeXt. (arXiv:2202.05343v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32534;&#30721;&#29702;&#35770;&#35774;&#35745;&#29305;&#23450;&#36335;&#24452;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#36879;&#26126;&#24230;&#22686;&#24378;&#21644;&#20998;&#31867;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#35270;&#20026;&#40657;&#31665;&#65292;&#23545;&#20110;&#25351;&#23548;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#30340;&#26426;&#21046;&#25552;&#20379;&#20102;&#26377;&#38480;&#25110;&#27809;&#26377;&#27934;&#23519;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25237;&#20837;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#21162;&#21147;&#26469;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;&#22823;&#22810;&#25968;&#21162;&#21147;&#35201;&#20040;&#19987;&#27880;&#20110;&#19982;&#26368;&#21518;&#20960;&#23618;&#30456;&#20851;&#30340;&#39640;&#32423;&#29305;&#24449;&#65292;&#35201;&#20040;&#23581;&#35797;&#35299;&#37322;&#21333;&#20010;&#23618;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#25972;&#20010;&#32593;&#32476;&#21151;&#33021;&#30340;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20854;&#20013;&#19982;&#27599;&#20010;&#31867;&#30456;&#20851;&#30340;&#20449;&#24687;&#36890;&#36807;&#29305;&#23450;&#30340;&#36335;&#24452;&#27969;&#21160;&#12290;&#36825;&#20123;&#36335;&#24452;&#22312;&#35757;&#32451;&#20043;&#21069;&#21033;&#29992;&#32534;&#30721;&#29702;&#35770;&#35774;&#35745;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#31867;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#29305;&#24615;&#26159;&#27599;&#20010;&#36335;&#24452;&#37117;&#21487;&#20197;&#29992;&#20316;&#33258;&#20027;&#21333;&#29992;&#36884;&#27169;&#22411;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#22312;&#20219;&#20309;&#31867;&#21035;&#19978;&#33719;&#24471;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#20108;&#20998;&#31867;&#22120;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional, widely used treatment of deep learning models as black boxes provides limited or no insights into the mechanisms that guide neural network decisions. Significant research effort has been dedicated to building interpretable models to address this issue. Most efforts either focus on the high-level features associated with the last layers, or attempt to interpret the output of a single layer. In this paper, we take a novel approach to enhance the transparency of the function of the whole network. We propose a neural network architecture for classification, in which the information that is relevant to each class flows through specific paths. These paths are designed in advance before training leveraging coding theory and without depending on the semantic similarities between classes. A key property is that each path can be used as an autonomous single-purpose model. This enables us to obtain, without any additional training and for any class, a lightweight binary classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31934;&#30830;&#24555;&#36895;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#21363;XnODR&#21644;XnIDR&#65292;&#36890;&#36807;&#22312;&#21160;&#24577;&#36335;&#30001;&#26426;&#21046;&#20013;&#37319;&#29992;XNOR&#22788;&#29702;&#32447;&#24615;&#25237;&#24433;&#65292;&#35299;&#20915;&#20102;&#33014;&#22218;&#32593;&#32476;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2111.10854</link><description>&lt;p&gt;
XnODR&#21644;XnIDR&#65306;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#31181;&#31934;&#30830;&#24555;&#36895;&#30340;&#20840;&#36830;&#25509;&#23618;
&lt;/p&gt;
&lt;p&gt;
XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks. (arXiv:2111.10854v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31934;&#30830;&#24555;&#36895;&#30340;&#20840;&#36830;&#25509;&#23618;&#65292;&#21363;XnODR&#21644;XnIDR&#65292;&#36890;&#36807;&#22312;&#21160;&#24577;&#36335;&#30001;&#26426;&#21046;&#20013;&#37319;&#29992;XNOR&#22788;&#29702;&#32447;&#24615;&#25237;&#24433;&#65292;&#35299;&#20915;&#20102;&#33014;&#22218;&#32593;&#32476;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33014;&#22218;&#32593;&#32476;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#23450;&#20041;&#29305;&#24449;&#20043;&#38388;&#30340;&#20301;&#32622;&#20851;&#31995;&#26041;&#38754;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#35745;&#31639;&#22797;&#26434;&#19988;&#19981;&#36866;&#21512;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;&#35745;&#31639;&#22797;&#26434;&#24230;&#38598;&#20013;&#22312;&#33014;&#22218;&#20043;&#38388;&#37319;&#29992;&#30340;&#21160;&#24577;&#36335;&#30001;&#26426;&#21046;&#20013;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;XNOR-Net&#34429;&#28982;&#24555;&#36895;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#20294;&#30001;&#20110;&#20108;&#20540;&#21270;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#25439;&#22833;&#32780;&#23548;&#33268;&#20934;&#30830;&#24615;&#19981;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#21160;&#24577;&#36335;&#30001;&#26426;&#21046;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#23545;CapsFC&#23618;&#20869;&#25110;&#22806;&#30340;&#32447;&#24615;&#25237;&#24433;&#36827;&#34892;XNOR&#22788;&#29702;&#30340;&#26032;&#30340;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;FC&#23618;&#26377;&#20004;&#20010;&#29256;&#26412;&#65292;&#21363;XnODR&#65288;&#22312;&#21160;&#24577;&#36335;&#30001;&#20043;&#22806;&#23545;&#32447;&#24615;&#25237;&#24433;&#36827;&#34892;XNOR&#22788;&#29702;&#65289;&#21644;XnIDR&#65288;&#22312;&#21160;&#24577;&#36335;&#30001;&#20043;&#20869;&#23545;&#32447;&#24615;&#25237;&#24433;&#36827;&#34892;XNOR&#22788;&#29702;&#65289;&#12290;&#20026;&#20102;&#27979;&#35797;XnODR&#21644;XnIDR&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#25554;&#20837;&#21040;&#20004;&#20010;&#19981;&#21516;&#30340;&#32593;&#32476;&#20013;&#65292;MobileNetV...
&lt;/p&gt;
&lt;p&gt;
Capsule Network is powerful at defining the positional relationship between features in deep neural networks for visual recognition tasks, but it is computationally expensive and not suitable for running on mobile devices. The bottleneck is in the computational complexity of the Dynamic Routing mechanism used between the capsules. On the other hand, XNOR-Net is fast and computationally efficient, though it suffers from low accuracy due to information loss in the binarization process. To address the computational burdens of the Dynamic Routing mechanism, this paper proposes new Fully Connected (FC) layers by xnorizing the linear projection outside or inside the Dynamic Routing within the CapsFC layer. Specifically, our proposed FC layers have two versions, XnODR (Xnorize the Linear Projection Outside Dynamic Routing) and XnIDR (Xnorize the Linear Projection Inside Dynamic Routing). To test the generalization of both XnODR and XnIDR, we insert them into two different networks, MobileNetV
&lt;/p&gt;</description></item><item><title>&#22312;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#32676;&#36873;&#25321;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#22238;&#24402;&#20989;&#25968;&#36229;&#36807;&#39044;&#35774;&#38408;&#20540;&#30340;&#29305;&#24449;&#31354;&#38388;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#30830;&#23450;&#20102;&#22312;&#26679;&#26412;&#35268;&#27169;&#21644;&#31867;&#22411;I&#38169;&#35823;&#27010;&#29575;&#19978;&#36951;&#25022;&#30340;&#26368;&#20339;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2109.01077</link><description>&lt;p&gt;
&#26368;&#20339;&#23376;&#32676;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Optimal subgroup selection. (arXiv:2109.01077v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01077
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23376;&#32676;&#36873;&#25321;&#25361;&#25112;&#65292;&#20197;&#30830;&#23450;&#22238;&#24402;&#20989;&#25968;&#36229;&#36807;&#39044;&#35774;&#38408;&#20540;&#30340;&#29305;&#24449;&#31354;&#38388;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#30830;&#23450;&#20102;&#22312;&#26679;&#26412;&#35268;&#27169;&#21644;&#31867;&#22411;I&#38169;&#35823;&#27010;&#29575;&#19978;&#36951;&#25022;&#30340;&#26368;&#20339;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#35797;&#39564;&#21644;&#20854;&#20182;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#30475;&#21040;&#29305;&#24449;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#26377;&#36259;&#30340;&#34892;&#20026;&#21306;&#22495;&#65292;&#20294;&#19981;&#28165;&#26970;&#36825;&#20123;&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#26159;&#21542;&#22312;&#24635;&#20307;&#27700;&#24179;&#19978;&#26377;&#25152;&#21453;&#26144;&#12290;&#38024;&#23545;&#22238;&#24402;&#35774;&#32622;&#65292;&#25105;&#20204;&#32771;&#34385;&#23376;&#32676;&#36873;&#25321;&#25361;&#25112;&#65292;&#21363;&#35782;&#21035;&#19968;&#20010;&#29305;&#24449;&#31354;&#38388;&#30340;&#21306;&#22495;&#65292;&#22312;&#35813;&#21306;&#22495;&#19978;&#65292;&#22238;&#24402;&#20989;&#25968;&#36229;&#36807;&#20102;&#39044;&#35774;&#30340;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#23547;&#25214;&#19968;&#20010;&#20302;&#22797;&#26434;&#24230;&#12289;&#25968;&#25454;&#30456;&#20851;&#30340;&#36873;&#25321;&#38598;&#65292;&#22312;&#36825;&#20010;&#36873;&#25321;&#38598;&#19978;&#65292;&#22238;&#24402;&#20989;&#25968;&#26377;&#33267;&#23569;&#19982;&#38408;&#20540;&#19968;&#26679;&#22823;&#30340;&#27010;&#29575;&#65292;&#21516;&#26102;&#35201;&#27714;&#35813;&#21306;&#22495;&#22312;&#36793;&#32536;&#29305;&#24449;&#20998;&#24067;&#19979;&#30340;&#36136;&#37327;&#23613;&#21487;&#33021;&#22823;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#36951;&#25022;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#30830;&#23450;&#20102;&#36951;&#25022;&#22312;&#26679;&#26412;&#35268;&#27169;&#21644;&#31532;&#19968;&#31867;&#38169;&#35823;&#27010;&#29575;&#19978;&#30340;&#26368;&#20248;&#20540;&#12290;&#36825;&#20010;&#26368;&#20248;&#20540;&#28041;&#21450;&#21040;&#26679;&#26412;&#22823;&#23567;&#21644;&#31867;&#22411;I&#38169;&#35823;&#27010;&#29575;&#30340;&#24494;&#22937;&#30456;&#20114;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In clinical trials and other applications, we often see regions of the feature space that appear to exhibit interesting behaviour, but it is unclear whether these observed phenomena are reflected at the population level. Focusing on a regression setting, we consider the subgroup selection challenge of identifying a region of the feature space on which the regression function exceeds a pre-determined threshold. We formulate the problem as one of constrained optimisation, where we seek a low-complexity, data-dependent selection set on which, with a guaranteed probability, the regression function is uniformly at least as large as the threshold; subject to this constraint, we would like the region to contain as much mass under the marginal feature distribution as possible. This leads to a natural notion of regret, and our main contribution is to determine the minimax optimal rate for this regret in both the sample size and the Type I error probability. The rate involves a delicate interpla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#39046;&#22495;&#20197;&#23454;&#38469;&#30340;&#35270;&#35282;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#37325;&#26032;&#23454;&#29616;&#20102;6&#31181;&#19981;&#21516;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#20849;&#21516;&#30340;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#19987;&#23478;&#36712;&#36857;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#65292;&#27604;&#36739;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2108.01867</link><description>&lt;p&gt;
&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#30340;&#23454;&#29992;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Pragmatic Look at Deep Imitation Learning. (arXiv:2108.01867v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.01867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#28145;&#24230;&#27169;&#20223;&#23398;&#20064;&#39046;&#22495;&#20197;&#23454;&#38469;&#30340;&#35270;&#35282;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#37325;&#26032;&#23454;&#29616;&#20102;6&#31181;&#19981;&#21516;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#20849;&#21516;&#30340;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#19987;&#23478;&#36712;&#36857;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#65292;&#27604;&#36739;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;GAIL&#65289;&#31639;&#27861;&#30340;&#24341;&#20837;&#25512;&#21160;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#25193;&#23637;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#35768;&#22810;&#21518;&#32493;&#31639;&#27861;&#20351;&#29992;&#20102;&#31867;&#20284;&#30340;&#36807;&#31243;&#65292;&#23558;&#22312;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#19982;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#26368;&#36817;&#20986;&#29616;&#20102;&#26356;&#22810;&#31181;&#31867;&#30340;&#26041;&#27861;&#65292;&#22823;&#22810;&#25968;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31639;&#27861;&#30340;&#24191;&#27867;&#24615;&#65292;&#20174;&#25968;&#25454;&#38598;&#21040;&#22522;&#30784;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20877;&#21040;&#35780;&#20272;&#35774;&#32622;&#37117;&#21487;&#33021;&#26377;&#25152;&#21464;&#21270;&#65292;&#36825;&#20351;&#24471;&#20844;&#27491;&#27604;&#36739;&#23427;&#20204;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23454;&#29616;&#20102;6&#31181;&#19981;&#21516;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#20854;&#20013;3&#31181;&#26356;&#26032;&#20026;&#31163;&#32447;&#31574;&#30053;&#65292;&#23558;&#23427;&#20204;&#22522;&#20110;&#19968;&#20010;&#24120;&#29992;&#30340;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#65288;SAC&#65289;&#65292;&#24182;&#22312;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#19987;&#23478;&#36712;&#36857;&#25968;&#25454;&#38598;&#65288;D4RL&#65289;&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#36827;&#34892;&#26368;&#24120;&#35265;&#30340;&#22522;&#20934;&#27979;&#35797;&#65288;MuJoCo&#65289;&#12290;&#22312;&#32473;&#25152;&#26377;&#31639;&#27861;&#30456;&#21516;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#39044;&#31639;&#20043;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#19968;&#31995;&#21015;&#19987;&#23478;&#36712;&#36857;&#27979;&#35797;&#19978;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of the generative adversarial imitation learning (GAIL) algorithm has spurred the development of scalable imitation learning approaches using deep neural networks. Many of the algorithms that followed used a similar procedure, combining on-policy actor-critic algorithms with inverse reinforcement learning. More recently there have been an even larger breadth of approaches, most of which use off-policy algorithms. However, with the breadth of algorithms, everything from datasets to base reinforcement learning algorithms to evaluation settings can vary, making it difficult to fairly compare them. In this work we re-implement 6 different IL algorithms, updating 3 of them to be off-policy, base them on a common off-policy algorithm (SAC), and evaluate them on a widely-used expert trajectory dataset (D4RL) for the most common benchmark (MuJoCo). After giving all algorithms the same hyperparameter optimisation budget, we compare their results for a range of expert trajectori
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#29992;&#20110;&#23558;&#21333;&#28857;&#33719;&#21462;&#20989;&#25968;&#36866;&#24212;&#20110;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#65292;&#19982;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25209;&#37327;&#37319;&#38598;&#20989;&#25968;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#65292;&#20294;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2106.12059</link><description>&lt;p&gt;
&#38543;&#26426;&#25209;&#37327;&#33719;&#21462;&#65306;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#30340;&#31616;&#21333;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Batch Acquisition: A Simple Baseline for Deep Active Learning. (arXiv:2106.12059v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.12059
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#29992;&#20110;&#23558;&#21333;&#28857;&#33719;&#21462;&#20989;&#25968;&#36866;&#24212;&#20110;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#65292;&#19982;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25209;&#37327;&#37319;&#38598;&#20989;&#25968;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#65292;&#20294;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38543;&#26426;&#31574;&#30053;&#65292;&#29992;&#20110;&#23558;&#20247;&#25152;&#21608;&#30693;&#30340;&#21333;&#28857;&#33719;&#21462;&#20989;&#25968;&#36866;&#24212;&#20110;&#25209;&#37327;&#20027;&#21160;&#23398;&#20064;&#12290;&#19982;&#20174;&#27744;&#38598;&#21512;&#20013;&#33719;&#21462;&#21069;K&#20010;&#28857;&#19981;&#21516;&#65292;&#22522;&#20110;&#20998;&#25968;&#25110;&#25490;&#21517;&#30340;&#37319;&#26679;&#32771;&#34385;&#21040;&#33719;&#21462;&#25968;&#25454;&#21518;&#37319;&#38598;&#20998;&#25968;&#30340;&#21464;&#21270;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#21487;&#20197;&#19982;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#26368;&#26032;&#25209;&#37327;&#37319;&#38598;&#20989;&#25968;&#65288;&#22914;BatchBALD&#25110;BADGE&#65289;&#19968;&#26679;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#25968;&#37327;&#32423;&#36739;&#23569;&#12290;&#38500;&#20102;&#20026;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#36873;&#39033;&#22806;&#65292;&#22312;&#21508;&#31181;&#23454;&#39564;&#29615;&#22659;&#20013;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24847;&#22806;&#25104;&#21151;&#36824;&#20026;&#36825;&#20010;&#39046;&#22495;&#25552;&#20986;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65306;&#36825;&#20123;&#26114;&#36149;&#30340;&#25209;&#37327;&#37319;&#38598;&#26041;&#27861;&#20309;&#26102;&#25165;&#33021;&#21457;&#25381;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
We examine a simple stochastic strategy for adapting well-known single-point acquisition functions to allow batch active learning. Unlike acquiring the top-K points from the pool set, score- or rank-based sampling takes into account that acquisition scores change as new data are acquired. This simple strategy for adapting standard single-sample acquisition strategies can even perform just as well as compute-intensive state-of-the-art batch acquisition functions, like BatchBALD or BADGE, while using orders of magnitude less compute. In addition to providing a practical option for machine learning practitioners, the surprising success of the proposed method in a wide range of experimental settings raises a difficult question for the field: when are these expensive batch acquisition methods pulling their weight?
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#21322;&#31354;&#38388;&#22312;Massart&#22122;&#22768;&#19979;&#30340;&#38169;&#37197;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#31639;&#27861;&#24182;&#22238;&#31572;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#12290;&#36890;&#36807;&#40657;&#30418;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#23558;&#22797;&#26434;&#20998;&#31867;&#22120;&#36716;&#25442;&#20026;&#21516;&#26679;&#22909;&#30340;&#21512;&#36866;&#20998;&#31867;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#23567;&#26679;&#26412;&#30340;&#21512;&#36866;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#30697;&#24863;&#30693;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2006.04787</link><description>&lt;p&gt;
&#22312;&#38169;&#37197;&#24773;&#20917;&#19979;&#30340;&#20998;&#31867;&#65306;&#21322;&#31354;&#38388;&#12289;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19982;&#21487;&#36827;&#21270;&#24615;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability. (arXiv:2006.04787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.04787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#21322;&#31354;&#38388;&#22312;Massart&#22122;&#22768;&#19979;&#30340;&#38169;&#37197;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#31639;&#27861;&#24182;&#22238;&#31572;&#20102;&#19968;&#20123;&#24320;&#25918;&#38382;&#39064;&#12290;&#36890;&#36807;&#40657;&#30418;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#23558;&#22797;&#26434;&#20998;&#31867;&#22120;&#36716;&#25442;&#20026;&#21516;&#26679;&#22909;&#30340;&#21512;&#36866;&#20998;&#31867;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#23567;&#26679;&#26412;&#30340;&#21512;&#36866;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#30697;&#24863;&#30693;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#19968;&#20123;&#20851;&#20110;&#38169;&#37197;&#24773;&#20917;&#19979;&#30340;&#20998;&#31867;&#30340;&#32463;&#20856;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Massart&#22122;&#22768;&#19979;&#20197;&#36895;&#29575;$\eta$&#23398;&#20064;&#21322;&#31354;&#38388;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#20013;&#65292;Diakonikolas&#12289;Goulekakis&#21644;Tzamos&#36890;&#36807;&#25552;&#20379;&#31532;&#19968;&#20010;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#20102;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23398;&#20064;&#21040;&#20934;&#30830;&#24230;$\eta + \epsilon$&#65292;&#20854;&#20013;$\epsilon &gt; 0$&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#31639;&#27861;&#36755;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#20551;&#35774;&#65292;&#23558;&#31354;&#38388;&#20998;&#21106;&#20026;$\text{poly}(d,1/\epsilon)$&#20010;&#21306;&#22495;&#12290;&#36825;&#37324;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#24182;&#22312;&#27492;&#36807;&#31243;&#20013;&#35299;&#20915;&#20102;&#19968;&#20123;&#24748;&#32780;&#26410;&#20915;&#30340;&#24320;&#25918;&#38382;&#39064;&#65306;(1)&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#23454;&#29616;$\eta + \epsilon$&#30340;Massart&#21322;&#31354;&#38388;&#21512;&#36866;&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#25913;&#36827;&#30028;&#38480;&#12290;(2)&#22522;&#20110;(1)&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#40657;&#30418;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#23558;&#20219;&#24847;&#22797;&#26434;&#30340;&#20998;&#31867;&#22120;&#36716;&#25442;&#20026;&#21516;&#26679;&#22909;&#30340;&#21512;&#36866;&#30340;&#20998;&#31867;&#22120;&#12290;(3)&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#31616;&#21333;&#20294;&#34987;&#24573;&#35270;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#20219;&#20309;&#39069;&#22806;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#26500;&#36896;&#20102;&#19968;&#20010;&#23567;&#26679;&#26412;&#30340;&#21512;&#36866;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#22522;&#20110;&#30697;&#24863;&#30693;&#30340;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#19968;&#20010;&#20855;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we revisit some classic problems on classification under misspecification. In particular, we study the problem of learning halfspaces under Massart noise with rate $\eta$. In a recent work, Diakonikolas, Goulekakis, and Tzamos resolved a long-standing problem by giving the first efficient algorithm for learning to accuracy $\eta + \epsilon$ for any $\epsilon &gt; 0$. However, their algorithm outputs a complicated hypothesis, which partitions space into $\text{poly}(d,1/\epsilon)$ regions. Here we give a much simpler algorithm and in the process resolve a number of outstanding open questions:  (1) We give the first proper learner for Massart halfspaces that achieves $\eta + \epsilon$. We also give improved bounds on the sample complexity achievable by polynomial time algorithms.  (2) Based on (1), we develop a blackbox knowledge distillation procedure to convert an arbitrarily complex classifier to an equally good proper classifier.  (3) By leveraging a simple but overlooked 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23454;&#20307;&#23545;&#40784;&#25110;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#30340;&#25490;&#21517;&#35780;&#20272;&#30340;&#27495;&#20041;&#24615;&#12290;&#20998;&#26512;&#20102;&#24403;&#21069;&#35780;&#20272;&#25351;&#26631;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#30340;&#35843;&#25972;&#65292;&#20197;&#23454;&#29616;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20844;&#24179;&#12289;&#21487;&#27604;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2002.06914</link><description>&lt;p&gt;
&#20851;&#20110;&#23454;&#20307;&#23545;&#40784;&#25110;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#25490;&#21517;&#35780;&#20272;&#30340;&#27495;&#20041;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Ambiguity of Rank-Based Evaluation of Entity Alignment or Link Prediction Methods. (arXiv:2002.06914v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.06914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23454;&#20307;&#23545;&#40784;&#25110;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#30340;&#25490;&#21517;&#35780;&#20272;&#30340;&#27495;&#20041;&#24615;&#12290;&#20998;&#26512;&#20102;&#24403;&#21069;&#35780;&#20272;&#25351;&#26631;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26041;&#27861;&#30340;&#35843;&#25972;&#65292;&#20197;&#23454;&#29616;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20844;&#24179;&#12289;&#21487;&#27604;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20004;&#31181;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#22686;&#24378;&#20449;&#24687;&#30340;&#26041;&#27861;&#65306;&#38142;&#25509;&#39044;&#27979;&#21644;&#23454;&#20307;&#23545;&#40784;&#65292;&#23545;&#20854;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#24403;&#21069;&#23454;&#39564;&#35774;&#32622;&#20013;&#65292;&#37319;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#35780;&#20272;&#25351;&#26631;&#30340;&#20449;&#24687;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#20960;&#20046;&#19981;&#33021;&#29992;&#20110;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#27604;&#36739;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#27979;&#35797;&#38598;&#22823;&#23567;&#30340;&#21464;&#21270;&#20250;&#23545;&#30456;&#21516;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#65292;&#36825;&#26159;&#22522;&#20110;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#24120;&#29992;&#24230;&#37327;&#26631;&#20934;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#23548;&#33268;&#20102;&#32467;&#26524;&#35299;&#37322;&#19978;&#30340;&#21508;&#31181;&#38382;&#39064;&#65292;&#21487;&#33021;&#25903;&#25345;&#35823;&#23548;&#24615;&#30340;&#32467;&#35770;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#35780;&#20272;&#26041;&#27861;&#30340;&#35843;&#25972;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22914;&#20309;&#23454;&#29616;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20844;&#24179;&#12289;&#21487;&#27604;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we take a closer look at the evaluation of two families of methods for enriching information from knowledge graphs: Link Prediction and Entity Alignment. In the current experimental setting, multiple different scores are employed to assess different aspects of model performance. We analyze the informativeness of these evaluation measures and identify several shortcomings. In particular, we demonstrate that all existing scores can hardly be used to compare results across different datasets. Moreover, we demonstrate that varying size of the test size automatically has impact on the performance of the same model based on commonly used metrics for the Entity Alignment task. We show that this leads to various problems in the interpretation of results, which may support misleading conclusions. Therefore, we propose adjustments to the evaluation and demonstrate empirically how this supports a fair, comparable, and interpretable assessment of model performance. Our code is availa
&lt;/p&gt;</description></item></channel></rss>