<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24418;&#24335;&#21270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#30740;&#31350;&#36235;&#21183;&#65292;&#35752;&#35770;&#20102;&#33258;&#21160;&#35774;&#35745;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06532</link><description>&lt;p&gt;
&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Automated Design of Metaheuristic Algorithms. (arXiv:2303.06532v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24418;&#24335;&#21270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#30740;&#31350;&#36235;&#21183;&#65292;&#35752;&#35770;&#20102;&#33258;&#21160;&#35774;&#35745;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, and discusses the potential future directions and open issues in this field.
&lt;/p&gt;
&lt;p&gt;
&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30001;&#20110;&#20854;&#33021;&#22815;&#29420;&#31435;&#20110;&#38382;&#39064;&#32467;&#26500;&#21644;&#38382;&#39064;&#39046;&#22495;&#36827;&#34892;&#25628;&#32034;&#30340;&#33021;&#21147;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36890;&#24120;&#65292;&#38656;&#35201;&#20154;&#31867;&#19987;&#23478;&#25163;&#21160;&#35843;&#25972;&#31639;&#27861;&#20197;&#36866;&#24212;&#35299;&#20915;&#30446;&#26631;&#38382;&#39064;&#12290;&#25163;&#21160;&#35843;&#25972;&#36807;&#31243;&#21487;&#33021;&#26159;&#36153;&#21147;&#30340;&#12289;&#23481;&#26131;&#20986;&#38169;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#36825;&#24341;&#36215;&#20102;&#23545;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#21644;&#38656;&#27714;&#65292;&#20197;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;&#33258;&#21160;&#35774;&#35745;&#21487;&#20197;&#20351;&#39640;&#24615;&#33021;&#31639;&#27861;&#23545;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21487;&#29992;&#65307;&#36890;&#36807;&#21033;&#29992;&#35745;&#31639;&#33021;&#21147;&#26469;&#20805;&#20998;&#25506;&#32034;&#28508;&#22312;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#33258;&#21160;&#35774;&#35745;&#21487;&#20197;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#30340;&#35774;&#35745;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;&#24037;&#20316;&#30340;&#20849;&#21516;&#28857;&#21644;&#24046;&#24322;&#36827;&#34892;&#35843;&#26597;&#65292;&#25552;&#20986;&#20102;&#33258;&#21160;&#35774;&#35745;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24418;&#24335;&#21270;&#12289;&#26041;&#27861;&#35770;&#12289;&#25361;&#25112;&#21644;&#30740;&#31350;&#36235;&#21183;&#30340;&#24191;&#27867;&#27010;&#36848;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metaheuristic algorithms have attracted wide attention from academia and industry due to their capability of conducting search independent of problem structures and problem domains. Often, human experts are requested to manually tailor algorithms to fit for solving a targeted problem. The manual tailoring process may be laborious, error-prone, and require intensive specialized knowledge. This gives rise to increasing interests and demands for automated design of metaheuristic algorithms with less human intervention. The automated design could make high-performance algorithms accessible to a much broader range of researchers and practitioners; and by leveraging computing power to fully explore the potential design choices, automated design could reach or even surpass human-level design. This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, by conducting a survey on the common grounds and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.06530</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#25209;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.
&lt;/p&gt;
&lt;p&gt;
&#25209;&#26631;&#20934;&#21270;&#65288;BN&#65289;&#36890;&#24120;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#24182;&#21152;&#36895;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#38750;IID&#20998;&#25955;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#20351;&#29992;BN&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20250;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;BN&#32479;&#35745;&#19981;&#21305;&#37197;&#32780;&#38459;&#30861;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#32676;&#32452;&#24402;&#19968;&#21270;&#65288;GN&#65289;&#26356;&#24120;&#29992;&#20110;FL&#20316;&#20026;BN&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25105;&#20204;&#22312;&#21508;&#31181;FL&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;BN&#21644;GN&#20043;&#38388;&#27809;&#26377;&#19968;&#33268;&#30340;&#20248;&#32988;&#32773;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;FL&#20013;&#24402;&#19968;&#21270;&#23618;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;BN&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;FL&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#25104;&#20026;FL&#26410;&#26469;&#23454;&#38469;&#20351;&#29992;&#21644;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#30456;&#20851;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20840;&#19987;&#23478;&#21453;&#39304;&#21644;Bandit&#21453;&#39304;&#35774;&#32622;&#20013;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#30340;&#36951;&#25022;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.06526</link><description>&lt;p&gt;
&#25968;&#25454;&#30456;&#20851;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Data Dependent Regret Guarantees Against General Comparators for Full or Bandit Feedback. (arXiv:2303.06526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#30456;&#20851;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20840;&#19987;&#23478;&#21453;&#39304;&#21644;Bandit&#21453;&#39304;&#35774;&#32622;&#20013;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#30340;&#36951;&#25022;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a data-dependent online learning algorithm framework that has data-dependent regret guarantees in both full expert feedback and bandit feedback settings, applicable for a wide variety of problem scenarios.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#23436;&#20840;&#22312;&#32447;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#20855;&#26377;&#22312;&#20840;&#19987;&#23478;&#21453;&#39304;&#21644;Bandit&#21453;&#39304;&#35774;&#32622;&#20013;&#20855;&#26377;&#25968;&#25454;&#30456;&#20851;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#19968;&#33324;&#27604;&#36739;&#22120;&#30340;&#39044;&#26399;&#24615;&#33021;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#21508;&#31181;&#38382;&#39064;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20174;&#36890;&#29992;&#39044;&#27979;&#35282;&#24230;&#24037;&#20316;&#65292;&#20351;&#29992;&#30340;&#24615;&#33021;&#24230;&#37327;&#26159;&#23545;&#20219;&#24847;&#27604;&#36739;&#22120;&#24207;&#21015;&#30340;&#39044;&#26399;&#36951;&#25022;&#65292;&#21363;&#25105;&#20204;&#30340;&#25439;&#22833;&#19982;&#31454;&#20105;&#25439;&#22833;&#24207;&#21015;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#31454;&#20105;&#31867;&#21487;&#20197;&#35774;&#35745;&#20026;&#21253;&#25324;&#22266;&#23450;&#33218;&#36873;&#25321;&#12289;&#20999;&#25442;Bandit&#12289;&#19978;&#19979;&#25991;Bandit&#12289;&#21608;&#26399;Bandit&#25110;&#20219;&#20309;&#20854;&#20182;&#24863;&#20852;&#36259;&#30340;&#31454;&#20105;&#12290;&#31454;&#20105;&#31867;&#20013;&#30340;&#24207;&#21015;&#36890;&#24120;&#30001;&#20855;&#20307;&#24212;&#29992;&#31243;&#24207;&#30830;&#23450;&#65292;&#24182;&#24212;&#30456;&#24212;&#22320;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26082;&#19981;&#20351;&#29992;&#20063;&#19981;&#38656;&#35201;&#20219;&#20309;&#26377;&#20851;&#25439;&#22833;&#24207;&#21015;&#30340;&#21021;&#27493;&#20449;&#24687;&#65292;&#23436;&#20840;&#22312;&#32447;&#12290;&#20854;
&lt;/p&gt;
&lt;p&gt;
We study the adversarial online learning problem and create a completely online algorithmic framework that has data dependent regret guarantees in both full expert feedback and bandit feedback settings. We study the expected performance of our algorithm against general comparators, which makes it applicable for a wide variety of problem scenarios. Our algorithm works from a universal prediction perspective and the performance measure used is the expected regret against arbitrary comparator sequences, which is the difference between our losses and a competing loss sequence. The competition class can be designed to include fixed arm selections, switching bandits, contextual bandits, periodic bandits or any other competition of interest. The sequences in the competition class are generally determined by the specific application at hand and should be designed accordingly. Our algorithm neither uses nor needs any preliminary information about the loss sequences and is completely online. Its
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20960;&#20309;&#21644;&#39068;&#33394;&#27010;&#29575;&#20998;&#24067;&#30340;&#39640;&#25928;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#21644;&#26356;&#24555;&#30340;&#21387;&#32553;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06519</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#30340;&#26465;&#20214;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#26080;&#25439;&#28857;&#20113;&#20960;&#20309;&#21644;&#23646;&#24615;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossless Point Cloud Geometry and Attribute Compression Using a Learned Conditional Probability Model. (arXiv:2303.06519v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20960;&#20309;&#21644;&#39068;&#33394;&#27010;&#29575;&#20998;&#24067;&#30340;&#39640;&#25928;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#21644;&#26356;&#24555;&#30340;&#21387;&#32553;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient lossless point cloud compression method that uses sparse tensor-based deep neural networks to learn point cloud geometry and color probability distributions, achieving higher compression ratio and faster compression speed compared to the state-of-the-art method from Moving Pict.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25105;&#20204;&#22312;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#37117;&#35265;&#35777;&#20102;&#28857;&#20113;&#25968;&#25454;&#30340;&#23384;&#22312;&#65292;&#20174;&#27785;&#28024;&#24335;&#23186;&#20307;&#12289;&#33258;&#21160;&#39550;&#39542;&#21040;&#21307;&#30103;&#20445;&#20581;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#25968;&#25454;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#65292;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#28857;&#20113;&#20960;&#20309;&#21644;&#39068;&#33394;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32479;&#19968;&#30340;&#31232;&#30095;&#34920;&#31034;&#23558;&#28857;&#20113;&#34920;&#31034;&#20026;&#20855;&#26377;&#19981;&#21516;&#20301;&#28145;&#24230;&#30340;&#21344;&#29992;&#29305;&#24449;&#21644;&#19977;&#20010;&#23646;&#24615;&#29305;&#24449;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#31232;&#30095;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#21033;&#29992;&#28857;&#20113;&#20869;&#30340;&#29305;&#24449;&#21644;&#28857;&#20869;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#20026;&#31639;&#26415;&#32534;&#30721;&#22120;&#26500;&#24314;&#20934;&#30830;&#30340;&#33258;&#22238;&#24402;&#19978;&#19979;&#25991;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#26080;&#25439;&#28857;&#20113;&#20960;&#20309;&#21644;&#23646;&#24615;&#21387;&#32553;&#26041;&#27861;&#12290;&#19982;Moving Pict&#30340;&#26368;&#26032;&#26080;&#25439;&#28857;&#20113;&#21387;&#32553;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#26080;&#25439;&#21387;&#32553;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#21387;&#32553;&#27604;&#21644;&#26356;&#24555;&#30340;&#21387;&#32553;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, we have witnessed the presence of point cloud data in many aspects of our life, from immersive media, autonomous driving to healthcare, although at the cost of a tremendous amount of data. In this paper, we present an efficient lossless point cloud compression method that uses sparse tensor-based deep neural networks to learn point cloud geometry and color probability distributions. Our method represents a point cloud with both occupancy feature and three attribute features at different bit depths in a unified sparse representation. This allows us to efficiently exploit feature-wise and point-wise dependencies within point clouds using a sparse tensor-based neural network and thus build an accurate auto-regressive context model for an arithmetic coder. To the best of our knowledge, this is the first learning-based lossless point cloud geometry and attribute compression approach. Compared with the-state-of-the-art lossless point cloud compression method from Moving Pict
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#26080;&#25439;&#21487;&#25193;&#23637;&#28857;&#20113;&#23646;&#24615;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#26550;&#26500;&#25552;&#20379;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#32534;&#30721;&#27604;&#29305;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#20174;&#26080;&#25439;&#21387;&#32553;&#30340;&#27604;&#29305;&#27969;&#20013;&#36731;&#26494;&#25552;&#21462;&#36739;&#20302;&#36136;&#37327;&#30340;&#29256;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#26032;&#30340;G-PCC&#29256;&#26412;14&#30456;&#24403;&#65292;&#19988;&#32534;&#30721;&#26102;&#38388;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2303.06517</link><description>&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#29992;&#20110;&#26080;&#25439;&#21487;&#25193;&#23637;&#28857;&#20113;&#23646;&#24615;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Deep probabilistic model for lossless scalable point cloud attribute compression. (arXiv:2303.06517v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#26080;&#25439;&#21487;&#25193;&#23637;&#28857;&#20113;&#23646;&#24615;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#23610;&#24230;&#26550;&#26500;&#25552;&#20379;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#32534;&#30721;&#27604;&#29305;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#20174;&#26080;&#25439;&#21387;&#32553;&#30340;&#27604;&#29305;&#27969;&#20013;&#36731;&#26494;&#25552;&#21462;&#36739;&#20302;&#36136;&#37327;&#30340;&#29256;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#26032;&#30340;G-PCC&#29256;&#26412;14&#30456;&#24403;&#65292;&#19988;&#32534;&#30721;&#26102;&#38388;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a deep probabilistic model for lossless scalable point cloud attribute compression, which utilizes a multiscale architecture to provide accurate context for attribute probability modeling and allows for easily extracting lower quality versions from the losslessly compressed bitstream. The method outperforms recently proposed methods and is on par with the latest G-PCC version 14, with substantially faster coding time.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#28857;&#20113;&#20960;&#20309;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#26159;&#20851;&#20110;&#23646;&#24615;&#21387;&#32553;&#65292;&#29305;&#21035;&#26159;&#26080;&#25439;&#21387;&#32553;&#30340;&#24037;&#20316;&#36824;&#24456;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#23610;&#24230;&#28857;&#20113;&#23646;&#24615;&#32534;&#30721;&#26041;&#27861;&#65288;MNeT&#65289;&#65292;&#35813;&#26041;&#27861;&#36880;&#27493;&#23558;&#23646;&#24615;&#25237;&#24433;&#21040;&#22810;&#23610;&#24230;&#28508;&#22312;&#31354;&#38388;&#19978;&#12290;&#22810;&#23610;&#24230;&#26550;&#26500;&#20026;&#23646;&#24615;&#27010;&#29575;&#24314;&#27169;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#36890;&#36807;&#21333;&#20010;&#32593;&#32476;&#39044;&#27979;&#26368;&#23567;&#21270;&#32534;&#30721;&#27604;&#29305;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#21487;&#25193;&#23637;&#32534;&#30721;&#65292;&#21487;&#20197;&#20174;&#26080;&#25439;&#21387;&#32553;&#30340;&#27604;&#29305;&#27969;&#20013;&#36731;&#26494;&#25552;&#21462;&#36739;&#20302;&#36136;&#37327;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;MVUB&#21644;MPEG&#30340;&#19968;&#32452;&#28857;&#20113;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#26368;&#26032;&#30340;G-PCC&#29256;&#26412;14&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32534;&#30721;&#26102;&#38388;&#27604;G-PCC&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, several point cloud geometry compression methods that utilize advanced deep learning techniques have been proposed, but there are limited works on attribute compression, especially lossless compression. In this work, we build an end-to-end multiscale point cloud attribute coding method (MNeT) that progressively projects the attributes onto multiscale latent spaces. The multiscale architecture provides an accurate context for the attribute probability modeling and thus minimizes the coding bitrate with a single network prediction. Besides, our method allows scalable coding that lower quality versions can be easily extracted from the losslessly compressed bitstream. We validate our method on a set of point clouds from MVUB and MPEG and show that our method outperforms recently proposed methods and on par with the latest G-PCC version 14. Besides, our coding time is substantially faster than G-PCC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.06516</link><description>&lt;p&gt;
&#25171;&#24320;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20197;&#35745;&#31639;Shap&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Opening Up the Neural Network Classifier for Shap Score Computation. (arXiv:2303.06516v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#20013;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#36716;&#25442;&#20026;&#24067;&#23572;&#30005;&#36335;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#65292;&#23558;&#30005;&#36335;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#65292;&#30456;&#27604;&#20110;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient method for computing Shap explanation scores in machine learning model classification by transforming binary neural networks into Boolean circuits and treating the resulting circuit as an open-box model, which leads to a significant improvement in performance compared to computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;Shap&#35299;&#37322;&#20998;&#25968;&#30340;&#39640;&#25928;&#35745;&#31639;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#36716;&#25442;&#20026;&#30830;&#23450;&#24615;&#21644;&#21487;&#20998;&#35299;&#30340;&#24067;&#23572;&#30005;&#36335;&#65292;&#20351;&#29992;&#30693;&#35782;&#32534;&#35793;&#25216;&#26415;&#12290;&#25152;&#24471;&#21040;&#30340;&#30005;&#36335;&#34987;&#35270;&#20026;&#24320;&#25918;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#36817;&#30340;&#39640;&#25928;&#31639;&#27861;&#35745;&#31639;Shap&#20998;&#25968;&#12290;&#35814;&#32454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#23558;BNN&#35270;&#20026;&#40657;&#30418;&#27169;&#22411;&#30452;&#25509;&#35745;&#31639;Shap&#30456;&#27604;&#65292;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of efficiently computing Shap explanation scores for classifications with machine learning models. With this goal, we show the transformation of binary neural networks (BNNs) for classification into deterministic and decomposable Boolean circuits, for which knowledge compilation techniques are used. The resulting circuit is treated as an open-box model, to compute Shap scores by means of a recent efficient algorithm for this class of circuits. Detailed experiments show a considerable gain in performance in comparison with computing Shap directly on the BNN treated as a black-box model.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#32500;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#35268;&#27169;&#36739;&#22823;&#26102;&#20173;&#28982;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2303.06515</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#30340;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Multistage Stochastic Optimization via Kernels. (arXiv:2303.06515v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06515
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#32500;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#35268;&#27169;&#36739;&#22823;&#26102;&#20173;&#28982;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#12289;&#25968;&#25454;&#39537;&#21160;&#12289;&#21487;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#19981;&#24433;&#21709;&#19981;&#30830;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#20915;&#31574;&#21464;&#37327;&#34920;&#31034;&#20026;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#20803;&#32032;&#65292;&#24182;&#25191;&#34892;&#20989;&#25968;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#32463;&#39564;&#27491;&#21017;&#21270;&#25439;&#22833;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#20989;&#25968;&#23376;&#31354;&#38388;&#25237;&#24433;&#30340;&#31232;&#30095;&#21270;&#25216;&#26415;&#65292;&#25105;&#20204;&#33021;&#22815;&#20811;&#26381;&#26631;&#20934;&#26680;&#26041;&#27861;&#24341;&#20837;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#38543;&#30528;&#25968;&#25454;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#36741;&#21161;&#20449;&#24687;&#30340;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#20013;&#26159;&#28176;&#36817;&#26368;&#20248;&#30340;&#12290;&#22312;&#21508;&#31181;&#38543;&#26426;&#24211;&#23384;&#31649;&#29702;&#38382;&#39064;&#30340;&#35745;&#31639;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#32500;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#35268;&#27169;&#36739;&#22823;&#26102;&#20173;&#28982;&#21487;&#34892;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35745;&#31639;&#24211;&#23384;&#25511;&#21046;&#38382;&#39064;&#30340;&#26368;&#20248;&#25439;&#22833;&#30340;&#19979;&#30028;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a non-parametric, data-driven, tractable approach for solving multistage stochastic optimization problems in which decisions do not affect the uncertainty. The proposed framework represents the decision variables as elements of a reproducing kernel Hilbert space and performs functional stochastic gradient descent to minimize the empirical regularized loss. By incorporating sparsification techniques based on function subspace projections we are able to overcome the computational complexity that standard kernel methods introduce as the data size increases. We prove that the proposed approach is asymptotically optimal for multistage stochastic optimization with side information. Across various computational experiments on stochastic inventory management problems, {our method performs well in multidimensional settings} and remains tractable when the data size is large. Lastly, by computing lower bounds for the optimal loss of the inventory control problem, we show that the propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;SDN&#65289;&#29615;&#22659;&#20013;&#26816;&#27979;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#27979;&#35797;&#22235;&#31181;&#31639;&#27861;&#65292;&#20854;&#20013;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2303.06513</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#20013;&#30340;DDoS&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Detection of DDoS Attacks in Software Defined Networking Using Machine Learning Models. (arXiv:2303.06513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;SDN&#65289;&#29615;&#22659;&#20013;&#26816;&#27979;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#27979;&#35797;&#22235;&#31181;&#31639;&#27861;&#65292;&#20854;&#20013;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the effectiveness of using machine learning algorithms to detect distributed denial-of-service (DDoS) attacks in software-defined networking (SDN) environments, and tests four algorithms on the CICDDoS2019 dataset, with Random Forest performing the best.
&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;SDN&#65289;&#30340;&#27010;&#24565;&#20195;&#34920;&#20102;&#19968;&#31181;&#29616;&#20195;&#30340;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#32593;&#32476;&#25277;&#35937;&#23558;&#25511;&#21046;&#24179;&#38754;&#19982;&#25968;&#25454;&#24179;&#38754;&#20998;&#31163;&#65292;&#20174;&#32780;&#23454;&#29616;&#19982;&#20256;&#32479;&#32593;&#32476;&#30456;&#27604;&#26356;&#28789;&#27963;&#12289;&#21487;&#32534;&#31243;&#21644;&#21160;&#24577;&#30340;&#26550;&#26500;&#12290;&#25511;&#21046;&#24179;&#38754;&#21644;&#25968;&#25454;&#24179;&#38754;&#30340;&#20998;&#31163;&#23548;&#33268;&#20102;&#39640;&#24230;&#30340;&#32593;&#32476;&#24377;&#24615;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#21253;&#25324;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#36825;&#22312;SDN&#29615;&#22659;&#20013;&#26500;&#25104;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;SDN&#65289;&#29615;&#22659;&#20013;&#26816;&#27979;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;CICDDoS2019&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#22235;&#31181;&#31639;&#27861;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#12289;&#20915;&#31574;&#26641;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;XGBoost&#65292;&#20854;&#20013;&#26102;&#38388;&#25139;&#29305;&#24449;&#34987;&#21024;&#38500;&#31561;&#12290;&#36890;&#36807;&#20934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#35780;&#20272;&#20102;&#24615;&#33021;&#65292;&#20854;&#20013;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of Software Defined Networking (SDN) represents a modern approach to networking that separates the control plane from the data plane through network abstraction, resulting in a flexible, programmable and dynamic architecture compared to traditional networks. The separation of control and data planes has led to a high degree of network resilience, but has also given rise to new security risks, including the threat of distributed denial-of-service (DDoS) attacks, which pose a new challenge in the SDN environment. In this paper, the effectiveness of using machine learning algorithms to detect distributed denial-of-service (DDoS) attacks in software-defined networking (SDN) environments is investigated. Four algorithms, including Random Forest, Decision Tree, Support Vector Machine, and XGBoost, were tested on the CICDDoS2019 dataset, with the timestamp feature dropped among others. Performance was assessed by measures of accuracy, recall, accuracy, and F1 score, with the Rando
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20551;&#35774;&#65292;&#26377;&#25928;&#22320;&#21253;&#21547;&#20102;&#21407;&#22987;&#31070;&#32463;&#22349;&#22604;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#23567;&#21270;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#26368;&#22823;&#21270;&#31867;&#38388;&#21487;&#20998;&#24615;&#12290;&#20351;&#29992;&#36229;&#29699;&#32479;&#19968;&#24615;&#20316;&#20026;&#37327;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30446;&#26631;&#8212;&#8212;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#65288;HUG&#65289;&#65292;&#23427;&#30001;&#31867;&#38388;&#21644;&#31867;&#20869;&#36229;&#29699;&#32479;&#19968;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.06484</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#22635;&#34917;&#31070;&#32463;&#22349;&#22604;&#30340;&#27867;&#21270;&#21644;&#35299;&#32806;
&lt;/p&gt;
&lt;p&gt;
Generalizing and Decoupling Neural Collapse via Hyperspherical Uniformity Gap. (arXiv:2303.06484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20551;&#35774;&#65292;&#26377;&#25928;&#22320;&#21253;&#21547;&#20102;&#21407;&#22987;&#31070;&#32463;&#22349;&#22604;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#23567;&#21270;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#26368;&#22823;&#21270;&#31867;&#38388;&#21487;&#20998;&#24615;&#12290;&#20351;&#29992;&#36229;&#29699;&#32479;&#19968;&#24615;&#20316;&#20026;&#37327;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30446;&#26631;&#8212;&#8212;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#65288;HUG&#65289;&#65292;&#23427;&#30001;&#31867;&#38388;&#21644;&#31867;&#20869;&#36229;&#29699;&#32479;&#19968;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a generalized neural collapse hypothesis that effectively subsumes the original neural collapse and decomposes it into two objectives: minimizing intra-class variability and maximizing inter-class separability. The authors use hyperspherical uniformity as a unified framework to quantify these objectives and propose a general objective, hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical uniformity.
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22349;&#22604;&#29616;&#35937;&#25551;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24213;&#23618;&#20960;&#20309;&#23545;&#31216;&#24615;&#65292;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#21644;&#20998;&#31867;&#22120;&#37117;&#25910;&#25947;&#20110;&#19968;&#20010;&#31561;&#35282;&#32039;&#26694;&#26550;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22343;&#26041;&#35823;&#24046;&#37117;&#21487;&#20197;&#23548;&#33268;&#31070;&#32463;&#22349;&#22604;&#12290;&#25105;&#20204;&#28040;&#38500;&#20102;&#31070;&#32463;&#22349;&#22604;&#23545;&#29305;&#24449;&#32500;&#24230;&#21644;&#31867;&#21035;&#25968;&#37327;&#30340;&#20851;&#38190;&#20551;&#35774;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20551;&#35774;&#65292;&#26377;&#25928;&#22320;&#21253;&#21547;&#20102;&#21407;&#22987;&#31070;&#32463;&#22349;&#22604;&#12290;&#21463;&#31070;&#32463;&#22349;&#22604;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30446;&#26631;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#24191;&#20041;&#31070;&#32463;&#22349;&#22604;&#20998;&#35299;&#20026;&#20004;&#20010;&#30446;&#26631;&#65306;&#26368;&#23567;&#21270;&#31867;&#20869;&#21464;&#24322;&#24615;&#21644;&#26368;&#22823;&#21270;&#31867;&#38388;&#21487;&#20998;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#29699;&#32479;&#19968;&#24615;&#65288;&#23427;&#25551;&#36848;&#20102;&#21333;&#20301;&#36229;&#29699;&#19978;&#22343;&#21248;&#24615;&#30340;&#31243;&#24230;&#65289;&#20316;&#20026;&#37327;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30446;&#26631;&#8212;&#8212;&#36229;&#29699;&#32479;&#19968;&#24615;&#24046;&#65288;HUG&#65289;&#65292;&#23427;&#30001;&#31867;&#38388;&#21644;&#31867;&#20869;&#36229;&#29699;&#32479;&#19968;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural collapse (NC) phenomenon describes an underlying geometric symmetry for deep neural networks, where both deeply learned features and classifiers converge to a simplex equiangular tight frame. It has been shown that both cross-entropy loss and mean square error can provably lead to NC. We remove NC's key assumption on the feature dimension and the number of classes, and then present a generalized neural collapse (GNC) hypothesis that effectively subsumes the original NC. Inspired by how NC characterizes the training target of neural networks, we decouple GNC into two objectives: minimal intra-class variability and maximal inter-class separability. We then use hyperspherical uniformity (which characterizes the degree of uniformity on the unit hypersphere) as a unified framework to quantify these two objectives. Finally, we propose a general objective -- hyperspherical uniformity gap (HUG), which is defined by the difference between inter-class and intra-class hyperspherical un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20808;&#21069;&#36816;&#34892;&#20013;&#30340;&#35745;&#31639;&#26469;&#20943;&#23569;&#26410;&#26469;&#36816;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#65292;&#36890;&#36807;&#23558;&#26410;&#26469;&#36816;&#34892;&#19982;&#26469;&#33258;&#20808;&#21069;&#36816;&#34892;&#30340;KD&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;KD&#30340;&#24320;&#38144;&#38477;&#20302;&#20102;80-90&#65285;&#65292;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#65292;&#24182;&#22312;&#25972;&#20307;&#25104;&#26412;&#26041;&#38754;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.06480</link><description>&lt;p&gt;
&#39640;&#25928;&#35757;&#32451;&#24207;&#21015;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation for Efficient Sequences of Training Runs. (arXiv:2303.06480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20808;&#21069;&#36816;&#34892;&#20013;&#30340;&#35745;&#31639;&#26469;&#20943;&#23569;&#26410;&#26469;&#36816;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#65292;&#36890;&#36807;&#23558;&#26410;&#26469;&#36816;&#34892;&#19982;&#26469;&#33258;&#20808;&#21069;&#36816;&#34892;&#30340;KD&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;KD&#30340;&#24320;&#38144;&#38477;&#20302;&#20102;80-90&#65285;&#65292;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#65292;&#24182;&#22312;&#25972;&#20307;&#25104;&#26412;&#26041;&#38754;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies how to reduce the cost of future runs by utilizing the computation invested in previous runs using knowledge distillation (KD). Augmenting future runs with KD from previous runs dramatically reduces the time necessary to train these models, and the overhead of KD can be reduced by 80-90% with minimal effect on accuracy, resulting in vast pareto-improvements in overall cost.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#22914;&#36229;&#21442;&#25968;&#25628;&#32034;&#25110;&#20351;&#29992;&#26032;&#25968;&#25454;&#36827;&#34892;&#25345;&#32493;&#37325;&#26032;&#35757;&#32451;&#65292;&#30456;&#20851;&#30340;&#35757;&#32451;&#36816;&#34892;&#20250;&#25353;&#39034;&#24207;&#25191;&#34892;&#22810;&#27425;&#12290;&#30446;&#21069;&#30340;&#20570;&#27861;&#26159;&#20174;&#22836;&#24320;&#22987;&#29420;&#31435;&#35757;&#32451;&#27599;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#20808;&#21069;&#36816;&#34892;&#20013;&#30340;&#35745;&#31639;&#26469;&#20943;&#23569;&#26410;&#26469;&#36816;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23558;&#26410;&#26469;&#36816;&#34892;&#19982;&#26469;&#33258;&#20808;&#21069;&#36816;&#34892;&#30340;KD&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#21363;&#20351;&#32771;&#34385;&#21040;KD&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#25913;&#36827;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#23558;KD&#30340;&#24320;&#38144;&#38477;&#20302;&#20102;80-90&#65285;&#65292;&#23545;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#65292;&#24182;&#22312;&#25972;&#20307;&#25104;&#26412;&#26041;&#38754;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#24085;&#32047;&#25176;&#25913;&#36827;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;KD&#26159;&#20943;&#23569;&#23454;&#36341;&#20013;&#35757;&#32451;&#26368;&#32456;&#27169;&#22411;&#20043;&#21069;&#26114;&#36149;&#30340;&#20934;&#22791;&#24037;&#20316;&#25104;&#26412;&#30340;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many practical scenarios -- like hyperparameter search or continual retraining with new data -- related training runs are performed many times in sequence. Current practice is to train each of these models independently from scratch. We study the problem of exploiting the computation invested in previous runs to reduce the cost of future runs using knowledge distillation (KD). We find that augmenting future runs with KD from previous runs dramatically reduces the time necessary to train these models, even taking into account the overhead of KD. We improve on these results with two strategies that reduce the overhead of KD by 80-90% with minimal effect on accuracy and vast pareto-improvements in overall cost. We conclude that KD is a promising avenue for reducing the cost of the expensive preparatory work that precedes training final models in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06471</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#20195;&#30340;&#32959;&#30244;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review. (arXiv:2303.06471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20197;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This review article analyzes the application of deep neural networks in multimodal data integration to improve the accuracy and reliability of cancer diagnosis and treatment.
&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#22312;&#19981;&#21516;&#23610;&#24230;&#12289;&#27169;&#24577;&#21644;&#20998;&#36776;&#29575;&#30340;&#33719;&#21462;&#25968;&#25454;&#20013;&#20855;&#26377;&#20851;&#31995;&#20449;&#24687;&#65292;&#20363;&#22914;&#25918;&#23556;&#23398;&#12289;&#30149;&#29702;&#23398;&#12289;&#22522;&#22240;&#32452;&#23398;&#12289;&#34507;&#30333;&#36136;&#32452;&#23398;&#21644;&#20020;&#24202;&#35760;&#24405;&#12290;&#25972;&#21512;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#21487;&#20197;&#25552;&#39640;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#21487;&#33021;&#23384;&#22312;&#20154;&#31867;&#25110;&#29616;&#26377;&#25216;&#26415;&#24037;&#20855;&#26080;&#27861;&#35270;&#35273;&#19978;&#21306;&#20998;&#30340;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#21333;&#20010;&#23610;&#24230;&#30340;&#29983;&#29289;&#31995;&#32479;&#30340;&#37096;&#20998;&#25110;&#21333;&#19968;&#27169;&#24577;&#20449;&#24687;&#65292;&#24182;&#26410;&#28085;&#30422;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#23436;&#25972;&#20809;&#35889;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20419;&#36827;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#21644;&#25972;&#21512;&#30456;&#20851;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#22914;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#21464;&#21387;&#22120;&#65292;&#22312;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#25552;&#20379;&#20102;&#23545;&#24403;&#21069;&#22810;&#27169;&#24577;&#25968;&#25454;&#25972;&#21512;&#26041;&#27861;&#30340;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer has relational information residing at varying scales, modalities, and resolutions of the acquired data, such as radiology, pathology, genomics, proteomics, and clinical records. Integrating diverse data types can improve the accuracy and reliability of cancer diagnosis and treatment. There can be disease-related information that is too subtle for humans or existing technological tools to discern visually. Traditional methods typically focus on partial or unimodal information about biological systems at individual scales and fail to encapsulate the complete spectrum of the heterogeneous nature of data. Deep neural networks have facilitated the development of sophisticated multimodal data fusion approaches that can extract and integrate relevant information from multiple sources. Recent deep learning frameworks such as Graph Neural Networks (GNNs) and Transformers have shown remarkable success in multimodal learning. This review article provides an in-depth analysis of the state-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06470</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20998;&#23376;&#36136;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prefix-tree Decoding for Predicting Mass Spectra from Molecules. (arXiv:2303.06470v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms, and decoding the formula set using a prefix tree structure, atom-type by atom-type, overcoming the combinatorial possibilities for chemical subformulae.
&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#24050;&#32463;&#23454;&#29616;&#20102;&#20020;&#24202;&#30456;&#20851;&#20195;&#35874;&#29289;&#30340;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#39044;&#27979;&#24037;&#20855;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21344;&#25454;&#20102;&#20004;&#20010;&#26497;&#31471;&#65292;&#35201;&#20040;&#36890;&#36807;&#36807;&#24230;&#21018;&#24615;&#30340;&#32422;&#26463;&#21644;&#36739;&#24046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#32452;&#21512;&#20998;&#23376;&#26469;&#36827;&#34892;&#25805;&#20316;&#65292;&#35201;&#20040;&#36890;&#36807;&#35299;&#30721;&#26377;&#25439;&#21644;&#38750;&#29289;&#29702;&#31163;&#25955;&#21270;&#30340;&#20809;&#35889;&#21521;&#37327;&#26469;&#36827;&#34892;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20013;&#38388;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#36136;&#35889;&#35270;&#20026;&#21270;&#23398;&#20844;&#24335;&#30340;&#38598;&#21512;&#26469;&#39044;&#27979;&#20998;&#23376;&#30340;&#36136;&#35889;&#65292;&#36825;&#20123;&#21270;&#23398;&#20844;&#24335;&#26412;&#36523;&#26159;&#21407;&#23376;&#30340;&#22810;&#37325;&#38598;&#21512;&#12290;&#22312;&#39318;&#20808;&#23545;&#36755;&#20837;&#20998;&#23376;&#22270;&#36827;&#34892;&#32534;&#30721;&#21518;&#65292;&#25105;&#20204;&#35299;&#30721;&#19968;&#32452;&#21270;&#23398;&#23376;&#20844;&#24335;&#65292;&#27599;&#20010;&#21270;&#23398;&#23376;&#20844;&#24335;&#25351;&#23450;&#36136;&#35889;&#20013;&#30340;&#19968;&#20010;&#39044;&#27979;&#23792;&#65292;&#20854;&#24378;&#24230;&#30001;&#31532;&#20108;&#20010;&#27169;&#22411;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#36890;&#36807;&#20351;&#29992;&#21069;&#32512;&#26641;&#32467;&#26500;&#65292;&#36880;&#20010;&#21407;&#23376;&#31867;&#22411;&#22320;&#35299;&#30721;&#20844;&#24335;&#38598;&#65292;&#20811;&#26381;&#20102;&#21270;&#23398;&#23376;&#20844;&#24335;&#30340;&#32452;&#21512;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational predictions of mass spectra from molecules have enabled the discovery of clinically relevant metabolites. However, such predictive tools are still limited as they occupy one of two extremes, either operating (a) by fragmenting molecules combinatorially with overly rigid constraints on potential rearrangements and poor time complexity or (b) by decoding lossy and nonphysical discretized spectra vectors. In this work, we introduce a new intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms. After first encoding an input molecular graph, we decode a set of chemical subformulae, each of which specify a predicted peak in the mass spectra, the intensities of which are predicted by a second model. Our key insight is to overcome the combinatorial possibilities for chemical subformulae by decoding the formula set using a prefix tree structure, atom-type by atom-type, represent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#20114;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#24182;&#19988;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06455</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#22312;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network contextual embedding for Deep Learning on Tabular Data. (arXiv:2303.06455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#20114;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#24182;&#19988;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep learning model based on Graph Neural Network (GNN) with Interaction Network (IN) for contextual embedding, which outperforms the recent DL benchmark on five public datasets and achieves competitive results compared to boosted-tree solutions in tabular data processing.
&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#34892;&#19994;&#37117;&#35797;&#22270;&#21033;&#29992;&#29616;&#26377;&#30340;&#22823;&#25968;&#25454;&#36827;&#34892;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#20197;&#25152;&#35859;&#30340;&#34920;&#26684;&#24418;&#24335;&#23384;&#22312;&#65292;&#20854;&#20013;&#27599;&#20010;&#35760;&#24405;&#30001;&#35768;&#22810;&#24322;&#26500;&#30340;&#36830;&#32493;&#21644;&#20998;&#31867;&#21015;&#32452;&#25104;&#65292;&#20063;&#31216;&#20026;&#29305;&#24449;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#19982;&#20154;&#31867;&#25216;&#33021;&#30456;&#20851;&#30340;&#39046;&#22495;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#20854;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26356;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#20132;&#20114;&#32593;&#32476;&#65288;IN&#65289;&#65292;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#20854;&#32467;&#26524;&#20248;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#22522;&#20110;&#20116;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
All industries are trying to leverage Artificial Intelligence (AI) based on their existing big data which is available in so called tabular form, where each record is composed of a number of heterogeneous continuous and categorical columns also known as features. Deep Learning (DL) has consituted a major breathrough for AI in fields related to human skills like natural language processing, but its applicability to tabular data has been more challenging. More classical Machine Learning (ML) models like tree-based ensemble ones usually perform better. In this manuscript a novel DL model that uses Graph Neural Network (GNN), more specifically Interaction Network (IN), for contextual embedding is introduced. Its results outperform those of the recently published survey with DL benchmark based on five public datasets, achieving also competitive results when compared to boosted-tree solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#29699;&#35270;&#39057;&#22242;&#20307;&#27963;&#21160;&#35782;&#21035;&#25216;&#26415;DECOMPL&#65292;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#20998;&#25903;&#32452;&#25104;&#65292;&#20351;&#29992;&#36873;&#25321;&#24615;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#25552;&#21462;&#29305;&#24449;&#65292;&#32771;&#34385;&#21442;&#19982;&#32773;&#30340;&#24403;&#21069;&#37197;&#32622;&#65292;&#24182;&#20174;&#26694;&#22352;&#26631;&#20013;&#25552;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#25490;&#29699;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#26041;&#26696;&#38477;&#20302;&#20102;&#27963;&#21160;&#20013;&#30340;&#22242;&#20307;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2303.06439</link><description>&lt;p&gt;
DECOMPL: &#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#27744;&#21270;&#30340;&#20998;&#35299;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#21333;&#20010;&#25490;&#29699;&#22270;&#20687;&#20013;&#35782;&#21035;&#22242;&#20307;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
DECOMPL: Decompositional Learning with Attention Pooling for Group Activity Recognition from a Single Volleyball Image. (arXiv:2303.06439v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#29699;&#35270;&#39057;&#22242;&#20307;&#27963;&#21160;&#35782;&#21035;&#25216;&#26415;DECOMPL&#65292;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#20998;&#25903;&#32452;&#25104;&#65292;&#20351;&#29992;&#36873;&#25321;&#24615;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#25552;&#21462;&#29305;&#24449;&#65292;&#32771;&#34385;&#21442;&#19982;&#32773;&#30340;&#24403;&#21069;&#37197;&#32622;&#65292;&#24182;&#20174;&#26694;&#22352;&#26631;&#20013;&#25552;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#21457;&#29616;&#25490;&#29699;&#25968;&#25454;&#38598;&#30340;&#26631;&#31614;&#26041;&#26696;&#38477;&#20302;&#20102;&#27963;&#21160;&#20013;&#30340;&#22242;&#20307;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel GAR technique for volleyball videos, DECOMPL, which consists of two complementary branches, using selective attention pooling to extract features, considering the current configuration of actors and extracting spatial information from box coordinates. The paper also reveals that the labeling scheme of the Volleyball dataset degrades the group concept in activities.
&lt;/p&gt;
&lt;p&gt;
&#22242;&#20307;&#27963;&#21160;&#35782;&#21035;&#26088;&#22312;&#26816;&#27979;&#22330;&#26223;&#20013;&#22810;&#20010;&#21442;&#19982;&#32773;&#25191;&#34892;&#30340;&#27963;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#22522;&#20110;RGB&#12289;&#20809;&#27969;&#25110;&#20851;&#38190;&#28857;&#25968;&#25454;&#31867;&#22411;&#23545;&#26102;&#31354;&#29305;&#24449;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#20351;&#29992;&#26102;&#38388;&#24615;&#21644;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#20165;&#20351;&#29992;RGB&#25968;&#25454;&#32780;&#19981;&#32771;&#34385;&#26102;&#38388;&#24615;&#65292;&#21487;&#20197;&#22312;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#29699;&#35270;&#39057;&#22242;&#20307;&#27963;&#21160;&#35782;&#21035;&#25216;&#26415;DECOMPL&#65292;&#23427;&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#20998;&#25903;&#32452;&#25104;&#12290;&#22312;&#35270;&#35273;&#20998;&#25903;&#20013;&#65292;&#23427;&#20351;&#29992;&#36873;&#25321;&#24615;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#25552;&#21462;&#29305;&#24449;&#12290;&#22312;&#22352;&#26631;&#20998;&#25903;&#20013;&#65292;&#23427;&#32771;&#34385;&#21442;&#19982;&#32773;&#30340;&#24403;&#21069;&#37197;&#32622;&#65292;&#24182;&#20174;&#26694;&#22352;&#26631;&#20013;&#25552;&#21462;&#31354;&#38388;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25490;&#29699;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20854;&#26631;&#31614;&#26041;&#26696;&#38477;&#20302;&#20102;&#27963;&#21160;&#20013;&#30340;&#22242;&#20307;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group Activity Recognition (GAR) aims to detect the activity performed by multiple actors in a scene. Prior works model the spatio-temporal features based on the RGB, optical flow or keypoint data types. However, using both the temporality and these data types altogether increase the computational complexity significantly. Our hypothesis is that by only using the RGB data without temporality, the performance can be maintained with a negligible loss in accuracy. To that end, we propose a novel GAR technique for volleyball videos, DECOMPL, which consists of two complementary branches. In the visual branch, it extracts the features using attention pooling in a selective way. In the coordinate branch, it considers the current configuration of the actors and extracts the spatial information from the box coordinates. Moreover, we analyzed the Volleyball dataset that the recent literature is mostly based on, and realized that its labeling scheme degrades the group concept in the activities to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;OFDM&#20449;&#21495;&#30340;&#21333;&#36890;&#36947;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#36890;&#36807;&#21407;&#22411;&#38382;&#39064;&#35780;&#20272;&#20102;&#20351;&#29992;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20998;&#31163;&#20849;&#20449;&#36947;OFDM&#27874;&#24418;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#20462;&#25913;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.06438</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#28304;&#20998;&#31163;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65306;&#20849;&#20449;&#36947;OFDM&#20449;&#21495;&#30340;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
On Neural Architectures for Deep Learning-based Source Separation of Co-Channel OFDM Signals. (arXiv:2303.06438v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;OFDM&#20449;&#21495;&#30340;&#21333;&#36890;&#36947;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#36890;&#36807;&#21407;&#22411;&#38382;&#39064;&#35780;&#20272;&#20102;&#20351;&#29992;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20998;&#31163;&#20849;&#20449;&#36947;OFDM&#27874;&#24418;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#20462;&#25913;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the single-channel source separation problem involving OFDM signals and evaluates the efficacy of using audio-oriented neural architectures in separating co-channel OFDM waveforms. Critical domain-informed modifications to the network parameterization are proposed based on insights from OFDM structures.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28041;&#21450;&#27491;&#20132;&#39057;&#20998;&#22797;&#29992;&#65288;OFDM&#65289;&#20449;&#21495;&#30340;&#21333;&#36890;&#36947;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#36825;&#31181;&#20449;&#21495;&#22312;&#35768;&#22810;&#29616;&#20195;&#25968;&#23383;&#36890;&#20449;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#22312;&#21333;&#22768;&#36947;&#28304;&#20998;&#31163;&#26041;&#38754;&#24050;&#32463;&#36827;&#34892;&#20102;&#30456;&#20851;&#30340;&#21162;&#21147;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#35757;&#32451;&#31471;&#21040;&#31471;&#30340;&#38899;&#39057;&#20449;&#21495;&#20998;&#31163;&#22120;&#65288;&#20316;&#20026;&#19968;&#32500;&#26102;&#38388;&#24207;&#21015;&#65289;&#12290;&#36890;&#36807;&#22522;&#20110;OFDM&#28304;&#27169;&#22411;&#30340;&#21407;&#22411;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#24182;&#36136;&#30097;&#20102;&#20351;&#29992;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#22522;&#20110;&#36890;&#20449;&#27874;&#24418;&#30456;&#20851;&#29305;&#24449;&#20998;&#31163;&#20449;&#21495;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20063;&#35768;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#37197;&#32622;&#20013;&#65292;&#21363;&#20351;&#22312;&#29702;&#35770;&#19978;&#21487;&#20197;&#23454;&#29616;&#23436;&#32654;&#20998;&#31163;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#38754;&#21521;&#38899;&#39057;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;&#20998;&#31163;&#20849;&#20449;&#36947;OFDM&#27874;&#24418;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#30340;&#39046;&#22495;&#30693;&#35782;&#20462;&#25913;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#22522;&#20110;OFDM&#32467;&#26500;&#30340;&#27934;&#23519;&#65292;&#21487;&#20197;&#20849;&#21516;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the single-channel source separation problem involving orthogonal frequency-division multiplexing (OFDM) signals, which are ubiquitous in many modern-day digital communication systems. Related efforts have been pursued in monaural source separation, where state-of-the-art neural architectures have been adopted to train an end-to-end separator for audio signals (as 1-dimensional time series). In this work, through a prototype problem based on the OFDM source model, we assess -- and question -- the efficacy of using audio-oriented neural architectures in separating signals based on features pertinent to communication waveforms. Perhaps surprisingly, we demonstrate that in some configurations, where perfect separation is theoretically attainable, these audio-oriented neural architectures perform poorly in separating co-channel OFDM waveforms. Yet, we propose critical domain-informed modifications to the network parameterization, based on insights from OFDM structures, that can co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21019;&#24314;&#20102;&#21453;&#34394;&#20551;&#20449;&#24687;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#24110;&#21161;&#26222;&#36890;&#29992;&#25143;&#26377;&#25928;&#32416;&#27491;&#34394;&#20551;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.06433</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#34394;&#20551;&#20449;&#24687;&#21709;&#24212;&#29983;&#25104;&#65306;&#20197;COVID-19&#30123;&#33495;&#34394;&#20551;&#20449;&#24687;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-based Counter-Misinformation Response Generation: A Case Study of COVID-19 Vaccine Misinformation. (arXiv:2303.06433v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21019;&#24314;&#20102;&#21453;&#34394;&#20551;&#20449;&#24687;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#24110;&#21161;&#26222;&#36890;&#29992;&#25143;&#26377;&#25928;&#32416;&#27491;&#34394;&#20551;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study creates a counter-misinformation response generation model using reinforcement learning algorithm to empower ordinary users to effectively correct misinformation.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#23041;&#32961;&#30528;&#20844;&#20849;&#21355;&#29983;&#12289;&#27665;&#20027;&#21644;&#26356;&#24191;&#27867;&#30340;&#31038;&#20250;&#12290;&#26412;&#25991;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#21453;&#34394;&#20551;&#20449;&#24687;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#36171;&#20104;&#29992;&#25143;&#26377;&#25928;&#32416;&#27491;&#34394;&#20551;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#20004;&#20010;&#34394;&#20551;&#20449;&#24687;&#21644;&#21453;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#26377;&#25928;&#30340;&#21453;&#34394;&#20551;&#20449;&#24687;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of online misinformation threatens public health, democracy, and the broader society. While professional fact-checkers form the first line of defense by fact-checking popular false claims, they do not engage directly in conversations with misinformation spreaders. On the other hand, non-expert ordinary users act as eyes-on-the-ground who proactively counter misinformation -- recent research has shown that 96% counter-misinformation responses are made by ordinary users. However, research also found that 2/3 times, these responses are rude and lack evidence. This work seeks to create a counter-misinformation response generation model to empower users to effectively correct misinformation. This objective is challenging due to the absence of datasets containing ground-truth of ideal counter-misinformation responses, and the lack of models that can generate responses backed by communication theories. In this work, we create two novel datasets of misinformation and counter-misinfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#27491;&#24120;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#23558;&#36755;&#20837;&#26679;&#26412;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#20174;&#28508;&#22312;&#21521;&#37327;&#20013;&#37325;&#26500;&#36755;&#20986;&#26679;&#26412;&#65292;&#26368;&#32456;&#23558;&#37325;&#26500;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#28508;&#22312;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#21644;&#32534;&#30721;&#26469;&#20248;&#21270;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.06431</link><description>&lt;p&gt;
&#24102;&#26377;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#38598;&#25104;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Anomaly Detection with Ensemble of Encoder and Decoder. (arXiv:2303.06431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#27491;&#24120;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#23558;&#36755;&#20837;&#26679;&#26412;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#20174;&#28508;&#22312;&#21521;&#37327;&#20013;&#37325;&#26500;&#36755;&#20986;&#26679;&#26412;&#65292;&#26368;&#32456;&#23558;&#37325;&#26500;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#28508;&#22312;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#21644;&#32534;&#30721;&#26469;&#20248;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel anomaly detection method by modeling the data distribution of normal samples via multiple encoders and decoders, mapping input samples into a latent space, reconstructing output samples from latent vectors, and mapping reconstructed samples to latent representations. Parameters are optimized during the training phase by minimizing the reconstruction loss and encoding.
&lt;/p&gt;
&lt;p&gt;
&#40657;&#23458;&#21644;&#34394;&#20551;&#25968;&#25454;&#27880;&#20837;&#21487;&#33021;&#20250;&#23041;&#32961;&#30005;&#32593;&#30340;&#26085;&#24120;&#36816;&#33829;&#24182;&#36896;&#25104;&#37325;&#22823;&#32463;&#27982;&#25439;&#22833;&#12290;&#30005;&#32593;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26088;&#22312;&#26816;&#27979;&#21644;&#21306;&#20998;&#30001;&#38024;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#32593;&#32476;&#25915;&#20987;&#24341;&#36215;&#30340;&#24322;&#24120;&#65292;&#36825;&#23545;&#20110;&#20445;&#25345;&#30005;&#32593;&#30340;&#27491;&#24120;&#36816;&#34892;&#21644;&#39640;&#25928;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24212;&#29992;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#20363;&#22914;&#32479;&#35745;&#26041;&#27861;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#24120;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#38656;&#35201;&#23545;&#27491;&#24120;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#27491;&#24120;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#36755;&#20837;&#26679;&#26412;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#20174;&#28508;&#22312;&#21521;&#37327;&#20013;&#37325;&#26500;&#36755;&#20986;&#26679;&#26412;&#12290;&#39069;&#22806;&#30340;&#32534;&#30721;&#22120;&#26368;&#32456;&#23558;&#37325;&#26500;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#28508;&#22312;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;&#37325;&#26500;&#25439;&#22833;&#21644;&#32534;&#30721;&#26469;&#20248;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hacking and false data injection from adversaries can threaten power grids' everyday operations and cause significant economic loss. Anomaly detection in power grids aims to detect and discriminate anomalies caused by cyber attacks against the power system, which is essential for keeping power grids working correctly and efficiently. Different methods have been applied for anomaly detection, such as statistical methods and machine learning-based methods. Usually, machine learning-based methods need to model the normal data distribution. In this work, we propose a novel anomaly detection method by modeling the data distribution of normal samples via multiple encoders and decoders. Specifically, the proposed method maps input samples into a latent space and then reconstructs output samples from latent vectors. The extra encoder finally maps reconstructed samples to latent representations. During the training phase, we optimize parameters by minimizing the reconstruction loss and encoding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;iMIIC&#65289;&#65292;&#24182;&#22312;&#26469;&#33258;&#32654;&#22269;&#30417;&#27979;&#12289;&#27969;&#34892;&#30149;&#23398;&#21644;&#32456;&#26411;&#32467;&#26524;&#35745;&#21010;&#30340;396,179&#21517;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20854;&#29420;&#29305;&#33021;&#21147;&#12290;&#36229;&#36807;90&#65285;&#30340;&#39044;&#27979;&#22240;&#26524;&#25928;&#24212;&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#20854;&#20313;&#30340;&#24847;&#22806;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#21487;&#20197;&#35299;&#37322;&#20026;&#35786;&#26029;&#31243;&#24207;&#12289;&#27835;&#30103;&#26102;&#38388;&#12289;&#24739;&#32773;&#20559;&#22909;&#25110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.06423</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#32593;&#32476;&#65292;&#20197;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;40&#19975;&#20221;&#21307;&#30103;&#35760;&#24405;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable causal networks from very large datasets, application to 400,000 medical records of breast cancer patients. (arXiv:2303.06423v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;iMIIC&#65289;&#65292;&#24182;&#22312;&#26469;&#33258;&#32654;&#22269;&#30417;&#27979;&#12289;&#27969;&#34892;&#30149;&#23398;&#21644;&#32456;&#26411;&#32467;&#26524;&#35745;&#21010;&#30340;396,179&#21517;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20854;&#29420;&#29305;&#33021;&#21147;&#12290;&#36229;&#36807;90&#65285;&#30340;&#39044;&#27979;&#22240;&#26524;&#25928;&#24212;&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#20854;&#20313;&#30340;&#24847;&#22806;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#21487;&#20197;&#35299;&#37322;&#20026;&#35786;&#26029;&#31243;&#24207;&#12289;&#27835;&#30103;&#26102;&#38388;&#12289;&#24739;&#32773;&#20559;&#22909;&#25110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a more reliable and scalable causal discovery method (iMIIC) and showcases its unique capabilities on healthcare data from 396,179 breast cancer patients from the US Surveillance, Epidemiology, and End Results program. Over 90% of predicted causal effects appear correct, while the remaining unexpected direct and indirect causal effects can be interpreted in terms of diagnostic procedures, therapeutic timing, patient preference or socio-economic disparity.
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#22240;&#26524;&#25928;&#24212;&#26159;&#31185;&#23398;&#30740;&#31350;&#30340;&#26680;&#24515;&#65292;&#20294;&#24403;&#21482;&#26377;&#35266;&#23519;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#36825;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#22240;&#26524;&#32593;&#32476;&#38590;&#20197;&#23398;&#20064;&#21644;&#35299;&#37322;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21487;&#25193;&#23637;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65288;iMIIC&#65289;&#65292;&#22522;&#20110;&#19968;&#33324;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21407;&#21017;&#65292;&#23427;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25512;&#26029;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#21306;&#20998;&#20102;&#30495;&#27491;&#30340;&#21407;&#22240;&#21644;&#20551;&#23450;&#30340;&#21644;&#28508;&#22312;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;iMIIC&#22312;&#26469;&#33258;&#32654;&#22269;&#30417;&#27979;&#12289;&#27969;&#34892;&#30149;&#23398;&#21644;&#32456;&#26411;&#32467;&#26524;&#35745;&#21010;&#30340;396,179&#21517;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#21512;&#25104;&#21644;&#29616;&#23454;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#19978;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#36229;&#36807;90&#65285;&#30340;&#39044;&#27979;&#22240;&#26524;&#25928;&#24212;&#26159;&#27491;&#30830;&#30340;&#65292;&#32780;&#20854;&#20313;&#30340;&#24847;&#22806;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#21487;&#20197;&#35299;&#37322;&#20026;&#35786;&#26029;&#31243;&#24207;&#12289;&#27835;&#30103;&#26102;&#38388;&#12289;&#24739;&#32773;&#20559;&#22909;&#25110;&#31038;&#20250;&#32463;&#27982;&#24046;&#36317;&#12290;iMIIC&#30340;&#29420;&#29305;&#33021;&#21147;&#24320;&#36767;&#20102;&#21457;&#29616;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#32593;&#32476;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering causal effects is at the core of scientific investigation but remains challenging when only observational data is available. In practice, causal networks are difficult to learn and interpret, and limited to relatively small datasets. We report a more reliable and scalable causal discovery method (iMIIC), based on a general mutual information supremum principle, which greatly improves the precision of inferred causal relations while distinguishing genuine causes from putative and latent causal effects. We showcase iMIIC on synthetic and real-life healthcare data from 396,179 breast cancer patients from the US Surveillance, Epidemiology, and End Results program. More than 90\% of predicted causal effects appear correct, while the remaining unexpected direct and indirect causal effects can be interpreted in terms of diagnostic procedures, therapeutic timing, patient preference or socio-economic disparity. iMIIC's unique capabilities open up new avenues to discover reliable and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#20174;&#35299;&#37322;&#65288;MLX&#65289;&#37325;&#26032;&#26500;&#24314;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#25351;&#23450;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23545;&#24378;&#21442;&#25968;&#27491;&#21017;&#21270;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06419</link><description>&lt;p&gt;
&#20174;&#35299;&#37322;&#20013;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning from Explanations. (arXiv:2303.06419v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#20174;&#35299;&#37322;&#65288;MLX&#65289;&#37325;&#26032;&#26500;&#24314;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#20154;&#31867;&#25552;&#20379;&#30340;&#35299;&#37322;&#26469;&#25351;&#23450;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#23545;&#24378;&#21442;&#25968;&#27491;&#21017;&#21270;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new machine learning approach, recasting machine learning from explanations (MLX) as an adversarial robustness problem, which specifies a lower dimensional manifold from which perturbations can be drawn based on human-provided annotations, and shows improved performance over prior MLX methods on both synthetic and real-world benchmarks.
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20174;&#35299;&#37322;&#65288;MLX&#65289;&#26159;&#19968;&#31181;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20154;&#31867;&#25552;&#20379;&#30340;&#26377;&#20851;&#27599;&#20010;&#36755;&#20837;&#30340;&#30456;&#20851;&#29305;&#24449;&#30340;&#27880;&#37322;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#39044;&#27979;&#30340;&#21407;&#22240;&#27491;&#30830;&#12290;&#29616;&#26377;&#30340;MLX&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#38656;&#35201;&#24378;&#22823;&#30340;&#21442;&#25968;&#27491;&#21017;&#21270;&#26469;&#23545;&#40784;&#27169;&#22411;&#21644;&#20154;&#31867;&#35299;&#37322;&#65292;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;MLX&#37325;&#26032;&#26500;&#24314;&#20026;&#23545;&#25239;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#20154;&#31867;&#35299;&#37322;&#25351;&#23450;&#20102;&#19968;&#20010;&#20302;&#32500;&#27969;&#24418;&#65292;&#21487;&#20197;&#20174;&#20013;&#32472;&#21046;&#25200;&#21160;&#65292;&#24182;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#20943;&#36731;&#23545;&#24378;&#21442;&#25968;&#27491;&#21017;&#21270;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#23454;&#29616;&#40065;&#26834;&#24615;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20808;&#21069;MLX&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#40065;&#26834;&#24615;&#19982;&#26089;&#26399;&#30340;MLX&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning from explanations (MLX) is an approach to learning that uses human-provided annotations of relevant features for each input to ensure that model predictions are right for the right reasons. Existing MLX approaches rely heavily on a specific model interpretation approach and require strong parameter regularization to align model and human explanations, leading to sub-optimal performance. We recast MLX as an adversarial robustness problem, where human explanations specify a lower dimensional manifold from which perturbations can be drawn, and show both theoretically and empirically how this approach alleviates the need for strong parameter regularization. We consider various approaches to achieving robustness, leading to improved performance over prior MLX methods. Finally, we combine robustness with an earlier MLX method, yielding state-of-the-art results on both synthetic and real-world benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;ADNI&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06410</link><description>&lt;p&gt;
Brain Diffuser&#65306;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#33041;&#22270;&#20687;&#21040;&#33041;&#32593;&#32476;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Brain Diffuser: An End-to-End Brain Image to Brain Network Pipeline. (arXiv:2303.06410v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;ADNI&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.
&lt;/p&gt;
&lt;p&gt;
&#33041;&#32593;&#32476;&#20998;&#26512;&#23545;&#20110;&#35786;&#26029;&#21644;&#24178;&#39044;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#32791;&#26102;&#21644;&#20027;&#35266;&#30340;&#24037;&#20855;&#21253;&#12290;&#21482;&#26377;&#23569;&#25968;&#24037;&#20855;&#21487;&#20197;&#20174;&#33041;&#25193;&#25955;&#24352;&#37327;&#22270;&#20687;&#65288;DTI&#65289;&#20013;&#33719;&#21462;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#31471;&#21040;&#31471;&#33041;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;Brain Diffuser&#65292;&#30452;&#25509;&#20174;DTI&#20013;&#24418;&#25104;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#12290;&#19982;&#29616;&#26377;&#24037;&#20855;&#21253;&#30456;&#27604;&#65292;Brain Diffuser&#36890;&#36807;&#20998;&#26512;&#21463;&#35797;&#32773;&#20043;&#38388;&#32467;&#26500;&#24615;&#33041;&#32593;&#32476;&#30340;&#24046;&#24322;&#65292;&#21033;&#29992;&#26356;&#22810;&#30340;&#32467;&#26500;&#36830;&#25509;&#29305;&#24449;&#21644;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#23545;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#24773;&#20917;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#23398;&#20513;&#35758;&#65288;ADNI&#65289;&#25968;&#25454;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#24037;&#20855;&#21253;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain network analysis is essential for diagnosing and intervention for Alzheimer's disease (AD). However, previous research relied primarily on specific time-consuming and subjective toolkits. Only few tools can obtain the structural brain networks from brain diffusion tensor images (DTI). In this paper, we propose a diffusion based end-to-end brain network generative model Brain Diffuser that directly shapes the structural brain networks from DTI. Compared to existing toolkits, Brain Diffuser exploits more structural connectivity features and disease-related information by analyzing disparities in structural brain networks across subjects. For the case of Alzheimer's disease, the proposed model performs better than the results from existing toolkits on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#33258;&#21160;&#26816;&#27979;&#36741;&#21161;&#29356;&#39044;&#27979;&#20154;&#31867;&#30315;&#30187;&#21457;&#20316;&#26102;&#30340;&#20449;&#21495;&#34892;&#20026;&#65292;&#20197;&#25552;&#39640;&#30315;&#30187;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06407</link><description>&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#36741;&#21161;&#29356;&#39044;&#27979;&#20154;&#31867;&#30315;&#30187;&#21457;&#20316;&#26102;&#30340;&#20449;&#21495;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Automatic Detection of Signalling Behaviour from Assistance Dogs as they Forecast the Onset of Epileptic Seizures in Humans. (arXiv:2303.06407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#33258;&#21160;&#26816;&#27979;&#36741;&#21161;&#29356;&#39044;&#27979;&#20154;&#31867;&#30315;&#30187;&#21457;&#20316;&#26102;&#30340;&#20449;&#21495;&#34892;&#20026;&#65292;&#20197;&#25552;&#39640;&#30315;&#30187;&#24739;&#32773;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores how to automatically detect signalling behaviour from assistance dogs as they forecast the onset of epileptic seizures in humans, to improve the quality of life for epilepsy patients.
&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19990;&#30028;&#19978;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20043;&#19968;&#65292;&#24433;&#21709;&#30528;&#25968;&#30334;&#19975;&#20154;&#12290;&#30315;&#30187;&#21457;&#20316;&#36890;&#24120;&#26159;&#30001;&#20110;&#20154;&#33041;&#20013;&#30340;&#38750;&#21327;&#35843;&#30005;&#25918;&#30005;&#24341;&#36215;&#30340;&#65292;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#65292;&#21253;&#25324;&#20498;&#22320;&#21644;&#22833;&#21435;&#24847;&#35782;&#12290;&#22914;&#26524;&#33021;&#22815;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#24320;&#22987;&#65292;&#37027;&#20040;&#21487;&#20197;&#23558;&#21463;&#35797;&#32773;&#32622;&#20110;&#23433;&#20840;&#30340;&#29615;&#22659;&#25110;&#20301;&#32622;&#65292;&#20197;&#26368;&#23567;&#21270;&#30001;&#20110;&#20498;&#22320;&#32780;&#23548;&#33268;&#30340;&#33258;&#25105;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#22312;&#26085;&#24120;&#30340;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#30315;&#30187;&#21457;&#20316;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23456;&#29289;&#29399;&#26377;&#33021;&#21147;&#36890;&#36807;&#21957;&#25506;&#21463;&#35797;&#32773;&#22312;&#30315;&#30187;&#21457;&#20316;&#21069;&#30382;&#32932;&#25955;&#21457;&#30340;&#29305;&#24449;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#26469;&#26816;&#27979;&#30315;&#30187;&#21457;&#20316;&#30340;&#24320;&#22987;&#65292;&#26377;&#20123;&#36741;&#21161;&#29356;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#21521;&#20854;&#20027;&#20154;/&#35757;&#32451;&#21592;&#21457;&#20986;&#20449;&#21495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22914;&#20309;&#33258;&#21160;&#26816;&#27979;&#20449;&#21495;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy or the occurrence of epileptic seizures, is one of the world's most well-known neurological disorders affecting millions of people. Seizures mostly occur due to non-coordinated electrical discharges in the human brain and may cause damage, including collapse and loss of consciousness. If the onset of a seizure can be forecast then the subject can be placed into a safe environment or position so that self-injury as a result of a collapse can be minimised. However there are no definitive methods to predict seizures in an everyday, uncontrolled environment. Previous studies have shown that pet dogs have the ability to detect the onset of an epileptic seizure by scenting the characteristic volatile organic compounds exuded through the skin by a subject prior a seizure occurring and there are cases where assistance dogs, trained to scent the onset of a seizure, can signal this to their owner/trainer. In this work we identify how we can automatically detect the signalling behaviours
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#36951;&#25022;&#31639;&#27861;&#65292;&#29992;&#20110;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;$c_\alpha$-&#36817;&#20284;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#20854;&#20013;&#36817;&#20284;&#22240;&#23376;$c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445$&#65292;&#23545;&#20110;$0\leq \alpha &lt; 1$&#12290;</title><link>http://arxiv.org/abs/2303.06396</link><description>&lt;p&gt;
&#26080;&#36951;&#25022;&#31639;&#27861;&#29992;&#20110;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
No-regret Algorithms for Fair Resource Allocation. (arXiv:2303.06396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#36951;&#25022;&#31639;&#27861;&#65292;&#29992;&#20110;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23454;&#29616;$c_\alpha$-&#36817;&#20284;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#20854;&#20013;&#36817;&#20284;&#22240;&#23376;$c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445$&#65292;&#23545;&#20110;$0\leq \alpha &lt; 1$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a no-regret algorithm for fair resource allocation, which achieves $c_\alpha$-approximate sublinear regret with the approximation factor $c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445,$ for $0\leq \alpha &lt; 1$.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#20844;&#24179;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#26080;&#36951;&#25022;&#35774;&#32622;&#19979;&#38024;&#23545;&#26080;&#38480;&#21046;&#30340;&#23545;&#25163;&#12290;&#30446;&#26631;&#26159;&#20197;&#22312;&#32447;&#26041;&#24335;&#20844;&#24179;&#22320;&#20998;&#37197;&#22810;&#20010;&#20195;&#29702;&#30340;&#36164;&#28304;&#65292;&#20351;&#24471;&#26368;&#20248;&#38745;&#24577;&#39044;&#30693;&#20998;&#37197;&#21644;&#22312;&#32447;&#31574;&#30053;&#30340;&#20195;&#29702;&#30340;&#32858;&#21512;&#945;-&#20844;&#24179;&#25928;&#29992;&#20043;&#24046;&#38543;&#26102;&#38388;&#22686;&#38271;&#30340;&#36895;&#24230;&#20026;&#27425;&#32447;&#24615;&#12290;&#30001;&#20110;&#945;-&#20844;&#24179;&#24615;&#20989;&#25968;&#30340;&#38750;&#21152;&#24615;&#29305;&#24615;&#65292;&#35813;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#38382;&#39064;&#19981;&#23384;&#22312;&#20855;&#26377;&#27425;&#32447;&#24615;&#26631;&#20934;&#36951;&#25022;&#30340;&#22312;&#32447;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#65292;&#31216;&#20026;&#22312;&#32447;&#27604;&#20363;&#20844;&#24179;&#65288;OPF&#65289;&#65292;&#35813;&#31574;&#30053;&#23454;&#29616;&#20102;$c_\alpha$-&#36817;&#20284;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#20854;&#20013;&#36817;&#20284;&#22240;&#23376;$c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445$&#65292;&#23545;&#20110;$0\leq \alpha &lt; 1$&#12290;&#35813;&#38382;&#39064;&#30340;$c_\alpha$-&#36951;&#25022;&#19978;&#30028;&#23637;&#29616;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#30456;&#21464;&#29616;&#35937;&#12290;&#36951;&#25022;&#19978;&#30028;&#20174;&#19968;&#20010;&#24130;&#20989;&#25968;&#21464;&#20026;&#19968;&#20010;&#23545;&#25968;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a fair resource allocation problem in the no-regret setting against an unrestricted adversary. The objective is to allocate resources equitably among several agents in an online fashion so that the difference of the aggregate $\alpha$-fair utilities of the agents between an optimal static clairvoyant allocation and that of the online policy grows sub-linearly with time. The problem is challenging due to the non-additive nature of the $\alpha$-fairness function. Previously, it was shown that no online policy can exist for this problem with a sublinear standard regret. In this paper, we propose an efficient online resource allocation policy, called Online Proportional Fair (OPF), that achieves $c_\alpha$-approximate sublinear regret with the approximation factor $c_\alpha=(1-\alpha)^{-(1-\alpha)}\leq 1.445,$ for $0\leq \alpha &lt; 1$. The upper bound to the $c_\alpha$-regret for this problem exhibits a surprising phase transition phenomenon. The regret bound changes from a power
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#31227;&#21160;&#21069;&#27839;&#12289;&#25968;&#25454;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32463;&#39564;&#23567;&#27874;&#21464;&#25442;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#21069;&#27839;&#26041;&#27861;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#12290;</title><link>http://arxiv.org/abs/2303.06394</link><description>&lt;p&gt;
&#19968;&#31181;&#32467;&#21512;&#31227;&#21160;&#21069;&#27839;&#12289;&#25968;&#25454;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
A Novel Method Combines Moving Fronts, Data Decomposition and Deep Learning to Forecast Intricate Time Series. (arXiv:2303.06394v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#31227;&#21160;&#21069;&#27839;&#12289;&#25968;&#25454;&#20998;&#35299;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#29992;&#20110;&#39044;&#27979;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32463;&#39564;&#23567;&#27874;&#21464;&#25442;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#21069;&#27839;&#26041;&#27861;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel method that combines moving fronts, data decomposition, and deep learning to forecast intricate time series. The method decomposes the time series into simpler constituent series using empirical wavelet transform and prevents data leakage using the moving front method.
&lt;/p&gt;
&lt;p&gt;
&#39640;&#21464;&#24322;&#24615;&#30340;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#29978;&#33267;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20063;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34987;&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#23427;&#20204;&#30340;&#24635;&#21644;&#31561;&#20110;&#21407;&#22987;&#24207;&#21015;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#20256;&#32479;&#30340;&#19968;&#27425;&#20998;&#35299;&#25216;&#26415;&#23384;&#22312;&#25968;&#25454;&#27844;&#28431;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31227;&#21160;&#21069;&#27839;&#65288;MF&#65289;&#26041;&#27861;&#26469;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#65292;&#20351;&#20998;&#35299;&#21518;&#30340;&#24207;&#21015;&#21487;&#20197;&#20687;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#19968;&#26679;&#22788;&#29702;&#12290;&#21360;&#24230;&#22799;&#23395;&#23395;&#39118;&#38477;&#38632;&#65288;ISMR&#65289;&#26159;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#23545;DNN&#26500;&#25104;&#25361;&#25112;&#65292;&#22240;&#27492;&#34987;&#36873;&#20026;&#31034;&#20363;&#12290;&#20174;&#20247;&#22810;&#21487;&#29992;&#30340;&#20449;&#21495;&#22788;&#29702;&#24037;&#20855;&#20013;&#65292;&#32463;&#39564;&#23567;&#27874;&#21464;&#25442;&#65288;EWT&#65289;&#34987;&#36873;&#25321;&#29992;&#20110;&#23558;ISMR&#20998;&#35299;&#25104;&#26356;&#31616;&#21333;&#30340;&#32452;&#25104;&#24207;&#21015;&#65292;&#22240;&#20026;&#23427;&#34987;&#21457;&#29616;&#27604;&#20854;&#20182;&#27969;&#34892;&#30340;&#31639;&#27861;&#65292;&#22914;&#33258;&#36866;&#24212;&#22122;&#22768;&#23436;&#20840;&#38598;&#21512;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#65288;CEEMDAN&#65289;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
A univariate time series with high variability can pose a challenge even to Deep Neural Network (DNN). To overcome this, a univariate time series is decomposed into simpler constituent series, whose sum equals the original series. As demonstrated in this article, the conventional one-time decomposition technique suffers from a leak of information from the future, referred to as a data leak. In this work, a novel Moving Front (MF) method is proposed to prevent data leakage, so that the decomposed series can be treated like other time series. Indian Summer Monsoon Rainfall (ISMR) is a very complex time series, which poses a challenge to DNN and is therefore selected as an example. From the many signal processing tools available, Empirical Wavelet Transform (EWT) was chosen for decomposing the ISMR into simpler constituent series, as it was found to be more effective than the other popular algorithm, Complete Ensemble Empirical Mode Decomposition with Adaptive Noise (CEEMDAN). The propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#65292;&#36890;&#36807;&#26126;&#30830;&#27169;&#25311;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06389</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#31163;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Off-Policy Learning. (arXiv:2303.06389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#65292;&#29992;&#20110;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#65292;&#36890;&#36807;&#26126;&#30830;&#27169;&#25311;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning, which explicitly models the uncertainty in the estimated logging policy and demonstrates advantageous sample efficiency against an extensive list of state-of-the-art baselines on synthetic and three real-world recommendation datasets.
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#23398;&#20064;&#26159;&#25351;&#20165;&#36890;&#36807;&#35760;&#24405;&#30340;&#21453;&#39304;&#25968;&#25454;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#22312;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#37325;&#35201;&#24615;&#65292;&#20363;&#22914;&#25628;&#32034;&#24341;&#25806;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#12290;&#34429;&#28982;&#29983;&#25104;&#35760;&#24405;&#25968;&#25454;&#30340;&#30495;&#23454;&#35760;&#24405;&#31574;&#30053;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#20294;&#20197;&#21069;&#30340;&#24037;&#20316;&#20165;&#22312;&#31163;&#32447;&#23398;&#20064;&#20013;&#37319;&#29992;&#20854;&#20272;&#35745;&#20540;&#65292;&#24573;&#30053;&#20102;&#30001;&#20110;&#36825;&#31181;&#20272;&#35745;&#22120;&#23548;&#33268;&#30340;&#39640;&#20559;&#24046;&#21644;&#39640;&#26041;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#23567;&#19988;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#35760;&#24405;&#27010;&#29575;&#30340;&#26679;&#26412;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#27169;&#25311;&#20102;&#20272;&#35745;&#30340;&#35760;&#24405;&#31574;&#30053;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20498;&#25968;&#27010;&#29575;&#20998;&#25968;&#20272;&#35745;&#22120;&#65288;UIPS&#65289;&#26469;&#25913;&#36827;&#31163;&#32447;&#23398;&#20064;&#12290;&#22312;&#21512;&#25104;&#21644;&#19977;&#20010;&#30495;&#23454;&#30340;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;UIPS&#20272;&#35745;&#22120;&#30456;&#23545;&#20110;&#24191;&#27867;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#20855;&#26377;&#20248;&#36234;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning, referring to the procedure of policy optimization with access only to logged feedback data, has shown importance in various real-world applications, such as search engines, recommender systems, and etc. While the ground-truth logging policy, which generates the logged data, is usually unknown, previous work simply takes its estimated value in off-policy learning, ignoring both high bias and high variance resulted from such an estimator, especially on samples with small and inaccurately estimated logging probabilities. In this work, we explicitly model the uncertainty in the estimated logging policy and propose a Uncertainty-aware Inverse Propensity Score estimator (UIPS) for improved off-policy learning. Experiment results on synthetic and three real-world recommendation datasets demonstrate the advantageous sample efficiency of the proposed UIPS estimator against an extensive list of state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20020;&#24202;&#33041;&#30005;&#22270;&#20998;&#31867;&#20013;&#31383;&#21475;&#26631;&#31614;&#21487;&#33021;&#35823;&#23548;&#30340;&#38382;&#39064;&#65306;&#22686;&#21152;&#31383;&#21475;&#38271;&#24230;&#21644;&#24341;&#20837;&#31532;&#20108;&#38454;&#27573;&#27169;&#22411;&#26469;&#20210;&#35009;&#35760;&#24405;&#20869;&#30340;&#31383;&#21475;&#29305;&#23450;&#39044;&#27979;&#12290;&#22312;Temple&#22823;&#23398;&#21307;&#38498;&#24322;&#24120;&#33041;&#30005;&#22270;&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#26368;&#20808;&#36827;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20174;89.8&#65285;&#26174;&#30528;&#25552;&#39640;&#21040;93.3&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.06386</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20020;&#24202;&#33041;&#30005;&#22270;&#20998;&#31867;&#20013;&#30340;&#33539;&#22260;&#21644;&#20210;&#35009;
&lt;/p&gt;
&lt;p&gt;
Scope and Arbitration in Machine Learning Clinical EEG Classification. (arXiv:2303.06386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20020;&#24202;&#33041;&#30005;&#22270;&#20998;&#31867;&#20013;&#31383;&#21475;&#26631;&#31614;&#21487;&#33021;&#35823;&#23548;&#30340;&#38382;&#39064;&#65306;&#22686;&#21152;&#31383;&#21475;&#38271;&#24230;&#21644;&#24341;&#20837;&#31532;&#20108;&#38454;&#27573;&#27169;&#22411;&#26469;&#20210;&#35009;&#35760;&#24405;&#20869;&#30340;&#31383;&#21475;&#29305;&#23450;&#39044;&#27979;&#12290;&#22312;Temple&#22823;&#23398;&#21307;&#38498;&#24322;&#24120;&#33041;&#30005;&#22270;&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#26368;&#20808;&#36827;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20174;89.8&#65285;&#26174;&#30528;&#25552;&#39640;&#21040;93.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes two methods to address the problem of potentially misleading window labels in machine learning clinical EEG classification: increasing window length and introducing a second-stage model to arbitrate between window-specific predictions within a recording. Evaluating these methods on the Temple University Hospital Abnormal EEG Corpus, the state-of-the-art average accuracy was significantly improved from 89.8% to 93.3%.
&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#33041;&#30005;&#22270;&#35299;&#35835;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#26159;&#23558;&#35760;&#24405;&#25110;&#20250;&#35805;&#20998;&#31867;&#20026;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20026;&#20102;&#23454;&#38469;&#25805;&#20316;&#30340;&#21407;&#22240;&#65292;&#36890;&#24120;&#23558;&#35760;&#24405;&#20998;&#25104;&#36739;&#30701;&#30340;&#31383;&#21475;&#65292;&#24182;&#19988;&#36825;&#20123;&#31383;&#21475;&#32487;&#25215;&#20854;&#29238;&#35760;&#24405;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#20551;&#35774;&#20197;&#36825;&#31181;&#26041;&#24335;&#27966;&#29983;&#30340;&#31383;&#21475;&#26631;&#31614;&#21487;&#33021;&#20250;&#35823;&#23548;&#65292;&#20363;&#22914;&#65292;&#27809;&#26377;&#26126;&#26174;&#24322;&#24120;&#30340;&#31383;&#21475;&#21487;&#33021;&#20250;&#34987;&#26631;&#35760;&#20026;&#8220;&#24322;&#24120;&#8221;&#65292;&#20174;&#32780;&#30772;&#22351;&#23398;&#20064;&#36807;&#31243;&#24182;&#38477;&#20302;&#24615;&#33021;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#21487;&#20998;&#31163;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65306;&#22686;&#21152;&#31383;&#21475;&#38271;&#24230;&#21644;&#24341;&#20837;&#31532;&#20108;&#38454;&#27573;&#27169;&#22411;&#26469;&#20210;&#35009;&#35760;&#24405;&#20869;&#30340;&#31383;&#21475;&#29305;&#23450;&#39044;&#27979;&#12290;&#22312;Temple&#22823;&#23398;&#21307;&#38498;&#24322;&#24120;&#33041;&#30005;&#22270;&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#26368;&#20808;&#36827;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20174;89.8&#65285;&#26174;&#30528;&#25552;&#39640;&#21040;93.3&#65285;&#12290;&#36825;&#20010;&#32467;&#26524;&#25361;&#25112;&#20102;&#20808;&#21069;&#23545;&#35813;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#38480;&#30340;&#20272;&#35745;&#65292;&#20195;&#34920;&#20102;&#36808;&#21521;&#26356;&#22909;&#24615;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key task in clinical EEG interpretation is to classify a recording or session as normal or abnormal. In machine learning approaches to this task, recordings are typically divided into shorter windows for practical reasons, and these windows inherit the label of their parent recording. We hypothesised that window labels derived in this manner can be misleading for example, windows without evident abnormalities can be labelled `abnormal' disrupting the learning process and degrading performance. We explored two separable approaches to mitigate this problem: increasing the window length and introducing a second-stage model to arbitrate between the window-specific predictions within a recording. Evaluating these methods on the Temple University Hospital Abnormal EEG Corpus, we significantly improved state-of-the-art average accuracy from 89.8 percent to 93.3 percent. This result defies previous estimates of the upper limit for performance on this dataset and represents a major step towar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#30340;&#20256;&#36755;&#39044;&#32534;&#30721;&#22120;&#65292;&#20197;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#29031;&#26126;&#21151;&#29575;&#65292;&#21516;&#26102;&#30830;&#20445;&#25152;&#26377;&#29992;&#25143;&#30340;&#26368;&#23567;&#20449;&#24178;&#22122;&#27604;&#65288;SINR&#65289;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23384;&#22312;&#20449;&#36947;&#20272;&#35745;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20135;&#29983;&#36739;&#23567;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.06381</link><description>&lt;p&gt;
&#23398;&#20064;&#39044;&#32534;&#30721;&#29992;&#20110;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning to Precode for Integrated Sensing and Communications Systems. (arXiv:2303.06381v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#30340;&#20256;&#36755;&#39044;&#32534;&#30721;&#22120;&#65292;&#20197;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#29031;&#26126;&#21151;&#29575;&#65292;&#21516;&#26102;&#30830;&#20445;&#25152;&#26377;&#29992;&#25143;&#30340;&#26368;&#23567;&#20449;&#24178;&#22122;&#27604;&#65288;SINR&#65289;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#23384;&#22312;&#20449;&#36947;&#20272;&#35745;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20135;&#29983;&#36739;&#23567;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an unsupervised learning neural model to design transmit precoders for integrated sensing and communication (ISAC) systems to maximize the worst-case target illumination power while ensuring a minimum signal-to-interference-plus-noise ratio (SINR) for all the users. The proposed method outperforms traditional optimization-based methods in presence of channel estimation errors while incurring lesser computational complexity and generalizing well across different channel conditions that were not shown during training.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#27169;&#22411;&#65292;&#29992;&#20110;&#35774;&#35745;&#38598;&#25104;&#24863;&#30693;&#21644;&#36890;&#20449;&#65288;ISAC&#65289;&#31995;&#32479;&#30340;&#20256;&#36755;&#39044;&#32534;&#30721;&#22120;&#65292;&#20197;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#30446;&#26631;&#29031;&#26126;&#21151;&#29575;&#65292;&#21516;&#26102;&#30830;&#20445;&#25152;&#26377;&#29992;&#25143;&#30340;&#26368;&#23567;&#20449;&#24178;&#22122;&#27604;&#65288;SINR&#65289;&#12290;&#20174;&#19978;&#34892;&#23548;&#39057;&#21644;&#22238;&#27874;&#20013;&#23398;&#20064;&#20256;&#36755;&#39044;&#32534;&#30721;&#22120;&#30340;&#38382;&#39064;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#20010;&#21442;&#25968;&#21270;&#20989;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#23398;&#20064;&#36825;&#20010;&#20989;&#25968;&#12290;&#20026;&#20102;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#32435;&#20837;SINR&#21644;&#21151;&#29575;&#32422;&#26463;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23384;&#22312;&#20449;&#36947;&#20272;&#35745;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20135;&#29983;&#36739;&#23567;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#20123;&#26465;&#20214;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#26174;&#31034;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an unsupervised learning neural model to design transmit precoders for integrated sensing and communication (ISAC) systems to maximize the worst-case target illumination power while ensuring a minimum signal-to-interference-plus-noise ratio (SINR) for all the users. The problem of learning transmit precoders from uplink pilots and echoes can be viewed as a parameterized function estimation problem and we propose to learn this function using a neural network model. To learn the neural network parameters, we develop a novel loss function based on the first-order optimality conditions to incorporate the SINR and power constraints. Through numerical simulations, we demonstrate that the proposed method outperforms traditional optimization-based methods in presence of channel estimation errors while incurring lesser computational complexity and generalizing well across different channel conditions that were not shown during training.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22810;&#20013;&#24515;&#29615;&#22659;&#20013;&#23545;&#22522;&#20110;EEG&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#24615;&#21035;&#23376;&#32452;&#32676;&#30340;&#26816;&#27979;&#33021;&#21147;&#20998;&#26512;&#65292;&#21457;&#29616;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;PD&#26816;&#27979;&#33021;&#21147;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.06376</link><description>&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;EEG&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;&#20013;&#30340;&#24615;&#21035;&#20844;&#24179;&#24615;&#65306;&#19968;&#39033;&#22810;&#20013;&#24515;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing gender fairness in EEG-based machine learning detection of Parkinson's disease: A multi-center study. (arXiv:2303.06376v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22810;&#20013;&#24515;&#29615;&#22659;&#20013;&#23545;&#22522;&#20110;EEG&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#24615;&#21035;&#23376;&#32452;&#32676;&#30340;&#26816;&#27979;&#33021;&#21147;&#20998;&#26512;&#65292;&#21457;&#29616;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;PD&#26816;&#27979;&#33021;&#21147;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study analyzed the detection ability of gender sub-groups in a multi-center setting of a previously developed machine learning algorithm based on EEG, finding significant differences in Parkinson's disease detection ability between males and females.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#38745;&#24687;&#24577;&#33041;&#30005;&#22270;&#65288;rs-EEG&#65289;&#30340;&#33258;&#21160;&#24037;&#20855;&#22312;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#26816;&#27979;&#20013;&#30340;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#36890;&#36807;&#20844;&#24179;&#24615;&#21644;&#20559;&#24046;&#20998;&#26512;&#35780;&#20272;&#21487;&#33021;&#21152;&#21095;&#20581;&#24247;&#24046;&#24322;&#30340;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#12290;&#21463;&#20445;&#25252;&#30340;&#23646;&#24615;&#65292;&#22914;&#24615;&#21035;&#65292;&#22312;PD&#35786;&#26029;&#24320;&#21457;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26469;&#33258;&#19981;&#21516;&#24615;&#21035;&#30340;&#23376;&#32452;&#32676;&#20307;&#30340;&#20998;&#26512;&#24456;&#23569;&#22312;ML&#27169;&#22411;&#30340;&#24320;&#21457;&#25110;PD&#26816;&#27979;&#30340;&#24615;&#33021;&#35780;&#20272;&#20013;&#32771;&#34385;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#38745;&#24687;&#24577;&#33041;&#30005;&#22270;&#21151;&#29575;&#35889;&#23494;&#24230;&#65288;PSD&#65289;&#29305;&#24449;&#30340;&#20808;&#21069;&#24320;&#21457;&#30340;ML&#31639;&#27861;&#22312;&#22810;&#20013;&#24515;&#29615;&#22659;&#20013;&#30340;&#24615;&#21035;&#23376;&#32452;&#32676;&#30340;&#26816;&#27979;&#33021;&#21147;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27979;&#35797;&#26102;&#38388;&#65288;80.5&#65285;&#23545;63.7&#65285;&#30340;&#20934;&#30830;&#24615;&#65289;&#30007;&#24615;&#21644;&#22899;&#24615;&#30340;PD&#26816;&#27979;&#33021;&#21147;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#24182;&#19988;&#19968;&#32452;&#39030;&#37096;&#21644;&#21069;&#39069;&#33041;&#30005;&#22270;&#36890;&#36947;&#21644;&#39057;&#29575;&#23384;&#22312;&#26174;&#30528;&#26356;&#39640;&#30340;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of automatic tools based on machine learning (ML) and resting-state electroencephalography (rs-EEG) for Parkinson's disease (PD) detection keeps growing, the assessment of possible exacerbation of health disparities by means of fairness and bias analysis becomes more relevant. Protected attributes, such as gender, play an important role in PD diagnosis development. However, analysis of sub-group populations stemming from different genders is seldom taken into consideration in ML models' development or the performance assessment for PD detection. In this work, we perform a systematic analysis of the detection ability for gender sub-groups in a multi-center setting of a previously developed ML algorithm based on power spectral density (PSD) features of rs-EEG. We find significant differences in the PD detection ability for males and females at testing time (80.5% vs. 63.7% accuracy) and significantly higher activity for a set of parietal and frontal EEG channels and frequen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;</title><link>http://arxiv.org/abs/2303.06365</link><description>&lt;p&gt;
&#36890;&#36807;&#34394;&#25311;&#26816;&#26597;&#23618;&#23454;&#29616;&#26102;&#38388;&#24207;&#21015;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Explainable AI for Time Series via Virtual Inspection Layers. (arXiv:2303.06365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a virtual inspection layer that transforms time series into an interpretable representation and allows for relevance attributions to be propagated to this representation via local XAI methods. The applicability of a family of XAI methods is extended to domains where the input is only interpretable after a transformation. The usefulness of DFT-LRP is demonstrated in various time series classification settings, such as audio and electronic health records.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#20027;&#35201;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#12290;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#30001;&#20110;&#36755;&#20837;&#36890;&#24120;&#19981;&#21487;&#35299;&#37322;&#65292;&#22240;&#27492;&#21482;&#26377;&#26377;&#38480;&#30340;XAI&#30740;&#31350;&#21487;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;&#26816;&#26597;&#23618;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#20026;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#26412;&#22320;XAI&#26041;&#27861;&#65288;&#22914;&#36880;&#23618;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#65289;&#23558;&#30456;&#20851;&#24615;&#24402;&#22240;&#20256;&#25773;&#21040;&#35813;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;XAI&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#25193;&#23637;&#21040;&#38656;&#35201;&#36716;&#25442;&#21518;&#25165;&#33021;&#35299;&#37322;&#36755;&#20837;&#30340;&#39046;&#22495;&#65288;&#20363;&#22914;&#35821;&#38899;&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#36825;&#22312;&#26102;&#38388;&#24207;&#21015;&#35299;&#37322;&#21644;LRP&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;DFT-LRP&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DFT-LRP&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#35774;&#32622;&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#22914;&#38899;&#39057;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;DFT-LRP&#26469;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of eXplainable Artificial Intelligence (XAI) has greatly advanced in recent years, but progress has mainly been made in computer vision and natural language processing. For time series, where the input is often not interpretable, only limited research on XAI is available. In this work, we put forward a virtual inspection layer, that transforms the time series to an interpretable representation and allows to propagate relevance attributions to this representation via local XAI methods like layer-wise relevance propagation (LRP). In this way, we extend the applicability of a family of XAI methods to domains (e.g. speech) where the input is only interpretable after a transformation. Here, we focus on the Fourier transformation which is prominently applied in the interpretation of time series and LRP and refer to our method as DFT-LRP. We demonstrate the usefulness of DFT-LRP in various time series classification settings like audio and electronic health records. We showcase how 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#26041;&#26696;&#65292;&#36890;&#36807;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.06361</link><description>&lt;p&gt;
&#38754;&#21521;&#38750;&#38745;&#24577;&#29615;&#22659;&#30340;&#38544;&#31169;&#20445;&#25252;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#65306;&#32852;&#37030;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Cooperative Visible Light Positioning for Nonstationary Environment: A Federated Learning Perspective. (arXiv:2303.06361v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#26041;&#26696;&#65292;&#36890;&#36807;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a cooperative visible light positioning scheme based on federated learning, which improves the positioning accuracy and generalization capability in nonstationary environments by jointly training a global model adaptive to environmental changes without sharing private data of users.
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35265;&#20809;&#23450;&#20301;&#65288;VLP&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#23460;&#20869;&#23450;&#20301;&#25216;&#26415;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#65292;&#30001;&#20110;&#39640;&#24230;&#26102;&#21464;&#30340;&#20449;&#36947;&#65292;VLP&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#25552;&#39640;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#30340;&#23450;&#20301;&#31934;&#24230;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21512;&#20316;VLP&#26041;&#26696;&#12290;&#21033;&#29992;FL&#26694;&#26550;&#65292;&#29992;&#25143;&#21487;&#20197;&#20849;&#21516;&#35757;&#32451;&#36866;&#24212;&#29615;&#22659;&#21464;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#20849;&#20139;&#29992;&#25143;&#30340;&#31169;&#26377;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#20316;&#21487;&#35265;&#20809;&#23450;&#20301;&#32593;&#32476;&#65288;CVPosNet&#65289;&#65292;&#20197;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#19979;&#20248;&#20110;&#22522;&#20934;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visible light positioning (VLP) has drawn plenty of attention as a promising indoor positioning technique. However, in nonstationary environments, the performance of VLP is limited because of the highly time-varying channels. To improve the positioning accuracy and generalization capability in nonstationary environments, a cooperative VLP scheme based on federated learning (FL) is proposed in this paper. Exploiting the FL framework, a global model adaptive to environmental changes can be jointly trained by users without sharing private data of users. Moreover, a Cooperative Visible-light Positioning Network (CVPosNet) is proposed to accelerate the convergence rate and improve the positioning accuracy. Simulation results show that the proposed scheme outperforms the benchmark schemes, especially in nonstationary environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.06360</link><description>&lt;p&gt;
FedLP: &#19968;&#31181;&#29992;&#20110;&#36890;&#20449;&#35745;&#31639;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#23618;&#27425;&#21098;&#26525;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedLP: Layer-wise Pruning Mechanism for Communication-Computation Efficient Federated Learning. (arXiv:2303.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an explicit FL pruning framework, FedLP, which adopts layer-wise pruning in local training and federated updating, and is model-agnostic and universal for different types of deep learning models. FedLP can relieve the system bottlenecks of communication and computation with marginal performance decay.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#39640;&#25928;&#19988;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#26696;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;FL&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#30340;&#20248;&#21270;&#65292;&#37319;&#29992;&#23616;&#37096;&#35757;&#32451;&#21644;&#32852;&#37030;&#26356;&#26032;&#20013;&#30340;&#23618;&#27425;&#21098;&#26525;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26174;&#24335;&#30340;FL&#21098;&#26525;&#26694;&#26550;FedLP&#65288;Federated Layer-wise Pruning&#65289;&#65292;&#35813;&#26694;&#26550;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;&#20026;&#20855;&#26377;&#21516;&#36136;&#26412;&#22320;&#27169;&#22411;&#21644;&#24322;&#36136;&#26412;&#22320;&#27169;&#22411;&#30340;&#22330;&#26223;&#35774;&#35745;&#20102;&#20004;&#31181;&#29305;&#23450;&#30340;FedLP&#26041;&#26696;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FedLP&#21487;&#20197;&#32531;&#35299;&#36890;&#20449;&#21644;&#35745;&#31639;&#30340;&#31995;&#32479;&#29942;&#39048;&#65292;&#24182;&#19988;&#24615;&#33021;&#19979;&#38477;&#36739;&#23567;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FedLP&#26159;&#31532;&#19968;&#20010;&#27491;&#24335;&#23558;&#23618;&#27425;&#21098;&#26525;&#24341;&#20837;FL&#30340;&#26694;&#26550;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#33539;&#22260;&#20869;&#65292;&#21487;&#20197;&#22522;&#20110;FedLP&#36827;&#19968;&#27493;&#35774;&#35745;&#26356;&#22810;&#30340;&#21464;&#20307;&#21644;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has prevailed as an efficient and privacy-preserved scheme for distributed learning. In this work, we mainly focus on the optimization of computation and communication in FL from a view of pruning. By adopting layer-wise pruning in local training and federated updating, we formulate an explicit FL pruning framework, FedLP (Federated Layer-wise Pruning), which is model-agnostic and universal for different types of deep learning models. Two specific schemes of FedLP are designed for scenarios with homogeneous local models and heterogeneous ones. Both theoretical and experimental evaluations are developed to verify that FedLP relieves the system bottlenecks of communication and computation with marginal performance decay. To the best of our knowledge, FedLP is the first framework that formally introduces the layer-wise pruning into FL. Within the scope of federated learning, more variants and combinations can be further designed based on FedLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#26631;&#20934;RNN&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#65292;&#21253;&#25324;&#32447;&#24615;&#21270;&#21644;&#23545;&#35282;&#21270;&#24490;&#29615;&#12289;&#20351;&#29992;&#26356;&#22909;&#30340;&#21442;&#25968;&#21270;&#21644;&#21021;&#22987;&#21270;&#20197;&#21450;&#30830;&#20445;&#27491;&#24120;&#21270;&#21069;&#21521;&#20256;&#36882;&#31561;&#19968;&#31995;&#21015;&#25913;&#21464;&#65292;&#26469;&#24674;&#22797;&#28145;&#24230;SSM&#22312;&#38271;&#36317;&#31163;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#21305;&#37197;&#23427;&#20204;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06349</link><description>&lt;p&gt;
&#22797;&#27963;&#38271;&#24207;&#21015;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Resurrecting Recurrent Neural Networks for Long Sequences. (arXiv:2303.06349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#23545;&#26631;&#20934;RNN&#36827;&#34892;&#31934;&#24515;&#35774;&#35745;&#65292;&#21253;&#25324;&#32447;&#24615;&#21270;&#21644;&#23545;&#35282;&#21270;&#24490;&#29615;&#12289;&#20351;&#29992;&#26356;&#22909;&#30340;&#21442;&#25968;&#21270;&#21644;&#21021;&#22987;&#21270;&#20197;&#21450;&#30830;&#20445;&#27491;&#24120;&#21270;&#21069;&#21521;&#20256;&#36882;&#31561;&#19968;&#31995;&#21015;&#25913;&#21464;&#65292;&#26469;&#24674;&#22797;&#28145;&#24230;SSM&#22312;&#38271;&#36317;&#31163;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#21305;&#37197;&#23427;&#20204;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores how to recover the impressive performance of deep state-space models (SSMs) on long-range reasoning tasks by carefully designing deep RNNs using standard signal propagation arguments, including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass.
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#22312;&#38271;&#24207;&#21015;&#19978;&#25552;&#20379;&#24555;&#36895;&#25512;&#29702;&#65292;&#20294;&#38590;&#20197;&#20248;&#21270;&#19988;&#35757;&#32451;&#36895;&#24230;&#24930;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#20855;&#26377;&#24555;&#36895;&#21487;&#24182;&#34892;&#21270;&#30340;&#35757;&#32451;&#21644;&#31867;&#20284;RNN&#30340;&#24555;&#36895;&#25512;&#29702;&#30340;&#39069;&#22806;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;SSM&#19982;RNN&#22312;&#34920;&#38754;&#19978;&#30456;&#20284;&#65292;&#20294;&#23384;&#22312;&#37325;&#35201;&#24046;&#24322;&#65292;&#20351;&#24471;&#19981;&#28165;&#26970;&#23427;&#20204;&#22312;RNN&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#26469;&#33258;&#20309;&#22788;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26631;&#20934;&#20449;&#21495;&#20256;&#25773;&#35770;&#25454;&#31934;&#24515;&#35774;&#35745;&#30340;&#28145;&#24230;RNN&#21487;&#20197;&#24674;&#22797;&#28145;&#24230;SSM&#22312;&#38271;&#36317;&#31163;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#21305;&#37197;&#23427;&#20204;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20998;&#26512;&#21644;&#28040;&#34701;&#20102;&#19968;&#31995;&#21015;&#23545;&#26631;&#20934;RNN&#30340;&#26356;&#25913;&#65292;&#21253;&#25324;&#32447;&#24615;&#21270;&#21644;&#23545;&#35282;&#21270;&#24490;&#29615;&#65292;&#20351;&#29992;&#26356;&#22909;&#30340;&#21442;&#25968;&#21270;&#21644;&#21021;&#22987;&#21270;&#65292;&#24182;&#30830;&#20445;&#27491;&#24120;&#21270;&#21069;&#21521;&#20256;&#36882;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#28145;&#24230;RNN&#21644;&#28145;&#24230;SSM&#24615;&#33021;&#24046;&#24322;&#26469;&#28304;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#24322;&#36136;&#24615;&#19979;&#26080;&#27861;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06344</link><description>&lt;p&gt;
&#24322;&#36136;&#24615;&#19979;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning under Heterophily. (arXiv:2303.06344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#24322;&#36136;&#24615;&#19979;&#26080;&#27861;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the first graph contrastive learning method to address the problem that existing graph contrastive learning methods cannot learn high-quality representations under heterophily.
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#20855;&#26377;&#29305;&#23450;&#20219;&#21153;&#33410;&#28857;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20026;&#22270;&#24418;&#33719;&#21462;&#26631;&#31614;&#26159;&#26114;&#36149;&#30340;&#12290;&#36825;&#22312;&#22823;&#22411;&#22270;&#24418;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#26377;&#19968;&#20123;&#24037;&#20316;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#22312;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#26041;&#38754;&#29305;&#21035;&#21463;&#27426;&#36814;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;CL&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#21516;&#31034;&#20363;&#30340;&#22686;&#24378;&#35270;&#22270;&#30340;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26368;&#23567;&#21270;&#19981;&#21516;&#31034;&#20363;&#30340;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#24037;&#20316;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#24418;CL&#26041;&#27861;&#19981;&#33021;&#22312;&#24322;&#36136;&#24615;&#19979;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#65292;&#20854;&#20013;&#36830;&#25509;&#30340;&#33410;&#28857;&#20542;&#21521;&#20110;&#23646;&#20110;&#19981;&#21516;&#30340;&#31867;&#12290;&#36825;&#26159;&#22240;&#20026;&#22312;&#24322;&#36136;&#24615;&#19979;&#65292;&#21516;&#19968;&#31034;&#20363;&#30340;&#22686;&#24378;&#21487;&#33021;&#24444;&#27492;&#19981;&#30456;&#20284;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#31532;&#19968;&#20010;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks are powerful tools for learning node representations when task-specific node labels are available. However, obtaining labels for graphs is expensive in many applications. This is particularly the case for large graphs. To address this, there has been a body of work to learn node representations in a self-supervised manner without labels. Contrastive learning (CL), has been particularly popular to learn representations in a self-supervised manner. In general, CL methods work by maximizing the similarity between representations of augmented views of the same example, and minimizing the similarity between augmented views of different examples. However, existing graph CL methods cannot learn high-quality representations under heterophily, where connected nodes tend to belong to different classes. This is because under heterophily, augmentations of the same example may not be similar to each other. In this work, we address the above problem by proposing the first graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#31579;&#26597;&#21628;&#20986;&#27668;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOC&#65289;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#39044;&#27979;&#32954;&#30284;&#24739;&#32773;&#21450;&#20854;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2303.06340</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#32954;&#30284;&#26234;&#33021;&#35786;&#26029;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Intelligent diagnostic scheme for lung cancer screening with Raman spectra data by tensor network machine learning. (arXiv:2303.06340v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#31579;&#26597;&#21628;&#20986;&#27668;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOC&#65289;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#39044;&#27979;&#32954;&#30284;&#24739;&#32773;&#21450;&#20854;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a tensor-network machine learning method to reliably predict lung cancer patients and their stages via screening Raman spectra data of Volatile organic compounds (VOCs) in exhaled breath.
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#22312;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#20013;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24433;&#21709;&#65292;&#20174;&#23398;&#26415;&#30740;&#31350;&#21040;&#20020;&#24202;&#24212;&#29992;&#65292;&#20363;&#22914;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#26816;&#27979;&#21644;&#35786;&#26029;&#12289;&#27835;&#30103;&#20248;&#21270;&#20197;&#21450;&#33647;&#29289;&#21457;&#29616;&#20013;&#26032;&#30340;&#27835;&#30103;&#38774;&#28857;&#30340;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;AI&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;&#20005;&#37325;&#21463;&#21040;&#38750;&#21487;&#35299;&#37322;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#19981;&#21487;&#25511;&#22320;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#23545;&#20110;ML&#30340;&#21487;&#35299;&#37322;&#24615;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#28040;&#36153;&#32773;&#24517;&#39035;&#20174;&#22362;&#23454;&#30340;&#22522;&#30784;&#25110;&#20196;&#20154;&#20449;&#26381;&#30340;&#35299;&#37322;&#20013;&#33719;&#24471;&#24517;&#35201;&#30340;&#23433;&#20840;&#24863;&#21644;&#20449;&#20219;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;-ML&#26041;&#27861;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#31579;&#26597;&#21628;&#20986;&#27668;&#20013;&#25381;&#21457;&#24615;&#26377;&#26426;&#21270;&#21512;&#29289;&#65288;VOC&#65289;&#30340;Raman&#20809;&#35889;&#25968;&#25454;&#65292;&#21487;&#21487;&#38752;&#22320;&#39044;&#27979;&#32954;&#30284;&#24739;&#32773;&#21450;&#20854;&#38454;&#27573;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#36866;&#29992;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#38750;&#20405;&#20837;&#24615;&#32954;&#30284;&#31579;&#26597;&#30340;&#29702;&#24819;&#26041;&#24335;&#12290;TN-ML&#30340;&#39044;&#27979;&#22522;&#20110;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has brought tremendous impacts on biomedical sciences from academic researches to clinical applications, such as in biomarkers' detection and diagnosis, optimization of treatment, and identification of new therapeutic targets in drug discovery. However, the contemporary AI technologies, particularly deep machine learning (ML), severely suffer from non-interpretability, which might uncontrollably lead to incorrect predictions. Interpretability is particularly crucial to ML for clinical diagnosis as the consumers must gain necessary sense of security and trust from firm grounds or convincing interpretations. In this work, we propose a tensor-network (TN)-ML method to reliably predict lung cancer patients and their stages via screening Raman spectra data of Volatile organic compounds (VOCs) in exhaled breath, which are generally suitable as biomarkers and are considered to be an ideal way for non-invasive lung cancer screening. The prediction of TN-ML is based
&lt;/p&gt;</description></item><item><title>AutoMLP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#27169;&#25311;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06337</link><description>&lt;p&gt;
AutoMLP: &#33258;&#21160;&#21270;MLP&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
AutoMLP: Automated MLP for Sequential Recommendations. (arXiv:2303.06337v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06337
&lt;/p&gt;
&lt;p&gt;
AutoMLP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#27169;&#25311;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#25512;&#33616;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
AutoMLP is a novel sequential recommender system that models users' long/short-term interests through an automated and adaptive search algorithm, achieving better recommendation performance.
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#26681;&#25454;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#26469;&#39044;&#27979;&#20182;&#20204;&#19979;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#21306;&#20998;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#65292;&#36825;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#24182;&#23545;&#19979;&#19968;&#20010;&#25512;&#33616;&#20135;&#29983;&#19981;&#21516;&#30340;&#36129;&#29486;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#31351;&#20030;&#25628;&#32034;&#25110;&#32463;&#39564;&#32463;&#39564;&#35774;&#32622;&#39044;&#23450;&#20041;&#30340;&#30701;&#26399;&#20852;&#36259;&#38271;&#24230;&#65292;&#36825;&#26082;&#39640;&#24230;&#20302;&#25928;&#21448;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23384;&#22312;&#19978;&#36848;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#23545;&#36755;&#20837;&#24207;&#21015;&#30340;&#38271;&#24230;&#20855;&#26377;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;AutoMLP&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#27169;&#25311;&#29992;&#25143;&#30340;&#38271;&#26399;/&#30701;&#26399;&#20852;&#36259;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#21644;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#36890;&#36807;&#31471;&#21040;&#31471;&#20248;&#21270;&#33719;&#24471;&#26356;&#22909;&#30340;&#30701;&#26399;&#20852;&#36259;&#38271;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AutoMLP&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential recommender systems aim to predict users' next interested item given their historical interactions. However, a long-standing issue is how to distinguish between users' long/short-term interests, which may be heterogeneous and contribute differently to the next recommendation. Existing approaches usually set pre-defined short-term interest length by exhaustive search or empirical experience, which is either highly inefficient or yields subpar results. The recent advanced transformer-based models can achieve state-of-the-art performances despite the aforementioned issue, but they have a quadratic computational complexity to the length of the input sequence. To this end, this paper proposes a novel sequential recommender system, AutoMLP, aiming for better modeling users' long/short-term interests from their historical interactions. In addition, we design an automated and adaptive search algorithm for preferable short-term interest length via end-to-end optimization. Through ext
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;MetaViewer&#65292;&#36890;&#36807;&#32479;&#19968;&#21040;&#29305;&#23450;&#30340;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#39044;&#20808;&#25351;&#23450;&#30340;&#34701;&#21512;&#20989;&#25968;&#21644;&#28151;&#21512;&#22312;&#29305;&#24449;&#20013;&#30340;&#35270;&#22270;&#19987;&#29992;&#20887;&#20313;&#20449;&#24687;&#21487;&#33021;&#20250;&#38477;&#20302;&#25152;&#24471;&#34920;&#31034;&#30340;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06329</link><description>&lt;p&gt;
MetaViewer: &#26397;&#30528;&#32479;&#19968;&#30340;&#22810;&#35270;&#22270;&#34920;&#31034;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
MetaViewer: Towards A Unified Multi-View Representation. (arXiv:2303.06329v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;MetaViewer&#65292;&#36890;&#36807;&#32479;&#19968;&#21040;&#29305;&#23450;&#30340;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#39044;&#20808;&#25351;&#23450;&#30340;&#34701;&#21512;&#20989;&#25968;&#21644;&#28151;&#21512;&#22312;&#29305;&#24449;&#20013;&#30340;&#35270;&#22270;&#19987;&#29992;&#20887;&#20313;&#20449;&#24687;&#21487;&#33021;&#20250;&#38477;&#20302;&#25152;&#24471;&#34920;&#31034;&#30340;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel bi-level-optimization-based multi-view learning framework, MetaViewer, which learns the representation in a uniform-to-specific manner, avoiding the problem of manually pre-specify fusion functions and view-private redundant information mixed in features that potentially degrade the quality of the derived representation.
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#36981;&#24490;&#29305;&#23450;&#21040;&#32479;&#19968;&#30340;&#27969;&#31243;&#65292;&#20174;&#27599;&#20010;&#35270;&#22270;&#20013;&#25552;&#21462;&#28508;&#22312;&#29305;&#24449;&#65292;&#28982;&#21518;&#34701;&#21512;&#25110;&#23545;&#40784;&#23427;&#20204;&#20197;&#33719;&#24471;&#32479;&#19968;&#30340;&#23545;&#35937;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#39044;&#20808;&#25351;&#23450;&#30340;&#34701;&#21512;&#20989;&#25968;&#21644;&#28151;&#21512;&#22312;&#29305;&#24449;&#20013;&#30340;&#35270;&#22270;&#19987;&#29992;&#20887;&#20313;&#20449;&#24687;&#21487;&#33021;&#20250;&#38477;&#20302;&#25152;&#24471;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#34920;&#31034;&#26159;&#20197;&#32479;&#19968;&#21040;&#29305;&#23450;&#30340;&#26041;&#24335;&#23398;&#20064;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#20803;&#23398;&#20064;&#22120;&#65292;&#21363;MetaViewer&#65292;&#22312;&#22806;&#23618;&#20248;&#21270;&#20013;&#23398;&#20064;&#34701;&#21512;&#21644;&#24314;&#27169;&#35270;&#22270;&#20849;&#20139;&#30340;&#20803;&#34920;&#31034;&#12290;&#20174;&#36825;&#20010;&#20803;&#34920;&#31034;&#24320;&#22987;&#65292;&#38656;&#35201;&#22312;&#20869;&#23618;&#35757;&#32451;&#35270;&#22270;&#29305;&#23450;&#30340;&#22522;&#23398;&#20064;&#22120;&#65292;&#20197;&#24555;&#36895;&#37325;&#26500;&#30456;&#24212;&#30340;&#35270;&#22270;&#12290;MetaViewer&#26368;&#32456;&#36890;&#36807;&#35266;&#23519;&#25152;&#26377;&#35270;&#22270;&#19978;&#20174;&#32479;&#19968;&#21040;&#29305;&#23450;&#30340;&#37325;&#26500;&#36807;&#31243;&#26469;&#26356;&#26032;&#65292;&#24182;&#23398;&#20064;&#26368;&#20339;&#34701;&#21512;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing multi-view representation learning methods typically follow a specific-to-uniform pipeline, extracting latent features from each view and then fusing or aligning them to obtain the unified object representation. However, the manually pre-specify fusion functions and view-private redundant information mixed in features potentially degrade the quality of the derived representation. To overcome them, we propose a novel bi-level-optimization-based multi-view learning framework, where the representation is learned in a uniform-to-specific manner. Specifically, we train a meta-learner, namely MetaViewer, to learn fusion and model the view-shared meta representation in outer-level optimization. Start with this meta representation, view-specific base-learners are then required to rapidly reconstruct the corresponding view in inner-level. MetaViewer eventually updates by observing reconstruction processes from uniform to specific over all views, and learns an optimal fusion scheme that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.06318</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#24352;&#37327;&#19987;&#23478;&#28151;&#21512;&#24182;&#34892;&#26041;&#27861;&#26469;&#25193;&#23637;&#28151;&#21512;&#19987;&#23478;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
A Novel Tensor-Expert Hybrid Parallelism Approach to Scale Mixture-of-Experts Training. (arXiv:2303.06318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mixture-of-Experts&#65288;MoE&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#28155;&#21152;&#31232;&#30095;&#28608;&#27963;&#30340;&#19987;&#23478;&#22359;&#26469;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#65288;&#22522;&#26412;&#27169;&#22411;&#65289;&#30340;&#21442;&#25968;&#65292;&#32780;&#19981;&#25913;&#21464;&#35757;&#32451;&#25110;&#25512;&#29702;&#30340;&#24635;&#28014;&#28857;&#25805;&#20316;&#25968;&#12290;&#29702;&#35770;&#19978;&#65292;&#36825;&#31181;&#26550;&#26500;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#20219;&#24847;&#22823;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19982;&#22522;&#26412;&#27169;&#22411;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#22312;64&#21040;128&#20010;&#19987;&#23478;&#22359;&#20043;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35266;&#23519;&#21040;&#36825;&#20123;MoE&#27169;&#22411;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#36882;&#20943;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;MoE&#27169;&#22411;&#38656;&#35201;&#25105;&#20204;&#25193;&#23637;&#22522;&#26412;&#27169;&#22411;&#30340;&#22823;&#23567;&#20197;&#21450;&#19987;&#23478;&#22359;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#32500;&#28151;&#21512;&#24182;&#34892;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#24352;&#37327;&#12289;&#19987;&#23478;&#21644;&#25968;&#25454;&#24182;&#34892;&#65292;&#20197;&#23454;&#29616;MoE&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#22522;&#26412;&#27169;&#22411;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DeepSpeed-MoE&#22823;4-8&#20493;&#12290;&#25105;&#20204;&#22312;&#20248;&#21270;&#22120;&#27493;&#39588;&#20013;&#25552;&#20986;&#20102;&#20869;&#23384;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new neural network architecture called Mixture-of-Experts (MoE) has been proposed recently that increases the parameters of a neural network (the base model) by adding sparsely activated expert blocks, without changing the total number of floating point operations for training or inference. In theory, this architecture allows us to train arbitrarily large models while keeping the computational costs same as that of the base model. However, beyond 64 to 128 experts blocks, prior work has observed diminishing returns in the test accuracies of these MoE models. Thus, training high quality MoE models requires us to scale the size of the base models, along with the number of expert blocks. In this work, we propose a novel, three-dimensional, hybrid parallel algorithm that combines tensor, expert, and data parallelism to enable the training of MoE models with 4-8x larger base models than the current state-of-the-art -- DeepSpeed-MoE. We propose memory optimizations in the optimizer step, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#25928;&#29575;&#65292;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#24615;&#33021;&#26159;&#30001;&#20110;&#20869;&#22312;&#34920;&#36798;&#33021;&#21147;&#32780;&#38750;&#21442;&#25968;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2303.06316</link><description>&lt;p&gt;
&#19968;&#20010;&#31070;&#32463;&#20803;&#30340;&#33410;&#30465;&#23601;&#26159;&#19968;&#20010;&#31070;&#32463;&#20803;&#30340;&#25910;&#30410;&#65306;&#20851;&#20110;&#20108;&#27425;&#32593;&#32476;&#21442;&#25968;&#25928;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
One Neuron Saved Is One Neuron Earned: On Parametric Efficiency of Quadratic Networks. (arXiv:2303.06316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#21442;&#25968;&#25928;&#29575;&#65292;&#35777;&#26126;&#20102;&#20854;&#21331;&#36234;&#24615;&#33021;&#26159;&#30001;&#20110;&#20869;&#22312;&#34920;&#36798;&#33021;&#21147;&#32780;&#38750;&#21442;&#25968;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the parametric efficiency of quadratic neurons and confirms that their superior performance is due to intrinsic expressive capability rather than increased parameters.
&lt;/p&gt;
&lt;p&gt;
&#21463;&#29983;&#29289;&#31070;&#32463;&#31995;&#32479;&#20013;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#30340;&#21551;&#21457;&#65292;&#22823;&#37327;&#30740;&#31350;&#25552;&#20986;&#20102;&#35774;&#35745;&#26032;&#22411;&#20154;&#24037;&#31070;&#32463;&#20803;&#24182;&#23558;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#24341;&#20837;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#20108;&#27425;&#31070;&#32463;&#20803;&#65292;&#23558;&#20256;&#32479;&#31070;&#32463;&#20803;&#20013;&#30340;&#20869;&#31215;&#25805;&#20316;&#26367;&#25442;&#20026;&#20108;&#27425;&#25805;&#20316;&#65292;&#22312;&#35768;&#22810;&#37325;&#35201;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#20108;&#27425;&#31070;&#32463;&#20803;&#30340;&#32467;&#26524;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#65306;&#20108;&#27425;&#32593;&#32476;&#30340;&#21331;&#36234;&#24615;&#33021;&#20165;&#20165;&#26159;&#30001;&#20110;&#21442;&#25968;&#22686;&#21152;&#36824;&#26159;&#30001;&#20110;&#20869;&#22312;&#34920;&#36798;&#33021;&#21147;&#65311;&#22312;&#26410;&#28548;&#28165;&#36825;&#20010;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#20108;&#27425;&#32593;&#32476;&#30340;&#24615;&#33021;&#24635;&#26159;&#20196;&#20154;&#24576;&#30097;&#12290;&#27492;&#22806;&#65292;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23601;&#26159;&#25214;&#21040;&#20108;&#27425;&#32593;&#32476;&#30340;&#26432;&#25163;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#27425;&#32593;&#32476;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#65292;&#20174;&#32780;&#30830;&#35748;&#20102;&#20108;&#27425;&#32593;&#32476;&#30340;&#21331;&#36234;&#24615;&#33021;&#26159;&#30001;&#20110;&#20854;&#20869;&#22312;&#34920;&#36798;&#33021;&#21147;&#32780;&#38750;&#21442;&#25968;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by neuronal diversity in the biological neural system, a plethora of studies proposed to design novel types of artificial neurons and introduce neuronal diversity into artificial neural networks. Recently proposed quadratic neuron, which replaces the inner-product operation in conventional neurons with a quadratic one, have achieved great success in many essential tasks. Despite the promising results of quadratic neurons, there is still an unresolved issue: \textit{Is the superior performance of quadratic networks simply due to the increased parameters or due to the intrinsic expressive capability?} Without clarifying this issue, the performance of quadratic networks is always suspicious. Additionally, resolving this issue is reduced to finding killer applications of quadratic networks. In this paper, with theoretical and empirical studies, we show that quadratic networks enjoy parametric efficiency, thereby confirming that the superior performance of quadratic networks is due
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20808;&#39564;&#26657;&#20934;&#30340;softmax&#20989;&#25968;&#26469;&#35745;&#31639;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#32500;&#25252;&#19968;&#20010;&#24179;&#34913;&#30340;&#20998;&#31867;&#22120;&#22836;&#65292;&#20197;&#31283;&#23450;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.06314</link><description>&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#36890;&#36807;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#23458;&#25143;&#31471;dropout&#26469;&#31283;&#23450;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stabilizing and Improving Federated Learning with Non-IID Data and Client Dropout in IoT Systems. (arXiv:2303.06314v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20808;&#39564;&#26657;&#20934;&#30340;softmax&#20989;&#25968;&#26469;&#35745;&#31639;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#32500;&#25252;&#19968;&#20010;&#24179;&#34913;&#30340;&#20998;&#31867;&#22120;&#22836;&#65292;&#20197;&#31283;&#23450;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple yet effective framework to stabilize and improve federated learning by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based feature extraction to maintain a balanced classifier head.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#19981;&#26292;&#38706;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22312;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#19978;&#35757;&#32451;&#28145;&#24230;&#27169;&#22411;&#65292;&#28982;&#32780;&#23427;&#21463;&#21040;&#26631;&#31614;&#20998;&#24067;&#20559;&#26012;&#30340;&#24433;&#21709;&#65292;&#36890;&#24120;&#23548;&#33268;&#25910;&#25947;&#32531;&#24930;&#21644;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#24403;&#21442;&#19982;&#30340;&#23458;&#25143;&#31471;&#22788;&#20110;&#19981;&#31283;&#23450;&#30340;&#29615;&#22659;&#24182;&#32463;&#24120;&#25481;&#32447;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#21487;&#33021;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#20808;&#39564;&#26657;&#20934;&#30340;softmax&#20989;&#25968;&#26469;&#35745;&#31639;&#20132;&#21449;&#29109;&#25439;&#22833;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#26469;&#32500;&#25252;&#19968;&#20010;&#24179;&#34913;&#30340;&#20998;&#31867;&#22120;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an emerging technique for training deep models over decentralized clients without exposing private data, which however suffers from label distribution skew and usually results in slow convergence and degraded model performance. This challenge could be more serious when the participating clients are in unstable circumstances and dropout frequently. Previous work and our empirical observations demonstrate that the classifier head for classification task is more sensitive to label skew and the unstable performance of FedAvg mainly lies in the imbalanced training samples across different classes. The biased classifier head will also impact the learning of feature representations. Therefore, maintaining a balanced classifier head is of significant importance for building a better global model. To tackle this issue, we propose a simple yet effective framework by introducing a prior-calibrated softmax function for computing the cross-entropy loss and a prototype-based fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EXO-200&#23454;&#39564;&#30340;&#26102;&#38388;&#25237;&#24433;&#23460;&#20013;&#27169;&#25311;&#20809;&#30005;&#25506;&#27979;&#22120;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27604;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#22320;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27169;&#25311;&#27874;&#24418;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#24191;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#26174;&#33879;&#39640;&#32423;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.06311</link><description>&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;EXO-200&#38378;&#28865;&#20449;&#21495;&#27169;&#25311;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks for Scintillation Signal Simulation in EXO-200. (arXiv:2303.06311v1 [hep-ex])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EXO-200&#23454;&#39564;&#30340;&#26102;&#38388;&#25237;&#24433;&#23460;&#20013;&#27169;&#25311;&#20809;&#30005;&#25506;&#27979;&#22120;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#27604;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#22320;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27169;&#25311;&#27874;&#24418;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#24191;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#26174;&#33879;&#39640;&#32423;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach using Generative Adversarial Networks to simulate photodetector signals from the time projection chamber of the EXO-200 experiment. The method is able to produce high-quality simulated waveforms an order of magnitude faster than traditional simulation methods and can generalize from the training sample and discern salient high-level features of the data.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#25110;&#23454;&#38469;&#20107;&#20214;&#26679;&#26412;&#35757;&#32451;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#20026;&#20195;&#20215;&#29983;&#25104;&#22823;&#35268;&#27169;&#27169;&#25311;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;EXO-200&#23454;&#39564;&#30340;&#26102;&#38388;&#25237;&#24433;&#23460;&#20013;&#27169;&#25311;&#20809;&#30005;&#25506;&#27979;&#22120;&#20449;&#21495;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#20801;&#35768;&#23545;&#32473;&#23450;&#23545;&#35937;&#38598;&#30340;&#24635;&#20307;&#20998;&#24067;&#36827;&#34892;&#38544;&#24335;&#38750;&#21442;&#25968;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#20351;&#29992;&#21407;&#22987;&#38378;&#28865;&#27874;&#24418;&#20316;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#26657;&#20934;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23427;&#33021;&#22815;&#27604;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#22320;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#27169;&#25311;&#27874;&#24418;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#65292;&#33021;&#22815;&#20174;&#35757;&#32451;&#26679;&#26412;&#20013;&#25512;&#24191;&#24182;&#35782;&#21035;&#25968;&#25454;&#30340;&#26174;&#33879;&#39640;&#32423;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#32593;&#32476;&#27491;&#30830;&#25512;&#26029;&#20986;&#25506;&#27979;&#22120;&#20013;&#38378;&#28865;&#20809;&#21709;&#24212;&#30340;&#20301;&#32622;&#20381;&#36182;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks trained on samples of simulated or actual events have been proposed as a way of generating large simulated datasets at a reduced computational cost. In this work, a novel approach to perform the simulation of photodetector signals from the time projection chamber of the EXO-200 experiment is demonstrated. The method is based on a Wasserstein Generative Adversarial Network - a deep learning technique allowing for implicit non-parametric estimation of the population distribution for a given set of objects. Our network is trained on real calibration data using raw scintillation waveforms as input. We find that it is able to produce high-quality simulated waveforms an order of magnitude faster than the traditional simulation approach and, importantly, generalize from the training sample and discern salient high-level features of the data. In particular, the network correctly deduces position dependency of scintillation light response in the detector and corr
&lt;/p&gt;</description></item><item><title>&#39550;&#39542;&#21592;&#30130;&#21171;&#26159;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#19968;&#31181;&#23454;&#26102;&#26816;&#27979;&#39550;&#39542;&#21592;&#30130;&#21171;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.06310</link><description>&lt;p&gt;
&#39550;&#39542;&#21592;&#30130;&#21171;&#26816;&#27979;&#31995;&#32479;&#65306;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Driver Drowsiness Detection System: An Approach By Machine Learning Application. (arXiv:2303.06310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06310
&lt;/p&gt;
&lt;p&gt;
&#39550;&#39542;&#21592;&#30130;&#21171;&#26159;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#21457;&#19968;&#31181;&#23454;&#26102;&#26816;&#27979;&#39550;&#39542;&#21592;&#30130;&#21171;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driver drowsiness is one of the main causes of traffic accidents. This study aims to develop a real-time detection system for driver drowsiness using machine learning algorithms.
&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20107;&#25925;&#23548;&#33268;&#22823;&#22810;&#25968;&#20154;&#30340;&#27515;&#20129;&#21644;&#20260;&#23475;&#12290;&#27599;&#24180;&#26377;&#25968;&#30334;&#19975;&#20154;&#22240;&#20132;&#36890;&#20107;&#25925;&#21463;&#20260;&#25110;&#27515;&#20129;&#65292;&#36825;&#19982;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#30340;&#25968;&#25454;&#19968;&#33268;&#12290;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30561;&#30496;&#12289;&#20241;&#24687;&#25110;&#24863;&#21040;&#30130;&#20518;&#30340;&#39550;&#39542;&#21592;&#21487;&#33021;&#20250;&#22312;&#39550;&#39542;&#36807;&#31243;&#20013;&#30561;&#30528;&#65292;&#21361;&#21450;&#33258;&#24049;&#21644;&#20854;&#20182;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#23433;&#20840;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#30130;&#21171;&#39550;&#39542;&#32780;&#23548;&#33268;&#30340;&#37325;&#22823;&#36947;&#36335;&#20107;&#25925;&#12290;&#29616;&#22312;&#65292;&#30130;&#21171;&#39550;&#39542;&#25104;&#20026;&#21457;&#29983;&#30130;&#21171;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#29616;&#22312;&#65292;&#30130;&#21171;&#25104;&#20026;&#22686;&#21152;&#36947;&#36335;&#20107;&#25925;&#25968;&#37327;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#36825;&#25104;&#20026;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#23613;&#24555;&#35299;&#20915;&#12290;&#25152;&#26377;&#35774;&#22791;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#23454;&#26102;&#26816;&#27979;&#30130;&#21171;&#30340;&#24615;&#33021;&#12290;&#35768;&#22810;&#35774;&#22791;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#65292;&#29992;&#20110;&#26816;&#27979;&#30130;&#21171;&#65292;&#36825;&#20123;&#35774;&#22791;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20063;&#19982;&#39550;&#39542;&#21592;&#30130;&#21171;&#26816;&#27979;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The majority of human deaths and injuries are caused by traffic accidents. A million people worldwide die each year due to traffic accident injuries, consistent with the World Health Organization. Drivers who do not receive enough sleep, rest, or who feel weary may fall asleep behind the wheel, endangering both themselves and other road users. The research on road accidents specified that major road accidents occur due to drowsiness while driving. These days, it is observed that tired driving is the main reason to occur drowsiness. Now, drowsiness becomes the main principle for to increase in the number of road accidents. This becomes a major issue in a world which is very important to resolve as soon as possible. The predominant goal of all devices is to improve the performance to detect drowsiness in real time. Many devices were developed to detect drowsiness, which depend on different artificial intelligence algorithms. So, our research is also related to driver drowsiness detection
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#34394;&#25311;&#21161;&#25163;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34394;&#25311;&#21161;&#25163;&#26159;&#19968;&#31181;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#35821;&#38899;&#21629;&#20196;&#24182;&#33021;&#20195;&#34920;&#24744;&#25191;&#34892;&#20219;&#21153;&#30340;&#36719;&#20214;&#65292;&#21487;&#20197;&#23436;&#25104;&#20960;&#20046;&#20219;&#20309;&#24744;&#33258;&#24049;&#21487;&#20197;&#23436;&#25104;&#30340;&#29305;&#23450;&#26234;&#33021;&#25163;&#26426;&#25110;PC&#27963;&#21160;&#65292;&#32780;&#19988;&#21015;&#34920;&#19981;&#26029;&#25193;&#22823;&#12290;</title><link>http://arxiv.org/abs/2303.06309</link><description>&lt;p&gt;
&#34394;&#25311;&#40736;&#26631;&#21644;&#21161;&#25163;&#65306;&#20154;&#24037;&#26234;&#33021;&#30340;&#25216;&#26415;&#38761;&#21629;
&lt;/p&gt;
&lt;p&gt;
Virtual Mouse And Assistant: A Technological Revolution Of Artificial Intelligence. (arXiv:2303.06309v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#34394;&#25311;&#21161;&#25163;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34394;&#25311;&#21161;&#25163;&#26159;&#19968;&#31181;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#35821;&#38899;&#21629;&#20196;&#24182;&#33021;&#20195;&#34920;&#24744;&#25191;&#34892;&#20219;&#21153;&#30340;&#36719;&#20214;&#65292;&#21487;&#20197;&#23436;&#25104;&#20960;&#20046;&#20219;&#20309;&#24744;&#33258;&#24049;&#21487;&#20197;&#23436;&#25104;&#30340;&#29305;&#23450;&#26234;&#33021;&#25163;&#26426;&#25110;PC&#27963;&#21160;&#65292;&#32780;&#19988;&#21015;&#34920;&#19981;&#26029;&#25193;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the performance improvement of virtual assistants, which are software that understands natural language voice commands and can perform tasks on your behalf. They can complete practically any specific smartphone or PC activity that you can complete on your own, and the list is continually expanding.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#34394;&#25311;&#21161;&#25163;&#30340;&#24615;&#33021;&#12290;&#37027;&#20040;&#20160;&#20040;&#26159;&#34394;&#25311;&#21161;&#25163;&#65311;&#24212;&#29992;&#36719;&#20214;&#65292;&#36890;&#24120;&#31216;&#20026;&#34394;&#25311;&#21161;&#25163;&#65292;&#20063;&#31216;&#20026;AI&#21161;&#25163;&#25110;&#25968;&#23383;&#21161;&#25163;&#65292;&#26159;&#19968;&#31181;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#35821;&#38899;&#21629;&#20196;&#24182;&#33021;&#20195;&#34920;&#24744;&#25191;&#34892;&#20219;&#21153;&#30340;&#36719;&#20214;&#12290;&#34394;&#25311;&#21161;&#25163;&#21487;&#20197;&#23436;&#25104;&#20960;&#20046;&#20219;&#20309;&#24744;&#33258;&#24049;&#21487;&#20197;&#23436;&#25104;&#30340;&#29305;&#23450;&#26234;&#33021;&#25163;&#26426;&#25110;PC&#27963;&#21160;&#65292;&#32780;&#19988;&#21015;&#34920;&#19981;&#26029;&#25193;&#22823;&#12290;&#34394;&#25311;&#21161;&#25163;&#36890;&#24120;&#21487;&#20197;&#23436;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#23433;&#25490;&#20250;&#35758;&#12289;&#21457;&#36865;&#28040;&#24687;&#21644;&#30417;&#25511;&#22825;&#27668;&#12290;&#20197;&#21069;&#30340;&#34394;&#25311;&#21161;&#25163;&#65292;&#22914;Google&#21161;&#25163;&#21644;Cortana&#65292;&#22312;&#26576;&#20123;&#26041;&#38754;&#26377;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#25191;&#34892;&#25628;&#32034;&#65292;&#32780;&#19981;&#26159;&#23436;&#20840;&#33258;&#21160;&#21270;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#24341;&#25806;&#27809;&#26377;&#33021;&#21147;&#21069;&#36827;&#21644;&#20498;&#24102;&#27468;&#26354;&#65292;&#20197;&#20445;&#25345;&#27468;&#26354;&#30340;&#25511;&#21046;&#21151;&#33021;&#65307;&#23427;&#20204;&#21482;&#33021;&#20855;&#26377;&#25628;&#32034;&#27468;&#26354;&#30340;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of this paper is to enhance the performance of the virtual assistant. So, what exactly is a virtual assistant. Application software, often called virtual assistants, also known as AI assistants or digital assistants, is software that understands natural language voice commands and can perform tasks on your behalf. What does a virtual assistant do. Virtual assistants can complete practically any specific smartphone or PC activity that you can complete on your own, and the list is continually expanding. Virtual assistants typically do an impressive variety of tasks, including scheduling meetings, delivering messages, and monitoring the weather. Previous virtual assistants, like Google Assistant and Cortana, had limits in that they could only perform searches and were not entirely automated. For instance, these engines do not have the ability to forward and rewind the song in order to maintain the control function of the song; they can only have the module to search for songs 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21435;&#20013;&#24515;&#21270;&#25237;&#31080;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36523;&#20221;&#35782;&#21035;&#26041;&#24335;&#65292;&#20351;&#24471;&#27599;&#20010;&#20154;&#37117;&#33021;&#36861;&#36394;&#25237;&#31080;&#27450;&#35784;&#65292;&#31995;&#32479;&#38750;&#24120;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2303.06306</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21435;&#20013;&#24515;&#21270;&#25237;&#31080;&#31995;&#32479;&#23433;&#20840;&#35270;&#35282;&#65306;&#25968;&#23383;&#25237;&#31080;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#20445;&#38556;
&lt;/p&gt;
&lt;p&gt;
Blockchain-based decentralized voting system security Perspective: Safe and secure for digital voting system. (arXiv:2303.06306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#21435;&#20013;&#24515;&#21270;&#25237;&#31080;&#31995;&#32479;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36523;&#20221;&#35782;&#21035;&#26041;&#24335;&#65292;&#20351;&#24471;&#27599;&#20010;&#20154;&#37117;&#33021;&#36861;&#36394;&#25237;&#31080;&#27450;&#35784;&#65292;&#31995;&#32479;&#38750;&#24120;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the blockchain-based decentralized voting system and proposes a unique identification method that enables everyone to trace vote fraud, making the system incredibly safe.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#25237;&#31080;&#31995;&#32479;&#65292;&#20026;&#36873;&#27665;&#12289;&#20505;&#36873;&#20154;&#21644;&#23448;&#21592;&#21442;&#19982;&#21644;&#31649;&#29702;&#25237;&#31080;&#25552;&#20379;&#20415;&#21033;&#12290;&#30001;&#20110;&#25105;&#20204;&#22312;&#21518;&#31471;&#20351;&#29992;&#20102;&#21306;&#22359;&#38142;&#65292;&#20351;&#24471;&#27599;&#20010;&#20154;&#37117;&#33021;&#36861;&#36394;&#25237;&#31080;&#27450;&#35784;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#31995;&#32479;&#38750;&#24120;&#23433;&#20840;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#36523;&#20221;&#35782;&#21035;&#26041;&#24335;&#65292;&#21363;&#20351;&#29992;Aadhar&#21345;&#21495;&#25110;OTP&#29983;&#25104;&#65292;&#28982;&#21518;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#25237;&#31080;&#31995;&#32479;&#25237;&#31080;&#12290;&#25552;&#20986;&#20102;&#27604;&#29305;&#24065;&#30340;&#24314;&#35758;&#65292;&#27604;&#29305;&#24065;&#26159;&#19968;&#31181;&#34394;&#25311;&#36135;&#24065;&#31995;&#32479;&#65292;&#30001;&#20013;&#22830;&#26426;&#26500;&#20915;&#23450;&#29983;&#20135;&#36135;&#24065;&#12289;&#36716;&#31227;&#25152;&#26377;&#26435;&#21644;&#39564;&#35777;&#20132;&#26131;&#65292;&#21253;&#25324;&#28857;&#23545;&#28857;&#32593;&#32476;&#22312;&#21306;&#22359;&#38142;&#31995;&#32479;&#20013;&#65292;&#36134;&#26412;&#22312;&#22810;&#20010;&#30456;&#21516;&#30340;&#25968;&#25454;&#24211;&#20013;&#22797;&#21046;&#65292;&#30001;&#19981;&#21516;&#30340;&#36827;&#31243;&#25176;&#31649;&#21644;&#26356;&#26032;&#65292;&#22914;&#26524;&#23545;&#19968;&#20010;&#33410;&#28857;&#36827;&#34892;&#26356;&#25913;&#24182;&#21457;&#29983;&#20132;&#26131;&#65292;&#21017;&#25152;&#26377;&#20854;&#20182;&#33410;&#28857;&#20250;&#21516;&#26102;&#26356;&#26032;&#65292;&#20215;&#20540;&#21644;&#36164;&#20135;&#30340;&#35760;&#24405;&#23558;&#27704;&#20037;&#20132;&#25442;&#65292;&#21482;&#26377;&#29992;&#25143;&#21644;&#31995;&#32479;&#38656;&#35201;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research study focuses primarily on Block-Chain-based voting systems, which facilitate participation in and administration of voting for voters, candidates, and officials. Because we used Block-Chain in the backend, which enables everyone to trace vote fraud, our system is incredibly safe. This paper approach any unique identification the Aadhar Card number or an OTP will be generated then user can utilise the voting system to cast his/her vote. A proposal for Bit-coin, a virtual currency system that is decided by a central authority for producing money, transferring ownership, and validating transactions, included the peer-to-peer network in a Block-Chain system, the ledger is duplicated across several, identical databases which is hosted and updated by a different process and all other nodes are updated concurrently if changes made to one node and a transaction occurs, the records of the values and assets are permanently exchanged, Only the user and the system need to be verifie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23545;&#26368;&#36817;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20197;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#34920;&#26684;&#21644;&#26641;&#29366;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#26041;&#27861;&#20998;&#31867;&#20026;&#21453;&#25915;&#20987;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#25915;&#20987;&#36884;&#24452;&#65292;&#21253;&#25324;&#22522;&#20110;&#25628;&#32034;&#30340;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.06302</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#19982;&#38450;&#24481;&#65306;&#29616;&#20195;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey. (arXiv:2303.06302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#23545;&#26368;&#36817;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20197;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#34920;&#26684;&#21644;&#26641;&#29366;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#26041;&#27861;&#20998;&#31867;&#20026;&#21453;&#25915;&#20987;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#25915;&#20987;&#36884;&#24452;&#65292;&#21253;&#25324;&#22522;&#20110;&#25628;&#32034;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey provides a comprehensive overview of recent advancements in adversarial attack and defense techniques in machine learning and deep neural networks, with a focus on deep neural network-based classification models. The methods are classified into counter-attack detection and robustness enhancement, with a specific focus on regularization-based methods for enhancing robustness. New avenues of attack are also explored, including search-based attacks.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#22312;&#20114;&#32852;&#32593;&#21644;&#30456;&#20851;&#22330;&#26223;&#20013;&#30340;&#24555;&#36895;&#22686;&#38271;&#24212;&#29992;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#24050;&#32463;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#25915;&#20987;&#21407;&#29702;&#23545;&#26368;&#36817;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#65292;&#24182;&#20197;&#35270;&#35273;&#19978;&#21560;&#24341;&#20154;&#30340;&#34920;&#26684;&#21644;&#26641;&#29366;&#22270;&#30340;&#24418;&#24335;&#21576;&#29616;&#12290;&#36825;&#26159;&#22522;&#20110;&#23545;&#29616;&#26377;&#24037;&#20316;&#30340;&#20005;&#26684;&#35780;&#20272;&#65292;&#21253;&#25324;&#23545;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#23558;&#26041;&#27861;&#20998;&#31867;&#20026;&#21453;&#25915;&#20987;&#26816;&#27979;&#21644;&#40065;&#26834;&#24615;&#22686;&#24378;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#22522;&#20110;&#27491;&#21017;&#21270;&#30340;&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#36824;&#25506;&#35752;&#20102;&#26032;&#30340;&#25915;&#20987;&#36884;&#24452;&#65292;&#21253;&#25324;&#22522;&#20110;&#25628;&#32034;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks and defenses in machine learning and deep neural network have been gaining significant attention due to the rapidly growing applications of deep learning in the Internet and relevant scenarios. This survey provides a comprehensive overview of the recent advancements in the field of adversarial attack and defense techniques, with a focus on deep neural network-based classification models. Specifically, we conduct a comprehensive classification of recent adversarial attack methods and state-of-the-art adversarial defense techniques based on attack principles, and present them in visually appealing tables and tree diagrams. This is based on a rigorous evaluation of the existing works, including an analysis of their strengths and limitations. We also categorize the methods into counter-attack detection and robustness enhancement, with a specific focus on regularization-based methods for enhancing robustness. New avenues of attack are also explored, including search-base
&lt;/p&gt;</description></item><item><title>MLP-SRGAN&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN&#65292;&#20351;&#29992;MLP-Mixer&#21644;&#21367;&#31215;&#23618;&#36827;&#34892;&#19978;&#37319;&#26679;&#65292;&#21487;&#29992;&#20110;FLAIR MRI&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.06298</link><description>&lt;p&gt;
MLP-SRGAN: &#20351;&#29992;MLP-Mixer&#30340;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer. (arXiv:2303.06298v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06298
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;GAN&#65292;&#20351;&#29992;MLP-Mixer&#21644;&#21367;&#31215;&#23618;&#36827;&#34892;&#19978;&#37319;&#26679;&#65292;&#21487;&#29992;&#20110;FLAIR MRI&#22270;&#20687;&#30340;&#36229;&#20998;&#36776;&#29575;&#37325;&#24314;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
MLP-SRGAN is a single-dimension Super Resolution GAN that utilizes MLP-Mixers and convolutional layers for upsampling, and can be used for super-resolution reconstruction of FLAIR MRI images. New image quality metrics were proposed.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;MLP-SRGAN&#65292;&#23427;&#26159;&#19968;&#31181;&#21333;&#32500;&#36229;&#20998;&#36776;&#29575;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;SRGAN&#65289;&#65292;&#21033;&#29992;&#22810;&#23618;&#24863;&#30693;&#22120;&#28151;&#21512;&#22120;&#65288;MLP-Mixer&#65289;&#20197;&#21450;&#21367;&#31215;&#23618;&#22312;&#20999;&#29255;&#26041;&#21521;&#19978;&#36827;&#34892;&#19978;&#37319;&#26679;&#12290; MLP-SRGAN&#20351;&#29992;MSSEG2&#25361;&#25112;&#25968;&#25454;&#38598;&#20013;&#30340;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;FLAIR MRI&#36827;&#34892;&#35757;&#32451;&#21644;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#20010;&#20302;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#22810;&#20013;&#24515;FLAIR&#25968;&#25454;&#38598;&#65288;CAIN&#65292;ADNI&#65292;CCNA&#65289;&#30340;&#22270;&#20687;&#65292;&#20197;&#26816;&#26597;&#22312;&#20445;&#30041;&#65288;&#26410;&#35265;&#65289;&#20020;&#24202;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#23558;&#19978;&#37319;&#26679;&#32467;&#26524;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;SR&#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#65288;HR&#65289;&#22522;&#26412;&#20107;&#23454;&#30340;&#22270;&#20687;&#65292;&#20351;&#29992;&#23792;&#20540;&#20449;&#22122;&#27604;&#65288;PSNR&#65289;&#21644;&#32467;&#26500;&#30456;&#20284;&#24615;&#25351;&#25968;&#65288;SSIM&#65289;&#26469;&#34913;&#37327;&#19978;&#37319;&#26679;&#24615;&#33021;&#12290;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#30340;&#32467;&#26500;&#65292;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#65292;&#20197;&#22312;&#32570;&#20047;&#22522;&#30784;&#20107;&#23454;&#30340;&#24773;&#20917;&#19979;&#37327;&#21270;&#38160;&#24230;&#65288;&#36793;&#32536;&#24378;&#24230;&#65289;&#65292;&#22122;&#22768;&#65288;&#29109;&#65289;&#21644;&#27169;&#31946;&#24230;&#65288;&#20302;&#39057;&#20449;&#24687;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel architecture called MLP-SRGAN, which is a single-dimension Super Resolution Generative Adversarial Network (SRGAN) that utilizes Multi-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to upsample in the slice direction. MLP-SRGAN is trained and validated using high resolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was applied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with low spatial resolution in the slice dimension to examine performance on held-out (unseen) clinical data. Upsampled results are compared to several state-of-the-art SR networks. For images with high resolution (HR) ground truths, peak-signal-to-noise-ratio (PSNR) and structural similarity index (SSIM) are used to measure upsampling performance. Several new structural, no-reference image quality metrics were proposed to quantify sharpness (edge strength), noise (entropy), and blurriness (low frequency information) in the absence of groun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.06296</link><description>&lt;p&gt;
&#38450;&#27490;&#27880;&#24847;&#21147;&#29109;&#23849;&#28291;&#30340;Transformer&#35757;&#32451;&#31283;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Transformer Training by Preventing Attention Entropy Collapse. (arXiv:2303.06296v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;$\sigma$Reparam&#65292;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the training dynamics of Transformers and proposes a simple and efficient solution, $\sigma$Reparam, to prevent entropy collapse in the attention layers, promoting more stable training.
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#31283;&#23450;&#24615;&#23545;&#20110;Transformer&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#27880;&#24847;&#21147;&#23618;&#30340;&#28436;&#21464;&#26469;&#25506;&#31350;Transformer&#30340;&#35757;&#32451;&#21160;&#24577;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36319;&#36394;&#27599;&#20010;&#27880;&#24847;&#21147;&#22836;&#30340;&#27880;&#24847;&#21147;&#29109;&#65292;&#36825;&#26159;&#27169;&#22411;&#38160;&#24230;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#20219;&#21153;&#20013;&#23384;&#22312;&#19968;&#31181;&#24120;&#35265;&#27169;&#24335;&#65292;&#21363;&#20302;&#27880;&#24847;&#21147;&#29109;&#20276;&#38543;&#30528;&#39640;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#37319;&#21462;&#25391;&#33633;&#25439;&#22833;&#25110;&#21457;&#25955;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#23558;&#30149;&#24577;&#20302;&#27880;&#24847;&#21147;&#29109;&#65292;&#23545;&#24212;&#39640;&#24230;&#38598;&#20013;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#31216;&#20026;$\textit{&#29109;&#23849;&#28291;}$&#12290;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\sigma$Reparam&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#35889;&#24402;&#19968;&#21270;&#21644;&#39069;&#22806;&#30340;&#23398;&#20064;&#26631;&#37327;&#37325;&#26032;&#21442;&#25968;&#21270;&#25152;&#26377;&#32447;&#24615;&#23618;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#37325;&#26032;&#21442;&#25968;&#21270;&#25104;&#21151;&#22320;&#38450;&#27490;&#20102;&#27880;&#24847;&#21147;&#23618;&#20013;&#30340;&#29109;&#23849;&#28291;&#65292;&#20419;&#36827;&#20102;&#26356;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\textit{entropy collapse}$. As a remedy, we propose $\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that the proposed reparameterization successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#22823;&#26032;&#33410;&#28857;&#25968;&#37327;&#30340;&#38408;&#20540;&#65292;&#35813;&#38408;&#20540;&#20351;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#20445;&#25345;&#36817;&#20284;&#31561;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;&#31354;&#38388;&#19981;&#21464;&#25237;&#24433;&#65288;SIP&#65289;&#65292;&#20351;&#20219;&#24847;&#38745;&#24577;MF&#23884;&#20837;&#26041;&#26696;&#33021;&#22815;&#24555;&#36895;&#23884;&#20837;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#26032;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.06293</link><description>&lt;p&gt;
&#27969;&#24335;&#32593;&#32476;&#23884;&#20837;&#20013;&#30340;&#31354;&#38388;&#19981;&#21464;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Space-Invariant Projection in Streaming Network Embedding. (arXiv:2303.06293v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#22823;&#26032;&#33410;&#28857;&#25968;&#37327;&#30340;&#38408;&#20540;&#65292;&#35813;&#38408;&#20540;&#20351;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#20445;&#25345;&#36817;&#20284;&#31561;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;&#31354;&#38388;&#19981;&#21464;&#25237;&#24433;&#65288;SIP&#65289;&#65292;&#20351;&#20219;&#24847;&#38745;&#24577;MF&#23884;&#20837;&#26041;&#26696;&#33021;&#22815;&#24555;&#36895;&#23884;&#20837;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#26032;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a threshold for the maximum number of new nodes that keep the node embedding space approximately equivalent, and proposes a generation framework called Space-Invariant Projection (SIP) to enable fast embedding of new nodes in dynamic networks using any static MF-based embedding scheme.
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#32593;&#32476;&#20013;&#26032;&#21040;&#36798;&#30340;&#33410;&#28857;&#20250;&#36880;&#28176;&#20351;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#28418;&#31227;&#65292;&#22240;&#27492;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#33410;&#28857;&#23884;&#20837;&#21644;&#19979;&#28216;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#20154;&#22312;&#29702;&#35770;&#25110;&#23454;&#39564;&#20013;&#32771;&#34385;&#36825;&#20123;&#26032;&#33410;&#28857;&#30340;&#30830;&#20999;&#38408;&#20540;&#22823;&#23567;&#65292;&#21363;&#20351;&#36825;&#20123;&#26032;&#33410;&#28857;&#30340;&#22823;&#23567;&#20302;&#20110;&#26576;&#20010;&#38408;&#20540;&#65292;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#20063;&#24456;&#38590;&#34987;&#32500;&#25252;&#12290;&#26412;&#25991;&#20174;&#30697;&#38453;&#25200;&#21160;&#29702;&#35770;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#22823;&#26032;&#33410;&#28857;&#25968;&#37327;&#30340;&#38408;&#20540;&#65292;&#35813;&#38408;&#20540;&#20351;&#33410;&#28857;&#23884;&#20837;&#31354;&#38388;&#20445;&#25345;&#36817;&#20284;&#31561;&#25928;&#65292;&#24182;&#32463;&#36807;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;&#22240;&#27492;&#65292;&#29702;&#35770;&#19978;&#20445;&#35777;&#20102;&#24403;&#26032;&#21040;&#36798;&#33410;&#28857;&#30340;&#25968;&#37327;&#20302;&#20110;&#27492;&#38408;&#20540;&#26102;&#65292;&#36825;&#20123;&#26032;&#33410;&#28857;&#30340;&#23884;&#20837;&#21487;&#20197;&#24555;&#36895;&#20174;&#21407;&#22987;&#33410;&#28857;&#30340;&#23884;&#20837;&#20013;&#23548;&#20986;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;&#31354;&#38388;&#19981;&#21464;&#25237;&#24433;&#65288;SIP&#65289;&#65292;&#20351;&#20219;&#24847;&#38745;&#24577;MF&#23884;&#20837;&#26041;&#26696;&#33021;&#22815;&#24555;&#36895;&#23884;&#20837;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#26032;&#33410;&#28857;&#12290;SIP&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19982;&#32593;&#32476;&#22823;&#23567;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Newly arriving nodes in dynamics networks would gradually make the node embedding space drifted and the retraining of node embedding and downstream models indispensable. An exact threshold size of these new nodes, below which the node embedding space will be predicatively maintained, however, is rarely considered in either theory or experiment. From the view of matrix perturbation theory, a threshold of the maximum number of new nodes that keep the node embedding space approximately equivalent is analytically provided and empirically validated. It is therefore theoretically guaranteed that as the size of newly arriving nodes is below this threshold, embeddings of these new nodes can be quickly derived from embeddings of original nodes. A generation framework, Space-Invariant Projection (SIP), is accordingly proposed to enables arbitrary static MF-based embedding schemes to embed new nodes in dynamics networks fast. The time complexity of SIP is linear with the network size. By combinin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;DMD&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;DLHDMD&#65292;&#21033;&#29992;Takens&#23884;&#20837;&#23450;&#29702;&#30340;&#22522;&#26412;&#35265;&#35299;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#39640;&#32500;&#21644;&#28151;&#27788;&#21160;&#21147;&#23398;&#65292;&#33021;&#22815;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#30340;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.06289</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22686;&#24378;&#30340;Hankel&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Enhanced Hankel Dynamic-Mode Decomposition. (arXiv:2303.06289v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;DMD&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;DLHDMD&#65292;&#21033;&#29992;Takens&#23884;&#20837;&#23450;&#29702;&#30340;&#22522;&#26412;&#35265;&#35299;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#39640;&#32500;&#21644;&#28151;&#27788;&#21160;&#21147;&#23398;&#65292;&#33021;&#22815;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a deep learning DMD based method, called DLHDMD, which uses the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics, and is able to generate accurate dynamics for chaotic time series.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#30340;&#33719;&#21462;&#21464;&#24471;&#36234;&#26469;&#36234;&#31616;&#21333;&#21644;&#22797;&#26434;&#65292;&#20294;&#20174;&#26102;&#38388;&#24207;&#21015;&#20013;&#24320;&#21457;&#21160;&#24577;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#38382;&#39064;&#39046;&#22495;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#24050;&#32463;&#19982;&#25152;&#35859;&#30340;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;DMD&#65289;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#36890;&#29992;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#29305;&#21035;&#26377;&#21069;&#36884;&#30340;&#31934;&#23494;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#24320;&#21457;&#36884;&#24452;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;DMD&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;Takens&#23884;&#20837;&#23450;&#29702;&#30340;&#22522;&#26412;&#35265;&#35299;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#26041;&#26696;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#20102;&#39640;&#32500;&#21644;&#28151;&#27788;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;&#28145;&#24230;&#23398;&#20064;Hankel DMD&#65288;DLHDMD&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DLHDMD&#33021;&#22815;&#20026;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#30340;&#21160;&#24577;&#65292;&#24182;&#25506;&#35752;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#23398;&#20064;&#26144;&#23556;&#65292;&#36825;&#20123;&#26144;&#23556;&#22312;&#25104;&#21151;&#35757;&#32451;&#21518;&#24448;&#24448;&#36235;&#21521;&#20110;&#26174;&#33879;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the acquisition of time series has become increasingly more straightforward and sophisticated, developing dynamical models from time series is still a challenging and ever evolving problem domain. Within the last several years, to address this problem, there has been a merging of machine learning tools with what is called the dynamic mode decomposition (DMD). This general approach has been shown to be an especially promising avenue for sophisticated and accurate model development. Building on this prior body of work, we develop a deep learning DMD based method which makes use of the fundamental insight of Takens' Embedding Theorem to develop an adaptive learning scheme that better captures higher dimensional and chaotic dynamics. We call this method the Deep Learning Hankel DMD (DLHDMD). We show that the DLHDMD is able to generate accurate dynamics for chaotic time series, and we likewise explore how our method learns mappings which tend, after successful training, to significant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#40657;&#30418;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;82.2&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;76.5&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06280</link><description>&lt;p&gt;
&#25506;&#31350;&#26377;&#29366;&#24577;&#38450;&#24481;&#40657;&#30418;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Investigating Stateful Defenses Against Black-Box Adversarial Examples. (arXiv:2303.06280v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#40657;&#30418;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;82.2&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;76.5&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates stateful defenses against black-box adversarial examples and proposes a new stateful defense model that achieves 82.2% accuracy on the CIFAR10 dataset and 76.5% accuracy on the ImageNet dataset.
&lt;/p&gt;
&lt;p&gt;
&#38450;&#24481;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20813;&#21463;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#24050;&#34987;&#35777;&#26126;&#26497;&#20026;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#65292;&#35797;&#22270;&#38450;&#24481;&#26356;&#21463;&#38480;&#21046;&#30340;&#40657;&#30418;&#25915;&#20987;&#32773;&#12290;&#36825;&#20123;&#38450;&#24481;&#36890;&#36807;&#36319;&#36394;&#20256;&#20837;&#27169;&#22411;&#26597;&#35810;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#25298;&#32477;&#37027;&#20123;&#21487;&#30097;&#22320;&#30456;&#20284;&#30340;&#26597;&#35810;&#26469;&#25805;&#20316;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;Blacklight&#26159;&#22312;USENIX Security '22&#19978;&#25552;&#20986;&#30340;&#65292;&#22768;&#31216;&#21487;&#20197;&#38450;&#27490;&#20960;&#20046;100&#65285;&#30340;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#35843;&#25972;&#29616;&#26377;&#40657;&#30418;&#25915;&#20987;&#30340;&#21442;&#25968;&#65292;&#26174;&#33879;&#38477;&#20302;&#21463;Blacklight&#20445;&#25252;&#30340;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#65288;&#20363;&#22914;&#65292;&#22312;CIFAR10&#19978;&#20174;82.2&#65285;&#38477;&#33267;6.4&#65285;&#65289;&#12290;&#21463;&#21040;&#36825;&#19968;&#24778;&#20154;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#30340;&#31995;&#32479;&#21270;&#65292;&#20197;&#20102;&#35299;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#20250;&#22833;&#36133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#20026;82.2&#65285;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#20026;76.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defending machine-learning (ML) models against white-box adversarial attacks has proven to be extremely difficult. Instead, recent work has proposed stateful defenses in an attempt to defend against a more restricted black-box attacker. These defenses operate by tracking a history of incoming model queries, and rejecting those that are suspiciously similar. The current state-of-the-art stateful defense Blacklight was proposed at USENIX Security '22 and claims to prevent nearly 100% of attacks on both the CIFAR10 and ImageNet datasets. In this paper, we observe that an attacker can significantly reduce the accuracy of a Blacklight-protected classifier (e.g., from 82.2% to 6.4% on CIFAR10) by simply adjusting the parameters of an existing black-box attack. Motivated by this surprising observation, since existing attacks were evaluated by the Blacklight authors, we provide a systematization of stateful defenses to understand why existing stateful defense models fail. Finally, we propose a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26126;&#30830;&#22320;&#32534;&#30721;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26500;&#24863;&#30693;&#34507;&#30333;&#36136;&#34920;&#31034;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06275</link><description>&lt;p&gt;
&#32467;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enhancing Protein Language Models with Structure-based Encoder and Pre-training. (arXiv:2303.06275v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26126;&#30830;&#22320;&#32534;&#30721;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26500;&#24863;&#30693;&#34507;&#30333;&#36136;&#34920;&#31034;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes enhancing protein language models with structure-based encoder and pre-training to explicitly encode protein structures for better structure-aware protein representations, and empirically verifies its effectiveness.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#34507;&#30333;&#36136;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#34507;&#30333;&#36136;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#23613;&#31649;&#33021;&#22815;&#38544;&#24335;&#22320;&#25429;&#33719;&#27531;&#22522;&#38388;&#30340;&#25509;&#35302;&#20449;&#24687;&#65292;&#20294;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;PLMs&#19981;&#33021;&#26126;&#30830;&#22320;&#32534;&#30721;&#34507;&#30333;&#36136;&#32467;&#26500;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26500;&#24863;&#30693;&#34507;&#30333;&#36136;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#32467;&#26500;&#23545;&#20110;&#30830;&#23450;&#21151;&#33021;&#24456;&#37325;&#35201;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#22312;&#21487;&#29992;&#34507;&#30333;&#36136;&#32467;&#26500;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#25913;&#36827;&#36825;&#20123;PLMs&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20351;&#29992;&#22522;&#20110;&#32467;&#26500;&#30340;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;PLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models (PLMs) pre-trained on large-scale protein sequence corpora have achieved impressive performance on various downstream protein understanding tasks. Despite the ability to implicitly capture inter-residue contact information, transformer-based PLMs cannot encode protein structures explicitly for better structure-aware protein representations. Besides, the power of pre-training on available protein structures has not been explored for improving these PLMs, though structures are important to determine functions. To tackle these limitations, in this work, we enhance the PLMs with structure-based encoder and pre-training. We first explore feasible model architectures to combine the advantages of a state-of-the-art PLM (i.e., ESM-1b1) and a state-of-the-art protein structure encoder (i.e., GearNet). We empirically verify the ESM-GearNet that connects two encoders in a series way as the most effective combination model. To further improve the effectiveness of ESM-GearNe
&lt;/p&gt;</description></item><item><title>CoNIC&#25361;&#25112;&#20351;&#29992;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#26680;&#20998;&#21106;&#21644;&#32454;&#32990;&#32452;&#25104;&#65292;&#21050;&#28608;&#20102;&#21487;&#37325;&#22797;&#30340;&#32454;&#32990;&#35782;&#21035;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#21457;&#29616;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21644;&#20013;&#24615;&#31890;&#32454;&#32990;&#22312;&#32959;&#30244;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.06274</link><description>&lt;p&gt;
CoNIC&#25361;&#25112;&#65306;&#25512;&#21160;&#26680;&#26816;&#27979;&#12289;&#20998;&#21106;&#12289;&#20998;&#31867;&#21644;&#35745;&#25968;&#30340;&#21069;&#27839;&#65288;arXiv:2303.06274v1 [cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
CoNIC Challenge: Pushing the Frontiers of Nuclear Detection, Segmentation, Classification and Counting. (arXiv:2303.06274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06274
&lt;/p&gt;
&lt;p&gt;
CoNIC&#25361;&#25112;&#20351;&#29992;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#26680;&#20998;&#21106;&#21644;&#32454;&#32990;&#32452;&#25104;&#65292;&#21050;&#28608;&#20102;&#21487;&#37325;&#22797;&#30340;&#32454;&#32990;&#35782;&#21035;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#21457;&#29616;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21644;&#20013;&#24615;&#31890;&#32454;&#32990;&#22312;&#32959;&#30244;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CoNIC challenge used the largest dataset to evaluate nuclear segmentation and cellular composition, stimulated the development of reproducible algorithms for cellular recognition, and found that eosinophils and neutrophils play an important role in tumors.
&lt;/p&gt;
&lt;p&gt;
&#26680;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#24418;&#24577;&#27979;&#37327;&#26159;&#24110;&#21161;&#25105;&#20204;&#36827;&#19968;&#27493;&#20102;&#35299;&#32452;&#32455;&#23398;&#21644;&#24739;&#32773;&#39044;&#21518;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#19968;&#39046;&#22495;&#30340;&#21019;&#26032;&#65292;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#35774;&#32622;&#20102;&#19968;&#20010;&#31038;&#21306;&#24191;&#27867;&#30340;&#25361;&#25112;&#65292;&#20197;&#35780;&#20272;&#26680;&#20998;&#21106;&#21644;&#32454;&#32990;&#32452;&#25104;&#12290;&#25105;&#20204;&#30340;&#25361;&#25112;&#21517;&#20026;CoNIC&#65292;&#21050;&#28608;&#20102;&#21487;&#37325;&#22797;&#30340;&#32454;&#32990;&#35782;&#21035;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#20849;&#25490;&#34892;&#27036;&#19978;&#36827;&#34892;&#23454;&#26102;&#32467;&#26524;&#26816;&#26597;&#12290;&#25105;&#20204;&#22522;&#20110;1,658&#20010;&#32467;&#32928;&#32452;&#32455;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#23545;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#21518;&#25361;&#25112;&#20998;&#26512;&#12290;&#27599;&#20010;&#27169;&#22411;&#26816;&#27979;&#21040;&#32422;7&#20159;&#20010;&#32454;&#32990;&#26680;&#65292;&#30456;&#20851;&#29305;&#24449;&#29992;&#20110;&#19981;&#33391;&#22686;&#29983;&#20998;&#32423;&#21644;&#29983;&#23384;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25361;&#25112;&#23545;&#20808;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#25913;&#36827;&#23548;&#33268;&#20102;&#19979;&#28216;&#24615;&#33021;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#36824;&#34920;&#26126;&#65292;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#21644;&#20013;&#24615;&#31890;&#32454;&#32990;&#22312;&#32959;&#30244;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nuclear detection, segmentation and morphometric profiling are essential in helping us further understand the relationship between histology and patient outcome. To drive innovation in this area, we setup a community-wide challenge using the largest available dataset of its kind to assess nuclear segmentation and cellular composition. Our challenge, named CoNIC, stimulated the development of reproducible algorithms for cellular recognition with real-time result inspection on public leaderboards. We conducted an extensive post-challenge analysis based on the top-performing models using 1,658 whole-slide images of colon tissue. With around 700 million detected nuclei per model, associated features were used for dysplasia grading and survival analysis, where we demonstrated that the challenge's improvement over the previous state-of-the-art led to significant boosts in downstream performance. Our findings also suggest that eosinophils and neutrophils play an important role in the tumour m
&lt;/p&gt;</description></item><item><title>DEPLOYR&#26159;&#19968;&#20010;&#25216;&#26415;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#30740;&#31350;&#20154;&#21592;&#21019;&#24314;&#30340;&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#26102;&#37096;&#32626;&#21040;&#24191;&#27867;&#20351;&#29992;&#30340;&#30005;&#23376;&#30149;&#21382;&#31995;&#32479;&#20013;&#65292;&#24182;&#25552;&#20379;&#30417;&#25511;&#21644;&#21453;&#39304;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.06269</link><description>&lt;p&gt;
DEPLOYR&#65306;&#23558;&#33258;&#23450;&#20041;&#23454;&#26102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#21040;&#30005;&#23376;&#30149;&#21382;&#30340;&#25216;&#26415;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DEPLOYR: A technical framework for deploying custom real-time machine learning models into the electronic medical record. (arXiv:2303.06269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06269
&lt;/p&gt;
&lt;p&gt;
DEPLOYR&#26159;&#19968;&#20010;&#25216;&#26415;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#30740;&#31350;&#20154;&#21592;&#21019;&#24314;&#30340;&#20020;&#24202;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#26102;&#37096;&#32626;&#21040;&#24191;&#27867;&#20351;&#29992;&#30340;&#30005;&#23376;&#30149;&#21382;&#31995;&#32479;&#20013;&#65292;&#24182;&#25552;&#20379;&#30417;&#25511;&#21644;&#21453;&#39304;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
DEPLOYR is a technical framework that enables real-time deployment and monitoring of researcher-created clinical machine learning models into widely used electronic medical record systems, with mechanisms for triggering inference, collecting real-time data, displaying inferences back to end-users, monitoring performance, and silent deployment.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#25104;&#21151;&#30340;&#36716;&#21270;&#21364;&#24456;&#23569;&#12290;&#21307;&#30103;&#26426;&#26500;&#27491;&#22312;&#24314;&#31435;&#26694;&#26550;&#26469;&#31649;&#29702;&#21644;&#20419;&#36827;&#20934;&#30830;&#12289;&#21487;&#25805;&#20316;&#21644;&#21487;&#38752;&#30340;&#27169;&#22411;&#30340;&#23454;&#26045;&#65292;&#36825;&#20123;&#27169;&#22411;&#19982;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#30456;&#32467;&#21512;&#12290;&#36825;&#26679;&#30340;&#27835;&#29702;&#26694;&#26550;&#38656;&#35201;&#19968;&#20010;&#30456;&#24212;&#30340;&#25216;&#26415;&#26694;&#26550;&#65292;&#20197;&#36164;&#28304;&#26377;&#25928;&#30340;&#26041;&#24335;&#37096;&#32626;&#27169;&#22411;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DEPLOYR&#65292;&#19968;&#20010;&#25216;&#26415;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#37096;&#32626;&#21644;&#30417;&#25511;&#30740;&#31350;&#20154;&#21592;&#21019;&#24314;&#30340;&#20020;&#24202;ML&#27169;&#22411;&#21040;&#24191;&#27867;&#20351;&#29992;&#30340;&#30005;&#23376;&#30149;&#21382;&#65288;EMR&#65289;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26680;&#24515;&#21151;&#33021;&#21644;&#35774;&#35745;&#20915;&#31574;&#65292;&#21253;&#25324;&#22522;&#20110;EMR&#36719;&#20214;&#20869;&#37096;&#25805;&#20316;&#35302;&#21457;&#25512;&#29702;&#30340;&#26426;&#21046;&#12289;&#25910;&#38598;&#23454;&#26102;&#25968;&#25454;&#20197;&#36827;&#34892;&#25512;&#29702;&#30340;&#27169;&#22359;&#12289;&#36890;&#36807;&#26174;&#31034;&#25512;&#29702;&#32467;&#26524;&#22238;&#39304;&#32473;&#26368;&#32456;&#29992;&#25143;&#30340;&#26426;&#21046;&#12289;&#36319;&#36394;&#37096;&#32626;&#27169;&#22411;&#24615;&#33021;&#30340;&#30417;&#25511;&#27169;&#22359;&#12289;&#38745;&#40664;&#37096;&#32626;&#31561;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) applications in healthcare are extensively researched, but successful translations to the bedside are scant. Healthcare institutions are establishing frameworks to govern and promote the implementation of accurate, actionable and reliable models that integrate with clinical workflow. Such governance frameworks require an accompanying technical framework to deploy models in a resource efficient manner. Here we present DEPLOYR, a technical framework for enabling real-time deployment and monitoring of researcher created clinical ML models into a widely used electronic medical record (EMR) system. We discuss core functionality and design decisions, including mechanisms to trigger inference based on actions within EMR software, modules that collect real-time data to make inferences, mechanisms that close-the-loop by displaying inferences back to end-users within their workflow, monitoring modules that track performance of deployed models over time, silent deployment ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#29702;&#35770;&#25552;&#35758;&#21450;&#20854;&#23454;&#39564;&#23454;&#29616;&#65292;&#37325;&#28857;&#22238;&#39038;&#20102;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#12289;&#37327;&#23376;&#33258;&#32534;&#30721;&#22120;&#21644;&#37327;&#23376;&#35760;&#24518;&#30005;&#38459;&#22120;&#31561;&#39640;&#24433;&#21709;&#20027;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#25512;&#21160;&#36825;&#39033;&#25216;&#26415;&#30340;&#21021;&#27493;&#37327;&#23376;&#23454;&#29616;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06263</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#65306;&#25552;&#35758;&#21644;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning Implementations: Proposals and Experiments. (arXiv:2303.06263v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#29702;&#35770;&#25552;&#35758;&#21450;&#20854;&#23454;&#39564;&#23454;&#29616;&#65292;&#37325;&#28857;&#22238;&#39038;&#20102;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#12289;&#37327;&#23376;&#33258;&#32534;&#30721;&#22120;&#21644;&#37327;&#23376;&#35760;&#24518;&#30005;&#38459;&#22120;&#31561;&#39640;&#24433;&#21709;&#20027;&#39064;&#65292;&#24182;&#24378;&#35843;&#20102;&#25512;&#21160;&#36825;&#39033;&#25216;&#26415;&#30340;&#21021;&#27493;&#37327;&#23376;&#23454;&#29616;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides an overview of recent theoretical proposals and experimental implementations in the field of quantum machine learning, with a focus on high-impact topics such as quantum reinforcement learning, quantum autoencoders, and quantum memristors. The article emphasizes the necessity of pushing forward initial quantum implementations of this technology to achieve better machine learning calculations than any current or future computing paradigm.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#29702;&#35770;&#25552;&#35758;&#21450;&#20854;&#23454;&#39564;&#23454;&#29616;&#65292;&#24182;&#22238;&#39038;&#20102;&#29305;&#23450;&#30340;&#39640;&#24433;&#21709;&#20027;&#39064;&#65292;&#22914;&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#12289;&#37327;&#23376;&#33258;&#32534;&#30721;&#22120;&#21644;&#37327;&#23376;&#35760;&#24518;&#30005;&#38459;&#22120;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#37327;&#23376;&#20809;&#23376;&#23398;&#21644;&#36229;&#23548;&#30005;&#36335;&#24179;&#21488;&#19978;&#30340;&#23454;&#39564;&#23454;&#29616;&#12290;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21487;&#33021;&#26159;&#39318;&#25209;&#20026;&#24037;&#19994;&#21644;&#31038;&#20250;&#24102;&#26469;&#30410;&#22788;&#30340;&#37327;&#23376;&#25216;&#26415;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#25512;&#21160;&#36825;&#39033;&#25216;&#26415;&#30340;&#21021;&#27493;&#37327;&#23376;&#23454;&#29616;&#65292;&#20197;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#23454;&#29616;&#27604;&#20219;&#20309;&#24403;&#21069;&#25110;&#26410;&#26469;&#35745;&#31639;&#33539;&#24335;&#26356;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article gives an overview and a perspective of recent theoretical proposals and their experimental implementations in the field of quantum machine learning. Without an aim to being exhaustive, the article reviews specific high-impact topics such as quantum reinforcement learning, quantum autoencoders, and quantum memristors, and their experimental realizations in the platforms of quantum photonics and superconducting circuits. The field of quantum machine learning could be among the first quantum technologies producing results that are beneficial for industry and, in turn, to society. Therefore, it is necessary to push forward initial quantum implementations of this technology, in Noisy Intermediate-Scale Quantum Computers, aiming for achieving fruitful calculations in machine learning that are better than with any other current or future computing paradigm.
&lt;/p&gt;</description></item><item><title>STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06261</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;
&lt;/p&gt;
&lt;p&gt;
Interpretable Outlier Summarization. (arXiv:2303.06261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06261
&lt;/p&gt;
&lt;p&gt;
STAIR&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#27719;&#24635;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
STAIR proposes an interpretable outlier summarization method by learning a compact set of human understandable rules to summarize and explain the anomaly detection results, which has strong interpretability to accurately summarize the detection results.
&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20540;&#26816;&#27979;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#38450;&#27490;&#37329;&#34701;&#27450;&#35784;&#12289;&#38450;&#24481;&#32593;&#32476;&#20837;&#20405;&#25110;&#26816;&#27979;&#21363;&#23558;&#21457;&#29983;&#30340;&#35774;&#22791;&#25925;&#38556;&#12290;&#20026;&#20102;&#20943;&#23569;&#20154;&#21147;&#35780;&#20272;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#26377;&#25928;&#22320;&#23558;&#24322;&#24120;&#20540;&#36716;&#21270;&#20026;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#65292;&#29992;&#25143;&#36890;&#24120;&#24076;&#26395;&#31995;&#32479;&#33258;&#21160;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#32467;&#26524;&#30340;&#23376;&#32452;&#30340;&#27719;&#24635;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#31995;&#32479;&#23384;&#22312;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAIR&#65292;&#23427;&#23398;&#20064;&#20102;&#19968;&#32452;&#32039;&#20945;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#35268;&#21017;&#65292;&#20197;&#27719;&#24635;&#21644;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#19981;&#20351;&#29992;&#32463;&#20856;&#30340;&#20915;&#31574;&#26641;&#31639;&#27861;&#26469;&#20135;&#29983;&#36825;&#20123;&#35268;&#21017;&#65292;&#32780;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#20135;&#29983;&#23569;&#37327;&#35268;&#21017;&#65292;&#20855;&#26377;&#26368;&#23567;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20855;&#26377;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#20934;&#30830;&#22320;&#24635;&#32467;&#26816;&#27979;&#32467;&#26524;&#12290;STAIR&#30340;&#23398;&#20064;&#31639;&#27861;&#36890;&#36807;&#36845;&#20195;&#20998;&#21106;&#22823;&#35268;&#21017;&#26469;&#20135;&#29983;&#35268;&#21017;&#38598;&#65292;&#24182;&#22312;&#27599;&#20010;i&#20013;&#26368;&#22823;&#21270;&#36825;&#20010;&#30446;&#26631;&#65292;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outlier detection is critical in real applications to prevent financial fraud, defend network intrusions, or detecting imminent device failures. To reduce the human effort in evaluating outlier detection results and effectively turn the outliers into actionable insights, the users often expect a system to automatically produce interpretable summarizations of subgroups of outlier detection results. Unfortunately, to date no such systems exist. To fill this gap, we propose STAIR which learns a compact set of human understandable rules to summarize and explain the anomaly detection results. Rather than use the classical decision tree algorithms to produce these rules, STAIR proposes a new optimization objective to produce a small number of rules with least complexity, hence strong interpretability, to accurately summarize the detection results. The learning algorithm of STAIR produces a rule set by iteratively splitting the large rules and is optimal in maximizing this objective in each i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29615;&#22659;&#22122;&#22768;&#21644;&#20809;&#32447;&#20449;&#24687;&#65292;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ICU&#24739;&#32773;&#35893;&#22916;&#39044;&#27979;&#27169;&#22411;&#65292;&#20026;&#35893;&#22916;&#30340;&#39044;&#38450;&#21644;&#27835;&#30103;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.06253</link><description>&lt;p&gt;
ICU&#29615;&#22659;&#22122;&#22768;&#21644;&#20809;&#32447;&#20449;&#24687;&#39044;&#27979;&#35893;&#22916;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Predicting risk of delirium from ambient noise and light information in the ICU. (arXiv:2303.06253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29615;&#22659;&#22122;&#22768;&#21644;&#20809;&#32447;&#20449;&#24687;&#65292;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ICU&#24739;&#32773;&#35893;&#22916;&#39044;&#27979;&#27169;&#22411;&#65292;&#20026;&#35893;&#22916;&#30340;&#39044;&#38450;&#21644;&#27835;&#30103;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study developed the first deep-learning based delirium prediction model for ICU patients using only ambient noise and light information, providing new insights for the prevention and treatment of delirium.
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#37325;&#30151;&#30417;&#25252;&#23460;&#65288;ICU&#65289;&#35893;&#22916;&#39044;&#27979;&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#29615;&#22659;&#22240;&#32032;&#65292;&#23613;&#31649;&#26377;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#23427;&#20204;&#23545;&#35893;&#22916;&#26377;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;Thunderboard&#12289;ActiGraph&#20256;&#24863;&#22120;&#21644;iPod with AudioTools&#24212;&#29992;&#31243;&#24207;&#65292;&#20165;&#20351;&#29992;&#29615;&#22659;&#22122;&#22768;&#21644;&#20809;&#32447;&#20449;&#24687;&#65292;&#25253;&#36947;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;ICU&#24739;&#32773;&#35893;&#22916;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#27979;&#37327;&#25968;&#25454;&#20174;2021&#24180;5&#26376;&#33267;2022&#24180;9&#26376;&#25910;&#38598;&#33258;102&#21517;&#24739;&#32773;&#30340;ICU&#30149;&#25151;&#65292;&#20998;&#20026;&#30333;&#22825;&#65288;0700&#33267;1859&#65289;&#21644;&#22812;&#38388;&#65288;1900&#33267;0659&#65289;&#12290;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#39044;&#27979;ICU&#20303;&#38498;&#26399;&#38388;&#25110;&#20986;&#38498;&#21518;4&#22825;&#20869;&#30340;&#35893;&#22916;&#21457;&#29983;&#29575;&#12290;&#26368;&#21518;&#65292;&#20998;&#26512;&#32467;&#26524;&#24471;&#20998;&#20197;&#35780;&#20272;&#27599;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#21644;&#26041;&#21521;&#24615;&#12290;&#30333;&#22825;&#30340;&#22122;&#22768;&#27700;&#24179;&#26174;&#33879;&#39640;&#20110;&#22812;&#38388;&#30340;&#22122;&#22768;&#27700;&#24179;&#12290;&#24403;&#20165;&#20351;&#29992;&#22122;&#22768;&#29305;&#24449;&#25110;&#22122;&#22768;&#21644;&#20809;&#29305;&#24449;&#30340;&#32452;&#21512;&#26102;&#65292;1-D&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Existing Intensive Care Unit (ICU) delirium prediction models do not consider environmental factors despite strong evidence of their influence on delirium. This study reports the first deep-learning based delirium prediction model for ICU patients using only ambient noise and light information. Ambient light and noise intensities were measured from ICU rooms of 102 patients from May 2021 to September 2022 using Thunderboard, ActiGraph sensors and an iPod with AudioTools application. These measurements were divided into daytime (0700 to 1859) and nighttime (1900 to 0659). Deep learning models were trained using this data to predict the incidence of delirium during ICU stay or within 4 days of discharge. Finally, outcome scores were analyzed to evaluate the importance and directionality of every feature. Daytime noise levels were significantly higher than nighttime noise levels. When using only noise features or a combination of noise and light features 1-D convolutional neural networks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2303.06246</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Zone-based Federated Learning for Mobile Sensing Data. (arXiv:2303.06246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#24182;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a zone-based federated learning method for training deep learning models with mobile sensing data. The method divides the physical space into geographical zones and maps them to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model that adapts well to the data and behaviors of users in that zone, while protecting user data privacy.
&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;mHealth&#21644;&#20581;&#24247;&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#20174;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#25110;&#21487;&#31359;&#25140;&#35774;&#22791;&#25910;&#38598;&#30340;&#31227;&#21160;&#24863;&#30693;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20013;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#31227;&#21160;&#24863;&#30693;DL&#31995;&#32479;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#36866;&#24212;&#29992;&#25143;&#30340;&#31227;&#21160;&#34892;&#20026;&#65292;&#38543;&#30528;&#29992;&#25143;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#24182;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;ZoneFL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#35201;&#27714;&#12290;ZoneFL&#23558;&#29289;&#29702;&#31354;&#38388;&#21010;&#20998;&#20026;&#22320;&#29702;&#21306;&#22495;&#65292;&#26144;&#23556;&#21040;&#31227;&#21160;&#36793;&#32536;&#20113;&#31995;&#32479;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#27599;&#20010;&#21306;&#22495;&#37117;&#26377;&#19968;&#20010;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#65292;&#31216;&#20026;&#21306;&#22495;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#24456;&#22909;&#22320;&#36866;&#24212;&#35813;&#21306;&#22495;&#29992;&#25143;&#30340;&#25968;&#25454;&#21644;&#34892;&#20026;&#12290;&#21463;&#30410;&#20110;FL&#35774;&#35745;&#65292;ZoneFL&#22521;&#35757;&#26399;&#38388;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21306;&#22495;&#30340;&#32852;&#21512;&#35757;&#32451;&#31639;&#27861;&#26469;&#20248;&#21270;&#21306;&#22495;&#27169;&#22411;&#20197;&#36866;&#24212;&#29992;&#25143;&#30340;&#31227;&#21160;&#34892;&#20026;&#65306;&#21306;&#22495;&#21512;&#24182;&#21644;&#20998;&#35010;&#65288;ZMS&#65289;&#21644;Zo
&lt;/p&gt;
&lt;p&gt;
Mobile apps, such as mHealth and wellness applications, can benefit from deep learning (DL) models trained with mobile sensing data collected by smart phones or wearable devices. However, currently there is no mobile sensing DL system that simultaneously achieves good model accuracy while adapting to user mobility behavior, scales well as the number of users increases, and protects user data privacy. We propose Zone-based Federated Learning (ZoneFL) to address these requirements. ZoneFL divides the physical space into geographical zones mapped to a mobile-edge-cloud system architecture for good model accuracy and scalability. Each zone has a federated training model, called a zone model, which adapts well to data and behaviors of users in that zone. Benefiting from the FL design, the user data privacy is protected during the ZoneFL training. We propose two novel zone-based federated training algorithms to optimize zone models to user mobility behavior: Zone Merge and Split (ZMS) and Zo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#27169;&#22411;&#65288;HYSP&#65289;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#29983;&#25104;&#21516;&#19968;&#26679;&#26412;&#30340;&#20004;&#20010;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#23558;&#19968;&#20010;&#35270;&#22270;&#19982;&#21478;&#19968;&#20010;&#35270;&#22270;&#21305;&#37197;&#26469;&#23398;&#20064;&#65292;&#20351;&#29992;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#65292;&#20551;&#35774;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#30340;&#26679;&#26412;&#24212;&#26356;&#24378;&#28872;&#22320;&#25512;&#21160;&#35757;&#32451;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.06242</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#35270;&#35282;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#23398;&#20064;&#29992;&#20110;&#33258;&#30417;&#30563;&#39592;&#26550;&#21160;&#20316;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action Representations. (arXiv:2303.06242v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#27169;&#22411;&#65288;HYSP&#65289;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#29983;&#25104;&#21516;&#19968;&#26679;&#26412;&#30340;&#20004;&#20010;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#23558;&#19968;&#20010;&#35270;&#22270;&#19982;&#21478;&#19968;&#20010;&#35270;&#22270;&#21305;&#37197;&#26469;&#23398;&#20064;&#65292;&#20351;&#29992;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#65292;&#20551;&#35774;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#30340;&#26679;&#26412;&#24212;&#26356;&#24378;&#28872;&#22320;&#25512;&#21160;&#35757;&#32451;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations, which adopts self-supervision and uses data augmentations to generate two views of the same sample, and learns by matching one to the other. It uses hyperbolic uncertainty to determine the algorithmic learning pace, assuming that less uncertain samples should be more strongly driving the training, with a larger weight and pace.
&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#23398;&#20064;&#22312;&#19968;&#20123;&#20219;&#21153;&#20013;&#26377;&#30410;&#65292;&#20363;&#22914;&#24369;&#30417;&#30563;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#21487;&#20197;&#36873;&#25321;&#21644;&#25490;&#24207;&#35757;&#32451;&#26679;&#26412;&#24207;&#21015;&#65292;&#20174;&#26131;&#21040;&#38590;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#20173;&#26410;&#34987;&#25506;&#32034;&#65292;&#20854;&#20013;&#20219;&#21153;&#30340;&#30693;&#35782;&#22312;&#35757;&#32451;&#26399;&#38388;&#25104;&#29087;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36229;&#20284;&#26354;&#33258;&#36866;&#24212;&#27169;&#22411;&#65288;HYSP&#65289;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#39592;&#26550;&#30340;&#21160;&#20316;&#34920;&#31034;&#12290;HYSP&#37319;&#29992;&#33258;&#30417;&#30563;&#65306;&#23427;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#26469;&#29983;&#25104;&#21516;&#19968;&#26679;&#26412;&#30340;&#20004;&#20010;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#23558;&#19968;&#20010;&#35270;&#22270;&#65288;&#31216;&#20026;&#22312;&#32447;&#65289;&#19982;&#21478;&#19968;&#20010;&#35270;&#22270;&#65288;&#30446;&#26631;&#65289;&#21305;&#37197;&#26469;&#23398;&#20064;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#65292;&#20551;&#35774;&#19981;&#30830;&#23450;&#24615;&#36739;&#23567;&#30340;&#26679;&#26412;&#24212;&#26356;&#24378;&#28872;&#22320;&#25512;&#21160;&#35757;&#32451;&#65292;&#20855;&#26377;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36895;&#24230;&#12290;&#36229;&#20284;&#26354;&#19981;&#30830;&#23450;&#24615;&#26159;&#37319;&#29992;&#30340;&#36229;&#20284;&#26354;&#31070;&#32463;&#32593;&#32476;&#30340;&#21103;&#20135;&#21697;&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#25104;&#29087;&#65292;&#19982;&#39069;&#22806;&#25104;&#26412;&#30456;&#27604;&#65292;&#27809;&#26377;&#39069;&#22806;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-paced learning has been beneficial for tasks where some initial knowledge is available, such as weakly supervised learning and domain adaptation, to select and order the training sample sequence, from easy to complex. However its applicability remains unexplored in unsupervised learning, whereby the knowledge of the task matures during training. We propose a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations. HYSP adopts self-supervision: it uses data augmentations to generate two views of the same sample, and it learns by matching one (named online) to the other (the target). We propose to use hyperbolic uncertainty to determine the algorithmic learning pace, under the assumption that less uncertain samples should be more strongly driving the training, with a larger weight and pace. Hyperbolic uncertainty is a by-product of the adopted hyperbolic neural networks, it matures during training and it comes with no extra cost, compared to the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#31579;&#36873;&#65292;&#20165;&#20351;&#29992;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.06241</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#38656;&#35201;&#20351;&#29992;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do we need entire training data for adversarial training?. (arXiv:2303.06241v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#31579;&#36873;&#65292;&#20165;&#20351;&#29992;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new adversarial training method that reduces training time by selecting only the adversarially-prone samples from the training dataset.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#29992;&#20110;&#35299;&#20915;&#35768;&#22810;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#21307;&#23398;&#22270;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;DNN&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#24050;&#32463;&#34987;&#24191;&#27867;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#26041;&#27861;&#37117;&#20250;&#20026;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20165;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#23569;&#20219;&#20309;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#36873;&#25321;&#23376;&#38598;&#65292;&#25105;&#20204;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#36807;&#28388;&#20986;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#23545;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25191;&#34892;&#31616;&#21333;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#20197;&#36807;&#28388;&#20986;&#36825;&#20010;&#23376;&#38598;&#12290;&#22312;&#36825;&#20010;&#25915;&#20987;&#20013;&#65292;&#25105;&#20204;&#21521;&#27599;&#20010;&#20687;&#32032;&#28155;&#21152;&#19968;&#20010;&#23567;&#25200;&#21160;&#21644;&#20960;&#26465;&#32593;&#26684;&#32447;&#21040;&#36755;&#20837;&#22270;&#20687;&#20013;&#12290;&#25105;&#20204;&#23545;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#23376;&#38598;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#24182;&#19988;...
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are being used to solve a wide range of problems in many domains including safety-critical domains like self-driving cars and medical imagery. DNNs suffer from vulnerability against adversarial attacks. In the past few years, numerous approaches have been proposed to tackle this problem by training networks using adversarial training. Almost all the approaches generate adversarial examples for the entire training dataset, thus increasing the training time drastically. We show that we can decrease the training time for any adversarial training algorithm by using only a subset of training data for adversarial training. To select the subset, we filter the adversarially-prone samples from the training data. We perform a simple adversarial attack on all training examples to filter this subset. In this attack, we add a small perturbation to each pixel and a few grid lines to the input image.  We perform adversarial training on the adversarially-prone subset and mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34917;&#20805;&#31232;&#30095;&#21270;&#30340;&#27169;&#22411;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#32852;&#37030;&#23398;&#20064;&#20013;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.06237</link><description>&lt;p&gt;
&#20302;&#24320;&#38144;&#27169;&#22411;&#21098;&#26525;&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#34917;&#20805;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Complement Sparsification: Low-Overhead Model Pruning for Federated Learning. (arXiv:2303.06237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34917;&#20805;&#31232;&#30095;&#21270;&#30340;&#27169;&#22411;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#32852;&#37030;&#23398;&#20064;&#20013;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a model pruning mechanism called Complement Sparsification (CS), which satisfies the requirements of low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy in federated learning through complementary and collaborative pruning done at the server and the clients.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#33539;&#20363;&#65292;&#28041;&#21450;&#22823;&#37327;&#36890;&#20449;&#21644;&#35745;&#31639;&#24037;&#20316;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#27169;&#22411;&#21098;&#26525;/&#31232;&#30095;&#21270;&#24320;&#21457;&#20102;&#21487;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#31232;&#30095;&#27169;&#22411;&#65292;&#20294;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#35299;&#20915;&#26041;&#26696;&#19981;&#33021;&#21516;&#26102;&#28385;&#36275;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#20302;&#21452;&#21521;&#36890;&#20449;&#24320;&#38144;&#12289;&#23458;&#25143;&#31471;&#20302;&#35745;&#31639;&#24320;&#38144;&#21644;&#33391;&#22909;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#35201;&#27714;&#65292;&#22312;FL&#20551;&#35774;&#19979;&#65292;&#26381;&#21153;&#22120;&#26080;&#27861;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#20197;&#24494;&#35843;&#20462;&#21098;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#34917;&#20805;&#31232;&#30095;&#21270;&#65288;CS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21098;&#26525;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#20114;&#34917;&#21644;&#21327;&#20316;&#30340;&#21098;&#26525;&#26469;&#28385;&#36275;&#25152;&#26377;&#36825;&#20123;&#35201;&#27714;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;CS&#21019;&#24314;&#19968;&#20010;&#20840;&#23616;&#31232;&#30095;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#25429;&#33719;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#19968;&#33324;&#25968;&#25454;&#20998;&#24067;&#30340;&#26435;&#37325;&#65292;&#32780;&#23458;&#25143;&#31471;&#21017;&#21019;&#24314;&#26412;&#22320;&#31232;&#30095;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-preserving distributed deep learning paradigm that involves substantial communication and computation effort, which is a problem for resource-constrained mobile and IoT devices. Model pruning/sparsification develops sparse models that could solve this problem, but existing sparsification solutions cannot satisfy at the same time the requirements for low bidirectional communication overhead between the server and the clients, low computation overhead at the clients, and good model accuracy, under the FL assumption that the server does not have access to raw data to fine-tune the pruned models. We propose Complement Sparsification (CS), a pruning mechanism that satisfies all these requirements through a complementary and collaborative pruning done at the server and the clients. At each round, CS creates a global sparse model that contains the weights that capture the general data distribution of all clients, while the clients create local sparse model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24352;&#37327;&#29615;&#22240;&#23376;&#20998;&#35299;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#24674;&#22797;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20462;&#22797;&#21644;&#21435;&#22122;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06235</link><description>&lt;p&gt;
&#24102;&#24352;&#37327;&#33258;&#32534;&#30721;&#22120;&#30340;&#21387;&#32553;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Compressive Sensing with Tensorized Autoencoder. (arXiv:2303.06235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24352;&#37327;&#29615;&#22240;&#23376;&#20998;&#35299;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26469;&#24674;&#22797;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20462;&#22797;&#21644;&#21435;&#22122;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for image recovery using an autoencoder with tensor ring factorization, which achieves better reconstruction quality in inpainting and denoising applications.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#21487;&#20197;&#35757;&#32451;&#25104;&#23558;&#22270;&#20687;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#24037;&#20855;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#38598;&#21512;&#20013;&#30340;&#19981;&#21516;&#22270;&#20687;&#26159;&#24444;&#27492;&#30340;&#20851;&#33410;&#29256;&#26412;&#65307;&#20363;&#22914;&#65292;&#21516;&#19968;&#29289;&#20307;&#20855;&#26377;&#19981;&#21516;&#30340;&#29031;&#26126;&#12289;&#32972;&#26223;&#25110;&#23039;&#21183;&#12290;&#27492;&#22806;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#22270;&#20687;&#30340;&#26576;&#20123;&#37096;&#20998;&#21487;&#33021;&#20250;&#21463;&#21040;&#22122;&#22768;&#25110;&#32570;&#22833;&#26465;&#30446;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#25968;&#25454;&#30340;&#32467;&#26500;&#20808;&#39564;&#26469;&#24674;&#22797;&#22270;&#20687;&#65292;&#32780;&#27809;&#26377;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#65288;&#24178;&#20928;&#65289;&#22270;&#20687;&#12290;&#36825;&#26679;&#30340;&#24674;&#22797;&#38382;&#39064;&#23646;&#20110;&#21387;&#32553;&#24863;&#30693;&#39046;&#22495;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#23884;&#20837;&#31354;&#38388;&#19978;&#23398;&#20064;&#24102;&#26377;&#24352;&#37327;&#29615;&#22240;&#23376;&#20998;&#35299;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20197;&#23545;&#25968;&#25454;&#26045;&#21152;&#32467;&#26500;&#32422;&#26463;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#29942;&#39048;&#23618;&#20013;&#20351;&#29992;&#24352;&#37327;&#29615;&#32467;&#26500;&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#30340;&#36719;&#26631;&#31614;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20462;&#22797;&#21644;&#21435;&#22122;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25152;&#24471;&#21040;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep networks can be trained to map images into a low-dimensional latent space. In many cases, different images in a collection are articulated versions of one another; for example, same object with different lighting, background, or pose. Furthermore, in many cases, parts of images can be corrupted by noise or missing entries. In this paper, our goal is to recover images without access to the ground-truth (clean) images using the articulations as structural prior of the data. Such recovery problems fall under the domain of compressive sensing. We propose to learn autoencoder with tensor ring factorization on the the embedding space to impose structural constraints on the data. In particular, we use a tensor ring structure in the bottleneck layer of the autoencoder that utilizes the soft labels of the structured dataset. We empirically demonstrate the effectiveness of the proposed approach for inpainting and denoising applications. The resulting method achieves better reconstruction qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#20272;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#22312;&#28201;&#21644;&#30340;&#37319;&#26679;&#26465;&#20214;&#19979;&#23454;&#29616;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#22312;top-$K$&#24674;&#22797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#20855;&#26377;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06234</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#24212;&#25968;&#25454;&#20013;&#36827;&#34892;&#26368;&#20248;&#21644;&#31169;&#23494;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal and Private Learning from Human Response Data. (arXiv:2303.06234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#20272;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#29305;&#28857;&#65292;&#21487;&#20197;&#22312;&#28201;&#21644;&#30340;&#37319;&#26679;&#26465;&#20214;&#19979;&#23454;&#29616;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#30028;&#38480;&#65292;&#21516;&#26102;&#22312;top-$K$&#24674;&#22797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#20855;&#26377;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new spectral estimation algorithm that is efficient and accurate, achieving the minimax optimal error bound (modulo a log factor) under mild sampling conditions, and enjoys optimal sample complexity for top-$K$ recovery.
&lt;/p&gt;
&lt;p&gt;
&#39033;&#30446;&#21453;&#24212;&#29702;&#35770;&#65288;IRT&#65289;&#26159;&#30740;&#31350;&#20154;&#20204;&#22914;&#20309;&#20570;&#20986;&#27010;&#29575;&#20915;&#31574;&#30340;&#23398;&#31185;&#65292;&#20855;&#26377;&#25945;&#32946;&#27979;&#35797;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;&#20108;&#20803;&#21453;&#24212;&#25968;&#25454;&#30340;Rasch&#27169;&#22411;&#26159;IRT&#20013;&#26368;&#22522;&#26412;&#30340;&#27169;&#22411;&#20043;&#19968;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#37325;&#35201;&#23454;&#38469;&#24847;&#20041;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;Nguyen&#21644;Zhang&#65288;2022&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35889;&#20272;&#35745;&#31639;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#29305;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#20004;&#31181;&#37325;&#35201;&#30340;&#26041;&#24335;&#25193;&#23637;&#20102;&#20182;&#20204;&#30340;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#35889;&#31639;&#27861;&#30340;&#31934;&#32454;&#36880;&#39033;&#35823;&#24046;&#30028;&#38480;&#65292;&#34917;&#20805;&#20102;&#20182;&#20204;&#24037;&#20316;&#20013;&#30340;&#8220;&#24179;&#22343;&#35823;&#24046;&#8221;$\ell_2$&#30028;&#38480;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#28201;&#21644;&#30340;&#37319;&#26679;&#26465;&#20214;&#19979;&#65292;&#35889;&#31639;&#27861;&#23454;&#29616;&#20102;&#26497;&#23567;&#21270;&#26368;&#20248;&#35823;&#24046;&#30028;&#38480;&#65288;&#27169;&#38500;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#22312;&#31934;&#32454;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35889;&#31639;&#27861;&#22312;top-$K$&#24674;&#22797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#20855;&#26377;&#26368;&#20248;&#24615;&#65288;&#20363;&#22914;&#65292;&#20174;&#25209;&#20934;/&#19981;&#25209;&#20934;&#21453;&#24212;&#25968;&#25454;&#20013;&#35782;&#21035;&#26368;&#20339;&#30340;$K$&#20010;&#39033;&#30446;&#65289;&#65292;&#35299;&#37322;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Item response theory (IRT) is the study of how people make probabilistic decisions, with diverse applications in education testing, recommendation systems, among others. The Rasch model of binary response data, one of the most fundamental models in IRT, remains an active area of research with important practical significance. Recently, Nguyen and Zhang (2022) proposed a new spectral estimation algorithm that is efficient and accurate. In this work, we extend their results in two important ways. Firstly, we obtain a refined entrywise error bound for the spectral algorithm, complementing the `average error' $\ell_2$ bound in their work. Notably, under mild sampling conditions, the spectral algorithm achieves the minimax optimal error bound (modulo a log factor). Building on the refined analysis, we also show that the spectral algorithm enjoys optimal sample complexity for top-$K$ recovery (e.g., identifying the best $K$ items from approval/disapproval response data), explaining the empir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#22810;&#31867;OOD&#26816;&#27979;&#22120;&#65292;&#35813;&#26816;&#27979;&#22120;&#22312;&#38647;&#36798;&#36317;&#31163;&#22810;&#26222;&#21202;&#22270;&#20687;&#65288;RDIs&#65289;&#19978;&#36816;&#34892;&#12290;&#26816;&#27979;&#22120;&#26088;&#22312;&#23558;&#38500;&#22352;&#12289;&#31449;&#25110;&#36208;&#30340;&#20154;&#20197;&#22806;&#30340;&#20219;&#20309;&#31227;&#21160;&#29289;&#20307;&#20998;&#31867;&#20026;OOD&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#26816;&#27979;&#21628;&#21560;&#31561;&#24494;&#23567;&#30340;&#20154;&#20307;&#36816;&#21160;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.06232</link><description>&lt;p&gt;
MCROOD: &#22810;&#31867;&#38647;&#36798;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MCROOD: Multi-Class Radar Out-Of-Distribution Detection. (arXiv:2303.06232v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#22810;&#31867;OOD&#26816;&#27979;&#22120;&#65292;&#35813;&#26816;&#27979;&#22120;&#22312;&#38647;&#36798;&#36317;&#31163;&#22810;&#26222;&#21202;&#22270;&#20687;&#65288;RDIs&#65289;&#19978;&#36816;&#34892;&#12290;&#26816;&#27979;&#22120;&#26088;&#22312;&#23558;&#38500;&#22352;&#12289;&#31449;&#25110;&#36208;&#30340;&#20154;&#20197;&#22806;&#30340;&#20219;&#20309;&#31227;&#21160;&#29289;&#20307;&#20998;&#31867;&#20026;OOD&#12290;&#20316;&#32773;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#26816;&#27979;&#21628;&#21560;&#31561;&#24494;&#23567;&#30340;&#20154;&#20307;&#36816;&#21160;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a reconstruction-based multi-class OOD detector that operates on radar range doppler images (RDIs). The detector aims to classify any moving object other than a person sitting, standing, or walking as OOD. The authors also provide a simple yet effective pre-processing technique to detect minor human body movements like breathing. The method outperforms state-of-the-art OOD detection methods in experiments.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#23433;&#20840;&#37096;&#32626;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26550;&#26500;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#21463;&#21040;&#29305;&#21035;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#22810;&#31867;OOD&#26816;&#27979;&#22120;&#65292;&#35813;&#26816;&#27979;&#22120;&#22312;&#38647;&#36798;&#36317;&#31163;&#22810;&#26222;&#21202;&#22270;&#20687;&#65288;RDIs&#65289;&#19978;&#36816;&#34892;&#12290;&#26816;&#27979;&#22120;&#26088;&#22312;&#23558;&#38500;&#22352;&#12289;&#31449;&#25110;&#36208;&#30340;&#20154;&#20197;&#22806;&#30340;&#20219;&#20309;&#31227;&#21160;&#29289;&#20307;&#20998;&#31867;&#20026;OOD&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#20197;&#26816;&#27979;&#21628;&#21560;&#31561;&#24494;&#23567;&#30340;&#20154;&#20307;&#36816;&#21160;&#12290;&#36825;&#20010;&#31616;&#21333;&#30340;&#24819;&#27861;&#34987;&#31216;&#20026;&#21628;&#21560;&#26816;&#27979;&#22120;&#65288;RESPD&#65289;&#65292;&#21487;&#20197;&#20943;&#36731;OOD&#26816;&#27979;&#30340;&#36127;&#25285;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20154;&#22352;&#21644;&#20154;&#31449;&#30340;&#31867;&#21035;&#12290;&#22312;&#25105;&#20204;&#25910;&#38598;&#30340;60GHz&#30701;&#36317;&#31163;FMCW&#38647;&#36798;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#20998;&#21035;&#20026;&#22352;&#12289;&#31449;&#21644;&#36208;&#19977;&#20010;&#31867;&#21035;&#23454;&#29616;&#20102;97.45&#65285;&#12289;92.13&#65285;&#21644;96.58&#65285;&#30340;AUROC&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27969;&#31243;&#27604;&#31532;&#20108;&#22909;&#30340;&#26041;&#27861;&#24555;24&#20493;&#65292;&#24182;&#19988;&#26159;v
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection has recently received special attention due to its critical role in safely deploying modern deep learning (DL) architectures. This work proposes a reconstruction-based multi-class OOD detector that operates on radar range doppler images (RDIs). The detector aims to classify any moving object other than a person sitting, standing, or walking as OOD. We also provide a simple yet effective pre-processing technique to detect minor human body movements like breathing. The simple idea is called respiration detector (RESPD) and eases the OOD detection, especially for human sitting and standing classes. On our dataset collected by 60GHz short-range FMCW Radar, we achieve AUROCs of 97.45%, 92.13%, and 96.58% for sitting, standing, and walking classes, respectively. We perform extensive experiments and show that our method outperforms state-of-the-art (SOTA) OOD detection methods. Also, our pipeline performs 24 times faster than the second-best method and is v
&lt;/p&gt;</description></item><item><title>CHGNN&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#12289;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.06213</link><description>&lt;p&gt;
CHGNN: &#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CHGNN: A Semi-Supervised Contrastive Hypergraph Learning Network. (arXiv:2303.06213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06213
&lt;/p&gt;
&lt;p&gt;
CHGNN&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#36229;&#22270;&#23398;&#20064;&#32593;&#32476;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#21253;&#25324;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#12289;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
CHGNN is a semi-supervised contrastive hypergraph learning network that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. It includes an adaptive hypergraph view generator, an improved hypergraph encoder, and a joint loss function.
&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#21487;&#20197;&#27169;&#25311;&#24212;&#29992;&#31243;&#24207;&#20013;&#21457;&#29616;&#30340;&#25968;&#25454;&#23545;&#35937;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#36229;&#22270;&#23398;&#20064;&#30740;&#31350;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#25193;&#23637;&#21040;&#36229;&#22270;&#65292;&#20294;&#26080;&#27861;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#29305;&#24449;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#26679;&#30340;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;CHGNN&#65292;&#23427;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#20174;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;CHGNN&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#36229;&#22270;&#35270;&#22270;&#29983;&#25104;&#22120;&#65292;&#37319;&#29992;&#33258;&#21160;&#22686;&#24378;&#31574;&#30053;&#65292;&#24182;&#23398;&#20064;&#26368;&#23567;&#20805;&#20998;&#35270;&#22270;&#30340;&#25200;&#21160;&#27010;&#29575;&#20998;&#24067;&#12290;&#20854;&#27425;&#65292;CHGNN&#21253;&#21547;&#19968;&#20010;&#25913;&#36827;&#30340;&#36229;&#22270;&#32534;&#30721;&#22120;&#65292;&#32771;&#34385;&#21040;&#36229;&#36793;&#30340;&#21516;&#36136;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#34701;&#21512;&#20449;&#24687;&#12290;&#31532;&#19977;&#65292;CHGNN&#37197;&#22791;&#20102;&#19968;&#20010;&#32852;&#21512;&#25439;&#22833;&#20989;&#25968;&#65292;&#32467;&#21512;&#20102;&#35270;&#22270;&#29983;&#25104;&#22120;&#30340;&#30456;&#20284;&#24615;&#25439;&#22833;&#12289;&#33410;&#28857;&#20998;&#31867;&#25439;&#22833;&#21644;&#36229;&#36793;&#21516;&#36136;&#24615;&#25439;&#22833;&#65292;&#27880;&#20837;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs can model higher-order relationships among data objects that are found in applications such as social networks and bioinformatics. However, recent studies on hypergraph learning that extend graph convolutional networks to hypergraphs cannot learn effectively from features of unlabeled data. To such learning, we propose a contrastive hypergraph neural network, CHGNN, that exploits self-supervised contrastive learning techniques to learn from labeled and unlabeled data. First, CHGNN includes an adaptive hypergraph view generator that adopts an auto-augmentation strategy and learns a perturbed probability distribution of minimal sufficient views. Second, CHGNN encompasses an improved hypergraph encoder that considers hyperedge homogeneity to fuse information effectively. Third, CHGNN is equipped with a joint loss function that combines a similarity loss for the view generator, a node classification loss, and a hyperedge homogeneity loss to inject supervision signals. It also i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;NN-Search&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36890;&#36807;&#36138;&#24515;&#25628;&#32034;ANN-Graph&#26469;&#35299;&#20915;&#20302;&#32500;&#23494;&#38598;&#21521;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#32467;&#26524;&#25552;&#20379;&#20102;&#19982;&#26500;&#24314;&#36817;&#37051;&#22270;&#26102;&#30340;&#36817;&#20284;&#30456;&#20851;&#30340;&#26435;&#34913;&#30340;&#37327;&#21270;&#65292;&#20026;&#26356;&#22810;&#21487;&#35777;&#26126;&#30340;&#39640;&#25928;&#22522;&#20110;&#22270;&#30340;NN-Search&#31639;&#27861;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;</title><link>http://arxiv.org/abs/2303.06210</link><description>&lt;p&gt;
&#36817;&#20284;&#26368;&#36817;&#37051;&#22270;&#19978;&#26368;&#36817;&#37051;&#25628;&#32034;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Analysis Of Nearest Neighbor Search On Approximate Near Neighbor Graph. (arXiv:2303.06210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;NN-Search&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36890;&#36807;&#36138;&#24515;&#25628;&#32034;ANN-Graph&#26469;&#35299;&#20915;&#20302;&#32500;&#23494;&#38598;&#21521;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#32467;&#26524;&#25552;&#20379;&#20102;&#19982;&#26500;&#24314;&#36817;&#37051;&#22270;&#26102;&#30340;&#36817;&#20284;&#30456;&#20851;&#30340;&#26435;&#34913;&#30340;&#37327;&#21270;&#65292;&#20026;&#26356;&#22810;&#21487;&#35777;&#26126;&#30340;&#39640;&#25928;&#22522;&#20110;&#22270;&#30340;NN-Search&#31639;&#27861;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents theoretical guarantees for solving NN-Search by greedy search on ANN-Graph for low dimensional and dense vectors. The results provide quantification of the trade-offs associated with the approximation while building a near neighbor graph, and open the door for more provable efficient graph-based NN-Search algorithms.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#22312;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32463;&#39564;&#25104;&#21151;&#20419;&#20351;&#38656;&#35201;&#29702;&#35770;&#32467;&#26524;&#26469;&#20445;&#35777;&#36825;&#20123;&#31639;&#27861;&#30340;&#25628;&#32034;&#36136;&#37327;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;&#22270;&#30340;&#26368;&#36817;&#37051;&#25628;&#32034;&#31639;&#27861;&#20013;&#23384;&#22312;&#23454;&#36341;&#19982;&#29702;&#35770;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#24403;&#21069;&#30340;&#29702;&#35770;&#25991;&#29486;&#38598;&#20013;&#20110;&#31934;&#30830;&#26368;&#36817;&#37051;&#22270;&#19978;&#30340;&#36138;&#24515;&#25628;&#32034;&#65292;&#32780;&#20174;&#19994;&#32773;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#22270;&#65288;ANN-Graph&#65289;&#26469;&#20943;&#23569;&#39044;&#22788;&#29702;&#26102;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#35299;&#20915;&#20302;&#32500;&#23494;&#38598;&#21521;&#37327;&#30340;&#36138;&#24515;&#25628;&#32034;ANN-Graph&#30340;NN-Search&#30340;&#29702;&#35770;&#20445;&#35777;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#24231;&#26725;&#26753;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#35745;&#31639;&#20960;&#20309;&#23398;&#20013;&#30340;&#20960;&#20010;&#26032;&#39062;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#19982;&#26500;&#24314;&#36817;&#37051;&#22270;&#26102;&#30340;&#36817;&#20284;&#30456;&#20851;&#30340;&#26435;&#34913;&#30340;&#37327;&#21270;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#32467;&#26524;&#23558;&#20026;&#26356;&#22810;&#21487;&#35777;&#26126;&#30340;&#39640;&#25928;&#22522;&#20110;&#22270;&#30340;NN-Search&#31639;&#27861;&#25171;&#24320;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based algorithms have demonstrated state-of-the-art performance in the nearest neighbor search (NN-Search) problem. These empirical successes urge the need for theoretical results that guarantee the search quality and efficiency of these algorithms. However, there exists a practice-to-theory gap in the graph-based NN-Search algorithms. Current theoretical literature focuses on greedy search on exact near neighbor graph while practitioners use approximate near neighbor graph (ANN-Graph) to reduce the preprocessing time. This work bridges this gap by presenting the theoretical guarantees of solving NN-Search via greedy search on ANN-Graph for low dimensional and dense vectors. To build this bridge, we leverage several novel tools from computational geometry. Our results provide quantification of the trade-offs associated with the approximation while building a near neighbor graph. We hope our results will open the door for more provable efficient graph-based NN-Search algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#21306;&#20195;&#25968;&#35745;&#31639;&#32622;&#25442;&#31561;&#21464;&#23618;&#36755;&#20986;&#21644;&#26799;&#24230;&#30340;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#22823;&#23567;&#30340;&#32447;&#24615;&#21644;&#20108;&#27425;&#26102;&#38388;&#20869;&#35745;&#31639;&#65292;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2303.06208</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#21306;&#20195;&#25968;&#24555;&#36895;&#35745;&#31639;&#32622;&#25442;&#31561;&#21464;&#23618;
&lt;/p&gt;
&lt;p&gt;
Fast computation of permutation equivariant layers with the partition algebra. (arXiv:2303.06208v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20998;&#21306;&#20195;&#25968;&#35745;&#31639;&#32622;&#25442;&#31561;&#21464;&#23618;&#36755;&#20986;&#21644;&#26799;&#24230;&#30340;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#22823;&#23567;&#30340;&#32447;&#24615;&#21644;&#20108;&#27425;&#26102;&#38388;&#20869;&#35745;&#31639;&#65292;&#26377;&#25928;&#24615;&#24471;&#21040;&#20102;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new algorithm for computing the output and gradient of permutation equivariant linear layers using the partition algebra, which can be computed in time linear and quadratic in the input size, respectively. The effectiveness of the approach is demonstrated on several benchmark datasets.
&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#26080;&#35770;&#26159;&#31561;&#21464;&#36824;&#26159;&#19981;&#21464;&#20110;&#20854;&#36755;&#20837;&#30340;&#25490;&#21015;&#65292;&#37117;&#26159;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#30340;&#26680;&#24515;&#26500;&#24314;&#22359;&#12290;&#20363;&#22914;DeepSets&#30340;&#23618;&#65292;&#20197;&#21450;&#20986;&#29616;&#22312;transformers&#21644;&#19968;&#20123;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27880;&#24847;&#21147;&#22359;&#20013;&#30340;&#32447;&#24615;&#23618;&#12290;&#32622;&#25442;&#31561;&#21464;&#32447;&#24615;&#23618;&#30340;&#31354;&#38388;&#21487;&#20197;&#34987;&#35782;&#21035;&#20026;&#26576;&#20010;&#23545;&#31216;&#32676;&#34920;&#31034;&#30340;&#19981;&#21464;&#23376;&#31354;&#38388;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#24037;&#20316;&#36890;&#36807;&#23637;&#31034;&#19968;&#32452;&#22522;&#30784;&#65292;&#20854;&#21521;&#37327;&#26159;&#26631;&#20934;&#22522;&#30784;&#20803;&#32032;&#22312;&#23545;&#31216;&#32676;&#20316;&#29992;&#19979;&#36712;&#36947;&#30340;&#24635;&#21644;&#65292;&#26469;&#21442;&#25968;&#21270;&#36825;&#20010;&#31354;&#38388;&#12290;&#21442;&#25968;&#21270;&#25171;&#24320;&#20102;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32622;&#25442;&#31561;&#21464;&#32447;&#24615;&#23618;&#26435;&#37325;&#30340;&#21487;&#33021;&#24615;&#12290;&#32622;&#25442;&#31561;&#21464;&#32447;&#24615;&#23618;&#30340;&#31354;&#38388;&#26159;&#20998;&#21306;&#20195;&#25968;&#30340;&#19968;&#33324;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#39318;&#27425;&#21457;&#29616;&#30340;&#23545;&#35937;&#65292;&#19982;&#23545;&#31216;&#32676;&#30340;&#34920;&#31034;&#35770;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#65292;&#32780;&#19978;&#36848;&#22522;&#30784;&#19982;&#20998;&#21306;&#20195;&#25968;&#30340;&#22522;&#30784;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#22312;&#36755;&#20837;&#22823;&#23567;&#30340;&#32447;&#24615;&#26102;&#38388;&#20869;&#35745;&#31639;&#32622;&#25442;&#31561;&#21464;&#32447;&#24615;&#23618;&#30340;&#36755;&#20986;&#65292;&#24182;&#22312;&#36755;&#20837;&#22823;&#23567;&#30340;&#20108;&#27425;&#26102;&#38388;&#20869;&#35745;&#31639;&#25439;&#22833;&#30456;&#23545;&#20110;&#26435;&#37325;&#30340;&#26799;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#31181;&#35745;&#31639;&#20998;&#21306;&#20195;&#25968;&#22312;&#21521;&#37327;&#19978;&#20316;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20998;&#21306;&#21367;&#31215;&#8221;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#21306;&#21367;&#31215;&#21487;&#20197;&#22312;&#36755;&#20837;&#21521;&#37327;&#22823;&#23567;&#30340;&#32447;&#24615;&#26102;&#38388;&#20869;&#35745;&#31639;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#22312;&#36755;&#20837;&#22823;&#23567;&#30340;&#32447;&#24615;&#21644;&#20108;&#27425;&#26102;&#38388;&#20869;&#35745;&#31639;&#32622;&#25442;&#31561;&#21464;&#32447;&#24615;&#23618;&#30340;&#36755;&#20986;&#21644;&#26799;&#24230;&#65292;&#20998;&#21035;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20998;&#21306;&#21367;&#31215;&#26469;&#35745;&#31639;&#26576;&#20123;&#38750;&#32447;&#24615;&#32622;&#25442;&#31561;&#21464;&#23618;&#30340;&#36755;&#20986;&#21644;&#26799;&#24230;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear neural network layers that are either equivariant or invariant to permutations of their inputs form core building blocks of modern deep learning architectures. Examples include the layers of DeepSets, as well as linear layers occurring in attention blocks of transformers and some graph neural networks. The space of permutation equivariant linear layers can be identified as the invariant subspace of a certain symmetric group representation, and recent work parameterized this space by exhibiting a basis whose vectors are sums over orbits of standard basis elements with respect to the symmetric group action. A parameterization opens up the possibility of learning the weights of permutation equivariant linear layers via gradient descent. The space of permutation equivariant linear layers is a generalization of the partition algebra, an object first discovered in statistical physics with deep connections to the representation theory of the symmetric group, and the basis described abo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;TransformerCVN&#65292;&#23558;&#21367;&#31215;&#21644;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;NOvA&#23454;&#39564;&#20013;&#22797;&#26434;&#20107;&#20214;&#20013;&#22810;&#20010;&#31890;&#23376;&#30340;&#32852;&#21512;&#20998;&#31867;&#21644;&#37325;&#24314;&#65292;&#20026;&#20934;&#30830;&#27979;&#37327;&#26631;&#20934;&#27169;&#22411;&#30340;&#20851;&#38190;&#21442;&#25968;&#25552;&#20379;&#20102;&#37325;&#35201;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.06201</link><description>&lt;p&gt;
NOvA&#20013;&#22522;&#20110;&#31232;&#30095;CNN&#21644;Transformer&#30340;&#21487;&#35299;&#37322;&#32852;&#21512;&#20107;&#20214;-&#31890;&#23376;&#37325;&#24314;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Interpretable Joint Event-Particle Reconstruction for Neutrino Physics at NOvA with Sparse CNNs and Transformers. (arXiv:2303.06201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;TransformerCVN&#65292;&#23558;&#21367;&#31215;&#21644;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;NOvA&#23454;&#39564;&#20013;&#22797;&#26434;&#20107;&#20214;&#20013;&#22810;&#20010;&#31890;&#23376;&#30340;&#32852;&#21512;&#20998;&#31867;&#21644;&#37325;&#24314;&#65292;&#20026;&#20934;&#30830;&#27979;&#37327;&#26631;&#20934;&#27169;&#22411;&#30340;&#20851;&#38190;&#21442;&#25968;&#25552;&#20379;&#20102;&#37325;&#35201;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a novel neural network architecture, TransformerCVN, which combines convolution and attention to achieve joint classification and reconstruction of multiple particles in complex events in the NOvA experiment, providing important support for accurately measuring key parameters of the standard model.
&lt;/p&gt;
&lt;p&gt;
NOvA&#38271;&#22522;&#32447;&#20013;&#24494;&#23376;&#25391;&#33633;&#23454;&#39564;&#35266;&#27979;&#21040;&#30340;&#22797;&#26434;&#20107;&#20214;&#21253;&#21547;&#20102;&#29702;&#35299;&#26631;&#20934;&#27169;&#22411;&#20013;&#26368;&#38590;&#20197;&#25417;&#25720;&#30340;&#31890;&#23376;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;NOvA&#25506;&#27979;&#22120;&#35266;&#27979;&#21040;&#26469;&#33258;Fermilab NuMI&#26463;&#27969;&#30340;&#20013;&#24494;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;&#23558;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#20013;&#20135;&#29983;&#30340;&#31890;&#23376;&#19982;&#23427;&#20204;&#30340;&#28304;&#31890;&#23376;&#20851;&#32852;&#36215;&#26469;&#65292;&#21363;&#37325;&#24314;&#65292;&#23545;&#20110;&#20934;&#30830;&#27979;&#37327;&#26631;&#20934;&#27169;&#22411;&#30340;&#20851;&#38190;&#21442;&#25968;&#33267;&#20851;&#37325;&#35201;&#12290;&#20107;&#20214;&#21487;&#33021;&#21253;&#21547;&#22810;&#20010;&#31890;&#23376;&#65292;&#27599;&#20010;&#31890;&#23376;&#20135;&#29983;&#31232;&#30095;&#30340;&#39640;&#32500;&#31354;&#38388;&#35266;&#27979;&#25968;&#25454;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#35780;&#20272;&#21333;&#20010;&#31890;&#23376;&#12290;&#20026;&#20102;&#20934;&#30830;&#26631;&#35760;&#36825;&#20123;&#20247;&#22810;&#30340;&#39640;&#32500;&#35266;&#27979;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#23558;&#21367;&#31215;&#25152;&#23454;&#29616;&#30340;&#31354;&#38388;&#23398;&#20064;&#19982;&#27880;&#24847;&#21147;&#25152;&#23454;&#29616;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#32852;&#21512;&#26041;&#27861;&#65292;TransformerCVN&#65292;&#21516;&#26102;&#23545;&#27599;&#20010;&#20107;&#20214;&#36827;&#34892;&#20998;&#31867;&#21644;&#37325;&#24314;&#27599;&#20010;&#21333;&#29420;&#31890;&#23376;&#30340;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
The complex events observed at the NOvA long-baseline neutrino oscillation experiment contain vital information for understanding the most elusive particles in the standard model. The NOvA detectors observe interactions of neutrinos from the NuMI beam at Fermilab. Associating the particles produced in these interaction events to their source particles, a process known as reconstruction, is critical for accurately measuring key parameters of the standard model. Events may contain several particles, each producing sparse high-dimensional spatial observations, and current methods are limited to evaluating individual particles. To accurately label these numerous, high-dimensional observations, we present a novel neural network architecture that combines the spatial learning enabled by convolutions with the contextual learning enabled by attention. This joint approach, TransformerCVN, simultaneously classifies each event and reconstructs every individual particle's identity. TransformerCVN 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#32553;&#20943;&#24322;&#26041;&#24046;PCA&#65292;&#23427;&#22312;&#20811;&#26381;&#30149;&#24577;&#38382;&#39064;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#21644;&#26080;&#26465;&#20214;&#25968;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.06198</link><description>&lt;p&gt;
&#20811;&#26381;&#24322;&#26041;&#24046;PCA&#20013;&#30149;&#24577;&#38382;&#39064;&#30340;&#32553;&#20943;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deflated HeteroPCA: Overcoming the curse of ill-conditioning in heteroskedastic PCA. (arXiv:2303.06198v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#32553;&#20943;&#24322;&#26041;&#24046;PCA&#65292;&#23427;&#22312;&#20811;&#26381;&#30149;&#24577;&#38382;&#39064;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#21644;&#26080;&#26465;&#20214;&#25968;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel algorithm, called Deflated-HeteroPCA, that overcomes the curse of ill-conditioning in heteroskedastic PCA while achieving near-optimal and condition-number-free theoretical guarantees.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#20174;&#21463;&#27745;&#26579;&#30340;&#25968;&#25454;&#20013;&#20272;&#35745;&#20302;&#31209;&#30697;&#38453;X*&#30340;&#21015;&#23376;&#31354;&#38388;&#12290;&#24403;&#23384;&#22312;&#24322;&#26041;&#24046;&#22122;&#22768;&#21644;&#19981;&#24179;&#34913;&#30340;&#32500;&#24230;&#65288;&#21363;n2 &gt;&gt; n1&#65289;&#26102;&#65292;&#22914;&#20309;&#22312;&#23481;&#32435;&#26368;&#24191;&#27867;&#30340;&#20449;&#22122;&#27604;&#33539;&#22260;&#30340;&#21516;&#26102;&#33719;&#24471;&#26368;&#20339;&#30340;&#32479;&#35745;&#31934;&#24230;&#21464;&#24471;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;HeteroPCA&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24378;&#26377;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#36973;&#21463;&#20102;&#8220;&#30149;&#24577;&#38382;&#39064;&#30340;&#35781;&#21650;&#8221;&#65292;&#21363;&#38543;&#30528;X*&#30340;&#26465;&#20214;&#25968;&#22686;&#38271;&#65292;&#20854;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#32780;&#19981;&#24433;&#21709;&#20801;&#35768;&#30340;&#20449;&#22122;&#27604;&#33539;&#22260;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#32553;&#20943;&#24322;&#26041;&#24046;PCA&#65292;&#23427;&#22312;$\ell_2$&#21644;$\ell_{2,\infty}$&#32479;&#35745;&#31934;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#36817;&#20046;&#26368;&#20248;&#21644;&#26080;&#26465;&#20214;&#25968;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#23558;&#35889;&#20998;&#25104;&#20004;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with estimating the column subspace of a low-rank matrix $\boldsymbol{X}^\star \in \mathbb{R}^{n_1\times n_2}$ from contaminated data. How to obtain optimal statistical accuracy while accommodating the widest range of signal-to-noise ratios (SNRs) becomes particularly challenging in the presence of heteroskedastic noise and unbalanced dimensionality (i.e., $n_2\gg n_1$). While the state-of-the-art algorithm $\textsf{HeteroPCA}$ emerges as a powerful solution for solving this problem, it suffers from "the curse of ill-conditioning," namely, its performance degrades as the condition number of $\boldsymbol{X}^\star$ grows. In order to overcome this critical issue without compromising the range of allowable SNRs, we propose a novel algorithm, called $\textsf{Deflated-HeteroPCA}$, that achieves near-optimal and condition-number-free theoretical guarantees in terms of both $\ell_2$ and $\ell_{2,\infty}$ statistical accuracy. The proposed algorithm divides the spectrum
&lt;/p&gt;</description></item><item><title>Papaya&#26159;&#19968;&#31181;&#28857;&#23545;&#28857;&#23398;&#20064;&#31995;&#32479;&#65292;&#33410;&#28857;&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23450;&#26399;&#26681;&#25454;&#23398;&#20064;&#30340;&#20449;&#20219;&#30697;&#38453;&#23558;&#20854;&#21442;&#25968;&#19982;&#21516;&#20276;&#30340;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#20174;&#32780;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#65292;&#36991;&#20813;&#20102;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#30340;&#24102;&#23485;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06189</link><description>&lt;p&gt;
Papaya&#65306;&#32852;&#37030;&#23398;&#20064;&#65292;&#20294;&#23436;&#20840;&#21435;&#20013;&#24515;&#21270;
&lt;/p&gt;
&lt;p&gt;
Papaya: Federated Learning, but Fully Decentralized. (arXiv:2303.06189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06189
&lt;/p&gt;
&lt;p&gt;
Papaya&#26159;&#19968;&#31181;&#28857;&#23545;&#28857;&#23398;&#20064;&#31995;&#32479;&#65292;&#33410;&#28857;&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23450;&#26399;&#26681;&#25454;&#23398;&#20064;&#30340;&#20449;&#20219;&#30697;&#38453;&#23558;&#20854;&#21442;&#25968;&#19982;&#21516;&#20276;&#30340;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#65292;&#20174;&#32780;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#30340;&#21435;&#20013;&#24515;&#21270;&#65292;&#36991;&#20813;&#20102;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#30340;&#24102;&#23485;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Papaya is a peer-to-peer learning system that allows nodes to train on their own data and periodically perform a weighted average of their parameters with that of their peers according to a learned trust matrix, achieving decentralized federated learning and avoiding the bandwidth and resource-heavy constraint and privacy concerns of a centralized server.
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20351;&#29992;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#26469;&#32858;&#21512;&#27169;&#22411;&#26356;&#26032;&#12290;&#36825;&#26159;&#19968;&#31181;&#24102;&#23485;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#38480;&#21046;&#65292;&#24182;&#26292;&#38706;&#31995;&#32479;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#28857;&#23545;&#28857;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#33410;&#28857;&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23450;&#26399;&#26681;&#25454;&#23398;&#20064;&#30340;&#20449;&#20219;&#30697;&#38453;&#23558;&#20854;&#21442;&#25968;&#19982;&#21516;&#20276;&#30340;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#25105;&#20204;&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#23458;&#25143;&#31471;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#34394;&#25311;&#33410;&#28857;&#22312;&#21516;&#19968;&#21488;&#35745;&#31639;&#26426;&#19978;&#36816;&#34892;&#23454;&#39564;&#26469;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#25105;&#20204;&#25552;&#26696;&#30340;&#31532;&#19968;&#36718;&#20013;&#25152;&#36848;&#30340;&#31574;&#30053;&#26469;&#35777;&#26126;&#20849;&#20139;&#21442;&#25968;&#30340;&#28857;&#23545;&#28857;&#23398;&#20064;&#27010;&#24565;&#12290;&#25105;&#20204;&#29616;&#22312;&#24076;&#26395;&#36816;&#34892;&#26356;&#22810;&#23454;&#39564;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#26356;&#21487;&#37096;&#32626;&#30340;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning systems use a centralized server to aggregate model updates. This is a bandwidth and resource-heavy constraint and exposes the system to privacy concerns. We instead implement a peer to peer learning system in which nodes train on their own data and periodically perform a weighted average of their parameters with that of their peers according to a learned trust matrix. So far, we have created a model client framework and have been using this to run experiments on the proposed system using multiple virtual nodes which in reality exist on the same computer. We used this strategy as stated in Iteration 1 of our proposal to prove the concept of peer to peer learning with shared parameters. We now hope to run more experiments and build a more deployable real world system for the same.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#65292;&#21253;&#25324;&#21160;&#24577;&#38376;&#25511;&#12289;&#19987;&#23478;&#32531;&#20914;&#21644;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25191;&#34892;&#26102;&#38388;&#21644;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.06182</link><description>&lt;p&gt;
&#36808;&#21521;MoE&#37096;&#32626;&#65306;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#25512;&#29702;&#20013;&#30340;&#20302;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference. (arXiv:2303.06182v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#65292;&#21253;&#25324;&#21160;&#24577;&#38376;&#25511;&#12289;&#19987;&#23478;&#32531;&#20914;&#21644;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25191;&#34892;&#26102;&#38388;&#21644;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes three optimization techniques to mitigate inefficiencies in Mixture-of-Experts (MoE) models during inference, including dynamic gating, expert buffering, and expert load balancing. These techniques can significantly improve execution time and reduce memory usage.
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#26368;&#36817;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24191;&#27867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#26377;&#25928;&#22320;&#25193;&#23637;&#20102;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#22686;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#23567;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#37096;&#32626;&#36825;&#26679;&#30340;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;MoE&#24037;&#20316;&#36127;&#36733;&#30340;&#29305;&#24449;&#21270;&#65292;&#21363;&#35821;&#35328;&#24314;&#27169;&#65288;LM&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#22312;&#37096;&#32626;&#26102;&#30340;&#20302;&#25928;&#29575;&#26469;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#20302;&#25928;&#29575;&#30340;&#26469;&#28304;&#65292;&#21363;&#65288;1&#65289;&#21160;&#24577;&#38376;&#25511;&#65292;&#65288;2&#65289;&#19987;&#23478;&#32531;&#20914;&#21644;&#65288;3&#65289;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21160;&#24577;&#38376;&#25511;&#21487;&#20197;&#20351;LM&#30340;&#25191;&#34892;&#26102;&#38388;&#25552;&#39640;1.25-4&#20493;&#65292;MT&#32534;&#30721;&#22120;&#25552;&#39640;2-5&#20493;&#65292;MT&#35299;&#30721;&#22120;&#25552;&#39640;1.09-1.5&#20493;&#12290;&#23427;&#36824;&#21487;&#20197;&#23558;LM&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;1.36&#20493;&#65292;MT&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;1.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Experts (MoE) models have recently gained steam in achieving the state-of-the-art performance in a wide range of tasks in computer vision and natural language processing. They effectively expand the model capacity while incurring a minimal increase in computation cost during training. However, deploying such models for inference is difficult due to their large model size and complex communication pattern. In this work, we provide a characterization of two MoE workloads, namely Language Modeling (LM) and Machine Translation (MT) and identify their sources of inefficiencies at deployment.  We propose three optimization techniques to mitigate sources of inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert load balancing. We show that dynamic gating improves execution time by 1.25-4$\times$ for LM, 2-5$\times$ for MT Encoder and 1.09-1.5$\times$ for MT Decoder. It also reduces memory usage by up to 1.36$\times$ for LM and up to 1.1$\times$ for MT. We f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedFBN&#65292;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#20316;&#20026;&#27169;&#22411;&#21518;&#31471;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20923;&#32467;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#20248;&#21270;&#20998;&#24067;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.06180</link><description>&lt;p&gt;
&#38024;&#23545;&#20998;&#24067;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#65292;&#20248;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimizing Federated Learning for Medical Image Classification on Distributed Non-iid Datasets with Partial Labels. (arXiv:2303.06180v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedFBN&#65292;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#20316;&#20026;&#27169;&#22411;&#21518;&#31471;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20923;&#32467;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#65292;&#20197;&#20248;&#21270;&#20998;&#24067;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#37096;&#20998;&#26631;&#31614;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes FedFBN, a federated learning framework that uses pretrained networks as the model backend and freezes the batch normalization layers throughout the training process to optimize medical image classification on distributed non-iid datasets with partial labels.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#33016;&#37096;X&#20809;&#25968;&#25454;&#38598;&#24050;&#32463;&#24102;&#22836;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#26816;&#27979;&#21487;&#33021;&#23384;&#22312;&#30340;&#19968;&#37096;&#20998;&#30142;&#30149;&#26631;&#31614;&#65292;&#22240;&#27492;&#20351;&#23427;&#20204;&#25104;&#20026;&#20998;&#24067;&#24335;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#37096;&#20998;&#26631;&#31614;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#25351;&#20986;&#65292;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#23545;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#20855;&#26377;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#20855;&#26377;&#37096;&#20998;&#26631;&#31614;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30456;&#20851;&#30340;&#22495;&#28418;&#31227;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedFBN&#65292;&#36825;&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20174;&#36801;&#31227;&#23398;&#20064;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32593;&#32476;&#20316;&#20026;&#27169;&#22411;&#21518;&#31471;&#65292;&#24182;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20923;&#32467;&#25209;&#37327;&#24402;&#19968;&#21270;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;iid&#29609;&#20855;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#38750;iid&#25968;&#25454;&#38598;&#35780;&#20272;FedFBN&#19982;&#24403;&#21069;FL&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;FedFBN&#20248;&#20110;&#20351;&#29992;&#20998;&#24067;&#24335;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#30340;&#24403;&#21069;&#32858;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous large-scale chest x-ray datasets have spearheaded expert-level detection of abnormalities using deep learning. However, these datasets focus on detecting a subset of disease labels that could be present, thus making them distributed and non-iid with partial labels. Recent literature has indicated the impact of batch normalization layers on the convergence of federated learning due to domain shift associated with non-iid data with partial labels. To that end, we propose FedFBN, a federated learning framework that draws inspiration from transfer learning by using pretrained networks as the model backend and freezing the batch normalization layers throughout the training process. We evaluate FedFBN with current FL strategies using synthetic iid toy datasets and large-scale non-iid datasets across scenarios with partial and complete labels. Our results demonstrate that FedFBN outperforms current aggregation strategies for training global models using distributed and non-iid data w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#38598;&#29983;&#25104;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24120;&#35265;&#28431;&#27934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#24179;&#22343;&#21484;&#22238;&#29575;&#20026;72&#65285;&#26816;&#27979;C&#21644;Java&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2303.06177</link><description>&lt;p&gt;
&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#36719;&#20214;&#28431;&#27934;&#39044;&#27979;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Software Vulnerability Prediction Knowledge Transferring Between Programming Languages. (arXiv:2303.06177v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#38598;&#29983;&#25104;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24120;&#35265;&#28431;&#27934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#24179;&#22343;&#21484;&#22238;&#29575;&#20026;72&#65285;&#26816;&#27979;C&#21644;Java&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a transfer learning technique to detect common vulnerabilities in different programming languages by leveraging available datasets. The results show that the proposed model detects vulnerabilities in both C and Java codes with an average recall of 72%.
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#30340;&#36719;&#20214;&#28431;&#27934;&#26816;&#27979;&#27169;&#22411;&#19968;&#30452;&#21463;&#21040;&#30740;&#31350;&#21644;&#24320;&#21457;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#39046;&#22495;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#25152;&#26377;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#30340;&#20195;&#30721;&#26679;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#21487;&#29992;&#25968;&#25454;&#38598;&#29983;&#25104;&#19968;&#20010;&#27169;&#22411;&#65292;&#20197;&#26816;&#27979;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#24120;&#35265;&#28431;&#27934;&#12290;&#25105;&#20204;&#20351;&#29992;C&#28304;&#20195;&#30721;&#26679;&#26412;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;Java&#28304;&#20195;&#30721;&#26679;&#26412;&#26469;&#37319;&#29992;&#21644;&#35780;&#20272;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20195;&#30721;&#26679;&#26412;&#65306;NIST&#36719;&#20214;&#20445;&#38556;&#21442;&#32771;&#25968;&#25454;&#38598;&#65288;SARD&#65289;&#21644;Draper VDISC&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#24179;&#22343;&#21484;&#22238;&#29575;&#20026;72&#65285;&#26816;&#27979;C&#21644;Java&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;AI&#26469;&#35843;&#26597;&#27599;&#20010;&#29305;&#24449;&#23545;&#30693;&#35782;&#36716;&#31227;&#26426;&#21046;&#30340;&#36129;&#29486;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing automated and smart software vulnerability detection models has been receiving great attention from both research and development communities. One of the biggest challenges in this area is the lack of code samples for all different programming languages. In this study, we address this issue by proposing a transfer learning technique to leverage available datasets and generate a model to detect common vulnerabilities in different programming languages. We use C source code samples to train a Convolutional Neural Network (CNN) model, then, we use Java source code samples to adopt and evaluate the learned model. We use code samples from two benchmark datasets: NIST Software Assurance Reference Dataset (SARD) and Draper VDISC dataset. The results show that proposed model detects vulnerabilities in both C and Java codes with average recall of 72\%. Additionally, we employ explainable AI to investigate how much each feature contributes to the knowledge transfer mechanisms between 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#23398;&#20064;&#36895;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;&#29702;&#35299;Grokking&#21644;&#21452;&#37325;&#19979;&#38477;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#27169;&#22411;&#26234;&#33021;Grokking&#30340;&#31532;&#19968;&#20010;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.06173</link><description>&lt;p&gt;
&#32479;&#19968;&#29702;&#35299;Grokking&#21644;&#21452;&#37325;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Unifying Grokking and Double Descent. (arXiv:2303.06173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#24335;&#23398;&#20064;&#36895;&#24230;&#26694;&#26550;&#65292;&#29992;&#20110;&#32479;&#19968;&#29702;&#35299;Grokking&#21644;&#21452;&#37325;&#19979;&#38477;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#27169;&#22411;&#26234;&#33021;Grokking&#30340;&#31532;&#19968;&#20010;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework of pattern learning speeds to unify the understanding of Grokking and double descent, and provides the first demonstration of model-wise Grokking.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#23545;&#27867;&#21270;&#30340;&#21407;&#21017;&#24615;&#29702;&#35299;&#21487;&#33021;&#38656;&#35201;&#23558;&#19981;&#21516;&#30340;&#35266;&#23519;&#32467;&#26524;&#32479;&#19968;&#21040;&#19968;&#20010;&#27010;&#24565;&#26694;&#26550;&#19979;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#8220;Grokking&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;&#21160;&#24577;&#65292;&#20854;&#20013;&#25345;&#32493;&#30340;&#36817;&#20046;&#23436;&#32654;&#30340;&#35757;&#32451;&#34920;&#29616;&#21644;&#36817;&#20046;&#20598;&#28982;&#30340;&#27979;&#35797;&#34920;&#29616;&#26368;&#32456;&#20250;&#23548;&#33268;&#27867;&#21270;&#65292;&#20197;&#21450;&#34920;&#38754;&#19978;&#31867;&#20284;&#30340;&#8220;&#21452;&#37325;&#19979;&#38477;&#8221;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36825;&#20123;&#20027;&#39064;&#24050;&#32463;&#34987;&#23396;&#31435;&#22320;&#30740;&#31350;&#12290;&#25105;&#20204;&#20551;&#35774;Grokking&#21644;&#21452;&#37325;&#19979;&#38477;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#27169;&#24335;&#23398;&#20064;&#36895;&#24230;&#26694;&#26550;&#20869;&#30456;&#21516;&#23398;&#20064;&#21160;&#24577;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#24403;&#25913;&#21464;&#27169;&#22411;&#23481;&#37327;&#32780;&#19981;&#26159;&#20248;&#21270;&#27493;&#39588;&#26102;&#65292;&#35813;&#26694;&#26550;&#20063;&#36866;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#22411;&#26234;&#33021;Grokking&#30340;&#31532;&#19968;&#20010;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A principled understanding of generalization in deep learning may require unifying disparate observations under a single conceptual framework. Previous work has studied \emph{grokking}, a training dynamic in which a sustained period of near-perfect training performance and near-chance test performance is eventually followed by generalization, as well as the superficially similar \emph{double descent}. These topics have so far been studied in isolation. We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds. We propose that this framework also applies when varying model capacity instead of optimization steps, and provide the first demonstration of model-wise grokking.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DP-Fast MH&#31639;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20855;&#26377;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.06171</link><description>&lt;p&gt;
DP-Fast MH: &#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#31169;&#26377;&#12289;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;Metropolis-Hastings&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for Large-Scale Bayesian Inference. (arXiv:2303.06171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DP-Fast MH&#31639;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20855;&#26377;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new DP-Fast MH algorithm for large-scale Bayesian inference, which is accurate, fast, and privacy-preserving.
&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#22797;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;&#21644;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#25512;&#29702;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#23427;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#35786;&#26029;&#12289;&#33647;&#29289;&#35774;&#35745;&#21644;&#25919;&#31574;&#21046;&#23450;&#12290;&#22312;&#36825;&#20123;&#24120;&#35265;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#21487;&#33021;&#38750;&#24120;&#25935;&#24863;&#12290;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25552;&#20379;&#20102;&#20855;&#26377;&#24378;&#22823;&#26368;&#22351;&#24773;&#20917;&#38544;&#31169;&#20445;&#35777;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#24182;&#24050;&#21457;&#23637;&#25104;&#20026;&#38544;&#31169;&#20445;&#25252;&#25968;&#25454;&#20998;&#26512;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Metropolis-Hastings&#65288;MH&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#26368;&#22522;&#26412;&#30340;MCMC&#26041;&#27861;&#20043;&#19968;&#65292;&#29992;&#20110;&#24046;&#20998;&#38544;&#31169;&#19979;&#30340;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31169;&#26377;MCMC&#31639;&#27861;&#20026;&#20102;&#33719;&#24471;&#38544;&#31169;&#32780;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#31934;&#30830;&#19988;&#24555;&#36895;&#30340;DP MH&#31639;&#27861;&#65292;&#22823;&#22810;&#25968;&#36845;&#20195;&#20013;&#20165;&#20351;&#29992;&#19968;&#20010;&#23567;&#25209;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#38544;&#31169;&#12289;&#21487;&#25193;&#23637;&#24615;&#65288;&#21363;&#25209;&#37327;&#22823;&#23567;&#65289;&#21644;&#25928;&#29575;&#65288;&#21363;&#25910;&#25947;&#36895;&#24230;&#65289;&#20043;&#38388;&#30340;&#19977;&#37325;&#26435;&#34913;&#65292;&#20174;&#29702;&#35770;&#19978;&#35828;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference provides a principled framework for learning from complex data and reasoning under uncertainty. It has been widely applied in machine learning tasks such as medical diagnosis, drug design, and policymaking. In these common applications, the data can be highly sensitive. Differential privacy (DP) offers data analysis tools with powerful worst-case privacy guarantees and has been developed as the leading approach in privacy-preserving data analysis. In this paper, we study Metropolis-Hastings (MH), one of the most fundamental MCMC methods, for large-scale Bayesian inference under differential privacy. While most existing private MCMC algorithms sacrifice accuracy and efficiency to obtain privacy, we provide the first exact and fast DP MH algorithm, using only a minibatch of data in most iterations. We further reveal, for the first time, a three-way trade-off among privacy, scalability (i.e. the batch size), and efficiency (i.e. the convergence rate), theoretically char
&lt;/p&gt;</description></item><item><title>MOELA&#26159;&#19968;&#20010;&#22810;&#30446;&#26631;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#36827;&#21270;&#25628;&#32034;&#21644;&#23398;&#20064;&#25628;&#32034;&#65292;&#29992;&#20110;&#20248;&#21270;3D NoC&#21551;&#29992;&#30340;&#24322;&#26500;&#22810;&#26680;&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#30446;&#26631;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#65292;MOELA&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#30340;&#26597;&#25214;&#36895;&#24230;&#65292;&#25552;&#39640;Pareto Hypervolume&#65288;PHV&#65289;&#21644;&#33021;&#37327;&#24310;&#36831;&#20056;&#31215;&#65288;EDP&#65289;&#12290;</title><link>http://arxiv.org/abs/2303.06169</link><description>&lt;p&gt;
MOELA&#65306;&#29992;&#20110;3D&#24322;&#26500;&#22810;&#26680;&#24179;&#21488;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;/&#23398;&#20064;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MOELA: A Multi-Objective Evolutionary/Learning Design Space Exploration Framework for 3D Heterogeneous Manycore Platforms. (arXiv:2303.06169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06169
&lt;/p&gt;
&lt;p&gt;
MOELA&#26159;&#19968;&#20010;&#22810;&#30446;&#26631;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#36827;&#21270;&#25628;&#32034;&#21644;&#23398;&#20064;&#25628;&#32034;&#65292;&#29992;&#20110;&#20248;&#21270;3D NoC&#21551;&#29992;&#30340;&#24322;&#26500;&#22810;&#26680;&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#30446;&#26631;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#65292;MOELA&#21487;&#20197;&#25552;&#39640;&#35299;&#20915;&#26041;&#26696;&#30340;&#26597;&#25214;&#36895;&#24230;&#65292;&#25552;&#39640;Pareto Hypervolume&#65288;PHV&#65289;&#21644;&#33021;&#37327;&#24310;&#36831;&#20056;&#31215;&#65288;EDP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
MOELA is a multi-objective design space exploration framework that combines evolutionary-based search with learning-based local search to optimize multiple objectives in 3D NoC enabled heterogeneous manycore systems. Compared to state-of-the-art approaches, MOELA increases the speed of finding solutions, improves Pareto Hypervolume (PHV) and energy-delay-product (EDP).
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#28145;&#24230;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#22788;&#29702;&#31561;&#26032;&#20852;&#24212;&#29992;&#65292;&#38656;&#35201;&#33021;&#22815;&#38598;&#25104;&#22810;&#20010;&#22788;&#29702;&#20803;&#20214;&#65288;PE&#65289;&#30340;3D&#32593;&#32476;&#33455;&#29255;&#65288;NoC&#65289;&#21551;&#29992;&#30340;&#24322;&#26500;&#22810;&#26680;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#35774;&#35745;&#31354;&#38388;&#21644;&#38271;&#26102;&#38388;&#30340;&#35780;&#20272;&#26102;&#38388;&#65292;&#35774;&#35745;&#20855;&#26377;&#22810;&#20010;&#30446;&#26631;&#30340;&#36825;&#31181;&#22797;&#26434;&#31995;&#32479;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20248;&#21270;&#36825;&#26679;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#26694;&#26550;MOELA&#65292;&#23427;&#23558;&#22522;&#20110;&#36827;&#21270;&#30340;&#25628;&#32034;&#30340;&#20248;&#28857;&#19982;&#22522;&#20110;&#23398;&#20064;&#30340;&#23616;&#37096;&#25628;&#32034;&#30456;&#32467;&#21512;&#65292;&#20197;&#24555;&#36895;&#30830;&#23450;PE&#21644;&#36890;&#20449;&#38142;&#36335;&#30340;&#25918;&#32622;&#20301;&#32622;&#65292;&#20197;&#20248;&#21270;3D NoC&#21551;&#29992;&#30340;&#24322;&#26500;&#22810;&#26680;&#31995;&#32479;&#20013;&#30340;&#22810;&#20010;&#30446;&#26631;&#65288;&#20363;&#22914;&#24310;&#36831;&#65292;&#21534;&#21520;&#37327;&#21644;&#33021;&#37327;&#65289;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;MOELA&#21487;&#20197;&#23558;&#35299;&#20915;&#26041;&#26696;&#30340;&#26597;&#25214;&#36895;&#24230;&#25552;&#39640;&#22810;&#36798;128&#20493;&#65292;&#22312;5&#20010;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23558;Pareto Hypervolume&#65288;PHV&#65289;&#25552;&#39640;&#22810;&#36798;12.14&#20493;&#65292;&#24182;&#23558;&#33021;&#37327;&#24310;&#36831;&#20056;&#31215;&#65288;EDP&#65289;&#25552;&#39640;&#22810;&#36798;7.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enable emerging applications such as deep machine learning and graph processing, 3D network-on-chip (NoC) enabled heterogeneous manycore platforms that can integrate many processing elements (PEs) are needed. However, designing such complex systems with multiple objectives can be challenging due to the huge associated design space and long evaluation times. To optimize such systems, we propose a new multi-objective design space exploration framework called MOELA that combines the benefits of evolutionary-based search with a learning-based local search to quickly determine PE and communication link placement to optimize multiple objectives (e.g., latency, throughput, and energy) in 3D NoC enabled heterogeneous manycore systems. Compared to state-of-the-art approaches, MOELA increases the speed of finding solutions by up to 128x, leads to a better Pareto Hypervolume (PHV) by up to 12.14x and improves energy-delay-product (EDP) by up to 7.7% in a 5-objective scenario.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#21457;&#29616;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#20294;&#36890;&#36807;&#23545;&#24494;&#35843;&#25968;&#25454;&#38598;&#36827;&#34892;&#24178;&#39044;&#21487;&#20197;&#32416;&#27491;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;&#36825;&#34920;&#26126;&#20180;&#32454;&#31574;&#21010;&#24494;&#35843;&#25968;&#25454;&#38598;&#23545;&#20110;&#20943;&#23569;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20559;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#26679;&#20570;&#29978;&#33267;&#21487;&#20197;&#24357;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2303.06167</link><description>&lt;p&gt;
&#36890;&#36807;&#25805;&#20316;&#24494;&#35843;&#25968;&#25454;&#38598;&#26469;&#20811;&#26381;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Overcoming Bias in Pretrained Models by Manipulating the Finetuning Dataset. (arXiv:2303.06167v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#21457;&#29616;&#24494;&#35843;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#20294;&#36890;&#36807;&#23545;&#24494;&#35843;&#25968;&#25454;&#38598;&#36827;&#34892;&#24178;&#39044;&#21487;&#20197;&#32416;&#27491;&#36825;&#31181;&#20559;&#35265;&#65292;&#32780;&#19988;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;&#36825;&#34920;&#26126;&#20180;&#32454;&#31574;&#21010;&#24494;&#35843;&#25968;&#25454;&#38598;&#23545;&#20110;&#20943;&#23569;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20559;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#26679;&#20570;&#29978;&#33267;&#21487;&#20197;&#24357;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the bias problem in pretrained models and finds that finetuned models can inherit the biases of pretrained models, but these biases can be corrected by manipulating the finetuning dataset with little impact on performance. This implies that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.
&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#36890;&#36807;&#20801;&#35768;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#34920;&#36798;&#29305;&#24449;&#34987;&#24494;&#35843;&#21040;&#26356;&#23567;&#12289;&#26356;&#20855;&#39046;&#22495;&#29305;&#23450;&#24615;&#30340;&#25968;&#25454;&#38598;&#30340;&#30446;&#26631;&#20219;&#21153;&#20013;&#32780;&#21463;&#30410;&#12290;&#28982;&#32780;&#65292;&#26377;&#20154;&#25285;&#24515;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#24102;&#26377;&#33258;&#24049;&#30340;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#20250;&#20256;&#25773;&#21040;&#24494;&#35843;&#27169;&#22411;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20559;&#35265;&#65292;&#24403;&#20559;&#35265;&#34987;&#27010;&#24565;&#21270;&#20026;&#30446;&#26631;&#20219;&#21153;&#21644;&#25935;&#24863;&#23646;&#24615;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#20197;&#21450;&#25968;&#25454;&#38598;&#20013;&#29305;&#23450;&#32676;&#20307;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#26102;&#12290;&#22312;&#20559;&#35265;&#30340;&#20004;&#31181;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;(1)&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#30830;&#23454;&#21487;&#20197;&#32487;&#25215;&#23427;&#20204;&#30340;&#20559;&#35265;&#65292;&#20294;(2)&#36890;&#36807;&#23545;&#24494;&#35843;&#25968;&#25454;&#38598;&#36827;&#34892;&#30456;&#23545;&#36739;&#23567;&#30340;&#24178;&#39044;&#65292;&#36825;&#31181;&#20559;&#35265;&#21487;&#20197;&#24471;&#21040;&#32416;&#27491;&#65292;&#32780;&#19988;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#24448;&#24448;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24847;&#21619;&#30528;&#65292;&#20180;&#32454;&#31574;&#21010;&#24494;&#35843;&#25968;&#25454;&#38598;&#23545;&#20110;&#20943;&#23569;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20559;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#26679;&#20570;&#29978;&#33267;&#21487;&#20197;&#24357;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is beneficial by allowing the expressive features of models pretrained on large-scale datasets to be finetuned for the target task of smaller, more domain-specific datasets. However, there is a concern that these pretrained models may come with their own biases which would propagate into the finetuned model. In this work, we investigate bias when conceptualized as both spurious correlations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. Under both notions of bias, we find that (1) models finetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the finetuning dataset, and often with a negligible impact to performance. Our findings imply that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#28436;&#21592;-&#35780;&#35770;&#23478;QD-RL&#26694;&#26550;&#65292;&#29992;&#20110;QD-RL&#35774;&#32622;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;RL&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;PGA-ME&#65288;SAC&#65289;&#21644;PGA-ME&#65288;DroQ&#65289;&#65292;&#23558;&#28145;&#24230;RL&#30340;&#26368;&#26032;&#36827;&#23637;&#24212;&#29992;&#20110;QD-RL&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QD-RL&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#20154;&#24418;&#29615;&#22659;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.06164</link><description>&lt;p&gt;
&#29702;&#35299;&#36136;&#37327;&#22810;&#26679;&#24615;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning. (arXiv:2303.06164v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#28436;&#21592;-&#35780;&#35770;&#23478;QD-RL&#26694;&#26550;&#65292;&#29992;&#20110;QD-RL&#35774;&#32622;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;RL&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;PGA-ME&#65288;SAC&#65289;&#21644;PGA-ME&#65288;DroQ&#65289;&#65292;&#23558;&#28145;&#24230;RL&#30340;&#26368;&#26032;&#36827;&#23637;&#24212;&#29992;&#20110;QD-RL&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QD-RL&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#20154;&#24418;&#29615;&#22659;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Generalized Actor-Critic QD-RL framework for actor-critic deep RL methods in the QD-RL setting. The framework introduces two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ), which apply recent advancements in Deep RL to the QD-RL setting and solve the humanoid environment problem that existing QD-RL algorithms cannot solve.
&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#65288;QD&#65289;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#24050;&#32463;&#23548;&#33268;&#20102;&#24378;&#22823;&#30340;&#28151;&#21512;QD-RL&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#24182;&#24102;&#26469;&#20102;&#20004;&#20010;&#39046;&#22495;&#30340;&#26368;&#20339;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#20182;RL&#31639;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#20808;&#21069;&#30340;&#28151;&#21512;&#26041;&#27861;&#20013;&#20165;&#20351;&#29992;&#20102;&#21333;&#20010;&#28145;&#24230;RL&#31639;&#27861;&#65288;TD3&#65289;&#12290;&#27492;&#22806;&#65292;QD&#21644;RL&#20043;&#38388;&#30340;&#20248;&#21270;&#36807;&#31243;&#23384;&#22312;&#26681;&#26412;&#24046;&#24322;&#65292;&#38656;&#35201;&#26356;&#21152;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#28436;&#21592;-&#35780;&#35770;&#23478;QD-RL&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;QD-RL&#35774;&#32622;&#20013;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#28145;&#24230;RL&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#26465;&#30740;&#31350;&#28145;&#24230;RL&#22312;QD-RL&#35774;&#32622;&#20013;&#30340;&#35265;&#35299;&#30340;&#36335;&#24452;&#65292;&#36825;&#26159;&#22312;QD-RL&#20013;&#21462;&#24471;&#36827;&#23637;&#30340;&#37325;&#35201;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#31639;&#27861;&#65292;PGA-ME&#65288;SAC&#65289;&#21644;PGA-ME&#65288;DroQ&#65289;&#65292;&#23558;&#28145;&#24230;RL&#30340;&#26368;&#26032;&#36827;&#23637;&#24212;&#29992;&#20110;QD-RL&#35774;&#32622;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QD-RL&#31639;&#27861;&#26080;&#27861;&#35299;&#20915;&#30340;&#20154;&#24418;&#29615;&#22659;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The synergies between Quality-Diversity (QD) and Deep Reinforcement Learning (RL) have led to powerful hybrid QD-RL algorithms that have shown tremendous potential, and brings the best of both fields. However, only a single deep RL algorithm (TD3) has been used in prior hybrid methods despite notable progress made by other RL algorithms. Additionally, there are fundamental differences in the optimization procedures between QD and RL which would benefit from a more principled approach. We propose Generalized Actor-Critic QD-RL, a unified modular framework for actor-critic deep RL methods in the QD-RL setting. This framework provides a path to study insights from Deep RL in the QD-RL setting, which is an important and efficient way to make progress in QD-RL. We introduce two new algorithms, PGA-ME (SAC) and PGA-ME (DroQ) which apply recent advancements in Deep RL to the QD-RL setting, and solves the humanoid environment which was not possible using existing QD-RL algorithms. However, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#33258;&#24049;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#65292;&#21516;&#26102;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#26368;&#32456;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;Q-learning&#31639;&#27861;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.06155</link><description>&lt;p&gt;
&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Digital Twin-Assisted Knowledge Distillation Framework for Heterogeneous Federated Learning. (arXiv:2303.06155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#23402;&#29983;&#36741;&#21161;&#30340;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#29992;&#25143;&#21487;&#20197;&#36873;&#25321;&#33258;&#24049;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#65292;&#21516;&#26102;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#22312;&#26381;&#21153;&#22120;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#65292;&#26368;&#32456;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#21644;Q-learning&#31639;&#27861;&#23454;&#29616;&#27169;&#22411;&#36873;&#25321;&#21644;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a digital twin-assisted knowledge distillation framework for heterogeneous federated learning, where users can select their own neural network models and distill knowledge from a big teacher model, and the teacher model can be trained on a digital twin located in the server. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming problem and solved using Q-learning and optimization algorithms.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#39537;&#21160;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#24212;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#24322;&#26500;&#24615;&#65292;&#20854;&#20013;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#36873;&#25321;&#20854;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#33258;&#24049;&#30340;&#31169;&#26377;&#25968;&#25454;&#38598;&#20174;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#20013;&#33976;&#39311;&#30693;&#35782;&#12290;&#20026;&#20102;&#20811;&#26381;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29992;&#25143;&#35774;&#22791;&#19978;&#35757;&#32451;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#21033;&#29992;&#25968;&#23383;&#23402;&#29983;&#30340;&#26041;&#24335;&#65292;&#25945;&#24072;&#27169;&#22411;&#21487;&#20197;&#22312;&#20855;&#26377;&#36275;&#22815;&#35745;&#31639;&#36164;&#28304;&#30340;&#26381;&#21153;&#22120;&#19978;&#30340;&#25968;&#23383;&#23402;&#29983;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#22312;&#27169;&#22411;&#33976;&#39311;&#26399;&#38388;&#65292;&#27599;&#20010;&#29992;&#25143;&#21487;&#20197;&#22312;&#29289;&#29702;&#23454;&#20307;&#25110;&#25968;&#23383;&#20195;&#29702;&#22788;&#26356;&#26032;&#20854;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#20026;&#29992;&#25143;&#36873;&#25321;&#27169;&#22411;&#21644;&#35757;&#32451;&#21368;&#36733;&#21644;&#36164;&#28304;&#20998;&#37197;&#21046;&#23450;&#20102;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;MIP&#65289;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32852;&#21512;&#20351;&#29992;Q-learning&#21644;&#20248;&#21270;&#65292;&#20854;&#20013;Q-learning&#20026;&#29992;&#25143;&#36873;&#25321;&#27169;&#22411;&#24182;&#30830;&#23450;&#26159;&#22312;&#26412;&#22320;&#36824;&#26159;&#22312;&#26381;&#21153;&#22120;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#20248;&#21270;&#21017;&#29992;&#20110;&#36164;&#28304;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, to deal with the heterogeneity in federated learning (FL) systems, a knowledge distillation (KD) driven training framework for FL is proposed, where each user can select its neural network model on demand and distill knowledge from a big teacher model using its own private dataset. To overcome the challenge of train the big teacher model in resource limited user devices, the digital twin (DT) is exploit in the way that the teacher model can be trained at DT located in the server with enough computing resources. Then, during model distillation, each user can update the parameters of its model at either the physical entity or the digital agent. The joint problem of model selection and training offloading and resource allocation for users is formulated as a mixed integer programming (MIP) problem. To solve the problem, Q-learning and optimization are jointly used, where Q-learning selects models for users and determines whether to train locally or on the server, and optimiz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;k-mer&#20998;&#24067;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#33410;&#32422;&#36164;&#28304;&#24182;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06154</link><description>&lt;p&gt;
&#22522;&#20110;k-mer&#20998;&#24067;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#36164;&#28304;&#33410;&#32422;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resource saving taxonomy classification with k-mer distributions and machine learning. (arXiv:2303.06154v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;k-mer&#20998;&#24067;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#31867;&#27861;&#65292;&#21487;&#20197;&#33410;&#32422;&#36164;&#28304;&#24182;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a resource-saving classification method based on k-mer distributions and machine learning, which can improve the performance of classifiers and reduce the consumption of energy.
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#39640;&#36890;&#37327;&#27979;&#24207;&#25216;&#26415;&#65288;&#22914;&#23439;&#22522;&#22240;&#32452;&#27979;&#24207;&#65289;&#29983;&#25104;&#25968;&#30334;&#19975;&#20010;&#24207;&#21015;&#65292;&#38656;&#35201;&#26681;&#25454;&#23427;&#20204;&#30340;&#20998;&#31867;&#32423;&#21035;&#36827;&#34892;&#20998;&#31867;&#12290;&#29616;&#20195;&#26041;&#27861;&#35201;&#20040;&#24212;&#29992;&#26412;&#22320;&#27604;&#23545;&#21644;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#65288;&#22914;MMseqs2&#65289;&#30340;&#27604;&#36739;&#65292;&#35201;&#20040;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;&#22914;DeepMicrobes&#21644;BERTax&#65289;&#12290;&#22522;&#20110;&#27604;&#23545;&#30340;&#26041;&#27861;&#22312;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#25104;&#26412;&#39640;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#25968;&#25454;&#24211;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#12290;&#23545;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#30828;&#20214;&#36827;&#34892;&#35745;&#31639;&#65292;&#36825;&#28040;&#32791;&#22823;&#37327;&#33021;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20174;DNA&#20013;&#33719;&#24471;&#30340;k-mer&#20998;&#24067;&#20316;&#20026;&#29305;&#24449;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#23376;&#31354;&#38388;k&#26368;&#36817;&#37051;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#25110;&#34955;&#35013;&#20915;&#31574;&#26641;&#65289;&#26469;&#20998;&#31867;&#20854;&#20998;&#31867;&#36215;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#31354;&#38388;&#25968;&#25454;&#38598;&#24179;&#34913;&#26041;&#27861;&#65292;&#20801;&#35768;&#20943;&#23569;&#35757;&#32451;&#25968;&#25454;&#38598;&#24182;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#27604;&#36739;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern high throughput sequencing technologies like metagenomic sequencing generate millions of sequences which have to be classified based on their taxonomic rank. Modern approaches either apply local alignment and comparison to existing data sets like MMseqs2 or use deep neural networks as it is done in DeepMicrobes and BERTax. Alignment-based approaches are costly in terms of runtime, especially since databases get larger and larger. For the deep learning-based approaches, specialized hardware is necessary for a computation, which consumes large amounts of energy. In this paper, we propose to use $k$-mer distributions obtained from DNA as features to classify its taxonomic origin using machine learning approaches like the subspace $k$-nearest neighbors algorithm, neural networks or bagged decision trees. In addition, we propose a feature space data set balancing approach, which allows reducing the data set for training and improves the performance of the classifiers. By comparing pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoiseCAM&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23450;&#20301;&#26131;&#21463;&#25915;&#20987;&#23618;&#24182;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#28151;&#20837;&#36755;&#20837;&#30340;&#39640;&#26031;&#38543;&#26426;&#22122;&#22768;&#20570;&#20986;&#21453;&#24212;&#12290;</title><link>http://arxiv.org/abs/2303.06151</link><description>&lt;p&gt;
NoiseCAM: &#29992;&#20110;&#22122;&#22768;&#21644;&#23545;&#25239;&#25915;&#20987;&#36793;&#30028;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial Attacks. (arXiv:2303.06151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NoiseCAM&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#23450;&#20301;&#26131;&#21463;&#25915;&#20987;&#23618;&#24182;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#28151;&#20837;&#36755;&#20837;&#30340;&#39640;&#26031;&#38543;&#26426;&#22122;&#22768;&#20570;&#20986;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a NoiseCAM algorithm that can locate vulnerable layers and detect adversarial examples, while not responding to Gaussian random noise mixed in the inputs.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#25915;&#20987;&#24456;&#23481;&#26131;&#35823;&#23548;&#31070;&#32463;&#32593;&#32476;&#24182;&#23548;&#33268;&#38169;&#35823;&#20915;&#31574;&#12290;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#39640;&#24230;&#38656;&#35201;&#38450;&#24481;&#26426;&#21046;&#12290;&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;&#26799;&#24230;&#31867;&#28608;&#27963;&#26144;&#23556;&#65288;GradCAM&#65289;&#20998;&#26512;VGG-16&#32593;&#32476;&#22312;&#20854;&#36755;&#20837;&#19982;&#23545;&#25239;&#25200;&#21160;&#25110;&#39640;&#26031;&#22122;&#22768;&#28151;&#21512;&#26102;&#30340;&#34892;&#20026;&#20559;&#24046;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23450;&#20301;&#23545;&#25239;&#25200;&#21160;&#21644;&#39640;&#26031;&#22122;&#22768;&#25935;&#24863;&#30340;&#26131;&#21463;&#25915;&#20987;&#23618;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#26131;&#21463;&#25915;&#20987;&#23618;&#30340;&#34892;&#20026;&#20559;&#24046;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;NoiseCAM&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#38598;&#25104;&#20102;&#20840;&#23616;&#21644;&#20687;&#32032;&#32423;&#21152;&#26435;&#31867;&#28608;&#27963;&#26144;&#23556;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#19981;&#20250;&#23545;&#28151;&#20837;&#36755;&#20837;&#30340;&#39640;&#26031;&#38543;&#26426;&#22122;&#22768;&#20570;&#20986;&#21453;&#24212;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#26816;&#27979;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) and Deep Neural Networks (DNNs) are widely used in various domains. However, adversarial attacks can easily mislead a neural network and lead to wrong decisions. Defense mechanisms are highly preferred in safety-critical applications. In this paper, firstly, we use the gradient class activation map (GradCAM) to analyze the behavior deviation of the VGG-16 network when its inputs are mixed with adversarial perturbation or Gaussian noise. In particular, our method can locate vulnerable layers that are sensitive to adversarial perturbation and Gaussian noise. We also show that the behavior deviation of vulnerable layers can be used to detect adversarial examples. Secondly, we propose a novel NoiseCAM algorithm that integrates information from globally and pixel-level weighted class activation maps. Our algorithm is susceptible to adversarial perturbations and will not respond to Gaussian random noise mixed in the inputs. Third, we compare detecting adversarial examples 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.05737</link><description>&lt;p&gt;
&#20020;&#24202;BERTScore&#65306;&#20020;&#24202;&#29615;&#22659;&#19979;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#25913;&#36827;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a Clinical BERTScore (CBERTScore) metric for ASR in medical contexts, which penalizes clinically-relevant mistakes more than other metrics and aligns more closely with clinician preferences. The authors also collect a benchmark of clinician preferences on medical sentences and release it for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26377;&#28508;&#21147;&#33410;&#30465;&#26102;&#38388;&#65292;&#38477;&#20302;&#25104;&#26412;&#65292;&#25552;&#39640;&#25253;&#21578;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#21307;&#29983;&#30340;&#30130;&#21171;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36991;&#20813;&#21307;&#23398;&#30456;&#20851;&#30340;&#36716;&#24405;&#38169;&#35823;&#30340;&#37325;&#35201;&#24615;&#65292;&#21307;&#30103;&#34892;&#19994;&#37319;&#29992;&#36825;&#31181;&#25216;&#26415;&#30340;&#36895;&#24230;&#36739;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;ASR&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#65288;WER&#12289;BLUE&#12289;METEOR&#31561;&#65289;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#24230;&#37327;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#65292;&#26377;&#26102;&#24046;&#36317;&#24456;&#22823;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We demonstrate that this metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins. We collect a benchmark of 13 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP), demonstrate that CBERTScore more closely matches what clinicians prefer, and release the benchmark for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#38480;&#21046;&#26410;&#27979;&#37327;&#28151;&#26434;&#19979;&#30340;&#21463;&#30410;&#21644;&#20260;&#23475;&#27010;&#29575;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#25935;&#24863;&#24615;&#21442;&#25968;&#35745;&#31639;&#27010;&#29575;&#30340;&#19978;&#38480;&#25110;&#19979;&#38480;&#65292;&#21478;&#19968;&#31181;&#26159;&#21033;&#29992;&#20195;&#29702;&#21464;&#37327;&#24471;&#21040;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2303.05396</link><description>&lt;p&gt;
&#36890;&#36807;&#25935;&#24863;&#24615;&#21442;&#25968;&#21644;&#20195;&#29702;&#21464;&#37327;&#38480;&#21046;&#21463;&#30410;&#21644;&#20260;&#23475;&#30340;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Bounding the Probabilities of Benefit and Harm Through Sensitivity Parameters and Proxies. (arXiv:2303.05396v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#38480;&#21046;&#26410;&#27979;&#37327;&#28151;&#26434;&#19979;&#30340;&#21463;&#30410;&#21644;&#20260;&#23475;&#27010;&#29575;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#25935;&#24863;&#24615;&#21442;&#25968;&#35745;&#31639;&#27010;&#29575;&#30340;&#19978;&#38480;&#25110;&#19979;&#38480;&#65292;&#21478;&#19968;&#31181;&#26159;&#21033;&#29992;&#20195;&#29702;&#21464;&#37327;&#24471;&#21040;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two methods for bounding the probabilities of benefit and harm under unmeasured confounding, one is to compute the upper or lower bound of the probability through sensitivity parameters, and the other is to derive tighter bounds using a measured nondifferential proxy of the unmeasured confounder.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#38480;&#21046;&#26410;&#27979;&#37327;&#28151;&#26434;&#19979;&#30340;&#21463;&#30410;&#21644;&#20260;&#23475;&#27010;&#29575;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#35745;&#31639;&#20219;&#19968;&#27010;&#29575;&#30340;&#65288;&#19978;&#38480;&#25110;&#19979;&#38480;&#65289;&#65292;&#20316;&#20026;&#35266;&#23519;&#25968;&#25454;&#20998;&#24067;&#21644;&#20004;&#20010;&#30452;&#35266;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#20989;&#25968;&#65292;&#28982;&#21518;&#21487;&#20197;&#23558;&#20854;&#21576;&#29616;&#32473;&#20998;&#26512;&#24072;&#20316;&#20026;2-D&#22270;&#20197;&#21327;&#21161;&#20854;&#20915;&#31574;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#20551;&#35774;&#23384;&#22312;&#26410;&#27979;&#37327;&#28151;&#26434;&#22240;&#32032;&#30340;&#27979;&#37327;&#38750;&#24046;&#24322;&#20195;&#29702;&#21464;&#37327;&#65288;&#21363;&#30452;&#25509;&#25928;&#24212;&#65289;&#12290;&#20351;&#29992;&#27492;&#20195;&#29702;&#21464;&#37327;&#65292;&#21487;&#20197;&#20174;&#20165;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#23548;&#20986;&#27604;&#29616;&#26377;&#30028;&#38480;&#26356;&#32039;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present two methods for bounding the probabilities of benefit and harm under unmeasured confounding. The first method computes the (upper or lower) bound of either probability as a function of the observed data distribution and two intuitive sensitivity parameters which, then, can be presented to the analyst as a 2-D plot to assist her in decision making. The second method assumes the existence of a measured nondifferential proxy (i.e., direct effect) of the unmeasured confounder. Using this proxy, tighter bounds than the existing ones can be derived from just the observed data distribution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#30495;&#23454;&#30005;&#21147;&#32593;&#29615;&#22659;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#21457;&#30005;&#26426;&#30340;&#35268;&#21010;&#21644;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#35843;&#25972;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05205</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#30005;&#21147;&#31995;&#32479;&#23454;&#26102;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Real-time scheduling of renewable power systems through planning-based reinforcement learning. (arXiv:2303.05205v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#30495;&#23454;&#30005;&#21147;&#32593;&#29615;&#22659;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#21457;&#30005;&#26426;&#30340;&#35268;&#21010;&#21644;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#35843;&#25972;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment, which enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's ability.
&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#22686;&#38271;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#26469;&#28304;&#23545;&#20256;&#32479;&#30005;&#21147;&#35843;&#24230;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#36816;&#33829;&#21830;&#38590;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#21457;&#30005;&#26085;&#21069;&#39044;&#27979;&#65292;&#22240;&#27492;&#38656;&#35201;&#26410;&#26469;&#35843;&#24230;&#31995;&#32479;&#26681;&#25454;&#36229;&#30701;&#26399;&#39044;&#27979;&#36827;&#34892;&#23454;&#26102;&#35843;&#24230;&#20915;&#31574;&#12290;&#21463;&#35745;&#31639;&#36895;&#24230;&#38480;&#21046;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#21457;&#23637;&#24050;&#32463;&#23637;&#31034;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RL&#26041;&#27861;&#22312;&#32422;&#26463;&#22797;&#26434;&#24615;&#12289;&#31639;&#27861;&#24615;&#33021;&#21644;&#29615;&#22659;&#20445;&#30495;&#24230;&#26041;&#38754;&#19981;&#36275;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#22522;&#20110;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#30495;&#23454;&#30005;&#21147;&#32593;&#29615;&#22659;&#30340;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#21457;&#30005;&#26426;&#30340;&#35268;&#21010;&#21644;&#26356;&#32454;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#35843;&#25972;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;&#26426;&#32452;&#32452;&#21512;&#21644;&#32463;&#27982;&#35843;&#24230;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing renewable energy sources have posed significant challenges to traditional power scheduling. It is difficult for operators to obtain accurate day-ahead forecasts of renewable generation, thereby requiring the future scheduling system to make real-time scheduling decisions aligning with ultra-short-term forecasts. Restricted by the computation speed, traditional optimization-based methods can not solve this problem. Recent developments in reinforcement learning (RL) have demonstrated the potential to solve this challenge. However, the existing RL methods are inadequate in terms of constraint complexity, algorithm performance, and environment fidelity. We are the first to propose a systematic solution based on the state-of-the-art reinforcement learning algorithm and the real power grid environment. The proposed approach enables planning and finer time resolution adjustments of power generators, including unit commitment and economic dispatch, thus increasing the grid's abilit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#37327;&#35282;&#33394;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#23545;&#20195;&#30721;&#31070;&#32463;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#31243;&#24207;&#20013;&#28155;&#21152;&#21333;&#20010;&#21464;&#37327;&#30340;&#35282;&#33394;&#26469;&#20016;&#23500;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#24182;&#22240;&#27492;&#23545;&#21464;&#37327;&#35282;&#33394;&#22686;&#24378;&#22312;&#35757;&#32451;Code2Seq&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.04942</link><description>&lt;p&gt;
&#20195;&#30721;&#31070;&#32463;&#27169;&#22411;&#20013;&#22522;&#20110;&#21464;&#37327;&#35282;&#33394;&#30340;&#29305;&#24449;&#22686;&#24378;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Variable-Role-based Feature Enrichment in Neural Models of Code. (arXiv:2303.04942v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#37327;&#35282;&#33394;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#23545;&#20195;&#30721;&#31070;&#32463;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#31243;&#24207;&#20013;&#28155;&#21152;&#21333;&#20010;&#21464;&#37327;&#30340;&#35282;&#33394;&#26469;&#20016;&#23500;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#24182;&#22240;&#27492;&#23545;&#21464;&#37327;&#35282;&#33394;&#22686;&#24378;&#22312;&#35757;&#32451;Code2Seq&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the impact of an unsupervised feature enrichment approach based on variable roles on the performance of neural models of code, and enriches a source code dataset by adding the role of individual variables in the dataset programs, thereby conducting a study on the impact of variable role enrichment in training the Code2Seq model.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#22823;&#22823;&#20943;&#23569;&#20102;&#29305;&#24449;&#24037;&#31243;&#30340;&#24320;&#38144;&#65292;&#20294;&#36755;&#20837;&#20013;&#21487;&#29992;&#30340;&#29305;&#24449;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#37327;&#35282;&#33394;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#22686;&#24378;&#26041;&#27861;&#23545;&#20195;&#30721;&#31070;&#32463;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#21464;&#37327;&#35282;&#33394;&#30340;&#27010;&#24565;&#65288;&#22914;Sajaniemi&#31561;&#20154;&#30340;&#20316;&#21697;&#20013;&#25152;&#20171;&#32461;&#30340;&#65289;&#24050;&#34987;&#21457;&#29616;&#26377;&#21161;&#20110;&#23398;&#29983;&#30340;&#32534;&#31243;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20010;&#27010;&#24565;&#26159;&#21542;&#20250;&#25552;&#39640;&#20195;&#30721;&#31070;&#32463;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31687;&#30740;&#31350;Sajaniemi&#31561;&#20154;&#30340;&#21464;&#37327;&#35282;&#33394;&#27010;&#24565;&#22914;&#20309;&#24433;&#21709;&#20195;&#30721;&#31070;&#32463;&#27169;&#22411;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#31243;&#24207;&#20013;&#28155;&#21152;&#21333;&#20010;&#21464;&#37327;&#30340;&#35282;&#33394;&#26469;&#20016;&#23500;&#28304;&#20195;&#30721;&#25968;&#25454;&#38598;&#65292;&#24182;&#22240;&#27492;&#23545;&#21464;&#37327;&#35282;&#33394;&#22686;&#24378;&#22312;&#35757;&#32451;Code2Seq&#27169;&#22411;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep neural models substantially reduce the overhead of feature engineering, the features readily available in the inputs might significantly impact training cost and the performance of the models. In this paper, we explore the impact of an unsuperivsed feature enrichment approach based on variable roles on the performance of neural models of code. The notion of variable roles (as introduced in the works of Sajaniemi et al. [Refs. 1,2]) has been found to help students' abilities in programming. In this paper, we investigate if this notion would improve the performance of neural models of code. To the best of our knowledge, this is the first work to investigate how Sajaniemi et al.'s concept of variable roles can affect neural models of code. In particular, we enrich a source code dataset by adding the role of individual variables in the dataset programs, and thereby conduct a study on the impact of variable role enrichment in training the Code2Seq model. In addition, we shed l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#24191;&#20041;QSplines&#65292;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#35745;&#31639;&#26469;&#36817;&#20284;&#38750;&#32447;&#24615;&#37327;&#23376;&#28608;&#27963;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#21407;&#22987;QSplines&#22312;&#37327;&#23376;&#30828;&#20214;&#26041;&#38754;&#30340;&#39640;&#35201;&#27714;&#65292;&#24182;&#36866;&#21512;&#23884;&#20837;&#29616;&#26377;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.04788</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#37327;&#23376;&#26679;&#26465;&#20351;&#38750;&#32447;&#24615;&#37327;&#23376;&#25805;&#20316;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enabling Non-Linear Quantum Operations through Variational Quantum Splines. (arXiv:2303.04788v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#24191;&#20041;QSplines&#65292;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#35745;&#31639;&#26469;&#36817;&#20284;&#38750;&#32447;&#24615;&#37327;&#23376;&#28608;&#27963;&#20989;&#25968;&#65292;&#20811;&#26381;&#20102;&#21407;&#22987;QSplines&#22312;&#37327;&#23376;&#30828;&#20214;&#26041;&#38754;&#30340;&#39640;&#35201;&#27714;&#65292;&#24182;&#36866;&#21512;&#23884;&#20837;&#29616;&#26377;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel method, Generalised QSplines (GQSplines), for approximating non-linear quantum activation functions using hybrid quantum-classical computation, which overcomes the highly demanding requirements of the original QSplines in terms of quantum hardware and is suitable to be embedded in existing quantum neural network architectures.
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21147;&#23398;&#30340;&#20551;&#35774;&#20165;&#23545;&#37327;&#23376;&#29366;&#24577;&#26045;&#21152;&#24186;&#27491;&#21464;&#25442;&#65292;&#36825;&#23545;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#37327;&#23376;&#26679;&#26465;&#65288;QSplines&#65289;&#26469;&#36817;&#20284;&#37327;&#23376;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#22312;&#37327;&#23376;&#31639;&#27861;&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#12290;&#28982;&#32780;&#65292;QSplines&#20351;&#29992;HHL&#20316;&#20026;&#23376;&#31243;&#24207;&#65292;&#24182;&#38656;&#35201;&#19968;&#20010;&#23481;&#38169;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#25165;&#33021;&#27491;&#30830;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;QSplines&#65288;GQSplines&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#35745;&#31639;&#26469;&#36817;&#20284;&#38750;&#32447;&#24615;&#37327;&#23376;&#28608;&#27963;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;GQSplines&#20811;&#26381;&#20102;&#21407;&#22987;QSplines&#22312;&#37327;&#23376;&#30828;&#20214;&#26041;&#38754;&#30340;&#39640;&#35201;&#27714;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#36817;&#26399;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#28789;&#27963;&#30340;&#38382;&#39064;&#34920;&#31034;&#65292;&#36866;&#21512;&#23884;&#20837;&#29616;&#26377;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The postulates of quantum mechanics impose only unitary transformations on quantum states, which is a severe limitation for quantum machine learning algorithms. Quantum Splines (QSplines) have recently been proposed to approximate quantum activation functions to introduce non-linearity in quantum algorithms. However, QSplines make use of the HHL as a subroutine and require a fault-tolerant quantum computer to be correctly implemented. This work proposes the Generalised QSplines (GQSplines), a novel method for approximating non-linear quantum activation functions using hybrid quantum-classical computation. The GQSplines overcome the highly demanding requirements of the original QSplines in terms of quantum hardware and can be implemented using near-term quantum computers. Furthermore, the proposed method relies on a flexible problem representation for non-linear approximation and it is suitable to be embedded in existing quantum neural network architectures. In addition, we provide a pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#22686;&#24378;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#26041;&#27861;&#65288;&#21363;MSGDA&#21644;AdaMSGDA&#65289;&#26469;&#35299;&#20915;&#38750;&#20984;-PL&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#20013;AdaMSGDA&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#26356;&#26032;&#21464;&#37327;$x$&#21644;$y$&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20840;&#23616;&#21644;&#22352;&#26631;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;MSGDA&#21644;AdaMSGDA&#26041;&#27861;&#22312;&#25214;&#21040;$\epsilon$-&#31283;&#23450;&#35299;&#26102;&#65292;&#21482;&#38656;&#35201;&#22312;&#27599;&#20010;&#24490;&#29615;&#20013;&#36827;&#34892;&#19968;&#27425;&#37319;&#26679;&#65292;&#23601;&#21487;&#20197;&#33719;&#24471;&#24050;&#30693;&#30340;&#26368;&#20339;&#26679;&#26412;&#65288;&#26799;&#24230;&#65289;&#22797;&#26434;&#24230;$O(\epsilon^{-3})$&#12290;</title><link>http://arxiv.org/abs/2303.03984</link><description>&lt;p&gt;
&#38750;&#20984;-PL&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30340;&#22686;&#24378;&#33258;&#36866;&#24212;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhanced Adaptive Gradient Algorithms for Nonconvex-PL Minimax Optimization. (arXiv:2303.03984v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#22686;&#24378;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#26041;&#27861;&#65288;&#21363;MSGDA&#21644;AdaMSGDA&#65289;&#26469;&#35299;&#20915;&#38750;&#20984;-PL&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#20013;AdaMSGDA&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#26356;&#26032;&#21464;&#37327;$x$&#21644;$y$&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20840;&#23616;&#21644;&#22352;&#26631;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;MSGDA&#21644;AdaMSGDA&#26041;&#27861;&#22312;&#25214;&#21040;$\epsilon$-&#31283;&#23450;&#35299;&#26102;&#65292;&#21482;&#38656;&#35201;&#22312;&#27599;&#20010;&#24490;&#29615;&#20013;&#36827;&#34892;&#19968;&#27425;&#37319;&#26679;&#65292;&#23601;&#21487;&#20197;&#33719;&#24471;&#24050;&#30693;&#30340;&#26368;&#20339;&#26679;&#26412;&#65288;&#26799;&#24230;&#65289;&#22797;&#26434;&#24230;$O(\epsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a class of enhanced momentum-based gradient descent ascent methods (MSGDA and AdaMSGDA) to solve nonconvex-PL minimax problems, where the AdaMSGDA algorithm can use various adaptive learning rates to update variables x and y without relying on any global and coordinate-wise adaptive learning rates. Theoretical analysis shows that MSGDA and AdaMSGDA methods have the best known sample (gradient) complexity of O(&#949;&#8722;3) in finding an &#949;-stationary solution.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#38750;&#20984;&#38750;&#20985;&#30340;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65288;&#21363;$\min_x\max_y f(x,y)$&#65289;&#65292;&#20854;&#20013;$f(x,y)$&#22312;$x$&#19978;&#21487;&#33021;&#26159;&#38750;&#20984;&#30340;&#65292;&#22312;$y$&#19978;&#26159;&#38750;&#20985;&#30340;&#65292;&#24182;&#28385;&#36275;Polyak-Lojasiewicz&#65288;PL&#65289;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#22686;&#24378;&#30340;&#22522;&#20110;&#21160;&#37327;&#30340;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#26041;&#27861;&#65288;&#21363;MSGDA&#21644;AdaMSGDA&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38543;&#26426;&#38750;&#20984;-PL&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;AdaMSGDA&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#26356;&#26032;&#21464;&#37327;$x$&#21644;$y$&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20840;&#23616;&#21644;&#22352;&#26631;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25910;&#25947;&#20998;&#26512;&#26694;&#26550;&#26469;&#35299;&#20915;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;MSGDA&#21644;AdaMSGDA&#26041;&#27861;&#22312;&#25214;&#21040;$\epsilon$-&#31283;&#23450;&#35299;&#65288;&#21363;$\mathbb{E}\|\nabla F(x)\|\leq \epsilon$&#65292;&#20854;&#20013;$F(x)=\max_y f(x,y)$&#65289;&#26102;&#65292;&#21482;&#38656;&#35201;&#22312;&#27599;&#20010;&#24490;&#29615;&#20013;&#36827;&#34892;&#19968;&#27425;&#37319;&#26679;&#65292;&#23601;&#21487;&#20197;&#33719;&#24471;&#24050;&#30693;&#30340;&#26368;&#20339;&#26679;&#26412;&#65288;&#26799;&#24230;&#65289;&#22797;&#26434;&#24230;$O(\epsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paper, we study a class of nonconvex nonconcave minimax optimization problems (i.e., $\min_x\max_y f(x,y)$), where $f(x,y)$ is possible nonconvex in $x$, and it is nonconcave and satisfies the Polyak-Lojasiewicz (PL) condition in $y$. Moreover, we propose a class of enhanced momentum-based gradient descent ascent methods (i.e., MSGDA and AdaMSGDA) to solve these stochastic Nonconvex-PL minimax problems. In particular, our AdaMSGDA algorithm can use various adaptive learning rates in updating the variables $x$ and $y$ without relying on any global and coordinate-wise adaptive learning rates. Theoretically, we present an effective convergence analysis framework for our methods. Specifically, we prove that our MSGDA and AdaMSGDA methods have the best known sample (gradient) complexity of $O(\epsilon^{-3})$ only requiring one sample at each loop in finding an $\epsilon$-stationary solution (i.e., $\mathbb{E}\|\nabla F(x)\|\leq \epsilon$, where $F(x)=\max_y f(x,y)$). This manuscript 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-ViT&#30693;&#35782;&#33976;&#39311;&#30340;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;PreFallKD&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#26816;&#27979;&#30693;&#35782;&#36716;&#31227;&#21040;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#29983;&#27169;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.03634</link><description>&lt;p&gt;
PreFallKD: &#22522;&#20110;CNN-ViT&#30693;&#35782;&#33976;&#39311;&#30340;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PreFallKD: Pre-Impact Fall Detection via CNN-ViT Knowledge Distillation. (arXiv:2303.03634v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-ViT&#30693;&#35782;&#33976;&#39311;&#30340;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;PreFallKD&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#26816;&#27979;&#30693;&#35782;&#36716;&#31227;&#21040;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#29983;&#27169;&#22411;&#19978;&#65292;&#23454;&#29616;&#20102;&#26816;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a pre-impact fall detection system called PreFallKD, which uses CNN-ViT knowledge distillation to transfer detection knowledge from a pre-trained teacher model to a lightweight convolutional neural network student model. The system achieves a balance between detection performance and computational complexity.
&lt;/p&gt;
&lt;p&gt;
&#36300;&#20498;&#20107;&#25925;&#26159;&#32769;&#40836;&#21270;&#31038;&#20250;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20102;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;&#65292;&#20197;&#25903;&#25345;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#36300;&#20498;&#20445;&#25252;&#31995;&#32479;&#65292;&#20197;&#39044;&#38450;&#20005;&#37325;&#30340;&#20260;&#23475;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#21482;&#20351;&#29992;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#32780;&#19981;&#26159;&#32771;&#34385;&#21040;&#36164;&#28304;&#21463;&#38480;&#30340;&#31227;&#21160;&#35774;&#22791;&#21644;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#30340;&#22797;&#26434;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;CNN-ViT&#30693;&#35782;&#33976;&#39311;&#30340;&#39044;&#38450;&#36300;&#20498;&#31995;&#32479;&#65292;&#21363;PreFallKD&#65292;&#20197;&#22312;&#26816;&#27979;&#24615;&#33021;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#25152;&#25552;&#20986;&#30340;PreFallKD&#23558;&#26816;&#27979;&#30693;&#35782;&#20174;&#39044;&#35757;&#32451;&#30340;&#25945;&#24072;&#27169;&#22411;&#65288;&#35270;&#35273;&#21464;&#25442;&#22120;&#65289;&#36716;&#31227;&#21040;&#23398;&#29983;&#27169;&#22411;&#65288;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24212;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;KFall&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23558;PreFallKD&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fall accidents are critical issues in an aging and aged society. Recently, many researchers developed pre-impact fall detection systems using deep learning to support wearable-based fall protection systems for preventing severe injuries. However, most works only employed simple neural network models instead of complex models considering the usability in resource-constrained mobile devices and strict latency requirements. In this work, we propose a novel pre-impact fall detection via CNN-ViT knowledge distillation, namely PreFallKD, to strike a balance between detection performance and computational complexity. The proposed PreFallKD transfers the detection knowledge from the pre-trained teacher model (vision transformer) to the student model (lightweight convolutional neural networks). Additionally, we apply data augmentation techniques to tackle issues of data imbalance. We conduct the experiment on the KFall public dataset and compare PreFallKD with other state-of-the-art models. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#24322;&#26500;&#22270;&#36328;&#27169;&#24577;&#32593;&#32476;&#65288;HGCN&#65289;&#65292;&#23427;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#26377;&#25928;&#22320;&#36830;&#25509;&#20102;&#36328;&#27169;&#24577;&#30340;&#30456;&#20851;&#33410;&#28857;&#65292;&#22312;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.02665</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#23398;&#20064;&#22312;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Learning for Acoustic Event Classification. (arXiv:2303.02665v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#24322;&#26500;&#22270;&#36328;&#27169;&#24577;&#32593;&#32476;&#65288;HGCN&#65289;&#65292;&#23427;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#26377;&#25928;&#22320;&#36830;&#25509;&#20102;&#36328;&#27169;&#24577;&#30340;&#30456;&#20851;&#33410;&#28857;&#65292;&#22312;&#22768;&#38899;&#20107;&#20214;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new model, Heterogeneous Graph Crossmodal Network (HGCN), which learns crossmodal edges and can adapt to various spatial and temporal scales, effectively connecting relevant nodes across modalities. It achieves state-of-the-art performance in acoustic event classification.
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#25552;&#20379;&#20102;&#19968;&#31181;&#32039;&#20945;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#26469;&#24314;&#27169;&#28041;&#21450;&#22810;&#20010;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#20351;&#29992;&#24322;&#26500;&#22270;&#26469;&#24314;&#27169;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#25104;&#20026;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22270;&#32467;&#26500;&#22312;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#20013;&#24182;&#19981;&#33258;&#28982;&#12290;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#30340;&#22270;&#26159;&#25163;&#21160;&#26500;&#24314;&#30340;&#65292;&#36825;&#26082;&#22256;&#38590;&#21448;&#27425;&#20248;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#65288;i&#65289;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#21270;&#22270;&#26500;&#24314;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#24322;&#26500;&#22270;&#36328;&#27169;&#24577;&#32593;&#32476;&#65288;HGCN&#65289;&#65292;&#23427;&#23398;&#20064;&#36328;&#27169;&#24577;&#36793;&#32536;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#22240;&#20026;&#23427;&#26159;&#21442;&#25968;&#21270;&#26500;&#24314;&#30340;&#65292;&#32780;&#21487;&#23398;&#20064;&#30340;&#36328;&#27169;&#24577;&#36793;&#32536;&#26377;&#25928;&#22320;&#36830;&#25509;&#20102;&#36328;&#27169;&#24577;&#30340;&#30456;&#20851;&#33410;&#28857;&#12290;&#22312;&#19968;&#20010;&#22823;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;AudioSet&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#65288;0.53&#24179;&#22343;&#31934;&#24230;&#65289;&#65292;&#20248;&#20110;transfo&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graphs provide a compact, efficient, and scalable way to model data involving multiple disparate modalities. This makes modeling audiovisual data using heterogeneous graphs an attractive option. However, graph structure does not appear naturally in audiovisual data. Graphs for audiovisual data are constructed manually which is both difficult and sub-optimal. In this work, we address this problem by (i) proposing a parametric graph construction strategy for the intra-modal edges, and (ii) learning the crossmodal edges. To this end, we develop a new model, heterogeneous graph crossmodal network (HGCN) that learns the crossmodal edges. Our proposed model can adapt to various spatial and temporal scales owing to its parametric construction, while the learnable crossmodal edges effectively connect the relevant nodes across modalities. Experiments on a large benchmark dataset (AudioSet) show that our model is state-of-the-art (0.53 mean average precision), outperforming transfo
&lt;/p&gt;</description></item><item><title>Prismer&#26159;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.02506</link><description>&lt;p&gt;
Prismer: &#19968;&#31181;&#20855;&#26377;&#19987;&#23478;&#38598;&#21512;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prismer: A Vision-Language Model with An Ensemble of Experts. (arXiv:2303.02506v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02506
&lt;/p&gt;
&lt;p&gt;
Prismer&#26159;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prismer is a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts, achieving fine-tuned and few-shot learning performance competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#23427;&#20204;&#38656;&#35201;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24222;&#22823;&#30340;&#27169;&#22411;&#12290;&#20316;&#20026;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Prismer&#65292;&#19968;&#31181;&#25968;&#25454;&#21644;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#32452;&#39046;&#22495;&#19987;&#23478;&#30340;&#38598;&#21512;&#12290;Prismer&#21482;&#38656;&#35201;&#35757;&#32451;&#23569;&#37327;&#32452;&#20214;&#65292;&#22823;&#37096;&#20998;&#32593;&#32476;&#26435;&#37325;&#20174;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#39046;&#22495;&#19987;&#23478;&#20013;&#32487;&#25215;&#65292;&#24182;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#25345;&#20923;&#32467;&#29366;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Prismer&#21487;&#20197;&#26377;&#25928;&#22320;&#27719;&#38598;&#36825;&#20123;&#19987;&#23478;&#30693;&#35782;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Prismer&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#31454;&#20105;&#30340;&#24494;&#35843;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#23569;&#33267;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/NVlabs/prismer&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent vision-language models have shown impressive multi-modal generation capabilities. However, typically they require training huge models on massive datasets. As a more scalable alternative, we introduce Prismer, a data- and parameter-efficient vision-language model that leverages an ensemble of domain experts. Prismer only requires training of a small number of components, with the majority of network weights inherited from readily-available, pre-trained domain experts, and kept frozen during training. By leveraging experts from a wide range of domains, we show that Prismer can efficiently pool this expert knowledge and adapt it to various vision-language reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned and few-shot learning performance which is competitive with current state-of-the-art models, whilst requiring up to two orders of magnitude less training data. Code is available at https://github.com/NVlabs/prismer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02265</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning to Influence Human Behavior with Offline Reinforcement Learning. (arXiv:2303.02265v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method of learning to influence human behavior through offline reinforcement learning, which can improve human performance in collaborative tasks.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#19982;&#20154;&#31867;&#20114;&#21160;&#26159;&#26368;&#22797;&#26434;&#30340;&#35774;&#32622;&#20043;&#19968;&#65292;&#22240;&#20026;&#20154;&#31867;&#24448;&#24448;&#30001;&#20110;&#22797;&#26434;&#30340;&#20559;&#35265;&#32780;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#20195;&#29702;&#26368;&#32456;&#20250;&#24433;&#21709;&#36825;&#20123;&#20154;&#25152;&#37319;&#21462;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24433;&#21709;&#26469;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#38543;&#30528;&#20219;&#21153;&#30340;&#23637;&#24320;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#19982;&#20154;&#21592;&#36827;&#34892;&#22312;&#32447;&#22521;&#35757;&#65288;&#36825;&#24448;&#24448;&#22826;&#26114;&#36149;&#21644;&#19981;&#23433;&#20840;&#65289;&#65292;&#20063;&#19981;&#20551;&#35774;&#26377;&#39640;&#20445;&#30495;&#24230;&#29615;&#22659;&#27169;&#25311;&#22120;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#65292;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#20219;&#21153;&#22870;&#21169;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#23398;&#20064;&#32452;&#21512;&#34892;&#20026;&#30340;&#32452;&#20214;&#65292;&#24182;&#21457;&#29616;&#23548;&#33268;&#26356;&#29702;&#24819;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#34892;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31163;&#32447;RL&#21487;&#20197;&#23398;&#20064;&#31574;&#30053;&#26469;&#24433;&#21709;&#21644;&#25913;&#21892;&#20154;&#31867;&#34892;&#20026;&#65292;&#23613;&#31649;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#19982;&#20154;&#31867;&#30340;&#26399;&#26395;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.02045</link><description>&lt;p&gt;
&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Estimation by Fisher Information-based Evidential Deep Learning. (arXiv:2303.02045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Fisher Information-based Evidential Deep Learning method to address the problem of over-penalization and hindrance in evidence learning for high data uncertainty samples annotated with one-hot labels.
&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26159;&#20351;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#35777;&#25454;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#23558;&#32593;&#32476;&#36755;&#20986;&#35270;&#20026;&#35777;&#25454;&#26469;&#21442;&#25968;&#21270;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#65292;&#26126;&#30830;&#32771;&#34385;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#26679;&#26412;&#20294;&#27880;&#37322;&#20026;one-hot&#26631;&#31614;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#38169;&#35823;&#26631;&#35760;&#30340;&#31867;&#21035;&#30340;&#35777;&#25454;&#23398;&#20064;&#36807;&#31243;&#20250;&#34987;&#36807;&#24230;&#24809;&#32602;&#24182;&#21463;&#21040;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;Fisher&#20449;&#24687;&#30340;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;$\mathcal{I}$-EDL&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;Fisher&#20449;&#24687;&#30697;&#38453;&#65288;FIM&#65289;&#26469;&#34913;&#37327;&#27599;&#20010;&#26679;&#26412;&#25152;&#25658;&#24102;&#30340;&#35777;&#25454;&#30340;&#20449;&#24687;&#37327;&#65292;&#26681;&#25454;&#36825;&#20010;&#20449;&#24687;&#37327;&#65292;&#25105;&#20204;&#21487;&#20197;&#21160;&#24577;&#22320;&#37325;&#26032;&#21152;&#26435;&#30446;&#26631;&#25439;&#22833;&#39033;&#65292;&#20351;&#32593;&#32476;&#26356;&#21152;&#19987;&#27880;&#20110;&#19981;&#30830;&#23450;&#31867;&#21035;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36890;&#36807;&#20248;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is a key factor that makes deep learning reliable in practical applications. Recently proposed evidential neural networks explicitly account for different uncertainties by treating the network's outputs as evidence to parameterize the Dirichlet distribution, and achieve impressive performance in uncertainty estimation. However, for high data uncertainty samples but annotated with the one-hot label, the evidence-learning process for those mislabeled classes is over-penalized and remains hindered. To address this problem, we propose a novel method, Fisher Information-based Evidential Deep Learning ($\mathcal{I}$-EDL). In particular, we introduce Fisher Information Matrix (FIM) to measure the informativeness of evidence carried by each sample, according to which we can dynamically reweight the objective loss terms to make the network more focused on the representation learning of uncertain classes. The generalization ability of our network is further improved by opt
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#21487;&#27979;&#35797;&#23398;&#20064;&#27169;&#22411;&#20013;&#23398;&#20064;&#21322;&#31354;&#38388;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#22312;&#20219;&#20309;&#24378;&#23545;&#25968;&#20985;&#30446;&#26631;&#20998;&#24067;&#19979;&#20855;&#26377;&#65288;&#20449;&#24687;&#29702;&#35770;&#19978;&#26368;&#20248;&#30340;&#65289;&#35823;&#24046;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2302.14853</link><description>&lt;p&gt;
&#21322;&#31354;&#38388;&#30340;&#39640;&#25928;&#27979;&#35797;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Efficient Tester-Learner for Halfspaces. (arXiv:2302.14853v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14853
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#21487;&#27979;&#35797;&#23398;&#20064;&#27169;&#22411;&#20013;&#23398;&#20064;&#21322;&#31354;&#38388;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#22312;&#20219;&#20309;&#24378;&#23545;&#25968;&#20985;&#30446;&#26631;&#20998;&#24067;&#19979;&#20855;&#26377;&#65288;&#20449;&#24687;&#29702;&#35770;&#19978;&#26368;&#20248;&#30340;&#65289;&#35823;&#24046;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the first efficient algorithm for learning halfspaces in the testable learning model, which runs in polynomial time and outputs a hypothesis with (information-theoretically optimal) error for any strongly log-concave target distribution.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;Rubinfeld&#21644;Vasilyan&#65288;2023&#65289;&#26368;&#36817;&#23450;&#20041;&#30340;&#21487;&#27979;&#35797;&#23398;&#20064;&#27169;&#22411;&#20013;&#23398;&#20064;&#21322;&#31354;&#38388;&#30340;&#39640;&#25928;&#31639;&#27861;&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#24403;&#35757;&#32451;&#38598;&#36890;&#36807;&#30456;&#20851;&#27979;&#35797;&#26102;&#65292;&#23398;&#20064;&#32773;&#35777;&#26126;&#20854;&#36755;&#20986;&#20551;&#35774;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#26368;&#20248;&#65292;&#24182;&#19988;&#20174;&#26576;&#20123;&#30446;&#26631;&#20998;&#24067;&#65288;&#20363;&#22914;&#39640;&#26031;&#20998;&#24067;&#65289;&#20013;&#25277;&#21462;&#30340;&#35757;&#32451;&#38598;&#24517;&#39035;&#36890;&#36807;&#27979;&#35797;&#12290;&#36825;&#20010;&#27169;&#22411;&#27604;&#20998;&#24067;&#29305;&#23450;&#30340;&#19981;&#21487;&#30693;&#25110;Massart&#22122;&#22768;&#27169;&#22411;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#22914;&#26524;&#20998;&#24067;&#20551;&#35774;&#19981;&#25104;&#31435;&#65292;&#23398;&#20064;&#32773;&#21487;&#20197;&#20219;&#24847;&#22833;&#36133;&#12290;&#25105;&#20204;&#32771;&#34385;&#30446;&#26631;&#20998;&#24067;&#20026;&#39640;&#26031;&#20998;&#24067;&#65288;&#25110;&#26356;&#19968;&#33324;&#30340;&#20219;&#20309;&#24378;&#23545;&#25968;&#20985;&#20998;&#24067;&#65289;&#30340;$d$&#32500;&#24773;&#20917;&#65292;&#22122;&#22768;&#27169;&#22411;&#20026;Massart&#25110;&#23545;&#25239;&#24615;&#65288;&#19981;&#21487;&#30693;&#65289;&#12290;&#23545;&#20110;Massart&#22122;&#22768;&#65292;&#25105;&#20204;&#30340;&#27979;&#35797;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#36816;&#34892;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#22312;&#20219;&#20309;&#24378;&#23545;&#25968;&#20985;&#30446;&#26631;&#20998;&#24067;&#19979;&#20855;&#26377;&#65288;&#20449;&#24687;&#29702;&#35770;&#19978;&#26368;&#20248;&#30340;&#65289;&#35823;&#24046;$\mathsf{opt}+\epsilon$&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give the first efficient algorithm for learning halfspaces in the testable learning model recently defined by Rubinfeld and Vasilyan (2023). In this model, a learner certifies that the accuracy of its output hypothesis is near optimal whenever the training set passes an associated test, and training sets drawn from some target distribution -- e.g., the Gaussian -- must pass the test. This model is more challenging than distribution-specific agnostic or Massart noise models where the learner is allowed to fail arbitrarily if the distributional assumption does not hold.  We consider the setting where the target distribution is Gaussian (or more generally any strongly log-concave distribution) in $d$ dimensions and the noise model is either Massart or adversarial (agnostic). For Massart noise, our tester-learner runs in polynomial time and outputs a hypothesis with (information-theoretically optimal) error $\mathsf{opt} + \epsilon$ for any strongly log-concave target distribution. For 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;ASOS&#25910;&#38598;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#23578;&#38646;&#21806;&#29983;&#24577;&#31995;&#32479;&#20013;&#39044;&#27979;&#23458;&#25143;&#36864;&#36135;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#36864;&#36135;&#39044;&#27979;&#20998;&#31867;&#20219;&#21153;&#30340;F1&#20998;&#25968;&#33267;0.792&#65292;&#36825;&#27604;&#20854;&#20182;&#27169;&#22411;&#26377;&#25152;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.14096</link><description>&lt;p&gt;
&#19968;&#20221;&#29992;&#20110;&#23398;&#20064;&#22270;&#34920;&#31034;&#20197;&#39044;&#27979;&#26102;&#23578;&#38646;&#21806;&#23458;&#25143;&#36864;&#36135;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Dataset for Learning Graph Representations to Predict Customer Returns in Fashion Retail. (arXiv:2302.14096v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14096
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;ASOS&#25910;&#38598;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#23578;&#38646;&#21806;&#29983;&#24577;&#31995;&#32479;&#20013;&#39044;&#27979;&#23458;&#25143;&#36864;&#36135;&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#36864;&#36135;&#39044;&#27979;&#20998;&#31867;&#20219;&#21153;&#30340;F1&#20998;&#25968;&#33267;0.792&#65292;&#36825;&#27604;&#20854;&#20182;&#27169;&#22411;&#26377;&#25152;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel dataset collected by ASOS for predicting customer returns in a fashion retail ecosystem. The researchers use Graph Representation Learning to improve the F1-score of the return prediction classification task to 0.792, outperforming other models.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;ASOS&#65288;&#19968;&#23478;&#20027;&#35201;&#30340;&#22312;&#32447;&#26102;&#23578;&#38646;&#21806;&#21830;&#65289;&#25910;&#38598;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#22312;&#26102;&#23578;&#38646;&#21806;&#29983;&#24577;&#31995;&#32479;&#20013;&#39044;&#27979;&#23458;&#25143;&#36864;&#36135;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#21457;&#24067;&#36825;&#20010;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24076;&#26395;&#28608;&#21457;&#30740;&#31350;&#31038;&#21306;&#21644;&#26102;&#23578;&#34892;&#19994;&#20043;&#38388;&#30340;&#36827;&#19968;&#27493;&#21512;&#20316;&#12290;&#25105;&#20204;&#39318;&#20808;&#25506;&#35752;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;&#65292;&#37325;&#28857;&#20851;&#27880;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#24212;&#29992;&#65292;&#20197;&#21033;&#29992;&#33258;&#28982;&#25968;&#25454;&#32467;&#26500;&#24182;&#25552;&#20379;&#23545;&#25968;&#25454;&#20013;&#29305;&#23450;&#29305;&#24449;&#30340;&#32479;&#35745;&#27934;&#23519;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#36864;&#36135;&#39044;&#27979;&#20998;&#31867;&#20219;&#21153;&#30340;&#31034;&#20363;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20123;&#22522;&#32447;&#27169;&#22411;&#65288;&#21363;&#27809;&#26377;&#20013;&#38388;&#34920;&#31034;&#23398;&#20064;&#27493;&#39588;&#65289;&#21644;&#22522;&#20110;&#22270;&#34920;&#31034;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19979;&#28216;&#36864;&#36135;&#39044;&#27979;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#25214;&#21040;F1&#20998;&#25968;&#20026;0.792&#65292;&#36825;&#27604;&#26412;&#25991;&#35752;&#35770;&#30340;&#20854;&#20182;&#27169;&#22411;&#26377;&#25152;&#25913;&#36827;&#12290;&#38500;&#20102;&#36825;&#20010;&#22686;&#21152;&#30340;F1&#20998;&#25968;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;l
&lt;/p&gt;
&lt;p&gt;
We present a novel dataset collected by ASOS (a major online fashion retailer) to address the challenge of predicting customer returns in a fashion retail ecosystem. With the release of this substantial dataset we hope to motivate further collaboration between research communities and the fashion industry. We first explore the structure of this dataset with a focus on the application of Graph Representation Learning in order to exploit the natural data structure and provide statistical insights into particular features within the data. In addition to this, we show examples of a return prediction classification task with a selection of baseline models (i.e. with no intermediate representation learning step) and a graph representation based model. We show that in a downstream return prediction classification task, an F1-score of 0.792 can be found using a Graph Neural Network (GNN), improving upon other models discussed in this work. Alongside this increased F1-score, we also present a l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#25105;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#35838;&#31243;&#20266;&#26631;&#31614;&#29992;&#20110;&#34920;&#26684;&#39046;&#22495;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#23427;&#35268;&#33539;&#21270;&#20102;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.14013</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#24102;&#26377;&#27491;&#21017;&#21270;&#20266;&#26631;&#31614;&#30340;&#34920;&#26684;&#25968;&#25454;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Revisiting Self-Training with Regularized Pseudo-Labeling for Tabular Data. (arXiv:2302.14013v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#25105;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#20102;&#35838;&#31243;&#20266;&#26631;&#31614;&#29992;&#20110;&#34920;&#26684;&#39046;&#22495;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#23427;&#35268;&#33539;&#21270;&#20102;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits self-training and introduces curriculum pseudo-labeling for tabular data. A novel pseudo-labeling approach is proposed to regularize the confidence scores of pseudo-labels generated from unlabeled data.
&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#25171;&#30772;&#20102;&#38271;&#26399;&#20197;&#26469;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#26080;&#20851;&#24615;&#30340;&#20449;&#20208;&#12290;&#34429;&#28982;&#22312;&#21508;&#31181;&#25968;&#25454;&#19978;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;&#27809;&#26377;&#19968;&#31181;&#20027;&#23548;&#30340;&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#34920;&#26684;&#25968;&#25454;&#65288;&#21363;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#36866;&#24403;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#25105;&#35757;&#32451;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26550;&#26500;&#8212;&#8212;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65292;&#24182;&#24341;&#20837;&#20102;&#35838;&#31243;&#20266;&#26631;&#31614;&#65288;&#19968;&#31181;&#22270;&#20687;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;&#65289;&#29992;&#20110;&#34920;&#26684;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20266;&#26631;&#31614;&#25216;&#26415;&#22312;&#35745;&#31639;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26102;&#19981;&#33021;&#20445;&#35777;&#32858;&#31867;&#20551;&#35774;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#23427;&#35268;&#33539;&#21270;&#20102;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in semi- and self-supervised learning has caused a rift in the long-held belief about the need for an enormous amount of labeled data for machine learning and the irrelevancy of unlabeled data. Although it has been successful in various data, there is no dominant semi- and self-supervised learning method that can be generalized for tabular data (i.e. most of the existing methods require appropriate tabular datasets and architectures). In this paper, we revisit self-training which can be applied to any kind of algorithm including the most widely used architecture, gradient boosting decision tree, and introduce curriculum pseudo-labeling (a state-of-the-art pseudo-labeling technique in image) for a tabular domain. Furthermore, existing pseudo-labeling techniques do not assure the cluster assumption when computing confidence scores of pseudo-labels generated from unlabeled data. To overcome this issue, we propose a novel pseudo-labeling approach that regularizes the confid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TopExpert&#65292;&#21033;&#29992;&#25299;&#25169;&#29305;&#23450;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;&#31216;&#20026;&#19987;&#23478;&#65289;&#65292;&#27599;&#20010;&#19987;&#23478;&#36127;&#36131;&#27599;&#20010;&#20849;&#20139;&#31867;&#20284;&#25299;&#25169;&#35821;&#20041;&#30340;&#20998;&#23376;&#32452;&#65292;&#20197;&#25552;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.13693</link><description>&lt;p&gt;
&#23398;&#20064;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#25299;&#25169;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Learning Topology-Specific Experts for Molecular Property Prediction. (arXiv:2302.13693v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TopExpert&#65292;&#21033;&#29992;&#25299;&#25169;&#29305;&#23450;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;&#31216;&#20026;&#19987;&#23478;&#65289;&#65292;&#27599;&#20010;&#19987;&#23478;&#36127;&#36131;&#27599;&#20010;&#20849;&#20139;&#31867;&#20284;&#25299;&#25169;&#35821;&#20041;&#30340;&#20998;&#23376;&#32452;&#65292;&#20197;&#25552;&#39640;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes TopExpert, which leverages topology-specific prediction models (referred to as experts) to improve the performance of molecular property prediction by assigning each expert to a molecular group sharing similar topological semantics.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#39044;&#27979;&#20998;&#23376;&#23646;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#30340;&#26368;&#32463;&#20856;&#30340;&#21270;&#23398;&#20449;&#24687;&#23398;&#20219;&#21153;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#26377;&#25928;&#65292;&#20294;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;&#20026;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#30340;&#22810;&#31181;&#20998;&#23376;&#35757;&#32451;&#21333;&#20010;GNN&#27169;&#22411;&#20250;&#38480;&#21046;&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TopExpert&#65292;&#21033;&#29992;&#25299;&#25169;&#29305;&#23450;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;&#31216;&#20026;&#19987;&#23478;&#65289;&#65292;&#27599;&#20010;&#19987;&#23478;&#36127;&#36131;&#27599;&#20010;&#20849;&#20139;&#31867;&#20284;&#25299;&#25169;&#35821;&#20041;&#30340;&#20998;&#23376;&#32452;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#27599;&#20010;&#19987;&#23478;&#22312;&#19982;&#20854;&#30456;&#24212;&#30340;&#25299;&#25169;&#32452;&#19968;&#36215;&#35757;&#32451;&#26102;&#23398;&#20064;&#29305;&#23450;&#20110;&#25299;&#25169;&#30340;&#21028;&#21035;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#25353;&#20854;&#25299;&#25169;&#27169;&#24335;&#23545;&#20998;&#23376;&#36827;&#34892;&#20998;&#32452;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#38376;&#25511;&#27169;&#22359;&#65292;&#23558;&#36755;&#20837;&#20998;&#23376;&#20998;&#37197;&#21040;&#20854;&#20013;&#19968;&#20010;&#32858;&#31867;&#20013;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#33258;&#30417;&#30563;&#36827;&#19968;&#27493;&#20248;&#21270;&#38376;&#25511;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, graph neural networks (GNNs) have been successfully applied to predicting molecular properties, which is one of the most classical cheminformatics tasks with various applications. Despite their effectiveness, we empirically observe that training a single GNN model for diverse molecules with distinct structural patterns limits its prediction performance. In this paper, motivated by this observation, we propose TopExpert to leverage topology-specific prediction models (referred to as experts), each of which is responsible for each molecular group sharing similar topological semantics. That is, each expert learns topology-specific discriminative features while being trained with its corresponding topological group. To tackle the key challenge of grouping molecules by their topological patterns, we introduce a clustering-based gating module that assigns an input molecule into one of the clusters and further optimizes the gating module with two different types of self-supervision:
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UnbiasedNets&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;K-means&#32858;&#31867;&#21644;NN&#30340;&#22122;&#22768;&#23481;&#24525;&#24230;&#26469;&#20351;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22810;&#26679;&#21270;&#65292;&#29983;&#25104;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#24182;&#20943;&#23569;&#25968;&#25454;&#38598;&#20869;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2302.12538</link><description>&lt;p&gt;
UnbiasedNets&#65306;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#40065;&#26834;&#24615;&#20559;&#24046;&#32531;&#35299;&#30340;&#25968;&#25454;&#38598;&#22810;&#26679;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UnbiasedNets: A Dataset Diversification Framework for Robustness Bias Alleviation in Neural Networks. (arXiv:2302.12538v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UnbiasedNets&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;K-means&#32858;&#31867;&#21644;NN&#30340;&#22122;&#22768;&#23481;&#24525;&#24230;&#26469;&#20351;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22810;&#26679;&#21270;&#65292;&#29983;&#25104;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#24182;&#20943;&#23569;&#25968;&#25454;&#38598;&#20869;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the UnbiasedNets framework, which leverages K-means clustering and the NN's noise tolerance to diversify the given training dataset, generating balanced datasets and reducing bias within them.
&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#20197;&#27979;&#35797;&#20934;&#30830;&#24615;&#20026;&#25351;&#26631;&#65289;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#26368;&#20934;&#30830;&#30340;NN&#20063;&#21487;&#33021;&#23545;&#29305;&#23450;&#36755;&#20986;&#20998;&#31867;&#23384;&#22312;&#20559;&#24046;&#65292;&#36825;&#26159;&#30001;&#20110;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22266;&#26377;&#20559;&#24046;&#25152;&#33268;&#65292;&#36825;&#21487;&#33021;&#20250;&#20256;&#25773;&#21040;&#23454;&#38469;&#23454;&#29616;&#20013;&#12290;&#26412;&#25991;&#28041;&#21450;&#40065;&#26834;&#24615;&#20559;&#24046;&#65292;&#21363;&#36890;&#36807;&#23545;&#26576;&#20010;&#36755;&#20986;&#31867;&#21035;&#30340;&#22122;&#22768;&#20855;&#26377;&#26174;&#30528;&#30340;&#22823;&#40065;&#26834;&#24615;&#65292;&#19982;&#20854;&#20313;&#36755;&#20986;&#31867;&#21035;&#30456;&#27604;&#65292;&#35757;&#32451;&#30340;NN&#25152;&#34920;&#29616;&#20986;&#30340;&#20559;&#24046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20559;&#24046;&#26469;&#33258;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#25152;&#26377;&#36755;&#20986;&#31867;&#21035;&#26410;&#34987;&#24179;&#31561;&#22320;&#34920;&#31034;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UnbiasedNets&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;K-means&#32858;&#31867;&#21644;NN&#30340;&#22122;&#22768;&#23481;&#24525;&#24230;&#26469;&#20351;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#22810;&#26679;&#21270;&#65292;&#21363;&#20351;&#26159;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#23558;&#29983;&#25104;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#24182;&#20943;&#23569;&#25968;&#25454;&#38598;&#20869;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance of trained neural network (NN) models, in terms of testing accuracy, has improved remarkably over the past several years, especially with the advent of deep learning. However, even the most accurate NNs can be biased toward a specific output classification due to the inherent bias in the available training datasets, which may propagate to the real-world implementations. This paper deals with the robustness bias, i.e., the bias exhibited by the trained NN by having a significantly large robustness to noise for a certain output class, as compared to the remaining output classes. The bias is shown to result from imbalanced datasets, i.e., the datasets where all output classes are not equally represented. Towards this, we propose the UnbiasedNets framework, which leverages K-means clustering and the NN's noise tolerance to diversify the given training dataset, even from relatively smaller datasets. This generates balanced datasets and reduces the bias within the datasets themse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CreamFL&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20855;&#26377;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#30340;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#21482;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20256;&#36882;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2302.08888</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65306;&#23545;&#27604;&#34920;&#31034;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multimodal Federated Learning via Contrastive Representation Ensemble. (arXiv:2302.08888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CreamFL&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20855;&#26377;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#30340;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#21482;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20256;&#36882;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a multimodal federated learning framework called CreamFL, which enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#31227;&#21160;&#31995;&#32479;&#21644;&#29289;&#32852;&#32593;&#22522;&#30784;&#35774;&#26045;&#19978;&#30340;&#22810;&#23186;&#20307;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#21033;&#29992;&#36825;&#20123;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#32780;&#19981;&#36829;&#21453;&#29992;&#25143;&#38544;&#31169;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#38598;&#20013;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#38544;&#31169;&#24847;&#35782;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;FL&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#21333;&#27169;&#24577;&#32423;&#21035;&#30340;&#27169;&#22411;&#32858;&#21512;&#65292;&#36825;&#38480;&#21046;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#22312;&#27599;&#20010;&#27169;&#24577;&#19978;&#20855;&#26377;&#30456;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#20840;&#23616;&#27169;&#22411;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#25968;&#25454;&#23481;&#37327;&#65292;&#26356;&#19981;&#29992;&#35828;&#20219;&#21153;&#22810;&#26679;&#24615;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#34920;&#31034;&#38598;&#25104;&#21644;&#22810;&#27169;&#24577;FL&#32858;&#21512;&#65288;CreamFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20174;&#20855;&#26377;&#24322;&#26500;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#30340;&#23458;&#25143;&#31471;&#20013;&#35757;&#32451;&#26356;&#22823;&#30340;&#26381;&#21153;&#22120;&#27169;&#22411;&#65292;&#21516;&#26102;&#21482;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20256;&#36882;&#30693;&#35782;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#34701;&#21512;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#23616;-
&lt;/p&gt;
&lt;p&gt;
With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL), a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#27979;&#37327;&#25216;&#26415;&#65292;&#29992;&#20110;&#30417;&#27979;&#28895;&#22257;&#28895;&#32701;&#24182;&#36827;&#34892;&#38271;&#26399;&#23454;&#26102;&#27979;&#37327;&#12290;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#25913;&#36827;&#30340;Mask R-CNN&#26469;&#35782;&#21035;&#28895;&#32701;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#27861;&#23558;&#28176;&#36817;&#27169;&#22411;&#25311;&#21512;&#21040;&#36793;&#30028;&#20013;&#24515;&#32447;&#20013;&#65292;&#20197;&#20272;&#35745;&#28895;&#22257;&#28895;&#32701;&#19978;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.07416</link><description>&lt;p&gt;
&#24037;&#19994;&#29615;&#22659;&#20013;&#28895;&#32701;&#19978;&#21319;&#27979;&#37327;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Convolutional Neural Network for Plume Rise Measurements in Industrial Environments. (arXiv:2302.07416v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#27979;&#37327;&#25216;&#26415;&#65292;&#29992;&#20110;&#30417;&#27979;&#28895;&#22257;&#28895;&#32701;&#24182;&#36827;&#34892;&#38271;&#26399;&#23454;&#26102;&#27979;&#37327;&#12290;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;&#25913;&#36827;&#30340;Mask R-CNN&#26469;&#35782;&#21035;&#28895;&#32701;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#27861;&#23558;&#28176;&#36817;&#27169;&#22411;&#25311;&#21512;&#21040;&#36793;&#30028;&#20013;&#24515;&#32447;&#20013;&#65292;&#20197;&#20272;&#35745;&#28895;&#22257;&#28895;&#32701;&#19978;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a low-cost measurement technology to monitor smokestack plume rise and make long-term, real-time measurements. A two-stage method based on Deep Convolutional Neural Networks (DCNNs) is developed, where the first stage uses an improved Mask R-CNN to recognize the plume and the second stage uses least squares to fit an asymptotic model to the plume boundaries centerline to estimate the smokestack plume rise.
&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#28895;&#32701;&#20113;&#39640;&#24230;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#20840;&#29699;&#27668;&#20505;&#27169;&#22411;&#12290;&#28895;&#22257;&#28895;&#32701;&#19978;&#21319;&#26159;&#28895;&#32701;&#22312;&#20854;&#21160;&#37327;&#32791;&#25955;&#24182;&#19988;&#28895;&#32701;&#21644;&#29615;&#22659;&#28201;&#24230;&#30456;&#31561;&#26102;&#21521;&#19979;&#39128;&#31227;&#30340;&#24658;&#23450;&#39640;&#24230;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#31354;&#27668;&#36136;&#37327;&#27169;&#22411;&#20351;&#29992;&#19981;&#21516;&#30340;&#21442;&#25968;&#21270;&#26469;&#39044;&#27979;&#28895;&#22257;&#28895;&#32701;&#19978;&#21319;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#39564;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#27979;&#37327;&#25216;&#26415;&#65292;&#29992;&#20110;&#30417;&#27979;&#28895;&#22257;&#28895;&#32701;&#24182;&#36827;&#34892;&#38271;&#26399;&#23454;&#26102;&#27979;&#37327;&#12290;&#20026;&#27492;&#65292;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#24212;&#29992;&#25913;&#36827;&#30340;Mask R-CNN&#65292;&#31216;&#20026;Deep Plume Rise Network&#65288;DPRNet&#65289;&#65292;&#26469;&#35782;&#21035;&#28895;&#32701;&#12290;&#36825;&#37324;&#65292;&#20998;&#21035;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#20998;&#26512;&#21644;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#26816;&#27979;&#28895;&#32701;&#36793;&#30028;&#24182;&#23558;&#28176;&#36817;&#27169;&#22411;&#25311;&#21512;&#21040;&#36793;&#30028;&#20013;&#24515;&#32447;&#20013;&#12290;&#35813;&#27169;&#22411;&#30340;&#20851;&#38190;&#28857;&#30340;y&#20998;&#37327;&#22352;&#26631;&#34987;&#35748;&#20026;&#26159;&#28895;&#22257;&#28895;&#32701;&#19978;&#21319;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;
&lt;/p&gt;
&lt;p&gt;
Estimating Plume Cloud (PC) height is essential for various applications, such as global climate models. Smokestack Plume Rise (PR) is the constant height at which the PC is carried downwind as its momentum dissipates and the PC and the ambient temperatures equalize. Although different parameterizations are used in most air-quality models to predict PR, they have yet to be verified thoroughly. This paper proposes a low-cost measurement technology to monitor smokestack PCs and make long-term, real-time measurements of PR. For this purpose, a two-stage method is developed based on Deep Convolutional Neural Networks (DCNNs). In the first stage, an improved Mask R-CNN, called Deep Plume Rise Network (DPRNet), is applied to recognize the PC. Here, image processing analyses and least squares, respectively, are used to detect PC boundaries and fit an asymptotic model into the boundaries centerline. The y-component coordinate of this model's critical point is considered PR. In the second stage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24378;&#35843;&#22312;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20559;&#24494;&#20998;&#26041;&#31243;&#25511;&#21046;&#26102;&#20351;&#29992;&#21160;&#24577;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#20351;&#29992;&#21367;&#31215;LSTM&#20316;&#20026;&#24102;&#26377;&#28608;&#21169;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20174;&#30495;&#23454;&#31995;&#32479;&#20013;&#37319;&#26679;&#25152;&#38656;&#30340;&#24635;&#25968;&#25454;&#37327;&#12290;&#36845;&#20195;&#26356;&#26032;&#27169;&#22411;&#23545;&#20110;&#36991;&#20813;RL&#35757;&#32451;&#20013;&#30340;&#20559;&#24046;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2302.07160</link><description>&lt;p&gt;
&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26679;&#26412;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning a model is paramount for sample efficiency in reinforcement learning control of PDEs. (arXiv:2302.07160v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#22312;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20559;&#24494;&#20998;&#26041;&#31243;&#25511;&#21046;&#26102;&#20351;&#29992;&#21160;&#24577;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#20351;&#29992;&#21367;&#31215;LSTM&#20316;&#20026;&#24102;&#26377;&#28608;&#21169;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20174;&#30495;&#23454;&#31995;&#32479;&#20013;&#37319;&#26679;&#25152;&#38656;&#30340;&#24635;&#25968;&#25454;&#37327;&#12290;&#36845;&#20195;&#26356;&#26032;&#27169;&#22411;&#23545;&#20110;&#36991;&#20813;RL&#35757;&#32451;&#20013;&#30340;&#20559;&#24046;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper emphasizes the importance of using dynamic models when using reinforcement learning for feedback control of partial differential equations. Using a convolutional LSTM as a data-driven surrogate model with actuation significantly reduces the total amount of required data sampled from the real system. Iteratively updating the model is crucial to avoid biases in the RL training.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24378;&#35843;&#22312;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#25511;&#21046;&#30340;&#21453;&#39304;&#25511;&#21046;&#26102;&#20351;&#29992;&#21160;&#24577;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#25105;&#20204;&#22312;RL&#20013;&#30475;&#21040;&#30340;&#24040;&#22823;&#25215;&#35834;&#19982;&#22797;&#26434;&#24037;&#31243;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#30340;&#24040;&#22823;&#38656;&#27714;&#20197;&#21450;&#32570;&#20047;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#21367;&#31215;LSTM&#20316;&#20026;&#24102;&#26377;&#28608;&#21169;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#26469;&#35299;&#20915;&#31532;&#19968;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#35757;&#32451;RL&#20195;&#29702;&#30340;&#21516;&#26102;&#24182;&#34892;&#23398;&#20064;&#19968;&#20010;&#24102;&#26377;&#28608;&#21169;&#30340;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20174;&#30495;&#23454;&#31995;&#32479;&#20013;&#37319;&#26679;&#25152;&#38656;&#30340;&#24635;&#25968;&#25454;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36845;&#20195;&#26356;&#26032;&#27169;&#22411;&#23545;&#20110;&#36991;&#20813;RL&#35757;&#32451;&#20013;&#30340;&#20559;&#24046;&#38750;&#24120;&#37325;&#35201;&#12290;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;&#25581;&#31034;&#20102;&#24314;&#27169;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#20351;&#29992;&#28151;&#27788;Kuramoto-Sivashinsky&#26041;&#31243;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this paper is to make a strong point for the usage of dynamical models when using reinforcement learning (RL) for feedback control of dynamical systems governed by partial differential equations (PDEs). To breach the gap between the immense promises we see in RL and the applicability in complex engineering systems, the main challenges are the massive requirements in terms of the training data, as well as the lack of performance guarantees. We present a solution for the first issue using a data-driven surrogate model in the form of a convolutional LSTM with actuation. We demonstrate that learning an actuated model in parallel to training the RL agent significantly reduces the total amount of required data sampled from the real system. Furthermore, we show that iteratively updating the model is of major importance to avoid biases in the RL training. Detailed ablation studies reveal the most important ingredients of the modeling process. We use the chaotic Kuramoto-Sivashinsky
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;LightSolver&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#39046;&#20808;&#30340;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#22312;MAX-2-SAT&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LightSolver&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#23567;&#30340;&#26368;&#20248;&#35299;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2302.06926</link><description>&lt;p&gt;
Lightsolver&#25361;&#25112;&#39046;&#20808;&#30340;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#35299;&#20915;Max-2-SAT&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Lightsolver challenges a leading deep learning solver for Max-2-SAT problems. (arXiv:2302.06926v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;LightSolver&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#39046;&#20808;&#30340;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#22312;MAX-2-SAT&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LightSolver&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#23567;&#30340;&#26368;&#20248;&#35299;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper compares LightSolver's quantum-inspired algorithm to a leading deep-learning solver for the MAX-2-SAT problem, and shows that LightSolver achieves significantly smaller time-to-optimal-solution compared to a state-of-the-art deep-learning algorithm.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;2-SAT&#38382;&#39064;&#65288;MAX-2-SAT&#65289;&#26159;&#19968;&#31181;&#24050;&#30693;&#20026;NP&#38590;&#30340;&#32452;&#21512;&#20915;&#31574;&#38382;&#39064;&#12290;&#26412;&#25991;&#27604;&#36739;&#20102;LightSolver&#30340;&#37327;&#23376;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#39046;&#20808;&#30340;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#22312;MAX-2-SAT&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30456;&#27604;&#65292;LightSolver&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#23567;&#30340;&#26368;&#20248;&#35299;&#26102;&#38388;&#65292;&#20854;&#20013;&#24615;&#33021;&#25552;&#21319;&#30340;&#22686;&#30410;&#24448;&#24448;&#38543;&#30528;&#38382;&#39064;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum 2-satisfiability (MAX-2-SAT) is a type of combinatorial decision problem that is known to be NP-hard. In this paper, we compare LightSolver's quantum-inspired algorithm to a leading deep-learning solver for the MAX-2-SAT problem. Experiments on benchmark data sets show that LightSolver achieves significantly smaller time-to-optimal-solution compared to a state-of-the-art deep-learning algorithm, where the gain in performance tends to increase with the problem size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05185</link><description>&lt;p&gt;
&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Penalty-based Bilevel Gradient Descent Method. (arXiv:2302.05185v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a penalty-based bilevel gradient descent algorithm to solve the constrained bilevel problem without lower-level strong convexity, and experiments show its efficiency.
&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#38590;&#20197;&#35299;&#20915;&#12290;&#26368;&#36817;&#30340;&#21487;&#25193;&#23637;&#21452;&#23618;&#31639;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#19979;&#23618;&#30446;&#26631;&#20989;&#25968;&#26159;&#24378;&#20984;&#25110;&#26080;&#32422;&#26463;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24809;&#32602;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#24809;&#32602;&#37325;&#26500;&#21487;&#20197;&#24674;&#22797;&#21407;&#22987;&#21452;&#23618;&#38382;&#39064;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#65288;PBGD&#65289;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;PBGD&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel optimization problems are difficult to solve. Recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent (PBGD) algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. Experiments showcase the efficiency of the proposed PBGD algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#12290;</title><link>http://arxiv.org/abs/2302.05045</link><description>&lt;p&gt;
&#21033;&#29992;&#21098;&#26525;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31232;&#30095;&#24615;&#26469;&#20248;&#21270;&#22823;&#22411;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training. (arXiv:2302.05045v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel approach that exploits sparse subnetworks to optimize memory utilization and communication in two popular algorithms for parallel deep learning, and demonstrates significant reductions in memory consumption and communication time on a 2.7 billion parameter model using 512 NVIDIA V100 GPUs.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36890;&#20449;&#24320;&#38144;&#30340;&#26174;&#33879;&#22686;&#21152;&#65292;&#35268;&#27169;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24182;&#34892;&#35757;&#32451;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21508;&#31181;&#21098;&#26525;&#31639;&#27861;&#65292;&#33021;&#22815;&#21098;&#26525;&#65288;&#21363;&#23558;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21442;&#25968;&#35774;&#32622;&#20026;&#38646;&#65289;80-90&#65285;&#30340;&#21442;&#25968;&#65292;&#20197;&#20135;&#29983;&#19982;&#26410;&#21098;&#26525;&#29238;&#32593;&#32476;&#30456;&#31561;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#31232;&#30095;&#23376;&#32593;&#32476;&#26469;&#20248;&#21270;&#20004;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#31639;&#27861; - &#25968;&#25454;&#24182;&#34892;&#21644;&#23618;&#38388;&#24182;&#34892;&#30340;&#20869;&#23384;&#21033;&#29992;&#21644;&#36890;&#20449;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;AxoNN&#20013;&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20381;&#36182;&#20110;&#25968;&#25454;&#21644;&#23618;&#38388;&#24182;&#34892;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#20449;&#26102;&#38388;&#21644;&#20869;&#23384;&#21033;&#29992;&#30340;&#20943;&#23569;&#12290;&#22312;512&#20010;NVIDIA V100 GPU&#19978;&#65292;&#25105;&#20204;&#30340;&#20248;&#21270;&#23558;27&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#20102;74&#65285;&#65292;&#24635;&#36890;&#20449;&#26102;&#38388;&#20943;&#23569;&#20102;40&#65285;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Parallel training of neural networks at scale is challenging due to significant overheads arising from communication. Recently, deep learning researchers have developed a variety of pruning algorithms that are capable of pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to yield sparse subnetworks that equal the accuracy of the unpruned parent network. In this work, we propose a novel approach that exploits these sparse subnetworks to optimize the memory utilization and communication in two popular algorithms for parallel deep learning namely -- data and inter-layer parallelism. We integrate our approach into AxoNN, a highly scalable framework for parallel deep learning that relies on data and inter-layer parallelism, and demonstrate the reduction in communication time and memory utilization. On 512 NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7 billion parameter model by 74%, and the total communication time by 40%, thus providing 
&lt;/p&gt;</description></item><item><title>&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30340;&#23450;&#20041;&#36807;&#20110;&#31616;&#21333;&#65292;&#35768;&#22810;&#20844;&#24179;&#24615;&#25514;&#26045;&#20250;&#23548;&#33268;&#24615;&#33021;&#38477;&#32423;&#21644;&#27700;&#24179;&#19979;&#38477;&#65292;&#20351;&#27599;&#20010;&#20154;&#37117;&#21464;&#24471;&#26356;&#31967;&#65292;&#36825;&#31181;&#20570;&#27861;&#19981;&#31526;&#21512;&#23454;&#36136;&#24179;&#31561;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2302.02404</link><description>&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30340;&#19981;&#20844;&#24179;&#24615;&#65306;&#40664;&#35748;&#30340;&#27700;&#24179;&#19979;&#38477;&#21644;&#20005;&#26684;&#24179;&#31561;&#20027;&#20041;
&lt;/p&gt;
&lt;p&gt;
The Unfairness of Fair Machine Learning: Levelling down and strict egalitarianism by default. (arXiv:2302.02404v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02404
&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#30340;&#23450;&#20041;&#36807;&#20110;&#31616;&#21333;&#65292;&#35768;&#22810;&#20844;&#24179;&#24615;&#25514;&#26045;&#20250;&#23548;&#33268;&#24615;&#33021;&#38477;&#32423;&#21644;&#27700;&#24179;&#19979;&#38477;&#65292;&#20351;&#27599;&#20010;&#20154;&#37117;&#21464;&#24471;&#26356;&#31967;&#65292;&#36825;&#31181;&#20570;&#27861;&#19981;&#31526;&#21512;&#23454;&#36136;&#24179;&#31561;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
The oversimplification of fairness in machine learning leads to performance degradation and leveling down, which does not achieve substantive equality.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#39640;&#24230;&#27963;&#36291;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#39046;&#22495;&#12290;&#22823;&#22810;&#25968;&#20154;&#31616;&#21333;&#22320;&#23450;&#20041;&#20844;&#24179;&#24615;&#65292;&#21363;&#20844;&#24179;&#24847;&#21619;&#30528;&#20943;&#23569;&#19981;&#21516;&#20154;&#32676;&#20043;&#38388;&#30340;&#34920;&#29616;&#25110;&#32467;&#26524;&#24046;&#36317;&#65292;&#21516;&#26102;&#23613;&#21487;&#33021;&#20445;&#30041;&#21407;&#22987;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#36890;&#36807;&#20844;&#24179;&#24615;&#25514;&#26045;&#31616;&#21270;&#24179;&#31561;&#30340;&#20570;&#27861;&#20196;&#20154;&#19981;&#23433;&#12290;&#35768;&#22810;&#24403;&#21069;&#30340;&#20844;&#24179;&#24615;&#25514;&#26045;&#26082;&#23384;&#22312;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#38477;&#32423;&#65292;&#25110;&#32773;&#8220;&#27700;&#24179;&#19979;&#38477;&#8221;&#65292;&#21363;&#36890;&#36807;&#20351;&#27599;&#20010;&#32676;&#20307;&#21464;&#24471;&#26356;&#31967;&#25110;&#23558;&#34920;&#29616;&#26356;&#22909;&#30340;&#32676;&#20307;&#38477;&#21040;&#26368;&#24046;&#30340;&#27700;&#24179;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#12290;&#24403;&#20844;&#24179;&#21482;&#33021;&#36890;&#36807;&#22312;&#29289;&#36136;&#25110;&#20851;&#31995;&#26041;&#38754;&#20351;&#27599;&#20010;&#20154;&#21464;&#24471;&#26356;&#31967;&#32780;&#23454;&#29616;&#65292;&#36890;&#36807;&#27745;&#21517;&#21270;&#12289;&#22242;&#32467;&#30340;&#25439;&#22833;&#12289;&#19981;&#24179;&#31561;&#30340;&#20851;&#27880;&#21644;&#38169;&#36807;&#23454;&#36136;&#24179;&#31561;&#30340;&#26426;&#20250;&#65292;&#20284;&#20046;&#22312;&#23558;&#27169;&#31946;&#30340;&#8220;&#20844;&#24179;&#8221;&#27010;&#24565;&#36716;&#21270;&#20026;&#23454;&#36341;&#26102;&#20986;&#29616;&#20102;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36896;&#25104;&#36825;&#31181;&#29616;&#35937;&#30340;&#21407;&#22240;&#21644;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years fairness in machine learning (ML) has emerged as a highly active area of research and development. Most define fairness in simple terms, where fairness means reducing gaps in performance or outcomes between demographic groups while preserving as much of the accuracy of the original system as possible. This oversimplification of equality through fairness measures is troubling. Many current fairness measures suffer from both fairness and performance degradation, or "levelling down," where fairness is achieved by making every group worse off, or by bringing better performing groups down to the level of the worst off. When fairness can only be achieved by making everyone worse off in material or relational terms through injuries of stigma, loss of solidarity, unequal concern, and missed opportunities for substantive equality, something would appear to have gone wrong in translating the vague concept of 'fairness' into practice. This paper examines the causes and prevalence 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23618;&#21464;&#20998;&#20998;&#26512;&#35777;&#26126;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#25104;&#21151;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#30340;&#25968;&#25454;&#26465;&#20214;&#24471;&#21040;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2302.01798</link><description>&lt;p&gt;
&#36890;&#36807;&#23618;&#21464;&#20998;&#20998;&#26512;&#35299;&#37322;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Interpretations of Domain Adaptations via Layer Variational Analysis. (arXiv:2302.01798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23618;&#21464;&#20998;&#20998;&#26512;&#35777;&#26126;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#25104;&#21151;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#30340;&#25968;&#25454;&#26465;&#20214;&#24471;&#21040;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study establishes the theory of transfer learning in deep learning through formal derivations and heuristic analysis, proving that the success of transfer learning can be guaranteed with corresponding data conditions. An alternative method for network-based transfer learning is proposed, which shows an increase in efficiency and accuracy for domain adaptation.
&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#24615;&#33021;&#65292;&#20294;&#26377;&#38480;&#30340;&#25991;&#29486;&#25253;&#36947;&#20102;&#20854;&#32972;&#21518;&#30340;&#26426;&#21046;&#12290;&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#27491;&#24335;&#30340;&#25512;&#23548;&#21644;&#21551;&#21457;&#24335;&#20998;&#26512;&#65292;&#20197;&#21046;&#23450;&#28145;&#24230;&#23398;&#20064;&#20013;&#36716;&#31227;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#23618;&#21464;&#20998;&#20998;&#26512;&#35777;&#26126;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#25104;&#21151;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#30340;&#25968;&#25454;&#26465;&#20214;&#24471;&#21040;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#35745;&#31639;&#20135;&#29983;&#20102;&#23545;&#30693;&#35782;&#36716;&#31227;&#36807;&#31243;&#30340;&#30452;&#35266;&#35299;&#37322;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;&#24403;&#36866;&#24212;&#26399;&#38388;&#30340;&#26032;&#39046;&#22495;&#25968;&#25454;&#36275;&#22815;&#31232;&#30095;&#26102;&#65292;&#23427;&#29305;&#21035;&#26377;&#20248;&#21183;&#12290;&#23545;&#21508;&#31181;&#20219;&#21153;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#27604;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is known to perform efficiently in many applications empirically, yet limited literature reports the mechanism behind the scene. This study establishes both formal derivations and heuristic analysis to formulate the theory of transfer learning in deep learning. Our framework utilizing layer variational analysis proves that the success of transfer learning can be guaranteed with corresponding data conditions. Moreover, our theoretical calculation yields intuitive interpretations towards the knowledge transfer process. Subsequently, an alternative method for network-based transfer learning is derived. The method shows an increase in efficiency and accuracy for domain adaptation. It is particularly advantageous when new domain data is sufficiently sparse during adaptation. Numerical experiments over diverse tasks validated our theory and verified that our analytic expression achieved better performance in domain adaptation than the gradient descent method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.01735</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65306;&#26041;&#24046;&#32553;&#20943;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Semi-Supervised Medical Image Segmentation: A Variance-Reduction Perspective. (arXiv:2302.01735v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. The concept of variance-reduced estimation is used to build ARCO, and certain variance-reduction techniques are shown to be particularly beneficial in medical image segmentation.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#25552;&#39640;&#35270;&#35273;&#34920;&#31034;&#36136;&#37327;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#26679;&#26412;&#23545;&#26469;&#23454;&#29616;&#12290;&#36825;&#26159;&#36890;&#36807;&#35266;&#23519;&#21040;&#65292;&#22312;&#27809;&#26377;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#37319;&#26679;&#20855;&#26377;&#30495;&#27491;&#19981;&#21516;&#35299;&#21078;&#29305;&#24449;&#30340;&#36127;&#26679;&#26412;&#65292;&#21017;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#26469;&#33258;&#30456;&#20284;&#30340;&#35299;&#21078;&#29305;&#24449;&#65292;&#27169;&#22411;&#21487;&#33021;&#38590;&#20197;&#21306;&#20998;&#23569;&#25968;&#23614;&#31867;&#26679;&#26412;&#65292;&#20351;&#24471;&#23614;&#31867;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#27169;&#22411;&#23849;&#28291;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARCO&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20998;&#23618;&#32452;&#37319;&#26679;&#29702;&#35770;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#36890;&#36807;&#26041;&#24046;&#32553;&#20943;&#20272;&#35745;&#30340;&#27010;&#24565;&#26469;&#26500;&#24314;ARCO&#65292;&#24182;&#34920;&#26126;&#26576;&#20123;&#26041;&#24046;&#32553;&#20943;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
For medical image segmentation, contrastive learning is the dominant practice to improve the quality of visual representations by contrasting semantically similar and dissimilar pairs of samples. This is enabled by the observation that without accessing ground truth label, negative examples with truly dissimilar anatomical features, if sampled, can significantly improve the performance. In reality, however, these samples may come from similar anatomical features and the models may struggle to distinguish the minority tail-class samples, making the tail classes more prone to misclassification, both of which typically lead to model collapse. In this paper, we propose ARCO, a semi-supervised contrastive learning (CL) framework with stratified group sampling theory in medical image segmentation. In particular, we first propose building ARCO through the concept of variance-reduced estimation, and show that certain variance-reduction techniques are particularly beneficial in medical image se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32452;&#32455;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#27010;&#29575;&#28857;&#20113;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#22330;&#26223;&#22797;&#26434;&#24230;&#33258;&#21160;&#35843;&#25972;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.00047</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32452;&#32455;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#27010;&#29575;&#28857;&#20113;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Point Cloud Modeling via Self-Organizing Gaussian Mixture Models. (arXiv:2302.00047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32452;&#32455;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#27010;&#29575;&#28857;&#20113;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#22330;&#26223;&#22797;&#26434;&#24230;&#33258;&#21160;&#35843;&#25972;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a probabilistic point cloud modeling method based on self-organizing Gaussian mixture models, which can automatically adjust the model complexity according to the scene complexity, and has better generalization performance compared to existing techniques.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#30340;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#26377;&#38480;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#23545;&#31354;&#38388;&#28857;&#20113;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#65292;&#20854;&#20013;&#32452;&#20214;&#30340;&#25968;&#37327;&#22522;&#20110;&#22330;&#26223;&#22797;&#26434;&#24615;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#21033;&#29992;&#20449;&#24687;&#35770;&#23398;&#20064;&#20013;&#30340;&#33258;&#32452;&#32455;&#21407;&#29702;&#65292;&#26681;&#25454;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#33258;&#21160;&#35843;&#25972;GMM&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#12290;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#19981;&#21516;&#22330;&#26223;&#22797;&#26434;&#24230;&#30340;&#23454;&#38469;&#25968;&#25454;&#19978;&#19982;&#29616;&#26377;&#30340;&#28857;&#20113;&#24314;&#27169;&#25216;&#26415;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This letter presents a continuous probabilistic modeling methodology for spatial point cloud data using finite Gaussian Mixture Models (GMMs) where the number of components are adapted based on the scene complexity. Few hierarchical and adaptive methods have been proposed to address the challenge of balancing model fidelity with size. Instead, state-of-the-art mapping approaches require tuning parameters for specific use cases, but do not generalize across diverse environments. To address this gap, we utilize a self-organizing principle from information-theoretic learning to automatically adapt the complexity of the GMM model based on the relevant information in the sensor data. The approach is evaluated against existing point cloud modeling techniques on real-world data with varying degrees of scene complexity.
&lt;/p&gt;</description></item><item><title>LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2301.09799</link><description>&lt;p&gt;
LDMIC&#65306;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LDMIC: Learning-based Distributed Multi-view Image Coding. (arXiv:2301.09799v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09799
&lt;/p&gt;
&lt;p&gt;
LDMIC&#26159;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#21644;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#23454;&#29616;&#20102;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#25429;&#25417;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
LDMIC is a learning-based distributed multi-view image coding framework that captures global inter-view correlations through independent encoders and a joint context transfer module based on the cross-attention mechanism, which is insensitive to geometric relations.
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#22270;&#20687;&#21387;&#32553;&#22312;3D&#30456;&#20851;&#24212;&#29992;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#39044;&#27979;&#32534;&#30721;&#26550;&#26500;&#65292;&#38656;&#35201;&#32852;&#21512;&#32534;&#30721;&#21387;&#32553;&#30456;&#24212;&#30340;&#35270;&#24046;&#21644;&#27531;&#24046;&#20449;&#24687;&#12290;&#36825;&#35201;&#27714;&#30456;&#26426;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#65292;&#24182;&#24378;&#21046;&#25191;&#34892;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#26497;&#32447;&#20960;&#20309;&#32422;&#26463;&#65292;&#36825;&#20351;&#24471;&#22312;&#20855;&#26377;&#38543;&#26426;&#37325;&#21472;&#35270;&#37326;&#30340;&#20998;&#24067;&#24335;&#30456;&#26426;&#31995;&#32479;&#20013;&#37096;&#32626;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#20998;&#24067;&#24335;&#28304;&#32534;&#30721;&#29702;&#35770;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#21644;&#32852;&#21512;&#35299;&#30721;&#23454;&#29616;&#30456;&#20851;&#28304;&#30340;&#39640;&#25928;&#25968;&#25454;&#21387;&#32553;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#35774;&#35745;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22810;&#35270;&#22270;&#22270;&#20687;&#32534;&#30721;&#65288;LDMIC&#65289;&#26694;&#26550;&#30340;&#21160;&#26426;&#12290;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#22120;&#65292;LDMIC&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30340;&#32852;&#21512;&#19978;&#19979;&#25991;&#20256;&#36755;&#27169;&#22359;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#20840;&#23616;&#35270;&#22270;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23545;&#20960;&#20309;&#20851;&#31995;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-view image compression plays a critical role in 3D-related applications. Existing methods adopt a predictive coding architecture, which requires joint encoding to compress the corresponding disparity as well as residual information. This demands collaboration among cameras and enforces the epipolar geometric constraint between different views, which makes it challenging to deploy these methods in distributed camera systems with randomly overlapping fields of view. Meanwhile, distributed source coding theory indicates that efficient data compression of correlated sources can be achieved by independent encoding and joint decoding, which motivates us to design a learning-based distributed multi-view image coding (LDMIC) framework. With independent encoders, LDMIC introduces a simple yet effective joint context transfer module based on the cross-attention mechanism at the decoder to effectively capture the global inter-view correlations, which is insensitive to the geometric relation
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#21487;&#20197;&#35299;&#20915;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36208;&#21521;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.09245</link><description>&lt;p&gt;
&#36208;&#21521;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#65306;&#23558;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#24341;&#20837;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural Networks. (arXiv:2301.09245v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09245
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#21487;&#20197;&#35299;&#20915;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#36208;&#21521;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introducing neuronal diversity can solve the fundamental problems of artificial neural networks and lead to NeuroAI.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#21382;&#21490;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#19968;&#30452;&#23545;&#36234;&#26469;&#36234;&#28145;&#20837;&#30340;&#22823;&#33041;&#29702;&#35299;&#25345;&#24320;&#25918;&#24577;&#24230;&#24182;&#19981;&#26029;&#21463;&#21040;&#21551;&#21457;&#65292;&#20363;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;neocognitron&#30340;&#21551;&#21457;&#12290;&#26681;&#25454;&#26032;&#20852;&#39046;&#22495;&#31070;&#32463;&#20154;&#24037;&#26234;&#33021;&#30340;&#21160;&#26426;&#65292;&#22823;&#37327;&#30340;&#31070;&#32463;&#31185;&#23398;&#30693;&#35782;&#21487;&#20197;&#36890;&#36807;&#36171;&#20104;&#32593;&#32476;&#26356;&#24378;&#22823;&#30340;&#33021;&#21147;&#26469;&#20652;&#21270;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30693;&#36947;&#65292;&#20154;&#31867;&#22823;&#33041;&#26377;&#35768;&#22810;&#24418;&#24577;&#21644;&#21151;&#33021;&#19981;&#21516;&#30340;&#31070;&#32463;&#20803;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20960;&#20046;&#23436;&#20840;&#24314;&#31435;&#22312;&#21333;&#19968;&#31070;&#32463;&#20803;&#31867;&#22411;&#19978;&#12290;&#22312;&#20154;&#31867;&#22823;&#33041;&#20013;&#65292;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#26159;&#21508;&#31181;&#29983;&#29289;&#26234;&#33021;&#34892;&#20026;&#30340;&#19968;&#20010;&#21551;&#21160;&#22240;&#32032;&#12290;&#30001;&#20110;&#20154;&#24037;&#32593;&#32476;&#26159;&#20154;&#31867;&#22823;&#33041;&#30340;&#32553;&#24433;&#65292;&#24341;&#20837;&#31070;&#32463;&#20803;&#22810;&#26679;&#24615;&#24212;&#35813;&#26377;&#21161;&#20110;&#35299;&#20915;&#20154;&#24037;&#32593;&#32476;&#30340;&#35832;&#22914;&#25928;&#29575;&#12289;&#35299;&#37322;&#24615;&#31561;&#22522;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout history, the development of artificial intelligence, particularly artificial neural networks, has been open to and constantly inspired by the increasingly deepened understanding of the brain, such as the inspiration of neocognitron, which is the pioneering work of convolutional neural networks. Per the motives of the emerging field: NeuroAI, a great amount of neuroscience knowledge can help catalyze the next generation of AI by endowing a network with more powerful capabilities. As we know, the human brain has numerous morphologically and functionally different neurons, while artificial neural networks are almost exclusively built on a single neuron type. In the human brain, neuronal diversity is an enabling factor for all kinds of biological intelligent behaviors. Since an artificial network is a miniature of the human brain, introducing neuronal diversity should be valuable in terms of addressing those essential problems of artificial networks such as efficiency, interpret
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;CNN&#39044;&#27979;&#21487;&#20449;&#24230;&#30340;&#21487;&#20449;&#24230;&#20998;&#25968;&#65288;TS&#65289;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#26816;&#26597;CNN&#25152;&#20570;&#39044;&#27979;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#23384;&#22312;&#26469;&#37327;&#21270;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.08839</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;CNN&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Trustworthiness Score to Evaluate CNNs Predictions. (arXiv:2301.08839v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;CNN&#39044;&#27979;&#21487;&#20449;&#24230;&#30340;&#21487;&#20449;&#24230;&#20998;&#25968;&#65288;TS&#65289;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#26816;&#26597;CNN&#25152;&#20570;&#39044;&#27979;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#23384;&#22312;&#26469;&#37327;&#21270;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a trustworthiness score (TS) metric to evaluate the confidence of CNN predictions, which quantifies the trustworthiness by checking for the existence of certain features in the predictions made by the CNN.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#26080;&#27861;&#22312;&#25805;&#20316;&#26399;&#38388;&#25345;&#32493;&#39564;&#35777;CNN&#65292;&#36825;&#20351;&#24471;&#24320;&#21457;&#20154;&#21592;&#21644;&#30417;&#31649;&#26426;&#26500;&#38590;&#20197;&#23545;&#20351;&#29992;CNN&#30340;&#33258;&#20027;&#31995;&#32479;&#30340;&#37096;&#32626;&#33719;&#24471;&#20449;&#24515;&#12290;&#22312;&#25805;&#20316;&#26399;&#38388;&#65292;&#20102;&#35299;CNN&#30340;&#39044;&#27979;&#20309;&#26102;&#21487;&#20449;&#25110;&#21487;&#30097;&#23545;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#26412;&#26041;&#27861;&#26159;&#20351;&#29992;&#27169;&#22411;&#30340;&#36755;&#20986;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#35780;&#20272;&#39044;&#27979;&#26159;&#21542;&#21487;&#20449;&#25110;&#21487;&#30097;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#26159;&#26469;&#33258;&#40657;&#30418;&#35745;&#31639;&#30340;&#32467;&#26524;&#65292;&#22240;&#27492;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#20351;&#24471;&#24456;&#38590;&#23558;&#21487;&#20449;&#24230;&#24402;&#22240;&#20110;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20449;&#24230;&#20998;&#25968;&#65288;TS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#36879;&#26126;&#21644;&#26377;&#25928;&#30340;&#26041;&#24335;&#26469;&#25552;&#20379;CNN&#39044;&#27979;&#30340;&#20449;&#24515;&#12290;&#35813;&#24230;&#37327;&#26631;&#20934;&#36890;&#36807;&#26816;&#26597;CNN&#25152;&#20570;&#39044;&#27979;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#30340;&#23384;&#22312;&#26469;&#37327;&#21270;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the black box nature of Convolutional neural networks (CNNs), the continuous validation of CNNs during operation is infeasible. As a result this makes it difficult for developers and regulators to gain confidence in the deployment of autonomous systems employing CNNs. It is critical for safety during operation to know when a CNN's predictions are trustworthy or suspicious. The basic approach is to use the model's output confidence score to assess if predictions are trustworthy or suspicious. However, the model's confidence score is a result of computations coming from a black box, therefore lacks transparency and makes it challenging to credit trustworthiness to predictions. We introduce the trustworthiness score (TS), a simple metric that provides a more transparent and effective way of providing confidence in CNNs predictions. The metric quantifies the trustworthiness in a prediction by checking for the existence of certain features in the predictions made by the CNN. The TS m
&lt;/p&gt;</description></item><item><title>SegViz&#26159;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;</title><link>http://arxiv.org/abs/2301.07074</link><description>&lt;p&gt;
SegViz&#65306;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#22810;&#22120;&#23448;&#20998;&#21106;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#24322;&#26500;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SegViz: A federated-learning based framework for multi-organ segmentation on heterogeneous data sets with partial annotations. (arXiv:2301.07074v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07074
&lt;/p&gt;
&lt;p&gt;
SegViz&#26159;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;
&lt;/p&gt;
&lt;p&gt;
SegViz is a federated learning-based framework for training segmentation models from distributed non-i.i.d datasets with partial annotations. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation.
&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#26159;&#21307;&#23398;&#22270;&#20687;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#22810;&#20010;&#19979;&#28216;&#20020;&#24202;&#24212;&#29992;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#25163;&#21160;&#27880;&#37322;&#26159;&#32791;&#26102;&#30340;&#12289;&#38656;&#35201;&#39640;&#25216;&#33021;&#30340;&#12289;&#26114;&#36149;&#30340;&#24037;&#20316;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;3D&#22270;&#20687;&#12290;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20174;&#22810;&#20010;&#32452;&#30340;&#37096;&#20998;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#32858;&#21512;&#30693;&#35782;&#65292;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21327;&#20316;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SegViz&#65292;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#20998;&#24067;&#24335;&#30340;&#38750;i.i.d&#25968;&#25454;&#38598;&#20013;&#35757;&#32451;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#20998;&#21106;&#27169;&#22411;&#12290;&#23558;SegViz&#30340;&#24615;&#33021;&#19982;&#20998;&#21035;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#20197;&#21450;&#38598;&#20013;&#32858;&#21512;&#25152;&#26377;&#25968;&#25454;&#38598;&#24182;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#20351;&#29992;FedBN&#20316;&#20026;&#32858;&#21512;&#31574;&#30053;&#30340;SegViz&#26694;&#26550;&#22312;&#22806;&#37096;BTCV&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20998;&#21106;&#30340;dice&#20998;&#25968;&#20998;&#21035;&#20026;0.93&#12289;0.83&#12289;0.55&#21644;0.75&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation is one of the most primary tasks in deep learning for medical imaging, owing to its multiple downstream clinical applications. However, generating manual annotations for medical images is time-consuming, requires high skill, and is an expensive effort, especially for 3D images. One potential solution is to aggregate knowledge from partially annotated datasets from multiple groups to collaboratively train global models using Federated Learning. To this end, we propose SegViz, a federated learning-based framework to train a segmentation model from distributed non-i.i.d datasets with partial annotations. The performance of SegViz was compared against training individual models separately on each dataset as well as centrally aggregating all the datasets in one place and training a single model. The SegViz framework using FedBN as the aggregation strategy demonstrated excellent performance on the external BTCV set with dice scores of 0.93, 0.83, 0.55, and 0.75 for segmentation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LASSO&#30340;&#24191;&#20041;&#19981;&#21464;&#21305;&#37197;&#24615;&#36136;&#65292;&#36890;&#36807;&#21046;&#23450;&#20855;&#26377;&#20869;&#22312;&#31232;&#30095;&#24615;&#30340;&#39640;&#32500;&#38382;&#39064;&#65292;&#23558;&#19981;&#21464;&#21305;&#37197;&#24615;&#36136;&#25512;&#24191;&#21040;&#20165;&#30446;&#26631;&#34987;&#24178;&#39044;&#30340;&#37325;&#35201;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#31283;&#20581;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.05975</link><description>&lt;p&gt;
&#22522;&#20110;LASSO&#30340;&#24191;&#20041;&#19981;&#21464;&#21305;&#37197;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Generalized Invariant Matching Property via LASSO. (arXiv:2301.05975v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LASSO&#30340;&#24191;&#20041;&#19981;&#21464;&#21305;&#37197;&#24615;&#36136;&#65292;&#36890;&#36807;&#21046;&#23450;&#20855;&#26377;&#20869;&#22312;&#31232;&#30095;&#24615;&#30340;&#39640;&#32500;&#38382;&#39064;&#65292;&#23558;&#19981;&#21464;&#21305;&#37197;&#24615;&#36136;&#25512;&#24191;&#21040;&#20165;&#30446;&#26631;&#34987;&#24178;&#39044;&#30340;&#37325;&#35201;&#24773;&#20917;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#31283;&#20581;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a generalized invariant matching property via LASSO, which formulates a high-dimensional problem with intrinsic sparsity and extends the invariant matching property to the important setting when only the target is intervened. The paper also presents a more robust and computation-efficient algorithm by leveraging a variant of Lasso, improving upon the existing algorithms.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#19968;&#31181;&#22522;&#26412;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21033;&#29992;&#19981;&#21464;&#24615;&#21407;&#21017;&#12290;&#28982;&#32780;&#65292;&#24403;&#21709;&#24212;&#34987;&#24178;&#39044;&#26102;&#65292;&#19981;&#21464;&#24615;&#21407;&#21017;&#34987;&#36829;&#21453;&#65292;&#20351;&#24471;&#36825;&#31181;&#24773;&#20917;&#21464;&#24471;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#19981;&#21464;&#21305;&#37197;&#24615;&#36136;&#26469;&#30740;&#31350;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#21046;&#23450;&#20855;&#26377;&#20869;&#22312;&#31232;&#30095;&#24615;&#30340;&#39640;&#32500;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#19981;&#21464;&#21305;&#37197;&#24615;&#36136;&#25512;&#24191;&#21040;&#20165;&#30446;&#26631;&#34987;&#24178;&#39044;&#30340;&#37325;&#35201;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#31283;&#20581;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;Lasso&#30340;&#21464;&#20307;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning under distribution shifts is a challenging task. One principled approach is to exploit the invariance principle via the structural causal models. However, the invariance principle is violated when the response is intervened, making it a difficult setting. In a recent work, the invariant matching property has been developed to shed light on this scenario and shows promising performance. In this work, by formulating a high-dimensional problem with intrinsic sparsity, we generalize the invariant matching property for an important setting when only the target is intervened. We propose a more robust and computation-efficient algorithm by leveraging a variant of Lasso, improving upon the existing algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;HD 142666&#30340;&#30424;&#20013;&#35782;&#21035;&#20986;&#24378;&#28872;&#30340;&#12289;&#23616;&#37096;&#30340;&#38750;&#24320;&#26222;&#21202;&#36816;&#21160;&#65292;&#36827;&#32780;&#24471;&#20986;&#35813;&#30424;&#20013;&#23384;&#22312;&#19968;&#20010;&#34892;&#26143;&#30340;&#32467;&#35770;&#65292;&#36825;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#35782;&#21035;&#21407;&#34892;&#26143;&#30424;&#20013;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#38750;&#24320;&#26222;&#21202;&#29305;&#24449;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2301.05075</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35782;&#21035;&#20986;HD 142666&#20013;&#23884;&#20837;&#30340;&#21407;&#34892;&#26143;&#30340;&#36816;&#21160;&#23398;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Kinematic Evidence of an Embedded Protoplanet in HD 142666 Identified by Machine Learning. (arXiv:2301.05075v2 [astro-ph.EP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05075
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;HD 142666&#30340;&#30424;&#20013;&#35782;&#21035;&#20986;&#24378;&#28872;&#30340;&#12289;&#23616;&#37096;&#30340;&#38750;&#24320;&#26222;&#21202;&#36816;&#21160;&#65292;&#36827;&#32780;&#24471;&#20986;&#35813;&#30424;&#20013;&#23384;&#22312;&#19968;&#20010;&#34892;&#26143;&#30340;&#32467;&#35770;&#65292;&#36825;&#26159;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#35782;&#21035;&#21407;&#34892;&#26143;&#30424;&#20013;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#38750;&#24320;&#26222;&#21202;&#29305;&#24449;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper uses machine learning models to identify strong, localized non-Keplerian motion in the disk HD 142666, and concludes that there is a planet in the disk, which represents a first step towards using machine learning to identify previously overlooked non-Keplerian features in protoplanetary disks.
&lt;/p&gt;
&lt;p&gt;
&#21407;&#34892;&#26143;&#30424;&#30340;&#35266;&#27979;&#34920;&#26126;&#65292;&#24418;&#25104;&#31995;&#22806;&#34892;&#26143;&#20250;&#22312;&#30424;&#20013;&#30340;&#27668;&#20307;&#21644;&#23576;&#22467;&#19978;&#30041;&#19979;&#29305;&#24449;&#21360;&#35760;&#12290;&#22312;&#27668;&#20307;&#20013;&#65292;&#36825;&#20123;&#24418;&#25104;&#20013;&#30340;&#31995;&#22806;&#34892;&#26143;&#20250;&#24341;&#36215;&#24320;&#26222;&#21202;&#36816;&#21160;&#30340;&#20559;&#24046;&#65292;&#21487;&#20197;&#36890;&#36807;&#20998;&#23376;&#32447;&#35266;&#27979;&#26469;&#26816;&#27979;&#12290;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#27491;&#30830;&#30830;&#23450;&#36825;&#20123;&#30424;&#20013;&#26159;&#21542;&#23384;&#22312;&#34892;&#26143;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;HD 142666&#30340;&#30424;&#20013;&#35782;&#21035;&#20986;&#24378;&#28872;&#30340;&#12289;&#23616;&#37096;&#30340;&#38750;&#24320;&#26222;&#21202;&#36816;&#21160;&#12290;&#38543;&#21518;&#36827;&#34892;&#30340;&#19968;&#20010;&#31995;&#32479;&#20013;&#26377;&#19968;&#20010;5&#20010;&#26408;&#26143;&#36136;&#37327;&#30340;&#34892;&#26143;&#22312;75&#22825;&#25991;&#21333;&#20301;&#22788;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#20877;&#29616;&#20102;&#36825;&#31181;&#36816;&#21160;&#23398;&#32467;&#26500;&#12290;&#26681;&#25454;&#35813;&#39046;&#22495;&#30446;&#21069;&#24050;&#32463;&#24314;&#31435;&#30340;&#26631;&#20934;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;HD 142666&#20013;&#23384;&#22312;&#19968;&#20010;&#34892;&#26143;&#12290;&#36825;&#39033;&#24037;&#20316;&#20195;&#34920;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#35782;&#21035;&#21407;&#34892;&#26143;&#30424;&#20013;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#38750;&#24320;&#26222;&#21202;&#29305;&#24449;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Observations of protoplanetary disks have shown that forming exoplanets leave characteristic imprints on the gas and dust of the disk. In the gas, these forming exoplanets cause deviations from Keplerian motion, which can be detected through molecular line observations. Our previous work has shown that machine learning can correctly determine if a planet is present in these disks. Using our machine learning models, we identify strong, localized non-Keplerian motion within the disk HD 142666. Subsequent hydrodynamics simulations of a system with a 5 Jupiter-mass planet at 75 au recreates the kinematic structure. By currently established standards in the field, we conclude that HD 142666 hosts a planet. This work represents a first step towards using machine learning to identify previously overlooked non-Keplerian features in protoplanetary disks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22768;&#38899;&#21305;&#37197;&#31639;&#27861;&#65292;&#31216;&#20026;&#24863;&#30693;-&#31070;&#32463;-&#29289;&#29702;&#25439;&#22833;&#65288;PNP&#65289;&#65292;&#23427;&#26159;&#39057;&#35889;&#25439;&#22833;&#30340;&#26368;&#20248;&#20108;&#27425;&#36817;&#20284;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#21442;&#25968;&#30340;&#24863;&#30693;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2301.02886</link><description>&lt;p&gt;
&#24863;&#30693;-&#31070;&#32463;-&#29289;&#29702;&#22768;&#38899;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Perceptual-Neural-Physical Sound Matching. (arXiv:2301.02886v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22768;&#38899;&#21305;&#37197;&#31639;&#27861;&#65292;&#31216;&#20026;&#24863;&#30693;-&#31070;&#32463;-&#29289;&#29702;&#25439;&#22833;&#65288;PNP&#65289;&#65292;&#23427;&#26159;&#39057;&#35889;&#25439;&#22833;&#30340;&#26368;&#20248;&#20108;&#27425;&#36817;&#20284;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#19981;&#21516;&#21442;&#25968;&#30340;&#24863;&#30693;&#37325;&#35201;&#24615;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new sound matching algorithm called Perceptual-Neural-Physical loss (PNP), which is the optimal quadratic approximation of spectral loss and can better accommodate the differing perceptual significance of each parameter while having fast convergence.
&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#21305;&#37197;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#21442;&#25968;&#21270;&#38899;&#39057;&#21512;&#25104;&#26469;&#36817;&#20284;&#30446;&#26631;&#27874;&#24418;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21305;&#37197;&#25345;&#32493;&#35856;&#27874;&#38899;&#35843;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#30446;&#26631;&#26159;&#38750;&#24179;&#31283;&#21644;&#38750;&#35856;&#27874;&#30340;&#26102;&#20505;&#65292;&#20363;&#22914;&#25171;&#20987;&#20048;&#22120;&#65292;&#20219;&#21153;&#23601;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24402;&#22240;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#19981;&#36275;&#12290;&#19968;&#26041;&#38754;&#65292;&#21442;&#25968;&#22495;&#20013;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#31216;&#20026;&#8220;P-loss&#8221;&#65292;&#31616;&#21333;&#24555;&#36895;&#65292;&#20294;&#26410;&#33021;&#36866;&#24212;&#27599;&#20010;&#21442;&#25968;&#30340;&#19981;&#21516;&#24863;&#30693;&#37325;&#35201;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#39057;&#35889;&#26102;&#38388;&#22495;&#20013;&#30340;&#22343;&#26041;&#35823;&#24046;&#65292;&#31216;&#20026;&#8220;&#39057;&#35889;&#25439;&#22833;&#8221;&#65292;&#22312;&#24863;&#30693;&#19978;&#26159;&#26377;&#21160;&#26426;&#30340;&#65292;&#24182;&#22312;&#21487;&#24494;&#20998;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;&#65288;DDSP&#65289;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#39057;&#35889;&#25439;&#22833;&#26159;&#38899;&#39640;&#38388;&#38548;&#30340;&#19981;&#33391;&#39044;&#27979;&#22240;&#32032;&#65292;&#20854;&#26799;&#24230;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;&#22312;&#36825;&#20010;&#22256;&#22659;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24863;&#30693;-&#31070;&#32463;-&#29289;&#29702;&#25439;&#22833;&#65288;PNP&#65289;&#12290;PNP&#26159;&#39057;&#35889;&#25439;&#22833;&#30340;&#26368;&#20248;&#20108;&#27425;&#36817;&#20284;&#65292;&#21516;&#26102;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound matching algorithms seek to approximate a target waveform by parametric audio synthesis. Deep neural networks have achieved promising results in matching sustained harmonic tones. However, the task is more challenging when targets are nonstationary and inharmonic, e.g., percussion. We attribute this problem to the inadequacy of loss function. On one hand, mean square error in the parametric domain, known as "P-loss", is simple and fast but fails to accommodate the differing perceptual significance of each parameter. On the other hand, mean square error in the spectrotemporal domain, known as "spectral loss", is perceptually motivated and serves in differentiable digital signal processing (DDSP). Yet, spectral loss is a poor predictor of pitch intervals and its gradient may be computationally expensive; hence a slow convergence. Against this conundrum, we present Perceptual-Neural-Physical loss (PNP). PNP is the optimal quadratic approximation of spectral loss while being as fast 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#21644;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#32473;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.02830</link><description>&lt;p&gt;
&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Image Data Augmentation Approaches: A Comprehensive Survey and Future directions. (arXiv:2301.02830v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#21644;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#32473;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article provides a comprehensive survey of advanced data augmentation techniques for computer vision tasks, including a novel taxonomy and evaluation of each technique's strengths and weaknesses. The article also presents comprehensive results of the data augmentation effect on popular computer vision tasks such as image classification, object detection, and semantic segmentation.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#35760;&#25968;&#25454;&#26377;&#38480;&#65292;&#23548;&#33268;&#32593;&#32476;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#21363;&#32593;&#32476;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#27604;&#35757;&#32451;&#25968;&#25454;&#24046;&#12290;&#22240;&#27492;&#65292;&#23427;&#38480;&#21046;&#20102;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#22914;dropout&#12289;&#24402;&#19968;&#21270;&#21644;&#39640;&#32423;&#25968;&#25454;&#22686;&#24378;&#12290;&#20854;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#26088;&#22312;&#36890;&#36807;&#21253;&#25324;&#26679;&#26412;&#22810;&#26679;&#24615;&#26469;&#25193;&#22823;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#36817;&#26469;&#25104;&#20026;&#28909;&#38376;&#35805;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#39640;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#30340;&#32972;&#26223;&#12289;&#19968;&#20010;&#26032;&#39062;&#32780;&#20840;&#38754;&#30340;&#20998;&#31867;&#27861;&#12289;&#20197;&#21450;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65288;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;&#19977;&#20010;&#27969;&#34892;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#65289;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) algorithms have shown significant performance in various computer vision tasks. However, having limited labelled data lead to a network overfitting problem, where network performance is bad on unseen data as compared to training data. Consequently, it limits performance improvement. To cope with this problem, various techniques have been proposed such as dropout, normalization and advanced data augmentation. Among these, data augmentation, which aims to enlarge the dataset size by including sample diversity, has been a hot topic in recent times. In this article, we focus on advanced data augmentation techniques. we provide a background of data augmentation, a novel and comprehensive taxonomy of reviewed data augmentation techniques, and the strengths and weaknesses (wherever possible) of each technique. We also provide comprehensive results of the data augmentation effect on three popular computer vision tasks, such as image classification, object detection and seman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23454;&#29992;&#30340;&#26631;&#31614;&#19981;&#21487;&#30693;&#35774;&#32622;&#65292;&#20197;&#29983;&#25104;&#19981;&#21487;&#23398;&#20064;&#30340;&#26679;&#26412;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2301.01217</link><description>&lt;p&gt;
&#19981;&#21487;&#23398;&#20064;&#30340;&#32858;&#31867;&#65306;&#38754;&#21521;&#26631;&#31614;&#19981;&#21487;&#30693;&#30340;&#19981;&#21487;&#23398;&#20064;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples. (arXiv:2301.01217v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#23454;&#29992;&#30340;&#26631;&#31614;&#19981;&#21487;&#30693;&#35774;&#32622;&#65292;&#20197;&#29983;&#25104;&#19981;&#21487;&#23398;&#20064;&#30340;&#26679;&#26412;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a more practical label-agnostic setting to generate unlearnable examples, which can prevent unauthorized training of machine learning models.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#32593;&#19978;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#24320;&#21457;&#19981;&#21487;&#23398;&#20064;&#30340;&#31034;&#20363;&#65288;UEs&#65289;&#26469;&#38450;&#27490;&#35270;&#35273;&#38544;&#31169;&#27844;&#38706;&#24863;&#20852;&#36259;&#12290;UEs&#26159;&#28155;&#21152;&#20102;&#19981;&#21487;&#35265;&#20294;&#19981;&#21487;&#23398;&#20064;&#22122;&#22768;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#24050;&#32463;&#21457;&#29616;&#21487;&#20197;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#12290;UEs&#36890;&#24120;&#26159;&#36890;&#36807;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#21644;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#29983;&#25104;&#30340;&#65292;&#20197;&#20174;&#21407;&#22987;&#26679;&#26412;&#20013;&#21435;&#38500;&#65288;&#26368;&#23567;&#21270;&#65289;&#38169;&#35823;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#20445;&#25252;&#25968;&#25454;&#20813;&#21463;&#26410;&#30693;&#30446;&#26631;&#27169;&#22411;&#30340;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;UE&#29983;&#25104;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#19968;&#20010;&#29702;&#24819;&#30340;&#20551;&#35774;&#65292;&#31216;&#20026;&#26631;&#31614;&#19968;&#33268;&#24615;&#65292;&#21363;&#20551;&#23450;&#40657;&#23458;&#21644;&#20445;&#25252;&#32773;&#23545;&#20110;&#32473;&#23450;&#30340;&#26679;&#26412;&#25345;&#26377;&#30456;&#21516;&#30340;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#25512;&#24191;&#20102;&#19968;&#20010;&#26356;&#23454;&#29992;&#30340;&#26631;&#31614;&#19981;&#21487;&#30693;&#35774;&#32622;&#65292;&#20854;&#20013;&#40657;&#23458;&#21487;&#33021;&#20250;&#20197;&#19982;&#20445;&#25252;&#32773;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#21033;&#29992;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#30001;&#20445;&#25252;&#32773;&#25345;&#26377;&#30340;m&#31867;&#19981;&#21487;&#23398;&#20064;&#25968;&#25454;&#38598;&#21487;&#33021;&#34987;&#40657;&#23458;&#20316;&#20026;n&#31867;&#25968;&#25454;&#38598;&#21033;&#29992;&#12290;&#29616;&#26377;&#30340;UE&#29983;&#25104;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#22833;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in developing unlearnable examples (UEs) against visual privacy leaks on the Internet. UEs are training samples added with invisible but unlearnable noise, which have been found can prevent unauthorized training of machine learning models. UEs typically are generated via a bilevel optimization framework with a surrogate model to remove (minimize) errors from the original samples, and then applied to protect the data against unknown target models. However, existing UE generation methods all rely on an ideal assumption called label-consistency, where the hackers and protectors are assumed to hold the same label for a given sample. In this work, we propose and promote a more practical label-agnostic setting, where the hackers may exploit the protected data quite differently from the protectors. E.g., a m-class unlearnable dataset held by the protector may be exploited by the hacker as a n-class dataset. Existing UE generation methods are rendered ineffective in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#35299;&#37322;&#22240;&#32032;&#20197;&#22686;&#24378;&#21306;&#20998;&#24615;&#34920;&#31034;&#25552;&#21462;&#65292;&#20197;&#21453;&#21521;&#20445;&#35777;&#32454;&#31890;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#31070;&#32463;&#24433;&#20687;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.00815</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23156;&#20799;&#33041;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A attention way in Explainable methods for infant brain. (arXiv:2301.00815v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#28145;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#35299;&#37322;&#22240;&#32032;&#20197;&#22686;&#24378;&#21306;&#20998;&#24615;&#34920;&#31034;&#25552;&#21462;&#65292;&#20197;&#21453;&#21521;&#20445;&#35777;&#32454;&#31890;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36866;&#29992;&#20110;&#31070;&#32463;&#24433;&#20687;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an explainable geometric deep network that enhances discriminative representation extraction by end-to-end learning of explanation factors, which is a more intuitive strategy to inversely assure fine-grained explainability, suitable for high-dimensional data in neuroimaging and neuroscience studies containing noisy, redundant, and task-irrelevant information.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#23398;&#31185;&#24212;&#29992;&#20013;&#37096;&#32626;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#38656;&#35201;&#23398;&#20064;&#27169;&#22411;&#36755;&#20986;&#20934;&#30830;&#19988;&#65288;&#26356;&#37325;&#35201;&#30340;&#26159;&#65289;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20197;&#20107;&#21518;&#26041;&#24335;&#35299;&#37322;&#32593;&#32476;&#36755;&#20986;&#65292;&#38544;&#21547;&#22320;&#20551;&#35774;&#24544;&#23454;&#30340;&#35299;&#37322;&#26469;&#33258;&#20934;&#30830;&#30340;&#39044;&#27979;/&#20998;&#31867;&#12290;&#25105;&#20204;&#25552;&#20986;&#30456;&#21453;&#30340;&#35266;&#28857;&#65292;&#21363;&#35299;&#37322;&#25552;&#21319;&#65288;&#29978;&#33267;&#20915;&#23450;&#65289;&#20998;&#31867;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#31471;&#21040;&#31471;&#23398;&#20064;&#35299;&#37322;&#22240;&#32032;&#20197;&#22686;&#24378;&#21306;&#20998;&#24615;&#34920;&#31034;&#25552;&#21462;&#21487;&#33021;&#26159;&#19968;&#31181;&#26356;&#30452;&#35266;&#30340;&#31574;&#30053;&#65292;&#20197;&#21453;&#21521;&#20445;&#35777;&#32454;&#31890;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20363;&#22914;&#22312;&#37027;&#20123;&#21253;&#21547;&#22122;&#22768;&#65292;&#20887;&#20313;&#21644;&#20219;&#21153;&#26080;&#20851;&#20449;&#24687;&#30340;&#39640;&#32500;&#25968;&#25454;&#30340;&#31070;&#32463;&#24433;&#20687;&#21644;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#28145;&#24230;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying reliable deep learning techniques in interdisciplinary applications needs learned models to output accurate and ({even more importantly}) explainable predictions. Existing approaches typically explicate network outputs in a post-hoc fashion, under an implicit assumption that faithful explanations come from accurate predictions/classifications. We have an opposite claim that explanations boost (or even determine) classification. That is, end-to-end learning of explanation factors to augment discriminative representation extraction could be a more intuitive strategy to inversely assure fine-grained explainability, e.g., in those neuroimaging and neuroscience studies with high-dimensional data containing noisy, redundant, and task-irrelevant information. In this paper, we propose such an explainable geometric deep network dubbed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#65292;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#21644;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#65292;&#20026;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#25552;&#20379;&#20102;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2301.00812</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
One-shot domain adaptation in video-based assessment of surgical skills. (arXiv:2301.00812v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#65292;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#21644;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#65292;&#20026;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#25552;&#20379;&#20102;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. The model successfully adapts to simulated tasks and laparoscopic cholecystectomy, providing a domain-agnostic procedure for video-based assessment of surgical skills.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#23454;&#29616;&#20102;&#25163;&#26415;&#25216;&#33021;&#30340;&#33258;&#21160;&#21644;&#23458;&#35266;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;&#24182;&#19988;&#21463;&#38480;&#20110;&#20854;&#35757;&#32451;&#39046;&#22495;&#12290;&#36825;&#38459;&#27490;&#20102;&#23427;&#20204;&#36807;&#28193;&#21040;&#25968;&#25454;&#26377;&#38480;&#30340;&#26032;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#20110;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#27169;&#22411;A-VBANet&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#24615;&#23398;&#20064;&#25552;&#20379;&#39046;&#22495;&#19981;&#21487;&#30693;&#30340;&#25163;&#26415;&#25216;&#33021;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#33145;&#33108;&#38236;&#21644;&#26426;&#22120;&#20154;&#25163;&#26415;&#27169;&#25311;&#22120;&#19978;&#24320;&#21457;&#20102;A-VBANet&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#30340;&#25163;&#26415;&#23460;&#35270;&#39057;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#21151;&#22320;&#36866;&#24212;&#20102;&#27169;&#25311;&#20219;&#21153;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;99.5%&#65288;&#19968;&#27425;&#24615;&#65289;&#21644;99.9%&#65288;&#23569;&#37327;&#26679;&#26412;&#65289;&#65292;&#22312;&#33145;&#33108;&#38236;&#32966;&#22218;&#20999;&#38500;&#26415;&#20013;&#30340;&#20934;&#30830;&#29575;&#20026;89.7%&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#22522;&#20110;&#35270;&#39057;&#30340;&#25163;&#26415;&#25216;&#33021;&#35780;&#20272;&#30340;&#39046;&#22495;&#19981;&#21487;&#30693;&#31243;&#24207;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#24433;&#21709;&#26159;&#23427;&#20801;&#35768;&#20351;&#29992;&#26469;&#33258;&#25163;&#26415;&#27169;&#25311;&#22120;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#25163;&#26415;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning (DL) has achieved automatic and objective assessment of surgical skills. However, DL models are data-hungry and restricted to their training domain. This prevents them from transitioning to new tasks where data is limited. Hence, domain adaptation is crucial to implement DL in real life. Here, we propose a meta-learning model, A-VBANet, that can deliver domain-agnostic surgical skill classification via one-shot learning. We develop the A-VBANet on five laparoscopic and robotic surgical simulators. Additionally, we test it on operating room (OR) videos of laparoscopic cholecystectomy. Our model successfully adapts with accuracies up to 99.5% in one-shot and 99.9% in few-shot settings for simulated tasks and 89.7% for laparoscopic cholecystectomy. For the first time, we provide a domain-agnostic procedure for video-based assessment of surgical skills. A significant implication of this approach is that it allows the use of data from surgical simulators to assess performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UniDA3D&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#31649;&#36947;&#65292;&#36890;&#36807;&#35774;&#35745;&#32479;&#19968;&#30340;&#28304;&#21644;&#30446;&#26631;&#20027;&#21160;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#35299;&#20915;&#19977;&#32500;&#20998;&#21106;&#39046;&#22495;&#20013;&#30340;&#22810;&#20010;&#33258;&#36866;&#24212;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#37319;&#26679;&#31574;&#30053;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10390</link><description>&lt;p&gt;
UniDA3D: &#32479;&#19968;&#30340;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
UniDA3D: Unified Domain Adaptive 3D Semantic Segmentation Pipeline. (arXiv:2212.10390v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UniDA3D&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#31649;&#36947;&#65292;&#36890;&#36807;&#35774;&#35745;&#32479;&#19968;&#30340;&#28304;&#21644;&#30446;&#26631;&#20027;&#21160;&#37319;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#35299;&#20915;&#19977;&#32500;&#20998;&#21106;&#39046;&#22495;&#20013;&#30340;&#22810;&#20010;&#33258;&#36866;&#24212;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#37319;&#26679;&#31574;&#30053;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes UniDA3D, a unified domain adaptive 3D semantic segmentation pipeline, which can tackle several adaptation tasks in 3D segmentation field by designing a unified source-and-target active sampling strategy, and investigates the possibility of achieving a multi-modal sampling strategy.
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#26159;&#22312;&#29616;&#25104;&#30340;&#20844;&#20849;&#22522;&#20934;&#19978;&#35757;&#32451;&#30340;&#65292;&#20294;&#24403;&#36825;&#20123;&#35757;&#32451;&#33391;&#22909;&#30340;&#27169;&#22411;&#37096;&#32626;&#21040;&#26032;&#39046;&#22495;&#26102;&#65292;&#23427;&#20204;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#35782;&#21035;&#31934;&#24230;&#19979;&#38477;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#22495;&#33258;&#36866;&#24212;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#31649;&#36947;&#65288;UniDA3D&#65289;&#65292;&#20197;&#22686;&#24378;&#24369;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24357;&#21512;&#22495;&#20043;&#38388;&#30340;&#28857;&#20998;&#24067;&#24046;&#36317;&#12290;&#19982;&#20043;&#21069;&#21482;&#20851;&#27880;&#21333;&#19968;&#33258;&#36866;&#24212;&#20219;&#21153;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;UniDA3D&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#32479;&#19968;&#30340;&#28304;&#21644;&#30446;&#26631;&#20027;&#21160;&#37319;&#26679;&#31574;&#30053;&#26469;&#35299;&#20915;&#19977;&#32500;&#20998;&#21106;&#39046;&#22495;&#20013;&#30340;&#22810;&#20010;&#33258;&#36866;&#24212;&#20219;&#21153;&#65292;&#35813;&#31574;&#30053;&#20174;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#23376;&#38598;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#27169;&#22411;&#33258;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;&#22810;&#27169;&#24577;&#20108;&#32500;-&#19977;&#32500;&#25968;&#25454;&#38598;&#30340;&#23835;&#36215;&#30340;&#24433;&#21709;&#65292;UniDA3D&#25506;&#32034;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#37319;&#26679;&#31574;&#30053;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#24320;&#21457;&#36328;&#27169;&#24577;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#65292;&#21487;&#20197;&#25552;&#21462;&#20195;&#34920;&#24615;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art 3D semantic segmentation models are trained on off-the-shelf public benchmarks, but they will inevitably face the challenge of recognition accuracy drop when these well-trained models are deployed to a new domain. In this paper, we introduce a Unified Domain Adaptive 3D semantic segmentation pipeline (UniDA3D) to enhance the weak generalization ability, and bridge the point distribution gap between domains. Different from previous studies that only focus on a single adaptation task, UniDA3D can tackle several adaptation tasks in 3D segmentation field, by designing a unified source-and-target active sampling strategy, which selects a maximally-informative subset from both source and target domains for effective model adaptation. Besides, benefiting from the rise of multi-modal 2D-3D datasets, UniDA3D investigates the possibility of achieving a multi-modal sampling strategy, by developing a cross-modality feature interaction module that can extract a representative pair 
&lt;/p&gt;</description></item><item><title>&#33521;&#22269;COVID-19 Vocal Audio Dataset&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;SARS-CoV-2 PCR&#21442;&#32771;&#38899;&#39057;&#35760;&#24405;&#38598;&#21512;&#65292;&#26088;&#22312;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;SARS-CoV-2&#24863;&#26579;&#29366;&#24577;&#25110;&#30456;&#20851;&#21628;&#21560;&#30151;&#29366;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2212.07738</link><description>&lt;p&gt;
&#19968;&#20221;&#22823;&#35268;&#27169;&#30340;&#12289;&#22522;&#20110;PCR&#30340;COVID-19&#22768;&#38899;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A large-scale and PCR-referenced vocal audio dataset for COVID-19. (arXiv:2212.07738v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07738
&lt;/p&gt;
&lt;p&gt;
&#33521;&#22269;COVID-19 Vocal Audio Dataset&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;SARS-CoV-2 PCR&#21442;&#32771;&#38899;&#39057;&#35760;&#24405;&#38598;&#21512;&#65292;&#26088;&#22312;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;SARS-CoV-2&#24863;&#26579;&#29366;&#24577;&#25110;&#30456;&#20851;&#21628;&#21560;&#30151;&#29366;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The UK COVID-19 Vocal Audio Dataset is the largest collection of SARS-CoV-2 PCR-referenced audio recordings to date, designed for the training and evaluation of machine learning models that classify SARS-CoV-2 infection status or associated respiratory symptoms using vocal audio.
&lt;/p&gt;
&lt;p&gt;
&#33521;&#22269;COVID-19 Vocal Audio Dataset&#26088;&#22312;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#22768;&#38899;&#25968;&#25454;&#20998;&#31867;SARS-CoV-2&#24863;&#26579;&#29366;&#24577;&#25110;&#30456;&#20851;&#21628;&#21560;&#30151;&#29366;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35774;&#35745;&#12290;&#33521;&#22269;&#21355;&#29983;&#23433;&#20840;&#23616;&#36890;&#36807;&#22269;&#23478;&#27979;&#35797;&#21644;&#36861;&#36394;&#35745;&#21010;&#21644;REACT-1&#35843;&#26597;&#22312;2021&#24180;3&#26376;&#33267;2022&#24180;3&#26376;&#26399;&#38388;&#25307;&#21215;&#20102;&#33258;&#24895;&#21442;&#19982;&#32773;&#65292;&#25910;&#38598;&#20102;&#33258;&#24895;&#21683;&#22013;&#12289;&#21628;&#27668;&#21644;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#65292;&#24182;&#23558;&#20854;&#19982;SARS-CoV-2&#26816;&#27979;&#32467;&#26524;&#30456;&#20851;&#32852;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;SARS-CoV-2 PCR&#21442;&#32771;&#38899;&#39057;&#35760;&#24405;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The UK COVID-19 Vocal Audio Dataset is designed for the training and evaluation of machine learning models that classify SARS-CoV-2 infection status or associated respiratory symptoms using vocal audio. The UK Health Security Agency recruited voluntary participants through the national Test and Trace programme and the REACT-1 survey in England from March 2021 to March 2022, during dominant transmission of the Alpha and Delta SARS-CoV-2 variants and some Omicron variant sublineages. Audio recordings of volitional coughs, exhalations, and speech were collected in the 'Speak up to help beat coronavirus' digital survey alongside demographic, self-reported symptom and respiratory condition data, and linked to SARS-CoV-2 test results. The UK COVID-19 Vocal Audio Dataset represents the largest collection of SARS-CoV-2 PCR-referenced audio recordings to date. PCR results were linked to 70,794 of 72,999 participants and 24,155 of 25,776 positive cases. Respiratory symptoms were reported by 45.6
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26680;&#21270;Stein&#24046;&#24322;&#65288;KSD&#65289;&#21644;&#24076;&#23572;&#20271;&#29305;&#20869;&#31215;&#65288;HIP&#65289;&#30340;DSVGD&#36873;&#25321;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#32852;&#37030;&#36125;&#21494;&#26031;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.05492</link><description>&lt;p&gt;
&#32852;&#37030;&#36125;&#21494;&#26031;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Client Selection for Federated Bayesian Learning. (arXiv:2212.05492v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26680;&#21270;Stein&#24046;&#24322;&#65288;KSD&#65289;&#21644;&#24076;&#23572;&#20271;&#29305;&#20869;&#31215;&#65288;HIP&#65289;&#30340;DSVGD&#36873;&#25321;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#32852;&#37030;&#36125;&#21494;&#26031;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes two selection schemes for Distributed Stein Variational Gradient Descent (DSVGD) based on Kernelized Stein Discrepancy (KSD) and Hilbert Inner Product (HIP) to improve the model convergence and communication efficiency in federated Bayesian learning.
&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;Stein&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#65288;DSVGD&#65289;&#26159;&#19968;&#31181;&#38750;&#21442;&#25968;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#36890;&#36807;&#19982;&#26381;&#21153;&#22120;&#36890;&#20449;&#19968;&#23450;&#25968;&#37327;&#30340;&#38750;&#38543;&#26426;&#21644;&#20132;&#20114;&#31890;&#23376;&#26469;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#30001;&#20110;&#36890;&#20449;&#36164;&#28304;&#26377;&#38480;&#65292;&#36873;&#25321;&#20855;&#26377;&#26368;&#20855;&#20449;&#24687;&#24615;&#30340;&#26412;&#22320;&#23398;&#20064;&#26356;&#26032;&#30340;&#23458;&#25143;&#31471;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#25910;&#25947;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26680;&#21270;Stein&#24046;&#24322;&#65288;KSD&#65289;&#21644;&#24076;&#23572;&#20271;&#29305;&#20869;&#31215;&#65288;HIP&#65289;&#30340;DSVGD&#36873;&#25321;&#26041;&#26696;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20004;&#31181;&#26041;&#26696;&#27599;&#27425;&#36845;&#20195;&#20840;&#23616;&#33258;&#30001;&#33021;&#19979;&#38477;&#30340;&#19978;&#30028;&#65292;&#28982;&#21518;&#23558;&#20854;&#26368;&#23567;&#21270;&#20197;&#21152;&#36895;&#27169;&#22411;&#25910;&#25947;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#23398;&#20064;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#26696;&#19982;&#20256;&#32479;&#26041;&#26696;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#25910;&#25947;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Stein Variational Gradient Descent (DSVGD) is a non-parametric distributed learning framework for federated Bayesian learning, where multiple clients jointly train a machine learning model by communicating a number of non-random and interacting particles with the server. Since communication resources are limited, selecting the clients with most informative local learning updates can improve the model convergence and communication efficiency. In this paper, we propose two selection schemes for DSVGD based on Kernelized Stein Discrepancy (KSD) and Hilbert Inner Product (HIP). We derive the upper bound on the decrease of the global free energy per iteration for both schemes, which is then minimized to speed up the model convergence. We evaluate and compare our schemes with conventional schemes in terms of model accuracy, convergence speed, and stability using various learning tasks and datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#26469;&#20248;&#21270;&#28304;&#29305;&#24449;&#30340;&#20223;&#23556;&#21464;&#25442;&#65292;&#23558;&#20854;&#24341;&#23548;&#21040;&#30446;&#26631;&#25991;&#26412;&#23884;&#20837;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20869;&#23481;&#21644;&#35821;&#20041;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;CLIP&#30340;&#39118;&#26684;&#36716;&#31227;&#22522;&#32447;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2212.03241</link><description>&lt;p&gt;
P{\O}DA: &#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
P{\O}DA: Prompt-driven Zero-shot Domain Adaptation. (arXiv:2212.03241v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#26469;&#20248;&#21270;&#28304;&#29305;&#24449;&#30340;&#20223;&#23556;&#21464;&#25442;&#65292;&#23558;&#20854;&#24341;&#23548;&#21040;&#30446;&#26631;&#25991;&#26412;&#23884;&#20837;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20869;&#23481;&#21644;&#35821;&#20041;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;CLIP&#30340;&#39118;&#26684;&#36716;&#31227;&#22522;&#32447;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a prompt-driven zero-shot domain adaptation method, which leverages a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Experiments demonstrate that the method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand.
&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20173;&#38656;&#35201;&#22312;&#35757;&#32451;&#26102;&#35775;&#38382;&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#22312;&#26576;&#20123;&#19981;&#24120;&#35265;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#8221;&#20219;&#21153;&#65292;&#20854;&#20013;&#25105;&#20204;&#20165;&#20351;&#29992;&#30446;&#26631;&#22495;&#30340;&#21333;&#20010;&#36890;&#29992;&#25991;&#26412;&#25551;&#36848;&#65288;&#21363;&#25552;&#31034;&#65289;&#26469;&#35843;&#25972;&#22312;&#28304;&#22495;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#26469;&#20248;&#21270;&#28304;&#29305;&#24449;&#30340;&#20223;&#23556;&#21464;&#25442;&#65292;&#23558;&#20854;&#24341;&#23548;&#21040;&#30446;&#26631;&#25991;&#26412;&#23884;&#20837;&#20013;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20869;&#23481;&#21644;&#35821;&#20041;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22686;&#24378;&#30340;&#29305;&#24449;&#21487;&#20197;&#29992;&#20110;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#30340;&#38646;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;CLIP&#30340;&#39118;&#26684;&#36716;&#31227;&#22522;&#32447;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#19968;&#27425;&#24615;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#24182;&#19988;gi
&lt;/p&gt;
&lt;p&gt;
Domain adaptation has been vastly investigated in computer vision but still requires access to target images at train time, which might be intractable in some uncommon conditions. In this paper, we propose the task of `Prompt-driven Zero-shot Domain Adaptation', where we adapt a model trained on a source domain using only a single general textual description of the target domain, i.e., a prompt. First, we leverage a pretrained contrastive vision-language model (CLIP) to optimize affine transformations of source features, steering them towards target text embeddings, while preserving their content and semantics. Second, we show that augmented features can be used to perform zero-shot domain adaptation for semantic segmentation. Experiments demonstrate that our method significantly outperforms CLIP-based style transfer baselines on several datasets for the downstream task at hand. Our prompt-driven approach even outperforms one-shot unsupervised domain adaptation on some datasets, and gi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#65288;UDOP&#65289;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#27169;&#24577;&#20197;&#21450;&#21508;&#31181;&#20219;&#21153;&#26684;&#24335;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#27169;&#22411;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#22810;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#32479;&#19968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25991;&#26723;&#32534;&#36753;&#21644;&#20869;&#23481;&#23450;&#21046;&#12290;</title><link>http://arxiv.org/abs/2212.02623</link><description>&lt;p&gt;
&#32479;&#19968;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#24067;&#23616;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Unifying Vision, Text, and Layout for Universal Document Processing. (arXiv:2212.02623v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#65288;UDOP&#65289;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#27169;&#24577;&#20197;&#21450;&#21508;&#31181;&#20219;&#21153;&#26684;&#24335;&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#27169;&#22411;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#22810;&#22495;&#19979;&#28216;&#20219;&#21153;&#30340;&#32479;&#19968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25991;&#26723;&#32534;&#36753;&#21644;&#20869;&#23481;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes the Universal Document Processing (UDOP) model, which unifies text, image, and layout modalities together with varied task formats, and achieves pretraining and multi-domain downstream tasks unification through a novel Transformer model. It also achieves high-quality neural document editing and content customization.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#65288;UDOP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#30784;&#30340;&#25991;&#26723;AI&#27169;&#22411;&#65292;&#23427;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#24067;&#23616;&#27169;&#24577;&#20197;&#21450;&#21508;&#31181;&#20219;&#21153;&#26684;&#24335;&#65288;&#21253;&#25324;&#25991;&#26723;&#29702;&#35299;&#21644;&#29983;&#25104;&#65289;&#32479;&#19968;&#36215;&#26469;&#12290;UDOP&#21033;&#29992;&#25991;&#26412;&#20869;&#23481;&#21644;&#25991;&#26723;&#22270;&#20687;&#20043;&#38388;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#34920;&#31034;&#26469;&#24314;&#27169;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#24577;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Vision-Text-Layout Transformer&#65292;UDOP&#23558;&#39044;&#35757;&#32451;&#21644;&#22810;&#22495;&#19979;&#28216;&#20219;&#21153;&#32479;&#19968;&#21040;&#22522;&#20110;&#25552;&#31034;&#30340;&#24207;&#21015;&#29983;&#25104;&#26041;&#26696;&#20013;&#12290;UDOP&#22312;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25991;&#26723;&#35821;&#26009;&#24211;&#21644;&#22810;&#26679;&#21270;&#26631;&#35760;&#25968;&#25454;&#19978;&#20351;&#29992;&#21019;&#26032;&#30340;&#33258;&#30417;&#30563;&#30446;&#26631;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;UDOP&#36824;&#36890;&#36807;&#36974;&#34109;&#22270;&#20687;&#37325;&#24314;&#23398;&#20064;&#20174;&#25991;&#26412;&#21644;&#24067;&#23616;&#27169;&#24577;&#29983;&#25104;&#25991;&#26723;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#25991;&#26723;AI&#39046;&#22495;&#20013;&#31532;&#19968;&#27425;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25991;&#26723;&#32534;&#36753;&#21644;&#20869;&#23481;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;8&#20010;&#25991;&#26723;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Universal Document Processing (UDOP), a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation. UDOP leverages the spatial correlation between textual content and document image to model image, text, and layout modalities with one uniform representation. With a novel Vision-Text-Layout Transformer, UDOP unifies pretraining and multi-domain downstream tasks into a prompt-based sequence generation scheme. UDOP is pretrained on both large-scale unlabeled document corpora using innovative self-supervised objectives and diverse labeled data. UDOP also learns to generate document images from text and layout modalities via masked image reconstruction. To the best of our knowledge, this is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization. Our method sets the state-of-the-art on 8 Docu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DL&#30340;Mel-Subband&#26102;&#31354;&#27874;&#26463;&#25104;&#24418;&#22120;&#65292;&#29992;&#20110;&#22312;&#36710;&#36733;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#65292;&#36890;&#36807;&#22522;&#20110;Mel&#23610;&#24230;&#30340;&#23376;&#24102;&#36873;&#25321;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#20302;&#39057;&#30340;&#32454;&#31890;&#24230;&#22788;&#29702;&#21644;&#23545;&#39640;&#39057;&#30340;&#31895;&#31890;&#24230;&#22788;&#29702;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2211.12590</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;Mel-Subband&#27874;&#26463;&#25104;&#24418;&#22120;&#29992;&#20110;&#36710;&#36733;&#35821;&#38899;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Mel-Subband Beamformer for In-car Speech Separation. (arXiv:2211.12590v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DL&#30340;Mel-Subband&#26102;&#31354;&#27874;&#26463;&#25104;&#24418;&#22120;&#65292;&#29992;&#20110;&#22312;&#36710;&#36733;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#65292;&#36890;&#36807;&#22522;&#20110;Mel&#23610;&#24230;&#30340;&#23376;&#24102;&#36873;&#25321;&#31574;&#30053;&#65292;&#23454;&#29616;&#23545;&#20302;&#39057;&#30340;&#32454;&#31890;&#24230;&#22788;&#29702;&#21644;&#23545;&#39640;&#39057;&#30340;&#31895;&#31890;&#24230;&#22788;&#29702;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a DL-based Mel-Subband spatio-temporal beamformer for speech separation in a car environment, which reduces computational costs and inference time by using a Mel-scale based subband selection strategy for fine-grained processing of lower frequencies and coarse-grained processing of higher frequencies.
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#22522;&#20110;&#27874;&#26463;&#25104;&#24418;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#22312;&#35821;&#38899;&#20998;&#31163;&#20013;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#34987;&#35774;&#35745;&#20026;&#29420;&#31435;&#22788;&#29702;&#31364;&#24102;&#65288;NB&#65289;&#39057;&#29575;&#65292;&#36825;&#23548;&#33268;&#26356;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#21512;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;DL&#30340;Mel-Subband&#26102;&#31354;&#27874;&#26463;&#25104;&#24418;&#22120;&#65292;&#20197;&#22312;&#36710;&#36733;&#29615;&#22659;&#20013;&#36827;&#34892;&#35821;&#38899;&#20998;&#31163;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;&#19982;&#20256;&#32479;&#30340;&#23376;&#24102;&#65288;SB&#65289;&#26041;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#22522;&#20110;Mel&#23610;&#24230;&#30340;&#23376;&#24102;&#36873;&#25321;&#31574;&#30053;&#65292;&#30830;&#20445;&#23545;&#22823;&#22810;&#25968;&#35821;&#38899;&#20849;&#25391;&#32467;&#26500;&#23384;&#22312;&#30340;&#20302;&#39057;&#36827;&#34892;&#32454;&#31890;&#24230;&#22788;&#29702;&#65292;&#23545;&#39640;&#39057;&#36827;&#34892;&#31895;&#31890;&#24230;&#22788;&#29702;&#12290;&#20197;&#36882;&#24402;&#26041;&#24335;&#65292;&#20174;&#20272;&#35745;&#30340;&#23376;&#24102;&#35821;&#38899;&#21644;&#22122;&#22768;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#30830;&#23450;&#27599;&#20010;&#25196;&#22768;&#22120;&#20301;&#32622;/&#21306;&#22495;&#30340;&#40065;&#26834;&#24103;&#32423;&#27874;&#26463;&#25104;&#24418;&#26435;&#37325;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#20272;&#35745;&#24182;&#25233;&#21046;&#20219;&#20309;&#22238;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
While current deep learning (DL)-based beamforming techniques have been proved effective in speech separation, they are often designed to process narrow-band (NB) frequencies independently which results in higher computational costs and inference times, making them unsuitable for real-world use. In this paper, we propose DL-based mel-subband spatio-temporal beamformer to perform speech separation in a car environment with reduced computation cost and inference time. As opposed to conventional subband (SB) approaches, our framework uses a mel-scale based subband selection strategy which ensures a fine-grained processing for lower frequencies where most speech formant structure is present, and coarse-grained processing for higher frequencies. In a recursive way, robust frame-level beamforming weights are determined for each speaker location/zone in a car from the estimated subband speech and noise covariance matrices. Furthermore, proposed framework also estimates and suppresses any echo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#24352;&#22270;&#20687;&#25110;&#35270;&#39057;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SinFusion&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#22270;&#20687;/&#35270;&#39057;&#29305;&#23450;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#30340;&#36816;&#21160;&#21644;&#21160;&#24577;&#65292;&#29983;&#25104;&#30456;&#21516;&#21160;&#24577;&#22330;&#26223;&#30340;&#22810;&#26679;&#21270;&#26032;&#35270;&#39057;&#26679;&#26412;&#65292;&#23558;&#30701;&#35270;&#39057;&#25512;&#24191;&#20026;&#38271;&#35270;&#39057;&#65288;&#21521;&#21069;&#21644;&#21521;&#21518;&#65289;&#24182;&#25191;&#34892;&#35270;&#39057;&#19978;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2211.11743</link><description>&lt;p&gt;
SinFusion&#65306;&#22312;&#21333;&#24352;&#22270;&#20687;&#25110;&#35270;&#39057;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SinFusion: Training Diffusion Models on a Single Image or Video. (arXiv:2211.11743v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#24352;&#22270;&#20687;&#25110;&#35270;&#39057;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;SinFusion&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#22270;&#20687;/&#35270;&#39057;&#29305;&#23450;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#30340;&#36816;&#21160;&#21644;&#21160;&#24577;&#65292;&#29983;&#25104;&#30456;&#21516;&#21160;&#24577;&#22330;&#26223;&#30340;&#22810;&#26679;&#21270;&#26032;&#35270;&#39057;&#26679;&#26412;&#65292;&#23558;&#30701;&#35270;&#39057;&#25512;&#24191;&#20026;&#38271;&#35270;&#39057;&#65288;&#21521;&#21069;&#21644;&#21521;&#21518;&#65289;&#24182;&#25191;&#34892;&#35270;&#39057;&#19978;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#36229;&#36807;&#20102;GAN&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#26159;&#22312;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#19981;&#33258;&#28982;&#22320;&#36866;&#24212;&#20110;&#25805;&#20316;&#32473;&#23450;&#30340;&#36755;&#20837;&#22270;&#20687;&#25110;&#35270;&#39057;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#21333;&#20010;&#36755;&#20837;&#22270;&#20687;&#25110;&#35270;&#39057;&#19978;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#22270;&#20687;/&#35270;&#39057;&#29305;&#23450;&#25193;&#25955;&#27169;&#22411;&#65288;SinFusion&#65289;&#23398;&#20064;&#21333;&#20010;&#22270;&#20687;&#25110;&#35270;&#39057;&#30340;&#22806;&#35266;&#21644;&#21160;&#24577;&#65292;&#21516;&#26102;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26465;&#20214;&#33021;&#21147;&#12290;&#23427;&#21487;&#20197;&#35299;&#20915;&#21508;&#31181;&#22270;&#20687;/&#35270;&#39057;&#29305;&#23450;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;&#23569;&#37327;&#24103;&#20013;&#23398;&#20064;&#21333;&#20010;&#36755;&#20837;&#35270;&#39057;&#30340;&#36816;&#21160;&#21644;&#21160;&#24577;&#12290;&#28982;&#21518;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#30456;&#21516;&#21160;&#24577;&#22330;&#26223;&#30340;&#22810;&#26679;&#21270;&#26032;&#35270;&#39057;&#26679;&#26412;&#65292;&#23558;&#30701;&#35270;&#39057;&#25512;&#24191;&#20026;&#38271;&#35270;&#39057;&#65288;&#21521;&#21069;&#21644;&#21521;&#21518;&#65289;&#24182;&#25191;&#34892;&#35270;&#39057;&#19978;&#37319;&#26679;&#12290;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22823;&#22810;&#25968;&#37117;&#26080;&#27861;&#36890;&#36807;&#24403;&#21069;&#30340;&#35270;&#39057;&#29305;&#23450;&#29983;&#25104;&#26041;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models exhibited tremendous progress in image and video generation, exceeding GANs in quality and diversity. However, they are usually trained on very large datasets and are not naturally adapted to manipulate a given input image or video. In this paper we show how this can be resolved by training a diffusion model on a single input image or video. Our image/video-specific diffusion model (SinFusion) learns the appearance and dynamics of the single image or video, while utilizing the conditioning capabilities of diffusion models. It can solve a wide array of image/video-specific manipulation tasks. In particular, our model can learn from few frames the motion and dynamics of a single input video. It can then generate diverse new video samples of the same dynamic scene, extrapolate short videos into long ones (both forward and backward in time) and perform video upsampling. Most of these tasks are not realizable by current video-specific generation methods.
&lt;/p&gt;</description></item><item><title>LA-VocE&#26159;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#23558;&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#39044;&#27979;&#30340;mel&#39057;&#35889;&#22270;&#36716;&#25442;&#20026;&#27874;&#24418;&#38899;&#39057;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#21644;&#19981;&#21516;&#27700;&#24179;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2211.10999</link><description>&lt;p&gt;
LA-VocE: &#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#30340;&#20302;&#20449;&#22122;&#27604;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
LA-VocE: Low-SNR Audio-visual Speech Enhancement using Neural Vocoders. (arXiv:2211.10999v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10999
&lt;/p&gt;
&lt;p&gt;
LA-VocE&#26159;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#23558;&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#39044;&#27979;&#30340;mel&#39057;&#35889;&#22270;&#36716;&#25442;&#20026;&#27874;&#24418;&#38899;&#39057;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#21644;&#19981;&#21516;&#27700;&#24179;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
LA-VocE is a new audio-visual speech enhancement method that uses a neural vocoder to convert mel-spectrograms predicted from noisy audio-visual speech via a transformer-based architecture into waveform audio, and is applicable to multiple languages and different levels of background noise and speech interference.
&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;&#26412;&#36523;&#20197;&#21450;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#21767;&#37096;&#36816;&#21160;&#20174;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#25552;&#21462;&#24178;&#20928;&#30340;&#35821;&#38899;&#12290;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#27604;&#20165;&#20351;&#29992;&#38899;&#39057;&#30340;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28040;&#38500;&#24178;&#25200;&#35821;&#38899;&#12290;&#23613;&#31649;&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#38899;&#39057;&#35270;&#35273;&#26041;&#27861;&#20173;&#28982;&#20351;&#29992;&#39057;&#35889;&#26144;&#23556;/&#25513;&#34109;&#26469;&#37325;&#29616;&#24178;&#20928;&#30340;&#38899;&#39057;&#65292;&#36890;&#24120;&#20250;&#22312;&#29616;&#26377;&#30340;&#35821;&#38899;&#22686;&#24378;&#26550;&#26500;&#20013;&#28155;&#21152;&#35270;&#35273;&#39592;&#24178;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LA-VocE&#65292;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#20174;&#22024;&#26434;&#30340;&#38899;&#39057;&#35270;&#35273;&#35821;&#38899;&#39044;&#27979;mel&#39057;&#35889;&#22270;&#65292;&#28982;&#21518;&#20351;&#29992;&#31070;&#32463;&#22768;&#30721;&#22120;&#65288;HiFi-GAN&#65289;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#27874;&#24418;&#38899;&#39057;&#12290;&#25105;&#20204;&#22312;&#25968;&#21315;&#20010;&#35828;&#35805;&#32773;&#21644;11&#31181;&#20197;&#19978;&#19981;&#21516;&#30340;&#35821;&#35328;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#25105;&#20204;&#30340;&#27169;&#22411;&#36866;&#24212;&#19981;&#21516;&#27700;&#24179;&#30340;&#32972;&#26223;&#22122;&#22768;&#21644;&#35821;&#38899;&#24178;&#25200;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Audio-visual speech enhancement aims to extract clean speech from a noisy environment by leveraging not only the audio itself but also the target speaker's lip movements. This approach has been shown to yield improvements over audio-only speech enhancement, particularly for the removal of interfering speech. Despite recent advances in speech synthesis, most audio-visual approaches continue to use spectral mapping/masking to reproduce the clean audio, often resulting in visual backbones added to existing speech enhancement architectures. In this work, we propose LA-VocE, a new two-stage approach that predicts mel-spectrograms from noisy audio-visual speech via a transformer-based architecture, and then converts them into waveform audio using a neural vocoder (HiFi-GAN). We train and evaluate our framework on thousands of speakers and 11+ different languages, and study our model's ability to adapt to different levels of background noise and speech interference. Our experiments show that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#25104;&#20687;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#27979;&#37327;&#22352;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#26679;&#26465;&#25554;&#20540;&#22120;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;2D&#21644;3D&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65292;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.10525</link><description>&lt;p&gt;
&#21487;&#24494;&#38750;&#26631;&#23450;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Differentiable Uncalibrated Imaging. (arXiv:2211.10525v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#30340;&#25104;&#20687;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#27979;&#37327;&#22352;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#26679;&#26465;&#25554;&#20540;&#22120;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;2D&#21644;3D&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65292;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a differentiable imaging framework to address uncertainty in measurement coordinates, using implicit neural networks and differentiable spline interpolators. The method is applied to 2D and 3D computed tomography and produces improved reconstructions.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#25104;&#20687;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#27979;&#37327;&#22352;&#26631;&#65288;&#22914;&#20256;&#24863;&#22120;&#20301;&#32622;&#21644;&#25237;&#24433;&#35282;&#24230;&#65289;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#20844;&#24335;&#21270;&#20026;&#22312;&#26410;&#30693;&#33410;&#28857;&#22788;&#30340;&#27979;&#37327;&#25554;&#20540;&#65292;&#36890;&#36807;&#27491;&#21521;&#31639;&#23376;&#36827;&#34892;&#30417;&#30563;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#38544;&#24335;&#31070;&#32463;&#32593;&#32476;&#65292;&#20063;&#31216;&#20026;&#31070;&#32463;&#22330;&#65292;&#23427;&#20204;&#22312;&#36755;&#20837;&#22352;&#26631;&#26041;&#38754;&#33258;&#28982;&#21487;&#24494;&#20998;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#21487;&#24494;&#20998;&#26679;&#26465;&#25554;&#20540;&#22120;&#65292;&#20854;&#24615;&#33021;&#19982;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#22909;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#20248;&#21270;&#26102;&#38388;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#36136;&#12290;&#21487;&#24494;&#24615;&#26159;&#20851;&#38190;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#25105;&#20204;&#20849;&#21516;&#25311;&#21512;&#27979;&#37327;&#34920;&#31034;&#65292;&#20248;&#21270;&#19981;&#30830;&#23450;&#30340;&#27979;&#37327;&#22352;&#26631;&#65292;&#24182;&#25191;&#34892;&#22270;&#20687;&#37325;&#24314;&#65292;&#20174;&#32780;&#30830;&#20445;&#19968;&#33268;&#30340;&#26631;&#23450;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;2D&#21644;3D&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#19981;&#32771;&#34385;&#32570;&#20047;&#26631;&#23450;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#23427;&#20135;&#29983;&#20102;&#25913;&#36827;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;
&lt;/p&gt;
&lt;p&gt;
We propose a differentiable imaging framework to address uncertainty in measurement coordinates such as sensor locations and projection angles. We formulate the problem as measurement interpolation at unknown nodes supervised through the forward operator. To solve it we apply implicit neural networks, also known as neural fields, which are naturally differentiable with respect to the input coordinates. We also develop differentiable spline interpolators which perform as well as neural networks, require less time to optimize and have well-understood properties. Differentiability is key as it allows us to jointly fit a measurement representation, optimize over the uncertain measurement coordinates, and perform image reconstruction which in turn ensures consistent calibration. We apply our approach to 2D and 3D computed tomography and show that it produces improved reconstructions compared to baselines that do not account for the lack of calibration. The flexibility of the proposed framew
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;HMOE&#65292;&#23427;&#19981;&#38656;&#35201;&#39046;&#22495;&#26631;&#31614;&#65292;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.08253</link><description>&lt;p&gt;
HMOE: &#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization. (arXiv:2211.08253v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;HMOE&#65292;&#23427;&#19981;&#38656;&#35201;&#39046;&#22495;&#26631;&#31614;&#65292;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel domain generalization method called HMOE, which does not rely on domain labels and is more interpretable. HMOE uses hypernetworks to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. Experimental results show that HMOE can divide mixed data and achieve better performance.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36890;&#24120;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#36825;&#23601;&#26159;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#30340;&#30446;&#30340;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;DG&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#21487;&#29992;&#30340;&#39046;&#22495;&#26631;&#31614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DG&#26041;&#27861;&#65292;&#31216;&#20026;HMOE&#65306;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65288;MoE&#65289;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#39046;&#22495;&#26631;&#31614;&#65292;&#24182;&#19988;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#12290;MoE&#22312;&#35782;&#21035;&#25968;&#25454;&#20013;&#30340;&#24322;&#36136;&#27169;&#24335;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#23545;&#20110;DG&#38382;&#39064;&#65292;&#24322;&#36136;&#24615;&#27491;&#26159;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#32780;&#20135;&#29983;&#30340;&#12290;HMOE&#20351;&#29992;&#36229;&#32593;&#32476;&#23558;&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;&#19987;&#23478;&#26435;&#37325;&#65292;&#36825;&#20351;&#24471;&#19987;&#23478;&#21487;&#20197;&#20849;&#20139;&#26377;&#29992;&#30340;&#20803;&#30693;&#35782;&#65292;&#24182;&#33021;&#22815;&#22312;&#20302;&#32500;&#21521;&#37327;&#31354;&#38388;&#20013;&#25506;&#32034;&#19987;&#23478;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22312;&#20844;&#24179;&#21644;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;-DomainBed&#19979;&#23558;HMOE&#19982;&#20854;&#20182;DG&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;HMOE&#21487;&#20197;&#21010;&#20998;&#28151;&#21512;&#25968;&#25454;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to domain shift, machine learning systems typically fail to generalize well to domains different from those of training data, which is what domain generalization (DG) aims to address. Although various DG methods have been developed, most of them lack interpretability and require domain labels that are not available in many real-world scenarios. This paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does not rely on domain labels and is more interpretable. MoE proves effective in identifying heterogeneous patterns in data. For the DG problem, heterogeneity arises exactly from domain shift. HMOE uses hypernetworks taking vectors as input to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. We compare HMOE with other DG algorithms under a fair and unified benchmark-DomainBed. Our extensive experiments show that HMOE can divide mixe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#20197;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#35821;&#35328;&#21644;&#24863;&#30693;&#21464;&#21270;&#65292;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#26500;&#36896;&#65292;&#22312;&#28508;&#22312;&#30340;&#31070;&#32463;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20801;&#35768;&#23545;&#36755;&#20837;&#22330;&#26223;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2211.06652</link><description>&lt;p&gt;
&#23398;&#20064;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#20197;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Neuro-symbolic Programs for Language Guided Robot Manipulation. (arXiv:2211.06652v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06652
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#20197;&#36827;&#34892;&#35821;&#35328;&#24341;&#23548;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#35821;&#35328;&#21644;&#24863;&#30693;&#21464;&#21270;&#65292;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#26500;&#36896;&#65292;&#22312;&#28508;&#22312;&#30340;&#31070;&#32463;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20801;&#35768;&#23545;&#36755;&#20837;&#22330;&#26223;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for learning neuro-symbolic programs for language guided robot manipulation, which can handle linguistic and perceptual variations, is end-to-end trainable, and requires no intermediate supervision. The method uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene.
&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#36755;&#20837;&#22330;&#26223;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#36755;&#20986;&#19968;&#20010;&#21487;&#20197;&#30001;&#26426;&#22120;&#20154;&#25191;&#34892;&#30340;&#25805;&#20316;&#31243;&#24207;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#38480;&#21046;&#20043;&#19968;&#65306;&#65288;i&#65289;&#20381;&#36182;&#25163;&#24037;&#32534;&#30721;&#30340;&#27010;&#24565;&#31526;&#21495;&#65292;&#38480;&#21046;&#20102;&#36229;&#20986;&#35757;&#32451;&#26399;&#38388;&#25152;&#35265;&#30340;&#19968;&#33324;&#21270;&#33021;&#21147;[1]&#65288;ii&#65289;&#20174;&#25351;&#20196;&#20013;&#25512;&#26029;&#20986;&#21160;&#20316;&#24207;&#21015;&#65292;&#20294;&#38656;&#35201;&#23494;&#38598;&#30340;&#23376;&#30446;&#26631;&#30417;&#30563;[2]&#25110;&#65288;iii&#65289;&#32570;&#20047;&#35299;&#37322;&#22797;&#26434;&#25351;&#20196;&#25152;&#38656;&#30340;&#35821;&#20041;&#65292;&#36825;&#31181;&#35821;&#20041;&#38656;&#35201;&#26356;&#28145;&#20837;&#30340;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#25512;&#29702;[3]&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#35821;&#35328;&#21644;&#24863;&#30693;&#21464;&#21270;&#65292;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#65292;&#19981;&#38656;&#35201;&#20013;&#38388;&#30417;&#30563;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#20351;&#29992;&#31526;&#21495;&#25512;&#29702;&#26500;&#36896;&#65292;&#36825;&#20123;&#26500;&#36896;&#22312;&#28508;&#22312;&#30340;&#31070;&#32463;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20801;&#35768;&#23545;&#36755;&#20837;&#22330;&#26223;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#32467;&#26500;&#65292;&#21253;&#25324;&#20998;&#23618;&#25351;&#20196;&#35299;&#26512;&#22120;&#21644;&#21160;&#20316;&#27169;&#25311;&#22120;&#65292;&#20197;&#23398;&#20064;&#35299;&#32806;&#30340;&#34892;&#21160;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a natural language instruction and an input scene, our goal is to train a model to output a manipulation program that can be executed by the robot. Prior approaches for this task possess one of the following limitations: (i) rely on hand-coded symbols for concepts limiting generalization beyond those seen during training [1] (ii) infer action sequences from instructions but require dense sub-goal supervision [2] or (iii) lack semantics required for deeper object-centric reasoning inherent in interpreting complex instructions [3]. In contrast, our approach can handle linguistic as well as perceptual variations, end-to-end trainable and requires no intermediate supervision. The proposed model uses symbolic reasoning constructs that operate on a latent neural object-centric representation, allowing for deeper reasoning over the input scene. Central to our approach is a modular structure consisting of a hierarchical instruction parser and an action simulator to learn disentangled act
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;2D&#25237;&#24433;&#20174;3D MRI&#20307;&#31215;&#20013;&#39640;&#25928;&#39044;&#27979;&#33041;&#40836;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;3D CNN&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36895;&#24230;&#19978;&#26377;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25552;&#21319;&#65292;&#23545;&#20110;&#27809;&#26377;3D CNN&#26114;&#36149;GPU&#30828;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.05762</link><description>&lt;p&gt;
&#20351;&#29992;2D&#25237;&#24433;&#20174;3D MRI&#20307;&#31215;&#20013;&#39640;&#25928;&#39044;&#27979;&#33041;&#40836;
&lt;/p&gt;
&lt;p&gt;
Efficient brain age prediction from 3D MRI volumes using 2D projections. (arXiv:2211.05762v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;2D&#25237;&#24433;&#20174;3D MRI&#20307;&#31215;&#20013;&#39640;&#25928;&#39044;&#27979;&#33041;&#40836;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20351;&#29992;3D CNN&#65292;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36895;&#24230;&#19978;&#26377;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#25552;&#21319;&#65292;&#23545;&#20110;&#27809;&#26377;3D CNN&#26114;&#36149;GPU&#30828;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient method for predicting brain age from 3D MRI volumes using 2D projections, which is two orders of magnitude faster than using 3D CNNs and is important for researchers without access to expensive GPU hardware.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#20998;&#36776;&#29575;&#21307;&#23398;&#20307;&#31215;&#19978;&#20351;&#29992;3D CNN&#38750;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20687;&#33521;&#22269;&#29983;&#29289;&#24211;&#36825;&#26679;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#35813;&#24211;&#26088;&#22312;&#25195;&#25551;10&#19975;&#20010;&#21463;&#35797;&#32773;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;2D CNN&#22312;3D&#20307;&#31215;&#30340;&#20960;&#20010;2D&#25237;&#24433;&#65288;&#20195;&#34920;&#36724;&#21521;&#65292;&#30690;&#29366;&#38754;&#21644;&#20896;&#29366;&#38754;&#20999;&#29255;&#30340;&#24179;&#22343;&#20540;&#21644;&#26631;&#20934;&#24046;&#65289;&#19978;&#36827;&#34892;&#39044;&#27979;&#33041;&#40836;&#26102;&#65292;&#21487;&#20197;&#33719;&#24471;&#21512;&#29702;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21333;&#20010;GPU&#36827;&#34892;&#30340;&#19968;&#27425;&#35757;&#32451;&#26102;&#65292;20324&#20010;&#21463;&#35797;&#32773;&#38656;&#35201;20-50&#31186;&#65292;&#27604;&#23567;&#22411;3D CNN&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#27809;&#26377;3D CNN&#26114;&#36149;GPU&#30828;&#20214;&#30340;&#30740;&#31350;&#20154;&#21592;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using 3D CNNs on high resolution medical volumes is very computationally demanding, especially for large datasets like the UK Biobank which aims to scan 100,000 subjects. Here we demonstrate that using 2D CNNs on a few 2D projections (representing mean and standard deviation across axial, sagittal and coronal slices) of the 3D volumes leads to reasonable test accuracy when predicting the age from brain volumes. Using our approach, one training epoch with 20,324 subjects takes 20 - 50 seconds using a single GPU, which two orders of magnitude faster compared to a small 3D CNN. These results are important for researchers who do not have access to expensive GPU hardware for 3D CNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65288;RKBS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#20598;&#24615;&#38382;&#39064;&#65292;&#26500;&#24314;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38797;&#28857;&#38382;&#39064;&#65292;&#21487;&#29992;&#20110;&#21407;&#22987;-&#23545;&#20598;&#20248;&#21270;&#30340;&#25972;&#20010;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2211.05020</link><description>&lt;p&gt;
&#36890;&#36807;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#20598;&#24615;
&lt;/p&gt;
&lt;p&gt;
Duality for Neural Networks through Reproducing Kernel Banach Spaces. (arXiv:2211.05020v3 [math.FA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65288;RKBS&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23545;&#20598;&#24615;&#38382;&#39064;&#65292;&#26500;&#24314;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#38797;&#28857;&#38382;&#39064;&#65292;&#21487;&#29992;&#20110;&#21407;&#22987;-&#23545;&#20598;&#20248;&#21270;&#30340;&#25972;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new method using Reproducing Kernel Banach spaces (RKBS) to solve the duality problem in neural networks, constructing the saddle point problem for neural networks, which can be used in the whole field of primal-dual optimization.
&lt;/p&gt;
&lt;p&gt;
&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21508;&#20010;&#39046;&#22495;&#20013;&#38750;&#24120;&#25104;&#21151;&#30340;&#24037;&#20855;&#12290;&#26368;&#36817;&#65292;Barron&#31354;&#38388;&#24050;&#34987;&#29992;&#20110;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#26435;&#37325;&#30340;&#24378;&#38750;&#32447;&#24615;&#32806;&#21512;&#65292;Barron&#31354;&#38388;&#26080;&#27861;&#29992;RKHS&#30340;&#26415;&#35821;&#29702;&#35299;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#26356;&#19968;&#33324;&#30340;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65288;RKBS&#65289;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;Barron&#31354;&#38388;&#23646;&#20110;&#19968;&#31867;&#31215;&#20998;RKBS&#12290;&#36825;&#20010;&#31867;&#20063;&#21487;&#20197;&#29702;&#35299;&#20026;RKHS&#31354;&#38388;&#30340;&#26080;&#38480;&#24182;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;RKBS&#30340;&#23545;&#20598;&#31354;&#38388;&#65292;&#20877;&#27425;&#26159;&#19968;&#20010;RKBS&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#21442;&#25968;&#30340;&#35282;&#33394;&#20114;&#25442;&#65292;&#24418;&#25104;&#19968;&#20010;&#21253;&#25324;&#20877;&#29983;&#26680;&#30340;&#20276;&#38543;RKBS&#23545;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#38797;&#28857;&#38382;&#39064;&#65292;&#21487;&#29992;&#20110;&#21407;&#22987;-&#23545;&#20598;&#20248;&#21270;&#30340;&#25972;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducing Kernel Hilbert spaces (RKHS) have been a very successful tool in various areas of machine learning. Recently, Barron spaces have been used to prove bounds on the generalisation error for neural networks. Unfortunately, Barron spaces cannot be understood in terms of RKHS due to the strong nonlinear coupling of the weights. This can be solved by using the more general Reproducing Kernel Banach spaces (RKBS). We show that these Barron spaces belong to a class of integral RKBS. This class can also be understood as an infinite union of RKHS spaces. Furthermore, we show that the dual space of such RKBSs, is again an RKBS where the roles of the data and parameters are interchanged, forming an adjoint pair of RKBSs including a reproducing kernel. This allows us to construct the saddle point problem for neural networks, which can be used in the whole field of primal-dual optimisation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;MVU&#26426;&#21046;&#65292;&#36890;&#36807;&#25968;&#20540;&#26426;&#21046;&#35774;&#35745;&#23454;&#29616;&#38754;&#21521;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#21387;&#32553;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#21644;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#36890;&#20449;&#39640;&#25928;&#30340;&#31169;&#26377;FL&#30340;SOTA&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.03942</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#21387;&#32553;&#65306;&#36890;&#36807;&#25968;&#20540;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Compression for Federated Learning Through Numerical Mechanism Design. (arXiv:2211.03942v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;MVU&#26426;&#21046;&#65292;&#36890;&#36807;&#25968;&#20540;&#26426;&#21046;&#35774;&#35745;&#23454;&#29616;&#38754;&#21521;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;&#21387;&#32553;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#21644;&#26356;&#39640;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#36890;&#20449;&#39640;&#25928;&#30340;&#31169;&#26377;FL&#30340;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new Interpolated MVU mechanism for privacy-aware compression in federated learning, which achieves a better privacy-utility trade-off and scalability through numerical mechanism design, and provides SOTA results on communication-efficient private FL on a variety of datasets.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#26381;&#21153;&#22120;&#32858;&#21512;&#26469;&#33258;&#22823;&#37327;&#23458;&#25143;&#31471;&#30340;&#24046;&#20998;&#38544;&#31169;&#26356;&#26032;&#65292;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#38544;&#31169;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#36890;&#20449;&#30340;&#20301;&#25968;&#20043;&#38388;&#24179;&#34913;&#38544;&#31169;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#38544;&#31169;&#24863;&#30693;&#21387;&#32553;&#26426;&#21046;&#65288;&#31216;&#20026;&#26368;&#23567;&#26041;&#24046;&#26080;&#20559;&#65288;MVU&#65289;&#26426;&#21046;&#65289;&#26469;&#23454;&#29616;&#33391;&#22909;&#30340;&#26435;&#34913;&#65292;&#35813;&#26426;&#21046;&#36890;&#36807;&#25968;&#20540;&#27714;&#35299;&#20248;&#21270;&#38382;&#39064;&#26469;&#30830;&#23450;&#26426;&#21046;&#30340;&#21442;&#25968;&#12290;&#26412;&#25991;&#22312;&#27492;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25554;&#20540;&#36807;&#31243;&#65292;&#29992;&#20110;&#25968;&#20540;&#35774;&#35745;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#38544;&#31169;&#20998;&#26512;&#12290;&#32467;&#26524;&#26159;&#26032;&#30340;&#25554;&#20540;MVU&#26426;&#21046;&#65292;&#23427;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#36890;&#20449;&#39640;&#25928;&#30340;&#31169;&#26377;FL&#30340;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In private federated learning (FL), a server aggregates differentially private updates from a large number of clients in order to train a machine learning model. The main challenge in this setting is balancing privacy with both classification accuracy of the learnt model as well as the number of bits communicated between the clients and server. Prior work has achieved a good trade-off by designing a privacy-aware compression mechanism, called the minimum variance unbiased (MVU) mechanism, that numerically solves an optimization problem to determine the parameters of the mechanism. This paper builds upon it by introducing a new interpolation procedure in the numerical design process that allows for a far more efficient privacy analysis. The result is the new Interpolated MVU mechanism that is more scalable, has a better privacy-utility trade-off, and provides SOTA results on communication-efficient private FL on a variety of datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#35757;&#32451;&#23567;&#35268;&#27169;CNN&#65292;&#20811;&#26381;&#20102;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#19982;&#23454;&#20540;CNN&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#20351;&#29992;&#20102;&#26174;&#30528;&#36739;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2211.02678</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#30340;&#39640;&#25928;ECG&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient ECG-based Atrial Fibrillation Detection via Parameterised Hypercomplex Neural Networks. (arXiv:2211.02678v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#24515;&#25151;&#39076;&#21160;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#35757;&#32451;&#23567;&#35268;&#27169;CNN&#65292;&#20811;&#26381;&#20102;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#19982;&#23454;&#20540;CNN&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#20351;&#29992;&#20102;&#26174;&#30528;&#36739;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a lightweight convolutional neural network method based on parameterized hypercomplex neural networks for atrial fibrillation detection. The method trains small-scale CNNs on wearable devices, overcoming limited computing resources. The approach shows comparable performance to real-valued CNNs on two publicly available ECG datasets using significantly fewer model parameters.
&lt;/p&gt;
&lt;p&gt;
&#24515;&#25151;&#39076;&#21160;&#65288;AF&#65289;&#26159;&#26368;&#24120;&#35265;&#30340;&#24515;&#24459;&#22833;&#24120;&#65292;&#19982;&#20013;&#39118;&#31561;&#20005;&#37325;&#30142;&#30149;&#30340;&#39640;&#39118;&#38505;&#30456;&#20851;&#12290;&#23884;&#20837;&#33258;&#21160;&#21644;&#21450;&#26102;&#30340;AF&#35780;&#20272;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#20351;&#29992;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#39044;&#38450;&#21361;&#21450;&#29983;&#21629;&#30340;&#24773;&#20917;&#26041;&#38754;&#20855;&#26377;&#21069;&#26223;&#12290;&#34429;&#28982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#30340;&#20351;&#29992;&#21463;&#21040;&#27169;&#22411;&#24615;&#33021;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24102;&#26377;&#21442;&#25968;&#21270;&#36229;&#22797;&#25968;&#65288;PH&#65289;&#23618;&#30340;&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#22522;&#20110;ECG&#36827;&#34892;AF&#26816;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35757;&#32451;&#23567;&#35268;&#27169;CNN&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#30340;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#20351;&#29992;&#26174;&#30528;&#36739;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#19982;&#30456;&#24212;&#30340;&#23454;&#20540;CNN&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;PH&#27169;&#22411;&#27604;&#20854;&#20182;&#36229;&#22797;&#25968;&#31070;&#32463;&#32593;&#32476;&#26356;&#28789;&#27963;&#65292;&#21487;&#20197;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Atrial fibrillation (AF) is the most common cardiac arrhythmia and associated with a high risk for serious conditions like stroke. The use of wearable devices embedded with automatic and timely AF assessment from electrocardiograms (ECGs) has shown to be promising in preventing life-threatening situations. Although deep neural networks have demonstrated superiority in model performance, their use on wearable devices is limited by the trade-off between model performance and complexity. In this work, we propose to use lightweight convolutional neural networks (CNNs) with parameterised hypercomplex (PH) layers for AF detection based on ECGs. The proposed approach trains small-scale CNNs, thus overcoming the limited computing resources on wearable devices. We show comparable performance to corresponding real-valued CNNs on two publicly available ECG datasets using significantly fewer model parameters. PH models are more flexible than other hypercomplex neural networks and can operate on an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.02641</link><description>&lt;p&gt;
&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65306;&#26469;&#33258;&#26102;&#39057;&#20998;&#26512;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;SPD&#27969;&#24418;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a graph neural network based on SPD manifolds for motor imagery classification, which utilizes second-order statistics of EEG signals and outperforms traditional methods.
&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#30340;&#20998;&#31867;&#26159;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#22522;&#30784;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#22791;&#21463;&#36861;&#25447;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#65292;MI-EEG&#20998;&#31867;&#22120;&#30340;&#36235;&#21183;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#30340;&#36716;&#21464;&#65292;&#20854;&#24615;&#33021;&#36880;&#28176;&#25552;&#39640;&#12290; Tensor-CSPNet&#30340;&#20986;&#29616;&#26159;BCI&#30740;&#31350;&#20013;&#31532;&#19968;&#20010;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#65292;&#20854;&#24402;&#22240;&#20110;&#20449;&#21495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#24615;&#36136;&#30340;&#29305;&#24449;&#21270;&#12290;&#20174;&#26681;&#26412;&#19978;&#35762;&#65292;Tensor-CSPNet&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;EEG&#30340;&#20108;&#38454;&#32479;&#35745;&#37327;&#12290;&#19982;&#21033;&#29992;EEG&#20449;&#21495;&#30340;&#19968;&#38454;&#32479;&#35745;&#37327;&#30340;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#21033;&#29992;&#36825;&#20123;&#20108;&#38454;&#32479;&#35745;&#37327;&#20195;&#34920;&#20102;&#32463;&#20856;&#30340;&#22788;&#29702;&#26041;&#27861;&#12290;&#36825;&#20123;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#21306;&#20998;&#20449;&#24687;&#65292;&#20351;&#23427;&#20204;&#36866;&#29992;&#20110;MI-EEG&#20998;&#31867;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21478;&#19968;&#31181;GDL&#20998;&#31867;&#22120;&#65292;
&lt;/p&gt;
&lt;p&gt;
The classification of motor imagery (MI) is a highly sought-after research topic in the field of Electroencephalography (EEG)-based brain-computer interfaces (BCIs), with immense commercial value. Over the past two decades, there has been a fundamental shift in the trend of MI-EEG classifiers, resulting in a gradual increase in their performance. The emergence of Tensor-CSPNet, the first geometric deep learning (GDL) framework in BCI research, is attributed to the imperative of characterizing the non-Euclidean nature of signals. Fundamentally, Tensor-CSPNet is a deep learning-based classifier that capitalizes on the second-order statistics of EEGs. In contrast to the conventional approach of utilizing first-order statistics for EEG signals, the utilization of these second-order statistics represents the classical treatment. These statistics provide adequate discriminative information, rendering them suitable for MI-EEG classification. In this study, we introduce another GDL classifier,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#21644;&#22122;&#22768;&#40065;&#26834;&#30340;&#21548;&#35273;&#26816;&#26597;&#31995;&#32479;&#65292;&#29992;&#20110;&#26816;&#27979;&#26426;&#26800;&#37096;&#20214;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#21253;&#32476;&#29305;&#24449;&#19982;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#24212;&#29992;&#26102;&#21464;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#20154;&#12290;</title><link>http://arxiv.org/abs/2211.01704</link><description>&lt;p&gt;
&#21435;&#38500;&#22122;&#38899;&#65306;&#24515;&#29702;&#22768;&#23398;&#21644;&#22522;&#20110;&#21253;&#32476;&#30340;&#29305;&#24449;&#22312;&#26426;&#26800;&#25925;&#38556;&#26816;&#27979;&#20013;&#30340;&#23454;&#35777;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Cutting Through the Noise: An Empirical Comparison of Psychoacoustic and Envelope-based Features for Machinery Fault Detection. (arXiv:2211.01704v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#21644;&#22122;&#22768;&#40065;&#26834;&#30340;&#21548;&#35273;&#26816;&#26597;&#31995;&#32479;&#65292;&#29992;&#20110;&#26816;&#27979;&#26426;&#26800;&#37096;&#20214;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#21253;&#32476;&#29305;&#24449;&#19982;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#24212;&#29992;&#26102;&#21464;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an automated and noise-robust auditory inspection system for detecting the health condition of mechanical parts. A benchmark is provided to compare different types of envelope features with psychoacoustic features. The authors are the first to apply time-varying psychoacoustic features for fault detection.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22768;&#23398;&#30340;&#25925;&#38556;&#26816;&#27979;&#20855;&#26377;&#30417;&#27979;&#26426;&#26800;&#37096;&#20214;&#20581;&#24247;&#29366;&#20917;&#30340;&#39640;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24037;&#19994;&#29615;&#22659;&#30340;&#32972;&#26223;&#22122;&#38899;&#21487;&#33021;&#20250;&#23545;&#25925;&#38556;&#26816;&#27979;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#30446;&#21069;&#23545;&#20110;&#25552;&#39640;&#25925;&#38556;&#26816;&#27979;&#23545;&#24037;&#19994;&#29615;&#22659;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#30340;&#20851;&#27880;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Lenze&#29983;&#20135;&#32972;&#26223;&#22122;&#22768;&#65288;LPBN&#65289;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#29992;&#20110;&#40831;&#36718;&#30005;&#26426;&#26411;&#31471;&#26816;&#26597;&#30340;&#33258;&#21160;&#21270;&#21644;&#22122;&#22768;&#40065;&#26834;&#30340;&#21548;&#35273;&#26816;&#26597;&#65288;ARAI&#65289;&#31995;&#32479;&#12290;&#37319;&#29992;&#22768;&#23398;&#38453;&#21015;&#20174;&#20855;&#26377;&#36731;&#24494;&#25925;&#38556;&#12289;&#37325;&#22823;&#25925;&#38556;&#25110;&#20581;&#24247;&#30340;&#30005;&#26426;&#20013;&#33719;&#21462;&#25968;&#25454;&#12290;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#22522;&#20110;&#19987;&#23478;&#23545;&#40831;&#36718;&#31665;&#30340;&#30693;&#35782;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#21253;&#32476;&#29305;&#24449;&#19982;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#24212;&#29992;&#26102;&#21464;&#24515;&#29702;&#22768;&#23398;&#29305;&#24449;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#20154;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#21333;&#31867;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#26469;&#33258;&#20581;&#24247;&#30005;&#26426;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acoustic-based fault detection has a high potential to monitor the health condition of mechanical parts. However, the background noise of an industrial environment may negatively influence the performance of fault detection. Limited attention has been paid to improving the robustness of fault detection against industrial environmental noise. Therefore, we present the Lenze production background-noise (LPBN) real-world dataset and an automated and noise-robust auditory inspection (ARAI) system for the end-of-line inspection of geared motors. An acoustic array is used to acquire data from motors with a minor fault, major fault, or which are healthy. A benchmark is provided to compare the psychoacoustic features with different types of envelope features based on expert knowledge of the gearbox. To the best of our knowledge, we are the first to apply time-varying psychoacoustic features for fault detection. We train a state-of-the-art one-class-classifier, on samples from healthy motors an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;Demux&#26469;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#21516;&#24773;&#24863;&#12289;&#35821;&#35328;&#21644;&#27880;&#37322;&#26684;&#24335;&#20043;&#38388;&#36716;&#25442;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#30693;&#35782;&#20849;&#20139;&#21644;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2211.00171</link><description>&lt;p&gt;
&#20351;&#29992;&#24773;&#24863;&#23884;&#20837;&#22312;&#24773;&#24863;&#12289;&#35821;&#35328;&#21644;&#27880;&#37322;&#26684;&#24335;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Using Emotion Embeddings to Transfer Knowledge Between Emotions, Languages, and Annotation Formats. (arXiv:2211.00171v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;Demux&#26469;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#22312;&#19981;&#21516;&#24773;&#24863;&#12289;&#35821;&#35328;&#21644;&#27880;&#37322;&#26684;&#24335;&#20043;&#38388;&#36716;&#25442;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#30693;&#35782;&#20849;&#20139;&#21644;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies how to build a single model that can transition between different emotions, languages, and annotation formats by leveraging multilingual models and Demux, to achieve knowledge sharing and reduce training costs.
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#23398;&#31185;&#23558;&#24773;&#24863;&#34701;&#20837;&#20854;&#29702;&#35770;&#21644;&#24212;&#29992;&#20013;&#65292;&#20174;&#25991;&#26412;&#20013;&#25512;&#26029;&#24773;&#24863;&#30340;&#38656;&#27714;&#19981;&#26029;&#22810;&#26679;&#21270;&#12290;&#36825;&#20123;&#38656;&#27714;&#21253;&#25324;&#25512;&#26029;&#19981;&#21516;&#31867;&#22411;&#30340;&#24773;&#24863;&#12289;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#21644;&#19981;&#21516;&#30340;&#27880;&#37322;&#26684;&#24335;&#12290;&#19981;&#21516;&#37197;&#32622;&#20043;&#38388;&#30340;&#20849;&#20139;&#27169;&#22411;&#23558;&#20351;&#30693;&#35782;&#20849;&#20139;&#21644;&#35757;&#32451;&#25104;&#26412;&#38477;&#20302;&#65292;&#24182;&#31616;&#21270;&#22312;&#26032;&#29615;&#22659;&#20013;&#37096;&#32626;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30340;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;Demux&#26469;&#26500;&#24314;&#19968;&#20010;&#21487;&#20197;&#22312;&#36825;&#20123;&#19981;&#21516;&#37197;&#32622;&#20043;&#38388;&#36716;&#25442;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;Demux&#26159;&#19968;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#65292;&#20854;&#36755;&#20837;&#21253;&#25324;&#24863;&#20852;&#36259;&#30340;&#24773;&#24863;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21160;&#24577;&#22320;&#25913;&#21464;&#27169;&#22411;&#39044;&#27979;&#30340;&#24773;&#24863;&#12290;Demux&#36824;&#20135;&#29983;&#24773;&#24863;&#23884;&#20837;&#65292;&#23545;&#23427;&#20204;&#25191;&#34892;&#25805;&#20316;&#21487;&#20197;&#36890;&#36807;&#27719;&#38598;&#27599;&#20010;&#31751;&#30340;&#23884;&#20837;&#26469;&#36807;&#28193;&#21040;&#24773;&#24863;&#31751;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Demux&#21487;&#20197;&#21516;&#26102;&#20256;&#36755;k
&lt;/p&gt;
&lt;p&gt;
The need for emotional inference from text continues to diversify as more and more disciplines integrate emotions into their theories and applications. These needs include inferring different emotion types, handling multiple languages, and different annotation formats. A shared model between different configurations would enable the sharing of knowledge and a decrease in training costs, and would simplify the process of deploying emotion recognition models in novel environments. In this work, we study how we can build a single model that can transition between these different configurations by leveraging multilingual models and Demux, a transformer-based model whose input includes the emotions of interest, enabling us to dynamically change the emotions predicted by the model. Demux also produces emotion embeddings, and performing operations on them allows us to transition to clusters of emotions by pooling the embeddings of each cluster. We show that Demux can simultaneously transfer k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#27668;&#35937;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#28085;&#30422;&#20102;&#24863;&#30693;&#22120;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;U&#22411;&#32593;&#32476;&#31561;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.00147</link><description>&lt;p&gt;
&#25805;&#20316;&#27668;&#35937;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#25945;&#31243;&#65292;&#31532;&#20108;&#37096;&#20998;&#65306;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Tutorial for Operational Meteorology, Part II: Neural Networks and Deep Learning. (arXiv:2211.00147v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#27668;&#35937;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#28085;&#30422;&#20102;&#24863;&#30693;&#22120;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;U&#22411;&#32593;&#32476;&#31561;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the application of machine learning in meteorology, specifically neural networks and deep learning. It covers methods such as perceptrons, artificial neural networks, convolutional neural networks, and U-networks.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#27668;&#35937;&#23398;&#20013;&#30340;&#24212;&#29992;&#36805;&#36895;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20351;&#29992;&#29575;&#21069;&#25152;&#26410;&#26377;&#12290;&#20026;&#20102;&#22635;&#34917;&#32570;&#20047;&#20197;&#27668;&#35937;&#23398;&#35270;&#35282;&#28085;&#30422;&#31070;&#32463;&#32593;&#32476;&#30340;&#36164;&#28304;&#65292;&#26412;&#25991;&#20197;&#24179;&#26131;&#36817;&#20154;&#30340;&#35821;&#35328;&#26684;&#24335;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#25805;&#20316;&#27668;&#35937;&#23398;&#30028;&#12290;&#36825;&#26159;&#19968;&#23545;&#26088;&#22312;&#20026;&#27668;&#35937;&#23398;&#23478;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#36164;&#28304;&#30340;&#35770;&#25991;&#20013;&#30340;&#31532;&#20108;&#31687;&#12290;&#31532;&#19968;&#31687;&#35770;&#25991;&#20391;&#37325;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914;&#38543;&#26426;&#26862;&#26519;&#65289;&#65292;&#32780;&#26412;&#25991;&#21017;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#28085;&#30422;&#20102;&#24863;&#30693;&#22120;&#12289;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;U&#22411;&#32593;&#32476;&#12290;&#19982;&#31532;&#19968;&#31687;&#35770;&#25991;&#19968;&#26679;&#65292;&#26412;&#25991;&#35752;&#35770;&#20102;&#19982;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#35757;&#32451;&#30456;&#20851;&#30340;&#26415;&#35821;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#27599;&#31181;&#26041;&#27861;&#32972;&#21518;&#30340;&#19968;&#20123;&#30452;&#35273;&#65292;&#24182;&#20197;&#23637;&#31034;&#27599;&#31181;&#26041;&#27861;&#30340;&#23454;&#20363;&#26469;&#32467;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade the use of machine learning in meteorology has grown rapidly. Specifically neural networks and deep learning have been used at an unprecedented rate. In order to fill the dearth of resources covering neural networks with a meteorological lens, this paper discusses machine learning methods in a plain language format that is targeted for the operational meteorological community. This is the second paper in a pair that aim to serve as a machine learning resource for meteorologists. While the first paper focused on traditional machine learning methods (e.g., random forest), here a broad spectrum of neural networks and deep learning methods are discussed. Specifically this paper covers perceptrons, artificial neural networks, convolutional neural networks and U-networks. Like the part 1 paper, this manuscript discusses the terms associated with neural networks and their training. Then the manuscript provides some intuition behind every method and concludes by showing ea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34920;&#26684;&#25968;&#25454;&#26465;&#20214;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#8221;&#65288;TabCSDI&#65289;&#30340;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#25554;&#34917;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#22788;&#29702;&#20998;&#31867;&#21464;&#37327;&#21644;&#25968;&#20540;&#21464;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.17128</link><description>&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#25554;&#34917;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion models for missing value imputation in tabular data. (arXiv:2210.17128v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34920;&#26684;&#25968;&#25454;&#26465;&#20214;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#8221;&#65288;TabCSDI&#65289;&#30340;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#25554;&#34917;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#22788;&#29702;&#20998;&#31867;&#21464;&#37327;&#21644;&#25968;&#20540;&#21464;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a diffusion model approach called "Conditional Score-based Diffusion Models for Tabular data" (TabCSDI) for missing value imputation in tabular data, which effectively handles categorical variables and numerical variables simultaneously, and achieves excellent performance on various datasets.
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#32570;&#22833;&#20540;&#25554;&#34917;&#26159;&#20351;&#29992;&#21487;&#29992;&#20449;&#24687;&#20934;&#30830;&#20272;&#35745;&#25968;&#25454;&#38598;&#20013;&#32570;&#22833;&#20540;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#29992;&#24615;&#65292;&#20363;&#22914;&#29983;&#25104;&#23545;&#25239;&#25554;&#34917;&#32593;&#32476;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#31561;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23545;&#20110;&#34920;&#26684;&#25968;&#25454;&#20013;&#32570;&#22833;&#20540;&#25554;&#34917;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#22522;&#20110;&#26368;&#36817;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25554;&#34917;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34920;&#26684;&#25968;&#25454;&#26465;&#20214;&#20998;&#25968;&#25193;&#25955;&#27169;&#22411;&#8221;&#65288;TabCSDI&#65289;&#30340;&#25193;&#25955;&#27169;&#22411;&#26041;&#27861;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#21516;&#26102;&#22788;&#29702;&#20998;&#31867;&#21464;&#37327;&#21644;&#25968;&#20540;&#21464;&#37327;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#25216;&#26415;&#65306;&#29420;&#28909;&#32534;&#30721;&#12289;&#27169;&#25311;&#20301;&#32534;&#30721;&#21644;&#29305;&#24449;&#26631;&#35760;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing value imputation in machine learning is the task of estimating the missing values in the dataset accurately using available information. In this task, several deep generative modeling methods have been proposed and demonstrated their usefulness, e.g., generative adversarial imputation networks. Recently, diffusion models have gained popularity because of their effectiveness in the generative modeling task in images, texts, audio, etc. To our knowledge, less attention has been paid to the investigation of the effectiveness of diffusion models for missing value imputation in tabular data. Based on recent development of diffusion models for time-series data imputation, we propose a diffusion model approach called "Conditional Score-based Diffusion Models for Tabular data" (TabCSDI). To effectively handle categorical variables and numerical variables simultaneously, we investigate three techniques: one-hot encoding, analog bits encoding, and feature tokenization. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#21487;&#29992;&#20803;&#25968;&#25454;&#35299;&#20915;&#22810;&#20010;&#21069;&#32622;&#20219;&#21153;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#20165;&#20351;&#29992;&#20803;&#25968;&#25454;&#23398;&#20064;&#34920;&#31034;&#21487;&#20197;&#33719;&#24471;&#19982;&#20165;&#20351;&#29992;&#31867;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23558;&#31867;&#26631;&#31614;&#19982;&#20803;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2210.16192</link><description>&lt;p&gt;
&#21033;&#29992;&#20803;&#25968;&#25454;&#21644;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#38899;&#39057;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Learning Audio Features with Metadata and Contrastive Learning. (arXiv:2210.16192v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#21487;&#29992;&#20803;&#25968;&#25454;&#35299;&#20915;&#22810;&#20010;&#21069;&#32622;&#20219;&#21153;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65292;&#20165;&#20351;&#29992;&#20803;&#25968;&#25454;&#23398;&#20064;&#34920;&#31034;&#21487;&#20197;&#33719;&#24471;&#19982;&#20165;&#20351;&#29992;&#31867;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23558;&#31867;&#26631;&#31614;&#19982;&#20803;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study uses supervised contrastive learning combined with available metadata to solve multiple pretext tasks that learn a good representation of data. Learning representations using only metadata obtains similar performance as using cross entropy with class labels only. State-of-the-art score is obtained when combining class labels with metadata using multiple supervised contrastive learning.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#37322;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19968;&#30452;&#26159;&#20998;&#31867;&#38382;&#39064;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#32467;&#21512;&#21487;&#29992;&#20803;&#25968;&#25454;&#35299;&#20915;&#22810;&#20010;&#21069;&#32622;&#20219;&#21153;&#65292;&#23398;&#20064;&#25968;&#25454;&#30340;&#33391;&#22909;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;ICBHI&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#21512;&#36825;&#31181;&#24773;&#20917;&#30340;&#21628;&#21560;&#38899;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#20803;&#25968;&#25454;&#23398;&#20064;&#34920;&#31034;&#65292;&#32780;&#19981;&#20351;&#29992;&#31867;&#26631;&#31614;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20165;&#20351;&#29992;&#36825;&#20123;&#26631;&#31614;&#30340;&#20132;&#21449;&#29109;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23558;&#31867;&#26631;&#31614;&#19982;&#20803;&#25968;&#25454;&#30456;&#32467;&#21512;&#26102;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#30417;&#30563;&#23545;&#27604;&#35774;&#32622;&#20013;&#20351;&#29992;&#22810;&#20010;&#20803;&#25968;&#25454;&#28304;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#19981;&#24179;&#34913;&#21644;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods based on supervised learning using annotations in an end-to-end fashion have been the state-of-the-art for classification problems. However, they may be limited in their generalization capability, especially in the low data regime. In this study, we address this issue using supervised contrastive learning combined with available metadata to solve multiple pretext tasks that learn a good representation of data. We apply our approach on ICBHI, a respiratory sound classification dataset suited for this setting. We show that learning representations using only metadata, without class labels, obtains similar performance as using cross entropy with those labels only. In addition, we obtain state-of-the-art score when combining class labels with metadata using multiple supervised contrastive learning. This work suggests the potential of using multiple metadata sources in supervised contrastive settings, in particular in settings with class imbalance and few data. Our code is released 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#26469;&#25429;&#25417;&#24773;&#24863;&#35789;&#26412;&#36523;&#30340;&#35789;&#27719;&#20851;&#32852;&#24615;&#65292;&#24182;&#23558;&#24773;&#24863;&#34920;&#31034;&#30340;&#25104;&#23545;&#32422;&#26463;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#19982;&#27169;&#22411;&#30340;&#20998;&#31867;&#25439;&#22833;&#19968;&#36215;&#38598;&#25104;&#65292;&#23637;&#31034;&#20102;&#22312;SemEval 2018&#20219;&#21153;1 E-c&#20013;&#20351;&#29992;&#21333;&#35821;BERT&#27169;&#22411;&#23637;&#31034;&#20102;&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.15842</link><description>&lt;p&gt;
&#22312;&#22810;&#26631;&#31614;&#24773;&#24863;&#35782;&#21035;&#20013;&#21033;&#29992;&#26631;&#31614;&#30456;&#20851;&#24615;&#30340;&#30740;&#31350;&#65306;&#20197;&#24773;&#24863;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Leveraging Label Correlations in a Multi-label Setting: A Case Study in Emotion. (arXiv:2210.15842v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#26469;&#25429;&#25417;&#24773;&#24863;&#35789;&#26412;&#36523;&#30340;&#35789;&#27719;&#20851;&#32852;&#24615;&#65292;&#24182;&#23558;&#24773;&#24863;&#34920;&#31034;&#30340;&#25104;&#23545;&#32422;&#26463;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#19982;&#27169;&#22411;&#30340;&#20998;&#31867;&#25439;&#22833;&#19968;&#36215;&#38598;&#25104;&#65292;&#23637;&#31034;&#20102;&#22312;SemEval 2018&#20219;&#21153;1 E-c&#20013;&#20351;&#29992;&#21333;&#35821;BERT&#27169;&#22411;&#23637;&#31034;&#20102;&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates ways to exploit label correlations in multi-label emotion recognition models to improve emotion detection, develops two modeling approaches to capture word associations of the emotion words themselves, and integrates pairwise constraints of emotion representations as regularization terms alongside the classification loss of the models, demonstrating state-of-the-art performance across Spanish, English, and Arabic in SemEval 2018 Task 1 E-c using monolingual BERT-based models.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#34920;&#36798;&#30340;&#24773;&#24863;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#39046;&#22495;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22810;&#26631;&#31614;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#26631;&#31614;&#30456;&#20851;&#24615;&#26469;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#26469;&#25429;&#25417;&#24773;&#24863;&#35789;&#26412;&#36523;&#30340;&#35789;&#27719;&#20851;&#32852;&#24615;&#65292;&#19968;&#31181;&#26159;&#23558;&#24773;&#24863;&#21253;&#21547;&#22312;&#36755;&#20837;&#20013;&#65292;&#21478;&#19968;&#31181;&#26159;&#21033;&#29992;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#24773;&#24863;&#34920;&#31034;&#30340;&#25104;&#23545;&#32422;&#26463;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#19982;&#27169;&#22411;&#30340;&#20998;&#31867;&#25439;&#22833;&#19968;&#36215;&#38598;&#25104;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#39033;&#20998;&#20026;&#20004;&#31867;&#65292;&#23616;&#37096;&#21644;&#20840;&#23616;&#12290;&#21069;&#32773;&#26681;&#25454;&#37329;&#26631;&#31614;&#21160;&#24577;&#21464;&#21270;&#65292;&#32780;&#21518;&#32773;&#22312;&#35757;&#32451;&#26399;&#38388;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#22312;SemEval 2018&#20219;&#21153;1 E-c&#20013;&#20351;&#29992;&#21333;&#35821;BERT&#27169;&#22411;&#23637;&#31034;&#20102;&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#38500;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting emotions expressed in text has become critical to a range of fields. In this work, we investigate ways to exploit label correlations in multi-label emotion recognition models to improve emotion detection. First, we develop two modeling approaches to the problem in order to capture word associations of the emotion words themselves, by either including the emotions in the input, or by leveraging Masked Language Modeling (MLM). Second, we integrate pairwise constraints of emotion representations as regularization terms alongside the classification loss of the models. We split these terms into two categories, local and global. The former dynamically change based on the gold labels, while the latter remain static during training. We demonstrate state-of-the-art performance across Spanish, English, and Arabic in SemEval 2018 Task 1 E-c using monolingual BERT-based models. On top of better performance, we also demonstrate improved robustness. Code is available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>D-Shape&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;IL&#21644;RL&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#30446;&#26631;&#26465;&#20214;&#21270;RL&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#27425;&#20248;&#28436;&#31034;&#19982;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#33021;&#22815;&#22312;&#31232;&#30095;&#22870;&#21169;&#32593;&#26684;&#19990;&#30028;&#39046;&#22495;&#20013;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.14428</link><description>&lt;p&gt;
D-Shape: &#36890;&#36807;&#30446;&#26631;&#26465;&#20214;&#21270;&#23454;&#29616;&#28436;&#31034;&#24418;&#29366;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
D-Shape: Demonstration-Shaped Reinforcement Learning via Goal Conditioning. (arXiv:2210.14428v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14428
&lt;/p&gt;
&lt;p&gt;
D-Shape&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;IL&#21644;RL&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#30446;&#26631;&#26465;&#20214;&#21270;RL&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#27425;&#20248;&#28436;&#31034;&#19982;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#65292;&#33021;&#22815;&#22312;&#31232;&#30095;&#22870;&#21169;&#32593;&#26684;&#19990;&#30028;&#39046;&#22495;&#20013;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
D-Shape is a new method that combines imitation learning (IL) and reinforcement learning (RL) using reward shaping and goal-conditioned RL to resolve the conflict between suboptimal demonstrations and return-maximization objective of RL. It improves sample efficiency and consistently converges to the optimal policy in sparse-reward gridworld domains.
&lt;/p&gt;
&lt;p&gt;
&#23558;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#26159;&#35299;&#20915;&#33258;&#20027;&#34892;&#20026;&#33719;&#21462;&#20013;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#26679;&#20570;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#25152;&#38656;&#30340;&#34892;&#20026;&#28436;&#31034;&#30001;&#19987;&#23478;&#25552;&#20379;&#65292;&#35813;&#19987;&#23478;&#30456;&#23545;&#20110;&#20219;&#21153;&#22870;&#21169;&#34920;&#29616;&#26368;&#20339;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#28436;&#31034;&#26159;&#27425;&#20248;&#30340;&#65292;&#21017;&#38754;&#20020;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#65292;&#21363;IL&#30340;&#28436;&#31034;&#21305;&#37197;&#30446;&#26631;&#19982;RL&#30340;&#22238;&#25253;&#26368;&#22823;&#21270;&#30446;&#26631;&#20914;&#31361;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;D-Shape&#65292;&#19968;&#31181;&#26032;&#30340;&#32467;&#21512;IL&#21644;RL&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22870;&#21169;&#22609;&#24418;&#21644;&#30446;&#26631;&#26465;&#20214;&#21270;RL&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#19978;&#36848;&#20914;&#31361;&#12290;D-Shape&#20801;&#35768;&#20174;&#27425;&#20248;&#28436;&#31034;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#25214;&#21040;&#30456;&#23545;&#20110;&#20219;&#21153;&#22870;&#21169;&#30340;&#26368;&#20248;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#31232;&#30095;&#22870;&#21169;&#32593;&#26684;&#19990;&#30028;&#39046;&#22495;&#23454;&#39564;&#39564;&#35777;&#20102;D-Shape&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;RL&#65292;&#24182;&#19988;&#33021;&#22815;&#19968;&#33268;&#22320;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
While combining imitation learning (IL) and reinforcement learning (RL) is a promising way to address poor sample efficiency in autonomous behavior acquisition, methods that do so typically assume that the requisite behavior demonstrations are provided by an expert that behaves optimally with respect to a task reward. If, however, suboptimal demonstrations are provided, a fundamental challenge appears in that the demonstration-matching objective of IL conflicts with the return-maximization objective of RL. This paper introduces D-Shape, a new method for combining IL and RL that uses ideas from reward shaping and goal-conditioned RL to resolve the above conflict. D-Shape allows learning from suboptimal demonstrations while retaining the ability to find the optimal policy with respect to the task reward. We experimentally validate D-Shape in sparse-reward gridworld domains, showing that it both improves over RL in terms of sample efficiency and converges consistently to the optimal polic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#21147;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20960;&#20010;&#24037;&#20855;&#26469;&#35299;&#20915;&#38750;&#20984;&#37319;&#26679;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#23545;&#20110;&#19968;&#22823;&#31867;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23427;&#20204;&#22312;Wasserstein&#36317;&#31163;&#19979;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21487;&#20197;&#24402;&#32467;&#20026;&#23545;&#23427;&#20204;&#30340;&#36830;&#32493;&#26102;&#38388;&#23545;&#24212;&#29289;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#26356;&#22909;&#29702;&#35299;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.13867</link><description>&lt;p&gt;
Langevin-Based Non-Convex Sampling&#30340;&#21160;&#21147;&#23398;&#31995;&#32479;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Dynamical System View of Langevin-Based Non-Convex Sampling. (arXiv:2210.13867v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#21147;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20960;&#20010;&#24037;&#20855;&#26469;&#35299;&#20915;&#38750;&#20984;&#37319;&#26679;&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#23545;&#20110;&#19968;&#22823;&#31867;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23427;&#20204;&#22312;Wasserstein&#36317;&#31163;&#19979;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21487;&#20197;&#24402;&#32467;&#20026;&#23545;&#23427;&#20204;&#30340;&#36830;&#32493;&#26102;&#38388;&#23545;&#24212;&#29289;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#26356;&#22909;&#29702;&#35299;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new framework that uses tools from the theory of dynamical systems to address important challenges in non-convex sampling. For a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood.
&lt;/p&gt;
&lt;p&gt;
&#38750;&#20984;&#37319;&#26679;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#38750;&#20984;&#20248;&#21270;&#20197;&#21450;&#36817;&#20284;&#27010;&#29575;&#25512;&#26029;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#29702;&#35770;&#19978;&#20173;&#23384;&#22312;&#35768;&#22810;&#37325;&#35201;&#25361;&#25112;&#65306;&#29616;&#26377;&#30340;&#20445;&#35777;&#36890;&#24120;&#20165;&#36866;&#29992;&#20110;&#24179;&#22343;&#36845;&#20195;&#32780;&#19981;&#26159;&#26356;&#29702;&#24819;&#30340;&#26368;&#21518;&#36845;&#20195;&#65292;&#32570;&#20047;&#25429;&#25417;&#21464;&#37327;&#23610;&#24230;&#65288;&#22914;Wasserstein&#36317;&#31163;&#65289;&#30340;&#25910;&#25947;&#24230;&#37327;&#65292;&#20027;&#35201;&#36866;&#29992;&#20110;&#38543;&#26426;&#26799;&#24230;Langevin&#21160;&#21147;&#23398;&#31561;&#22522;&#26412;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#21147;&#31995;&#32479;&#29702;&#35770;&#20013;&#30340;&#20960;&#20010;&#24037;&#20855;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#32467;&#26524;&#26159;&#65292;&#23545;&#20110;&#19968;&#22823;&#31867;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#23427;&#20204;&#22312;Wasserstein&#36317;&#31163;&#19979;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#21487;&#20197;&#24402;&#32467;&#20026;&#23545;&#23427;&#20204;&#30340;&#36830;&#32493;&#26102;&#38388;&#23545;&#24212;&#29289;&#30340;&#30740;&#31350;&#65292;&#36825;&#26159;&#26356;&#22909;&#29702;&#35299;&#30340;&#12290;&#32467;&#21512;MCMC&#37319;&#26679;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#31435;&#21363;&#20135;&#29983;&#20102;
&lt;/p&gt;
&lt;p&gt;
Non-convex sampling is a key challenge in machine learning, central to non-convex optimization in deep learning as well as to approximate probabilistic inference. Despite its significance, theoretically there remain many important challenges: Existing guarantees (1) typically only hold for the averaged iterates rather than the more desirable last iterates, (2) lack convergence metrics that capture the scales of the variables such as Wasserstein distances, and (3) mainly apply to elementary schemes such as stochastic gradient Langevin dynamics. In this paper, we develop a new framework that lifts the above issues by harnessing several tools from the theory of dynamical systems. Our key result is that, for a large class of state-of-the-art sampling schemes, their last-iterate convergence in Wasserstein distances can be reduced to the study of their continuous-time counterparts, which is much better understood. Coupled with standard assumptions of MCMC sampling, our theory immediately yie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DAARE&#30340;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#28040;&#38500;&#26497;&#20809;&#21315;&#31859;&#36752;&#23556;&#20013;&#30340;&#23556;&#39057;&#24178;&#25200;&#12290;DAARE&#22312;&#21512;&#25104;AKR&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;42.2&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;(PSNR)&#21644;0.981&#30340;&#32467;&#26500;&#30456;&#20284;&#24230;(SSIM)&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#28388;&#27874;&#21644;&#21435;&#22122;&#32593;&#32476;&#65292;PSNR&#25552;&#39640;&#20102;3.9&#65292;SSIM&#25552;&#39640;&#20102;0.064&#12290;&#23450;&#24615;&#27604;&#36739;&#34920;&#26126;&#65292;DAARE&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;RFI&#12290;</title><link>http://arxiv.org/abs/2210.12931</link><description>&lt;p&gt;
&#21033;&#29992;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#28040;&#38500;&#26497;&#20809;&#21315;&#31859;&#36752;&#23556;&#20013;&#30340;&#23556;&#39057;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Removing Radio Frequency Interference from Auroral Kilometric Radiation with Stacked Autoencoders. (arXiv:2210.12931v3 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DAARE&#30340;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#28040;&#38500;&#26497;&#20809;&#21315;&#31859;&#36752;&#23556;&#20013;&#30340;&#23556;&#39057;&#24178;&#25200;&#12290;DAARE&#22312;&#21512;&#25104;AKR&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;42.2&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;(PSNR)&#21644;0.981&#30340;&#32467;&#26500;&#30456;&#20284;&#24230;(SSIM)&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#28388;&#27874;&#21644;&#21435;&#22122;&#32593;&#32476;&#65292;PSNR&#25552;&#39640;&#20102;3.9&#65292;SSIM&#25552;&#39640;&#20102;0.064&#12290;&#23450;&#24615;&#27604;&#36739;&#34920;&#26126;&#65292;DAARE&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;RFI&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a denoising autoencoder named DAARE to remove radio frequency interference (RFI) from auroral kilometric radiation (AKR) signals collected at the South Pole Station. DAARE achieves 42.2 peak signal-to-noise ratio (PSNR) and 0.981 structural similarity (SSIM) on synthesized AKR observations, improving PSNR by 3.9 and SSIM by 0.064 compared to state-of-the-art filtering and denoising networks.
&lt;/p&gt;
&lt;p&gt;
&#22825;&#25991;&#23556;&#30005;&#39057;&#29575;&#25968;&#25454;&#21487;&#20197;&#24110;&#21161;&#31185;&#23398;&#23478;&#20998;&#26512;&#22825;&#20307;&#29289;&#29702;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#20250;&#21463;&#21040;&#23556;&#39057;&#24178;&#25200;(RFI)&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#22522;&#30784;&#33258;&#28982;&#36807;&#31243;&#30340;&#35266;&#27979;&#12290;&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#25193;&#23637;&#21040;&#22825;&#25991;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#20174;&#21335;&#26497;&#31449;&#25910;&#38598;&#30340;&#26497;&#20809;&#21315;&#31859;&#36752;&#23556;(AKR)&#20449;&#21495;&#20013;&#65292;&#21033;&#29992;&#21512;&#25104;&#20809;&#35889;&#22270;&#35757;&#32451;&#20102;&#19968;&#31181;&#21517;&#20026;DAARE&#30340;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65292;&#20197;&#28040;&#38500;RFI&#12290;DAARE&#22312;&#21512;&#25104;AKR&#35266;&#27979;&#20013;&#23454;&#29616;&#20102;42.2&#30340;&#23792;&#20540;&#20449;&#22122;&#27604;(PSNR)&#21644;0.981&#30340;&#32467;&#26500;&#30456;&#20284;&#24230;(SSIM)&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#28388;&#27874;&#21644;&#21435;&#22122;&#32593;&#32476;&#65292;PSNR&#25552;&#39640;&#20102;3.9&#65292;SSIM&#25552;&#39640;&#20102;0.064&#12290;&#23450;&#24615;&#27604;&#36739;&#34920;&#26126;&#65292;DAARE&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;RFI&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio frequency data in astronomy enable scientists to analyze astrophysical phenomena. However, these data can be corrupted by radio frequency interference (RFI) that limits the observation of underlying natural processes. In this study, we extend recent developments in deep learning algorithms to astronomy data. We remove RFI from time-frequency spectrograms containing auroral kilometric radiation (AKR), a coherent radio emission originating from the Earth's auroral zones that is used to study astrophysical plasmas. We propose a Denoising Autoencoder for Auroral Radio Emissions (DAARE) trained with synthetic spectrograms to denoise AKR signals collected at the South Pole Station. DAARE achieves 42.2 peak signal-to-noise ratio (PSNR) and 0.981 structural similarity (SSIM) on synthesized AKR observations, improving PSNR by 3.9 and SSIM by 0.064 compared to state-of-the-art filtering and denoising networks. Qualitative comparisons demonstrate DAARE's capability to effectively remove RFI
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#37325;&#22797;&#36328;&#36234;&#38899;&#39057;&#24207;&#21015;&#30340;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#22768;&#38899;&#26469;&#36827;&#34892;&#20851;&#27880;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#38899;&#39057;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.11328</link><description>&lt;p&gt;
&#22238;&#25918;&#65306;&#36845;&#20195;&#27880;&#24847;&#21147;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Play It Back: Iterative Attention for Audio Recognition. (arXiv:2210.11328v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#37325;&#22797;&#36328;&#36234;&#38899;&#39057;&#24207;&#21015;&#30340;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#22768;&#38899;&#26469;&#36827;&#34892;&#20851;&#27880;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#22312;&#19977;&#20010;&#38899;&#39057;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes an end-to-end attention-based architecture that attends over the most discriminative sounds across the audio sequence through selective repetition, achieving consistently state-of-the-art performance across three audio-classification benchmarks.
&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#35748;&#30693;&#30340;&#19968;&#20010;&#20851;&#38190;&#21151;&#33021;&#26159;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#23558;&#29305;&#24449;&#22768;&#38899;&#19982;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#32852;&#36215;&#26469;&#12290;&#20154;&#31867;&#35797;&#22270;&#21306;&#20998;&#32454;&#31890;&#24230;&#38899;&#39057;&#31867;&#21035;&#26102;&#65292;&#36890;&#24120;&#20250;&#37325;&#25773;&#30456;&#21516;&#30340;&#21306;&#20998;&#24615;&#22768;&#38899;&#20197;&#22686;&#21152;&#20854;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#37325;&#22797;&#36328;&#36234;&#38899;&#39057;&#24207;&#21015;&#30340;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#22768;&#38899;&#26469;&#36827;&#34892;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26368;&#21021;&#20351;&#29992;&#23436;&#25972;&#30340;&#38899;&#39057;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#25554;&#27133;&#27880;&#24847;&#21147;&#36845;&#20195;&#22320;&#32454;&#21270;&#37325;&#25773;&#30340;&#26102;&#38388;&#27573;&#12290;&#22312;&#27599;&#27425;&#25773;&#25918;&#26102;&#65292;&#25152;&#36873;&#27573;&#20351;&#29992;&#36739;&#23567;&#30340;&#36339;&#36291;&#38271;&#24230;&#37325;&#25773;&#65292;&#36825;&#20195;&#34920;&#20102;&#36825;&#20123;&#27573;&#20869;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19977;&#20010;&#38899;&#39057;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65306;AudioSet&#12289;VGG-Sound&#21644;EPIC-KITCHENS-100&#12290;
&lt;/p&gt;
&lt;p&gt;
A key function of auditory cognition is the association of characteristic sounds with their corresponding semantics over time. Humans attempting to discriminate between fine-grained audio categories, often replay the same discriminative sounds to increase their prediction confidence. We propose an end-to-end attention-based architecture that through selective repetition attends over the most discriminative sounds across the audio sequence. Our model initially uses the full audio sequence and iteratively refines the temporal segments replayed based on slot attention. At each playback, the selected segments are replayed using a smaller hop length which represents higher resolution features within these segments. We show that our method can consistently achieve state-of-the-art performance across three audio-classification benchmarks: AudioSet, VGG-Sound, and EPIC-KITCHENS-100.
&lt;/p&gt;</description></item><item><title>ROBOT&#26159;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#19968;&#32452;&#39640;&#24615;&#33021;&#12289;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21333;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21482;&#33021;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10953</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#21457;&#29616;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Discovering Many Diverse Solutions with Bayesian Optimization. (arXiv:2210.10953v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10953
&lt;/p&gt;
&lt;p&gt;
ROBOT&#26159;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#19968;&#32452;&#39640;&#24615;&#33021;&#12289;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#21333;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21482;&#33021;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ROBOT is a new Bayesian optimization method that can find a portfolio of high-performing diverse solutions, addressing the limitation of traditional single-objective Bayesian optimization methods that only seek to find a single best solution.
&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#29992;&#20110;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#30340;&#39640;&#25928;&#20248;&#21270;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#20256;&#32479;&#30340;&#21333;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#21482;&#23547;&#27714;&#25214;&#21040;&#19968;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#22312;&#35299;&#20915;&#26041;&#26696;&#21518;&#26399;&#21487;&#33021;&#21464;&#24471;&#26840;&#25163;&#30340;&#24773;&#20917;&#19979;&#20250;&#26377;&#24456;&#22823;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROBOT&#30340;&#25490;&#24207;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#25214;&#21040;&#19968;&#32452;&#39640;&#24615;&#33021;&#12289;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#35780;&#20272;&#20102;ROBOT&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#21457;&#29616;&#22823;&#37327;&#39640;&#24615;&#33021;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#19982;&#23547;&#25214;&#21333;&#20010;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#38656;&#35201;&#24456;&#23569;&#30340;&#39069;&#22806;&#20989;&#25968;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a popular approach for sample-efficient optimization of black-box objective functions. While BO has been successfully applied to a wide range of scientific applications, traditional approaches to single-objective BO only seek to find a single best solution. This can be a significant limitation in situations where solutions may later turn out to be intractable. For example, a designed molecule may turn out to violate constraints that can only be reasonably evaluated after the optimization process has concluded. To address this issue, we propose Rank-Ordered Bayesian Optimization with Trust-regions (ROBOT) which aims to find a portfolio of high-performing solutions that are diverse according to a user-specified diversity metric. We evaluate ROBOT on several real-world applications and show that it can discover large sets of high-performing diverse solutions while requiring few additional function evaluations compared to finding a single best solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#22312;&#20316;&#32773;&#24402;&#23646;&#21644;&#28151;&#28102;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#38656;&#35201;&#24320;&#21457;&#26032;&#22411;AA / AO&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#31070;&#32463;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.10488</link><description>&lt;p&gt;
&#31070;&#32463;&#25991;&#26412;&#20316;&#32773;&#24402;&#23646;&#21644;&#28151;&#28102;&#65306;&#25968;&#25454;&#25366;&#25496;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective. (arXiv:2210.10488v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#22312;&#20316;&#32773;&#24402;&#23646;&#21644;&#28151;&#28102;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#38656;&#35201;&#24320;&#21457;&#26032;&#22411;AA / AO&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#31070;&#32463;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey investigates the application of neural text generation techniques in authorship attribution and obfuscation, and highlights the need for developing novel AA/AO solutions to deal with neural texts.
&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#24402;&#23646;&#65288;AA&#65289;&#21644;&#20316;&#32773;&#28151;&#28102;&#65288;AO&#65289;&#26159;&#38544;&#31169;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#21644;&#37325;&#35201;&#30340;&#20004;&#20010;&#20132;&#32455;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20256;&#32479;&#19978;&#65292;&#20316;&#32773;&#30340;&#27010;&#24565;&#21450;&#20854;&#38543;&#20043;&#32780;&#26469;&#30340;&#38544;&#31169;&#20851;&#27880;&#20165;&#38024;&#23545;&#20154;&#31867;&#20316;&#32773;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#65288;NTG&#65289;&#25216;&#26415;&#30340;&#29190;&#28856;&#24615;&#36827;&#23637;&#65292;&#29616;&#22312;&#24517;&#39035;&#32771;&#34385;&#20154;&#31867;&#12289;&#26426;&#22120;&#25110;&#23427;&#20204;&#30340;&#32452;&#21512;&#30340;&#20316;&#32773;&#36523;&#20221;&#12290;&#30001;&#20110;&#31070;&#32463;&#25991;&#26412;&#22312;&#24694;&#24847;&#20351;&#29992;&#26102;&#30340;&#28508;&#22312;&#23041;&#32961;&#65292;&#20102;&#35299;&#20256;&#32479;AA / AO&#35299;&#20915;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#24182;&#24320;&#21457;&#22788;&#29702;&#31070;&#32463;&#25991;&#26412;&#30340;&#26032;&#22411;AA / AO&#35299;&#20915;&#26041;&#26696;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two interlocking research questions of growing interest and importance in privacy research are Authorship Attribution (AA) and Authorship Obfuscation (AO). Given an artifact, especially a text t in question, an AA solution aims to accurately attribute t to its true author out of many candidate authors while an AO solution aims to modify t to hide its true authorship. Traditionally, the notion of authorship and its accompanying privacy concern is only toward human authors. However, in recent years, due to the explosive advancements in Neural Text Generation (NTG) techniques in NLP, capable of synthesizing human-quality open-ended texts (so-called "neural texts"), one has to now consider authorships by humans, machines, or their combination. Due to the implications and potential threats of neural texts when used maliciously, it has become critical to understand the limitations of traditional AA/AO solutions and develop novel AA/AO solutions in dealing with neural texts. In this survey, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#32467;&#26500;&#23478;&#26063;EquiTopo&#65292;&#23427;&#20855;&#26377;&#65288;&#20960;&#20046;&#65289;&#24658;&#23450;&#30340;&#24230;&#25968;&#21644;&#19982;&#32593;&#32476;&#22823;&#23567;&#26080;&#20851;&#30340;&#20849;&#35782;&#36895;&#29575;&#65292;&#29992;&#20110;&#34913;&#37327;&#28151;&#21512;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.07881</link><description>&lt;p&gt;
&#24102;&#26377;$O(1)$&#20849;&#35782;&#36895;&#29575;&#30340;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#36890;&#20449;&#39640;&#25928;&#25299;&#25169;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Topologies for Decentralized Learning with $O(1)$ Consensus Rate. (arXiv:2210.07881v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#32467;&#26500;&#23478;&#26063;EquiTopo&#65292;&#23427;&#20855;&#26377;&#65288;&#20960;&#20046;&#65289;&#24658;&#23450;&#30340;&#24230;&#25968;&#21644;&#19982;&#32593;&#32476;&#22823;&#23567;&#26080;&#20851;&#30340;&#20849;&#35782;&#36895;&#29575;&#65292;&#29992;&#20110;&#34913;&#37327;&#28151;&#21512;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new family of topologies, EquiTopo, which has an (almost) constant degree and a network-size-independent consensus rate that is used to measure the mixing efficiency.
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#20248;&#21270;&#26159;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#26032;&#20852;&#33539;&#20363;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#28857;&#23545;&#28857;&#36890;&#20449;&#23454;&#29616;&#32593;&#32476;&#33539;&#22260;&#20869;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;&#12290;&#30001;&#20110;&#36890;&#20449;&#24448;&#24448;&#27604;&#35745;&#31639;&#24930;&#65292;&#22240;&#27492;&#24403;&#27599;&#20010;&#20195;&#29702;&#27599;&#27425;&#36845;&#20195;&#20165;&#19982;&#23569;&#25968;&#30456;&#37051;&#20195;&#29702;&#36890;&#20449;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#27604;&#20351;&#29992;&#26356;&#22810;&#20195;&#29702;&#25110;&#20013;&#22830;&#26381;&#21153;&#22120;&#26356;&#24555;&#22320;&#23436;&#25104;&#36845;&#20195;&#12290;&#28982;&#32780;&#65292;&#21040;&#36798;&#32593;&#32476;&#33539;&#22260;&#20869;&#30340;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#24635;&#36845;&#20195;&#27425;&#25968;&#21463;&#21040;&#20195;&#29702;&#20449;&#24687;&#36890;&#36807;&#36890;&#20449;&#8220;&#28151;&#21512;&#8221;&#30340;&#36895;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27969;&#34892;&#30340;&#36890;&#20449;&#25299;&#25169;&#32467;&#26500;&#35201;&#20040;&#20855;&#26377;&#36739;&#22823;&#30340;&#26368;&#22823;&#24230;&#25968;&#65288;&#20363;&#22914;&#26143;&#24418;&#21644;&#23436;&#20840;&#22270;&#65289;&#65292;&#35201;&#20040;&#22312;&#28151;&#21512;&#20449;&#24687;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#65288;&#20363;&#22914;&#29615;&#21644;&#32593;&#26684;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#32467;&#26500;&#23478;&#26063;EquiTopo&#65292;&#23427;&#20855;&#26377;&#65288;&#20960;&#20046;&#65289;&#24658;&#23450;&#30340;&#24230;&#25968;&#21644;&#19982;&#32593;&#32476;&#22823;&#23567;&#26080;&#20851;&#30340;&#20849;&#35782;&#36895;&#29575;&#65292;&#29992;&#20110;&#34913;&#37327;&#28151;&#21512;&#25928;&#29575;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#23478;&#26063;&#20013;&#65292;EquiStatic&#30340;&#24230;&#25968;&#20026;$
&lt;/p&gt;
&lt;p&gt;
Decentralized optimization is an emerging paradigm in distributed learning in which agents achieve network-wide solutions by peer-to-peer communication without the central server. Since communication tends to be slower than computation, when each agent communicates with only a few neighboring agents per iteration, they can complete iterations faster than with more agents or a central server. However, the total number of iterations to reach a network-wide solution is affected by the speed at which the agents' information is ``mixed'' by communication. We found that popular communication topologies either have large maximum degrees (such as stars and complete graphs) or are ineffective at mixing information (such as rings and grids). To address this problem, we propose a new family of topologies, EquiTopo, which has an (almost) constant degree and a network-size-independent consensus rate that is used to measure the mixing efficiency.  In the proposed family, EquiStatic has a degree of $
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#29289;&#20307;&#35270;&#39057;&#19978;&#36827;&#34892;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#34920;&#38754;&#23884;&#20837;&#23398;&#20064;&#20102;&#36755;&#20837;&#22270;&#20687;&#21644;&#35268;&#33539;&#24418;&#29366;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#23398;&#20064;&#21040;&#30340;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#24212;&#29992;&#20110;6D&#23039;&#24577;&#20272;&#35745;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.07199</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#20960;&#20309;&#23545;&#24212;&#29992;&#20110;&#37326;&#22806;&#31867;&#21035;&#32423;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Geometric Correspondence for Category-Level 6D Object Pose Estimation in the Wild. (arXiv:2210.07199v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#29289;&#20307;&#35270;&#39057;&#19978;&#36827;&#34892;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#34920;&#38754;&#23884;&#20837;&#23398;&#20064;&#20102;&#36755;&#20837;&#22270;&#20687;&#21644;&#35268;&#33539;&#24418;&#29366;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290;&#23398;&#20064;&#21040;&#30340;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#24212;&#29992;&#20110;6D&#23039;&#24577;&#20272;&#35745;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a self-supervised learning approach for category-level 6D object pose estimation in the wild, which reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. The proposed novel geometrical cycle-consistency losses construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and other tasks.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;6D&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#65292;&#23427;&#20173;&#28982;&#36828;&#26410;&#35299;&#20915;&#12290;&#24403;&#36716;&#21521;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#26102;&#65292;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#23545;&#26410;&#35265;&#23454;&#20363;&#36827;&#34892;&#27867;&#21270;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#21463;&#21040;&#20174;&#27169;&#25311;&#25110;&#20174;&#20154;&#31867;&#25910;&#38598;&#30340;&#27880;&#37322;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#29289;&#20307;&#35270;&#39057;&#19978;&#36827;&#34892;&#31867;&#21035;&#32423;6D&#23039;&#24577;&#20272;&#35745;&#65292;&#20811;&#26381;&#20102;&#36825;&#19968;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37325;&#26500;&#20102;&#29289;&#20307;&#31867;&#21035;&#30340;&#35268;&#33539;3D&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#34920;&#38754;&#23884;&#20837;&#23398;&#20064;&#20102;&#36755;&#20837;&#22270;&#20687;&#21644;&#35268;&#33539;&#24418;&#29366;&#20043;&#38388;&#30340;&#23494;&#38598;&#23545;&#24212;&#20851;&#31995;&#12290;&#23545;&#20110;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#20960;&#20309;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#65292;&#23427;&#20204;&#22312;2D-3D&#31354;&#38388;&#12289;&#19981;&#21516;&#23454;&#20363;&#21644;&#19981;&#21516;&#26102;&#38388;&#27493;&#20043;&#38388;&#26500;&#24314;&#24490;&#29615;&#12290;&#23398;&#20064;&#21040;&#30340;&#23545;&#24212;&#20851;&#31995;&#21487;&#20197;&#24212;&#29992;&#20110;6D&#23039;&#24577;&#20272;&#35745;&#21644;&#20854;&#20182;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While 6D object pose estimation has wide applications across computer vision and robotics, it remains far from being solved due to the lack of annotations. The problem becomes even more challenging when moving to category-level 6D pose, which requires generalization to unseen instances. Current approaches are restricted by leveraging annotations from simulation or collected from humans. In this paper, we overcome this barrier by introducing a self-supervised learning approach trained directly on large-scale real-world object videos for category-level 6D pose estimation in the wild. Our framework reconstructs the canonical 3D shape of an object category and learns dense correspondences between input images and the canonical shape via surface embedding. For training, we propose novel geometrical cycle-consistency losses which construct cycles across 2D-3D spaces, across different instances and different time steps. The learned correspondence can be applied for 6D pose estimation and othe
&lt;/p&gt;</description></item><item><title>PDEBench&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#27169;&#25311;&#20219;&#21153;&#22522;&#20934;&#22871;&#20214;&#65292;&#21253;&#25324;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#23545;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#19982;&#32463;&#20856;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2210.07182</link><description>&lt;p&gt;
PDEBENCH&#65306;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
PDEBENCH: An Extensive Benchmark for Scientific Machine Learning. (arXiv:2210.07182v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07182
&lt;/p&gt;
&lt;p&gt;
PDEBench&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#27169;&#25311;&#20219;&#21153;&#22522;&#20934;&#22871;&#20214;&#65292;&#21253;&#25324;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#23545;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#19982;&#32463;&#20856;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
PDEBench is a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs), which includes code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines.
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29289;&#29702;&#31995;&#32479;&#24314;&#27169;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#32570;&#20047;&#26131;&#20110;&#20351;&#29992;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#20195;&#34920;&#24615;&#30340;&#31185;&#23398;ML&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PDEBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#27169;&#25311;&#20219;&#21153;&#22522;&#20934;&#22871;&#20214;&#12290;PDEBench&#21253;&#25324;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#23545;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#19982;&#32463;&#20856;&#25968;&#20540;&#27169;&#25311;&#21644;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20934;&#38382;&#39064;&#38598;&#20855;&#26377;&#20197;&#19979;&#29420;&#29305;&#29305;&#24449;&#65306;&#65288;1&#65289;&#19982;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;PDE&#30340;&#33539;&#22260;&#26356;&#24191;&#65292;&#20174;&#30456;&#23545;&#24120;&#35265;&#30340;&#31034;&#20363;&#21040;&#26356;&#29616;&#23454;&#21644;&#22256;&#38590;&#30340;&#38382;&#39064;&#65307;&#65288;2&#65289;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#20934;&#22791;&#22909;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#26356;&#22823;&#65292;&#21253;&#25324;&#36328;&#26356;&#22810;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#20197;&#21450;PDE&#21442;&#25968;&#30340;&#22810;&#20010;&#27169;&#25311;&#36816;&#34892;&#65307;&#65288;3&#65289;&#26356;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#26356;&#22810;&#30340;&#24615;&#33021;&#25351;&#26631;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning-based modeling of physical systems has experienced increased interest in recent years. Despite some impressive progress, there is still a lack of benchmarks for Scientific ML that are easy to use but still challenging and representative of a wide range of problems. We introduce PDEBench, a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs). PDEBench comprises both code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines. Our proposed set of benchmark problems contribute the following unique features: (1) A much wider range of PDEs compared to existing benchmarks, ranging from relatively common examples to more realistic and difficult problems; (2) much larger ready-to-use datasets compared to prior work, comprising multiple simulation runs across a larger number of initial and boundary conditions and PDE parameters; (3) more exte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#36890;&#36807;&#21516;&#26102;&#20351;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#25968;&#25454;&#65292;&#21487;&#20197;&#35774;&#35745;&#20986;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#25903;&#25345;&#39640;&#36136;&#37327;&#31574;&#30053;&#19988;&#29615;&#22659;&#20855;&#26377;&#26377;&#30028;&#21452;&#32447;&#24615;&#31209;&#30340;&#24773;&#20917;&#19979;&#26082;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21448;&#20855;&#26377;&#32479;&#35745;&#25928;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#30340;Hy-Q&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.06718</link><description>&lt;p&gt;
&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#65306;&#21516;&#26102;&#20351;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#25968;&#25454;&#21487;&#20197;&#20351;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient. (arXiv:2210.06718v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#36890;&#36807;&#21516;&#26102;&#20351;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#25968;&#25454;&#65292;&#21487;&#20197;&#35774;&#35745;&#20986;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#25903;&#25345;&#39640;&#36136;&#37327;&#31574;&#30053;&#19988;&#29615;&#22659;&#20855;&#26377;&#26377;&#30028;&#21452;&#32447;&#24615;&#31209;&#30340;&#24773;&#20917;&#19979;&#26082;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21448;&#20855;&#26377;&#32479;&#35745;&#25928;&#29575;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#30340;Hy-Q&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a hybrid reinforcement learning setting that uses both offline and online data to design simple and efficient algorithms. The authors prove that the algorithm is both computationally and statistically efficient whenever the offline dataset supports a high-quality policy and the environment has bounded bilinear rank. In experiments, they show that the Hy-Q algorithm with neural network function approximation outperforms other algorithms.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65288;&#28151;&#21512;RL&#65289;&#65292;&#20854;&#20013;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#23454;&#26102;&#22312;&#32447;&#20132;&#20114;&#25910;&#38598;&#32463;&#39564;&#12290;&#35813;&#26694;&#26550;&#32531;&#35299;&#20102;&#32431;&#31163;&#32447;&#21644;&#22312;&#32447;RL&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#25361;&#25112;&#65292;&#20801;&#35768;&#35774;&#35745;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#26080;&#35770;&#26159;&#22312;&#29702;&#35770;&#36824;&#26159;&#23454;&#36341;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#32463;&#20856;&#30340;Q&#23398;&#20064;/&#36845;&#20195;&#31639;&#27861;&#36866;&#24212;&#20110;&#28151;&#21512;&#35774;&#32622;&#26469;&#23637;&#31034;&#36825;&#20123;&#20248;&#21183;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#28151;&#21512;Q&#23398;&#20064;&#25110;Hy-Q&#12290;&#22312;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#31163;&#32447;&#25968;&#25454;&#38598;&#25903;&#25345;&#39640;&#36136;&#37327;&#31574;&#30053;&#19988;&#29615;&#22659;&#20855;&#26377;&#26377;&#30028;&#21452;&#32447;&#24615;&#31209;&#30340;&#24773;&#20917;&#19979;&#26082;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#21448;&#20855;&#26377;&#32479;&#35745;&#25928;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19982;&#31574;&#30053;&#26799;&#24230;/&#36845;&#20195;&#26041;&#27861;&#30340;&#20445;&#35777;&#30456;&#21453;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#23545;&#21021;&#22987;&#20998;&#24067;&#25552;&#20379;&#30340;&#35206;&#30422;&#33539;&#22260;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#36924;&#36817;&#30340;Hy-Q&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a hybrid reinforcement learning setting (Hybrid RL), in which an agent has access to an offline dataset and the ability to collect experience via real-world online interaction. The framework mitigates the challenges that arise in both pure offline and online RL settings, allowing for the design of simple and highly effective algorithms, in both theory and practice. We demonstrate these advantages by adapting the classical Q learning/iteration algorithm to the hybrid setting, which we call Hybrid Q-Learning or Hy-Q. In our theoretical results, we prove that the algorithm is both computationally and statistically efficient whenever the offline dataset supports a high-quality policy and the environment has bounded bilinear rank. Notably, we require no assumptions on the coverage provided by the initial distribution, in contrast with guarantees for policy gradient/iteration methods. In our experimental results, we show that Hy-Q with neural network function approximation outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;Transformer&#25512;&#24191;&#21040;&#22270;&#32467;&#26500;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#32771;&#34385;&#21644;&#26356;&#26032;&#36793;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25512;&#29702;&#65292;&#30456;&#27604;&#20110;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25512;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#21508;&#31181;&#22270;&#32467;&#26500;&#20219;&#21153;&#19978;&#37117;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.05062</link><description>&lt;p&gt;
&#20851;&#31995;&#27880;&#24847;&#21147;&#65306;&#23558;Transformer&#25512;&#24191;&#21040;&#22270;&#32467;&#26500;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Relational Attention: Generalizing Transformers for Graph-Structured Tasks. (arXiv:2210.05062v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;Transformer&#25512;&#24191;&#21040;&#22270;&#32467;&#26500;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#32771;&#34385;&#21644;&#26356;&#26032;&#36793;&#21521;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#25512;&#29702;&#65292;&#30456;&#27604;&#20110;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25512;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#21508;&#31181;&#22270;&#32467;&#26500;&#20219;&#21153;&#19978;&#37117;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a relational attention mechanism that generalizes Transformers for graph-structured tasks by considering and updating edge vectors in each Transformer layer, achieving reasoning over graph-structured data. Compared to state-of-the-art graph neural networks designed for reasoning over graph-structured data, it has significant advantages in various graph-structured tasks.
&lt;/p&gt;
&lt;p&gt;
Transformer&#21487;&#20197;&#28789;&#27963;&#22320;&#25805;&#20316;&#34920;&#31034;&#20219;&#21153;&#29305;&#23450;&#23454;&#20307;&#21450;&#20854;&#23646;&#24615;&#30340;&#23454;&#20540;&#21521;&#37327;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#21521;&#37327;&#21487;&#33021;&#32534;&#30721;&#19968;&#20010;&#21333;&#35789;&#29255;&#27573;&#20196;&#29260;&#21450;&#20854;&#22312;&#24207;&#21015;&#20013;&#30340;&#20301;&#32622;&#65292;&#25110;&#32773;&#19968;&#20123;&#19981;&#24102;&#20301;&#32622;&#30340;&#20449;&#24687;&#12290;&#20294;&#20316;&#20026;&#38598;&#21512;&#22788;&#29702;&#22120;&#65292;Transformer&#22312;&#25512;&#29702;&#26356;&#19968;&#33324;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#65288;&#20854;&#20013;&#33410;&#28857;&#34920;&#31034;&#23454;&#20307;&#65292;&#36793;&#34920;&#31034;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65289;&#26041;&#38754;&#22788;&#20110;&#21155;&#21183;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#23558;Transformer&#27880;&#24847;&#21147;&#25512;&#24191;&#21040;&#27599;&#20010;Transformer&#23618;&#20013;&#32771;&#34385;&#21644;&#26356;&#26032;&#36793;&#21521;&#37327;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22270;&#32467;&#26500;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20010;&#20851;&#31995;Transformer&#65292;&#21253;&#25324;&#22823;&#22411;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CLRS&#31639;&#27861;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#37027;&#37324;&#65292;&#23427;&#26126;&#26174;&#20248;&#20110;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25512;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#25910;&#30410;&#24402;&#22240;&#20110;&#20851;&#31995;&#27880;&#24847;&#21147;&#22266;&#26377;&#30340;&#33021;&#21147;&#65292;&#21363;&#21033;&#29992;&#22823;&#37327;&#30340;&#36793;&#20449;&#24687;&#26469;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers flexibly operate over sets of real-valued vectors representing task-specific entities and their attributes, where each vector might encode one word-piece token and its position in a sequence, or some piece of information that carries no position at all. But as set processors, transformers are at a disadvantage in reasoning over more general graph-structured data where nodes represent entities and edges represent relations between entities. To address this shortcoming, we generalize transformer attention to consider and update edge vectors in each transformer layer. We evaluate this relational transformer on a diverse array of graph-structured tasks, including the large and challenging CLRS Algorithmic Reasoning Benchmark. There, it dramatically outperforms state-of-the-art graph neural networks expressly designed to reason over graph-structured data. Our analysis demonstrates that these gains are attributable to relational attention's inherent ability to leverage the great
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#25110;&#27700;&#24179;&#32763;&#36716;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#23376;&#32676;&#23545;&#31216;&#24615;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2210.04087</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;CNN&#25200;&#21160;&#25915;&#20987;&#30340;&#23545;&#31216;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Symmetry Defense Against CNN Adversarial Perturbation Attacks. (arXiv:2210.04087v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#25110;&#27700;&#24179;&#32763;&#36716;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#23376;&#32676;&#23545;&#31216;&#24615;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a symmetry defense method to improve adversarial robustness by flipping or horizontally flipping symmetric adversarial samples, and uses subgroup symmetries for classification.
&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#65288;CNN&#65289;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#20250;&#25200;&#21160;&#21407;&#22987;&#26679;&#26412;&#20197;&#27450;&#39575;&#20998;&#31867;&#22120;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#36947;&#36335;&#26631;&#24535;&#22270;&#20687;&#20998;&#31867;&#22120;&#12290;CNN&#22312;&#23545;&#31216;&#26679;&#26412;&#30340;&#20998;&#31867;&#20013;&#20063;&#32570;&#20047;&#19981;&#21464;&#24615;&#65292;&#22240;&#20026;CNN&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#23545;&#31216;&#26679;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#32771;&#34385;&#21040;CNN&#32570;&#20047;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#21644;CNN&#32570;&#20047;&#19981;&#21464;&#24615;&#65292;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#30340;&#20998;&#31867;&#21487;&#33021;&#19982;&#20854;&#38169;&#35823;&#20998;&#31867;&#19981;&#21516;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#23545;&#31216;&#38450;&#24481;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#23545;&#25239;&#32773;&#19981;&#30693;&#36947;&#38450;&#24481;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#23545;&#31216;&#23545;&#25239;&#26679;&#26412;&#32763;&#36716;&#25110;&#27700;&#24179;&#32763;&#36716;&#21518;&#20877;&#36827;&#34892;&#20998;&#31867;&#12290;&#23545;&#20110;&#30693;&#36947;&#38450;&#24481;&#30340;&#23545;&#25163;&#65292;&#38450;&#24481;&#35774;&#35745;&#20102;&#19968;&#20010;Klein&#22235;&#20010;&#23545;&#31216;&#23376;&#32676;&#65292;&#20854;&#20013;&#21253;&#25324;&#27700;&#24179;&#32763;&#36716;&#21644;&#20687;&#32032;&#21453;&#36716;&#23545;&#31216;&#24615;&#12290;&#23545;&#31216;&#38450;&#24481;&#20351;&#29992;&#23376;&#32676;&#23545;&#31216;&#24615;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural network classifiers (CNNs) are susceptible to adversarial attacks that perturb original samples to fool classifiers such as an autonomous vehicle's road sign image classifier. CNNs also lack invariance in the classification of symmetric samples because CNNs can classify symmetric samples differently. Considered together, the CNN lack of adversarial robustness and the CNN lack of invariance mean that the classification of symmetric adversarial samples can differ from their incorrect classification. Could symmetric adversarial samples revert to their correct classification? This paper answers this question by designing a symmetry defense that inverts or horizontally flips adversarial samples before classification against adversaries unaware of the defense. Against adversaries aware of the defense, the defense devises a Klein four symmetry subgroup that includes the horizontal flip and pixel inversion symmetries. The symmetry defense uses the subgroup symmetries in ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24369;&#30417;&#30563;&#20449;&#24687;&#30340;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#30340;&#27010;&#29575;&#20551;&#35774;&#26631;&#31614;&#65292;&#32467;&#21512;&#23616;&#37096;&#20960;&#20309;&#29305;&#24615;&#21644;&#20808;&#39564;&#20449;&#24687;&#30340;&#36136;&#37327;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35823;&#24046;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#24182;&#22810;&#20010;&#22122;&#22768;&#20449;&#24687;&#28304;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#24369;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#26174;&#31034;&#20986;&#23545;&#29616;&#26377;&#21322;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.03594</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#19979;&#30340;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Label Propagation with Weak Supervision. (arXiv:2210.03594v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24369;&#30417;&#30563;&#20449;&#24687;&#30340;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#30340;&#27010;&#29575;&#20551;&#35774;&#26631;&#31614;&#65292;&#32467;&#21512;&#23616;&#37096;&#20960;&#20309;&#29305;&#24615;&#21644;&#20808;&#39564;&#20449;&#24687;&#30340;&#36136;&#37327;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35823;&#24046;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#24182;&#22810;&#20010;&#22122;&#22768;&#20449;&#24687;&#28304;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#24369;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#26174;&#31034;&#20986;&#23545;&#29616;&#26377;&#21322;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a label propagation algorithm that utilizes weak supervision information, specifically probabilistic hypothesized labels on the unlabeled data, and provides an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. The approach is demonstrated on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.
&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#26159;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#26088;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#25454;&#38656;&#27714;&#30340;&#37325;&#35201;&#33539;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#32463;&#20856;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#65288;LPA&#65289;&#65288;Zhu&#65286;Ghahramani&#65292;2002&#65289;&#30340;&#20998;&#26512;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#26377;&#29992;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#30340;&#27010;&#29575;&#20551;&#35774;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35823;&#24046;&#30028;&#65292;&#21033;&#29992;&#20102;&#24213;&#23618;&#22270;&#24418;&#30340;&#23616;&#37096;&#20960;&#20309;&#29305;&#24615;&#21644;&#20808;&#39564;&#20449;&#24687;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#21512;&#24182;&#22810;&#20010;&#22122;&#22768;&#20449;&#24687;&#28304;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24369;&#30417;&#30563;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#20449;&#24687;&#26469;&#28304;&#26159;&#24369;&#26631;&#35760;&#32773;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#24369;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#26174;&#31034;&#20986;&#23545;&#29616;&#26377;&#21322;&#30417;&#30563;&#21644;&#24369;&#30417;&#30563;&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning and weakly supervised learning are important paradigms that aim to reduce the growing demand for labeled data in current machine learning applications. In this paper, we introduce a novel analysis of the classical label propagation algorithm (LPA) (Zhu &amp; Ghahramani, 2002) that moreover takes advantage of useful prior information, specifically probabilistic hypothesized labels on the unlabeled data. We provide an error bound that exploits both the local geometric properties of the underlying graph and the quality of the prior information. We also propose a framework to incorporate multiple sources of noisy information. In particular, we consider the setting of weak supervision, where our sources of information are weak labelers. We demonstrate the ability of our approach on multiple benchmark weakly supervised classification tasks, showing improvements upon existing semi-supervised and weakly supervised methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#26041;&#27861;PSVRF&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#36824;&#21407;&#21464;&#35843;&#35821;&#38899;&#65292;&#21487;&#20197;&#22686;&#24378;ASV&#31995;&#32479;&#23545;&#38899;&#39640;&#32553;&#25918;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24615;&#33021;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.02731</link><description>&lt;p&gt;
PSVRF: &#26080;&#21442;&#32771;&#23398;&#20064;&#36824;&#21407;&#21464;&#35843;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
PSVRF: Learning to restore Pitch-Shifted Voice without reference. (arXiv:2210.02731v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#26041;&#27861;PSVRF&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#36824;&#21407;&#21464;&#35843;&#35821;&#38899;&#65292;&#21487;&#20197;&#22686;&#24378;ASV&#31995;&#32479;&#23545;&#38899;&#39640;&#32553;&#25918;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24615;&#33021;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a no-reference approach called PSVRF for high-quality restoration of pitch-shifted voice, which enhances the robustness of ASV systems to pitch-scaling attacks and even outperforms the state-of-the-art reference-based approach.
&lt;/p&gt;
&lt;p&gt;
&#38899;&#39640;&#32553;&#25918;&#31639;&#27861;&#23545;&#33258;&#21160;&#35828;&#35805;&#20154;&#39564;&#35777;&#65288;ASV&#65289;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21453;&#27450;&#39575;&#31639;&#27861;&#26469;&#35782;&#21035;&#21464;&#35843;&#35821;&#38899;&#24182;&#23558;&#20854;&#24674;&#22797;&#21040;&#21407;&#22987;&#29256;&#26412;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#24615;&#33021;&#36739;&#24046;&#65292;&#35201;&#20040;&#38656;&#35201;&#21407;&#22987;&#35821;&#38899;&#20316;&#20026;&#21442;&#32771;&#65292;&#38480;&#21046;&#20102;&#24212;&#29992;&#21069;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#32771;&#26041;&#27861;PSVRF&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#36824;&#21407;&#21464;&#35843;&#35821;&#38899;&#12290;&#22312;AISHELL-1&#21644;AISHELL-3&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PSVRF&#21487;&#20197;&#24674;&#22797;&#34987;&#21508;&#31181;&#38899;&#39640;&#32553;&#25918;&#25216;&#26415;&#20266;&#35013;&#30340;&#35821;&#38899;&#65292;&#26174;&#28982;&#22686;&#24378;&#20102;ASV&#31995;&#32479;&#23545;&#38899;&#39640;&#32553;&#25918;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;PSVRF&#30340;&#24615;&#33021;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pitch scaling algorithms have a significant impact on the security of Automatic Speaker Verification (ASV) systems. Although numerous anti-spoofing algorithms have been proposed to identify the pitch-shifted voice and even restore it to the original version, they either have poor performance or require the original voice as a reference, limiting the prospects of applications. In this paper, we propose a no-reference approach termed PSVRF$^1$ for high-quality restoration of pitch-shifted voice. Experiments on AISHELL-1 and AISHELL-3 demonstrate that PSVRF can restore the voice disguised by various pitch-scaling techniques, which obviously enhances the robustness of ASV systems to pitch-scaling attacks. Furthermore, the performance of PSVRF even surpasses that of the state-of-the-art reference-based approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20887;&#20313;&#37325;&#21442;&#25968;&#21270;&#21644;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#24102;&#26377;$L_1$&#24809;&#32602;&#30340;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;\textit{spred}&#65292;&#26159;$L_1$&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#22522;&#22240;&#36873;&#25321;&#20219;&#21153;&#21644;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20219;&#21153;&#65292;&#24357;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2210.01212</link><description>&lt;p&gt;
&#36890;&#36807;&#20887;&#20313;&#24615;&#23454;&#29616;&#31232;&#30095;&#24615;&#65306;&#29992;SGD&#27714;&#35299;$L_1$
&lt;/p&gt;
&lt;p&gt;
Sparsity by Redundancy: Solving $L_1$ with SGD. (arXiv:2210.01212v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01212
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20887;&#20313;&#37325;&#21442;&#25968;&#21270;&#21644;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#24102;&#26377;$L_1$&#24809;&#32602;&#30340;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;\textit{spred}&#65292;&#26159;$L_1$&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#22522;&#22240;&#36873;&#25321;&#20219;&#21153;&#21644;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20219;&#21153;&#65292;&#24357;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method called "spred" to minimize a generic differentiable loss function with $L_1$ penalty using redundant reparametrization and straightforward stochastic gradient descent. It is an exact solver of $L_1$ and can be used to train sparse neural networks for gene selection tasks and neural network compression tasks, bridging the gap between sparsity in deep learning and conventional statistical learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20887;&#20313;&#37325;&#21442;&#25968;&#21270;&#21644;&#31616;&#21333;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26469;&#26368;&#23567;&#21270;&#24102;&#26377;$L_1$&#24809;&#32602;&#30340;&#36890;&#29992;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26159;$L_1$&#24809;&#32602;&#31561;&#20215;&#20110;&#24102;&#26377;&#26435;&#37325;&#34928;&#20943;&#30340;&#21487;&#24494;&#37325;&#21442;&#25968;&#21270;&#30340;&#30452;&#25509;&#25512;&#24191;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21363;\textit{spred}&#65292;&#26159;$L_1$&#30340;&#31934;&#30830;&#27714;&#35299;&#22120;&#65292;&#24182;&#19988;&#23545;&#20110;&#36890;&#29992;&#30340;&#38750;&#20984;&#20989;&#25968;&#65292;&#37325;&#21442;&#25968;&#21270;&#25216;&#24039;&#26159;&#23436;&#20840;&#8220;&#33391;&#24615;&#8221;&#30340;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;(1)&#35757;&#32451;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#20197;&#25191;&#34892;&#22522;&#22240;&#36873;&#25321;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#22312;&#38750;&#24120;&#39640;&#32500;&#31354;&#38388;&#20013;&#25214;&#21040;&#30456;&#20851;&#29305;&#24449;&#65292;&#20197;&#21450;(2)&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20219;&#21153;&#65292;&#20808;&#21069;&#23581;&#35797;&#24212;&#29992;$L_1$&#24809;&#32602;&#30340;&#26041;&#27861;&#22343;&#26410;&#25104;&#21151;&#12290;&#20174;&#27010;&#24565;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24357;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31232;&#30095;&#24615;&#21644;&#20256;&#32479;&#32479;&#35745;&#23398;&#20064;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to minimize a generic differentiable loss function with $L_1$ penalty with a redundant reparametrization and straightforward stochastic gradient descent. Our proposal is the direct generalization of a series of previous ideas that the $L_1$ penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that the proposed method, \textit{spred}, is an exact solver of $L_1$ and that the reparametrization trick is completely ``benign" for a generic nonconvex function. Practically, we demonstrate the usefulness of the method in (1) training sparse neural networks to perform gene selection tasks, which involves finding relevant features in a very high dimensional space, and (2) neural network compression task, to which previous attempts at applying the $L_1$-penalty have been unsuccessful. Conceptually, our result bridges the gap between the sparsity in deep learning and conventional statistical learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#19977;&#31867;&#26367;&#20195;&#24230;&#37327;&#65292;&#21363;&#39044;&#27979;&#35823;&#24046;&#12289;&#27169;&#22411;&#20013;&#24515;&#24615;&#21644;&#27880;&#20837;&#21512;&#25104;&#24322;&#24120;&#30340;&#24615;&#33021;&#65292;&#36873;&#25321;&#26368;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.01078</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#27169;&#22411;&#36873;&#25321;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Model Selection for Time-series Anomaly Detection. (arXiv:2210.01078v3 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#19977;&#31867;&#26367;&#20195;&#24230;&#37327;&#65292;&#21363;&#39044;&#27979;&#35823;&#24046;&#12289;&#27169;&#22411;&#20013;&#24515;&#24615;&#21644;&#27880;&#20837;&#21512;&#25104;&#24322;&#24120;&#30340;&#24615;&#33021;&#65292;&#36873;&#25321;&#26368;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an unsupervised model selection method for time-series anomaly detection, which selects the most accurate model through three classes of surrogate metrics, namely, prediction error, model centrality, and performance on injected synthetic anomalies.
&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#26368;&#36817;&#30340;&#19968;&#39033;&#35843;&#26597;&#24471;&#20986;&#32467;&#35770;&#65292;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#26368;&#20934;&#30830;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#24322;&#24120;&#26631;&#31614;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#21487;&#29992;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#22914;&#20309;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#26368;&#20934;&#30830;&#27169;&#22411;&#30340;&#23454;&#38469;&#38382;&#39064;&#24471;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#32473;&#23450;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#32452;&#20505;&#36873;&#24322;&#24120;&#26816;&#27979;&#22120;&#65292;&#22914;&#20309;&#36873;&#25321;&#26368;&#20934;&#30830;&#30340;&#27169;&#22411;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31867;&#26367;&#20195;&#65288;&#26080;&#30417;&#30563;&#65289;&#24230;&#37327;&#65292;&#21363;&#39044;&#27979;&#35823;&#24046;&#12289;&#27169;&#22411;&#20013;&#24515;&#24615;&#21644;&#27880;&#20837;&#21512;&#25104;&#24322;&#24120;&#30340;&#24615;&#33021;&#65292;&#24182;&#34920;&#26126;&#19968;&#20123;&#24230;&#37327;&#19982;&#26631;&#20934;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#24230;&#37327;&#65288;&#22914;$F_1$&#20998;&#25968;&#65289;&#39640;&#24230;&#30456;&#20851;&#65292;&#20294;&#31243;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#22810;&#20010;&#24230;&#37327;&#32452;&#21512;&#30340;&#24230;&#37327;&#32452;&#21512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in time-series has a wide range of practical applications. While numerous anomaly detection methods have been proposed in the literature, a recent survey concluded that no single method is the most accurate across various datasets. To make matters worse, anomaly labels are scarce and rarely available in practice. The practical problem of selecting the most accurate model for a given dataset without labels has received little attention in the literature. This paper answers this question i.e. Given an unlabeled dataset and a set of candidate anomaly detectors, how can we select the most accurate model? To this end, we identify three classes of surrogate (unsupervised) metrics, namely, prediction error, model centrality, and performance on injected synthetic anomalies, and show that some metrics are highly correlated with standard supervised anomaly detection performance metrics such as the $F_1$ score, but to varying degrees. We formulate metric combination with multipl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#65292;&#22238;&#31572;&#20102;&#32500;&#24230;&#23849;&#28291;&#30340;&#21407;&#22240;&#21644;&#24433;&#21709;&#65292;&#20197;&#21450;&#32500;&#24230;&#23849;&#28291;&#22914;&#20309;&#26377;&#30410;&#65292;&#24182;&#24433;&#21709;SSL&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00638</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#26159;&#22914;&#20309;&#24418;&#25104;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
What shapes the loss landscape of self-supervised learning?. (arXiv:2210.00638v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#65292;&#22238;&#31572;&#20102;&#32500;&#24230;&#23849;&#28291;&#30340;&#21407;&#22240;&#21644;&#24433;&#21709;&#65292;&#20197;&#21450;&#32500;&#24230;&#23849;&#28291;&#22914;&#20309;&#26377;&#30410;&#65292;&#24182;&#24433;&#21709;SSL&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper answers questions about the causes and effects of dimensional collapse in self-supervised learning (SSL) by analyzing the SSL loss landscape, and explores how dimensional collapse can be beneficial and affect the robustness of SSL against data imbalance.
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38450;&#27490;&#34920;&#31034;&#23436;&#20840;&#21644;&#32500;&#24230;&#23849;&#28291;&#24050;&#25104;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#29702;&#35770;&#30340;&#29702;&#35299;&#20173;&#26377;&#30097;&#38382;&#65306;&#36825;&#20123;&#23849;&#28291;&#20309;&#26102;&#21457;&#29983;&#65311;&#26426;&#21046;&#21644;&#21407;&#22240;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#21644;&#24443;&#24213;&#20998;&#26512;SSL&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#21487;&#20998;&#26512;&#29702;&#35770;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#29702;&#35770;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#32500;&#24230;&#23849;&#28291;&#30340;&#21407;&#22240;&#65292;&#24182;&#30740;&#31350;&#20102;&#24402;&#19968;&#21270;&#21644;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20998;&#26512;&#29702;&#35770;&#25152;&#25552;&#20379;&#30340;&#21487;&#35299;&#37322;&#24615;&#26469;&#29702;&#35299;&#32500;&#24230;&#23849;&#28291;&#22914;&#20309;&#26377;&#30410;&#65292;&#24182;&#24433;&#21709;SSL&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prevention of complete and dimensional collapse of representations has recently become a design principle for self-supervised learning (SSL). However, questions remain in our theoretical understanding: When do those collapses occur? What are the mechanisms and causes? We answer these questions by deriving and thoroughly analyzing an analytically tractable theory of SSL loss landscapes. In this theory, we identify the causes of the dimensional collapse and study the effect of normalization and bias. Finally, we leverage the interpretability afforded by the analytical theory to understand how dimensional collapse can be beneficial and what affects the robustness of SSL against data imbalance.
&lt;/p&gt;</description></item><item><title>ParaGon&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#21644;&#35270;&#35273;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#20026;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#20197;&#21333;&#29420;&#23450;&#20301;&#23545;&#35937;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#29702;&#20851;&#20110;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29289;&#20307;&#25918;&#32622;&#12290;</title><link>http://arxiv.org/abs/2210.00215</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#21644;&#35270;&#35273;&#23450;&#20301;&#22312;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Differentiable Parsing and Visual Grounding of Natural Language Instructions for Object Placement. (arXiv:2210.00215v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00215
&lt;/p&gt;
&lt;p&gt;
ParaGon&#26159;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#21644;&#35270;&#35273;&#23450;&#20301;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#20026;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#20197;&#21333;&#29420;&#23450;&#20301;&#23545;&#35937;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#29702;&#20851;&#20110;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29289;&#20307;&#25918;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
ParaGon is a differentiable method for natural language instruction parsing and visual grounding in object placement tasks. It parses language instructions into an object-centric graph representation to ground objects individually and uses a novel particle-based graph neural network to reason about object placements with uncertainty.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;PARsing And visual GrOuNding (ParaGon)&#65292;&#29992;&#20110;&#22312;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#20013;&#23545;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23450;&#20301;&#12290;&#33258;&#28982;&#35821;&#35328;&#36890;&#24120;&#29992;&#32452;&#21512;&#24615;&#21644;&#27495;&#20041;&#24615;&#25551;&#36848;&#23545;&#35937;&#21644;&#31354;&#38388;&#20851;&#31995;&#65292;&#36825;&#26159;&#26377;&#25928;&#35821;&#35328;&#23450;&#20301;&#30340;&#20004;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#32452;&#21512;&#24615;&#65292;ParaGon&#23558;&#35821;&#35328;&#25351;&#20196;&#35299;&#26512;&#20026;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#20197;&#21333;&#29420;&#23450;&#20301;&#23545;&#35937;&#12290;&#23545;&#20110;&#27495;&#20041;&#24615;&#65292;ParaGon&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#29702;&#20851;&#20110;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29289;&#20307;&#25918;&#32622;&#12290;&#26412;&#36136;&#19978;&#65292;ParaGon&#23558;&#35299;&#26512;&#31639;&#27861;&#38598;&#25104;&#21040;&#27010;&#29575;&#30340;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#23427;&#26159;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#65292;&#24182;&#20174;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#20197;&#23545;&#25239;&#22797;&#26434;&#30340;&#65292;&#27169;&#31946;&#30340;&#35821;&#35328;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new method, PARsing And visual GrOuNding (ParaGon), for grounding natural language in object placement tasks. Natural language generally describes objects and spatial relations with compositionality and ambiguity, two major obstacles to effective language grounding. For compositionality, ParaGon parses a language instruction into an object-centric graph representation to ground objects individually. For ambiguity, ParaGon uses a novel particle-based graph neural network to reason about object placements with uncertainty. Essentially, ParaGon integrates a parsing algorithm into a probabilistic, data-driven learning framework. It is fully differentiable and trained end-to-end from data for robustness against complex, ambiguous language input.
&lt;/p&gt;</description></item><item><title>Recipro-CAM&#26159;&#19968;&#31181;&#24555;&#36895;&#26080;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25552;&#21462;&#30340;&#29305;&#24449;&#22270;&#36827;&#34892;&#31354;&#38388;&#25513;&#34109;&#65292;&#21033;&#29992;&#28608;&#27963;&#22270;&#21644;&#30446;&#26631;&#31867;&#21035;&#30340;&#32593;&#32476;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#35299;&#20915;&#20102;CAM&#21644;Grad-CAM&#26041;&#27861;&#30340;&#26550;&#26500;&#38480;&#21046;&#21644;&#26799;&#24230;&#35745;&#31639;&#36127;&#25285;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#30701;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#36866;&#29992;&#20110;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2209.14074</link><description>&lt;p&gt;
Recipro-CAM: &#22522;&#20110;&#24555;&#36895;&#26080;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recipro-CAM: Fast gradient-free visual explanations for convolutional neural networks. (arXiv:2209.14074v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14074
&lt;/p&gt;
&lt;p&gt;
Recipro-CAM&#26159;&#19968;&#31181;&#24555;&#36895;&#26080;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25552;&#21462;&#30340;&#29305;&#24449;&#22270;&#36827;&#34892;&#31354;&#38388;&#25513;&#34109;&#65292;&#21033;&#29992;&#28608;&#27963;&#22270;&#21644;&#30446;&#26631;&#31867;&#21035;&#30340;&#32593;&#32476;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#35299;&#20915;&#20102;CAM&#21644;Grad-CAM&#26041;&#27861;&#30340;&#26550;&#26500;&#38480;&#21046;&#21644;&#26799;&#24230;&#35745;&#31639;&#36127;&#25285;&#38382;&#39064;&#65292;&#20855;&#26377;&#26356;&#30701;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#36866;&#29992;&#20110;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recipro-CAM is a fast gradient-free visual explanation method for interpretable convolutional neural networks. It solves the architectural constraints and gradient computing burden issues of CAM and Grad-CAM methods by spatially masking the extracted feature maps to exploit the correlation between activation maps and network predictions for target classes, with shorter execution time and practical applicability.
&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#40657;&#30418;&#26412;&#36136;&#20351;&#24471;&#38590;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;AI&#20174;&#19994;&#32773;&#25506;&#32034;&#20102;&#21487;&#35299;&#37322;&#24615;AI&#26041;&#27861;&#65292;&#22914;Class Activation Map&#65288;CAM&#65289;&#21644;Grad-CAM&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#26550;&#26500;&#38480;&#21046;&#25110;&#26799;&#24230;&#35745;&#31639;&#36127;&#25285;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Score-CAM&#21644;Ablation-CAM&#20316;&#20026;&#26080;&#26799;&#24230;&#26041;&#27861;&#65292;&#20294;&#19982;&#22522;&#20110;CAM&#25110;Grad-CAM&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20204;&#20855;&#26377;&#26356;&#38271;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#21512;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#65292;&#23613;&#31649;&#23427;&#20204;&#35299;&#20915;&#20102;&#26799;&#24230;&#30456;&#20851;&#38382;&#39064;&#24182;&#21551;&#29992;&#20102;&#25512;&#29702;&#27169;&#24335;XAI&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26080;&#26799;&#24230;&#30340;Recipro-CAM&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#25552;&#21462;&#30340;&#29305;&#24449;&#22270;&#36827;&#34892;&#31354;&#38388;&#25513;&#34109;&#65292;&#20197;&#21033;&#29992;&#28608;&#27963;&#22270;&#21644;&#30446;&#26631;&#31867;&#21035;&#30340;&#32593;&#32476;&#39044;&#27979;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Convolutional Neural Network (CNN) is a widely used deep learning architecture for computer vision. However, its black box nature makes it difficult to interpret the behavior of the model. To mitigate this issue, AI practitioners have explored explainable AI methods like Class Activation Map (CAM) and Grad-CAM. Although these methods have shown promise, they are limited by architectural constraints or the burden of gradient computing. To overcome this issue, Score-CAM and Ablation-CAM have been proposed as gradient-free methods, but they have longer execution times compared to CAM or Grad-CAM based methods, making them unsuitable for real-world solution though they resolved gradient related issues and enabled inference mode XAI. To address this challenge, we propose a fast gradient-free Reciprocal CAM (Recipro-CAM) method. Our approach involves spatially masking the extracted feature maps to exploit the correlation between activation maps and network predictions for target classes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#21363;&#24320;&#25918;&#24335;FL&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#20551;&#35774;&#26412;&#22320;&#23458;&#25143;&#31471;&#20989;&#25968;&#26159;&#24378;&#20984;&#21644;&#24179;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35770;&#19978;&#37327;&#21270;&#20102;&#20004;&#31181;FL&#31639;&#27861;&#30340;&#31283;&#23450;&#21322;&#24452;&#12290;</title><link>http://arxiv.org/abs/2209.12307</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the Stability Analysis of Open Federated Learning Systems. (arXiv:2209.12307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#21363;&#24320;&#25918;&#24335;FL&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#20551;&#35774;&#26412;&#22320;&#23458;&#25143;&#31471;&#20989;&#25968;&#26159;&#24378;&#20984;&#21644;&#24179;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35770;&#19978;&#37327;&#21270;&#20102;&#20004;&#31181;FL&#31639;&#27861;&#30340;&#31283;&#23450;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the stability issue of open federated learning systems, proposes a new performance metric, namely the stability of open FL systems, and theoretically quantifies the stability radius of two FL algorithms under the assumption that local clients' functions are strongly convex and smooth.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#24320;&#25918;&#24335;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#21487;&#33021;&#22312;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#21644;/&#25110;&#31163;&#24320;&#31995;&#32479;&#12290;&#30001;&#20110;&#23384;&#22312;&#23458;&#25143;&#31471;&#25968;&#37327;&#30340;&#21464;&#21270;&#65292;&#26080;&#27861;&#20445;&#35777;&#22312;&#24320;&#25918;&#31995;&#32479;&#20013;&#25910;&#25947;&#21040;&#22266;&#23450;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#24230;&#37327;&#65292;&#31216;&#20026;&#24320;&#25918;&#24335;FL&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#23427;&#37327;&#21270;&#20102;&#22312;&#24320;&#25918;&#31995;&#32479;&#20013;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#22312;&#20551;&#35774;&#26412;&#22320;&#23458;&#25143;&#31471;&#20989;&#25968;&#26159;&#24378;&#20984;&#21644;&#24179;&#28369;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#37327;&#21270;&#20102;&#20004;&#31181;FL&#31639;&#27861;&#65288;&#21363;&#26412;&#22320;SGD&#21644;&#26412;&#22320;Adam&#65289;&#30340;&#31283;&#23450;&#21322;&#24452;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#20010;&#21322;&#24452;&#20381;&#36182;&#20110;&#20960;&#20010;&#20851;&#38190;&#21442;&#25968;&#65292;&#21253;&#25324;&#20989;&#25968;&#26465;&#20214;&#25968;&#20197;&#21450;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the open federated learning (FL) systems, where clients may join and/or leave the system during the FL process. Given the variability of the number of present clients, convergence to a fixed model cannot be guaranteed in open systems. Instead, we resort to a new performance metric that we term the stability of open FL systems, which quantifies the magnitude of the learned model in open systems. Under the assumption that local clients' functions are strongly convex and smooth, we theoretically quantify the radius of stability for two FL algorithms, namely local SGD and local Adam. We observe that this radius relies on several key parameters, including the function condition number as well as the variance of the stochastic gradient. Our theoretical results are further verified by numerical simulations on both synthetic and real-world benchmark data-sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;U-Sleep&#30561;&#30496;&#35780;&#20998;&#31639;&#27861;&#21487;&#20197;&#24377;&#24615;&#22320;&#20351;&#29992;&#38750;&#25512;&#33616;&#25110;&#38750;&#20256;&#32479;&#30340;&#23548;&#32852;&#65292;&#32780;&#19981;&#38656;&#35201;&#20005;&#26684;&#36981;&#23432;AASM&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2209.11173</link><description>&lt;p&gt;
U-Sleep&#23545;AASM&#25351;&#21335;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
U-Sleep's resilience to AASM guidelines. (arXiv:2209.11173v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.11173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;U-Sleep&#30561;&#30496;&#35780;&#20998;&#31639;&#27861;&#21487;&#20197;&#24377;&#24615;&#22320;&#20351;&#29992;&#38750;&#25512;&#33616;&#25110;&#38750;&#20256;&#32479;&#30340;&#23548;&#32852;&#65292;&#32780;&#19981;&#38656;&#35201;&#20005;&#26684;&#36981;&#23432;AASM&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study shows that the deep learning-based U-Sleep sleep scoring algorithm can flexibly use non-recommended or non-traditional derivations without strictly adhering to AASM guidelines.
&lt;/p&gt;
&lt;p&gt;
AASM&#25351;&#21335;&#26159;&#20960;&#21313;&#24180;&#21162;&#21147;&#30340;&#32467;&#26524;&#65292;&#26088;&#22312;&#26631;&#20934;&#21270;&#30561;&#30496;&#35780;&#20998;&#31243;&#24207;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#20849;&#20139;&#20840;&#29699;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#35813;&#25351;&#21335;&#28085;&#30422;&#20102;&#20174;&#25216;&#26415;/&#25968;&#23383;&#35268;&#33539;&#65288;&#20363;&#22914;&#65292;&#25512;&#33616;&#30340;EEG&#23548;&#32852;&#65289;&#21040;&#26681;&#25454;&#24180;&#40836;&#35814;&#32454;&#30340;&#30561;&#30496;&#35780;&#20998;&#35268;&#21017;&#30340;&#20960;&#20010;&#26041;&#38754;&#12290;&#33258;&#21160;&#30561;&#30496;&#35780;&#20998;&#31995;&#32479;&#22987;&#32456;&#23558;&#26631;&#20934;&#20316;&#20026;&#22522;&#26412;&#25351;&#21335;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30561;&#30496;&#35780;&#20998;&#31639;&#27861;&#21487;&#33021;&#19981;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#20020;&#24202;&#30693;&#35782;&#25110;&#20005;&#26684;&#36981;&#23432;AASM&#25351;&#21335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;U-Sleep&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#30561;&#30496;&#35780;&#20998;&#31639;&#27861;&#65292;&#21363;&#20351;&#20351;&#29992;&#20020;&#24202;&#38750;&#25512;&#33616;&#25110;&#38750;&#20256;&#32479;&#30340;&#23548;&#32852;&#65292;&#20063;&#21487;&#20197;&#36275;&#22815;&#24378;&#22823;&#22320;&#35299;&#20915;&#35780;&#20998;&#20219;&#21153;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#21033;&#29992;&#26377;&#20851;&#21463;&#35797;&#32773;&#24180;&#40836;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
AASM guidelines are the result of decades of efforts aiming at standardizing sleep scoring procedure, with the final goal of sharing a worldwide common methodology. The guidelines cover several aspects from the technical/digital specifications,e.g., recommended EEG derivations, to detailed sleep scoring rules accordingly to age. Automated sleep scoring systems have always largely exploited the standards as fundamental guidelines. In this context, deep learning has demonstrated better performance compared to classical machine learning. Our present work shows that a deep learning based sleep scoring algorithm may not need to fully exploit the clinical knowledge or to strictly adhere to the AASM guidelines. Specifically, we demonstrate that U-Sleep, a state-of-the-art sleep scoring algorithm, can be strong enough to solve the scoring task even using clinically non-recommended or non-conventional derivations, and with no need to exploit information about the chronological age of the subjec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;GMFG&#20844;&#24335;&#65292;&#31216;&#20026;LPGMFG&#65292;&#23427;&#21033;&#29992;$L^p$&#22270;&#24418;&#30340;&#22270;&#24418;&#29702;&#35770;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#20197;&#26377;&#25928;&#19988;&#20934;&#30830;&#22320;&#36817;&#20284;&#35299;&#20915;&#31232;&#30095;&#32593;&#32476;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24130;&#24459;&#32593;&#32476;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#29702;&#35770;&#23384;&#22312;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#35777;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.03880</link><description>&lt;p&gt;
&#23398;&#20064;&#31232;&#30095;&#22270;&#24418;&#22343;&#22330;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Learning Sparse Graphon Mean Field Games. (arXiv:2209.03880v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;GMFG&#20844;&#24335;&#65292;&#31216;&#20026;LPGMFG&#65292;&#23427;&#21033;&#29992;$L^p$&#22270;&#24418;&#30340;&#22270;&#24418;&#29702;&#35770;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#20197;&#26377;&#25928;&#19988;&#20934;&#30830;&#22320;&#36817;&#20284;&#35299;&#20915;&#31232;&#30095;&#32593;&#32476;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24130;&#24459;&#32593;&#32476;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#29702;&#35770;&#23384;&#22312;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#35777;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel formulation of GMFGs, called LPGMFG, which leverages the graph theoretical concept of $L^p$ graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems, especially power law networks. The paper derives theoretical existence and convergence guarantees and gives empirical examples that demonstrate the accuracy of the learning method.
&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#35299;&#20915;&#20855;&#26377;&#22823;&#37327;&#20195;&#29702;&#30340;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#22270;&#24418;&#22343;&#22330;&#21338;&#24328;&#65288;GMFG&#65289;&#20351;&#24471;&#21487;&#20197;&#23545;&#21542;&#21017;&#38590;&#20197;&#22788;&#29702;&#30340;MARL&#38382;&#39064;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20998;&#26512;&#12290;&#30001;&#20110;&#22270;&#24418;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#38480;&#20110;&#25551;&#36848;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#65288;&#22914;&#24130;&#24459;&#22270;&#65289;&#30340;&#31264;&#23494;&#22270;&#24418;&#65292;&#36825;&#26159;&#19981;&#36275;&#30340;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20171;&#32461;&#20102;GMFG&#30340;&#26032;&#22411;&#20844;&#24335;&#65292;&#31216;&#20026;LPGMFG&#65292;&#23427;&#21033;&#29992;$L^p$&#22270;&#24418;&#30340;&#22270;&#24418;&#29702;&#35770;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#20197;&#26377;&#25928;&#19988;&#20934;&#30830;&#22320;&#36817;&#20284;&#35299;&#20915;&#31232;&#30095;&#32593;&#32476;&#38382;&#39064;&#12290;&#36825;&#23588;&#20854;&#21253;&#25324;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#20013;&#32463;&#39564;&#35266;&#23519;&#21040;&#30340;&#24130;&#24459;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#26080;&#27861;&#34987;&#26631;&#20934;&#22270;&#24418;&#25152;&#25429;&#25417;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#29702;&#35770;&#23384;&#22312;&#21644;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#32473;&#20986;&#20102;&#23454;&#35777;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the field of multi-agent reinforcement learning (MARL) has made considerable progress in the last years, solving systems with a large number of agents remains a hard challenge. Graphon mean field games (GMFGs) enable the scalable analysis of MARL problems that are otherwise intractable. By the mathematical structure of graphons, this approach is limited to dense graphs which are insufficient to describe many real-world networks such as power law graphs. Our paper introduces a novel formulation of GMFGs, called LPGMFGs, which leverages the graph theoretical concept of $L^p$ graphons and provides a machine learning tool to efficiently and accurately approximate solutions for sparse network problems. This especially includes power law networks which are empirically observed in various application areas and cannot be captured by standard graphons. We derive theoretical existence and convergence guarantees and give empirical examples that demonstrate the accuracy of our learning ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#23569;&#25968;&#33410;&#28857;&#30340;&#26377;&#38480;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#39044;&#27979;&#25972;&#20010;&#31995;&#32479;&#30456;&#20851;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#21482;&#26377;&#22522;&#30784;&#31995;&#32479;&#30340;&#23376;&#38598;&#30340;&#26377;&#38480;&#26102;&#38388;&#24207;&#21015;&#36275;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#30456;&#20851;&#30697;&#38453;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2209.01198</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#26377;&#38480;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#20272;&#35745;&#30456;&#20851;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Estimation of Correlation Matrices from Limited time series Data using Machine Learning. (arXiv:2209.01198v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#23569;&#25968;&#33410;&#28857;&#30340;&#26377;&#38480;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#39044;&#27979;&#25972;&#20010;&#31995;&#32479;&#30456;&#20851;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#39564;&#35777;&#20102;&#21482;&#26377;&#22522;&#30784;&#31995;&#32479;&#30340;&#23376;&#38598;&#30340;&#26377;&#38480;&#26102;&#38388;&#24207;&#21015;&#36275;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#30456;&#20851;&#30697;&#38453;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method to predict the correlation matrix of entire systems from finite time series information of a few randomly selected nodes using supervised machine learning, and validates that only a limited time series of a subset of the entire system is enough to make good correlation matrix predictions.
&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#30697;&#38453;&#21253;&#21547;&#20851;&#20110;&#21160;&#24577;&#31995;&#32479;&#30340;&#21508;&#31181;&#26102;&#31354;&#20449;&#24687;&#12290;&#20174;&#23569;&#25968;&#33410;&#28857;&#30340;&#37096;&#20998;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#39044;&#27979;&#30456;&#20851;&#30697;&#38453;&#34920;&#24449;&#20102;&#25972;&#20010;&#22522;&#30784;&#31995;&#32479;&#30340;&#26102;&#31354;&#21160;&#24577;&#12290;&#36825;&#20123;&#20449;&#24687;&#26377;&#21161;&#20110;&#39044;&#27979;&#22522;&#30784;&#32593;&#32476;&#32467;&#26500;&#65292;&#20363;&#22914;&#20174;&#23574;&#23792;&#25968;&#25454;&#25512;&#26029;&#31070;&#32463;&#20803;&#36830;&#25509;&#65292;&#20174;&#34920;&#36798;&#25968;&#25454;&#25512;&#26029;&#22522;&#22240;&#20043;&#38388;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#21450;&#21457;&#29616;&#27668;&#20505;&#21464;&#21270;&#20013;&#30340;&#38271;&#36317;&#31163;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;&#39044;&#27979;&#30456;&#20851;&#30697;&#38453;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#30784;&#32593;&#32476;&#30340;&#25152;&#26377;&#33410;&#28857;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#23569;&#25968;&#38543;&#26426;&#36873;&#25321;&#30340;&#33410;&#28857;&#30340;&#26377;&#38480;&#26102;&#38388;&#24207;&#21015;&#20449;&#24687;&#39044;&#27979;&#25972;&#20010;&#31995;&#32479;&#30340;&#30456;&#20851;&#30697;&#38453;&#12290;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#39564;&#35777;&#20102;&#21482;&#26377;&#22522;&#30784;&#31995;&#32479;&#30340;&#23376;&#38598;&#30340;&#26377;&#38480;&#26102;&#38388;&#24207;&#21015;&#36275;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#30456;&#20851;&#30697;&#38453;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#30784;&#31995;&#32479;&#30340;&#26102;&#31354;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correlation matrices contain a wide variety of spatio-temporal information about a dynamical system. Predicting correlation matrices from partial time series information of a few nodes characterizes the spatio-temporal dynamics of the entire underlying system. This information can help to predict the underlying network structure, e.g., inferring neuronal connections from spiking data, deducing causal dependencies between genes from expression data, and discovering long spatial range influences in climate variations. Traditional methods of predicting correlation matrices utilize time series data of all the nodes of the underlying networks. Here, we use a supervised machine learning technique to predict the correlation matrix of entire systems from finite time series information of a few randomly selected nodes. The accuracy of the prediction validates that only a limited time series of a subset of the entire system is enough to make good correlation matrix predictions. Furthermore, usin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32452;&#21512;&#20998;&#37197;&#65288;BOCA&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#25429;&#33719;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#26426;&#21046;&#20013;&#65292;&#35299;&#20915;&#20102;&#32452;&#21512;&#20998;&#37197;&#39046;&#22495;&#20013;&#20808;&#21069;&#24037;&#20316;&#30340;&#20027;&#35201;&#32570;&#28857;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#24341;&#23548;&#20195;&#29702;&#25552;&#20379;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2208.14698</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32452;&#21512;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization-based Combinatorial Assignment. (arXiv:2208.14698v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32452;&#21512;&#20998;&#37197;&#65288;BOCA&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#23558;&#25429;&#33719;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#26426;&#21046;&#20013;&#65292;&#35299;&#20915;&#20102;&#32452;&#21512;&#20998;&#37197;&#39046;&#22495;&#20013;&#20808;&#21069;&#24037;&#20316;&#30340;&#20027;&#35201;&#32570;&#28857;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#24341;&#23548;&#20195;&#29702;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Bayesian optimization-based combinatorial assignment (BOCA) mechanism, which addresses the main shortcoming of prior work in the combinatorial assignment domain by integrating a method for capturing model uncertainty into an iterative combinatorial auction mechanism, and can better elicit information from agents.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32452;&#21512;&#20998;&#37197;&#39046;&#22495;&#65292;&#21253;&#25324;&#32452;&#21512;&#25293;&#21334;&#21644;&#35838;&#31243;&#20998;&#37197;&#12290;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#38543;&#30528;&#29289;&#21697;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#25414;&#32465;&#31354;&#38388;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#26377;&#20960;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20559;&#22909;&#24341;&#23548;&#31639;&#27861;&#65292;&#26088;&#22312;&#20174;&#20195;&#29702;&#20013;&#20165;&#24341;&#23548;&#20986;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20808;&#21069;&#24037;&#20316;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#27809;&#26377;&#23545;&#23578;&#26410;&#24341;&#23548;&#20986;&#30340;&#25414;&#32465;&#20540;&#30340;&#26426;&#21046;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32452;&#21512;&#20998;&#37197;&#65288;BOCA&#65289;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#23558;&#25429;&#33719;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#26426;&#21046;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#21487;&#29992;&#20110;&#23450;&#20041;&#33719;&#21462;&#20989;&#25968;&#20197;&#30830;&#23450;&#19979;&#19968;&#20010;&#26597;&#35810;&#30340;&#19978;&#38480;&#19981;&#30830;&#23450;&#24615;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#26426;&#21046;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
We study the combinatorial assignment domain, which includes combinatorial auctions and course allocation. The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning-based preference elicitation algorithms that aim to elicit only the most important information from agents. However, the main shortcoming of this prior work is that it does not model a mechanism's uncertainty over values for not yet elicited bundles. In this paper, we address this shortcoming by presenting a Bayesian optimization-based combinatorial assignment (BOCA) mechanism. Our key technical contribution is to integrate a method for capturing model uncertainty into an iterative combinatorial auction mechanism. Concretely, we design a new method for estimating an upper uncertainty bound that can be used to define an acquisition function to determine the next query to the agents. This enables the mechanism 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#32954;&#27963;&#26816;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#23545;EGFR&#31361;&#21464;&#30340;&#39044;&#27979;&#65292;&#20026;&#32954;&#30284;&#27835;&#30103;&#25552;&#20379;&#20102;&#26356;&#32463;&#27982;&#12289;&#26356;&#24555;&#25463;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.12506</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#32954;&#27963;&#26816;&#22270;&#20687;&#20013;&#30340;EGFR&#31361;&#21464;
&lt;/p&gt;
&lt;p&gt;
EGFR Mutation Prediction of Lung Biopsy Images using Deep Learning. (arXiv:2208.12506v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#32954;&#27963;&#26816;&#22270;&#20687;&#36827;&#34892;&#20998;&#26512;&#65292;&#23454;&#29616;&#20102;&#23545;EGFR&#31361;&#21464;&#30340;&#39044;&#27979;&#65292;&#20026;&#32954;&#30284;&#27835;&#30103;&#25552;&#20379;&#20102;&#26356;&#32463;&#27982;&#12289;&#26356;&#24555;&#25463;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper uses deep learning technology to predict EGFR mutations through analysis of lung biopsy images, providing a more economical and faster diagnostic method for lung cancer treatment.
&lt;/p&gt;
&lt;p&gt;
&#32954;&#30284;&#27835;&#30103;&#30340;&#26631;&#20934;&#35786;&#26029;&#31243;&#24207;&#28041;&#21450;&#32452;&#32455;&#23398;&#20122;&#22411;&#20998;&#22411;&#21644;&#38543;&#21518;&#30340;&#20851;&#38190;&#39537;&#21160;&#22522;&#22240;&#31361;&#21464;&#26816;&#27979;&#65292;&#22914;EGFR&#12290;&#23613;&#31649;&#20998;&#23376;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;&#39537;&#21160;&#22522;&#22240;&#31361;&#21464;&#65292;&#20294;&#35813;&#36807;&#31243;&#36890;&#24120;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#32463;&#27982;&#30340;&#36873;&#25321;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#20013;&#21457;&#29616;&#39537;&#21160;&#22522;&#22240;&#31361;&#21464;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#31649;&#36947;&#36827;&#34892;&#24369;&#30417;&#30563;&#65292;&#20197;&#35782;&#21035;hematoxylin&#21644;eosin&#26579;&#33394;&#30340;WSIs&#20013;EGFR&#31361;&#21464;&#30340;&#24418;&#24577;&#23398;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#26816;&#27979;&#32959;&#30244;&#21644;&#32452;&#32455;&#23398;&#20122;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#32954;&#30284;&#25968;&#25454;&#38598;&#65288;TCGA&#21644;&#26469;&#33258;&#21360;&#24230;&#30340;&#31169;&#20154;&#25968;&#25454;&#38598;&#65289;&#19978;&#36827;&#34892;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#26469;&#35777;&#26126;&#25105;&#20204;&#31649;&#36947;&#30340;&#26377;&#25928;&#24615;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31649;&#36947;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#32959;&#30244;&#26816;&#27979;&#30340;&#24179;&#22343;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20026;0.964&#65292;&#32452;&#32455;&#23398;&#20122;&#22411;&#20026;0.942&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard diagnostic procedures for targeted therapies in lung cancer treatment involve histological subtyping and subsequent detection of key driver mutations, such as EGFR. Even though molecular profiling can uncover the driver mutation, the process is often expensive and time-consuming. Deep learning-oriented image analysis offers a more economical alternative for discovering driver mutations directly from whole slide images (WSIs). In this work, we used customized deep learning pipelines with weak supervision to identify the morphological correlates of EGFR mutation from hematoxylin and eosin-stained WSIs, in addition to detecting tumor and histologically subtyping it. We demonstrate the effectiveness of our pipeline by conducting rigorous experiments and ablation studies on two lung cancer datasets - TCGA and a private dataset from India. With our pipeline, we achieved an average area under the curve (AUC) of 0.964 for tumor detection, and 0.942 for histological subtyping betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#35745;&#31639;&#37197;&#20998;&#20989;&#25968;&#30340;&#28508;&#21147;&#20272;&#35745;&#26041;&#27861;&#65292;&#22522;&#20110;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#65288;MAP&#65289;&#20272;&#35745;&#22120;&#65292;&#23558;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20316;&#29992;&#37327;&#31867;&#22411;&#30340;&#21183;&#20989;&#25968;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24555;&#36895;&#23558;&#20248;&#21270;&#38382;&#39064;&#35299;&#20915;&#20026;&#21069;&#39304;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2208.09433</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#35745;&#31639;&#37197;&#20998;&#20989;&#25968;&#30340;&#28508;&#21147;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Estimating a potential without the agony of the partition function. (arXiv:2208.09433v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#35745;&#31639;&#37197;&#20998;&#20989;&#25968;&#30340;&#28508;&#21147;&#20272;&#35745;&#26041;&#27861;&#65292;&#22522;&#20110;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#65288;MAP&#65289;&#20272;&#35745;&#22120;&#65292;&#23558;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20316;&#29992;&#37327;&#31867;&#22411;&#30340;&#21183;&#20989;&#25968;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24555;&#36895;&#23558;&#20248;&#21270;&#38382;&#39064;&#35299;&#20915;&#20026;&#21069;&#39304;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a potential estimation method that does not require the computation of the partition function, based on Maximum A-Posteriori (MAP) estimators, reformulating the problem as an optimization problem, and proposing a least-action type potential that allows for quick solution as a feed-forward hyperbolic neural network.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#32479;&#35745;&#21644;&#32479;&#35745;&#23398;&#20064;&#20013;&#65292;&#32473;&#23450;&#26679;&#26412;&#20272;&#35745;Gibbs&#23494;&#24230;&#20989;&#25968;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#26368;&#22823;&#20284;&#28982;&#26041;&#27861;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23427;&#38656;&#35201;&#35745;&#31639;&#37197;&#20998;&#20989;&#25968;&#65288;&#21363;&#23494;&#24230;&#30340;&#24402;&#19968;&#21270;&#65289;&#12290;&#23545;&#20110;&#31616;&#21333;&#30340;&#20302;&#32500;&#38382;&#39064;&#65292;&#21487;&#20197;&#36731;&#26494;&#35745;&#31639;&#35813;&#20989;&#25968;&#65292;&#20294;&#23545;&#20110;&#19968;&#33324;&#23494;&#24230;&#21644;&#39640;&#32500;&#38382;&#39064;&#65292;&#20854;&#35745;&#31639;&#26159;&#22256;&#38590;&#29978;&#33267;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#65288;MAP&#65289;&#20272;&#35745;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#26368;&#22823;&#24674;&#22797;MAP&#65288;MR-MAP&#65289;&#65292;&#20197;&#23548;&#20986;&#19981;&#38656;&#35201;&#35745;&#31639;&#37197;&#20998;&#20989;&#25968;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#23558;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#20316;&#29992;&#37327;&#31867;&#22411;&#30340;&#21183;&#20989;&#25968;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#24555;&#36895;&#23558;&#20248;&#21270;&#38382;&#39064;&#35299;&#20915;&#20026;&#21069;&#39304;&#21452;&#26354;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312;&#19968;&#20123;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating a Gibbs density function given a sample is an important problem in computational statistics and statistical learning. Although the well established maximum likelihood method is commonly used, it requires the computation of the partition function (i.e., the normalization of the density).  This function can be easily calculated for simple low-dimensional problems but its computation is difficult or even intractable for general densities and high-dimensional problems. In this paper we propose an alternative approach based on Maximum A-Posteriori (MAP) estimators, we name Maximum Recovery MAP (MR-MAP), to derive estimators that do not require the computation of the partition function, and reformulate the problem as an optimization problem. We further propose a least-action type potential that allows us to quickly solve the optimization problem as a feed-forward hyperbolic neural network. We demonstrate the effectiveness of our methods on some standard data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#38024;&#23545;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#39046;&#22495;&#28418;&#31227;&#30340;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65292;&#26080;&#38656;&#26631;&#31614;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;CoTTA&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.08767</link><description>&lt;p&gt;
&#35780;&#20272;&#38024;&#23545;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#39046;&#22495;&#28418;&#31227;&#30340;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Continual Test-Time Adaptation for Contextual and Semantic Domain Shifts. (arXiv:2208.08767v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#38024;&#23545;&#19978;&#19979;&#25991;&#21644;&#35821;&#20041;&#39046;&#22495;&#28418;&#31227;&#30340;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65292;&#26080;&#38656;&#26631;&#31614;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;CoTTA&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper evaluates continual test-time adaptation for contextual and semantic domain shifts without labels. The study finds that Continual Test-Time Adaptation (CoTTA) is an effective method.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#27979;&#35797;&#26102;&#38388;&#20869;&#36830;&#32493;&#36866;&#24212;&#39046;&#22495;&#28418;&#31227;&#65292;&#26080;&#38656;&#26631;&#31614;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#22914;&#39044;&#27979;&#26102;&#38388;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#12289;&#27979;&#35797;&#29109;&#26368;&#23567;&#21270;&#65288;TENT&#65289;&#21644;&#36830;&#32493;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#24615;&#65288;CoTTA&#65289;&#65292;&#24182;&#22312;&#20004;&#20010;&#29616;&#23454;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#28418;&#31227;&#28304;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, our goal is to adapt a pre-trained convolutional neural network to domain shifts at test time. We do so continually with the incoming stream of test batches, without labels. The existing literature mostly operates on artificial shifts obtained via adversarial perturbations of a test image. Motivated by this, we evaluate the state of the art on two realistic and challenging sources of domain shifts, namely contextual and semantic shifts. Contextual shifts correspond to the environment types, for example, a model pre-trained on indoor context has to adapt to the outdoor context on CORe-50. Semantic shifts correspond to the capture types, for example a model pre-trained on natural images has to adapt to cliparts, sketches, and paintings on DomainNet. We include in our analysis recent techniques such as Prediction-Time Batch Normalization (BN), Test Entropy Minimization (TENT) and Continual Test-Time Adaptation (CoTTA). Our findings are three-fold: i) Test-time adaptation me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;Uconv-Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#23558;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;16&#20493;&#65292;&#21152;&#36895;&#20013;&#38388;&#23618;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#19978;&#37319;&#26679;&#22359;&#35299;&#20915;&#20102;&#25910;&#25947;&#38382;&#39064;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;WER&#21644;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2208.07657</link><description>&lt;p&gt;
Uconv-Conformer: &#38024;&#23545;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#30340;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#22823;&#24133;&#32553;&#20943;&#30340;&#26032;&#22411;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Uconv-Conformer: High Reduction of Input Sequence Length for End-to-End Speech Recognition. (arXiv:2208.07657v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;Uconv-Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#23558;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32553;&#30701;16&#20493;&#65292;&#21152;&#36895;&#20013;&#38388;&#23618;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#19978;&#37319;&#26679;&#22359;&#35299;&#20915;&#20102;&#25910;&#25947;&#38382;&#39064;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;WER&#21644;&#26356;&#24555;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a new Uconv-Conformer architecture that reduces the input sequence length by 16 times, speeds up the work of intermediate layers, and solves the convergence issue by using upsampling blocks. The Uconv-Conformer architecture shows better WER and faster training and inference speed.
&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#29616;&#20195;ASR&#26550;&#26500;&#26159;&#26368;&#39640;&#20248;&#20808;&#32423;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#33410;&#30465;&#27169;&#22411;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#35768;&#22810;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;Conformer&#27169;&#22411;&#30340;&#26032;&#22411;Uconv-Conformer&#26550;&#26500;&#12290;&#23427;&#36890;&#36807;16&#20493;&#30340;&#19968;&#33268;&#24615;&#32553;&#30701;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#20013;&#38388;&#23618;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#19982;&#26102;&#38388;&#32500;&#24230;&#22823;&#24133;&#32553;&#20943;&#30456;&#20851;&#30340;&#25910;&#25947;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20687;U-Net&#26550;&#26500;&#20013;&#30340;&#19978;&#37319;&#26679;&#22359;&#26469;&#30830;&#20445;&#27491;&#30830;&#30340;CTC&#25439;&#22833;&#35745;&#31639;&#21644;&#31283;&#23450;&#32593;&#32476;&#35757;&#32451;&#12290;Uconv-Conformer&#26550;&#26500;&#19981;&#20165;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#26356;&#24555;&#65292;&#32780;&#19988;&#19982;&#22522;&#32447;Conformer&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;WER&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;Uconv-Conformer&#27169;&#22411;&#22312;CPU&#21644;GPU&#19978;&#20998;&#21035;&#26174;&#31034;&#20986;47.8&#65285;&#21644;23.5&#65285;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;&#30456;&#23545;WER&#30340;&#20943;&#23569;&#20998;&#21035;&#20026;7.3&#65285;&#21644;9.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization of modern ASR architectures is among the highest priority tasks since it saves many computational resources for model training and inference. The work proposes a new Uconv-Conformer architecture based on the standard Conformer model. It consistently reduces the input sequence length by 16 times, which results in speeding up the work of the intermediate layers. To solve the convergence issue connected with such a significant reduction of the time dimension, we use upsampling blocks like in the U-Net architecture to ensure the correct CTC loss calculation and stabilize network training. The Uconv-Conformer architecture appears to be not only faster in terms of training and inference speed but also shows better WER compared to the baseline Conformer. Our best Uconv-Conformer model shows 47.8% and 23.5% inference acceleration on the CPU and GPU, respectively. Relative WER reduction is 7.3% and 9.2% on LibriSpeech test_clean and test_other respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#38477;&#23610;&#24230;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21464;&#37327;&#26102;&#28385;&#36275;&#23432;&#24658;&#23450;&#24459;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.05424</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#27668;&#20505;&#38477;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Physics-Constrained Deep Learning for Climate Downscaling. (arXiv:2208.05424v6 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#38477;&#23610;&#24230;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#20445;&#35777;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21464;&#37327;&#26102;&#28385;&#36275;&#23432;&#24658;&#23450;&#24459;&#65292;&#24182;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for physics-constrained deep learning downscaling models to ensure that the models satisfy conservation laws when predicting physical variables, while improving their performance according to traditional metrics.
&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#39640;&#20998;&#36776;&#29575;&#27668;&#20505;&#21644;&#22825;&#27668;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#23545;&#20110;&#25351;&#23548;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#30340;&#38271;&#26399;&#20915;&#31574;&#20197;&#21450;&#25351;&#23548;&#23545;&#26497;&#31471;&#20107;&#20214;&#30340;&#24555;&#36895;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#39044;&#27979;&#27169;&#22411;&#21463;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#65292;&#22240;&#27492;&#36890;&#24120;&#29983;&#25104;&#31895;&#20998;&#36776;&#29575;&#39044;&#27979;&#12290;&#32479;&#35745;&#38477;&#23610;&#24230;&#65292;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#19978;&#37319;&#26679;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#35270;&#35273;&#19978;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#29289;&#29702;&#21464;&#37327;&#26102;&#32463;&#24120;&#36829;&#21453;&#23432;&#24658;&#23450;&#24459;&#12290;&#20026;&#20102;&#20445;&#25345;&#29289;&#29702;&#37327;&#30340;&#23432;&#24658;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20445;&#35777;&#28145;&#24230;&#23398;&#20064;&#38477;&#23610;&#24230;&#27169;&#22411;&#28385;&#36275;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#65292;&#21516;&#26102;&#26681;&#25454;&#20256;&#32479;&#25351;&#26631;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32422;&#26463;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#20197;&#21450;&#21508;&#31181;&#27668;&#20505;&#21644;&#22825;&#27668;&#25968;&#25454;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of reliable, high-resolution climate and weather data is important to inform long-term decisions on climate adaptation and mitigation and to guide rapid responses to extreme events. Forecasting models are limited by computational costs and, therefore, often generate coarse-resolution predictions. Statistical downscaling, including super-resolution methods from deep learning, can provide an efficient method of upsampling low-resolution data. However, despite achieving visually compelling results in some cases, such models frequently violate conservation laws when predicting physical variables. In order to conserve physical quantities, we develop methods that guarantee physical constraints are satisfied by a deep learning downscaling model while also improving their performance according to traditional metrics. We compare different constraining approaches and demonstrate their applicability across different neural architectures as well as a variety of climate and weather
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#23884;&#20837;&#31163;&#25955;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#32500;&#24230;&#65288;ID&#65289;&#65292;&#24182;&#22312;&#29289;&#31181;&#25351;&#32441;&#30340;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#23567;ID&#65292;&#32422;&#20026;2&#30340;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2207.09688</link><description>&lt;p&gt;
&#31163;&#25955;&#24230;&#37327;&#30340;&#20869;&#22312;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Intrinsic dimension estimation for discrete metrics. (arXiv:2207.09688v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#23884;&#20837;&#31163;&#25955;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#32500;&#24230;&#65288;ID&#65289;&#65292;&#24182;&#22312;&#29289;&#31181;&#25351;&#32441;&#30340;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#65292;&#21457;&#29616;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#23567;ID&#65292;&#32422;&#20026;2&#30340;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an algorithm to estimate the intrinsic dimension (ID) of datasets embedded in discrete spaces, and demonstrates its accuracy on a metagenomic dataset for species fingerprinting, finding a surprisingly small ID of order 2, suggesting that evolutive pressure acts on a low-dimensional manifold despite the high-dimensionality of sequences' space.
&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#31163;&#25955;&#29305;&#24449;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65306;&#20174;&#20998;&#31867;&#35843;&#26597;&#21040;&#20020;&#24202;&#38382;&#21367;&#65292;&#20174;&#26080;&#26435;&#32593;&#32476;&#21040;DNA&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#26368;&#24120;&#35265;&#30340;&#26080;&#30417;&#30563;&#38477;&#32500;&#26041;&#27861;&#26159;&#20026;&#36830;&#32493;&#31354;&#38388;&#35774;&#35745;&#30340;&#65292;&#23427;&#20204;&#22312;&#31163;&#25955;&#31354;&#38388;&#20013;&#30340;&#20351;&#29992;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#21644;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25512;&#26029;&#23884;&#20837;&#31163;&#25955;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#30340;&#20869;&#22312;&#32500;&#24230;&#65288;ID&#65289;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20998;&#26512;&#29992;&#20110;&#29289;&#31181;&#25351;&#32441;&#30340;&#20195;&#35874;&#32452;&#23398;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#23567;ID&#65292;&#32422;&#20026;2&#30340;&#25968;&#37327;&#32423;&#12290;&#36825;&#34920;&#26126;&#65292;&#23613;&#31649;&#24207;&#21015;&#31354;&#38388;&#30340;&#39640;&#32500;&#24230;&#65292;&#36827;&#21270;&#21387;&#21147;&#20173;&#28982;&#20316;&#29992;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real world-datasets characterized by discrete features are ubiquitous: from categorical surveys to clinical questionnaires, from unweighted networks to DNA sequences. Nevertheless, the most common unsupervised dimensional reduction methods are designed for continuous spaces, and their use for discrete spaces can lead to errors and biases. In this letter we introduce an algorithm to infer the intrinsic dimension (ID) of datasets embedded in discrete spaces. We demonstrate its accuracy on benchmark datasets, and we apply it to analyze a metagenomic dataset for species fingerprinting, finding a surprisingly small ID, of order 2. This suggests that evolutive pressure acts on a low-dimensional manifold despite the high-dimensionality of sequences' space.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XG-BoT&#30340;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#30340;&#24694;&#24847;&#20725;&#23608;&#32593;&#32476;&#33410;&#28857;&#65292;&#24182;&#36890;&#36807;&#31361;&#20986;&#26174;&#31034;&#21487;&#30097;&#30340;&#32593;&#32476;&#27969;&#21644;&#30456;&#20851;&#30340;&#20725;&#23608;&#32593;&#32476;&#33410;&#28857;&#26469;&#25191;&#34892;&#33258;&#21160;&#32593;&#32476;&#21462;&#35777;&#12290;&#35813;&#27169;&#22411;&#22312;&#20851;&#38190;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.09088</link><description>&lt;p&gt;
XG-BoT&#65306;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20725;&#23608;&#32593;&#32476;&#26816;&#27979;&#21644;&#21462;&#35777;
&lt;/p&gt;
&lt;p&gt;
XG-BoT: An Explainable Deep Graph Neural Network for Botnet Detection and Forensics. (arXiv:2207.09088v5 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XG-BoT&#30340;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#30340;&#24694;&#24847;&#20725;&#23608;&#32593;&#32476;&#33410;&#28857;&#65292;&#24182;&#36890;&#36807;&#31361;&#20986;&#26174;&#31034;&#21487;&#30097;&#30340;&#32593;&#32476;&#27969;&#21644;&#30456;&#20851;&#30340;&#20725;&#23608;&#32593;&#32476;&#33410;&#28857;&#26469;&#25191;&#34892;&#33258;&#21160;&#32593;&#32476;&#21462;&#35777;&#12290;&#35813;&#27169;&#22411;&#22312;&#20851;&#38190;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an explainable deep graph neural network model called XG-BoT for detecting malicious botnet nodes in large-scale networks and performing automatic network forensics by highlighting suspicious network flows and related botnet nodes. The model outperforms state-of-the-art approaches in terms of key evaluation metrics.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XG-BoT&#30340;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#20725;&#23608;&#32593;&#32476;&#33410;&#28857;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#20725;&#23608;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#19968;&#20010;&#33258;&#21160;&#21462;&#35777;&#30340;&#35299;&#37322;&#22120;&#12290;XG-BoT&#26816;&#27979;&#22120;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#30340;&#24694;&#24847;&#20725;&#23608;&#32593;&#32476;&#33410;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#21033;&#29992;&#20998;&#32452;&#21487;&#36870;&#27531;&#24046;&#36830;&#25509;&#21644;&#22270;&#21516;&#26500;&#32593;&#32476;&#20174;&#20725;&#23608;&#32593;&#32476;&#36890;&#20449;&#22270;&#20013;&#23398;&#20064;&#34920;&#36798;&#24615;&#33410;&#28857;&#34920;&#31034;&#12290;&#22522;&#20110;GNNExplainer&#21644;XG-BoT&#20013;&#30340;&#26174;&#33879;&#24615;&#22270;&#65292;&#35299;&#37322;&#22120;&#21487;&#20197;&#36890;&#36807;&#31361;&#20986;&#26174;&#31034;&#21487;&#30097;&#30340;&#32593;&#32476;&#27969;&#21644;&#30456;&#20851;&#30340;&#20725;&#23608;&#32593;&#32476;&#33410;&#28857;&#26469;&#25191;&#34892;&#33258;&#21160;&#32593;&#32476;&#21462;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#30340;&#22823;&#35268;&#27169;&#20725;&#23608;&#32593;&#32476;&#22270;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;XG-BoT&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;XG-BoT&#22312;&#20851;&#38190;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;XG-BoT&#35299;&#37322;&#22120;&#21487;&#20197;&#20026;&#33258;&#21160;&#32593;&#32476;&#21462;&#35777;&#29983;&#25104;&#26377;&#29992;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose XG-BoT, an explainable deep graph neural network model for botnet node detection. The proposed model comprises a botnet detector and an explainer for automatic forensics. The XG-BoT detector can effectively detect malicious botnet nodes in large-scale networks. Specifically, it utilizes a grouped reversible residual connection with a graph isomorphism network to learn expressive node representations from botnet communication graphs. The explainer, based on the GNNExplainer and saliency map in XG-BoT, can perform automatic network forensics by highlighting suspicious network flows and related botnet nodes. We evaluated XG-BoT using real-world, large-scale botnet network graph datasets. Overall, XG-BoT outperforms state-of-the-art approaches in terms of key evaluation metrics. Additionally, we demonstrate that the XG-BoT explainers can generate useful explanations for automatic network forensics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23481;&#38169;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#26816;&#32034;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;</title><link>http://arxiv.org/abs/2207.04827</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20998;&#31867;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Classification and Generation of real-world data with an Associative Memory Model. (arXiv:2207.04827v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#23481;&#38169;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#26816;&#32034;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a multi-modality framework based on the associative memory model, which can store and retrieve a large amount of real-world data in a fault-tolerant manner, and can be used to infer missing modalities.
&lt;/p&gt;
&lt;p&gt;
&#22238;&#24518;&#36215;&#22810;&#24180;&#26410;&#35265;&#30340;&#26379;&#21451;&#30340;&#38754;&#23380;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#20320;&#20204;&#20598;&#28982;&#30456;&#36935;&#65292;&#20320;&#20204;&#20250;&#36731;&#26131;&#22320;&#35748;&#20986;&#24444;&#27492;&#12290;&#29983;&#29289;&#35760;&#24518;&#37197;&#22791;&#20102;&#19968;&#20010;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#21387;&#32553;&#31639;&#27861;&#65292;&#21487;&#20197;&#23384;&#20648;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#28982;&#21518;&#25512;&#26029;&#32454;&#33410;&#20197;&#21305;&#37197;&#24863;&#30693;&#12290;Willshaw Memory&#26159;&#19968;&#31181;&#29992;&#20110;&#30382;&#23618;&#35745;&#31639;&#30340;&#31616;&#21333;&#25277;&#35937;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#29983;&#29289;&#35760;&#24518;&#30340;&#26426;&#21046;&#12290;&#20351;&#29992;&#25105;&#20204;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#35270;&#35273;&#27169;&#24335;&#30340;&#31232;&#30095;&#32534;&#30721;&#35268;&#21017;[34]&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20197;&#23481;&#38169;&#30340;&#26041;&#24335;&#23384;&#20648;&#21644;&#26816;&#32034;&#22823;&#37327;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#26694;&#26550;&#25193;&#23637;&#20102;&#22522;&#26412;&#32852;&#24819;&#35760;&#24518;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#35760;&#24518;&#21516;&#26102;&#23384;&#20648;&#27599;&#20010;&#27169;&#24335;&#30340;&#20960;&#31181;&#27169;&#24577;&#65288;&#20363;&#22914;&#65292;&#35270;&#35273;&#25110;&#25991;&#26412;&#65289;&#12290;&#35757;&#32451;&#21518;&#65292;&#24403;&#21482;&#24863;&#30693;&#21040;&#23376;&#38598;&#26102;&#65292;&#35760;&#24518;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#20351;&#29992;&#31616;&#21333;&#30340;&#32534;&#30721;&#22120;-&#35760;&#24518;&#35299;&#30721;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drawing from memory the face of a friend you have not seen in years is a difficult task. However, if you happen to cross paths, you would easily recognize each other. The biological memory is equipped with an impressive compression algorithm that can store the essential, and then infer the details to match perception. The Willshaw Memory is a simple abstract model for cortical computations which implements mechanisms of biological memories. Using our recently proposed sparse coding prescription for visual patterns [34], this model can store and retrieve an impressive amount of real-world data in a fault-tolerant manner. In this paper, we extend the capabilities of the basic Associative Memory Model by using a Multiple-Modality framework. In this setting, the memory stores several modalities (e.g., visual, or textual) of each pattern simultaneously. After training, the memory can be used to infer missing modalities when just a subset is perceived. Using a simple encoder-memory decoder a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;MLE&#20844;&#24335;&#24182;&#20174;&#22810;&#20010;&#39057;&#29575;&#30340;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#30456;&#23545;&#30456;&#20301;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#30340;&#32852;&#21512;&#31038;&#21306;&#26816;&#27979;&#21644;&#30456;&#20301;&#21516;&#27493;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.12276</link><description>&lt;p&gt;
&#22810;&#39057;&#32852;&#21512;&#31038;&#21306;&#26816;&#27979;&#21644;&#30456;&#20301;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Multi-Frequency Joint Community Detection and Phase Synchronization. (arXiv:2206.12276v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.12276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;MLE&#20844;&#24335;&#24182;&#20174;&#22810;&#20010;&#39057;&#29575;&#30340;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#30456;&#23545;&#30456;&#20301;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#30340;&#32852;&#21512;&#31038;&#21306;&#26816;&#27979;&#21644;&#30456;&#20301;&#21516;&#27493;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes two simple and efficient algorithms that leverage the MLE formulation and benefit from the information across multiple frequencies to solve the joint community detection and phase synchronization problem on the stochastic block model with relative phase.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#30456;&#23545;&#30456;&#20301;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#19978;&#30340;&#32852;&#21512;&#31038;&#21306;&#26816;&#27979;&#21644;&#30456;&#20301;&#21516;&#27493;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#33410;&#28857;&#37117;&#19982;&#19968;&#20010;&#26410;&#30693;&#30340;&#30456;&#20301;&#35282;&#30456;&#20851;&#32852;&#12290;&#36825;&#20010;&#38382;&#39064;&#20855;&#26377;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#65292;&#26088;&#22312;&#21516;&#26102;&#24674;&#22797;&#31751;&#32467;&#26500;&#21644;&#30456;&#20851;&#30340;&#30456;&#20301;&#35282;&#12290;&#25105;&#20204;&#36890;&#36807;&#20180;&#32454;&#30740;&#31350;&#20854;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#20844;&#24335;&#65292;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#21576;&#29616;&#20986;&#8220;&#22810;&#39057;&#8221;&#32467;&#26500;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#24182;&#38750;&#28304;&#20110;&#36825;&#20010;&#35282;&#24230;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;MLE&#20844;&#24335;&#24182;&#20174;&#22810;&#20010;&#39057;&#29575;&#30340;&#20449;&#24687;&#20013;&#21463;&#30410;&#12290;&#21069;&#32773;&#26159;&#22522;&#20110;&#26032;&#39062;&#30340;&#22810;&#39057;&#21015;&#20027;&#20803;QR&#20998;&#35299;&#30340;&#35889;&#26041;&#27861;&#12290;&#24212;&#29992;&#20110;&#35266;&#27979;&#30697;&#38453;&#30340;&#21069;&#20960;&#20010;&#29305;&#24449;&#21521;&#37327;&#30340;&#20998;&#35299;&#25552;&#20379;&#20102;&#26377;&#20851;&#31751;&#32467;&#26500;&#21644;&#30456;&#20851;&#30456;&#20301;&#35282;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#26159;&#36845;&#20195;&#30340;&#22810;&#39057;&#29575;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the joint community detection and phase synchronization problem on the stochastic block model with relative phase, where each node is associated with an unknown phase angle. This problem, with a variety of real-world applications, aims to recover the cluster structure and associated phase angles simultaneously. We show this problem exhibits a ``multi-frequency'' structure by closely examining its maximum likelihood estimation (MLE) formulation, whereas existing methods are not originated from this perspective. To this end, two simple yet efficient algorithms that leverage the MLE formulation and benefit from the information across multiple frequencies are proposed. The former is a spectral method based on the novel multi-frequency column-pivoted QR factorization. The factorization applied to the top eigenvectors of the observation matrix provides key information about the cluster structure and associated phase angles. The second approach is an iterative multi-frequen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;UC&#22833;&#36133;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#21487;&#20197;&#23454;&#29616;&#20960;&#20046;&#27809;&#26377;&#27979;&#35797;&#25439;&#22833;&#30340;&#27867;&#21270;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2206.07892</link><description>&lt;p&gt;
&#26368;&#22823;&#38388;&#38548;&#26377;&#25928;&#32780;&#22823;&#38388;&#38548;&#26080;&#25928;&#65306;&#26080;&#38656;&#22343;&#21248;&#25910;&#25947;&#30340;&#27867;&#21270;&#12290; (arXiv:2206.07892v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Max-Margin Works while Large Margin Fails: Generalization without Uniform Convergence. (arXiv:2206.07892v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;UC&#22833;&#36133;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#21487;&#20197;&#23454;&#29616;&#20960;&#20046;&#27809;&#26377;&#27979;&#35797;&#25439;&#22833;&#30340;&#27867;&#21270;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proves that in cases where uniform convergence fails, the max-margin classifier can achieve almost no test loss in generalization, providing new generalization bounds.
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#29702;&#35770;&#19978;&#29702;&#35299;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#35768;&#22810;&#29616;&#26377;&#24037;&#20855;&#20381;&#36182;&#20110;&#22343;&#21248;&#25910;&#25947;&#65288;UC&#65289;&#65292;&#24403;&#23427;&#25104;&#31435;&#26102;&#65292;&#20445;&#35777;&#27979;&#35797;&#25439;&#22833;&#22312;&#20505;&#36873;&#27169;&#22411;&#31867;&#19978;&#22343;&#21248;&#25509;&#36817;&#20110;&#35757;&#32451;&#25439;&#22833;&#12290;Nagarajan&#21644;Kolter&#65288;2019&#65289;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#31616;&#21333;&#30340;&#32447;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#32622;&#20013;&#65292;&#20219;&#20309;&#22343;&#21248;&#25910;&#25947;&#30028;&#38480;&#37117;&#23558;&#26159;&#26080;&#24847;&#20041;&#30340;&#65292;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;UC&#22833;&#36133;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22312;&#20004;&#20010;&#36825;&#26679;&#30340;&#35774;&#32622;&#20013;&#35777;&#26126;&#20102;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#19968;&#20010;&#26159;&#32447;&#24615;&#30340;&#65292;&#19968;&#20010;&#26159;&#38750;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;Nagarajan&#21644;Kolter&#30340;&#32447;&#24615;&#20998;&#31867;&#35774;&#32622;&#65292;&#20197;&#21450;&#36890;&#36807;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#20108;&#27425;&#22522;&#26412;&#20107;&#23454;&#20989;&#25968;&#22312;&#38750;&#32447;&#24615;&#21306;&#22495;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36793;&#30028;&#65292;&#34920;&#26126;&#22312;&#19968;&#23450;&#30340;&#20449;&#22122;&#27604;&#38408;&#20540;&#20197;&#19978;&#65292;&#20219;&#20309;&#25509;&#36817;&#26368;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#37117;&#23558;&#22312;&#27979;&#35797;&#25439;&#22833;&#19978;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in modern machine learning is theoretically understanding the generalization properties of overparameterized models. Many existing tools rely on uniform convergence (UC), a property that, when it holds, guarantees that the test loss will be close to the training loss, uniformly over a class of candidate models. Nagarajan and Kolter (2019) show that in certain simple linear and neural-network settings, any uniform convergence bound will be vacuous, leaving open the question of how to prove generalization in settings where UC fails. Our main contribution is proving novel generalization bounds in two such settings, one linear, and one non-linear. We study the linear classification setting of Nagarajan and Kolter, and a quadratic ground truth function learned via a two-layer neural network in the non-linear regime. We prove a new type of margin bound showing that above a certain signal-to-noise threshold, any near-max-margin classifier will achieve almost no test loss in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#22522;&#20110;&#23545;&#31216;&#20915;&#31574;&#26641;&#30340;&#26799;&#24230;&#25552;&#21319;&#21487;&#20197;&#31561;&#20215;&#22320;&#37325;&#26500;&#20026;&#19968;&#31181;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25910;&#25947;&#20110;&#26576;&#20010;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#30340;&#35299;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#22320;&#23558;&#26799;&#24230;&#25552;&#21319;&#36716;&#25442;&#20026;&#20174;&#21518;&#39564;&#20013;&#25552;&#20379;&#26356;&#22909;&#30340;&#30693;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#21518;&#39564;&#26041;&#24046;&#30340;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#65292;&#20174;&#32780;&#20801;&#35768;&#26356;&#22909;&#30340;&#30693;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#22495;&#22806;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2206.05608</link><description>&lt;p&gt;
&#26799;&#24230;&#25552;&#21319;&#25191;&#34892;&#39640;&#26031;&#36807;&#31243;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Gradient Boosting Performs Gaussian Process Inference. (arXiv:2206.05608v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#22522;&#20110;&#23545;&#31216;&#20915;&#31574;&#26641;&#30340;&#26799;&#24230;&#25552;&#21319;&#21487;&#20197;&#31561;&#20215;&#22320;&#37325;&#26500;&#20026;&#19968;&#31181;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25910;&#25947;&#20110;&#26576;&#20010;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#30340;&#35299;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#22320;&#23558;&#26799;&#24230;&#25552;&#21319;&#36716;&#25442;&#20026;&#20174;&#21518;&#39564;&#20013;&#25552;&#20379;&#26356;&#22909;&#30340;&#30693;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#21518;&#39564;&#26041;&#24046;&#30340;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#65292;&#20174;&#32780;&#20801;&#35768;&#26356;&#22909;&#30340;&#30693;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23548;&#33268;&#25913;&#36827;&#30340;&#22495;&#22806;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper shows that gradient boosting based on symmetric decision trees can be equivalently reformulated as a kernel method that converges to the solution of a certain Kernel Ridge Regression problem, which allows us to easily transform gradient boosting into a sampler from the posterior to provide better knowledge uncertainty estimates through Monte-Carlo estimation of the posterior variance, leading to improved out-of-domain detection.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#22522;&#20110;&#23545;&#31216;&#20915;&#31574;&#26641;&#30340;&#26799;&#24230;&#25552;&#21319;&#21487;&#20197;&#31561;&#20215;&#22320;&#37325;&#26500;&#20026;&#19968;&#31181;&#26680;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25910;&#25947;&#20110;&#26576;&#20010;&#26680;&#23725;&#22238;&#24402;&#38382;&#39064;&#30340;&#35299;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#25910;&#25947;&#20110;&#39640;&#26031;&#36807;&#31243;&#21518;&#39564;&#22343;&#20540;&#30340;&#25910;&#25947;&#24615;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#36731;&#26494;&#22320;&#23558;&#26799;&#24230;&#25552;&#21319;&#36716;&#25442;&#20026;&#20174;&#21518;&#39564;&#20013;&#25552;&#20379;&#26356;&#22909;&#30340;&#30693;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#21518;&#39564;&#26041;&#24046;&#30340;&#33945;&#29305;&#21345;&#32599;&#20272;&#35745;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#37319;&#26679;&#22120;&#20801;&#35768;&#26356;&#22909;&#30340;&#30693;&#35782;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#23548;&#33268;&#25913;&#36827;&#30340;&#22495;&#22806;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper shows that gradient boosting based on symmetric decision trees can be equivalently reformulated as a kernel method that converges to the solution of a certain Kernel Ridge Regression problem. Thus, we obtain the convergence to a Gaussian Process' posterior mean, which, in turn, allows us to easily transform gradient boosting into a sampler from the posterior to provide better knowledge uncertainty estimates through Monte-Carlo estimation of the posterior variance. We show that the proposed sampler allows for better knowledge uncertainty estimates leading to improved out-of-domain detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24341;&#23548;&#21551;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#26631;&#35760;&#36127;&#26679;&#26412;&#21644;&#25429;&#33719;&#26356;&#22810;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.02307</link><description>&lt;p&gt;
&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24341;&#23548;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping Semi-supervised Medical Image Segmentation with Anatomical-aware Contrastive Distillation. (arXiv:2206.02307v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#30340;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24341;&#23548;&#21551;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#36719;&#26631;&#35760;&#36127;&#26679;&#26412;&#21644;&#25429;&#33719;&#26356;&#22810;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29305;&#24449;&#26469;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a semi-supervised medical image segmentation bootstrapping method based on anatomical-aware contrastive distillation, which solves the problem of imbalanced medical image data by softly labeling negative samples and capturing more semantically similar features.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#27880;&#37322;&#31232;&#32570;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#21307;&#23398;&#22270;&#20687;&#20855;&#26377;&#24179;&#34913;&#30340;&#31867;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#30340;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#24179;&#34913;&#30340;&#65288;&#21363;&#22810;&#31867;&#26631;&#31614;&#19981;&#24179;&#34913;&#65289;&#65292;&#36825;&#33258;&#28982;&#22320;&#20135;&#29983;&#27169;&#31946;&#30340;&#36718;&#24275;&#24182;&#36890;&#24120;&#38169;&#35823;&#22320;&#26631;&#35760;&#32597;&#35265;&#30340;&#23545;&#35937;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#36127;&#26679;&#26412;&#26159;&#21542;&#21516;&#26679;&#36127;&#38754;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ACTION&#65292;&#19968;&#31181;&#35299;&#21078;&#24863;&#30693;&#23545;&#27604;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#36719;&#26631;&#35760;&#36127;&#26679;&#26412;&#32780;&#19981;&#26159;&#27491;&#36127;&#23545;&#20043;&#38388;&#30340;&#20108;&#20803;&#30417;&#30563;&#26469;&#24320;&#21457;&#36845;&#20195;&#23545;&#27604;&#33976;&#39311;&#31639;&#27861;&#12290;&#19982;&#27491;&#26679;&#26412;&#30456;&#27604;&#65292;&#25105;&#20204;&#36824;&#20174;&#38543;&#26426;&#36873;&#25321;&#30340;&#36127;&#26679;&#26412;&#38598;&#20013;&#25429;&#33719;&#26356;&#22810;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#29305;&#24449;&#65292;&#20197;&#24378;&#21046;&#25191;&#34892;&#37319;&#26679;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#21078;&#24863;&#30693;&#30340;&#21551;&#21160;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has shown great promise over annotation scarcity problems in the context of medical image segmentation. Existing approaches typically assume a balanced class distribution for both labeled and unlabeled medical images. However, medical image data in reality is commonly imbalanced (i.e., multi-class label imbalance), which naturally yields blurry contours and usually incorrectly labels rare objects. Moreover, it remains unclear whether all negative samples are equally negative. In this work, we present ACTION, an Anatomical-aware ConTrastive dIstillatiON framework, for semi-supervised medical image segmentation. Specifically, we first develop an iterative contrastive distillation algorithm by softly labeling the negatives rather than binary supervision between positive and negative pairs. We also capture more semantically similar features from the randomly chosen negative set compared to the positives to enforce the diversity of the sampled data. Second, we raise a m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;&#26799;&#24230;&#24456;&#26114;&#36149;&#25110;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#20123;&#36741;&#21161;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#21463;&#30410;&#20110;&#30446;&#26631;&#21644;&#36741;&#21161;&#20449;&#24687;&#20043;&#38388;&#30340;Hessian&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2206.00395</link><description>&lt;p&gt;
&#20855;&#22791;&#36741;&#21161;&#20449;&#24687;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimization with access to auxiliary information. (arXiv:2206.00395v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;&#26799;&#24230;&#24456;&#26114;&#36149;&#25110;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#20123;&#36741;&#21161;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#21463;&#30410;&#20110;&#30446;&#26631;&#21644;&#36741;&#21161;&#20449;&#24687;&#20043;&#38388;&#30340;Hessian&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the fundamental optimization question of minimizing a target function with expensive or limited gradient computation, given access to some auxiliary side function with cheaper or more available gradients. The authors propose two generic new algorithms and prove that this framework can benefit from the Hessian similarity assumption between the target and side information.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#26412;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#22312;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;$f(x)$&#30340;&#26799;&#24230;&#24456;&#26114;&#36149;&#25110;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#20123;&#36741;&#21161;&#20989;&#25968;$h(x)$&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#36825;&#20010;&#20844;&#24335;&#28085;&#30422;&#20102;&#35768;&#22810;&#23454;&#38469;&#30456;&#20851;&#30340;&#35774;&#32622;&#65292;&#22914;i&#65289;&#22312;SGD&#20013;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#65292;ii&#65289;&#36801;&#31227;&#23398;&#20064;&#65292;iii&#65289;&#32852;&#37030;&#23398;&#20064;&#65292;iv&#65289;&#20351;&#29992;&#21387;&#32553;&#27169;&#22411;/&#20002;&#24323;&#31561;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#36825;&#20123;&#35774;&#32622;&#65292;&#24182;&#35777;&#26126;&#20165;&#20351;&#29992;&#30446;&#26631;&#21644;&#36741;&#21161;&#20449;&#24687;&#20043;&#38388;&#30340;Hessian&#30456;&#20284;&#24615;&#20551;&#35774;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#36825;&#20010;&#26694;&#26550;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the fundamental optimization question of minimizing a target function $f(x)$ whose gradients are expensive to compute or have limited availability, given access to some auxiliary side function $h(x)$ whose gradients are cheap or more available. This formulation captures many settings of practical relevance such as i) re-using batches in SGD, ii) transfer learning, iii) federated learning, iv) training with compressed models/dropout, etc. We propose two generic new algorithms which are applicable in all these settings and prove using only an assumption on the Hessian similarity between the target and side information that we can benefit from this framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#29420;&#31435;&#22320;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#35775;&#38382;&#20998;&#24067;&#30340;&#30456;&#24212;&#20998;&#37327;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#29983;&#25104;&#30446;&#26631;&#20998;&#24067;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2206.00233</link><description>&lt;p&gt;
DM$^2$: &#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DM$^2$: Decentralized Multi-Agent Reinforcement Learning for Distribution Matching. (arXiv:2206.00233v3 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#29420;&#31435;&#22320;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#35775;&#38382;&#20998;&#24067;&#30340;&#30456;&#24212;&#20998;&#37327;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#29983;&#25104;&#30446;&#26631;&#20998;&#24067;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a decentralized multi-agent reinforcement learning method based on distribution matching, where each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution, achieving convergence to the joint policy that generated the target distribution.
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#26426;&#21046;&#25110;&#26174;&#24335;&#36890;&#20449;&#21327;&#35758;&#20197;&#30830;&#20445;&#25910;&#25947;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#21305;&#37197;&#22312;&#19981;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#32452;&#20214;&#25110;&#26174;&#24335;&#36890;&#20449;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#20013;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#29420;&#31435;&#22320;&#26368;&#23567;&#21270;&#19982;&#30446;&#26631;&#35775;&#38382;&#20998;&#24067;&#30340;&#30456;&#24212;&#20998;&#37327;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#26368;&#23567;&#21270;&#20854;&#20010;&#20307;&#20998;&#24067;&#19981;&#21305;&#37197;&#21487;&#20197;&#23454;&#29616;&#25910;&#25947;&#21040;&#29983;&#25104;&#30446;&#26631;&#20998;&#24067;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#22914;&#26524;&#30446;&#26631;&#20998;&#24067;&#26469;&#33258;&#20248;&#21270;&#21512;&#20316;&#20219;&#21153;&#30340;&#32852;&#21512;&#31574;&#30053;&#65292;&#21017;&#35813;&#20219;&#21153;&#22870;&#21169;&#21644;&#20998;&#24067;&#21305;&#37197;&#22870;&#21169;&#30340;&#32452;&#21512;&#30340;&#26368;&#20248;&#31574;&#30053;&#26159;&#30456;&#21516;&#30340;&#32852;&#21512;&#31574;&#30053;&#12290;&#36825;&#19968;&#35265;&#35299;&#34987;&#29992;&#26469;&#21046;&#23450;&#19968;&#20010;&#23454;&#29992;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches to multi-agent cooperation rely heavily on centralized mechanisms or explicit communication protocols to ensure convergence. This paper studies the problem of distributed multi-agent learning without resorting to centralized components or explicit communication. It examines the use of distribution matching to facilitate the coordination of independent agents. In the proposed scheme, each agent independently minimizes the distribution mismatch to the corresponding component of a target visitation distribution. The theoretical analysis shows that under certain conditions, each agent minimizing its individual distribution mismatch allows the convergence to the joint policy that generated the target distribution. Further, if the target distribution is from a joint policy that optimizes a cooperative task, the optimal policy for a combination of this task reward and the distribution matching reward is the same joint policy. This insight is used to formulate a practical al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#25968;&#31383;&#21475;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#22312;&#32447;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#25968;&#25454;&#27969;&#20013;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2205.12706</link><description>&lt;p&gt;
&#22522;&#20110;&#25351;&#25968;&#31383;&#21475;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#22312;&#32447;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Maximum Mean Discrepancy on Exponential Windows for Online Change Detection. (arXiv:2205.12706v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#25968;&#31383;&#21475;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#22312;&#32447;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#25968;&#25454;&#27969;&#20013;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Maximum Mean Discrepancy on Exponential Windows (MMDEW) algorithm for online change detection, which efficiently detects changes in data streams.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#26512;&#25968;&#25454;&#27969;&#26102;&#65292;&#26816;&#27979;&#21464;&#21270;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20363;&#22914;&#39044;&#27979;&#24615;&#32500;&#25252;&#12289;&#27450;&#35784;&#26816;&#27979;&#25110;&#21307;&#23398;&#12290;&#19968;&#31181;&#26816;&#27979;&#21464;&#21270;&#30340;&#21407;&#21017;&#26041;&#27861;&#26159;&#36890;&#36807;&#20551;&#35774;&#26816;&#39564;&#23558;&#27969;&#20013;&#35266;&#27979;&#20540;&#30340;&#20998;&#24067;&#30456;&#20114;&#27604;&#36739;&#12290;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65307;&#20063;&#31216;&#20026;&#33021;&#37327;&#36317;&#31163;&#65289;&#26159;&#27010;&#29575;&#20998;&#24067;&#31354;&#38388;&#19978;&#20247;&#25152;&#21608;&#30693;&#30340;&#65288;&#21322;&#65289;&#24230;&#37327;&#12290;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;MMD&#22312;&#26680;&#23500;&#38598;&#22495;&#19978;&#20135;&#29983;&#20102;&#24378;&#22823;&#30340;&#38750;&#21442;&#25968;&#20004;&#26679;&#26412;&#26816;&#39564;&#65292;&#36825;&#20351;&#24471;&#23427;&#22312;&#21464;&#21270;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#21464;&#24471;&#21487;&#21462;&#12290;&#28982;&#32780;&#65292;&#32463;&#20856;&#30340;MMD&#20272;&#35745;&#22120;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#36825;&#31105;&#27490;&#20102;&#23427;&#20204;&#22312;&#22312;&#32447;&#21464;&#21270;&#26816;&#27979;&#35774;&#32622;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21464;&#21270;&#26816;&#27979;&#31639;&#27861;&#65292;&#22522;&#20110;&#25351;&#25968;&#31383;&#21475;&#30340;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMDEW&#65289;&#65292;&#23427;&#21033;&#29992;MMD&#20004;&#26679;&#26412;&#26816;&#39564;&#65292;&#22312;&#20219;&#20309;&#26680;&#23500;&#38598;&#22495;&#19978;&#20419;&#36827;&#20854;&#26377;&#25928;&#30340;&#22312;&#32447;&#35745;&#31639;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#21040;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting changes is of fundamental importance when analyzing data streams and has many applications, e.g., predictive maintenance, fraud detection, or medicine. A principled approach to detect changes is to compare the distributions of observations within the stream to each other via hypothesis testing. Maximum mean discrepancy (MMD; also called energy distance) is a well-known (semi-)metric on the space of probability distributions. MMD gives rise to powerful non-parametric two-sample tests on kernel-enriched domains under mild conditions, which makes its deployment for change detection desirable. However, the classic MMD estimators suffer quadratic complexity, which prohibits their application in the online change detection setting. We propose a general-purpose change detection algorithm, Maximum Mean Discrepancy on Exponential Windows (MMDEW), which leverages the MMD two-sample test, facilitates its efficient online computation on any kernel-enriched domain, and is able to detect a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25239;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#21644;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#24179;&#28369;&#30340;&#65288;&#38750;&#65289;&#26465;&#20214;&#23494;&#24230;&#65292;&#24182;&#20801;&#35768;&#23436;&#20840;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#34920;&#26684;&#25968;&#25454;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#30005;&#36335;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25191;&#34892;&#36895;&#24230;&#24555;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2205.09435</link><description>&lt;p&gt;
&#23545;&#25239;&#38543;&#26426;&#26862;&#26519;&#29992;&#20110;&#23494;&#24230;&#20272;&#35745;&#21644;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Adversarial random forests for density estimation and generative modeling. (arXiv:2205.09435v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#25239;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#21644;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#24179;&#28369;&#30340;&#65288;&#38750;&#65289;&#26465;&#20214;&#23494;&#24230;&#65292;&#24182;&#20801;&#35768;&#23436;&#20840;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#34920;&#26684;&#25968;&#25454;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#30005;&#36335;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25191;&#34892;&#36895;&#24230;&#24555;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method for density estimation and data synthesis using adversarial random forests, which provides smooth (un)conditional densities and allows for fully synthetic data generation. The method achieves comparable or superior performance to state-of-the-art probabilistic circuits and deep learning models on various tabular data benchmarks while executing about two orders of magnitude faster on average.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26032;&#22411;&#26080;&#30417;&#30563;&#38543;&#26426;&#26862;&#26519;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#21644;&#25968;&#25454;&#21512;&#25104;&#30340;&#26041;&#27861;&#12290;&#21463;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#36882;&#24402;&#36807;&#31243;&#65292;&#20854;&#20013;&#26641;&#36890;&#36807;&#20132;&#26367;&#30340;&#29983;&#25104;&#21644;&#21028;&#21035;&#36718;&#27425;&#36880;&#28176;&#23398;&#20064;&#25968;&#25454;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#26368;&#23567;&#20551;&#35774;&#19979;&#21487;&#20197;&#34987;&#35777;&#26126;&#26159;&#19968;&#33268;&#30340;&#12290;&#19982;&#32463;&#20856;&#30340;&#22522;&#20110;&#26641;&#30340;&#26367;&#20195;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#24179;&#28369;&#30340;&#65288;&#38750;&#65289;&#26465;&#20214;&#23494;&#24230;&#65292;&#24182;&#20801;&#35768;&#23436;&#20840;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#34920;&#26684;&#25968;&#25454;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#30005;&#36335;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#24179;&#22343;&#25191;&#34892;&#36895;&#24230;&#24555;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;&#38468;&#24102;&#30340;R&#21253;arf&#21487;&#22312;CRAN&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose methods for density estimation and data synthesis using a novel form of unsupervised random forests. Inspired by generative adversarial networks, we implement a recursive procedure in which trees gradually learn structural properties of the data through alternating rounds of generation and discrimination. The method is provably consistent under minimal assumptions. Unlike classic tree-based alternatives, our approach provides smooth (un)conditional densities and allows for fully synthetic data generation. We achieve comparable or superior performance to state-of-the-art probabilistic circuits and deep learning models on various tabular data benchmarks while executing about two orders of magnitude faster on average. An accompanying $\texttt{R}$ package, $\texttt{arf}$, is available on $\texttt{CRAN}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#19981;&#33021;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#30001;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.06333</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#22312;&#22810;&#29289;&#20307;&#22330;&#26223;&#20013;&#36827;&#34892;&#35270;&#35273;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Visuomotor Control in Multi-Object Scenes Using Object-Aware Representations. (arXiv:2205.06333v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#19981;&#33021;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#30001;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the effectiveness of using object-aware representation learning techniques for robotic tasks, to address the problem that current methodologies learn task specific representations that do not necessarily transfer well to other tasks, and that representations learned by supervised methods require large labeled datasets for each task that are expensive to collect in the real world.
&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#30340;&#24863;&#30693;&#29702;&#35299;&#20197;&#21450;&#20854;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#23545;&#20110;&#25104;&#21151;&#23436;&#25104;&#26426;&#22120;&#20154;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#34920;&#31034;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#20294;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#26041;&#27861;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#65292;&#19981;&#19968;&#23450;&#33021;&#22815;&#24456;&#22909;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#30001;&#30417;&#30563;&#26041;&#27861;&#23398;&#20064;&#30340;&#34920;&#31034;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#25910;&#38598;&#36215;&#26469;&#24456;&#26114;&#36149;&#12290;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#33719;&#21462;&#34920;&#31034;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#26159;&#29289;&#20307;&#26080;&#20851;&#30340;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;&#27492;&#24471;&#21040;&#30340;&#34920;&#31034;&#23545;&#20110;&#20855;&#26377;&#35768;&#22810;&#32452;&#20214;&#30340;&#22330;&#26223;&#30340;&#36890;&#29992;&#26426;&#22120;&#20154;&#20219;&#21153;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#29289;&#20307;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perceptual understanding of the scene and the relationship between its different components is important for successful completion of robotic tasks. Representation learning has been shown to be a powerful technique for this, but most of the current methodologies learn task specific representations that do not necessarily transfer well to other tasks. Furthermore, representations learned by supervised methods require large labeled datasets for each task that are expensive to collect in the real world. Using self-supervised learning to obtain representations from unlabeled data can mitigate this problem. However, current self-supervised representation learning methods are mostly object agnostic, and we demonstrate that the resulting representations are insufficient for general purpose robotics tasks as they fail to capture the complexity of scenes with many components. In this paper, we explore the effectiveness of using object-aware representation learning techniques for robotic tasks. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Vine copula&#32467;&#26500;&#30340;&#30697;&#38453;&#21644;&#22270;&#24418;&#34920;&#31034;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#30697;&#38453;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#20063;&#34987;&#35745;&#31639;&#20102;&#12290;</title><link>http://arxiv.org/abs/2205.04783</link><description>&lt;p&gt;
Vine copula&#32467;&#26500;&#30340;&#30697;&#38453;&#21644;&#22270;&#24418;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Matrix and graph representations of vine copula structures. (arXiv:2205.04783v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Vine copula&#32467;&#26500;&#30340;&#30697;&#38453;&#21644;&#22270;&#24418;&#34920;&#31034;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#30697;&#38453;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#20063;&#34987;&#35745;&#31639;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vine copula&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#23427;&#20204;&#30340;&#32467;&#26500;&#65292;&#22240;&#20026;&#22312;&#25991;&#29486;&#20013;&#65292;vine copula&#30340;&#34920;&#31034;&#32463;&#24120;&#26159;&#27169;&#31946;&#30340;&#12290;&#22270;&#24418;&#34920;&#31034;&#21253;&#25324;&#21407;&#22987;&#30340;&#12289;cherry&#21644;chordal&#22270;&#24418;&#24207;&#21015;&#32467;&#26500;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20010;&#26032;&#30340;&#32467;&#26524;&#65292;&#21363;&#24403;&#32473;&#20986;vine&#32467;&#26500;&#30340;&#23436;&#32654;&#28040;&#38500;&#25490;&#24207;&#26102;&#65292;&#23427;&#24635;&#26159;&#21487;&#20197;&#29992;&#30697;&#38453;&#21807;&#19968;&#34920;&#31034;&#12290;O. M. N\'apoles&#24050;&#32463;&#23637;&#31034;&#20102;&#19968;&#31181;&#22312;&#30697;&#38453;&#20013;&#34920;&#31034;vine&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31639;&#27861;&#21270;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#36890;&#36807;cherry&#26641;&#24207;&#21015;&#26500;&#24314;&#36825;&#31181;&#30697;&#38453;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35745;&#31639;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#30697;&#38453;&#26500;&#24314;&#31639;&#27861;&#22312;&#20351;&#29992;&#30456;&#21516;&#30340;&#23436;&#32654;&#28040;&#38500;&#25490;&#24207;&#26102;&#26159;&#31561;&#20215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vine copulas can efficiently model multivariate probability distributions. This paper focuses on a more thorough understanding of their structures, since in the literature, vine copula representations are often ambiguous. The graph representations include the original, cherry and chordal graph sequence structures, which we show equivalence between. Importantly we also show a new result, namely that when a perfect elimination ordering of a vine structure is given, then it can always be uniquely represented with a matrix. O. M. N\'apoles has shown a way to represent vines in a matrix, and we algorithmify this previous approach, while also showing a new method for constructing such a matrix, through cherry tree sequences. We also calculate the runtime of these algorithms. Lastly, we prove that these two matrix-building algorithms are equivalent if the same perfect elimination ordering is being used.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#24179;&#31283;&#36172;&#21338;&#26426;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#38477;&#20302;&#33719;&#21462;&#20449;&#24687;&#30340;&#20248;&#20808;&#32423;&#65292;&#35299;&#20915;&#20102;Thompson&#25277;&#26679;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25152;&#26377;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20248;&#20110;Thompson&#25277;&#26679;&#12290;</title><link>http://arxiv.org/abs/2205.01970</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#36172;&#21338;&#26426;&#23398;&#20064;&#30340;&#39044;&#27979;&#25277;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Bandit Learning via Predictive Sampling. (arXiv:2205.01970v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01970
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#24179;&#31283;&#36172;&#21338;&#26426;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#38477;&#20302;&#33719;&#21462;&#20449;&#24687;&#30340;&#20248;&#20808;&#32423;&#65292;&#35299;&#20915;&#20102;Thompson&#25277;&#26679;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25152;&#26377;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20248;&#20110;Thompson&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a predictive sampling algorithm to solve the non-stationary bandit learning problem. By deprioritizing the acquisition of information that quickly loses usefulness, the algorithm outperforms Thompson sampling in all non-stationary environments examined.
&lt;/p&gt;
&lt;p&gt;
Thompson&#25277;&#26679;&#24050;&#32463;&#22312;&#24191;&#27867;&#30340;&#24179;&#31283;&#36172;&#21338;&#26426;&#29615;&#22659;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25152;&#23637;&#31034;&#30340;&#65292;&#24403;&#24212;&#29992;&#20110;&#38750;&#24179;&#31283;&#29615;&#22659;&#26102;&#65292;&#23427;&#30340;&#34920;&#29616;&#21487;&#33021;&#24456;&#24046;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#22833;&#36133;&#26159;&#30001;&#20110;&#22312;&#25506;&#32034;&#26102;&#65292;&#31639;&#27861;&#27809;&#26377;&#26681;&#25454;&#30001;&#20110;&#38750;&#24179;&#31283;&#24615;&#23548;&#33268;&#20449;&#24687;&#24555;&#36895;&#22833;&#21435;&#26377;&#29992;&#24615;&#30340;&#36895;&#24230;&#21306;&#20998;&#34892;&#21160;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#27979;&#25277;&#26679;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#38477;&#20302;&#20102;&#33719;&#21462;&#20449;&#24687;&#30340;&#20248;&#20808;&#32423;&#65292;&#36825;&#20123;&#20449;&#24687;&#30001;&#20110;&#24555;&#36895;&#22833;&#21435;&#26377;&#29992;&#24615;&#32780;&#19981;&#20877;&#37325;&#35201;&#12290;&#36890;&#36807;&#36125;&#21494;&#26031;&#36951;&#25022;&#30028;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#39044;&#27979;&#25277;&#26679;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39044;&#27979;&#25277;&#26679;&#30340;&#29256;&#26412;&#65292;&#20854;&#35745;&#31639;&#21487;&#25193;&#23637;&#21040;&#23454;&#38469;&#24863;&#20852;&#36259;&#30340;&#22797;&#26434;&#36172;&#21338;&#26426;&#29615;&#22659;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#39044;&#27979;&#25277;&#26679;&#22312;&#25152;&#26377;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#37117;&#20248;&#20110;Thompson&#25277;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thompson sampling has proven effective across a wide range of stationary bandit environments. However, as we demonstrate in this paper, it can perform poorly when applied to non-stationary environments. We show that such failures are attributed to the fact that, when exploring, the algorithm does not differentiate actions based on how quickly the information acquired loses its usefulness due to non-stationarity. Building upon this insight, we propose predictive sampling, an algorithm that deprioritizes acquiring information that quickly loses usefulness. Theoretical guarantee on the performance of predictive sampling is established through a Bayesian regret bound. We provide versions of predictive sampling for which computations tractably scale to complex bandit environments of practical interest. Through numerical simulations, we demonstrate that predictive sampling outperforms Thompson sampling in all non-stationary environments examined.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20271;&#24681;&#26031;&#22374;&#22810;&#39033;&#24335;&#24402;&#19968;&#21270;&#27969;&#30340;&#28789;&#27963;&#26465;&#20214;&#23494;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#30701;&#26399;&#20302;&#21387;&#36127;&#33655;&#39044;&#27979;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#21487;&#29992;&#20110;&#35268;&#21010;&#21644;&#36816;&#33829;&#20302;&#30899;&#33021;&#28304;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2204.13939</link><description>&lt;p&gt;
&#20302;&#21387;&#36127;&#33655;&#20271;&#24681;&#26031;&#22374;&#22810;&#39033;&#24335;&#24402;&#19968;&#21270;&#27969;&#30340;&#30701;&#26399;&#23494;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Short-Term Density Forecasting of Low-Voltage Load using Bernstein-Polynomial Normalizing Flows. (arXiv:2204.13939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20271;&#24681;&#26031;&#22374;&#22810;&#39033;&#24335;&#24402;&#19968;&#21270;&#27969;&#30340;&#28789;&#27963;&#26465;&#20214;&#23494;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#30701;&#26399;&#20302;&#21387;&#36127;&#33655;&#39044;&#27979;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#21487;&#29992;&#20110;&#35268;&#21010;&#21644;&#36816;&#33829;&#20302;&#30899;&#33021;&#28304;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a flexible conditional density forecasting method based on Bernstein polynomial normalizing flows for short-term low-voltage load forecasting, which outperforms traditional methods and can be used for planning and operating low-carbon energy systems.
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20840;&#38754;&#21487;&#20877;&#29983;&#33021;&#28304;&#30005;&#32593;&#30340;&#36716;&#22411;&#38656;&#35201;&#26356;&#22909;&#22320;&#39044;&#27979;&#20302;&#21387;&#27700;&#24179;&#30340;&#38656;&#27714;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#24182;&#30830;&#20445;&#21487;&#38752;&#30340;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#39640;&#27874;&#21160;&#24615;&#21644;&#19981;&#26029;&#22686;&#21152;&#30340;&#30005;&#27668;&#21270;&#23548;&#33268;&#24040;&#22823;&#30340;&#39044;&#27979;&#21464;&#24322;&#24615;&#65292;&#36825;&#22312;&#20256;&#32479;&#30340;&#28857;&#20272;&#35745;&#20013;&#27809;&#26377;&#21453;&#26144;&#20986;&#26469;&#12290;&#27010;&#29575;&#36127;&#36733;&#39044;&#27979;&#32771;&#34385;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22240;&#27492;&#20801;&#35768;&#26356;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#29992;&#20110;&#35268;&#21010;&#21644;&#36816;&#33829;&#20302;&#30899;&#33021;&#28304;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20271;&#24681;&#26031;&#22374;&#22810;&#39033;&#24335;&#24402;&#19968;&#21270;&#27969;&#30340;&#28789;&#27963;&#26465;&#20214;&#23494;&#24230;&#39044;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#27969;&#30340;&#21442;&#25968;&#12290;&#22312;&#19968;&#39033;&#21253;&#25324;363&#20010;&#26234;&#33021;&#30005;&#34920;&#23458;&#25143;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;&#23494;&#24230;&#39044;&#27979;&#19982;&#39640;&#26031;&#21644;&#39640;&#26031;&#28151;&#21512;&#23494;&#24230;&#30456;&#27604;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20004;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#20204;&#22312;24&#23567;&#26102;&#21069;&#30340;&#36127;&#36733;&#39044;&#27979;&#20013;&#20248;&#20110;&#22522;&#20110;&#38024;&#29699;&#25439;&#22833;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transition to a fully renewable energy grid requires better forecasting of demand at the low-voltage level to increase efficiency and ensure reliable control. However, high fluctuations and increasing electrification cause huge forecast variability, not reflected in traditional point estimates. Probabilistic load forecasts take future uncertainties into account and thus allow more informed decision-making for the planning and operation of low-carbon energy systems. We propose an approach for flexible conditional density forecasting of short-term load based on Bernstein polynomial normalizing flows, where a neural network controls the parameters of the flow. In an empirical study with 363 smart meter customers, our density predictions compare favorably against Gaussian and Gaussian mixture densities. Also, they outperform a non-parametric approach based on the pinball loss for 24h-ahead load forecasting for two different neural network architectures.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#26816;&#27979;&#38169;&#35823;&#30340;&#20301;&#32622;&#23884;&#20837;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;DILEMMA&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;MoCoV3&#12289;DINO&#21644;SimCLR&#65292;&#20998;&#21035;&#26174;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4.41%&#12289;3.97%&#21644;0.5%&#12290;</title><link>http://arxiv.org/abs/2204.04788</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#27979;&#38169;&#35823;&#30340;&#20301;&#32622;&#23884;&#20837;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning by Detecting Incorrect Location Embeddings. (arXiv:2204.04788v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.04788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#26816;&#27979;&#38169;&#35823;&#30340;&#20301;&#32622;&#23884;&#20837;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;DILEMMA&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;MoCoV3&#12289;DINO&#21644;SimCLR&#65292;&#20998;&#21035;&#26174;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4.41%&#12289;3.97%&#21644;0.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25439;&#22833;&#65292;&#29992;&#20110;&#22270;&#20687;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#35748;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#20854;&#21306;&#20998;&#23545;&#35937;&#24418;&#29366;&#30340;&#33021;&#21147;&#26377;&#20851;&#12290;&#30001;&#20110;&#23545;&#35937;&#24418;&#29366;&#19982;&#20854;&#37096;&#20214;&#30340;&#20301;&#32622;&#26377;&#20851;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#26816;&#27979;&#37027;&#20123;&#34987;&#20154;&#20026;&#31227;&#20301;&#30340;&#37096;&#20214;&#12290;&#25105;&#20204;&#29992;&#22270;&#20687;&#20196;&#29260;&#34920;&#31034;&#23545;&#35937;&#37096;&#20214;&#65292;&#24182;&#35757;&#32451;ViT&#26816;&#27979;&#21738;&#20010;&#20196;&#29260;&#19982;&#38169;&#35823;&#30340;&#20301;&#32622;&#23884;&#20837;&#32452;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#36755;&#20837;&#30340;&#31232;&#30095;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#20197;&#24212;&#23545;&#36974;&#25377;&#24182;&#21152;&#36895;&#35757;&#32451;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;DILEMMA&#65292;&#21363;&#26816;&#27979;&#38169;&#35823;&#20301;&#32622;&#23884;&#20837;&#21644;&#25513;&#34109;&#36755;&#20837;&#12290;&#25105;&#20204;&#23558;DILEMMA&#24212;&#29992;&#20110;MoCoV3&#12289;DINO&#21644;SimCLR&#65292;&#24182;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#65292;&#22312;ImageNet-1K&#19978;&#36827;&#34892;&#32447;&#24615;&#25506;&#27979;&#36716;&#31227;&#65292;&#20998;&#21035;&#26174;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;4.41%&#12289;3.97%&#21644;0.5%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;MAE&#19982;&#25105;&#20204;&#30340;&#23436;&#20840;&#24494;&#35843;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel self-supervised learning (SSL) loss for image representation learning. There is a growing belief that generalization in deep neural networks is linked to their ability to discriminate object shapes. Since object shape is related to the location of its parts, we propose to detect those that have been artificially misplaced. We represent object parts with image tokens and train a ViT to detect which token has been combined with an incorrect positional embedding. We then introduce sparsity in the inputs to make the model more robust to occlusions and to speed up the training. We call our method DILEMMA, which stands for Detection of Incorrect Location EMbeddings with MAsked inputs. We apply DILEMMA to MoCoV3, DINO and SimCLR and show an improvement in their performance of respectively 4.41%, 3.97%, and 0.5% under the same training time and with a linear probing transfer on ImageNet-1K. We also show full fine-tuning improvements of MAE combined with our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24050;&#34987;&#25764;&#22238;&#65292;&#21407;&#22240;&#26159;&#35821;&#35328;&#21644;&#29702;&#35770;&#25551;&#36848;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#20316;&#32773;&#24050;&#32463;&#36827;&#34892;&#20102;&#20462;&#35746;&#21644;&#26356;&#26032;&#12290;</title><link>http://arxiv.org/abs/2204.03471</link><description>&lt;p&gt;
DynLight: &#22810;&#32423;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#23454;&#29616;&#21160;&#24577;&#30456;&#20301;&#26102;&#38271;
&lt;/p&gt;
&lt;p&gt;
DynLight: Realize dynamic phase duration with multi-level traffic signal control. (arXiv:2204.03471v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24050;&#34987;&#25764;&#22238;&#65292;&#21407;&#22240;&#26159;&#35821;&#35328;&#21644;&#29702;&#35770;&#25551;&#36848;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#20316;&#32773;&#24050;&#32463;&#36827;&#34892;&#20102;&#20462;&#35746;&#21644;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article has been withdrawn due to unsatisfactory language and theoretical description, and the authors have revised and updated it.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22240;&#20197;&#19979;&#21407;&#22240;&#25764;&#22238;&#26412;&#25991;&#65306;1.&#26412;&#25991;&#30340;&#35821;&#35328;&#21644;&#29702;&#35770;&#25551;&#36848;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65307;2.&#25105;&#20204;&#22312;&#20854;&#20182;&#20316;&#32773;&#30340;&#24110;&#21161;&#19979;&#20016;&#23500;&#21644;&#20462;&#35746;&#20102;&#26412;&#25991;&#65307;3.&#25105;&#20204;&#24517;&#39035;&#26356;&#26032;&#20316;&#32773;&#36129;&#29486;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We would like to withdraw this article for the following reasons: 1 this article is not satisfactory for limited language and theoretical description; 2 we have enriched and revised this article with the help of other authors; 3 we must update the author contribution information.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#28145;&#24230;&#29305;&#24449;&#31579;&#36873;&#65288;DeepFS&#65289;&#65292;&#21487;&#20197;&#20811;&#26381;&#39640;&#32500;&#12289;&#20302;&#26679;&#26412;&#25968;&#25454;&#19978;&#30340;&#22256;&#38590;&#21644;&#25361;&#25112;&#65292;&#24182;&#23545;&#36229;&#39640;&#32500;&#12289;&#20302;&#26679;&#26412;&#25968;&#25454;&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#29305;&#24449;&#31579;&#36873;&#12290;</title><link>http://arxiv.org/abs/2204.01682</link><description>&lt;p&gt;
&#28145;&#24230;&#29305;&#24449;&#31579;&#36873;&#65306;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36229;&#39640;&#32500;&#25968;&#25454;&#30340;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Deep Feature Screening: Feature Selection for Ultra High-Dimensional Data via Deep Neural Networks. (arXiv:2204.01682v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.01682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#28145;&#24230;&#29305;&#24449;&#31579;&#36873;&#65288;DeepFS&#65289;&#65292;&#21487;&#20197;&#20811;&#26381;&#39640;&#32500;&#12289;&#20302;&#26679;&#26412;&#25968;&#25454;&#19978;&#30340;&#22256;&#38590;&#21644;&#25361;&#25112;&#65292;&#24182;&#23545;&#36229;&#39640;&#32500;&#12289;&#20302;&#26679;&#26412;&#25968;&#25454;&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#29305;&#24449;&#31579;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel two-step nonparametric approach called Deep Feature Screening (DeepFS) that can overcome the challenges of high-dimensional, low-sample-size data and identify significant features with high precision for ultra high-dimensional, low-sample-size data.
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#32479;&#35745;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#22312;&#39640;&#32500;&#12289;&#20302;&#26679;&#26412;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#32463;&#24120;&#36935;&#21040;&#22256;&#38590;&#21644;&#25361;&#25112;&#65292;&#22914;&#36807;&#25311;&#21512;&#12289;&#32500;&#25968;&#28798;&#38590;&#12289;&#35745;&#31639;&#19981;&#21487;&#34892;&#21644;&#24378;&#27169;&#22411;&#20551;&#35774;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#31216;&#20026;&#28145;&#24230;&#29305;&#24449;&#31579;&#36873;&#65288;DeepFS&#65289;&#65292;&#21487;&#20197;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#23545;&#36229;&#39640;&#32500;&#12289;&#20302;&#26679;&#26412;&#25968;&#25454;&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#29305;&#24449;&#31579;&#36873;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#28982;&#21518;&#24212;&#29992;&#22522;&#20110;Deb&#21644;Sen&#65288;2021&#65289;&#26368;&#36817;&#24320;&#21457;&#30340;&#22810;&#20803;&#31209;&#36317;&#30456;&#20851;&#24615;&#30340;&#29305;&#24449;&#31579;&#36873;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#24449;&#31579;&#36873;&#30340;&#20248;&#28857;&#65292;&#38500;&#20102;&#22788;&#29702;&#20855;&#26377;&#23569;&#37327;&#26679;&#26412;&#30340;&#36229;&#39640;&#32500;&#25968;&#25454;&#30340;&#33021;&#21147;&#22806;&#65292;&#36824;&#20855;&#26377;&#20197;&#19979;&#21560;&#24341;&#20154;&#30340;&#29305;&#28857;&#65306;&#65288;1&#65289;&#23427;&#26159;&#27169;&#22411;&#33258;&#30001;&#21644;&#20998;&#24067;&#33258;&#30001;&#30340;&#65307;&#65288;2&#65289;&#23427;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
The applications of traditional statistical feature selection methods to high-dimension, low sample-size data often struggle and encounter challenging problems, such as overfitting, curse of dimensionality, computational infeasibility, and strong model assumption. In this paper, we propose a novel two-step nonparametric approach called Deep Feature Screening (DeepFS) that can overcome these problems and identify significant features with high precision for ultra high-dimensional, low-sample-size data. This approach first extracts a low-dimensional representation of input data and then applies feature screening based on multivariate rank distance correlation recently developed by Deb and Sen (2021). This approach combines the strengths of both deep neural networks and feature screening, and thereby has the following appealing features in addition to its ability of handling ultra high-dimensional data with small number of samples: (1) it is model free and distribution free; (2) it can be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#24449;&#26381;&#30340;&#23545;&#27604;&#38598;&#25366;&#25496;&#31639;&#27861;RuleKit-CS&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22810;&#27425;&#36890;&#36807;&#20276;&#38543;&#23646;&#24615;&#24809;&#32602;&#26041;&#26696;&#25552;&#20379;&#25551;&#36848;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#30456;&#21516;&#31034;&#20363;&#30340;&#23545;&#27604;&#38598;&#65292;&#21306;&#21035;&#20110;&#26631;&#20934;&#30340;&#20998;&#31163;&#24449;&#26381;&#12290;&#35813;&#31639;&#27861;&#36824;&#34987;&#25512;&#24191;&#21040;&#22238;&#24402;&#21644;&#29983;&#23384;&#25968;&#25454;&#65292;&#20801;&#35768;&#35782;&#21035;&#26631;&#31614;&#23646;&#24615;/&#29983;&#23384;&#39044;&#27979;&#19982;&#39044;&#23450;&#20041;&#23545;&#27604;&#32452;&#30340;&#26631;&#31614;/&#39044;&#27979;&#19968;&#33268;&#30340;&#23545;&#27604;&#38598;&#12290;</title><link>http://arxiv.org/abs/2204.00497</link><description>&lt;p&gt;
&#20998;&#31163;&#24449;&#26381;&#21551;&#21457;&#24335;&#31639;&#27861;&#20801;&#35768;&#22312;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#29983;&#23384;&#25968;&#25454;&#20013;&#36827;&#34892;&#24378;&#22823;&#30340;&#23545;&#27604;&#38598;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Separate and conquer heuristic allows robust mining of contrast sets in classification, regression, and survival data. (arXiv:2204.00497v3 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#24449;&#26381;&#30340;&#23545;&#27604;&#38598;&#25366;&#25496;&#31639;&#27861;RuleKit-CS&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#22810;&#27425;&#36890;&#36807;&#20276;&#38543;&#23646;&#24615;&#24809;&#32602;&#26041;&#26696;&#25552;&#20379;&#25551;&#36848;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#30456;&#21516;&#31034;&#20363;&#30340;&#23545;&#27604;&#38598;&#65292;&#21306;&#21035;&#20110;&#26631;&#20934;&#30340;&#20998;&#31163;&#24449;&#26381;&#12290;&#35813;&#31639;&#27861;&#36824;&#34987;&#25512;&#24191;&#21040;&#22238;&#24402;&#21644;&#29983;&#23384;&#25968;&#25454;&#65292;&#20801;&#35768;&#35782;&#21035;&#26631;&#31614;&#23646;&#24615;/&#29983;&#23384;&#39044;&#27979;&#19982;&#39044;&#23450;&#20041;&#23545;&#27604;&#32452;&#30340;&#26631;&#31614;/&#39044;&#27979;&#19968;&#33268;&#30340;&#23545;&#27604;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a contrast set mining algorithm, RuleKit-CS, based on the separate and conquer heuristic, which provides contrast sets describing the same examples with different attributes through multiple passes accompanied with an attribute penalization scheme. The algorithm is also generalized for regression and survival data, allowing identification of contrast sets whose label attribute/survival prognosis is consistent with the label/prognosis for the predefined contrast groups.
&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#32676;&#20307;&#20043;&#38388;&#30340;&#24046;&#24322;&#26159;&#26368;&#37325;&#35201;&#30340;&#30693;&#35782;&#21457;&#29616;&#38382;&#39064;&#20043;&#19968;&#12290;&#35813;&#36807;&#31243;&#65292;&#20063;&#31216;&#20026;&#23545;&#27604;&#38598;&#25366;&#25496;&#65292;&#22312;&#21307;&#23398;&#12289;&#24037;&#19994;&#25110;&#32463;&#27982;&#31561;&#24191;&#27867;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RuleKit-CS&#65292;&#19968;&#31181;&#22522;&#20110;&#20998;&#31163;&#24449;&#26381;&#30340;&#23545;&#27604;&#38598;&#25366;&#25496;&#31639;&#27861;&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;&#20915;&#31574;&#35268;&#21017;&#24402;&#32435;&#30340;&#25104;&#29087;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#22810;&#27425;&#36890;&#36807;&#20276;&#38543;&#23646;&#24615;&#24809;&#32602;&#26041;&#26696;&#25552;&#20379;&#25551;&#36848;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#30456;&#21516;&#31034;&#20363;&#30340;&#23545;&#27604;&#38598;&#65292;&#21306;&#21035;&#20110;&#26631;&#20934;&#30340;&#20998;&#31163;&#24449;&#26381;&#12290;&#35813;&#31639;&#27861;&#36824;&#34987;&#25512;&#24191;&#21040;&#22238;&#24402;&#21644;&#29983;&#23384;&#25968;&#25454;&#65292;&#20801;&#35768;&#35782;&#21035;&#26631;&#31614;&#23646;&#24615;/&#29983;&#23384;&#39044;&#27979;&#19982;&#39044;&#23450;&#20041;&#23545;&#27604;&#32452;&#30340;&#26631;&#31614;/&#39044;&#27979;&#19968;&#33268;&#30340;&#23545;&#27604;&#38598;&#12290;&#36825;&#20010;&#29305;&#24615;&#65292;&#19981;&#26159;&#29616;&#26377;&#26041;&#27861;&#25152;&#25552;&#20379;&#30340;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;RuleKit-CS&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;130&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#21644;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying differences between groups is one of the most important knowledge discovery problems. The procedure, also known as contrast sets mining, is applied in a wide range of areas like medicine, industry, or economics.  In the paper we present RuleKit-CS, an algorithm for contrast set mining based on separate and conquer - a well established heuristic for decision rule induction. Multiple passes accompanied with an attribute penalization scheme provide contrast sets describing same examples with different attributes, distinguishing presented approach from the standard separate and conquer. The algorithm was also generalized for regression and survival data allowing identification of contrast sets whose label attribute/survival prognosis is consistent with the label/prognosis for the predefined contrast groups. This feature, not provided by the existing approaches, further extends the usability of RuleKit-CS.  Experiments on over 130 data sets from various areas and detailed analys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#31243;&#24207;&#24369;&#30417;&#30563;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#40784;&#31163;&#25955;&#28508;&#22312;&#21464;&#37327;&#21644;&#24369;&#30417;&#30563;&#27966;&#29983;&#30340;&#26631;&#31614;&#20272;&#35745;&#65292;&#25913;&#21892;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#30340;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2203.12023</link><description>&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#26377;&#21161;&#20110;&#24369;&#30417;&#30563;&#65288;&#21453;&#20043;&#20134;&#28982;&#65289;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling Helps Weak Supervision (and Vice Versa). (arXiv:2203.12023v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#31243;&#24207;&#24369;&#30417;&#30563;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#40784;&#31163;&#25955;&#28508;&#22312;&#21464;&#37327;&#21644;&#24369;&#30417;&#30563;&#27966;&#29983;&#30340;&#26631;&#31614;&#20272;&#35745;&#65292;&#25913;&#21892;&#20102;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#30340;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a model that fuses programmatic weak supervision and generative adversarial networks, improving the estimate of unobserved labels by aligning discrete latent variables and weak supervision derived label estimate, and enabling data augmentation through weak supervision.
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26377;&#21069;&#36884;&#30340;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22312;&#33719;&#21462;&#36275;&#22815;&#25968;&#37327;&#21644;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#65292;&#20174;&#32780;&#36896;&#25104;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#30740;&#31350;&#20102;&#19981;&#20381;&#36182;&#20110;&#22522;&#26412;&#30495;&#23454;&#26631;&#31614;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;&#24369;&#30417;&#30563;&#21644;&#29983;&#25104;&#24314;&#27169;&#12290;&#34429;&#28982;&#36825;&#20123;&#25216;&#26415;&#20284;&#20046;&#21487;&#20197;&#20849;&#21516;&#20351;&#29992;&#65292;&#30456;&#20114;&#25913;&#36827;&#65292;&#20294;&#22914;&#20309;&#22312;&#23427;&#20204;&#20043;&#38388;&#24314;&#31435;&#25509;&#21475;&#23578;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#31243;&#24207;&#24369;&#30417;&#30563;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#36825;&#31181;&#34701;&#21512;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#31163;&#25955;&#28508;&#22312;&#21464;&#37327;&#20197;&#21450;&#24369;&#30417;&#30563;&#27966;&#29983;&#30340;&#26631;&#31614;&#20272;&#35745;&#12290;&#20004;&#32773;&#30340;&#23545;&#40784;&#20801;&#35768;&#26356;&#22909;&#22320;&#24314;&#27169;&#24369;&#30417;&#30563;&#26469;&#28304;&#30340;&#26679;&#26412;&#30456;&#20851;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#30340;&#20272;&#35745;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#36890;&#36807;&#24369;&#30417;&#30563;&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many promising applications of supervised machine learning face hurdles in the acquisition of labeled data in sufficient quantity and quality, creating an expensive bottleneck. To overcome such limitations, techniques that do not depend on ground truth labels have been studied, including weak supervision and generative modeling. While these techniques would seem to be usable in concert, improving one another, how to build an interface between them is not well-understood. In this work, we propose a model fusing programmatic weak supervision and generative adversarial networks and provide theoretical justification motivating this fusion. The proposed approach captures discrete latent variables in the data alongside the weak supervision derived label estimate. Alignment of the two allows for better modeling of sample-dependent accuracies of the weak supervision sources, improving the estimate of unobserved labels. It is the first approach to enable data augmentation through weakly supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#30315;&#30187;&#26679;&#27963;&#21160;&#23545;&#21361;&#37325;&#30149;&#24739;&#32773;&#20986;&#38498;&#32467;&#23616;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22238;&#39038;&#24615;&#27178;&#26029;&#38754;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#27599;&#20010;&#20154;&#37117;&#32463;&#21382;&#20102;&#26576;&#31181;EA&#36127;&#33655;&#24182;&#19988;&#26410;&#25509;&#21463;&#27835;&#30103;&#65292;&#20986;&#38498;mRS&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2203.04920</link><description>&lt;p&gt;
&#30315;&#30187;&#26679;&#27963;&#21160;&#23545;&#21361;&#37325;&#30149;&#24739;&#32773;&#20986;&#38498;&#32467;&#23616;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of Epileptiform Activity on Discharge Outcome in Critically Ill Patients. (arXiv:2203.04920v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#30315;&#30187;&#26679;&#27963;&#21160;&#23545;&#21361;&#37325;&#30149;&#24739;&#32773;&#20986;&#38498;&#32467;&#23616;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22238;&#39038;&#24615;&#27178;&#26029;&#38754;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#27599;&#20010;&#20154;&#37117;&#32463;&#21382;&#20102;&#26576;&#31181;EA&#36127;&#33655;&#24182;&#19988;&#26410;&#25509;&#21463;&#27835;&#30103;&#65292;&#20986;&#38498;mRS&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to explore the effects of epileptiform activity on discharge outcomes in critically ill patients. Through a retrospective cross-sectional study, it was found that the discharge mRS would change if everyone had experienced a certain EA burden and were untreated.
&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26679;&#27963;&#21160;&#65288;EA&#65289;&#19982;&#26356;&#24046;&#30340;&#32467;&#23616;&#30456;&#20851;&#65292;&#21253;&#25324;&#22686;&#21152;&#27531;&#30142;&#21644;&#27515;&#20129;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;EA&#23545;&#31070;&#32463;&#31995;&#32479;&#32467;&#23616;&#30340;&#24433;&#21709;&#21463;&#21040;&#25239;&#30315;&#30187;&#33647;&#29289;&#65288;ASM&#65289;&#27835;&#30103;&#21644;EA&#36127;&#33655;&#20043;&#38388;&#30340;&#21453;&#39304;&#30340;&#24178;&#25200;&#12290;&#30001;&#20110;EA-ASM&#21453;&#39304;&#30340;&#39034;&#24207;&#24615;&#20197;&#21450;&#20262;&#29702;&#21407;&#22240;&#65292;&#38543;&#26426;&#20020;&#24202;&#35797;&#39564;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#26426;&#21046;&#30693;&#35782;&#26159;&#21487;&#29992;&#30340;&#65292;&#20363;&#22914;&#33647;&#29289;&#30340;&#21560;&#25910;&#26041;&#24335;&#12290;&#36825;&#20123;&#30693;&#35782;&#19982;&#35266;&#23519;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#21487;&#20197;&#20351;&#29992;&#22240;&#26524;&#25512;&#26029;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#25928;&#24212;&#20272;&#35745;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22238;&#39038;&#24615;&#27178;&#26029;&#38754;&#30740;&#31350;&#65292;&#20849;&#26377;995&#21517;&#24739;&#32773;&#65292;&#20197;&#20986;&#38498;&#26102;&#30340;&#20462;&#27491;Rankin&#37327;&#34920;&#65288;mRS&#65289;&#20026;&#32467;&#26524;&#65292;&#20197;&#22312;&#31532;&#19968;&#27425;&#33041;&#30005;&#22270;&#30340;&#21069;24&#23567;&#26102;&#20869;&#27599;&#20010;&#20845;&#23567;&#26102;&#31383;&#21475;&#20013;EA&#36127;&#33655;&#30340;&#24179;&#22343;&#25110;&#26368;&#22823;&#27604;&#20363;&#20026;&#26292;&#38706;&#12290;&#25105;&#20204;&#20272;&#35745;&#20102;&#22914;&#26524;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#20154;&#37117;&#32463;&#21382;&#20102;&#26576;&#31181;EA&#36127;&#33655;&#24182;&#19988;&#26410;&#25509;&#21463;&#27835;&#30103;&#65292;&#20986;&#38498;mRS&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epileptiform activity (EA) is associated with worse outcomes including increased risk of disability and death. However, the effect of EA on the neurologic outcome is confounded by the feedback between treatment with anti-seizure medications (ASM) and EA burden. A randomized clinical trial is challenging due to the sequential nature of EA-ASM feedback, as well as ethical reasons. However, some mechanistic knowledge is available, e.g., how drugs are absorbed. This knowledge together with observational data could provide a more accurate effect estimate using causal inference. We performed a retrospective cross-sectional study with 995 patients with the modified Rankin Scale (mRS) at discharge as the outcome and the EA burden defined as the mean or maximum proportion of time spent with EA in six-hour windows in the first 24 hours of electroencephalography as the exposure. We estimated the change in discharge mRS if everyone in the dataset had experienced a certain EA burden and were untrea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#21644;&#21333;&#31867;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#29983;&#24577;&#31995;&#32479;&#26576;&#20123;&#24178;&#25200;&#30340;&#24369;&#20449;&#21495;&#65292;&#20197;&#32472;&#21046;&#30001;&#20960;&#20309;&#34558;&#29190;&#21457;&#24341;&#36215;&#30340;&#26862;&#26519;&#27515;&#20129;&#29575;&#22312;&#31232;&#30095;&#26862;&#26519;-&#33492;&#21407;&#29983;&#24577;&#36807;&#28193;&#24102;&#20013;&#30340;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2203.00049</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500;&#36965;&#24863;&#22270;&#20687;&#30340;&#30446;&#26631;&#21464;&#21270;&#26816;&#27979;&#65292;&#29992;&#20110;&#26862;&#26519;&#27515;&#20129;&#29575;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Towards Targeted Change Detection with Heterogeneous Remote Sensing Images for Forest Mortality Mapping. (arXiv:2203.00049v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#21644;&#21333;&#31867;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#29983;&#24577;&#31995;&#32479;&#26576;&#20123;&#24178;&#25200;&#30340;&#24369;&#20449;&#21495;&#65292;&#20197;&#32472;&#21046;&#30001;&#20960;&#20309;&#34558;&#29190;&#21457;&#24341;&#36215;&#30340;&#26862;&#26519;&#27515;&#20129;&#29575;&#22312;&#31232;&#30095;&#26862;&#26519;-&#33492;&#21407;&#29983;&#24577;&#36807;&#28193;&#24102;&#20013;&#30340;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#29992;&#20110;&#24322;&#26500;&#36965;&#24863;&#25968;&#25454;&#65288;&#20363;&#22914;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#21644;&#22810;&#20809;&#35889;&#36752;&#23556;&#35745;&#65289;&#30340;&#21464;&#21270;&#26816;&#27979;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#21512;&#26816;&#27979;&#29983;&#24577;&#31995;&#32479;&#26576;&#20123;&#24178;&#25200;&#30340;&#24369;&#20449;&#21495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#21644;&#21333;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#26088;&#22312;&#20351;&#29992;&#22810;&#28304;&#21355;&#26143;&#22270;&#20687;&#32472;&#21046;&#30001;&#20960;&#20309;&#34558;&#29190;&#21457;&#24341;&#36215;&#30340;&#26862;&#26519;&#27515;&#20129;&#29575;&#22312;&#31232;&#30095;&#26862;&#26519;-&#33492;&#21407;&#29983;&#24577;&#36807;&#28193;&#24102;&#20013;&#30340;&#22320;&#22270;&#12290;&#20107;&#20214;&#21069;&#21644;&#20107;&#20214;&#21518;&#30340;&#22270;&#20687;&#20998;&#21035;&#30001;Landsat-5&#21644;RADARSAT-2&#25910;&#38598;&#12290;&#20351;&#29992;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#21464;&#21270;&#24863;&#30693;&#22270;&#20687;&#36716;&#25442;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#21355;&#26143;&#21508;&#33258;&#30340;&#22495;&#20013;&#35745;&#31639;&#24046;&#24322;&#22270;&#20687;&#12290;&#36825;&#20123;&#24046;&#24322;&#19982;&#21407;&#22987;&#30340;&#20107;&#20214;&#21069;&#21644;&#20107;&#20214;&#21518;&#30340;&#22270;&#20687;&#22534;&#21472;&#65292;&#24182;&#20256;&#36882;&#32473;&#22312;&#30446;&#26631;&#21464;&#21270;&#31867;&#30340;&#23567;&#26679;&#26412;&#19978;&#35757;&#32451;&#30340;OCC&#12290;&#20998;&#31867;&#22120;&#20135;&#29983;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Several generic methods have recently been developed for change detection in heterogeneous remote sensing data, such as images from synthetic aperture radar (SAR) and multispectral radiometers. However, these are not well suited to detect weak signatures of certain disturbances of ecological systems. To resolve this problem we propose a new approach based on image-to-image translation and one-class classification (OCC). We aim to map forest mortality caused by an outbreak of geometrid moths in a sparsely forested forest-tundra ecotone using multisource satellite images. The images preceding and following the event are collected by Landsat-5 and RADARSAT-2, respectively. Using a recent deep learning method for change-aware image translation, we compute difference images in both satellites' respective domains. These differences are stacked with the original pre- and post-event images and passed to an OCC trained on a small sample from the targeted change class. The classifier produces a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#30340;&#36229;&#20960;&#20309;&#23884;&#20837;&#65292;&#20351;&#29992;&#19968;&#31181;&#32534;&#30721;&#20960;&#20309;&#20808;&#39564;&#30340;&#26144;&#23556;&#23558;&#20854;&#26144;&#23556;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#24182;&#26368;&#32456;&#20351;&#29992;&#26631;&#20934;&#27431;&#20960;&#37324;&#24471;&#32593;&#32476;&#12290;&#20851;&#38190;&#30340;&#27934;&#35265;&#26159;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#29305;&#24449;&#20989;&#25968;&#30340;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#36817;&#20284;&#36229;&#20960;&#20309;&#31354;&#38388;&#19978;&#30340;&#20219;&#20309;&#31561;&#24230;&#37327;&#19981;&#21464;&#26680;&#12290;</title><link>http://arxiv.org/abs/2202.06854</link><description>&lt;p&gt;
&#38543;&#26426;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#29992;&#20110;&#36229;&#20960;&#20309;&#31354;&#38388;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random Laplacian Features for Learning with Hyperbolic Space. (arXiv:2202.06854v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#30340;&#36229;&#20960;&#20309;&#23884;&#20837;&#65292;&#20351;&#29992;&#19968;&#31181;&#32534;&#30721;&#20960;&#20309;&#20808;&#39564;&#30340;&#26144;&#23556;&#23558;&#20854;&#26144;&#23556;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#24182;&#26368;&#32456;&#20351;&#29992;&#26631;&#20934;&#27431;&#20960;&#37324;&#24471;&#32593;&#32476;&#12290;&#20851;&#38190;&#30340;&#27934;&#35265;&#26159;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#29305;&#24449;&#20989;&#25968;&#30340;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#36817;&#20284;&#36229;&#20960;&#20309;&#31354;&#38388;&#19978;&#30340;&#20219;&#20309;&#31561;&#24230;&#37327;&#19981;&#21464;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple approach to learning hyperbolic embeddings by mapping them to Euclidean space using a mapping that encodes geometric priors and then using a standard Euclidean network. The key insight is to use a random feature mapping via the eigenfunctions of the Laplace operator, which can approximate any isometry-invariant kernel on hyperbolic space.
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20960;&#20309;&#29305;&#24615;&#65292;&#36229;&#20960;&#20309;&#31354;&#38388;&#21487;&#20197;&#25903;&#25345;&#26641;&#24418;&#21644;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#30340;&#39640;&#20445;&#30495;&#23884;&#20837;&#65292;&#22522;&#20110;&#27492;&#65292;&#21508;&#31181;&#36229;&#20960;&#20309;&#32593;&#32476;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#29616;&#26377;&#30340;&#36229;&#20960;&#20309;&#32593;&#32476;&#19981;&#20165;&#23545;&#36755;&#20837;&#36827;&#34892;&#20960;&#20309;&#20808;&#39564;&#32534;&#30721;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#37117;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#21040;&#21453;&#22797;&#26144;&#23556;&#21040;&#21644;&#20174;&#36229;&#20960;&#20309;&#31354;&#38388;&#65292;&#20351;&#24471;&#36825;&#20123;&#32593;&#32476;&#38590;&#20197;&#23454;&#29616;&#65292;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35757;&#32451;&#25968;&#20540;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#65306;&#23398;&#20064;&#36755;&#20837;&#30340;&#36229;&#20960;&#20309;&#23884;&#20837;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#32534;&#30721;&#20960;&#20309;&#20808;&#39564;&#30340;&#26144;&#23556;&#23558;&#20854;&#26144;&#23556;&#21040;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#24182;&#26368;&#32456;&#20351;&#29992;&#26631;&#20934;&#27431;&#20960;&#37324;&#24471;&#32593;&#32476;&#12290;&#20851;&#38190;&#30340;&#27934;&#35265;&#26159;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#31639;&#23376;&#30340;&#29305;&#24449;&#20989;&#25968;&#30340;&#38543;&#26426;&#29305;&#24449;&#26144;&#23556;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#36817;&#20284;&#36229;&#20960;&#20309;&#31354;&#38388;&#19978;&#30340;&#20219;&#20309;&#31561;&#24230;&#37327;&#19981;&#21464;&#26680;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to its geometric properties, hyperbolic space can support high-fidelity embeddings of tree- and graph-structured data, upon which various hyperbolic networks have been developed. Existing hyperbolic networks encode geometric priors not only for the input, but also at every layer of the network. This approach involves repeatedly mapping to and from hyperbolic space, which makes these networks complicated to implement, computationally expensive to scale, and numerically unstable to train. In this paper, we propose a simpler approach: learn a hyperbolic embedding of the input, then map once from it to Euclidean space using a mapping that encodes geometric priors by respecting the isometries of hyperbolic space, and finish with a standard Euclidean network. The key insight is to use a random feature mapping via the eigenfunctions of the Laplace operator, which we show can approximate any isometry-invariant kernel on hyperbolic space. Our method can be used together with any graph neura
&lt;/p&gt;</description></item><item><title>PGMax&#26159;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#22240;&#23376;&#22270;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;JAX&#20013;&#33258;&#21160;&#36816;&#34892;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#65292;&#19982;&#29616;&#26377;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;PGMax&#33719;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#39640;&#36798;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2202.04110</link><description>&lt;p&gt;
PGMax: &#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#21644;JAX&#20013;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#30340;&#22240;&#23376;&#22270;
&lt;/p&gt;
&lt;p&gt;
PGMax: Factor Graphs for Discrete Probabilistic Graphical Models and Loopy Belief Propagation in JAX. (arXiv:2202.04110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04110
&lt;/p&gt;
&lt;p&gt;
PGMax&#26159;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#22240;&#23376;&#22270;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;JAX&#20013;&#33258;&#21160;&#36816;&#34892;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#65292;&#19982;&#29616;&#26377;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;PGMax&#33719;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#39640;&#36798;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
PGMax is a factor graph tool for discrete probabilistic graphical models that automatically runs efficient and scalable loopy belief propagation in JAX. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups.
&lt;/p&gt;
&lt;p&gt;
PGMax&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#36731;&#26494;&#25351;&#23450;&#31163;&#25955;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#20316;&#20026;&#22240;&#23376;&#22270;&#65292;&#24182;&#22312;JAX&#20013;&#33258;&#21160;&#36816;&#34892;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#24490;&#29615;&#32622;&#20449;&#20256;&#25773;&#65288;LBP&#65289;&#12290;PGMax&#25903;&#25345;&#20855;&#26377;&#21487;&#22788;&#29702;&#22240;&#23376;&#30340;&#19968;&#33324;&#22240;&#23376;&#22270;&#65292;&#24182;&#21033;&#29992;&#29616;&#20195;&#21152;&#36895;&#22120;&#65288;&#22914;GPU&#65289;&#36827;&#34892;&#25512;&#29702;&#12290;&#19982;&#29616;&#26377;&#26367;&#20195;&#26041;&#26696;&#30456;&#27604;&#65292;PGMax&#33719;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#25512;&#29702;&#32467;&#26524;&#65292;&#25512;&#29702;&#26102;&#38388;&#21152;&#36895;&#39640;&#36798;&#19977;&#20010;&#25968;&#37327;&#32423;&#12290;PGMax&#36824;&#19982;&#24555;&#36895;&#22686;&#38271;&#30340;JAX&#29983;&#24577;&#31995;&#32479;&#26080;&#32541;&#20132;&#20114;&#65292;&#24320;&#21551;&#20102;&#26032;&#30340;&#30740;&#31350;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#12289;&#31034;&#20363;&#21644;&#25991;&#26723;&#21487;&#22312;https://github.com/deepmind/PGMax&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
PGMax is an open-source Python package for (a) easily specifying discrete Probabilistic Graphical Models (PGMs) as factor graphs; and (b) automatically running efficient and scalable loopy belief propagation (LBP) in JAX. PGMax supports general factor graphs with tractable factors, and leverages modern accelerators like GPUs for inference. Compared with existing alternatives, PGMax obtains higher-quality inference results with up to three orders-of-magnitude inference time speedups. PGMax additionally interacts seamlessly with the rapidly growing JAX ecosystem, opening up new research possibilities. Our source code, examples and documentation are available at https://github.com/deepmind/PGMax.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#30456;&#20284;&#24230;&#27169;&#22411;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#36816;&#36755;&#21644;&#36816;&#21160;&#30340;&#27169;&#25311;&#20135;&#29983;&#30340;&#26631;&#37327;&#21644;&#30690;&#37327;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;CNN&#26550;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#20307;&#31215;&#30456;&#20284;&#24230;&#24230;&#37327;&#65288;VolSiM&#65289;&#12290;</title><link>http://arxiv.org/abs/2202.04109</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23610;&#24230;CNN&#30340;&#20307;&#31215;&#27169;&#25311;&#30456;&#20284;&#24230;&#24230;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Similarity Metrics for Volumetric Simulations with Multiscale CNNs. (arXiv:2202.04109v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#30456;&#20284;&#24230;&#27169;&#22411;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#36816;&#36755;&#21644;&#36816;&#21160;&#30340;&#27169;&#25311;&#20135;&#29983;&#30340;&#26631;&#37327;&#21644;&#30690;&#37327;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;CNN&#26550;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#20307;&#31215;&#30456;&#20284;&#24230;&#24230;&#37327;&#65288;VolSiM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a similarity model based on entropy for assessing the similarity of scalar and vectorial data produced from transport and motion-based simulations, and a multiscale CNN architecture for computing a volumetric similarity metric (VolSiM).
&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#25968;&#25454;&#27169;&#25311;&#22312;&#31185;&#23398;&#20013;&#24212;&#29992;&#24191;&#27867;&#65292;&#20174;&#27969;&#20307;&#27969;&#21160;&#21040;&#31561;&#31163;&#23376;&#29289;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#30456;&#20284;&#24230;&#27169;&#22411;&#65292;&#20801;&#35768;&#21019;&#24314;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#22522;&#20934;&#36317;&#31163;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#36816;&#36755;&#21644;&#36816;&#21160;&#30340;&#27169;&#25311;&#20135;&#29983;&#30340;&#26631;&#37327;&#21644;&#30690;&#37327;&#25968;&#25454;&#30340;&#30456;&#20284;&#24230;&#12290;&#21033;&#29992;&#20174;&#35813;&#27169;&#22411;&#23548;&#20986;&#30340;&#20004;&#31181;&#25968;&#25454;&#37319;&#38598;&#26041;&#27861;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20174;&#25968;&#20540;PDE&#27714;&#35299;&#22120;&#21644;&#29616;&#26377;&#27169;&#25311;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#25910;&#38598;&#30340;&#22330;&#38598;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;CNN&#26550;&#26500;&#65292;&#29992;&#20110;&#35745;&#31639;&#20307;&#31215;&#30456;&#20284;&#24230;&#24230;&#37327;&#65288;VolSiM&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#22825;&#28982;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#27169;&#25311;&#25968;&#25454;&#30456;&#20284;&#24230;&#35780;&#20272;&#25361;&#25112;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22522;&#20110;&#30456;&#20851;&#25439;&#22833;&#20989;&#25968;&#30340;&#22823;&#25209;&#37327;&#22823;&#23567;&#21644;&#20934;&#30830;&#30456;&#20851;&#35745;&#31639;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#30740;&#31350;&#20102;&#35813;&#24230;&#37327;&#30340;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulations that produce three-dimensional data are ubiquitous in science, ranging from fluid flows to plasma physics. We propose a similarity model based on entropy, which allows for the creation of physically meaningful ground truth distances for the similarity assessment of scalar and vectorial data, produced from transport and motion-based simulations. Utilizing two data acquisition methods derived from this model, we create collections of fields from numerical PDE solvers and existing simulation data repositories. Furthermore, a multiscale CNN architecture that computes a volumetric similarity metric (VolSiM) is proposed. To the best of our knowledge this is the first learning method inherently designed to address the challenges arising for the similarity assessment of high-dimensional simulation data. Additionally, the tradeoff between a large batch size and an accurate correlation computation for correlation-based loss functions is investigated, and the metric's invariance with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#25509;&#30340;&#36125;&#21494;&#26031;&#35823;&#24046;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#20998;&#31867;&#22120;&#30340;&#26631;&#20934;&#65292;&#24182;&#21487;&#29992;&#20110;&#26816;&#27979;&#27979;&#35797;&#38598;&#36807;&#25311;&#21512;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#27169;&#22411;&#30340;&#65292;&#29978;&#33267;&#26159;&#26080;&#23454;&#20363;&#30340;&#12290;&#27492;&#22806;&#65292;&#23427;&#27809;&#26377;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#19978;&#27604;&#20960;&#20010;&#22522;&#32447;&#32473;&#20986;&#20102;&#26356;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#35823;&#24046;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2202.00395</link><description>&lt;p&gt;
&#25105;&#30340;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#26159;&#21542;&#36807;&#20110;&#20248;&#31168;&#65311;&#19968;&#31181;&#30452;&#25509;&#20272;&#35745;&#20108;&#20803;&#20998;&#31867;&#20013;&#36125;&#21494;&#26031;&#35823;&#24046;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Is the Performance of My Deep Network Too Good to Be True? A Direct Approach to Estimating the Bayes Error in Binary Classification. (arXiv:2202.00395v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#25509;&#30340;&#36125;&#21494;&#26031;&#35823;&#24046;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#20855;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#20998;&#31867;&#22120;&#30340;&#26631;&#20934;&#65292;&#24182;&#21487;&#29992;&#20110;&#26816;&#27979;&#27979;&#35797;&#38598;&#36807;&#25311;&#21512;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#27169;&#22411;&#30340;&#65292;&#29978;&#33267;&#26159;&#26080;&#23454;&#20363;&#30340;&#12290;&#27492;&#22806;&#65292;&#23427;&#27809;&#26377;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#19978;&#27604;&#20960;&#20010;&#22522;&#32447;&#32473;&#20986;&#20102;&#26356;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#35823;&#24046;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple and direct Bayes error estimator, which can be used as a criterion to evaluate classifiers with state-of-the-art performance and can be used to detect test set overfitting. Our method is model-free and even instance-free, and gives a more accurate estimate of the Bayes error than several baselines empirically.
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#23384;&#22312;&#26681;&#26412;&#38480;&#21046;&#65292;&#36825;&#26159;&#30001;&#20110;&#39044;&#27979;&#30446;&#26631;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#19981;&#30830;&#23450;&#24615;&#25152;&#33268;&#12290;&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#36125;&#21494;&#26031;&#35823;&#24046;&#26469;&#25551;&#36848;&#65292;&#23427;&#26159;&#20219;&#20309;&#20998;&#31867;&#22120;&#21487;&#20197;&#36798;&#21040;&#30340;&#26368;&#20339;&#35823;&#24046;&#12290;&#36125;&#21494;&#26031;&#35823;&#24046;&#21487;&#20197;&#29992;&#20316;&#35780;&#20272;&#20855;&#26377;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#20998;&#31867;&#22120;&#30340;&#26631;&#20934;&#65292;&#24182;&#21487;&#29992;&#20110;&#26816;&#27979;&#27979;&#35797;&#38598;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#25509;&#30340;&#36125;&#21494;&#26031;&#35823;&#24046;&#20272;&#35745;&#22120;&#65292;&#20854;&#20013;&#25105;&#20204;&#21482;&#38656;&#21462;&#26174;&#31034;&#31867;&#21035;&#20998;&#37197;&#19981;&#30830;&#23450;&#24615;&#30340;&#26631;&#31614;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#30340;&#28789;&#27963;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#21363;&#20351;&#23545;&#20110;&#24369;&#30417;&#30563;&#25968;&#25454;&#20063;&#36827;&#34892;&#36125;&#21494;&#26031;&#35823;&#24046;&#20272;&#35745;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#27169;&#22411;&#30340;&#65292;&#29978;&#33267;&#26159;&#26080;&#23454;&#20363;&#30340;&#12290;&#27492;&#22806;&#65292;&#23427;&#27809;&#26377;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;&#23454;&#35777;&#19978;&#27604;&#20960;&#20010;&#22522;&#32447;&#32473;&#20986;&#20102;&#26356;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#35823;&#24046;&#20272;&#35745;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;&#28145;&#24230;&#32593;&#32476;&#65288;&#22914;Vision Transformer m&#65289;&#30340;&#34920;&#29616;&#21487;&#33021;&#36807;&#20110;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a fundamental limitation in the prediction performance that a machine learning model can achieve due to the inevitable uncertainty of the prediction target. In classification problems, this can be characterized by the Bayes error, which is the best achievable error with any classifier. The Bayes error can be used as a criterion to evaluate classifiers with state-of-the-art performance and can be used to detect test set overfitting. We propose a simple and direct Bayes error estimator, where we just take the mean of the labels that show \emph{uncertainty} of the class assignments. Our flexible approach enables us to perform Bayes error estimation even for weakly supervised data. In contrast to others, our method is model-free and even instance-free. Moreover, it has no hyperparameters and gives a more accurate estimate of the Bayes error than several baselines empirically. Experiments using our method suggest that recently proposed deep networks such as the Vision Transformer m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#20943;&#36731;&#39640;&#23618;&#31574;&#30053;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.09635</link><description>&lt;p&gt;
&#29366;&#24577;&#26465;&#20214;&#23545;&#25239;&#23376;&#30446;&#26631;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
State-Conditioned Adversarial Subgoal Generation. (arXiv:2201.09635v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#26469;&#20943;&#36731;&#39640;&#23618;&#31574;&#30053;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel hierarchical reinforcement learning approach that mitigates the problem of non-stationary high-level policy by adversarially enforcing the generation of subgoals compatible with the current instantiation of the low-level policy, resulting in improved learning efficiency and performance in challenging continuous control tasks.
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#25552;&#20986;&#36890;&#36807;&#22312;&#26102;&#38388;&#25277;&#35937;&#30340;&#36880;&#27493;&#26356;&#39640;&#30340;&#23618;&#27425;&#19978;&#25191;&#34892;&#20915;&#31574;&#21644;&#25511;&#21046;&#26469;&#35299;&#20915;&#22256;&#38590;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#31163;&#32447;HRL&#32463;&#24120;&#36973;&#21463;&#39640;&#23618;&#31574;&#30053;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#20302;&#23618;&#31574;&#30053;&#19981;&#26029;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HRL&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#22320;&#24378;&#21046;&#39640;&#23618;&#31574;&#30053;&#29983;&#25104;&#19982;&#24403;&#21069;&#20302;&#23618;&#31574;&#30053;&#23454;&#20363;&#20860;&#23481;&#30340;&#23376;&#30446;&#26631;&#26469;&#20943;&#36731;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#25239;&#24615;&#23398;&#20064;&#26159;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;&#29366;&#24577;&#26465;&#20214;&#37492;&#21035;&#22120;&#32593;&#32476;&#21644;&#20915;&#23450;&#23376;&#30446;&#26631;&#20860;&#23481;&#24615;&#27700;&#24179;&#30340;&#39640;&#23618;&#31574;&#30053;&#26469;&#23454;&#29616;&#30340;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning (HRL) proposes to solve difficult tasks by performing decision-making and control at successively higher levels of temporal abstraction. However, off-policy HRL often suffers from the problem of a non-stationary high-level policy since the low-level policy is constantly changing. In this paper, we propose a novel HRL approach for mitigating the non-stationarity by adversarially enforcing the high-level policy to generate subgoals compatible with the current instantiation of the low-level policy. In practice, the adversarial learning is implemented by training a simple state-conditioned discriminator network concurrently with the high-level policy which determines the compatibility level of subgoals. Comparison to state-of-the-art algorithms shows that our approach improves both learning efficiency and performance in challenging continuous control tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#28369;&#22369;&#26131;&#21457;&#24615;&#12290;SNN&#27169;&#22411;&#21457;&#29616;&#22369;&#24230;&#21644;&#38477;&#27700;&#30340;&#20056;&#31215;&#20197;&#21450;&#22369;&#21521;&#26159;&#39640;&#28369;&#22369;&#26131;&#21457;&#24615;&#30340;&#37325;&#35201;&#20027;&#35201;&#36129;&#29486;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2201.06837</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22312;&#28369;&#22369;&#26131;&#21457;&#24615;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Landslide Susceptibility Modeling by Interpretable Neural Network. (arXiv:2201.06837v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#28369;&#22369;&#26131;&#21457;&#24615;&#12290;SNN&#27169;&#22411;&#21457;&#29616;&#22369;&#24230;&#21644;&#38477;&#27700;&#30340;&#20056;&#31215;&#20197;&#21450;&#22369;&#21521;&#26159;&#39640;&#28369;&#22369;&#26131;&#21457;&#24615;&#30340;&#37325;&#35201;&#20027;&#35201;&#36129;&#29486;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an interpretable neural network framework, called superposable neural network (SNN) optimization, for assessing landslide susceptibility. The SNN models found the product of slope and precipitation and hillslope aspect to be important primary contributors to high landslide susceptibility.
&lt;/p&gt;
&lt;p&gt;
&#28369;&#22369;&#30001;&#20110;&#35768;&#22810;&#26102;&#31354;&#21464;&#21270;&#22240;&#32032;&#24433;&#21709;&#32780;&#38590;&#20197;&#39044;&#27979;&#12290;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#21152;&#24615;ANN&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#28369;&#22369;&#26131;&#21457;&#24615;&#65292;&#20197;&#21450;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#32467;&#26524;&#35299;&#37322;&#25216;&#26415;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#20855;&#26377;&#23436;&#20840;&#21487;&#35299;&#37322;&#24615;&#12289;&#39640;&#20934;&#30830;&#24615;&#12289;&#39640;&#27867;&#21270;&#24615;&#21644;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#31216;&#20026;&#21487;&#21472;&#21152;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20248;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#19996;&#21916;&#39532;&#25289;&#38597;&#22320;&#21306;&#30340;&#28369;&#22369;&#28165;&#21333;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;SNN&#20248;&#20110;&#22522;&#20110;&#29289;&#29702;&#21644;&#32479;&#35745;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;SNN&#27169;&#22411;&#21457;&#29616;&#22369;&#24230;&#21644;&#38477;&#27700;&#30340;&#20056;&#31215;&#20197;&#21450;&#22369;&#21521;&#26159;&#39640;&#28369;&#22369;&#26131;&#21457;&#24615;&#30340;&#37325;&#35201;&#20027;&#35201;&#36129;&#29486;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Landslides are notoriously difficult to predict because numerous spatially and temporally varying factors contribute to slope stability. Artificial neural networks (ANN) have been shown to improve prediction accuracy but are largely uninterpretable. Here we introduce an additive ANN optimization framework to assess landslide susceptibility, as well as dataset division and outcome interpretation techniques. We refer to our approach, which features full interpretability, high accuracy, high generalizability and low model complexity, as superposable neural network (SNN) optimization. We validate our approach by training models on landslide inventory from three different easternmost Himalaya regions. Our SNN outperformed physically-based and statistical models and achieved similar performance to state-of-the-art deep neural networks. The SNN models found the product of slope and precipitation and hillslope aspect to be important primary contributors to high landslide susceptibility, which 
&lt;/p&gt;</description></item><item><title>Egeria&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;DNN&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#36339;&#36807;DNN&#23618;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26469;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#65292;&#21033;&#29992;&#21442;&#32771;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#20934;&#30830;&#35780;&#20272;&#21333;&#20010;&#23618;&#30340;&#35757;&#32451;&#21487;&#22609;&#24615;&#65292;&#24182;&#23433;&#20840;&#22320;&#20923;&#32467;&#24050;&#25910;&#25947;&#30340;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#30456;&#24212;&#30340;&#21453;&#21521;&#35745;&#31639;&#21644;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2201.06227</link><description>&lt;p&gt;
Egeria: &#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;&#23618;&#20923;&#32467;&#25216;&#26415;&#23454;&#29616;&#39640;&#25928;DNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Egeria: Efficient DNN Training with Knowledge-Guided Layer Freezing. (arXiv:2201.06227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.06227
&lt;/p&gt;
&lt;p&gt;
Egeria&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;DNN&#35757;&#32451;&#31995;&#32479;&#65292;&#36890;&#36807;&#36339;&#36807;DNN&#23618;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26469;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#65292;&#21033;&#29992;&#21442;&#32771;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#20934;&#30830;&#35780;&#20272;&#21333;&#20010;&#23618;&#30340;&#35757;&#32451;&#21487;&#22609;&#24615;&#65292;&#24182;&#23433;&#20840;&#22320;&#20923;&#32467;&#24050;&#25910;&#25947;&#30340;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#30456;&#24212;&#30340;&#21453;&#21521;&#35745;&#31639;&#21644;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Egeria is a knowledge-guided DNN training system that skips computing and communication through DNN layer freezing, accurately evaluates individual layers' training plasticity using semantic knowledge from a reference model, and safely freezes the converged ones, saving their corresponding backward computation and communication.
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;DNN&#35757;&#32451;&#31995;&#32479;Egeria&#65292;&#36890;&#36807;&#36339;&#36807;DNN&#23618;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#26469;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#20869;&#37096;DNN&#23618;&#30340;&#35757;&#32451;&#36827;&#24230;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#21069;&#23618;&#36890;&#24120;&#27604;&#28145;&#23618;&#26356;&#26089;&#22320;&#24471;&#21040;&#24456;&#22909;&#30340;&#35757;&#32451;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#35757;&#32451;&#21487;&#22609;&#24615;&#30340;&#27010;&#24565;&#65292;&#20197;&#37327;&#21270;&#20869;&#37096;DNN&#23618;&#30340;&#35757;&#32451;&#36827;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Egeria&#65292;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24341;&#23548;&#30340;DNN&#35757;&#32451;&#31995;&#32479;&#65292;&#21033;&#29992;&#21442;&#32771;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#20934;&#30830;&#35780;&#20272;&#21333;&#20010;&#23618;&#30340;&#35757;&#32451;&#21487;&#22609;&#24615;&#65292;&#24182;&#23433;&#20840;&#22320;&#20923;&#32467;&#24050;&#25910;&#25947;&#30340;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#30456;&#24212;&#30340;&#21453;&#21521;&#35745;&#31639;&#21644;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks (DNNs) is time-consuming. While most existing solutions try to overlap/schedule computation and communication for efficient training, this paper goes one step further by skipping computing and communication through DNN layer freezing. Our key insight is that the training progress of internal DNN layers differs significantly, and front layers often become well-trained much earlier than deep layers. To explore this, we first introduce the notion of training plasticity to quantify the training progress of internal DNN layers. Then we design Egeria, a knowledge-guided DNN training system that employs semantic knowledge from a reference model to accurately evaluate individual layers' training plasticity and safely freeze the converged ones, saving their corresponding backward computation and communication. Our reference model is generated on the fly using quantization techniques and runs forward operations asynchronously on available CPUs to minimize the overhe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRONTO&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;WiFi&#22522;&#30784;&#27874;&#24418;&#20013;&#25191;&#34892;&#31895;&#30053;&#30340;&#26102;&#38388;&#21644;&#39057;&#29575;&#21516;&#27493;&#65292;&#20197;&#20943;&#23569;&#21069;&#23548;&#24320;&#38144;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;&#28040;&#38500;&#20256;&#32479;&#30701;&#35757;&#32451;&#22330;&#65288;L-STF&#65289;&#26469;&#32553;&#30701;&#21069;&#23548;&#38271;&#24230;&#65292;&#24182;&#20351;&#29992;&#20854;&#20182;&#21069;&#23548;&#23383;&#27573;&#65288;&#29305;&#21035;&#26159;&#20256;&#32479;&#30340;&#38271;&#35757;&#32451;&#22330;&#65288;L-LTF&#65289;&#65289;&#25191;&#34892;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2112.10885</link><description>&lt;p&gt;
PRONTO&#65306;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#31895;&#21516;&#27493;&#20013;&#30340;&#21069;&#23548;&#24320;&#38144;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
PRONTO: Preamble Overhead Reduction with Neural Networks for Coarse Synchronization. (arXiv:2112.10885v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.10885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRONTO&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;WiFi&#22522;&#30784;&#27874;&#24418;&#20013;&#25191;&#34892;&#31895;&#30053;&#30340;&#26102;&#38388;&#21644;&#39057;&#29575;&#21516;&#27493;&#65292;&#20197;&#20943;&#23569;&#21069;&#23548;&#24320;&#38144;&#12290;&#35813;&#26041;&#26696;&#36890;&#36807;&#28040;&#38500;&#20256;&#32479;&#30701;&#35757;&#32451;&#22330;&#65288;L-STF&#65289;&#26469;&#32553;&#30701;&#21069;&#23548;&#38271;&#24230;&#65292;&#24182;&#20351;&#29992;&#20854;&#20182;&#21069;&#23548;&#23383;&#27573;&#65288;&#29305;&#21035;&#26159;&#20256;&#32479;&#30340;&#38271;&#35757;&#32451;&#22330;&#65288;L-LTF&#65289;&#65289;&#25191;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;IEEE 802.11 WiFi&#22522;&#30784;&#27874;&#24418;&#20013;&#65292;&#25509;&#25910;&#22120;&#20351;&#29992;&#21069;&#23548;&#30340;&#31532;&#19968;&#20010;&#23383;&#27573;&#65288;&#31216;&#20026;&#20256;&#32479;&#30701;&#35757;&#32451;&#22330;&#65288;L-STF&#65289;&#65289;&#25191;&#34892;&#31895;&#30053;&#30340;&#26102;&#38388;&#21644;&#39057;&#29575;&#21516;&#27493;&#12290; L-STF&#21344;&#25454;&#20102;&#21069;&#23548;&#38271;&#24230;&#30340;&#39640;&#36798;40&#65285;&#65292;&#38656;&#35201;&#38271;&#36798;32&#24494;&#31186;&#30340;&#31354;&#27668;&#26102;&#38388;&#12290;&#20026;&#20102;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#27874;&#24418;&#65292;&#20854;&#20013;&#36890;&#36807;&#28040;&#38500;L-STF&#26469;&#32553;&#30701;&#21069;&#23548;&#38271;&#24230;&#12290;&#20026;&#20102;&#35299;&#30721;&#36825;&#31181;&#20462;&#25913;&#21518;&#30340;&#27874;&#24418;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#26041;&#26696;&#65292;&#31216;&#20026;PRONTO&#65292;&#23427;&#20351;&#29992;&#20854;&#20182;&#21069;&#23548;&#23383;&#27573;&#65288;&#29305;&#21035;&#26159;&#20256;&#32479;&#30340;&#38271;&#35757;&#32451;&#22330;&#65288;L-LTF&#65289;&#65289;&#25191;&#34892;&#31895;&#30053;&#30340;&#26102;&#38388;&#21644;&#39057;&#29575;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;PRONTO&#65292;&#20854;&#20013;&#21253;&#25324;&#29992;&#20110;&#25968;&#25454;&#26816;&#27979;&#21644;&#31895;&#30053;&#36733;&#27874;&#39057;&#29575;&#20559;&#31227;&#65288;CFO&#65289;&#20272;&#35745;&#30340;&#23450;&#21046;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#20197;&#21450;&#29992;&#20110;&#24378;&#21270;&#35757;&#32451;&#30340;&#25968;&#25454;&#22686;&#24378;&#27493;&#39588;&#12290; &#65288;ii&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#20915;&#31574;&#27969;&#31243;&#65292;&#20351;PRONTO&#19982;&#21253;&#25324;th&#22312;&#20869;&#30340;&#20256;&#32479;&#27874;&#24418;&#20860;&#23481;
&lt;/p&gt;
&lt;p&gt;
In IEEE 802.11 WiFi-based waveforms, the receiver performs coarse time and frequency synchronization using the first field of the preamble known as the legacy short training field (L-STF). The L-STF occupies upto 40% of the preamble length and takes upto 32 us of airtime. With the goal of reducing communication overhead, we propose a modified waveform, where the preamble length is reduced by eliminating the L-STF. To decode this modified waveform, we propose a neural network (NN)-based scheme called PRONTO that performs coarse time and frequency estimations using other preamble fields, specifically the legacy long training field (L-LTF). Our contributions are threefold: (i) We present PRONTO featuring customized convolutional neural networks (CNNs) for packet detection and coarse carrier frequency offset (CFO) estimation, along with data augmentation steps for robust training. (ii) We propose a generalized decision flow that makes PRONTO compatible with legacy waveforms that include th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WGE&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#21333;&#19968;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#22270;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.09231</link><description>&lt;p&gt;
&#20004;&#35270;&#35282;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Two-view Graph Neural Networks for Knowledge Graph Completion. (arXiv:2112.09231v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.09231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WGE&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#20010;&#21333;&#19968;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#22270;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a graph neural network model named WGE, which learns vector representations of entities and relations from two single entity- and relation-focused graphs, and achieves excellent performance on knowledge graph completion task.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;WGE&#65292;&#20197;&#25429;&#25417;&#23454;&#20307;&#21644;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#22270;&#32467;&#26500;&#12290;&#32473;&#23450;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;WGE&#26500;&#24314;&#19968;&#20010;&#21333;&#19968;&#30340;&#26080;&#21521;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#22270;&#65292;&#23558;&#23454;&#20307;&#35270;&#20026;&#33410;&#28857;&#12290;WGE&#36824;&#20174;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#32422;&#26463;&#26465;&#20214;&#26500;&#24314;&#21478;&#19968;&#20010;&#21333;&#19968;&#30340;&#26080;&#21521;&#22270;&#65292;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#35270;&#20026;&#33410;&#28857;&#12290;&#28982;&#21518;&#65292;WGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GNN&#30340;&#26550;&#26500;&#65292;&#20174;&#36825;&#20004;&#20010;&#21333;&#19968;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#20026;&#20013;&#24515;&#30340;&#22270;&#20013;&#26356;&#22909;&#22320;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;WGE&#23558;&#23398;&#20064;&#21040;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#39304;&#36865;&#21040;&#21152;&#26435;&#24471;&#20998;&#20989;&#25968;&#20013;&#65292;&#20197;&#36820;&#22238;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#19977;&#20803;&#32452;&#24471;&#20998;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WGE&#22312;&#19971;&#20010;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an effective graph neural network (GNN)-based knowledge graph embedding model, which we name WGE, to capture entity- and relation-focused graph structures. Given a knowledge graph, WGE builds a single undirected entity-focused graph that views entities as nodes. WGE also constructs another single undirected graph from relation-focused constraints, which views entities and relations as nodes. WGE then proposes a GNN-based architecture to better learn vector representations of entities and relations from these two single entity- and relation-focused graphs. WGE feeds the learned entity and relation representations into a weighted score function to return the triple scores for knowledge graph completion. Experimental results show that WGE outperforms strong baselines on seven benchmark datasets for knowledge graph completion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#25512;&#26029;&#29992;&#25143;&#20301;&#32622;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;&#20301;&#32622;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.03452</link><description>&lt;p&gt;
&#32852;&#37030;&#20449;&#21495;&#22320;&#22270;&#20013;&#30340;&#20301;&#32622;&#27844;&#38706;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Location Leakage in Federated Signal Maps. (arXiv:2112.03452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#25512;&#26029;&#29992;&#25143;&#20301;&#32622;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;&#20301;&#32622;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of inferring user location through gradient leakage attacks in the federated learning framework, and proposes a method to protect location privacy.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20174;&#22810;&#20010;&#31227;&#21160;&#35774;&#22791;&#25910;&#38598;&#30340;&#27979;&#37327;&#25968;&#25454;&#20013;&#39044;&#27979;&#34562;&#31389;&#32593;&#32476;&#24615;&#33021;&#65288;&#20449;&#21495;&#22320;&#22270;&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20869;&#21046;&#23450;&#20102;&#38382;&#39064;&#65306;&#65288;i&#65289;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#29992;&#25143;&#33021;&#22815;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#35757;&#32451;&#25968;&#25454;&#22312;&#20854;&#35774;&#22791;&#19978;&#65307;&#65288;ii&#65289;&#27979;&#37327;&#25968;&#25454;&#26159;&#38543;&#30528;&#29992;&#25143;&#38543;&#26102;&#38388;&#31227;&#21160;&#32780;&#25910;&#38598;&#30340;&#65292;&#24182;&#20197;&#22312;&#32447;&#26041;&#24335;&#29992;&#20110;&#26412;&#22320;&#35757;&#32451;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#35802;&#23454;&#20294;&#22909;&#22855;&#30340;&#26381;&#21153;&#22120;&#65292;&#35266;&#23519;&#21442;&#19982;FL&#30340;&#30446;&#26631;&#29992;&#25143;&#30340;&#26356;&#26032;&#24182;&#20351;&#29992;&#26799;&#24230;&#27844;&#28431;&#65288;DLG&#65289;&#31867;&#22411;&#30340;&#25915;&#20987;&#25512;&#26029;&#20182;&#20204;&#30340;&#20301;&#32622;&#65292;&#35813;&#25915;&#20987;&#26368;&#21021;&#26159;&#20026;&#37325;&#26500;DNN&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#25968;&#25454;&#32780;&#24320;&#21457;&#30340;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#20851;&#38190;&#35266;&#23519;&#65292;&#21363;DLG&#25915;&#20987;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#35774;&#32622;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#26412;&#22320;&#25968;&#25454;&#25209;&#27425;&#30340;&#24179;&#22343;&#20301;&#32622;&#65292;&#24182;&#22240;&#27492;&#21487;&#20197;&#29992;&#20110;&#22312;&#31895;&#30053;&#31890;&#24230;&#19978;&#37325;&#26500;&#30446;&#26631;&#29992;&#25143;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#26469;&#20445;&#25252;&#20301;&#32622;&#38544;&#31169;&#65292;&#22312;&#25105;&#20204;&#30340;s&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of predicting cellular network performance (signal maps) from measurements collected by several mobile devices. We formulate the problem within the online federated learning framework: (i) federated learning (FL) enables users to collaboratively train a model, while keeping their training data on their devices; (ii) measurements are collected as users move around over time and are used for local training in an online fashion. We consider an honest-but-curious server, who observes the updates from target users participating in FL and infers their location using a deep leakage from gradients (DLG) type of attack, originally developed to reconstruct training data of DNN image classifiers. We make the key observation that a DLG attack, applied to our setting, infers the average location of a batch of local data, and can thus be used to reconstruct the target users' trajectory at a coarse granularity. We build on this observation to protect location privacy, in our s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FLSys&#65292;&#19968;&#20010;&#31227;&#21160;-&#20113;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#25104;&#20026;FL&#27169;&#22411;&#21644;&#24212;&#29992;&#31243;&#24207;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;FLSys&#26088;&#22312;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#31227;&#21160;&#24863;&#27979;&#25968;&#25454;&#12290;&#23427;&#24179;&#34913;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#23481;&#24525;&#36890;&#20449;&#25925;&#38556;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;FLSys&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;API&#65292;&#20379;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;FL&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2111.09445</link><description>&lt;p&gt;
FLSys&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#31227;&#21160;&#24212;&#29992;&#30340;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FLSys: Toward an Open Ecosystem for Federated Learning Mobile Apps. (arXiv:2111.09445v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLSys&#65292;&#19968;&#20010;&#31227;&#21160;-&#20113;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#25104;&#20026;FL&#27169;&#22411;&#21644;&#24212;&#29992;&#31243;&#24207;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;FLSys&#26088;&#22312;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#31227;&#21160;&#24863;&#27979;&#25968;&#25454;&#12290;&#23427;&#24179;&#34913;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#23481;&#24525;&#36890;&#20449;&#25925;&#38556;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;FLSys&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;API&#65292;&#20379;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;FL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces FLSys, a mobile-cloud federated learning (FL) system that can be a key component for an open ecosystem of FL models and apps. FLSys is designed to work on smart phones with mobile sensing data. It balances model performance with resource consumption, tolerates communication failures, and achieves scalability. FLSys provides advanced privacy preserving mechanisms and a common API for third-party app developers to access FL models.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FLSys&#30340;&#35774;&#35745;&#12289;&#23454;&#29616;&#21644;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#31227;&#21160;-&#20113;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#65292;&#21487;&#20197;&#25104;&#20026;FL&#27169;&#22411;&#21644;&#24212;&#29992;&#31243;&#24207;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;FLSys&#26088;&#22312;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#20351;&#29992;&#31227;&#21160;&#24863;&#27979;&#25968;&#25454;&#12290;&#23427;&#24179;&#34913;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#36164;&#28304;&#28040;&#32791;&#65292;&#23481;&#24525;&#36890;&#20449;&#25925;&#38556;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;FLSys&#20013;&#65292;&#19981;&#21516;&#30340;DL&#27169;&#22411;&#21644;&#19981;&#21516;&#30340;FL&#32858;&#21512;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#34987;&#19981;&#21516;&#30340;&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;&#21644;&#35775;&#38382;&#12290;&#27492;&#22806;&#65292;FLSys&#25552;&#20379;&#20102;&#20808;&#36827;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#21644;&#19968;&#20010;&#36890;&#29992;&#30340;API&#65292;&#20379;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#20154;&#21592;&#35775;&#38382;FL&#27169;&#22411;&#12290;FLSys&#37319;&#29992;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#23454;&#29616;&#22312;Android&#21644;AWS&#20113;&#20013;&#12290;&#25105;&#20204;&#19982;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#27169;&#22411;&#20849;&#21516;&#35774;&#35745;&#20102;FLSys&#12290;&#22312;4&#20010;&#26376;&#30340;&#26102;&#38388;&#37324;&#65292;&#20174;100&#22810;&#21517;&#22823;&#23398;&#29983;&#20013;&#25910;&#38598;&#20102;HAR&#24863;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;HAR-Wild&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#31227;&#21160;&#35774;&#22791;&#37327;&#36523;&#23450;&#21046;&#30340;CNN&#27169;&#22411;&#65292;&#20855;&#26377;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#20197;&#20943;&#36731;p
&lt;/p&gt;
&lt;p&gt;
This article presents the design, implementation, and evaluation of FLSys, a mobile-cloud federated learning (FL) system, which can be a key component for an open ecosystem of FL models and apps. FLSys is designed to work on smart phones with mobile sensing data. It balances model performance with resource consumption, tolerates communication failures, and achieves scalability. In FLSys, different DL models with different FL aggregation methods can be trained and accessed concurrently by different apps. Furthermore, FLSys provides advanced privacy preserving mechanisms and a common API for third-party app developers to access FL models. FLSys adopts a modular design and is implemented in Android and AWS cloud. We co-designed FLSys with a human activity recognition (HAR) model. HAR sensing data was collected in the wild from 100+ college students during a 4-month period. We implemented HAR-Wild, a CNN model tailored to mobile devices, with a data augmentation mechanism to mitigate the p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FXAM&#30340;&#32479;&#19968;&#19988;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#21487;&#21152;&#27169;&#22411;&#25193;&#23637;&#20102;GAM&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#29992;&#20110;&#25968;&#20540;&#12289;&#20998;&#31867;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;FXAM&#36827;&#34892;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#31216;&#20026;&#19977;&#38454;&#27573;&#36845;&#20195;&#65288;TSI&#65289;&#65292;&#20998;&#21035;&#23545;&#25968;&#20540;&#12289;&#20998;&#31867;&#21644;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#23398;&#20064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2111.08255</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#19988;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#20998;&#26512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Unified and Fast Interpretable Model for Predictive Analytics. (arXiv:2111.08255v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FXAM&#30340;&#32479;&#19968;&#19988;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#20998;&#26512;&#27169;&#22411;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#21487;&#21152;&#27169;&#22411;&#25193;&#23637;&#20102;GAM&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#29992;&#20110;&#25968;&#20540;&#12289;&#20998;&#31867;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;FXAM&#36827;&#34892;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#31216;&#20026;&#19977;&#38454;&#27573;&#36845;&#20195;&#65288;TSI&#65289;&#65292;&#20998;&#21035;&#23545;&#25968;&#20540;&#12289;&#20998;&#31867;&#21644;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#23398;&#20064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a unified and fast interpretable model for predictive analytics called FXAM, which extends GAM's modeling capability with a unified additive model for numerical, categorical, and temporal features. FXAM conducts a novel training procedure called Three-Stage Iteration (TSI) and achieves high accuracy and training efficiency.
&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20998;&#26512;&#26088;&#22312;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#39044;&#27979;&#34892;&#20026;&#27169;&#24335;&#24182;&#20351;&#29992;&#39044;&#27979;&#25351;&#23548;&#20915;&#31574;&#12290;&#30001;&#20110;&#39044;&#27979;&#20998;&#26512;&#38656;&#35201;&#20154;&#31867;&#21442;&#19982;&#65292;&#22240;&#27492;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26368;&#22909;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#65288;GAM&#65289;&#26159;&#35299;&#37322;&#24615;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24120;&#35265;&#30340;&#19968;&#23545;&#22810;&#21644;&#22810;&#23545;&#19968;&#29616;&#35937;&#65292;&#29616;&#26377;&#30340;GAM&#22312;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#37117;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FXAM&#65288;&#24555;&#36895;&#19988;&#21487;&#35299;&#37322;&#30340;&#21487;&#21152;&#27169;&#22411;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32479;&#19968;&#19988;&#24555;&#36895;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#20998;&#26512;&#27169;&#22411;&#12290;FXAM&#36890;&#36807;&#32479;&#19968;&#30340;&#21487;&#21152;&#27169;&#22411;&#25193;&#23637;&#20102;GAM&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#29992;&#20110;&#25968;&#20540;&#12289;&#20998;&#31867;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;FXAM&#36827;&#34892;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#31216;&#20026;&#19977;&#38454;&#27573;&#36845;&#20195;&#65288;TSI&#65289;&#12290;TSI&#20998;&#21035;&#23545;&#25968;&#20540;&#12289;&#20998;&#31867;&#21644;&#26102;&#38388;&#29305;&#24449;&#36827;&#34892;&#23398;&#20064;&#12290;&#27599;&#20010;&#38454;&#27573;&#23398;&#20064;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive analytics aims to build machine learning models to predict behavior patterns and use predictions to guide decision-making. Predictive analytics is human involved, thus the machine learning model is preferred to be interpretable. In literature, Generalized Additive Model (GAM) is a standard for interpretability. However, due to the one-to-many and many-to-one phenomena which appear commonly in real-world scenarios, existing GAMs have limitations to serve predictive analytics in terms of both accuracy and training efficiency. In this paper, we propose FXAM (Fast and eXplainable Additive Model), a unified and fast interpretable model for predictive analytics. FXAM extends GAM's modeling capability with a unified additive model for numerical, categorical, and temporal features. FXAM conducts a novel training procedure called Three-Stage Iteration (TSI). TSI corresponds to learning over numerical, categorical, and temporal features respectively. Each stage learns a local optimum 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#34892;&#21160;&#24433;&#21709;&#35268;&#24459;&#21644;&#22806;&#29983;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#25104;&#31435;&#65292;&#21253;&#25324;&#37329;&#34701;&#24066;&#22330;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#21516;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2111.08066</link><description>&lt;p&gt;
&#21033;&#29992;&#34892;&#21160;&#24433;&#21709;&#35268;&#24459;&#21644;&#22806;&#29983;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Action Impact Regularity and Exogenous State Variables for Offline Reinforcement Learning. (arXiv:2111.08066v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#34892;&#21160;&#24433;&#21709;&#35268;&#24459;&#21644;&#22806;&#29983;&#29366;&#24577;&#21464;&#37327;&#36827;&#34892;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#25104;&#31435;&#65292;&#21253;&#25324;&#37329;&#34701;&#24066;&#22330;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#21516;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an algorithm for offline reinforcement learning that exploits the Action Impact Regularity (AIR) property, which holds in many real-world domains including financial markets, and outperforms existing algorithms across different data collection policies in simulated and real world.
&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#8212;&#8212;&#20174;&#19968;&#25209;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#8212;&#8212;&#24050;&#30693;&#23545;&#20110;&#19968;&#33324;&#30340;MDP&#26469;&#35828;&#26159;&#22256;&#38590;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#20851;&#27880;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21487;&#33021;&#21487;&#34892;&#30340;&#29305;&#23450;&#31867;&#21035;&#30340;MDP&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31867;&#21463;&#38480;&#21046;&#30340;MDP&#65292;&#20197;&#33719;&#24471;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#34892;&#21160;&#24433;&#21709;&#35268;&#24459;&#65288;AIR&#65289;&#30340;&#20851;&#38190;&#23646;&#24615;&#26159;&#65292;&#34892;&#21160;&#20027;&#35201;&#24433;&#21709;&#29366;&#24577;&#30340;&#19968;&#37096;&#20998;&#65288;&#20869;&#29983;&#32452;&#20214;&#65289;&#65292;&#24182;&#19988;&#23545;&#29366;&#24577;&#30340;&#20854;&#20313;&#37096;&#20998;&#65288;&#22806;&#29983;&#32452;&#20214;&#65289;&#24433;&#21709;&#26377;&#38480;&#12290;AIR&#26159;&#19968;&#20010;&#24378;&#20551;&#35774;&#65292;&#20294;&#23427;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#39046;&#22495;&#20013;&#20173;&#28982;&#25104;&#31435;&#65292;&#21253;&#25324;&#37329;&#34701;&#24066;&#22330;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;AIR&#23646;&#24615;&#30340;&#31639;&#27861;&#65292;&#24182;&#20026;&#22522;&#20110;Fitted-Q&#36845;&#20195;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#19981;&#21516;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning -- learning a policy from a batch of data -is known to be hard for general MDPs. These results motivate the need to look at specific classes of MDPs where offline reinforcement learning might be feasible. In this work, we explore a restricted class of MDPs to obtain guarantees for offline reinforcement learning. The key property, which we call Action Impact Regularity (AIR), is that actions primarily impact a part of the state (an endogenous component) and have limited impact on the remaining part of the state (an exogenous component). AIR is a strong assumption, but it nonetheless holds in a number of real-world domains including financial markets. We discuss algorithms that exploit the AIR property, and provide a theoretical analysis for an algorithm based on Fitted-Q Iteration. Finally, we demonstrate that the algorithm outperforms existing offline reinforcement learning algorithms across different data collection policies in simulated and real world
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#23398;&#20064;&#29575;&#26102;&#38388;&#34920;&#65292;&#21487;&#20197;&#22823;&#22823;&#32553;&#30701;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#32447;&#24615;&#26102;&#38388;&#34920;&#30340;&#21021;&#22987;&#20540;&#65292;&#24182;&#23545;&#21021;&#22987;&#23494;&#38598;&#35757;&#32451;&#38454;&#27573;&#26045;&#21152;&#39044;&#31639;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2111.00843</link><description>&lt;p&gt;
&#22914;&#20309;&#23398;&#20250;&#19981;&#20877;&#25285;&#24515;&#24182;&#28909;&#29233;&#37325;&#26032;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
How I Learned to Stop Worrying and Love Retraining. (arXiv:2111.00843v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.00843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#23398;&#20064;&#29575;&#26102;&#38388;&#34920;&#65292;&#21487;&#20197;&#22823;&#22823;&#32553;&#30701;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#32447;&#24615;&#26102;&#38388;&#34920;&#30340;&#21021;&#22987;&#20540;&#65292;&#24182;&#23545;&#21021;&#22987;&#23494;&#38598;&#35757;&#32451;&#38454;&#27573;&#26045;&#21152;&#39044;&#31639;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a simple linear learning rate schedule that can significantly shorten the retraining phase, and a method to adaptively select the initial value of the linear schedule, as well as imposing a budget on the initial dense training phase, improving on existing retraining approaches.
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#26041;&#27861;&#21253;&#25324;&#20960;&#20010;&#36845;&#20195;&#30340;&#35757;&#32451;&#21644;&#21098;&#26525;&#27493;&#39588;&#65292;&#30475;&#20284;&#22312;&#21098;&#26525;&#21518;&#22833;&#21435;&#20102;&#22823;&#37327;&#24615;&#33021;&#65292;&#28982;&#21518;&#22312;&#38543;&#21518;&#30340;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#24674;&#22797;&#20102;&#23427;&#12290;&#26368;&#36817;&#30340;Renda&#31561;&#20154;&#65288;2020&#65289;&#21644;Le&#65286;Hua&#65288;2021&#65289;&#30340;&#20316;&#21697;&#23637;&#31034;&#20102;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#20013;&#23398;&#20064;&#29575;&#35843;&#24230;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36873;&#25321;IMP&#65288;Han&#31561;&#20154;&#65292;2015&#65289;&#30340;&#36825;&#31181;&#35843;&#24230;&#30340;&#29305;&#23450;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#21457;&#29616;&#32622;&#20110;Li&#31561;&#20154;&#65288;2020&#65289;&#20851;&#20110;&#22312;&#22266;&#23450;&#35757;&#32451;&#39044;&#31639;&#20869;&#35757;&#32451;&#27169;&#22411;&#30340;&#32467;&#26524;&#30340;&#32972;&#26223;&#19979;&#65292;&#24182;&#35777;&#26126;&#65292;&#22240;&#27492;&#65292;&#21487;&#20197;&#20351;&#29992;&#31616;&#21333;&#30340;&#32447;&#24615;&#23398;&#20064;&#29575;&#26102;&#38388;&#34920;&#22823;&#22823;&#32553;&#30701;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#12290;&#22312;&#29616;&#26377;&#30340;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#32447;&#24615;&#26102;&#38388;&#34920;&#30340;&#21021;&#22987;&#20540;&#12290;&#26356;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#31867;&#20284;&#22320;&#23545;&#21021;&#22987;&#23494;&#38598;&#35757;&#32451;&#38454;&#27573;&#26045;&#21152;&#39044;&#31639;&#65292;&#24182;&#23637;&#31034;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many Neural Network Pruning approaches consist of several iterative training and pruning steps, seemingly losing a significant amount of their performance after pruning and then recovering it in the subsequent retraining phase. Recent works of Renda et al. (2020) and Le &amp; Hua (2021) demonstrate the significance of the learning rate schedule during the retraining phase and propose specific heuristics for choosing such a schedule for IMP (Han et al., 2015). We place these findings in the context of the results of Li et al. (2020) regarding the training of models within a fixed training budget and demonstrate that, consequently, the retraining phase can be massively shortened using a simple linear learning rate schedule. Improving on existing retraining approaches, we additionally propose a method to adaptively select the initial value of the linear schedule. Going a step further, we propose similarly imposing a budget on the initial dense training phase and show that the resulting simple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12289;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#65292;&#20840;&#38754;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#25552;&#20379;&#20102;&#36873;&#25321;&#36866;&#24403;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2110.15829</link><description>&lt;p&gt;
&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Holistic Deep Learning. (arXiv:2110.15829v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#20915;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12289;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#65292;&#20840;&#38754;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#25552;&#20379;&#20102;&#36873;&#25321;&#36866;&#24403;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a holistic deep learning framework that addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. A prescriptive approach is provided to support practitioners in selecting an appropriate training loss function based on their specific objectives.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#38754;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23545;&#36755;&#20837;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#12289;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#26469;&#33258;&#19981;&#21516;&#35757;&#32451;&#39564;&#35777;&#25286;&#20998;&#30340;&#24615;&#33021;&#19981;&#31283;&#23450;&#24615;&#31561;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#20840;&#38754;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#31232;&#30095;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#36825;&#22312;&#23545;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#32467;&#26524;&#36827;&#19968;&#27493;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#21644;SHAP&#20540;&#20998;&#26512;&#36827;&#34892;&#39564;&#35777;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#35780;&#20272;&#25351;&#26631;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#21644;&#26435;&#34913;&#12290;&#20026;&#20102;&#25903;&#25345;&#23454;&#36341;&#32773;&#24212;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#25351;&#23548;&#24615;&#26041;&#27861;&#65292;&#26681;&#25454;&#20182;&#20204;&#30340;&#20855;&#20307;&#30446;&#26631;&#65292;&#25552;&#20379;&#36873;&#25321;&#36866;&#24403;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#30340;&#24314;&#35758;&#12290;&#25152;&#26377;&#29992;&#20110;&#37325;&#29616;&#32467;&#26524;&#30340;&#20195;&#30721;&#37117;&#21487;&#20197;&#22312;https://github.com/kimvc7/HDL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel holistic deep learning framework that simultaneously addresses the challenges of vulnerability to input perturbations, overparametrization, and performance instability from different train-validation splits. The proposed framework holistically improves accuracy, robustness, sparsity, and stability over standard deep learning models, as demonstrated by extensive experiments on both tabular and image data sets. The results are further validated by ablation experiments and SHAP value analysis, which reveal the interactions and trade-offs between the different evaluation metrics. To support practitioners applying our framework, we provide a prescriptive approach that offers recommendations for selecting an appropriate training loss function based on their specific objectives. All the code to reproduce the results can be found at https://github.com/kimvc7/HDL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProxyBO&#30340;&#39640;&#25928;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#38646;&#25104;&#26412;&#20195;&#29702;&#21152;&#36895;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#24191;&#20041;&#33021;&#21147;&#27979;&#37327;&#20272;&#35745;&#20195;&#29702;&#22312;&#20219;&#21153;&#19978;&#30340;&#36866;&#24212;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#36141;&#20989;&#25968;&#65292;&#22522;&#20110;&#23427;&#20204;&#30340;&#21160;&#24577;&#24433;&#21709;&#23558;BO&#19982;&#38646;&#25104;&#26412;&#20195;&#29702;&#30456;&#32467;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;ProxyBO&#22312;&#20116;&#20010;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2110.10423</link><description>&lt;p&gt;
ProxyBO: &#36890;&#36807;&#38646;&#25104;&#26412;&#20195;&#29702;&#21152;&#36895;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
ProxyBO: Accelerating Neural Architecture Search via Bayesian Optimization with Zero-cost Proxies. (arXiv:2110.10423v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.10423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ProxyBO&#30340;&#39640;&#25928;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#38646;&#25104;&#26412;&#20195;&#29702;&#21152;&#36895;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#24191;&#20041;&#33021;&#21147;&#27979;&#37327;&#20272;&#35745;&#20195;&#29702;&#22312;&#20219;&#21153;&#19978;&#30340;&#36866;&#24212;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#36141;&#20989;&#25968;&#65292;&#22522;&#20110;&#23427;&#20204;&#30340;&#21160;&#24577;&#24433;&#21709;&#23558;BO&#19982;&#38646;&#25104;&#26412;&#20195;&#29702;&#30456;&#32467;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;ProxyBO&#22312;&#20116;&#20010;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an efficient Bayesian optimization framework called ProxyBO, which utilizes zero-cost proxies to accelerate neural architecture search, estimates the fitness of proxies on the task during each iteration using generalization ability measurement, and designs a novel acquisition function to combine BO with zero-cost proxies based on their dynamic influence. Extensive empirical studies show that ProxyBO consistently outperforms competitive baselines on five tasks from three public benchmarks.
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#31070;&#32463;&#26550;&#26500;&#38656;&#35201;&#24040;&#22823;&#30340;&#20154;&#24037;&#21162;&#21147;&#12290;&#36825;&#20419;&#36827;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30340;&#21457;&#23637;&#65292;&#20197;&#33258;&#21160;&#21270;&#35774;&#35745;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;NAS&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#36816;&#34892;&#36895;&#24230;&#36739;&#24930;&#65292;&#32780;&#38646;&#25104;&#26412;&#20195;&#29702;&#36816;&#34892;&#26497;&#24555;&#65292;&#20294;&#21069;&#26223;&#19981;&#22826;&#20048;&#35266;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#36825;&#20123;&#38646;&#25104;&#26412;&#20195;&#29702;&#21152;&#36895;NAS&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#29616;&#26377;&#26041;&#27861;&#26377;&#20004;&#20010;&#38480;&#21046;&#65292;&#21363;&#19981;&#21487;&#39044;&#35265;&#30340;&#21487;&#38752;&#24615;&#21644;&#19968;&#27425;&#24615;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ProxyBO&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#38646;&#25104;&#26412;&#20195;&#29702;&#21152;&#36895;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#25105;&#20204;&#24212;&#29992;&#24191;&#20041;&#33021;&#21147;&#27979;&#37327;&#26469;&#20272;&#35745;&#27599;&#27425;&#36845;&#20195;&#20013;&#20195;&#29702;&#22312;&#20219;&#21153;&#19978;&#30340;&#36866;&#24212;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#36141;&#20989;&#25968;&#65292;&#22522;&#20110;&#23427;&#20204;&#30340;&#21160;&#24577;&#24433;&#21709;&#23558;BO&#19982;&#38646;&#25104;&#26412;&#20195;&#29702;&#30456;&#32467;&#21512;&#12290;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;ProxyBO&#22312;&#26469;&#33258;&#19977;&#20010;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#30340;&#20116;&#20010;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing neural architectures requires immense manual efforts. This has promoted the development of neural architecture search (NAS) to automate the design. While previous NAS methods achieve promising results but run slowly, zero-cost proxies run extremely fast but are less promising. Therefore, it is of great potential to accelerate NAS via those zero-cost proxies. The existing method has two limitations, which are unforeseeable reliability and one-shot usage. To address the limitations, we present ProxyBO, an efficient Bayesian optimization (BO) framework that utilizes the zero-cost proxies to accelerate neural architecture search. We apply the generalization ability measurement to estimate the fitness of proxies on the task during each iteration and design a novel acquisition function to combine BO with zero-cost proxies based on their dynamic influence. Extensive empirical studies show that ProxyBO consistently outperforms competitive baselines on five tasks from three public ben
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#24615;&#37325;&#32534;&#31243;&#21487;&#20197;&#37325;&#26032;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#25104;&#21151;&#21462;&#20915;&#20110;&#24179;&#22343;&#36755;&#20837;&#26799;&#24230;&#30340;&#22823;&#23567;&#65292;&#24403;&#36755;&#20837;&#26799;&#24230;&#26356;&#21152;&#23545;&#40784;&#19988;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#32500;&#24230;&#26102;&#65292;&#24179;&#22343;&#36755;&#20837;&#26799;&#24230;&#20250;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2108.11673</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#23545;&#25239;&#24615;&#37325;&#32534;&#31243;&#26377;&#25928;&#65292;&#20309;&#26102;&#20250;&#22833;&#36133;&#65292;&#20197;&#21450;&#22914;&#20309;&#21306;&#20998;&#20108;&#32773;
&lt;/p&gt;
&lt;p&gt;
Why Adversarial Reprogramming Works, When It Fails, and How to Tell the Difference. (arXiv:2108.11673v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11673
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#37325;&#32534;&#31243;&#21487;&#20197;&#37325;&#26032;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#25104;&#21151;&#21462;&#20915;&#20110;&#24179;&#22343;&#36755;&#20837;&#26799;&#24230;&#30340;&#22823;&#23567;&#65292;&#24403;&#36755;&#20837;&#26799;&#24230;&#26356;&#21152;&#23545;&#40784;&#19988;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#32500;&#24230;&#26102;&#65292;&#24179;&#22343;&#36755;&#20837;&#26799;&#24230;&#20250;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial reprogramming can repurpose machine-learning models to perform different tasks, and its success depends on the size of the average input gradient, which grows when input gradients are more aligned and when inputs have higher dimensionality.
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#37325;&#32534;&#31243;&#20801;&#35768;&#37325;&#26032;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;&#22312;&#25552;&#20379;&#30340;&#25968;&#23383;&#22270;&#20687;&#20013;&#23884;&#20837;&#23545;&#25239;&#24615;&#31243;&#24207;&#65292;&#21487;&#20197;&#23558;&#35757;&#32451;&#29992;&#20110;&#35782;&#21035;&#21160;&#29289;&#30340;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#20026;&#35782;&#21035;&#25968;&#23383;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#25239;&#24615;&#37325;&#32534;&#31243;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#28389;&#29992;&#20316;&#20026;&#26381;&#21153;&#25552;&#20379;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#36824;&#21487;&#20197;&#26377;&#30410;&#22320;&#25913;&#21892;&#36801;&#31227;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#24433;&#21709;&#20854;&#25104;&#21151;&#30340;&#22240;&#32032;&#20173;&#28982;&#22823;&#22810;&#26410;&#34987;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23545;&#25239;&#24615;&#37325;&#32534;&#31243;&#30340;&#19968;&#38454;&#32447;&#24615;&#27169;&#22411;&#65292;&#20197;&#34920;&#26126;&#20854;&#25104;&#21151;&#26412;&#36136;&#19978;&#21462;&#20915;&#20110;&#24179;&#22343;&#36755;&#20837;&#26799;&#24230;&#30340;&#22823;&#23567;&#65292;&#24403;&#36755;&#20837;&#26799;&#24230;&#26356;&#21152;&#23545;&#40784;&#19988;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#32500;&#24230;&#26102;&#65292;&#24179;&#22343;&#36755;&#20837;&#26799;&#24230;&#20250;&#22686;&#38271;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20998;&#26512;&#32467;&#26524;&#28041;&#21450;14&#20010;&#19981;&#21516;&#30340;&#37325;&#32534;&#31243;&#20219;&#21153;&#65292;&#34920;&#26126;&#19978;&#36848;&#22240;&#32032;&#19982;&#23545;&#25239;&#24615;&#37325;&#32534;&#31243;&#30340;&#25104;&#21151;&#21644;&#22833;&#36133;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial reprogramming allows repurposing a machine-learning model to perform a different task. For example, a model trained to recognize animals can be reprogrammed to recognize digits by embedding an adversarial program in the digit images provided as input. Recent work has shown that adversarial reprogramming may not only be used to abuse machine-learning models provided as a service, but also beneficially, to improve transfer learning when training data is scarce. However, the factors affecting its success are still largely unexplained. In this work, we develop a first-order linear model of adversarial reprogramming to show that its success inherently depends on the size of the average input gradient, which grows when input gradients are more aligned, and when inputs have higher dimensionality. The results of our experimental analysis, involving fourteen distinct reprogramming tasks, show that the above factors are correlated with the success and the failure of adversarial repro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#27491;&#21017;&#21270;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#26631;&#35760;&#30340;&#22270;&#20013;&#20998;&#31867;&#26032;&#28155;&#21152;&#30340;&#33410;&#28857;&#65292;&#36890;&#36807;&#32858;&#21512;&#20854;&#30456;&#37051;&#33410;&#28857;&#30340;&#20449;&#24687;&#29983;&#25104;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2106.03393</link><description>&lt;p&gt;
&#23545;&#25239;&#27491;&#21017;&#21270;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#37096;&#20998;&#26631;&#35760;&#22270;&#30340;&#24402;&#32435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversarially Regularized Graph Attention Networks for Inductive Learning on Partially Labeled Graphs. (arXiv:2106.03393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#27491;&#21017;&#21270;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#26631;&#35760;&#30340;&#22270;&#20013;&#20998;&#31867;&#26032;&#28155;&#21152;&#30340;&#33410;&#28857;&#65292;&#36890;&#36807;&#32858;&#21512;&#20854;&#30456;&#37051;&#33410;&#28857;&#30340;&#20449;&#24687;&#29983;&#25104;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an adversarially regularized graph attention model for classifying newly added nodes in a partially labeled graph, which generates the representation of a node by aggregating information from its neighboring nodes and naturally generalizes to previously unseen nodes. Adversarial training is employed to improve the model's robustness and generalization ability.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#26631;&#35760;&#30340;&#39640;&#25104;&#26412;&#32463;&#24120;&#23548;&#33268;&#33410;&#28857;&#26631;&#35760;&#30701;&#32570;&#12290;&#20026;&#20102;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#22522;&#20110;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#20016;&#23500;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#19982;&#31232;&#32570;&#30340;&#21487;&#29992;&#26631;&#35760;&#33410;&#28857;&#19968;&#36215;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#38656;&#35201;&#25152;&#26377;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#35201;&#39044;&#27979;&#30340;&#33410;&#28857;&#65292;&#36825;&#22312;&#20855;&#26377;&#26032;&#28155;&#21152;&#33410;&#28857;&#30340;&#21160;&#24577;&#22270;&#20013;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#27491;&#21017;&#21270;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#26631;&#35760;&#30340;&#22270;&#20013;&#20998;&#31867;&#26032;&#28155;&#21152;&#30340;&#33410;&#28857;&#12290;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32858;&#21512;&#22120;&#65292;&#36890;&#36807;&#32858;&#21512;&#20854;&#30456;&#37051;&#33410;&#28857;&#30340;&#20449;&#24687;&#29983;&#25104;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#33410;&#28857;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#36890;&#36807;&#24378;&#21046;&#33410;&#28857;&#34920;&#31034;&#21305;&#37197;&#20808;&#39564;&#20998;&#24067;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high cost of data labeling often results in node label shortage in real applications. To improve node classification accuracy, graph-based semi-supervised learning leverages the ample unlabeled nodes to train together with the scarce available labeled nodes. However, most existing methods require the information of all nodes, including those to be predicted, during model training, which is not practical for dynamic graphs with newly added nodes. To address this issue, an adversarially regularized graph attention model is proposed to classify newly added nodes in a partially labeled graph. An attention-based aggregator is designed to generate the representation of a node by aggregating information from its neighboring nodes, thus naturally generalizing to previously unseen nodes. In addition, adversarial training is employed to improve the model's robustness and generalization ability by enforcing node representations to match a prior distribution. Experiments on real-world datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#35745;&#31639;&#21487;&#36798;&#38598;&#30340;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#32447;&#24615;&#12289;&#22810;&#39033;&#24335;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;zonotope&#65292;&#21487;&#20197;&#25552;&#20379;&#36739;&#23569;&#20445;&#23432;&#30340;&#21487;&#36798;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#20851;&#20110;&#26410;&#30693;&#31995;&#32479;&#27169;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#32435;&#20837;&#35745;&#31639;&#12290;&#31639;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#20540;&#31034;&#20363;&#21644;&#23454;&#38469;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2105.07229</link><description>&lt;p&gt;
&#20174;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Reachability Analysis from Noisy Data. (arXiv:2105.07229v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.07229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#35745;&#31639;&#21487;&#36798;&#38598;&#30340;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#32447;&#24615;&#12289;&#22810;&#39033;&#24335;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#31639;&#27861;&#22522;&#20110;&#30697;&#38453;zonotope&#65292;&#21487;&#20197;&#25552;&#20379;&#36739;&#23569;&#20445;&#23432;&#30340;&#21487;&#36798;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#23558;&#20851;&#20110;&#26410;&#30693;&#31995;&#32479;&#27169;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#32435;&#20837;&#35745;&#31639;&#12290;&#31639;&#27861;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#20540;&#31034;&#20363;&#21644;&#23454;&#38469;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an algorithm for computing reachable sets directly from noisy data without a given system model, which is applicable to different types of systems including linear, polynomial, and nonlinear systems. The algorithm is based on matrix zonotopes and can provide less conservative reachable sets while incorporating prior knowledge about the unknown system model. Theoretical guarantees are given and the applicability of the algorithm is demonstrated through numerical examples and real experiments.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#27809;&#26377;&#32473;&#23450;&#31995;&#32479;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#20174;&#22024;&#26434;&#30340;&#25968;&#25454;&#20013;&#35745;&#31639;&#21487;&#36798;&#38598;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#25968;&#25454;&#30340;&#19981;&#21516;&#31867;&#22411;&#31995;&#32479;&#30340;&#21487;&#36798;&#24615;&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;zonotope&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#32447;&#24615;&#31995;&#32479;&#30340;&#36807;&#20272;&#35745;&#21487;&#36798;&#38598;&#12290;&#24341;&#20837;&#20102;&#32422;&#26463;&#30697;&#38453;zonotope&#20197;&#25552;&#20379;&#36739;&#23569;&#20445;&#23432;&#30340;&#21487;&#36798;&#38598;&#65292;&#20294;&#20195;&#20215;&#26159;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#65292;&#24182;&#29992;&#20110;&#23558;&#20851;&#20110;&#26410;&#30693;&#31995;&#32479;&#27169;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#32435;&#20837;&#35745;&#31639;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#39033;&#24335;&#31995;&#32479;&#65292;&#24182;&#22312;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#19979;&#25193;&#23637;&#21040;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#29702;&#35770;&#20445;&#35777;&#26159;&#23427;&#20204;&#32473;&#20986;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;&#21487;&#36798;&#38598;&#30340;&#36866;&#24403;&#36807;&#20272;&#35745;&#21487;&#36798;&#38598;&#12290;&#22810;&#20010;&#25968;&#20540;&#31034;&#20363;&#21644;&#23454;&#38469;&#23454;&#39564;&#26174;&#31034;&#20102;&#24341;&#20837;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#31639;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of computing reachable sets directly from noisy data without a given system model. Several reachability algorithms are presented for different types of systems generating the data. First, an algorithm for computing over-approximated reachable sets based on matrix zonotopes is proposed for linear systems. Constrained matrix zonotopes are introduced to provide less conservative reachable sets at the cost of increased computational expenses and utilized to incorporate prior knowledge about the unknown system model. Then we extend the approach to polynomial systems and, under the assumption of Lipschitz continuity, to nonlinear systems. Theoretical guarantees are given for these algorithms in that they give a proper over-approximate reachable set containing the true reachable set. Multiple numerical examples and real experiments show the applicability of the introduced algorithms, and comparisons are made between algorithms.
&lt;/p&gt;</description></item><item><title>NOMU&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26080;&#22122;&#22768;&#35774;&#32622;&#19979;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#30001;&#20004;&#20010;&#36830;&#25509;&#30340;&#23376;NN&#32452;&#25104;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#26469;&#25429;&#25417;NN&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#28385;&#36275;&#20116;&#20010;&#20851;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24895;&#26395;&#12290;</title><link>http://arxiv.org/abs/2102.13640</link><description>&lt;p&gt;
NOMU: &#22522;&#20110;&#31070;&#32463;&#20248;&#21270;&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
NOMU: Neural Optimization-based Model Uncertainty. (arXiv:2102.13640v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.13640
&lt;/p&gt;
&lt;p&gt;
NOMU&#26159;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26080;&#22122;&#22768;&#35774;&#32622;&#19979;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#30001;&#20004;&#20010;&#36830;&#25509;&#30340;&#23376;NN&#32452;&#25104;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#26469;&#25429;&#25417;NN&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#27169;&#22411;&#28385;&#36275;&#20116;&#20010;&#20851;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24895;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
NOMU is a new neural network model that captures model uncertainty for neural networks (NNs) in regression by designing a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and training it using a carefully-designed loss function. The model satisfies five important desiderata regarding model uncertainty.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#22238;&#24402;&#20013;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;&#20026;&#20102;&#38548;&#31163;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#31232;&#32570;&#35757;&#32451;&#25968;&#25454;&#30340;&#26080;&#22122;&#22768;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20116;&#20010;&#20851;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#37325;&#35201;&#24895;&#26395;&#65292;&#20219;&#20309;&#26041;&#27861;&#37117;&#24212;&#35813;&#28385;&#36275;&#36825;&#20123;&#24895;&#26395;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#36125;&#21494;&#26031;&#29702;&#35770;&#25152;&#35201;&#27714;&#30340;&#19968;&#20123;&#24895;&#26395;&#65292;&#24050;&#32463;&#24314;&#31435;&#30340;&#22522;&#20934;&#27979;&#35797;&#20063;&#32463;&#24120;&#26080;&#27861;&#21487;&#38752;&#22320;&#25429;&#25417;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;NN&#30340;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#31216;&#20026;&#31070;&#32463;&#20248;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65288;NOMU&#65289;&#12290; NOMU&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#35774;&#35745;&#19968;&#20010;&#30001;&#20004;&#20010;&#36830;&#25509;&#30340;&#23376;NN&#32452;&#25104;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#19968;&#20010;&#29992;&#20110;&#27169;&#22411;&#39044;&#27979;&#65292;&#19968;&#20010;&#29992;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#24378;&#21046;NOMU&#28385;&#36275;&#25105;&#20204;&#30340;&#20116;&#20010;&#24895;&#26395;&#12290;&#30001;&#20110;&#20854;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#22914;&#26524;&#32473;&#23450;&#35775;&#38382;&#26435;&#38480;&#65292;NOMU&#21487;&#20197;&#20026;&#20219;&#20309;&#32473;&#23450;&#30340;&#65288;&#20808;&#21069;&#35757;&#32451;&#30340;&#65289;NN&#25552;&#20379;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study methods for estimating model uncertainty for neural networks (NNs) in regression. To isolate the effect of model uncertainty, we focus on a noiseless setting with scarce training data. We introduce five important desiderata regarding model uncertainty that any method should satisfy. However, we find that established benchmarks often fail to reliably capture some of these desiderata, even those that are required by Bayesian theory. To address this, we introduce a new approach for capturing model uncertainty for NNs, which we call Neural Optimization-based Model Uncertainty (NOMU). The main idea of NOMU is to design a network architecture consisting of two connected sub-NNs, one for model prediction and one for model uncertainty, and to train it using a carefully-designed loss function. Importantly, our design enforces that NOMU satisfies our five desiderata. Due to its modular architecture, NOMU can provide model uncertainty for any given (previously trained) NN if given access
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#23545;&#19981;&#21516;&#20154;&#32676;&#30340;&#24433;&#21709;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#34429;&#28982;&#23427;&#20250;&#22312;&#25152;&#26377;&#20154;&#21475;&#32676;&#20307;&#20013;&#20135;&#29983;&#35823;&#24046;&#65292;&#20294;&#35823;&#24046;&#30340;&#31867;&#22411;&#20250;&#26377;&#31995;&#32479;&#24615;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#26435;&#20449;&#24687;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24046;&#24322;&#24182;&#25193;&#22823;&#26426;&#20250;&#30340;&#33719;&#21462;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#24179;&#26435;&#34892;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2102.10019</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#24179;&#31561;&#24433;&#21709;&#65306;&#24179;&#26435;&#34892;&#21160;&#19982;&#24179;&#26435;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
The Disparate Impact of Uncertainty: Affirmative Action vs. Affirmative Information. (arXiv:2102.10019v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.10019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#23545;&#19981;&#21516;&#20154;&#32676;&#30340;&#24433;&#21709;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#34429;&#28982;&#23427;&#20250;&#22312;&#25152;&#26377;&#20154;&#21475;&#32676;&#20307;&#20013;&#20135;&#29983;&#35823;&#24046;&#65292;&#20294;&#35823;&#24046;&#30340;&#31867;&#22411;&#20250;&#26377;&#31995;&#32479;&#24615;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24179;&#26435;&#20449;&#24687;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24046;&#24322;&#24182;&#25193;&#22823;&#26426;&#20250;&#30340;&#33719;&#21462;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#24179;&#26435;&#34892;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proves that uncertainty has a disparate impact on different demographic groups, with varying types of errors. The proposed strategy, called Affirmative Information, can eliminate this disparity and broaden access to opportunity, serving as an alternative to Affirmative Action.
&lt;/p&gt;
&lt;p&gt;
&#20687;&#36151;&#27454;&#25209;&#20934;&#12289;&#21307;&#30103;&#24178;&#39044;&#21644;&#22823;&#23398;&#24405;&#21462;&#36825;&#26679;&#30340;&#20851;&#38190;&#20915;&#31574;&#26159;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#27979;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#19981;&#24179;&#31561;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#23427;&#20250;&#22312;&#25152;&#26377;&#20154;&#21475;&#32676;&#20307;&#20013;&#20135;&#29983;&#35823;&#24046;&#65292;&#20294;&#35823;&#24046;&#30340;&#31867;&#22411;&#20250;&#26377;&#31995;&#32479;&#24615;&#30340;&#21464;&#21270;&#65306;&#24179;&#22343;&#32467;&#26524;&#36739;&#39640;&#30340;&#32676;&#20307;&#36890;&#24120;&#34987;&#20998;&#37197;&#26356;&#39640;&#30340;&#20551;&#38451;&#24615;&#29575;&#65292;&#32780;&#24179;&#22343;&#32467;&#26524;&#36739;&#20302;&#30340;&#32676;&#20307;&#21017;&#34987;&#20998;&#37197;&#26356;&#39640;&#30340;&#20551;&#38452;&#24615;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39069;&#22806;&#30340;&#25968;&#25454;&#33719;&#21462;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24046;&#24322;&#24182;&#25193;&#22823;&#26426;&#20250;&#30340;&#33719;&#21462;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#24179;&#26435;&#20449;&#24687;&#30340;&#31574;&#30053;&#21487;&#20197;&#20316;&#20026;&#24179;&#26435;&#34892;&#21160;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical decisions like loan approvals, medical interventions, and college admissions are guided by predictions made in the presence of uncertainty. In this paper, we prove that uncertainty has a disparate impact. While it imparts errors across all demographic groups, the types of errors vary systematically: Groups with higher average outcomes are typically assigned higher false positive rates, while those with lower average outcomes are assigned higher false negative rates. We show that additional data acquisition can eliminate the disparity and broaden access to opportunity. The strategy, which we call Affirmative Information, could stand as an alternative to Affirmative Action.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38544;&#31169;&#20445;&#25252;&#19979;&#22810;&#28304;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20004;&#31181;&#20027;&#27969;&#35299;&#20915;&#26041;&#26696;&#65306;&#23433;&#20840;&#22810;&#26041;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#12289;&#25968;&#25454;&#20998;&#24067;&#12289;&#35757;&#32451;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#24212;&#29992;&#22330;&#26223;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2012.03386</link><description>&lt;p&gt;
SoK: &#38544;&#31169;&#20445;&#25252;&#19979;&#22810;&#28304;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SoK: Training Machine Learning Models over Multiple Sources with Privacy Preservation. (arXiv:2012.03386v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.03386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38544;&#31169;&#20445;&#25252;&#19979;&#22810;&#28304;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20004;&#31181;&#20027;&#27969;&#35299;&#20915;&#26041;&#26696;&#65306;&#23433;&#20840;&#22810;&#26041;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#12289;&#25968;&#25454;&#20998;&#24067;&#12289;&#35757;&#32451;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#24212;&#29992;&#22330;&#26223;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reviews two mainstream solutions for training machine learning models over multiple sources with privacy preservation: Secure Multi-party Learning (MPL) and Federated Learning (FL). The security, efficiency, data distribution, accuracy of trained models, and application scenarios of these two solutions are compared and discussed, and future research directions are explored.
&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20174;&#22810;&#20010;&#25968;&#25454;&#28304;&#20013;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#24182;&#20445;&#25252;&#38544;&#31169;&#26159;&#35757;&#32451;&#39640;&#24615;&#33021;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#25171;&#30772;&#23396;&#31435;&#25968;&#25454;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#38556;&#30861;&#65292;&#20174;&#32780;&#25193;&#22823;&#21487;&#29992;&#20110;&#22788;&#29702;&#30340;&#25968;&#25454;&#33539;&#22260;&#12290;&#20026;&#27492;&#65292;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#19994;&#20379;&#24212;&#21830;&#26368;&#36817;&#24378;&#28872;&#21160;&#21147;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20027;&#27969;&#35299;&#20915;&#26041;&#26696;&#65292;&#20027;&#35201;&#22522;&#20110;&#36719;&#20214;&#26500;&#36896;&#65306;1&#65289;&#23433;&#20840;&#22810;&#26041;&#23398;&#20064;&#65288;MPL&#65289;&#65307;&#21644;2&#65289;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#12290;&#24403;&#25105;&#20204;&#26681;&#25454;&#20197;&#19979;&#20116;&#20010;&#26631;&#20934;&#35780;&#20272;&#23427;&#20204;&#26102;&#65292;&#19978;&#36848;&#20004;&#20010;&#25216;&#26415;&#25991;&#20214;&#22841;&#37117;&#26377;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65306;&#23433;&#20840;&#24615;&#65292;&#25928;&#29575;&#65292;&#25968;&#25454;&#20998;&#24067;&#65292;&#35757;&#32451;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;&#20026;&#20102;&#23637;&#31034;&#30740;&#31350;&#36827;&#23637;&#24182;&#35752;&#35770;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#24443;&#24213;&#35843;&#26597;&#20102;&#36825;&#20123;&#21327;&#35758;&#21644;MPL&#21644;FL&#30340;&#26694;&#26550;&#65292;&#24182;&#24635;&#32467;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20004;&#20010;&#25216;&#26415;&#25991;&#20214;&#22841;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#24182;&#35752;&#35770;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, gathering high-quality training data from multiple data sources with privacy preservation is a crucial challenge to training high-performance machine learning models. The potential solutions could break the barriers among isolated data corpus, and consequently enlarge the range of data available for processing. To this end, both academic researchers and industrial vendors are recently strongly motivated to propose two main-stream folders of solutions mainly based on software constructions: 1) Secure Multi-party Learning (MPL for short); and 2) Federated Learning (FL for short). The above two technical folders have their advantages and limitations when we evaluate them according to the following five criteria: security, efficiency, data distribution, the accuracy of trained models, and application scenarios.  Motivated to demonstrate the research progress and discuss the insights on the future directions, we thoroughly investigate these protocols and frameworks of both MPL and
&lt;/p&gt;</description></item><item><title>LOCUS&#26159;&#19968;&#31181;&#26032;&#30340;&#22823;&#33041;&#32593;&#32476;&#36830;&#25509;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#20351;&#29992;&#20302;&#31209;&#32467;&#26500;&#21644;&#22343;&#21248;&#31232;&#30095;&#24615;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#21644;&#20934;&#30830;&#22320;&#20998;&#31163;&#36830;&#25509;&#30697;&#38453;&#28304;&#65292;&#26377;&#26395;&#25104;&#20026;&#29702;&#35299;&#22823;&#33041;&#32452;&#32455;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2008.08915</link><description>&lt;p&gt;
LOCUS&#65306;&#19968;&#31181;&#20351;&#29992;&#20302;&#31209;&#32467;&#26500;&#21644;&#22343;&#21248;&#31232;&#30095;&#24615;&#30340;&#22823;&#33041;&#32593;&#32476;&#36830;&#25509;&#30697;&#38453;&#20998;&#35299;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LOCUS: A Novel Decomposition Method for Brain Network Connectivity Matrices using Low-rank Structure with Uniform Sparsity. (arXiv:2008.08915v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.08915
&lt;/p&gt;
&lt;p&gt;
LOCUS&#26159;&#19968;&#31181;&#26032;&#30340;&#22823;&#33041;&#32593;&#32476;&#36830;&#25509;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#20351;&#29992;&#20302;&#31209;&#32467;&#26500;&#21644;&#22343;&#21248;&#31232;&#30095;&#24615;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#21644;&#20934;&#30830;&#22320;&#20998;&#31163;&#36830;&#25509;&#30697;&#38453;&#28304;&#65292;&#26377;&#26395;&#25104;&#20026;&#29702;&#35299;&#22823;&#33041;&#32452;&#32455;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
LOCUS is a novel method for decomposing brain network connectivity matrices using low-rank structure and uniform sparsity, which achieves more efficient and accurate source separation and has the potential to serve as a key for understanding brain organizations.
&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23548;&#21521;&#30740;&#31350;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#22312;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#25104;&#20687;&#30340;&#32593;&#32476;&#36830;&#25509;&#24230;&#37327;&#24050;&#25104;&#20026;&#29702;&#35299;&#22823;&#33041;&#32452;&#32455;&#30340;&#20851;&#38190;&#65292;&#21487;&#33021;&#20316;&#20026;&#20010;&#20307;&#31070;&#32463;&#25351;&#32441;&#12290;&#20998;&#26512;&#36830;&#25509;&#30697;&#38453;&#23384;&#22312;&#20027;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#22823;&#33041;&#32593;&#32476;&#30340;&#39640;&#32500;&#24230;&#65292;&#35266;&#23519;&#21040;&#30340;&#36830;&#25509;&#19979;&#38754;&#30340;&#26410;&#30693;&#28508;&#22312;&#28304;&#20197;&#21450;&#23548;&#33268;&#34394;&#20551;&#21457;&#29616;&#30340;&#22823;&#37327;&#33041;&#36830;&#25509;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30450;&#28304;&#20998;&#31163;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#21644;&#22343;&#21248;&#31232;&#30095;&#24615;&#65288;LOCUS&#65289;&#30340;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#32593;&#32476;&#24230;&#37327;&#12290;&#19982;&#23558;&#36830;&#25509;&#30697;&#38453;&#21521;&#37327;&#21270;&#24182;&#24573;&#30053;&#22823;&#33041;&#32593;&#32476;&#25299;&#25169;&#30340;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;LOCUS&#20351;&#29992;&#20302;&#31209;&#32467;&#26500;&#23454;&#29616;&#20102;&#26356;&#26377;&#25928;&#21644;&#20934;&#30830;&#30340;&#36830;&#25509;&#30697;&#38453;&#28304;&#20998;&#31163;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35282;&#24230;&#30340;&#22343;&#21248;&#31232;&#30095;&#27491;&#21017;&#21270;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#36830;&#25509;&#30697;&#38453;&#20998;&#35299;&#20013;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network-oriented research has been increasingly popular in many scientific areas. In neuroscience research, imaging-based network connectivity measures have become the key for understanding brain organizations, potentially serving as individual neural fingerprints. There are major challenges in analyzing connectivity matrices including the high dimensionality of brain networks, unknown latent sources underlying the observed connectivity, and the large number of brain connections leading to spurious findings. In this paper, we propose a novel blind source separation method with low-rank structure and uniform sparsity (LOCUS) as a fully data-driven decomposition method for network measures. Compared with the existing method that vectorizes connectivity matrices ignoring brain network topology, LOCUS achieves more efficient and accurate source separation for connectivity matrices using low-rank structure. We propose a novel angle-based uniform sparsity regularization that demonstrates bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#30340;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#23450;&#20041;&#19968;&#20010;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#21464;&#25442;&#30340;&#30446;&#26631;&#24207;&#21015;&#19978;&#65292;&#35813;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#30340;&#25311;&#21512;&#24230;&#19978;&#34920;&#29616;&#26174;&#33879;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#23545;&#25968;&#20284;&#28982;&#24230;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2008.02144</link><description>&lt;p&gt;
FRMDN: &#22522;&#20110;&#27969;&#30340;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
FRMDN: Flow-based Recurrent Mixture Density Network. (arXiv:2008.02144v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.02144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#30340;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#23450;&#20041;&#19968;&#20010;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#21464;&#25442;&#30340;&#30446;&#26631;&#24207;&#21015;&#19978;&#65292;&#35813;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#30340;&#25311;&#21512;&#24230;&#19978;&#34920;&#29616;&#26174;&#33879;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#23545;&#25968;&#20284;&#28982;&#24230;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a flow-based recurrent mixture density network (FRMDN) that generalizes recurrent mixture density networks by defining a Gaussian mixture model on a non-linearly transformed target sequence in each time-step. The model significantly improves the fit to image sequences and outperforms other state-of-the-art methods in terms of the log-likelihood.
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#26144;&#23556;&#24212;&#29992;&#20013;&#12290;&#22312;&#36825;&#31867;&#27169;&#22411;&#20013;&#65292;&#30446;&#26631;&#24207;&#21015;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#23494;&#24230;&#30001;&#20855;&#26377;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#24314;&#27169;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#19978;&#23450;&#20041;&#19968;&#20010;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#24490;&#29615;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#21464;&#25442;&#30340;&#30446;&#26631;&#24207;&#21015;&#19978;&#12290;&#38750;&#32447;&#24615;&#21464;&#25442;&#31354;&#38388;&#26159;&#36890;&#36807;&#24402;&#19968;&#21270;&#27969;&#21019;&#24314;&#30340;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#35813;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;&#24207;&#21015;&#30340;&#25311;&#21512;&#24230;&#65292;&#29992;&#23545;&#25968;&#20284;&#28982;&#24230;&#37327;&#12290;&#25105;&#20204;&#36824;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#19968;&#20123;&#35821;&#38899;&#21644;&#22270;&#20687;&#25968;&#25454;&#65292;&#24182;&#35266;&#23519;&#21040;&#35813;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#30340;&#24314;&#27169;&#33021;&#21147;&#65292;&#22312;&#23545;&#25968;&#20284;&#28982;&#24230;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The class of recurrent mixture density networks is an important class of probabilistic models used extensively in sequence modeling and sequence-to-sequence mapping applications. In this class of models, the density of a target sequence in each time-step is modeled by a Gaussian mixture model with the parameters given by a recurrent neural network. In this paper, we generalize recurrent mixture density networks by defining a Gaussian mixture model on a non-linearly transformed target sequence in each time-step. The non-linearly transformed space is created by normalizing flow. We observed that this model significantly improves the fit to image sequences measured by the log-likelihood. We also applied the proposed model on some speech and image data, and observed that the model has significant modeling power outperforming other state-of-the-art methods in terms of the log-likelihood.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#31216;&#20026;&#24322;&#24120;&#24863;&#30693;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20462;&#25913;&#25104;&#26412;&#20989;&#25968;&#26469;&#23398;&#20064;&#27491;&#24120;&#20107;&#20214;&#65292;&#24182;&#20102;&#35299;&#24322;&#24120;&#20107;&#20214;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#31890;&#23376;&#29289;&#29702;&#24773;&#20917;&#21644;&#26631;&#20934;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#65292;&#24182;&#22312;&#20102;&#35299;&#36275;&#22815;&#22810;&#30340;&#24322;&#24120;&#26102;&#21464;&#24471;&#26356;&#21152;&#31283;&#20581;&#12290;</title><link>http://arxiv.org/abs/2007.14462</link><description>&lt;p&gt;
&#24322;&#24120;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Anomaly Awareness. (arXiv:2007.14462v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.14462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#31216;&#20026;&#24322;&#24120;&#24863;&#30693;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20462;&#25913;&#25104;&#26412;&#20989;&#25968;&#26469;&#23398;&#20064;&#27491;&#24120;&#20107;&#20214;&#65292;&#24182;&#20102;&#35299;&#24322;&#24120;&#20107;&#20214;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#31890;&#23376;&#29289;&#29702;&#24773;&#20917;&#21644;&#26631;&#20934;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#65292;&#24182;&#22312;&#20102;&#35299;&#36275;&#22815;&#22810;&#30340;&#24322;&#24120;&#26102;&#21464;&#24471;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a new anomaly detection algorithm called Anomaly Awareness, which learns about normal events while being made aware of the anomalies through a modification of the cost function. The method is effective in identifying anomalies not seen before and becomes more robust as it is made aware of a varied-enough set of anomalies. It is applied in different Particle Physics situations and in standard Computer Vision tasks.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#31216;&#20026;&#24322;&#24120;&#24863;&#30693;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20462;&#25913;&#25104;&#26412;&#20989;&#25968;&#26469;&#23398;&#20064;&#27491;&#24120;&#20107;&#20214;&#65292;&#24182;&#20102;&#35299;&#24322;&#24120;&#20107;&#20214;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#31890;&#23376;&#29289;&#29702;&#24773;&#20917;&#21644;&#26631;&#20934;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#30001;&#26631;&#20934;&#27169;&#22411;&#39030;&#22840;&#20811;&#21644;QCD&#20107;&#20214;&#29983;&#25104;&#30340;Fat Jet&#25299;&#25169;&#30340;&#22270;&#20687;&#65292;&#24182;&#38024;&#23545;&#19968;&#31995;&#21015;&#26032;&#29289;&#29702;&#22330;&#26223;&#36827;&#34892;&#27979;&#35797;&#65292;&#21253;&#25324;&#20855;&#26377;EFT&#25928;&#24212;&#30340;&#24076;&#26684;&#26031;&#20135;&#29983;&#21644;&#34928;&#21464;&#25104;&#20004;&#20010;&#12289;&#19977;&#20010;&#25110;&#22235;&#20010;&#23376;&#21943;&#27880;&#30340;&#20849;&#25391;&#12290;&#25105;&#20204;&#21457;&#29616;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#35782;&#21035;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#24322;&#24120;&#65292;&#24182;&#22312;&#25105;&#20204;&#35753;&#23427;&#20102;&#35299;&#36275;&#22815;&#22810;&#30340;&#24322;&#24120;&#26102;&#21464;&#24471;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new algorithm for anomaly detection called Anomaly Awareness. The algorithm learns about normal events while being made aware of the anomalies through a modification of the cost function. We show how this method works in different Particle Physics situations and in standard Computer Vision tasks. For example, we apply the method to images from a Fat Jet topology generated by Standard Model Top and QCD events, and test it against an array of new physics scenarios, including Higgs production with EFT effects and resonances decaying into two, three or four subjets. We find that the algorithm is effective identifying anomalies not seen before, and becomes robust as we make it aware of a varied-enough set of anomalies.
&lt;/p&gt;</description></item><item><title>COMET&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#27169;&#25311;&#21382;&#21490;&#20132;&#20114;&#21644;&#23884;&#20837;&#32500;&#24230;&#20043;&#38388;&#30340;&#39640;&#38454;&#20132;&#20114;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2007.14129</link><description>&lt;p&gt;
COMET: &#21367;&#31215;&#32500;&#24230;&#20132;&#20114;&#29992;&#20110;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
COMET: Convolutional Dimension Interaction for Collaborative Filtering. (arXiv:2007.14129v6 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.14129
&lt;/p&gt;
&lt;p&gt;
COMET&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#27169;&#25311;&#21382;&#21490;&#20132;&#20114;&#21644;&#23884;&#20837;&#32500;&#24230;&#20043;&#38388;&#30340;&#39640;&#38454;&#20132;&#20114;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
COMET is a novel representation learning-based model that can simultaneously model the high-order interaction patterns among historical interactions and embedding dimensions.
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#30340;&#25512;&#33616;&#27169;&#22411;&#22312;&#25512;&#33616;&#25216;&#26415;&#20013;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#21382;&#21490;&#20132;&#20114;&#21644;&#23884;&#20837;&#32500;&#24230;&#30456;&#20114;&#29420;&#31435;&#65292;&#22240;&#27492;&#36951;&#25022;&#22320;&#24573;&#30053;&#20102;&#21382;&#21490;&#20132;&#20114;&#21644;&#23884;&#20837;&#32500;&#24230;&#20043;&#38388;&#30340;&#39640;&#38454;&#20132;&#20114;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;COMET&#65288;COnvolutional diMEnsion inTeraction&#65289;&#65292;&#23427;&#21516;&#26102;&#27169;&#25311;&#21382;&#21490;&#20132;&#20114;&#21644;&#23884;&#20837;&#32500;&#24230;&#20043;&#38388;&#30340;&#39640;&#38454;&#20132;&#20114;&#27169;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;COMET&#39318;&#20808;&#23558;&#21382;&#21490;&#20132;&#20114;&#30340;&#23884;&#20837;&#27700;&#24179;&#22534;&#21472;&#65292;&#20174;&#32780;&#20135;&#29983;&#20004;&#20010;&#8220;&#23884;&#20837;&#26144;&#23556;&#8221;&#12290;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21516;&#26102;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#30340;&#20869;&#37096;&#20132;&#20114;&#21644;&#32500;&#24230;&#20132;&#20114;&#20869;&#26680;&#65292;&#21487;&#20197;&#21033;&#29992;&#20869;&#37096;&#20132;&#20114;&#21644;&#32500;&#24230;&#20132;&#20114;&#12290;&#28982;&#21518;&#24212;&#29992;&#20840;&#36830;&#25509;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#26469;&#33719;&#24471;&#20004;&#20010;&#20132;&#20114;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning-based recommendation models play a dominant role among recommendation techniques. However, most of the existing methods assume both historical interactions and embedding dimensions are independent of each other, and thus regrettably ignore the high-order interaction information among historical interactions and embedding dimensions. In this paper, we propose a novel representation learning-based model called COMET (COnvolutional diMEnsion inTeraction), which simultaneously models the high-order interaction patterns among historical interactions and embedding dimensions. To be specific, COMET stacks the embeddings of historical interactions horizontally at first, which results in two "embedding maps". In this way, internal interactions and dimensional interactions can be exploited by convolutional neural networks (CNN) with kernels of different sizes simultaneously. A fully-connected multi-layer perceptron (MLP) is then applied to obtain two interaction vectors. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#30340;&#36731;&#37327;&#32423;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;OS-ELM&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;DQN&#38656;&#35201;&#22823;&#37327;&#32531;&#20914;&#21306;&#21644;&#25209;&#22788;&#29702;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;L2&#27491;&#21017;&#21270;&#21644;&#35889;&#24402;&#19968;&#21270;&#30340;&#32452;&#21512;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2005.04646</link><description>&lt;p&gt;
&#22522;&#20110;FPGA&#30340;&#22312;&#32447;&#39034;&#24207;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An FPGA-Based On-Device Reinforcement Learning Approach using Online Sequential Learning. (arXiv:2005.04646v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.04646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#30340;&#36731;&#37327;&#32423;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;OS-ELM&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;DQN&#38656;&#35201;&#22823;&#37327;&#32531;&#20914;&#21306;&#21644;&#25209;&#22788;&#29702;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;L2&#27491;&#21017;&#21270;&#21644;&#35889;&#24402;&#19968;&#21270;&#30340;&#32452;&#21512;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#26356;&#21152;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a lightweight on-device reinforcement learning approach for low-cost FPGA devices, which uses OS-ELM algorithm for training and avoids the problem of requiring large buffers and batch processing in DQN. The combination of L2 regularization and spectral normalization is used to make the reinforcement learning more stable.
&lt;/p&gt;
&lt;p&gt;
DQN&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;Q&#23398;&#20064;&#26041;&#27861;&#12290;DQN&#38656;&#35201;&#22823;&#37327;&#30340;&#32531;&#20914;&#21306;&#21644;&#25209;&#22788;&#29702;&#36827;&#34892;&#32463;&#39564;&#37325;&#25918;&#65292;&#24182;&#20381;&#36182;&#20110;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#30340;&#36845;&#20195;&#20248;&#21270;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#35774;&#22791;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#25104;&#26412;&#30340;FPGA&#35774;&#22791;&#12290;&#23427;&#21033;&#29992;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#26041;&#27861;&#65292;&#32780;&#26159;&#20351;&#29992;&#22522;&#20110;OS-ELM&#65288;&#22312;&#32447;&#39034;&#24207;&#26497;&#38480;&#23398;&#20064;&#26426;&#65289;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;L2&#27491;&#21017;&#21270;&#21644;&#35889;&#24402;&#19968;&#21270;&#30340;&#32452;&#21512;&#65292;&#29992;&#20110;&#35774;&#22791;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#20415;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#20540;&#36866;&#21512;&#20110;&#26576;&#20010;&#33539;&#22260;&#65292;&#24182;&#20351;&#24378;&#21270;&#23398;&#20064;&#21464;&#24471;&#31283;&#23450;&#12290;&#25152;&#25552;&#20986;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26159;&#20026;PYNQ-Z1&#26495;&#35774;&#35745;&#30340;&#65292;&#20316;&#20026;&#20302;&#25104;&#26412;&#30340;FPGA&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
DQN (Deep Q-Network) is a method to perform Q-learning for reinforcement learning using deep neural networks. DQNs require a large buffer and batch processing for an experience replay and rely on a backpropagation based iterative optimization, making them difficult to be implemented on resource-limited edge devices. In this paper, we propose a lightweight on-device reinforcement learning approach for low-cost FPGA devices. It exploits a recently proposed neural-network based on-device learning approach that does not rely on the backpropagation method but uses OS-ELM (Online Sequential Extreme Learning Machine) based training algorithm. In addition, we propose a combination of L2 regularization and spectral normalization for the on-device reinforcement learning so that output values of the neural network can be fit into a certain range and the reinforcement learning becomes stable. The proposed reinforcement learning approach is designed for PYNQ-Z1 board as a low-cost FPGA platform. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#21644;&#32479;&#19968;&#30340;&#25554;&#20540;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#20801;&#35768;&#25105;&#20204;&#22312;&#20219;&#24847;&#23494;&#24230;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#27979;&#22320;&#32447;&#21644;&#25554;&#20540;&#26354;&#32447;&#12290;&#26368;&#22823;&#21270;&#26354;&#32447;&#30340;&#36136;&#37327;&#24230;&#37327;&#21487;&#20197;&#31561;&#20215;&#22320;&#29702;&#35299;&#20026;&#22312;&#31354;&#38388;&#19978;&#26576;&#31181;&#37325;&#26032;&#23450;&#20041;&#30340;&#40654;&#26364;&#24230;&#37327;&#19979;&#25628;&#32034;&#27979;&#22320;&#32447;&#12290;</title><link>http://arxiv.org/abs/1904.03445</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#30340;&#29983;&#25104;&#27169;&#22411;&#28508;&#31354;&#38388;&#25554;&#20540;&#21644;&#27979;&#22320;&#32447;
&lt;/p&gt;
&lt;p&gt;
Feature-Based Interpolation and Geodesics in the Latent Spaces of Generative Models. (arXiv:1904.03445v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1904.03445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#21644;&#32479;&#19968;&#30340;&#25554;&#20540;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#20801;&#35768;&#25105;&#20204;&#22312;&#20219;&#24847;&#23494;&#24230;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#27979;&#22320;&#32447;&#21644;&#25554;&#20540;&#26354;&#32447;&#12290;&#26368;&#22823;&#21270;&#26354;&#32447;&#30340;&#36136;&#37327;&#24230;&#37327;&#21487;&#20197;&#31561;&#20215;&#22320;&#29702;&#35299;&#20026;&#22312;&#31354;&#38388;&#19978;&#26576;&#31181;&#37325;&#26032;&#23450;&#20041;&#30340;&#40654;&#26364;&#24230;&#37327;&#19979;&#25628;&#32034;&#27979;&#22320;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a general and unified approach to interpolation, which simultaneously allows us to search for geodesics and interpolating curves in latent space in the case of arbitrary density. Maximizing the quality measure of the curve can be equivalently understood as a search of geodesic for a certain redefinition of the Riemannian metric on the space.
&lt;/p&gt;
&lt;p&gt;
&#25554;&#20540;&#38382;&#39064;&#21516;&#26102;&#28041;&#21450;&#21040;&#27979;&#22320;&#32447;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;&#12290;&#22312;&#27979;&#22320;&#32447;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23547;&#25214;&#38271;&#24230;&#26368;&#30701;&#30340;&#26354;&#32447;&#65292;&#32780;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#24120;&#22312;&#28508;&#31354;&#38388;&#20013;&#24212;&#29992;&#32447;&#24615;&#25554;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25554;&#20540;&#38544;&#21547;&#22320;&#20351;&#29992;&#20102;&#39640;&#26031;&#20998;&#24067;&#26159;&#21333;&#23792;&#30340;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#22312;&#28508;&#22312;&#23494;&#24230;&#20026;&#38750;&#39640;&#26031;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#25554;&#20540;&#30340;&#38382;&#39064;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#21644;&#32479;&#19968;&#30340;&#25554;&#20540;&#26041;&#27861;&#65292;&#23427;&#21516;&#26102;&#20801;&#35768;&#25105;&#20204;&#22312;&#20219;&#24847;&#23494;&#24230;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#27979;&#22320;&#32447;&#21644;&#25554;&#20540;&#26354;&#32447;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#32972;&#26223;&#65292;&#22522;&#20110;&#24341;&#20837;&#30340;&#25554;&#20540;&#26354;&#32447;&#36136;&#37327;&#24230;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#22823;&#21270;&#26354;&#32447;&#30340;&#36136;&#37327;&#24230;&#37327;&#21487;&#20197;&#31561;&#20215;&#22320;&#29702;&#35299;&#20026;&#22312;&#31354;&#38388;&#19978;&#26576;&#31181;&#37325;&#26032;&#23450;&#20041;&#30340;&#40654;&#26364;&#24230;&#37327;&#19979;&#25628;&#32034;&#27979;&#22320;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpolating between points is a problem connected simultaneously with finding geodesics and study of generative models. In the case of geodesics, we search for the curves with the shortest length, while in the case of generative models we typically apply linear interpolation in the latent space. However, this interpolation uses implicitly the fact that Gaussian is unimodal. Thus the problem of interpolating in the case when the latent density is non-Gaussian is an open problem.  In this paper, we present a general and unified approach to interpolation, which simultaneously allows us to search for geodesics and interpolating curves in latent space in the case of arbitrary density. Our results have a strong theoretical background based on the introduced quality measure of an interpolating curve. In particular, we show that maximising the quality measure of the curve can be equivalently understood as a search of geodesic for a certain redefinition of the Riemannian metric on the space. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#32447;&#24615;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#32479;&#35745;&#30456;&#20851;&#26041;&#21521;&#19978;&#36827;&#34892;&#26799;&#24230;&#30340;&#22238;&#24402;&#26469;&#25552;&#39640;SGD&#30340;&#25910;&#25947;&#24615;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#26041;&#27861;&#21482;&#32771;&#34385;&#21333;&#20010;&#26041;&#21521;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#20108;&#38454;&#26041;&#27861;&#30340;&#25104;&#26412;&#21644;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/1901.11457</link><description>&lt;p&gt;
&#22312;&#22810;&#20010;&#32479;&#35745;&#30456;&#20851;&#26041;&#21521;&#19978;&#36827;&#34892;&#26799;&#24230;&#30340;&#22312;&#32447;&#32447;&#24615;&#22238;&#24402;&#20197;&#25552;&#39640;SGD&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving SGD convergence by online linear regression of gradients in multiple statistically relevant directions. (arXiv:1901.11457v11 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1901.11457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#32447;&#24615;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#32479;&#35745;&#30456;&#20851;&#26041;&#21521;&#19978;&#36827;&#34892;&#26799;&#24230;&#30340;&#22238;&#24402;&#26469;&#25552;&#39640;SGD&#30340;&#25910;&#25947;&#24615;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;&#26041;&#27861;&#21482;&#32771;&#34385;&#21333;&#20010;&#26041;&#21521;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#20108;&#38454;&#26041;&#27861;&#30340;&#25104;&#26412;&#21644;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an online linear regression method that improves the convergence of SGD by regressing gradients in multiple statistically relevant directions, addressing the issue of standard methods only considering a single direction and avoiding the cost and numerical instability of second order methods.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#20165;&#20351;&#29992;&#26799;&#24230;&#30340;&#38750;&#24120;&#31895;&#30053;&#30340;&#36817;&#20284;&#20540;&#26469;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#26631;&#20934;&#26041;&#27861;&#65288;&#22914;&#21160;&#37327;&#25110;ADAM&#65289;&#20165;&#32771;&#34385;&#21333;&#20010;&#26041;&#21521;&#65292;&#24182;&#19988;&#19981;&#23581;&#35797;&#27169;&#25311;&#21040;&#26497;&#20540;&#30340;&#36317;&#31163;&#65292;&#24573;&#30053;&#20102;&#20174;&#35745;&#31639;&#30340;&#26799;&#24230;&#24207;&#21015;&#20013;&#33719;&#24471;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#65292;&#24448;&#24448;&#20572;&#28382;&#22312;&#26576;&#20123;&#27425;&#20248;&#24179;&#21488;&#19978;&#12290;&#20108;&#38454;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#38169;&#36807;&#30340;&#26426;&#20250;&#65292;&#20294;&#38500;&#20102;&#36973;&#21463;&#38750;&#24120;&#22823;&#30340;&#25104;&#26412;&#21644;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#22806;&#65292;&#20854;&#20013;&#35768;&#22810;&#26041;&#27861;&#30001;&#20110;&#24573;&#30053;&#26354;&#29575;&#30340;&#31526;&#21495;&#65288;&#20316;&#20026;Hessian&#30340;&#29305;&#24449;&#20540;&#65289;&#32780;&#21560;&#24341;&#21040;&#20687;&#38797;&#28857;&#36825;&#26679;&#30340;&#27425;&#20248;&#28857;&#12290;&#26080;&#38797;&#29275;&#39039;&#27861;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#32597;&#35265;&#20363;&#23376;&#65292;&#23427;&#23558;&#38797;&#28857;&#21560;&#24341;&#21147;&#36716;&#21464;&#20026;&#25490;&#26021;&#21147;&#65292;&#24182;&#34987;&#35777;&#26126;&#22312;&#36825;&#31181;&#26041;&#24335;&#19979;&#25552;&#20379;&#20102;&#26368;&#32456;&#20540;&#30340;&#37325;&#35201;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#27169;&#25311;&#20108;&#38454;&#34892;&#20026;&#26102;&#24573;&#30053;&#20102;&#22122;&#22768;&#65292;&#19987;&#27880;&#20110;Krylov&#23376;&#31354;&#38388;&#20197;&#36827;&#34892;&#25968;&#20540;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are usually trained with stochastic gradient descent (SGD), which minimizes objective function using very rough approximations of gradient, only averaging to the real gradient. Standard approaches like momentum or ADAM only consider a single direction, and do not try to model distance from extremum - neglecting valuable information from calculated sequence of gradients, often stagnating in some suboptimal plateau. Second order methods could exploit these missed opportunities, however, beside suffering from very large cost and numerical instabilities, many of them attract to suboptimal points like saddles due to negligence of signs of curvatures (as eigenvalues of Hessian).  Saddle-free Newton method is a rare example of addressing this issue changes saddle attraction into repulsion, and was shown to provide essential improvement for final value this way. However, it neglects noise while modelling second order behavior, focuses on Krylov subspace for numerical rea
&lt;/p&gt;</description></item></channel></rss>