<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Weaver&#26159;&#19968;&#31995;&#21015;&#19987;&#27880;&#20110;&#21019;&#20316;&#20869;&#23481;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#20197;&#26356;&#20154;&#31867;&#21270;&#30340;&#26041;&#24335;&#29983;&#25104;&#25991;&#26412;&#21644;&#36981;&#24490;&#22810;&#26679;&#30340;&#20869;&#23481;&#21019;&#20316;&#25351;&#23548;&#12290;&#19981;&#21516;&#22823;&#23567;&#30340;Weaver&#27169;&#22411;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#21160;&#24577;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Weaver&#27169;&#22411;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20889;&#20316;&#33021;&#21147;&#19978;&#32988;&#36807;&#36890;&#29992;&#22411;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17268</link><description>&lt;p&gt;
Weaver: &#21019;&#24847;&#20889;&#20316;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Weaver: Foundation Models for Creative Writing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17268
&lt;/p&gt;
&lt;p&gt;
Weaver&#26159;&#19968;&#31995;&#21015;&#19987;&#27880;&#20110;&#21019;&#20316;&#20869;&#23481;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#20197;&#26356;&#20154;&#31867;&#21270;&#30340;&#26041;&#24335;&#29983;&#25104;&#25991;&#26412;&#21644;&#36981;&#24490;&#22810;&#26679;&#30340;&#20869;&#23481;&#21019;&#20316;&#25351;&#23548;&#12290;&#19981;&#21516;&#22823;&#23567;&#30340;Weaver&#27169;&#22411;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#21160;&#24577;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Weaver&#27169;&#22411;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20889;&#20316;&#33021;&#21147;&#19978;&#32988;&#36807;&#36890;&#29992;&#22411;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; Weaver&#65292;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#20869;&#23481;&#21019;&#20316;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31995;&#21015;&#12290;Weaver &#22312;&#19968;&#20010;&#31934;&#36873;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#19987;&#27880;&#20110;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20889;&#20316;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#22871;&#26032;&#39062;&#30340;&#26041;&#27861;&#36827;&#34892;&#25351;&#23548;&#25968;&#25454;&#21512;&#25104;&#21644; LLM &#23545;&#40784;&#65292;&#23545; Weaver &#36827;&#34892;&#21019;&#24847;&#21644;&#32844;&#19994;&#20889;&#20316;&#30340;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#20135;&#29983;&#26356;&#21152;&#20154;&#31867;&#21270;&#30340;&#25991;&#26412;&#24182;&#36981;&#24490;&#26356;&#21152;&#22810;&#26679;&#30340;&#20869;&#23481;&#21019;&#20316;&#25351;&#23548;&#12290;Weaver &#31995;&#21015;&#21253;&#25324; Weaver Mini&#65288;1.8B&#65289;&#12289;Weaver Base&#65288;6B&#65289;&#12289;Weaver Pro&#65288;14B&#65289;&#21644; Weaver Ultra&#65288;34B&#65289;&#31561;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#26681;&#25454;&#26597;&#35810;&#22797;&#26434;&#24230;&#30001;&#36335;&#30001;&#20195;&#29702;&#21160;&#24577;&#20998;&#37197;&#65292;&#20174;&#32780;&#24179;&#34913;&#21709;&#24212;&#36136;&#37327;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20889;&#20316;&#33021;&#21147;&#35780;&#20272;&#22522;&#20934;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Weaver &#30340;&#25152;&#26377;&#27169;&#22411;&#23610;&#23544;&#32988;&#36807;&#20102;&#36890;&#29992;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation. Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models. We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost. Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs s
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#23558;&#27493;&#39588;&#25991;&#26412;&#19982;&#21270;&#23398;&#21453;&#24212;AI&#27169;&#22411;&#32467;&#21512;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#36866;&#37197;&#22120;&#27169;&#22411;&#21644;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#26631;&#35760;&#35757;&#32451;&#65292;&#33021;&#22815;&#25552;&#39640;&#23545;&#19981;&#22826;&#21487;&#33021;&#30340;&#21453;&#24212;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#21644;&#26356;&#22909;&#30340;&#29305;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17267</link><description>&lt;p&gt;
&#23558;&#21270;&#23398;&#21644;&#25991;&#26412;&#20449;&#24687;&#21512;&#24182;&#21040;&#21270;&#23398;&#21453;&#24212;AI&#27169;&#22411;&#20013;&#30340;ReacLLaMA
&lt;/p&gt;
&lt;p&gt;
ReacLLaMA: Merging chemical and textual information in chemical reactivity AI models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#23558;&#27493;&#39588;&#25991;&#26412;&#19982;&#21270;&#23398;&#21453;&#24212;AI&#27169;&#22411;&#32467;&#21512;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;&#36866;&#37197;&#22120;&#27169;&#22411;&#21644;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#26631;&#35760;&#35757;&#32451;&#65292;&#33021;&#22815;&#25552;&#39640;&#23545;&#19981;&#22826;&#21487;&#33021;&#30340;&#21453;&#24212;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#21644;&#26356;&#22909;&#30340;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21453;&#24212;&#27169;&#22411;&#34987;&#29992;&#26469;&#39044;&#27979;&#21270;&#23398;&#21453;&#24212;&#32467;&#26524;&#65292;&#24418;&#24335;&#21487;&#20197;&#26159;&#20998;&#31867;&#65288;&#25104;&#21151;/&#22833;&#36133;&#65289;&#25110;&#22238;&#24402;&#65288;&#20135;&#29289;&#25910;&#29575;&#65289;&#20219;&#21153;&#12290;&#32477;&#22823;&#22810;&#25968;&#25253;&#36947;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#21270;&#23398;&#20449;&#24687;&#65292;&#22914;&#21453;&#24212;&#29289;&#12289;&#20135;&#29289;&#12289;&#35797;&#21058;&#21644;&#28342;&#21058;&#65292;&#32780;&#19981;&#20351;&#29992;&#21512;&#25104;&#27493;&#39588;&#30340;&#32454;&#33410;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#27493;&#39588;&#25991;&#26412;&#32435;&#20837;Graphormer&#21453;&#24212;&#24615;&#27169;&#22411;&#20197;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#65306;&#35757;&#32451;&#19968;&#20010;&#36866;&#37197;&#22120;Graphormer&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;GPT-2&#30340;&#25991;&#26412;&#27493;&#39588;&#30340;&#28508;&#22312;&#34920;&#31034;&#65288;ReacLLaMA-Adapter&#65289;&#65292;&#20197;&#21450;&#20351;&#29992;LLaMA 2&#27169;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#20010;&#26410;&#26631;&#35760;&#37096;&#20998;&#65292;&#28982;&#21518;&#22312;&#25193;&#23637;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Graphormer&#27169;&#22411;&#65288;Zero-Shot Labeling ReacLLaMA&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#22686;&#24378;&#20102;&#23545;&#19981;&#22826;&#21487;&#33021;&#30340;&#21453;&#24212;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#21644;&#26356;&#22909;&#30340;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chemical reactivity models are developed to predict chemical reaction outcomes in the form of classification (success/failure) or regression (product yield) tasks. The vast majority of the reported models are trained solely on chemical information such as reactants, products, reagents, and solvents, but not on the details of a synthetic protocol. Herein incorporation of procedural text with the aim to augment the Graphormer reactivity model and improve its accuracy is presented. Two major approaches are used: training an adapter Graphormer model that is provided with a GPT-2-derived latent representation of the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a dataset with the LLaMA 2 model followed by training the Graphormer on an extended dataset (Zero-Shot Labeling ReacLLaMA). Both methodologies enhance the discernment of unpromising reactions, thereby providing more accurate models with improved specificity.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#65292;&#25105;&#20204;&#30340;ReAlnet&#27169;&#22411;&#19982;&#20154;&#33041;&#27963;&#21160;&#30456;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17231</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#23454;&#29616;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#30340;ReAlnet&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ReAlnet: Achieving More Human Brain-Like Vision via Human Neural Representational Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17231
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#65292;&#25105;&#20204;&#30340;ReAlnet&#27169;&#22411;&#19982;&#20154;&#33041;&#27963;&#21160;&#30456;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#29289;&#20307;&#35782;&#21035;&#27169;&#22411;&#22312;&#27169;&#25311;&#20154;&#33041;&#35270;&#35273;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#31070;&#32463;&#25968;&#25454;&#26469;&#27169;&#20223;&#22823;&#33041;&#22788;&#29702;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#38750;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#30340;&#20405;&#20837;&#24615;&#31070;&#32463;&#35760;&#24405;&#65292;&#36825;&#22312;&#25105;&#20204;&#23545;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#21644;&#24320;&#21457;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#27169;&#22411;&#30340;&#29702;&#35299;&#19978;&#23384;&#22312;&#30528;&#37325;&#35201;&#30340;&#32570;&#21475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#8220;Re(presentational)Al(ignment)net&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#20026;&#22522;&#30784;&#30340;&#19982;&#20154;&#33041;&#27963;&#21160;&#30456;&#23545;&#40784;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#19982;&#20154;&#33041;&#34920;&#31034;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22270;&#20687;&#21040;&#33041;&#22810;&#23618;&#32534;&#30721;&#23545;&#40784;&#26694;&#26550;&#19981;&#20165;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#22810;&#20010;&#23618;&#27425;&#65292;&#26631;&#24535;&#30528;&#31070;&#32463;&#23545;&#40784;&#26041;&#38754;&#30340;&#37325;&#22823;&#31361;&#30772;&#65292;&#32780;&#19988;&#36824;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#21644;&#27169;&#20223;&#20154;&#33041;&#30340;&#35270;&#35273;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable strides made in artificial intelligence, current object recognition models still lag behind in emulating the mechanism of visual information processing in human brains. Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often reply on invasive neural recordings from non-human subjects, leaving a critical gap in our understanding of human visual perception and the development of more human brain-like vision models. Addressing this gap, we present, for the first time, "Re(presentational)Al(ignment)net", a vision model aligned with human brain activity based on non-invasive EEG recordings, demonstrating a significantly higher similarity to human brain representations. Our innovative image-to-brain multi-layer encoding alignment framework not only optimizes multiple layers of the model, marking a substantial leap in neural alignment, but also enables the model to efficiently learn and mimic human brain's visua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MouSi&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#35270;&#35273;&#19987;&#23478;&#30340;&#38598;&#25104;&#25216;&#26415;&#26469;&#21327;&#21516;&#22788;&#29702;&#22270;&#20687;&#30340;&#35270;&#35273;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21333;&#19968;&#35270;&#35273;&#32452;&#20214;&#33021;&#21147;&#19981;&#36275;&#21644;&#36807;&#38271;&#30340;&#35270;&#35273;&#26631;&#35760;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20301;&#32622;&#28322;&#20986;&#21644;&#38271;&#24230;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.17221</link><description>&lt;p&gt;
MouSi&#65306;&#22810;&#35270;&#35273;&#19987;&#23478;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MouSi: Poly-Visual-Expert Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MouSi&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#35270;&#35273;&#19987;&#23478;&#30340;&#38598;&#25104;&#25216;&#26415;&#26469;&#21327;&#21516;&#22788;&#29702;&#22270;&#20687;&#30340;&#35270;&#35273;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21333;&#19968;&#35270;&#35273;&#32452;&#20214;&#33021;&#21147;&#19981;&#36275;&#21644;&#36807;&#38271;&#30340;&#35270;&#35273;&#26631;&#35760;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20301;&#32622;&#28322;&#20986;&#21644;&#38271;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24120;&#24120;&#38754;&#20020;&#35832;&#22914;&#21333;&#19968;&#35270;&#35273;&#32452;&#20214;&#33021;&#21147;&#19981;&#36275;&#21644;&#36807;&#38271;&#30340;&#35270;&#35273;&#26631;&#35760;&#31561;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#38480;&#21046;&#27169;&#22411;&#22312;&#20934;&#30830;&#35299;&#35835;&#22797;&#26434;&#35270;&#35273;&#20449;&#24687;&#21644;&#36807;&#38271;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23545;&#20110;&#25552;&#39640;VLMs&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#38598;&#25104;&#19987;&#23478;&#25216;&#26415;&#26469;&#21327;&#21516;&#21333;&#29420;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25797;&#38271;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#12289;OCR&#12289;&#22270;&#20687;&#20998;&#21106;&#31561;&#12290;&#35813;&#25216;&#26415;&#24341;&#20837;&#20102;&#34701;&#21512;&#32593;&#32476;&#65292;&#32479;&#19968;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#35270;&#35273;&#19987;&#23478;&#30340;&#36755;&#20986;&#65292;&#21516;&#26102;&#24357;&#21512;&#20102;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#65292;&#20197;&#20943;&#36731;&#38271;&#24207;&#21015;&#22270;&#20687;&#29305;&#24449;&#30340;&#20301;&#32622;&#32534;&#30721;&#28010;&#36153;&#38382;&#39064;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20301;&#32622;&#28322;&#20986;&#21644;&#38271;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information. Addressing these challenges is crucial for enhancing the performance and applicability of VLMs. This paper proposes the use of ensemble experts technique to synergizes the capabilities of individual visual encoders, including those skilled in image-text matching, OCR, image segmentation, etc. This technique introduces a fusion network to unify the processing of outputs from different visual experts, while bridging the gap between image encoders and pre-trained LLMs. In addition, we explore different positional encoding schemes to alleviate the waste of positional encoding caused by lengthy image feature sequences, effectively addressing the issue of position overflow and length
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NormEnsembleXAI&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;XAI&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#12289;&#26368;&#22823;&#21644;&#24179;&#22343;&#20989;&#25968;&#19982;&#26631;&#20934;&#21270;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#23545;XAI&#38598;&#25104;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#24211;&#65292;&#26041;&#20415;&#23454;&#29616;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17200</link><description>&lt;p&gt;
NormEnsembleXAI: &#25581;&#31034;XAI&#38598;&#25104;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
NormEnsembleXAI: Unveiling the Strengths and Weaknesses of XAI Ensemble Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NormEnsembleXAI&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;XAI&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#12289;&#26368;&#22823;&#21644;&#24179;&#22343;&#20989;&#25968;&#19982;&#26631;&#20934;&#21270;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#23545;XAI&#38598;&#25104;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#24211;&#65292;&#26041;&#20415;&#23454;&#29616;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#38598;&#25104;&#26041;&#27861;&#30340;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20570;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;NormEnsembleXAI&#65292;&#23427;&#21033;&#29992;&#26368;&#23567;&#12289;&#26368;&#22823;&#21644;&#24179;&#22343;&#20989;&#25968;&#19982;&#26631;&#20934;&#21270;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;XAI&#38598;&#25104;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#27934;&#23519;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24211;&#65292;&#20419;&#36827;&#20102;XAI&#38598;&#25104;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#25512;&#21160;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive comparative analysis of explainable artificial intelligence (XAI) ensembling methods. Our research brings three significant contributions. Firstly, we introduce a novel ensembling method, NormEnsembleXAI, that leverages minimum, maximum, and average functions in conjunction with normalization techniques to enhance interpretability. Secondly, we offer insights into the strengths and weaknesses of XAI ensemble methods. Lastly, we provide a library, facilitating the practical implementation of XAI ensembling, thus promoting the adoption of transparent and interpretable deep learning models.
&lt;/p&gt;</description></item><item><title>GraphViz2Vec&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#29305;&#24449;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#33410;&#28857;&#23616;&#37096;&#37051;&#22495;&#30340;&#32467;&#26500;&#20449;&#24687;&#26469;&#21019;&#24314;&#26377;&#24847;&#20041;&#30340;&#21021;&#22987;&#23884;&#20837;&#65292;&#24182;&#24110;&#21161;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.17178</link><description>&lt;p&gt;
GraphViz2Vec: &#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#29305;&#24449;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20998;&#31867;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
GraphViz2Vec: A Structure-aware Feature Generation Model to Improve Classification in GNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17178
&lt;/p&gt;
&lt;p&gt;
GraphViz2Vec&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#29305;&#24449;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#33410;&#28857;&#23616;&#37096;&#37051;&#22495;&#30340;&#32467;&#26500;&#20449;&#24687;&#26469;&#21019;&#24314;&#26377;&#24847;&#20041;&#30340;&#21021;&#22987;&#23884;&#20837;&#65292;&#24182;&#24110;&#21161;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#22312;&#20869;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;GNN&#20307;&#31995;&#32467;&#26500;&#20551;&#35774;&#21021;&#22987;&#23884;&#20837;&#26159;&#38543;&#26426;&#30340;&#25110;&#20174;&#27969;&#34892;&#30340;&#20998;&#24067;&#20013;&#29983;&#25104;&#30340;&#12290;&#36825;&#20123;&#21021;&#22987;&#23884;&#20837;&#38656;&#35201;&#22810;&#23618;&#36716;&#25442;&#25165;&#33021;&#25910;&#25947;&#20026;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#34429;&#28982;&#23618;&#25968;&#21487;&#20197;&#32047;&#31215;&#33410;&#28857;&#30340;&#26356;&#22823;&#37051;&#22495;&#65292;&#20294;&#20063;&#20250;&#24341;&#20837;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;GNN&#23545;&#20110;&#34920;&#31034;&#32467;&#26500;&#20449;&#24687;&#25928;&#26524;&#19981;&#20339;&#12290;&#20363;&#22914;&#65292;&#33410;&#28857;&#30340;&#36755;&#20986;&#23884;&#20837;&#19981;&#20250;&#25429;&#25417;&#21040;&#20854;&#21442;&#19982;&#30340;&#19977;&#35282;&#24418;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;GraphViz2Vec&#65292;&#21487;&#20197;&#25429;&#25417;&#33410;&#28857;&#23616;&#37096;&#37051;&#22495;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20026;GNN&#27169;&#22411;&#21019;&#24314;&#26377;&#24847;&#20041;&#30340;&#21021;&#22987;&#23884;&#20837;&#12290;&#36825;&#20123;&#21021;&#22987;&#23884;&#20837;&#26377;&#21161;&#20110;&#29616;&#26377;&#27169;&#22411;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#21021;&#22987;&#23884;&#20837;&#36824;&#24110;&#21161;&#27169;&#22411;&#20135;&#29983;...
&lt;/p&gt;
&lt;p&gt;
GNNs are widely used to solve various tasks including node classification and link prediction. Most of the GNN architectures assume the initial embedding to be random or generated from popular distributions. These initial embeddings require multiple layers of transformation to converge into a meaningful latent representation. While number of layers allow accumulation of larger neighbourhood of a node it also introduce the problem of over-smoothing. In addition, GNNs are inept at representing structural information. For example, the output embedding of a node does not capture its triangles participation. In this paper, we presented a novel feature extraction methodology GraphViz2Vec that can capture the structural information of a node's local neighbourhood to create meaningful initial embeddings for a GNN model. These initial embeddings helps existing models achieve state-of-the-art results in various classification tasks. Further, these initial embeddings help the model to produce des
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#65292;&#20294;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24615;&#19982;PDE-FIND&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2401.17177</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#21457;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Discovery of PDEs via the Adjoint Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#65292;&#20294;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24615;&#19982;PDE-FIND&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#26469;&#21457;&#29616;&#32473;&#23450;&#25968;&#25454;&#30340;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26159;&#20197;&#19968;&#33324;&#24418;&#24335;&#32771;&#34385;&#21442;&#25968;&#21270;&#30340;PDE&#65292;&#24182;&#21046;&#23450;&#26368;&#23567;&#21270;PDE&#35299;&#19982;&#25968;&#25454;&#35823;&#24046;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#21033;&#29992;&#21464;&#20998;&#35745;&#31639;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#65288;&#20276;&#38543;&#26041;&#31243;&#65289;&#30340;&#28436;&#21270;&#26041;&#31243;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#20986;&#19982;PDE&#21442;&#25968;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#19968;&#26063;&#21442;&#25968;&#21270;&#21644;&#38750;&#32447;&#24615;PDEs&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25512;&#23548;&#20986;&#30456;&#24212;&#30340;&#20276;&#38543;&#26041;&#31243;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#65292;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#20276;&#38543;&#26041;&#27861;&#21487;&#20197;&#20197;&#26426;&#22120;&#31934;&#24230;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20276;&#38543;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#19982;&#33879;&#21517;&#30340;PDE-FIND&#65288;Rudy et al., 2017&#65289;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form, and formulate the optimization problem that minimizes the error of PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, for a family of parameterized and nonlinear PDEs, we show how the corresponding adjoint equations can be derived. Here, we show that given smooth data set, the proposed adjoint method can recover the true PDE up to machine accuracy. However, in the presence of noise, the accuracy of the adjoint method becomes comparable to the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017). Even th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#29702;&#21644;&#26631;&#20934;&#21270;&#22810;&#20010;&#20844;&#20849;&#36164;&#28304;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#21010;&#20998;&#31574;&#30053;&#23558;&#25968;&#25454;&#38598;&#20998;&#20026;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#30340;&#39044;&#27979;&#30740;&#31350;&#30340;&#26631;&#20934;&#21270;&#21644;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2401.17174</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#25972;&#29702;&#19982;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A large dataset curation and benchmark for drug target interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25972;&#29702;&#21644;&#26631;&#20934;&#21270;&#22810;&#20010;&#20844;&#20849;&#36164;&#28304;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#21010;&#20998;&#31574;&#30053;&#23558;&#25968;&#25454;&#38598;&#20998;&#20026;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#30340;&#39044;&#27979;&#30740;&#31350;&#30340;&#26631;&#20934;&#21270;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#27963;&#24615;&#25968;&#25454;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#37325;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22240;&#20026;\textit{&#20307;&#22806;}&#21644;\textit{&#20307;&#20869;}&#23454;&#39564;&#30340;&#36164;&#28304;&#38656;&#27714;&#22823;&#20197;&#21450;&#25968;&#25454;&#39537;&#21160;&#30340;&#35745;&#31639;&#29983;&#29289;&#21270;&#23398;&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20102;\textit{&#20307;&#20869;}&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#65288;DTI&#65289;&#39044;&#27979;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#22823;&#22411;&#20844;&#24320;&#29983;&#29289;&#27963;&#24615;&#25968;&#25454;&#28304;&#65292;&#20294;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21487;&#20197;&#20174;&#23545;&#29616;&#26377;&#25968;&#25454;&#36164;&#28304;&#36827;&#34892;&#26356;&#22909;&#30340;&#26631;&#20934;&#21270;&#20013;&#21463;&#30410;&#12290;&#30446;&#21069;&#65292;&#19981;&#21516;&#30740;&#31350;&#24037;&#20316;&#24448;&#24448;&#38590;&#20197;&#27491;&#30830;&#27604;&#36739;&#65292;&#22240;&#20026;&#23427;&#20204;&#36873;&#25321;&#19981;&#21516;&#30340;&#25968;&#25454;&#28304;&#21644;&#35757;&#32451;/&#39564;&#35777;/&#27979;&#35797;&#38598;&#25286;&#20998;&#31574;&#30053;&#12290;&#21478;&#22806;&#65292;&#35768;&#22810;&#24037;&#20316;&#22522;&#20110;&#23567;&#25968;&#25454;&#23376;&#38598;&#65292;&#23548;&#33268;&#21487;&#33021;&#20855;&#26377;&#26377;&#38480;&#26377;&#25928;&#24615;&#30340;&#32467;&#26524;&#21644;&#27934;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#26631;&#20934;&#21270;&#21644;&#39640;&#25928;&#34920;&#31034;&#20174;&#22810;&#20010;&#20844;&#20849;&#36164;&#28304;&#20013;&#25972;&#29702;&#30340;&#38750;&#24120;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#26681;&#25454;&#19981;&#21516;&#30340;&#36873;&#25321;&#23558;&#25968;&#25454;&#38598;&#21010;&#20998;&#20026;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bioactivity data plays a key role in drug discovery and repurposing. The resource-demanding nature of \textit{in vitro} and \textit{in vivo} experiments, as well as the recent advances in data-driven computational biochemistry research, highlight the importance of \textit{in silico} drug target interaction (DTI) prediction approaches. While numerous large public bioactivity data sources exist, research in the field could benefit from better standardization of existing data resources. At present, different research works that share similar goals are often difficult to compare properly because of different choices of data sources and train/validation/test split strategies. Additionally, many works are based on small data subsets, leading to results and insights of possible limited validity. In this paper we propose a way to standardize and represent efficiently a very large dataset curated from multiple public sources, split the data into train, validation and test sets based on differen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#30340;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#20102;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2401.17173</link><description>&lt;p&gt;
&#36890;&#36807;&#20989;&#25968;&#32534;&#30721;&#22120;&#23454;&#29616;&#38646;-shot&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Reinforcement Learning via Function Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#30340;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#20102;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#20294;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#38590;&#28857;&#22312;&#20110;&#23547;&#25214;&#19968;&#20010;&#33391;&#22909;&#30340;&#34920;&#31034;&#26469;&#34920;&#36798;&#24403;&#21069;&#20219;&#21153;&#65292;&#20197;&#20415;&#20195;&#29702;&#31243;&#24207;&#29702;&#35299;&#23427;&#19982;&#20808;&#21069;&#30475;&#21040;&#30340;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#20989;&#25968;&#32534;&#30721;&#22120;&#26469;&#34920;&#31034;&#22870;&#21169;&#20989;&#25968;&#25110;&#36716;&#31227;&#20989;&#25968;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#19968;&#20010;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#26377;&#20851;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#30340;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#36801;&#31227;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#23558;&#22522;&#26412;RL&#31639;&#27861;&#19982;&#20989;&#25968;&#32534;&#30721;&#22120;&#32467;&#21512;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;RL&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#25928;&#29575;&#12289;&#28176;&#36817;&#24615;&#33021;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions. By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#29420;&#31435;&#20110;&#39046;&#22495;&#30340;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26684;&#26519;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#24452;&#21521;&#22522;&#20989;&#25968;&#26680;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#36793;&#30028;&#31215;&#20998;&#32593;&#32476;BIN-G&#20013;&#35780;&#20272;&#26684;&#26519;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#26041;&#26696;&#33021;&#22815;&#24555;&#36895;&#35757;&#32451;&#21644;&#20934;&#30830;&#35780;&#20272;&#20855;&#26377;&#21487;&#21464;&#31995;&#25968;&#30340;PDE&#30340;&#26684;&#26519;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2401.17172</link><description>&lt;p&gt;
&#23398;&#20064;&#29420;&#31435;&#20110;&#39046;&#22495;&#30340;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26684;&#26519;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Learning Domain-Independent Green's Function For Elliptic Partial Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#29420;&#31435;&#20110;&#39046;&#22495;&#30340;&#26925;&#22278;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26684;&#26519;&#20989;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#24452;&#21521;&#22522;&#20989;&#25968;&#26680;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#36793;&#30028;&#31215;&#20998;&#32593;&#32476;BIN-G&#20013;&#35780;&#20272;&#26684;&#26519;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#26041;&#26696;&#33021;&#22815;&#24555;&#36895;&#35757;&#32451;&#21644;&#20934;&#30830;&#35780;&#20272;&#20855;&#26377;&#21487;&#21464;&#31995;&#25968;&#30340;PDE&#30340;&#26684;&#26519;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26684;&#26519;&#20989;&#25968;&#25551;&#36848;&#20102;&#19968;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#23558;&#23427;&#30340;&#35299;&#22312;&#25972;&#20010;&#21306;&#22495;&#20013;&#26144;&#23556;&#20026;&#31215;&#20998;&#12290;&#25214;&#21040;&#26684;&#26519;&#20989;&#25968;&#30340;&#35299;&#26512;&#24418;&#24335;&#26159;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23450;&#20041;&#22312;&#22797;&#26434;&#21306;&#22495;&#19978;&#25110;&#20855;&#26377;&#21487;&#21464;&#31995;&#25968;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#32780;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#36793;&#30028;&#31215;&#20998;&#32593;&#32476;BIN-G&#65292;&#29992;&#20110;&#23398;&#20064;&#29420;&#31435;&#20110;&#39046;&#22495;&#30340;&#26684;&#26519;&#20989;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#24452;&#21521;&#22522;&#20989;&#25968;(RBF)&#26680;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;BIN-G&#20013;&#35780;&#20272;&#26684;&#26519;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;PDE&#30340;&#27531;&#24046;&#21644;&#39044;&#23450;&#20041;&#27979;&#35797;&#20989;&#25968;&#30340;&#36793;&#30028;&#31215;&#20998;&#26041;&#31243;&#30340;&#22343;&#26041;&#35823;&#24046;&#26469;&#35757;&#32451;BIN-G&#12290;&#36890;&#36807;&#21033;&#29992;&#26684;&#26519;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#24182;&#25511;&#21046;RBF&#26680;&#22312;&#26684;&#26519;&#20989;&#25968;&#22855;&#24322;&#28857;&#38468;&#36817;&#30340;&#32454;&#21270;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25968;&#20540;&#26041;&#26696;&#33021;&#22815;&#24555;&#36895;&#35757;&#32451;&#21644;&#20934;&#30830;&#35780;&#20272;&#20855;&#26377;&#21487;&#21464;&#31995;&#25968;&#30340;PDE&#30340;&#26684;&#26519;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Green's function characterizes a partial differential equation (PDE) and maps its solution in the entire domain as integrals. Finding the analytical form of Green's function is a non-trivial exercise, especially for a PDE defined on a complex domain or a PDE with variable coefficients. In this paper, we propose a novel boundary integral network to learn the domain-independent Green's function, referred to as BIN-G. We evaluate the Green's function in the BIN-G using a radial basis function (RBF) kernel-based neural network. We train the BIN-G by minimizing the residual of the PDE and the mean squared errors of the solutions to the boundary integral equations for prescribed test functions. By leveraging the symmetry of the Green's function and controlling refinements of the RBF kernel near the singularity of the Green function, we demonstrate that our numerical scheme enables fast training and accurate evaluation of the Green's function for PDEs with variable coefficients. The learned G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#30697;&#38453;&#29109;&#65292;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#20960;&#20309;&#21407;&#29702;&#30340;&#26032;&#22411;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#25454;&#21387;&#32553;&#33021;&#21147;&#12290;&#35813;&#25351;&#26631;&#21453;&#26144;&#20102;&#27169;&#22411;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#21644;&#28040;&#38500;&#19981;&#24517;&#35201;&#20803;&#32032;&#30340;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#37117;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17139</link><description>&lt;p&gt;
&#36890;&#36807;&#30697;&#38453;&#29109;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Evaluation via Matrix Entropy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#30697;&#38453;&#29109;&#65292;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#20960;&#20309;&#21407;&#29702;&#30340;&#26032;&#22411;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#25454;&#21387;&#32553;&#33021;&#21147;&#12290;&#35813;&#25351;&#26631;&#21453;&#26144;&#20102;&#27169;&#22411;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#21644;&#28040;&#38500;&#19981;&#24517;&#35201;&#20803;&#32032;&#30340;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#37117;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;&#33021;&#21147;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#39046;&#22495;&#65292;&#20351;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21457;&#29983;&#20102;&#38761;&#21629;&#12290;&#22240;&#27492;&#65292;&#20026;LLMs&#23450;&#20041;&#36866;&#24403;&#19988;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#25351;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30697;&#38453;&#29109;&#65292;&#19968;&#31181;&#26681;&#26893;&#20110;&#20449;&#24687;&#35770;&#21644;&#20960;&#20309;&#21407;&#29702;&#30340;&#26032;&#22411;&#25351;&#26631;&#65292;&#29992;&#20110;&#37327;&#21270;LLMs&#20013;&#30340;&#25968;&#25454;&#21387;&#32553;&#33021;&#21147;&#12290;&#23427;&#21453;&#26144;&#20102;&#27169;&#22411;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#21644;&#28040;&#38500;&#19981;&#24517;&#35201;&#20803;&#32032;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#33021;&#21147;&#30340;&#27934;&#23519;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21333;&#27169;&#24577;&#65288;&#35821;&#35328;&#65289;&#21644;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20986;&#34920;&#31034;&#30340;&#30697;&#38453;&#29109;&#22312;&#27169;&#22411;&#25193;&#22823;&#26102;&#36981;&#24490;&#19968;&#20010;&#32553;&#25918;&#23450;&#24459;&#31867;&#22411;&#30340;&#38477;&#20302;&#65292;&#36825;&#20316;&#20026;&#20256;&#32479;&#25439;&#22833;&#32553;&#25918;&#23450;&#24459;&#30340;&#34917;&#20805;&#12290;&#23545;&#20110;&#22810;&#27169;&#24577;&#35774;&#32622;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#29109;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19968;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized the field of natural language processing, extending their strong capabilities into multi-modal domains. Thus, it is vital to define proper and diversified metrics for the evaluation of LLMs.   In this paper, we introduce matrix entropy, a novel metric rooted in information theory and geometry principles to quantify the data compression proficiency in LLMs. It reflects the model's ability to extract relevant information and eliminate unnecessary elements, thereby providing insight into the language model's intrinsic capability. Specifically, we demonstrate its applicability in both single-modal (language) and multi-modal settings. For language models, our findings reveal that the matrix entropy of representations follows a scaling law type reduction when the model scales up, serving as a complement to the traditional loss scaling law. For the multi-modal setting, we also propose an evaluation method based on matrix entropy for assessing a
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#36830;&#25509;&#22806;&#22260;&#35774;&#22791;&#30340;&#36830;&#25509;&#31995;&#32479;&#12290;&#20316;&#32773;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#34013;&#29273;&#36890;&#20449;&#28192;&#36947;&#28431;&#27934;&#23545;&#26426;&#22120;&#23398;&#20064;&#34880;&#31958;&#30417;&#27979;&#31995;&#32479;&#36827;&#34892;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#30340;&#39118;&#38505;&#35780;&#20272;&#25216;&#26415;&#19981;&#36275;&#20197;&#24212;&#23545;&#36825;&#20123;&#26032;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2401.17136</link><description>&lt;p&gt;
&#31995;&#32479;&#35780;&#20272;AI/ML&#25903;&#25345;&#30340;&#36830;&#25509;&#20581;&#24247;&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Systematically Assessing the Security Risks of AI/ML-enabled Connected Healthcare Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17136
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#21307;&#30103;&#31995;&#32479;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#36830;&#25509;&#22806;&#22260;&#35774;&#22791;&#30340;&#36830;&#25509;&#31995;&#32479;&#12290;&#20316;&#32773;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#34013;&#29273;&#36890;&#20449;&#28192;&#36947;&#28431;&#27934;&#23545;&#26426;&#22120;&#23398;&#20064;&#34880;&#31958;&#30417;&#27979;&#31995;&#32479;&#36827;&#34892;&#25915;&#20987;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#30340;&#39118;&#38505;&#35780;&#20272;&#25216;&#26415;&#19981;&#36275;&#20197;&#24212;&#23545;&#36825;&#20123;&#26032;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#31995;&#32479;&#30340;&#37319;&#29992;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#12290;&#34429;&#28982;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26377;&#24456;&#22810;&#22909;&#22788;&#65292;&#20294;&#23427;&#20063;&#25193;&#22823;&#20102;&#21307;&#30103;&#31995;&#32479;&#30340;&#23041;&#32961;&#38754;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#21307;&#30103;&#31995;&#32479;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#36807;&#31243;&#20013;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#23558;&#26426;&#22120;&#23398;&#20064;&#24341;&#25806;&#19982;&#22810;&#20010;&#22806;&#22260;&#35774;&#22791;&#30456;&#36830;&#30340;&#36830;&#25509;&#31995;&#32479;&#20013;&#65292;&#23384;&#22312;&#23433;&#20840;&#39118;&#38505;&#65292;&#21487;&#33021;&#22312;&#25932;&#23545;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23545;&#24739;&#32773;&#30340;&#20581;&#24247;&#36896;&#25104;&#21361;&#23475;&#12290;&#36825;&#20123;&#26032;&#30340;&#39118;&#38505;&#26469;&#33258;&#22806;&#22260;&#35774;&#22791;&#21644;&#36890;&#20449;&#28192;&#36947;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#28436;&#31034;&#20102;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36890;&#36807;&#24341;&#20837;&#25932;&#23545;&#25968;&#25454;&#28857;&#23545;&#26426;&#22120;&#23398;&#20064;&#34880;&#31958;&#30417;&#27979;&#31995;&#32479;&#36827;&#34892;&#25915;&#20987;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#25163;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#36830;&#25509;&#34880;&#31958;&#20202;&#19982;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20043;&#38388;&#30340;&#34013;&#29273;&#36890;&#20449;&#28192;&#36947;&#20013;&#24050;&#30693;&#30340;&#28431;&#27934;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#39118;&#38505;&#35780;&#20272;&#25216;&#26415;&#19981;&#36275;&#20197;&#24212;&#23545;&#36825;&#20123;&#26032;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of machine-learning-enabled systems in the healthcare domain is on the rise. While the use of ML in healthcare has several benefits, it also expands the threat surface of medical systems. We show that the use of ML in medical systems, particularly connected systems that involve interfacing the ML engine with multiple peripheral devices, has security risks that might cause life-threatening damage to a patient's health in case of adversarial interventions. These new risks arise due to security vulnerabilities in the peripheral devices and communication channels. We present a case study where we demonstrate an attack on an ML-enabled blood glucose monitoring system by introducing adversarial data points during inference. We show that an adversary can achieve this by exploiting a known vulnerability in the Bluetooth communication channel connecting the glucose meter with the ML-enabled app. We further show that state-of-the-art risk assessment techniques are not adequate for i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#21450;&#36229;&#21442;&#25968;&#36873;&#25321;&#12290;&#36890;&#36807;&#20856;&#22411;&#26696;&#20363;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20302;&#20301;&#25968;&#21644;&#22823;&#37327;&#21270;&#23485;&#24230;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#36229;&#21442;&#25968;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2401.17269</link><description>&lt;p&gt;
&#26435;&#37325;&#37327;&#21270;&#23545;&#20856;&#22411;&#26696;&#20363;&#20998;&#26512;&#20013;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of Weight Quantization on Learning Models by Typical Case Analysis. (arXiv:2401.17269v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#37327;&#21270;&#26041;&#27861;&#21450;&#36229;&#21442;&#25968;&#36873;&#25321;&#12290;&#36890;&#36807;&#20856;&#22411;&#26696;&#20363;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20302;&#20301;&#25968;&#21644;&#22823;&#37327;&#21270;&#23485;&#24230;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#36229;&#21442;&#25968;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#20998;&#26512;&#27169;&#22411;&#20013;&#20351;&#29992;&#30340;&#37327;&#21270;&#26041;&#27861;&#21450;&#20854;&#36229;&#21442;&#25968;&#36873;&#25321;&#12290;&#38543;&#30528;&#25968;&#25454;&#20998;&#26512;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#26174;&#33879;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#65289;&#20013;&#65292;&#37327;&#21270;&#27169;&#22411;&#26435;&#37325;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#23545;&#20110;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#37096;&#32626;&#22823;&#22411;&#27169;&#22411;&#65292;&#37327;&#21270;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#65292;&#22914;&#20301;&#25968;&#21644;&#26435;&#37325;&#37327;&#21270;&#30340;&#20540;&#33539;&#22260;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#32463;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30340;&#20856;&#22411;&#26696;&#20363;&#20998;&#26512;&#26041;&#27861;&#65292;&#20855;&#20307;&#26159;&#37325;&#22797;&#26041;&#27861;&#65292;&#26469;&#25506;&#32034;&#36229;&#21442;&#25968;&#23545;&#31616;&#21333;&#23398;&#20064;&#27169;&#22411;&#37327;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#65288;i&#65289;&#23567;&#20301;&#25968;&#21644;&#22823;&#37327;&#21270;&#23485;&#24230;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#36229;&#21442;&#25968;&#38454;&#27573;&#65292;&#21363;&#37325;&#22797;&#23545;&#31216;&#24615;&#30772;&#32570;&#65307;&#65288;ii&#65289;
&lt;/p&gt;
&lt;p&gt;
This paper examines the quantization methods used in large-scale data analysis models and their hyperparameter choices. The recent surge in data analysis scale has significantly increased computational resource requirements. To address this, quantizing model weights has become a prevalent practice in data analysis applications such as deep learning. Quantization is particularly vital for deploying large models on devices with limited computational resources. However, the selection of quantization hyperparameters, like the number of bits and value range for weight quantization, remains an underexplored area. In this study, we employ the typical case analysis from statistical physics, specifically the replica method, to explore the impact of hyperparameters on the quantization of simple learning models. Our analysis yields three key findings: (i) an unstable hyperparameter phase, known as replica symmetry breaking, occurs with a small number of bits and a large quantization width; (ii) t
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;Syntax&#65292;&#19968;&#20010;&#20855;&#26377;&#21512;&#25104;&#23545;&#29031;&#32452;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#20122;&#32676;&#20307;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#27491;&#38754;&#27835;&#30103;&#25928;&#26524;&#30340;&#20122;&#32676;&#20307;&#65292;&#23545;&#20110;&#22810;&#26679;&#21270;&#24739;&#32773;&#21453;&#24212;&#30340;&#20020;&#24202;&#35797;&#39564;&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.17205</link><description>&lt;p&gt;
&#20855;&#26377;&#21512;&#25104;&#23545;&#29031;&#32452;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Adaptive Experiment Design with Synthetic Controls. (arXiv:2401.17205v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17205
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;Syntax&#65292;&#19968;&#20010;&#20855;&#26377;&#21512;&#25104;&#23545;&#29031;&#32452;&#30340;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#20122;&#32676;&#20307;&#20013;&#35782;&#21035;&#20986;&#20855;&#26377;&#27491;&#38754;&#27835;&#30103;&#25928;&#26524;&#30340;&#20122;&#32676;&#20307;&#65292;&#23545;&#20110;&#22810;&#26679;&#21270;&#24739;&#32773;&#21453;&#24212;&#30340;&#20020;&#24202;&#35797;&#39564;&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#36890;&#24120;&#29992;&#20110;&#20102;&#35299;&#26032;&#27835;&#30103;&#23545;&#32473;&#23450;&#24739;&#32773;&#32676;&#20307;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#32676;&#20307;&#20013;&#30340;&#24739;&#32773;&#24456;&#23569;&#20197;&#30456;&#21516;&#30340;&#26041;&#24335;&#23545;&#24453;&#30456;&#21516;&#30340;&#27835;&#30103;&#20570;&#20986;&#21453;&#24212;&#12290;&#24739;&#32773;&#21453;&#24212;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#36827;&#34892;&#22810;&#20010;&#20122;&#32676;&#20307;&#30340;&#25928;&#26524;&#30740;&#31350; - &#23588;&#20854;&#26159;&#24403;&#27835;&#30103;&#23545;&#25972;&#20307;&#32676;&#20307;&#27809;&#26377;&#25110;&#20960;&#20046;&#27809;&#26377;&#30410;&#22788;&#65292;&#32780;&#23545;&#29305;&#23450;&#20122;&#32676;&#20307;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#30410;&#22788;&#26102;&#12290;&#22522;&#20110;&#36825;&#31181;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Syntax&#65292;&#19968;&#31181;&#25506;&#32034;&#24615;&#35797;&#39564;&#35774;&#35745;&#65292;&#22312;&#20247;&#22810;&#20122;&#32676;&#20307;&#20013;&#35782;&#21035;&#20855;&#26377;&#27491;&#38754;&#27835;&#30103;&#25928;&#26524;&#30340;&#20122;&#32676;&#20307;&#12290;Syntax&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;(i) &#33258;&#36866;&#24212;&#25307;&#21215;&#21644;&#20998;&#37197;&#24739;&#32773;&#65292;(ii) &#36890;&#36807;&#21512;&#25104;&#23545;&#29031;&#32452;&#24418;&#25104;&#27599;&#20010;&#20122;&#32676;&#20307;&#30340;&#25511;&#21046;&#26679;&#26412;&#65292;&#20174;&#32780;&#20272;&#35745;&#27835;&#30103;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;Syntax&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#23427;&#20309;&#26102;&#21487;&#33021;&#20248;&#20110;&#20256;&#32479;&#35797;&#39564;&#35774;&#35745;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are typically run in order to understand the effects of a new treatment on a given population of patients. However, patients in large populations rarely respond the same way to the same treatment. This heterogeneity in patient responses necessitates trials that investigate effects on multiple subpopulations - especially when a treatment has marginal or no benefit for the overall population but might have significant benefit for a particular subpopulation. Motivated by this need, we propose Syntax, an exploratory trial design that identifies subpopulations with positive treatment effect among many subpopulations. Syntax is sample efficient as it (i) recruits and allocates patients adaptively and (ii) estimates treatment effects by forming synthetic controls for each subpopulation that combines control samples from other subpopulations. We validate the performance of Syntax and provide insights into when it might have an advantage over conventional trial designs through e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#24615;&#30340;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#24178;&#25200;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;&#35813;&#26426;&#21046;&#26082;&#25200;&#20081;&#20102;&#27468;&#25163;&#36523;&#20221;&#65292;&#21448;&#25200;&#20081;&#20102;&#27468;&#35789;&#65292;&#20351;&#24471;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;</title><link>http://arxiv.org/abs/2401.17133</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#30340;&#20027;&#21160;&#24615;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65306;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Proactive and Dual Prevention Mechanism against Illegal Song Covers empowered by Singing Voice Conversion. (arXiv:2401.17133v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17133
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#24615;&#30340;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#24178;&#25200;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;&#35813;&#26426;&#21046;&#26082;&#25200;&#20081;&#20102;&#27468;&#25163;&#36523;&#20221;&#65292;&#21448;&#25200;&#20081;&#20102;&#27468;&#35789;&#65292;&#20351;&#24471;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;(SVC)&#36890;&#36807;&#23558;&#19968;&#20010;&#27468;&#25163;&#30340;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#25104;&#21478;&#19968;&#20010;&#30446;&#26631;&#27468;&#25163;&#30340;&#27468;&#21809;&#22768;&#38899;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;&#27468;&#35789;&#21644;&#26059;&#24459;&#65292;&#33258;&#21160;&#21270;&#20102;&#27468;&#26354;&#32763;&#21809;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#29256;&#26435;&#21644;&#20844;&#27665;&#26435;&#21033;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; SongBsAb&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20027;&#21160;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110; SVC &#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;SongBsAb &#22312;&#21457;&#24067;&#27468;&#21809;&#22768;&#38899;&#20043;&#21069;&#24341;&#20837;&#20102;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#36825;&#26679;&#24403;&#23427;&#20204;&#34987;&#20351;&#29992;&#26102;&#65292;SVC &#30340;&#29983;&#25104;&#36807;&#31243;&#23558;&#34987;&#24178;&#25200;&#65292;&#23548;&#33268;&#24847;&#22806;&#30340;&#27468;&#21809;&#22768;&#38899;&#12290; SongBsAb &#20855;&#26377;&#21452;&#37325;&#39044;&#38450;&#25928;&#26524;&#65292;&#24341;&#36215;&#27468;&#25163;&#36523;&#20221;&#21644;&#27468;&#35789;&#30340;&#28151;&#20081;&#65292;&#21363; SVC &#35206;&#30422;&#30340;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;&#20026;&#20102;&#25552;&#39640;&#25200;&#21160;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20197;&#20276;&#22863;&#26354;&#20316;&#20026;&#39069;&#22806;&#25513;&#34109;&#32773;&#30340;&#22522;&#20110;&#24515;&#29702;&#22768;&#23398;&#27169;&#22411;&#30340;&#25439;&#22833;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Singing voice conversion (SVC) automates song covers by converting one singer's singing voice into another target singer's singing voice with the original lyrics and melody. However, it raises serious concerns about copyright and civil right infringements to multiple entities. This work proposes SongBsAb, the first proactive approach to mitigate unauthorized SVC-based illegal song covers. SongBsAb introduces human-imperceptible perturbations to singing voices before releasing them, so that when they are used, the generation process of SVC will be interfered, resulting in unexpected singing voices. SongBsAb features a dual prevention effect by causing both (singer) identity disruption and lyric disruption, namely, the SVC-covered singing voice neither imitates the target singer nor preserves the original lyrics. To improve the imperceptibility of perturbations, we refine a psychoacoustic model-based loss with the backing track as an additional masker, a unique accompanying element for s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#36755;&#20986;&#25200;&#21160;&#26041;&#27861;&#65288;PDP-OP&#65289;&#65292;&#21487;&#20197;&#22312;Ridge&#22238;&#24402;&#20013;&#35757;&#32451;&#20855;&#26377;&#27599;&#20010;&#25968;&#25454;&#28857;&#20010;&#24615;&#21270;&#38544;&#31169;&#27700;&#24179;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#38544;&#31169;&#35777;&#26126;&#21644;&#20934;&#30830;&#24230;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.17127</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#22312;Ridge&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Personalized Differential Privacy for Ridge Regression. (arXiv:2401.17127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#36755;&#20986;&#25200;&#21160;&#26041;&#27861;&#65288;PDP-OP&#65289;&#65292;&#21487;&#20197;&#22312;Ridge&#22238;&#24402;&#20013;&#35757;&#32451;&#20855;&#26377;&#27599;&#20010;&#25968;&#25454;&#28857;&#20010;&#24615;&#21270;&#38544;&#31169;&#27700;&#24179;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#38544;&#31169;&#35777;&#26126;&#21644;&#20934;&#30830;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25935;&#24863;&#39046;&#22495;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#22686;&#21152;&#65292;&#38656;&#35201;&#36890;&#36807;&#24046;&#20998;&#38544;&#31169;&#31561;&#38544;&#31169;&#26694;&#26550;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#12290;&#24046;&#20998;&#38544;&#31169;&#35201;&#27714;&#25351;&#23450;&#19968;&#20010;&#32479;&#19968;&#30340;&#38544;&#31169;&#27700;&#24179;&#949;&#65292;&#20197;&#34920;&#31034;&#27599;&#20010;&#25968;&#25454;&#28857;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#20013;&#24895;&#24847;&#23481;&#24525;&#30340;&#26368;&#22823;&#38544;&#31169;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#19981;&#21516;&#30340;&#25968;&#25454;&#28857;&#36890;&#24120;&#26377;&#19981;&#21516;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;&#21482;&#35774;&#32622;&#19968;&#20010;&#32479;&#19968;&#30340;&#38544;&#31169;&#27700;&#24179;&#36890;&#24120;&#36807;&#20110;&#38480;&#21046;&#65292;&#19981;&#20801;&#35768;&#23398;&#20064;&#22120;&#20197;&#22823;&#37327;&#30340;&#31934;&#24230;&#24320;&#38144;&#26469;&#20445;&#35777;&#20005;&#26684;&#30340;&#38544;&#31169;&#35201;&#27714;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#36755;&#20986;&#25200;&#21160;&#26041;&#27861;&#65288;PDP-OP&#65289;&#65292;&#23427;&#21487;&#20197;&#35757;&#32451;&#20855;&#26377;&#27599;&#20010;&#25968;&#25454;&#28857;&#20010;&#24615;&#21270;&#38544;&#31169;&#27700;&#24179;&#30340;Ridge&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#30340;PDP-OP&#30340;&#20005;&#26684;&#38544;&#31169;&#35777;&#26126;&#20197;&#21450;&#29983;&#25104;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#20445;&#35777;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#22312;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#26041;&#38754;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#29702;&#35770;&#20934;&#30830;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increased application of machine learning (ML) in sensitive domains requires protecting the training data through privacy frameworks, such as differential privacy (DP). DP requires to specify a uniform privacy level $\varepsilon$ that expresses the maximum privacy loss that each data point in the entire dataset is willing to tolerate. Yet, in practice, different data points often have different privacy requirements. Having to set one uniform privacy level is usually too restrictive, often forcing a learner to guarantee the stringent privacy requirement, at a large cost to accuracy. To overcome this limitation, we introduce our novel Personalized-DP Output Perturbation method (PDP-OP) that enables to train Ridge regression models with individual per data point privacy levels. We provide rigorous privacy proofs for our PDP-OP as well as accuracy guarantees for the resulting model. This work is the first to provide such theoretical accuracy guarantees when it comes to personalized DP 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#35889;&#20449;&#24687;&#30340;&#20809;&#35889;&#20849;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#20849;&#33976;&#39311;&#26694;&#26550;&#65292;&#24314;&#31435;&#20102;&#36890;&#29992;&#21644;&#20010;&#24615;&#21270;&#27169;&#22411;&#35757;&#32451;&#20043;&#38388;&#30340;&#21452;&#21521;&#26725;&#26753;&#12290;</title><link>http://arxiv.org/abs/2401.17124</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20809;&#35889;&#20849;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Spectral Co-Distillation for Personalized Federated Learning. (arXiv:2401.17124v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#35889;&#20449;&#24687;&#30340;&#20809;&#35889;&#20849;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#20849;&#33976;&#39311;&#26694;&#26550;&#65292;&#24314;&#31435;&#20102;&#36890;&#29992;&#21644;&#20010;&#24615;&#21270;&#27169;&#22411;&#35757;&#32451;&#20043;&#38388;&#30340;&#21452;&#21521;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#34987;&#24191;&#27867;&#30740;&#31350;&#20197;&#35299;&#20915;&#25968;&#25454;&#24322;&#26500;&#24615;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#24403;&#21333;&#20010;&#36890;&#29992;&#27169;&#22411;&#26080;&#27861;&#28385;&#36275;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#19981;&#21516;&#24615;&#33021;&#35201;&#27714;&#26102;&#12290;&#29616;&#26377;&#30340;PFL&#26041;&#27861;&#26412;&#36136;&#19978;&#22522;&#20110;&#36890;&#29992;&#20840;&#23616;&#27169;&#22411;&#21644;&#20010;&#24615;&#21270;&#26412;&#22320;&#27169;&#22411;&#20043;&#38388;&#30340;&#20851;&#31995;&#30001;&#27169;&#22411;&#26435;&#37325;&#30340;&#30456;&#20284;&#24615;&#25429;&#33719;&#30340;&#24605;&#24819;&#12290;&#36825;&#31181;&#30456;&#20284;&#24615;&#20027;&#35201;&#22522;&#20110;&#23558;&#27169;&#22411;&#26550;&#26500;&#21010;&#20998;&#20026;&#36890;&#29992;&#19982;&#20010;&#24615;&#21270;&#32452;&#20214;&#65292;&#25110;&#36890;&#36807;&#27169;&#22411;&#26435;&#37325;&#24314;&#27169;&#23458;&#25143;&#20851;&#31995;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25429;&#33719;&#30456;&#20284;&#65288;&#20294;&#19981;&#21516;&#30340;&#65289;&#36890;&#29992;&#19982;&#20010;&#24615;&#21270;&#27169;&#22411;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#35889;&#20449;&#24687;&#30340;&#26032;&#22411;&#33976;&#39311;&#26041;&#27861;&#65306;&#20809;&#35889;&#20849;&#33976;&#39311;&#12290;&#22312;&#20809;&#35889;&#20849;&#33976;&#39311;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#20849;&#33976;&#39311;&#26694;&#26550;&#65292;&#24314;&#31435;&#20102;&#36890;&#29992;&#21644;&#20010;&#24615;&#21270;&#27169;&#22411;&#35757;&#32451;&#20043;&#38388;&#30340;&#21452;&#21521;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized federated learning (PFL) has been widely investigated to address the challenge of data heterogeneity, especially when a single generic model is inadequate in satisfying the diverse performance requirements of local clients simultaneously. Existing PFL methods are inherently based on the idea that the relations between the generic global and personalized local models are captured by the similarity of model weights. Such a similarity is primarily based on either partitioning the model architecture into generic versus personalized components, or modeling client relationships via model weights. To better capture similar (yet distinct) generic versus personalized model representations, we propose \textit{spectral distillation}, a novel distillation method based on model spectrum information. Building upon spectral distillation, we also introduce a co-distillation framework that establishes a two-way bridge between generic and personalized model training. Moreover, to utilize th
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;GraphCG&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#35757;&#32451;&#22270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#30417;&#30563;&#21457;&#29616;&#21487;&#25805;&#32437;&#22240;&#32032;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35821;&#20041;&#20016;&#23500;&#26041;&#21521;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36825;&#20123;&#21487;&#25805;&#32437;&#22240;&#32032;&#12290;&#23454;&#39564;&#35777;&#26126;GraphCG&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.17123</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21457;&#29616;&#24403;&#22270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20986;&#29616;&#20132;&#32455;&#26102;&#30340;&#21487;&#25805;&#32437;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Discovery of Steerable Factors When Graph Deep Generative Models Are Entangled. (arXiv:2401.17123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17123
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;GraphCG&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#35757;&#32451;&#22270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#30417;&#30563;&#21457;&#29616;&#21487;&#25805;&#32437;&#22240;&#32032;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35821;&#20041;&#20016;&#23500;&#26041;&#21521;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36825;&#20123;&#21487;&#25805;&#32437;&#22240;&#32032;&#12290;&#23454;&#39564;&#35777;&#26126;GraphCG&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#24191;&#27867;&#29992;&#20110;&#22270;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#39044;&#35757;&#32451;&#22270;DGMs&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#29702;&#35299;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#36825;&#20123;&#29702;&#35299;&#26377;&#28508;&#21147;&#20026;&#37325;&#35201;&#20219;&#21153;&#25552;&#20379;&#26377;&#30410;&#30340;&#25351;&#23548;&#65292;&#20363;&#22914;&#22270;&#30340;&#21487;&#25511;&#21046;&#29983;&#25104;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#24456;&#24863;&#20852;&#36259;&#65292;&#24182;&#25552;&#20986;GraphCG&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#39044;&#35757;&#32451;&#22270;DGMs&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#30417;&#30563;&#21457;&#29616;&#21487;&#25805;&#32437;&#22240;&#32032;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20845;&#20010;&#35299;&#32544;&#24230;&#24230;&#37327;&#26631;&#20934;&#26816;&#39564;&#20102;&#19977;&#20010;&#39044;&#35757;&#32451;&#22270;DGMs&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#26159;&#20132;&#32455;&#30340;&#12290;&#21463;&#36825;&#20010;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;GraphCG&#36890;&#36807;&#26368;&#22823;&#21270;&#35821;&#20041;&#20016;&#23500;&#26041;&#21521;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#21487;&#25805;&#32437;&#22240;&#32032;&#65292;&#27839;&#30528;&#30456;&#21516;&#26041;&#21521;&#31227;&#21160;&#30340;&#22270;&#23558;&#20849;&#20139;&#30456;&#21516;&#30340;&#21487;&#25805;&#32437;&#22240;&#32032;&#12290;&#25105;&#20204;&#23450;&#37327;&#39564;&#35777;&#20102;GraphCG&#20248;&#20110;&#20854;&#20182;&#22235;&#20010;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models (DGMs) have been widely developed for graph data. However, much less investigation has been carried out on understanding the latent space of such pretrained graph DGMs. These understandings possess the potential to provide constructive guidelines for crucial tasks, such as graph controllable generation. Thus in this work, we are interested in studying this problem and propose GraphCG, a method for the unsupervised discovery of steerable factors in the latent space of pretrained graph DGMs. We first examine the representation space of three pretrained graph DGMs with six disentanglement metrics, and we observe that the pretrained representation space is entangled. Motivated by this observation, GraphCG learns the steerable factors via maximizing the mutual information between semantic-rich directions, where the controlled graph moving along the same direction will share the same steerable factors. We quantitatively verify that GraphCG outperforms four competitive 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#19987;&#23478;&#28151;&#21512;&#8221;&#21407;&#29702;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28784;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#31995;&#32479;&#30340;&#20934;&#30830;&#24314;&#27169;&#65292;&#24182;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.17118</link><description>&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#65306;&#23454;&#29616;&#28784;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#30340;&#26377;&#25928;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Explainable data-driven modeling via mixture of experts: towards effective blending of grey and black-box models. (arXiv:2401.17118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17118
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#19987;&#23478;&#28151;&#21512;&#8221;&#21407;&#29702;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28784;&#30418;&#21644;&#40657;&#30418;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39537;&#21160;&#30340;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#31995;&#32479;&#30340;&#20934;&#30830;&#24314;&#27169;&#65292;&#24182;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#27169;&#22411;&#24120;&#24120;&#22312;&#20934;&#30830;&#24615;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34429;&#28982;&#21151;&#33021;&#24378;&#22823;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#22788;&#29702;&#29289;&#29702;&#32422;&#26463;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#21162;&#21147;&#24448;&#24448;&#24456;&#38590;&#22312;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#20043;&#38388;&#25214;&#21040;&#24179;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#8220;&#19987;&#23478;&#28151;&#21512;&#8221;&#21407;&#29702;&#30340;&#32508;&#21512;&#26694;&#26550;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23545;&#22810;&#26679;&#21270;&#30340;&#23616;&#37096;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#25968;&#25454;&#30340;&#34701;&#21512;&#65292;&#21033;&#29992;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20801;&#35768;&#19987;&#23478;&#30340;&#29420;&#31435;&#35757;&#32451;&#65292;&#20511;&#37492;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#31995;&#32479;&#35782;&#21035;&#30340;&#25216;&#26415;&#65292;&#24182;&#25903;&#25345;&#21327;&#20316;&#21644;&#31454;&#20105;&#23398;&#20064;&#33539;&#24335;&#12290;&#20026;&#20102;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#23545;&#19987;&#23478;&#30340;&#32452;&#21512;&#20013;&#30340;&#31361;&#21464;&#36827;&#34892;&#20102;&#24809;&#32602;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32452;&#21512;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional models grounded in first principles often struggle with accuracy as the system's complexity increases. Conversely, machine learning approaches, while powerful, face challenges in interpretability and in handling physical constraints. Efforts to combine these models often often stumble upon difficulties in finding a balance between accuracy and complexity. To address these issues, we propose a comprehensive framework based on a "mixture of experts" rationale. This approach enables the data-based fusion of diverse local models, leveraging the full potential of first-principle-based priors. Our solution allows independent training of experts, drawing on techniques from both machine learning and system identification, and it supports both collaborative and competitive learning paradigms. To enhance interpretability, we penalize abrupt variations in the expert's combination. Experimental results validate the effectiveness of our approach in producing an interpretable combination
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;Yang-Baxter&#26041;&#31243;&#26469;&#32531;&#35299;&#21644;&#20462;&#27491;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#38169;&#35823;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25511;&#21046;&#22122;&#22768;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#32463;&#20856;&#35745;&#31639;&#36827;&#34892;&#38169;&#35823;&#32531;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#32416;&#27491;&#26102;&#38388;&#28436;&#21270;&#30340;&#37327;&#23376;&#24577;&#20013;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2401.17116</link><description>&lt;p&gt;
&#36890;&#36807;Yang-Baxter&#26041;&#31243;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20171;&#23548;&#30340;&#37327;&#23376;&#38169;&#35823;&#32531;&#35299;&#21644;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Quantum error mitigation and correction mediated by Yang-Baxter equation and artificial neural network. (arXiv:2401.17116v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17116
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;Yang-Baxter&#26041;&#31243;&#26469;&#32531;&#35299;&#21644;&#20462;&#27491;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#38169;&#35823;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25511;&#21046;&#22122;&#22768;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#32463;&#20856;&#35745;&#31639;&#36827;&#34892;&#38169;&#35823;&#32531;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#32416;&#27491;&#26102;&#38388;&#28436;&#21270;&#30340;&#37327;&#23376;&#24577;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#38169;&#35823;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#21644;Yang-Baxter&#26041;&#31243;&#65288;YBE&#65289;&#26469;&#32531;&#35299;&#37327;&#23376;&#38169;&#35823;&#30340;&#26032;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#38169;&#35823;&#20462;&#27491;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#35745;&#31639;&#37327;&#24456;&#22823;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#24037;&#38169;&#35823;&#32531;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#23376;&#38169;&#35823;&#26469;&#28304;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#24182;&#25506;&#35752;&#20102;&#20351;&#29992;&#32463;&#20856;&#35745;&#31639;&#26469;&#36827;&#34892;&#38169;&#35823;&#32531;&#35299;&#30340;&#28508;&#21147;&#12290;Yang-Baxter&#26041;&#31243;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#26102;&#38388;&#21160;&#21147;&#23398;&#27169;&#25311;&#21387;&#32553;&#21040;&#24658;&#23450;&#28145;&#24230;&#30340;&#30005;&#36335;&#20013;&#12290;&#36890;&#36807;&#24341;&#20837;&#36890;&#36807;YBE&#25511;&#21046;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#29992;&#20110;&#38169;&#35823;&#32531;&#35299;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#37096;&#20998;&#37327;&#23376;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;ANN&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32416;&#27491;&#26102;&#38388;&#28436;&#21270;&#30340;&#37327;&#23376;&#24577;&#20013;&#30340;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing shows great potential, but errors pose a significant challenge. This study explores new strategies for mitigating quantum errors using artificial neural networks (ANN) and the Yang-Baxter equation (YBE). Unlike traditional error correction methods, which are computationally intensive, we investigate artificial error mitigation. The manuscript introduces the basics of quantum error sources and explores the potential of using classical computation for error mitigation. The Yang-Baxter equation plays a crucial role, allowing us to compress time dynamics simulations into constant-depth circuits. By introducing controlled noise through the YBE, we enhance the dataset for error mitigation. We train an ANN model on partial data from quantum simulations, demonstrating its effectiveness in correcting errors in time-evolving quantum states.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#39046;&#22495;&#23545;&#20110;&#35780;&#20272;&#26041;&#27861;&#30340;&#36873;&#25321;&#32570;&#20047;&#20849;&#35782;&#65292;&#26412;&#32508;&#36848;&#36890;&#36807;&#23545;&#29616;&#26377;&#35780;&#20272;&#25216;&#26415;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#26631;&#20934;&#21270;&#35780;&#20272;&#23454;&#36341;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.17109</link><description>&lt;p&gt;
&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#35780;&#20272;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Evaluation in Neural Style Transfer: A Review. (arXiv:2401.17109v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17109
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#39046;&#22495;&#23545;&#20110;&#35780;&#20272;&#26041;&#27861;&#30340;&#36873;&#25321;&#32570;&#20047;&#20849;&#35782;&#65292;&#26412;&#32508;&#36848;&#36890;&#36807;&#23545;&#29616;&#26377;&#35780;&#20272;&#25216;&#26415;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#26631;&#20934;&#21270;&#35780;&#20272;&#23454;&#36341;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#26041;&#27861;&#33021;&#22815;&#21512;&#25104;&#20986;&#33402;&#26415;&#24615;&#21644;&#36924;&#30495;&#24230;&#26497;&#39640;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#32467;&#26524;&#65292;&#20351;&#29992;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#25351;&#26631;&#65292;&#21253;&#25324;&#22522;&#20110;&#27604;&#36739;&#30340;&#20316;&#32773;&#35266;&#28857;&#12289;&#37327;&#21270;&#21442;&#19982;&#32773;&#20027;&#35266;&#21028;&#26029;&#30340;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#65292;&#20197;&#21450;&#37327;&#21270;&#35745;&#31639;&#25351;&#26631;&#65292;&#23458;&#35266;&#35780;&#20272;&#31639;&#27861;&#24615;&#33021;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#26368;&#36866;&#21512;&#21644;&#26368;&#26377;&#25928;&#30340;&#35780;&#20272;&#31243;&#24207;&#20197;&#30830;&#20445;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#19968;&#33268;&#30340;&#20849;&#35782;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#35780;&#20272;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21457;&#29616;&#20102;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#26631;&#20934;&#21270;&#35780;&#20272;&#23454;&#36341;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24314;&#31435;&#19968;&#20010;&#24378;&#22823;&#30340;&#35780;&#20272;&#26694;&#26550;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Neural Style Transfer (NST) has witnessed remarkable progress in the past few years, with approaches being able to synthesize artistic and photorealistic images and videos of exceptional quality. To evaluate such results, a diverse landscape of evaluation methods and metrics is used, including authors' opinions based on side-by-side comparisons, human evaluation studies that quantify the subjective judgements of participants, and a multitude of quantitative computational metrics which objectively assess the different aspects of an algorithm's performance. However, there is no consensus regarding the most suitable and effective evaluation procedure that can guarantee the reliability of the results. In this review, we provide an in-depth analysis of existing evaluation techniques, identify the inconsistencies and limitations of current evaluation methods, and give recommendations for standardized evaluation practices. We believe that the development of a robust evaluation fr
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#22797;&#26434;&#24615;&#23383;&#31526;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.17098</link><description>&lt;p&gt;
CharNet&#65306;&#39640;&#22797;&#26434;&#24615;&#23383;&#31526;&#20998;&#31867;&#30340;&#24191;&#20041;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CharNet: Generalized Approach for High-Complexity Character Classification. (arXiv:2401.17098v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17098
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#22797;&#26434;&#24615;&#23383;&#31526;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#25163;&#20889;&#23383;&#31526;&#35782;&#21035;&#65288;HCR&#65289;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19982;&#25171;&#21360;&#25991;&#26412;&#25968;&#25454;&#19981;&#21516;&#65292;&#25163;&#20889;&#23383;&#31526;&#25968;&#25454;&#38598;&#30001;&#20110;&#20154;&#31867;&#23548;&#33268;&#30340;&#20559;&#24046;&#32780;&#20855;&#26377;&#26356;&#22810;&#30340;&#21464;&#21270;&#12290;&#22522;&#20110;&#29420;&#29305;&#23383;&#31526;&#31867;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#35937;&#24418;&#25991;&#23383;&#25110;&#27721;&#23383;&#38889;&#23383;&#23383;&#31526;&#24207;&#21015;&#65292;&#32473;HCR&#38382;&#39064;&#24102;&#26469;&#26032;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#23398;&#20064;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#22270;&#20687;&#30340;&#39640;&#22797;&#26434;&#24615;&#32454;&#33410;&#12290;&#38543;&#30528;&#35745;&#31639;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#36827;&#19968;&#27493;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#29702;&#35770;&#30340;&#21457;&#23637;&#65292;&#19968;&#20123;&#30740;&#31350;&#22242;&#38431;&#24050;&#32463;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20986;&#29616;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#20197;&#39640;&#25928;&#29575;&#32780;&#38395;&#21517;&#65292;&#20294;&#35768;&#22810;&#24120;&#35265;&#30340;&#26041;&#27861;&#20173;&#28982;&#19981;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#38598;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#30001;&#20110;&#22797;&#26434;&#30340;&#32467;&#26500;&#21644;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#29616;&#26377;&#26041;&#27861;&#32463;&#24120;&#38459;&#27490;&#35299;&#20915;&#26041;&#26696;&#27969;&#34892;&#36215;&#26469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Handwritten character recognition (HCR) is a challenging problem for machine learning researchers. Unlike printed text data, handwritten character datasets have more variation due to human-introduced bias. With numerous unique character classes present, some data, such as Logographic Scripts or Sino-Korean character sequences, bring new complications to the HCR problem. The classification task on such datasets requires the model to learn high-complexity details of the images that share similar features. With recent advances in computational resource availability and further computer vision theory development, some research teams have effectively addressed the arising challenges. Although known for achieving high efficiency, many common approaches are still not generalizable and use dataset-specific solutions to achieve better results. Due to complex structure and high computing demands, existing methods frequently prevent the solutions from gaining popularity. This paper proposes a str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23439;&#35266;&#27169;&#22411;&#21644;&#22810;&#28304;&#26102;&#31354;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#26080;&#27861;&#35266;&#27979;&#21040;&#30340;&#32593;&#32476;&#20301;&#32622;&#30340;&#20132;&#36890;&#27969;&#37327;&#21644;&#34892;&#39542;&#26102;&#38388;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20256;&#24863;&#22120;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#24182;&#28385;&#36275;&#22522;&#26412;&#30340;&#27969;&#37327;&#23432;&#24658;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2401.17095</link><description>&lt;p&gt;
&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#32593;&#32476;&#20301;&#32622;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#23439;&#35266;&#27169;&#22411;&#36827;&#34892;&#20132;&#36890;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Traffic estimation in unobserved network locations using data-driven macroscopic models. (arXiv:2401.17095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23439;&#35266;&#27169;&#22411;&#21644;&#22810;&#28304;&#26102;&#31354;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#26080;&#27861;&#35266;&#27979;&#21040;&#30340;&#32593;&#32476;&#20301;&#32622;&#30340;&#20132;&#36890;&#27969;&#37327;&#21644;&#34892;&#39542;&#26102;&#38388;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20256;&#24863;&#22120;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#24182;&#28385;&#36275;&#22522;&#26412;&#30340;&#27969;&#37327;&#23432;&#24658;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#23439;&#35266;&#27169;&#22411;&#21644;&#33258;&#21160;&#20132;&#36890;&#35745;&#25968;&#22120;&#21644;&#25506;&#27979;&#36710;&#36742;&#25910;&#38598;&#30340;&#22810;&#28304;&#26102;&#31354;&#25968;&#25454;&#65292;&#20934;&#30830;&#20272;&#35745;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#27979;&#37327;&#25968;&#25454;&#30340;&#36335;&#27573;&#30340;&#20132;&#36890;&#27969;&#37327;&#21644;&#34892;&#39542;&#26102;&#38388;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#20132;&#36890;&#35268;&#21010;&#24212;&#29992;&#20013;&#26159;&#20851;&#38190;&#30340;&#65292;&#22240;&#20026;&#20256;&#24863;&#22120;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#65292;&#35268;&#21010;&#30340;&#24178;&#39044;&#25514;&#26045;&#20250;&#23545;&#25972;&#20010;&#32593;&#32476;&#20135;&#29983;&#24433;&#21709;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#21629;&#21517;&#20026;&#23439;&#35266;&#20132;&#36890;&#20272;&#35745;&#22120;&#65288;MaTE&#65289;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#36825;&#20123;&#25968;&#37327;&#30340;&#35266;&#27979;&#27979;&#37327;&#26469;&#36827;&#34892;&#32593;&#32476;&#33539;&#22260;&#30340;&#20132;&#36890;&#27969;&#37327;&#21644;&#34892;&#39542;&#26102;&#38388;&#20272;&#35745;&#12290;&#30001;&#20110;MaTE&#22522;&#20110;&#23439;&#35266;&#27969;&#37327;&#29702;&#35770;&#65292;&#25152;&#26377;&#21442;&#25968;&#21644;&#21464;&#37327;&#37117;&#26159;&#21487;&#20197;&#35299;&#37322;&#30340;&#12290;&#20272;&#35745;&#30340;&#20132;&#36890;&#27969;&#37327;&#28385;&#36275;&#22522;&#26412;&#30340;&#27969;&#37327;&#23432;&#24658;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#19988;&#19982;&#20272;&#35745;&#30340;&#34892;&#39542;&#26102;&#38388;&#21576;&#36882;&#22686;&#30340;&#21333;&#35843;&#20851;&#31995;&#12290;&#23558;&#22522;&#20110;logit&#30340;&#38543;&#26426;&#20132;&#36890;&#20998;&#37197;&#20316;&#20026;&#27969;&#37327;&#34892;&#20026;&#36335;&#30001;&#30340;&#21407;&#21017;&#20351;&#24471;&#27169;&#22411;&#22312;&#21487;&#24494;&#26041;&#38754;&#20855;&#26377;&#23436;&#20840;&#21487;&#21306;&#20998;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper leverages macroscopic models and multi-source spatiotemporal data collected from automatic traffic counters and probe vehicles to accurately estimate traffic flow and travel time in links where these measurements are unavailable. This problem is critical in transportation planning applications where the sensor coverage is low and the planned interventions have network-wide impacts. The proposed model, named the Macroscopic Traffic Estimator (MaTE), can perform network-wide estimations of traffic flow and travel time only using the set of observed measurements of these quantities. Because MaTE is grounded in macroscopic flow theory, all parameters and variables are interpretable. The estimated traffic flow satisfies fundamental flow conservation constraints and exhibits an increasing monotonic relationship with the estimated travel time. Using logit-based stochastic traffic assignment as the principle for routing flow behavior makes the model fully differentiable with respect
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#28508;&#22312;&#29366;&#24577;&#26469;&#23398;&#20064;&#20010;&#20307;&#29305;&#23450;&#30340;&#35745;&#25968;&#36807;&#31243;&#24378;&#24230;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#22312;&#31614;&#21517;&#31354;&#38388;&#20013;&#32447;&#24615;&#21270;&#27169;&#22411;&#65292;&#24471;&#21040;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#30340;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#23545;&#37329;&#34701;&#12289;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#39135;&#21697;&#20379;&#24212;&#38142;&#31649;&#29702;&#31561;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.17077</link><description>&lt;p&gt;
&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#19982;&#25511;&#21046;&#28508;&#22312;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Dynamical Survival Analysis with Controlled Latent States. (arXiv:2401.17077v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#28508;&#22312;&#29366;&#24577;&#26469;&#23398;&#20064;&#20010;&#20307;&#29305;&#23450;&#30340;&#35745;&#25968;&#36807;&#31243;&#24378;&#24230;&#12290;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36275;&#22815;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#22312;&#31614;&#21517;&#31354;&#38388;&#20013;&#32447;&#24615;&#21270;&#27169;&#22411;&#65292;&#24471;&#21040;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#30340;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#23545;&#37329;&#34701;&#12289;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#39135;&#21697;&#20379;&#24212;&#38142;&#31649;&#29702;&#31561;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#19968;&#32452;&#38745;&#24577;&#21464;&#37327;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#23398;&#20064;&#20010;&#20307;&#29305;&#23450;&#30340;&#35745;&#25968;&#36807;&#31243;&#24378;&#24230;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#20854;&#20013;&#24378;&#24230;&#26159;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#26469;&#35774;&#35745;&#19968;&#20010;&#31070;&#32463;&#20272;&#35745;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36275;&#22815;&#27491;&#21017;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#31614;&#21517;&#31354;&#38388;&#20013;&#32447;&#24615;&#21270;&#65292;&#24471;&#21040;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#30340;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;CoxSig&#12290;&#25105;&#20204;&#20026;&#36825;&#20004;&#31181;&#20272;&#35745;&#22120;&#25552;&#20379;&#29702;&#35770;&#23398;&#20064;&#20445;&#35777;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#37329;&#34701;&#12289;&#39044;&#27979;&#24615;&#32500;&#25252;&#21644;&#39135;&#21697;&#20379;&#24212;&#38142;&#31649;&#29702;&#31561;&#21508;&#31181;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of learning individual-specific intensities of counting processes from a set of static variables and irregularly sampled time series. We introduce a novel modelization approach in which the intensity is the solution to a controlled differential equation. We first design a neural estimator by building on neural controlled differential equations. In a second time, we show that our model can be linearized in the signature space under sufficient regularity conditions, yielding a signature-based estimator which we call CoxSig. We provide theoretical learning guarantees for both estimators, before showcasing the performance of our models on a vast array of simulated and real-world datasets from finance, predictive maintenance and food supply chain management.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#27979;&#35797;ML&#27169;&#22411;&#21644;&#22522;&#20110;ML&#30340;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29420;&#31435;&#39564;&#35777;&#20854;&#40657;&#30418;&#29305;&#24615;&#21644;&#38543;&#26426;&#23646;&#24615;&#12290;&#24314;&#35758;&#25193;&#23637;&#29616;&#26377;&#27979;&#35797;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;ML&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.17062</link><description>&lt;p&gt;
&#19968;&#31181;&#29420;&#31435;&#30340;&#31995;&#32479;&#21270;&#40657;&#30418;&#27979;&#35797;ML&#31995;&#32479;&#30340;&#27010;&#35201;
&lt;/p&gt;
&lt;p&gt;
Outline of an Independent Systematic Blackbox Test for ML-based Systems. (arXiv:2401.17062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#27979;&#35797;ML&#27169;&#22411;&#21644;&#22522;&#20110;ML&#30340;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29420;&#31435;&#39564;&#35777;&#20854;&#40657;&#30418;&#29305;&#24615;&#21644;&#38543;&#26426;&#23646;&#24615;&#12290;&#24314;&#35758;&#25193;&#23637;&#29616;&#26377;&#27979;&#35797;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;ML&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#31243;&#24207;&#65292;&#21487;&#29992;&#20110;&#29420;&#31435;&#20110;&#23454;&#38469;&#35757;&#32451;&#36807;&#31243;&#27979;&#35797;ML&#27169;&#22411;&#21644;&#22522;&#20110;ML&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#32771;&#34385;&#20854;&#40657;&#30418;&#29305;&#24615;&#21644;ML&#27169;&#22411;&#21450;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#22266;&#26377;&#38543;&#26426;&#23646;&#24615;&#65292;&#21487;&#20197;&#29420;&#31435;&#39564;&#35777;&#36825;&#20123;&#27169;&#22411;&#21644;&#31995;&#32479;&#30340;&#20856;&#22411;&#36136;&#37327;&#21442;&#25968;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#27979;&#35797;&#23454;&#39564;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#29616;&#26377;&#27979;&#35797;&#26041;&#27861;&#20197;&#21453;&#26144;ML&#27169;&#22411;&#21644;&#22522;&#20110;ML&#30340;&#31995;&#32479;&#30340;&#38543;&#26426;&#29305;&#24615;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article proposes a test procedure that can be used to test ML models and ML-based systems independently of the actual training process. In this way, the typical quality statements such as accuracy and precision of these models and system can be verified independently, taking into account their black box character and the immanent stochastic properties of ML models and their training data. The article presents first results from a set of test experiments and suggest extensions to existing test methods reflecting the stochastic nature of ML models and ML-based systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#34987;&#23631;&#34109;&#29305;&#24449;&#65292;&#32467;&#21512;KNN&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36873;&#25321;&#30456;&#20851;&#26679;&#26412;&#26469;&#24110;&#21161;&#30446;&#26631;&#26679;&#26412;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20351;&#29992;&#38750;&#21442;&#25968;&#21270;&#20851;&#31995;&#36827;&#34892;&#26816;&#32034;&#22686;&#24378;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.17052</link><description>&lt;p&gt;
&#20351;&#21442;&#25968;&#24322;&#24120;&#26816;&#27979;&#20877;&#27425;&#21464;&#24471;&#38750;&#21442;&#25968;&#21270;&#30340;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Making Parametric Anomaly Detection on Tabular Data Non-Parametric Again. (arXiv:2401.17052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#34987;&#23631;&#34109;&#29305;&#24449;&#65292;&#32467;&#21512;KNN&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36873;&#25321;&#30456;&#20851;&#26679;&#26412;&#26469;&#24110;&#21161;&#30446;&#26631;&#26679;&#26412;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20351;&#29992;&#38750;&#21442;&#25968;&#21270;&#20851;&#31995;&#36827;&#34892;&#26816;&#32034;&#22686;&#24378;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20294;&#26159;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#65292;&#20351;&#29992;&#28145;&#24230;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#25968;&#25454;&#26102;&#25928;&#26524;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#22312;&#30417;&#30563;&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#21644;&#22238;&#24402;&#65289;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;Transformer&#27169;&#22411;&#23398;&#20064;&#37325;&#24314;&#8220;&#27491;&#24120;&#8221;&#26679;&#26412;&#30340;&#34987;&#23631;&#34109;&#29305;&#24449;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22522;&#20110;KNN&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#27169;&#22359;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#36873;&#25321;&#30456;&#20851;&#26679;&#26412;&#26469;&#24110;&#21161;&#30446;&#26631;&#26679;&#26412;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;31&#20010;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#36890;&#36807;&#20351;&#29992;&#38750;&#21442;&#25968;&#21270;&#20851;&#31995;&#36827;&#34892;&#26816;&#32034;&#22686;&#24378;&#30340;&#24322;&#24120;&#26816;&#27979;&#65288;AD&#65289;&#26041;&#27861;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning for tabular data has garnered increasing attention in recent years, yet employing deep models for structured data remains challenging. While these models excel with unstructured data, their efficacy with structured data has been limited. Recent research has introduced retrieval-augmented models to address this gap, demonstrating promising results in supervised tasks such as classification and regression. In this work, we investigate using retrieval-augmented models for anomaly detection on tabular data. We propose a reconstruction-based approach in which a transformer model learns to reconstruct masked features of \textit{normal} samples. We test the effectiveness of KNN-based and attention-based modules to select relevant samples to help in the reconstruction process of the target sample. Our experiments on a benchmark of 31 tabular datasets reveal that augmenting this reconstruction-based anomaly detection (AD) method with non-parametric relationships via retrieval modu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#27874;&#21160;&#24615;&#25351;&#25968;VIX&#65292;&#24182;&#37319;&#29992;&#27010;&#29575;&#23545;&#24212;&#27169;&#22411;&#21644;&#26631;&#20934;&#24046;&#32553;&#25918;&#26041;&#27861;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#30340;MNF&#27169;&#22411;&#34920;&#29616;&#26368;&#20248;&#65292;&#20351;&#24471;TCN&#21644;WaveNet&#32593;&#32476;&#21487;&#20197;&#33391;&#22909;&#25512;&#26029;VIX&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.17042</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;VIX&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Forecasting VIX using Bayesian Deep Learning. (arXiv:2401.17042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#27874;&#21160;&#24615;&#25351;&#25968;VIX&#65292;&#24182;&#37319;&#29992;&#27010;&#29575;&#23545;&#24212;&#27169;&#22411;&#21644;&#26631;&#20934;&#24046;&#32553;&#25918;&#26041;&#27861;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#30340;MNF&#27169;&#22411;&#34920;&#29616;&#26368;&#20248;&#65292;&#20351;&#24471;TCN&#21644;WaveNet&#32593;&#32476;&#21487;&#20197;&#33391;&#22909;&#25512;&#26029;VIX&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36880;&#28176;&#20195;&#26367;&#20256;&#32479;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#20026;&#20215;&#26684;&#39044;&#27979;&#20219;&#21153;&#30340;&#39318;&#36873;&#12290;&#26412;&#25991;&#21033;&#29992;&#27010;&#29575;&#28145;&#24230;&#23398;&#20064;&#25512;&#26029;&#27874;&#21160;&#24615;&#25351;&#25968;VIX&#12290;&#25105;&#20204;&#20351;&#29992;WaveNet&#12289;Temporal Convolutional Network (TCN)&#21644;Transformers&#30340;&#27010;&#29575;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;TCN&#22312;RMSE&#32422;&#20026;0.189&#26102;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#30340;&#19981;&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#38382;&#39064;&#24050;&#34987;&#24191;&#27867;&#30693;&#26195;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#24046;&#32553;&#25918;&#26469;&#26657;&#20934;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#30340;MNF&#27169;&#22411;&#22312;&#31934;&#24230;&#21644;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#26041;&#38754;&#20248;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#21644;Flipout&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22768;&#31216;&#20855;&#26377;&#26607;&#35199;&#21644;&#23545;&#25968;&#22343;&#21248;&#20808;&#39564;&#20998;&#24067;&#30340;MNF&#21487;&#20197;&#26368;&#22909;&#22320;&#25512;&#26029;VIX&#20540;&#65292;&#24182;&#19988;&#33021;&#22815;&#20351;TCN&#21644;WaveNet&#32593;&#32476;&#36827;&#34892;&#33391;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning techniques are gradually replacing traditional statistical and machine learning models as the first choice for price forecasting tasks. In this paper, we leverage probabilistic deep learning for inferring the volatility index VIX. We employ the probabilistic counterpart of WaveNet, Temporal Convolutional Network (TCN), and Transformers. We show that TCN outperforms all models with an RMSE around 0.189. In addition, it has been well known that modern neural networks provide inaccurate uncertainty estimates. For solving this problem, we use the standard deviation scaling to calibrate the networks. Furthermore, we found out that MNF with Gaussian prior outperforms Reparameterization Trick and Flipout models in terms of precision and uncertainty predictions. Finally, we claim that MNF with Cauchy and LogUniform prior distributions yield well calibrated TCN and WaveNet networks being the former that best infer the VIX values.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26080;&#22122;&#22768;&#35266;&#27979;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25955;&#20081;&#25968;&#25454;&#36924;&#36817;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#38543;&#26426;&#25506;&#32034;&#27493;&#39588;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#22635;&#20805;&#36317;&#31163;&#30340;&#36895;&#29575;&#34928;&#20943;&#12290;&#35813;&#31639;&#27861;&#22312;&#23454;&#29616;&#30340;&#26131;&#29992;&#24615;&#21644;&#32047;&#31215;&#36951;&#25022;&#36793;&#30028;&#30340;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;GP-UCB&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#31034;&#20363;&#20013;&#20248;&#20110;&#20854;&#20182;&#36125;&#21494;&#26031;&#20248;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.17037</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#22122;&#22768;&#35266;&#27979;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65306;&#36890;&#36807;&#38543;&#26426;&#25506;&#32034;&#25913;&#21892;&#36951;&#25022;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Noise-Free Observations: Improved Regret Bounds via Random Exploration. (arXiv:2401.17037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17037
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26080;&#22122;&#22768;&#35266;&#27979;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25955;&#20081;&#25968;&#25454;&#36924;&#36817;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#38543;&#26426;&#25506;&#32034;&#27493;&#39588;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#22635;&#20805;&#36317;&#31163;&#30340;&#36895;&#29575;&#34928;&#20943;&#12290;&#35813;&#31639;&#27861;&#22312;&#23454;&#29616;&#30340;&#26131;&#29992;&#24615;&#21644;&#32047;&#31215;&#36951;&#25022;&#36793;&#30028;&#30340;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;GP-UCB&#31639;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#31034;&#20363;&#20013;&#20248;&#20110;&#20854;&#20182;&#36125;&#21494;&#26031;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26080;&#22122;&#22768;&#35266;&#27979;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20110;&#25955;&#20081;&#25968;&#25454;&#36924;&#36817;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#25506;&#32034;&#27493;&#39588;&#30830;&#20445;&#26597;&#35810;&#28857;&#30340;&#22635;&#20805;&#36317;&#31163;&#20197;&#25509;&#36817;&#26368;&#20248;&#30340;&#36895;&#29575;&#34928;&#20943;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#30041;&#20102;&#32463;&#20856;&#30340;GP-UCB&#31639;&#27861;&#30340;&#26131;&#23454;&#29616;&#24615;&#65292;&#24182;&#28385;&#36275;&#20102;&#20960;&#20046;&#19982;arXiv:2002.05096&#20013;&#30340;&#29468;&#24819;&#30456;&#21305;&#37197;&#30340;&#32047;&#31215;&#36951;&#25022;&#36793;&#30028;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;COLT&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26032;&#31639;&#27861;&#22312;&#20960;&#20010;&#31034;&#20363;&#20013;&#20248;&#20110;GP-UCB&#21644;&#20854;&#20182;&#27969;&#34892;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Bayesian optimization with noise-free observations. We introduce new algorithms rooted in scattered data approximation that rely on a random exploration step to ensure that the fill-distance of query points decays at a near-optimal rate. Our algorithms retain the ease of implementation of the classical GP-UCB algorithm and satisfy cumulative regret bounds that nearly match those conjectured in arXiv:2002.05096, hence solving a COLT open problem. Furthermore, the new algorithms outperform GP-UCB and other popular Bayesian optimization strategies in several examples.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#20108;&#20803;&#20998;&#31867;&#24615;&#33021;&#30340;&#20869;&#22312;&#25968;&#25454;&#38480;&#21046;&#21644;&#19978;&#30028;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#25512;&#29702;&#21644;&#23454;&#35777;&#26816;&#39564;&#65292;&#21457;&#29616;&#29702;&#35770;&#19978;&#38480;&#26159;&#21487;&#20197;&#34987;&#36798;&#21040;&#30340;&#65292;&#24182;&#35745;&#31639;&#20986;&#20102;&#19977;&#20010;&#24120;&#29992;&#35780;&#20272;&#25351;&#26631;&#30340;&#31934;&#30830;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.17036</link><description>&lt;p&gt;
&#20108;&#20803;&#20998;&#31867;&#24615;&#33021;&#20013;&#30340;&#20869;&#22312;&#25968;&#25454;&#38480;&#21046;&#21644;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Data Constraints and Upper Bounds in Binary Classification Performance. (arXiv:2401.17036v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17036
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20108;&#20803;&#20998;&#31867;&#24615;&#33021;&#30340;&#20869;&#22312;&#25968;&#25454;&#38480;&#21046;&#21644;&#19978;&#30028;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#24182;&#36827;&#34892;&#20102;&#29702;&#35770;&#25512;&#29702;&#21644;&#23454;&#35777;&#26816;&#39564;&#65292;&#21457;&#29616;&#29702;&#35770;&#19978;&#38480;&#26159;&#21487;&#20197;&#34987;&#36798;&#21040;&#30340;&#65292;&#24182;&#35745;&#31639;&#20986;&#20102;&#19977;&#20010;&#24120;&#29992;&#35780;&#20272;&#25351;&#26631;&#30340;&#31934;&#30830;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#32452;&#32455;&#30340;&#32467;&#26500;&#34987;&#24191;&#27867;&#35748;&#20026;&#23545;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35748;&#20026;&#32473;&#23450;&#25968;&#25454;&#38598;&#19978;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#26368;&#22823;&#28508;&#21147;&#20027;&#35201;&#21463;&#21040;&#25968;&#25454;&#30340;&#20869;&#22312;&#29305;&#24615;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#29702;&#35770;&#25512;&#29702;&#21644;&#23454;&#35777;&#26816;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#30446;&#26631;&#20989;&#25968;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#24471;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#32467;&#35770;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#20108;&#20803;&#20998;&#31867;&#24615;&#33021;&#30340;&#29702;&#35770;&#19978;&#38480;&#26159;&#21487;&#20197;&#34987;&#29702;&#35770;&#19978;&#36798;&#21040;&#30340;&#12290;&#36825;&#20010;&#19978;&#38480;&#20195;&#34920;&#20102;&#23398;&#20064;&#25439;&#22833;&#21644;&#35780;&#20272;&#25351;&#26631;&#20043;&#38388;&#30340;&#21487;&#35745;&#31639;&#24179;&#34913;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#19977;&#20010;&#24120;&#29992;&#35780;&#20272;&#25351;&#26631;&#30340;&#31934;&#30830;&#19978;&#38480;&#65292;&#25581;&#31034;&#20102;&#19982;&#25105;&#20204;&#24635;&#20307;&#35770;&#28857;&#30340;&#26681;&#26412;&#19968;&#33268;&#24615;&#65306;&#19978;&#30028;&#19982;&#20869;&#22312;&#25968;&#25454;&#38480;&#21046;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The structure of data organization is widely recognized as having a substantial influence on the efficacy of machine learning algorithms, particularly in binary classification tasks. Our research provides a theoretical framework suggesting that the maximum potential of binary classifiers on a given dataset is primarily constrained by the inherent qualities of the data. Through both theoretical reasoning and empirical examination, we employed standard objective functions, evaluative metrics, and binary classifiers to arrive at two principal conclusions. Firstly, we show that the theoretical upper bound of binary classification performance on actual datasets can be theoretically attained. This upper boundary represents a calculable equilibrium between the learning loss and the metric of evaluation. Secondly, we have computed the precise upper bounds for three commonly used evaluation metrics, uncovering a fundamental uniformity with our overarching thesis: the upper bound is intricately 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#31895;&#30095;&#25439;&#22351;&#25968;&#25454;&#30340;&#22362;&#22266;&#26680;&#31232;&#30095;&#23376;&#31354;&#38388;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.17035</link><description>&lt;p&gt;
&#22362;&#22266;&#30340;&#26680;&#31232;&#30095;&#23376;&#31354;&#38388;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Robust Kernel Sparse Subspace Clustering. (arXiv:2401.17035v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17035
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#31895;&#30095;&#25439;&#22351;&#25968;&#25454;&#30340;&#22362;&#22266;&#26680;&#31232;&#30095;&#23376;&#31354;&#38388;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#27169;&#24335;&#35782;&#21035;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#65292;&#21253;&#25324;&#23376;&#31354;&#38388;&#32858;&#31867;&#65288;SC&#65289;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#38750;&#32447;&#24615;&#38382;&#39064;&#22312;&#26144;&#23556;&#21040;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#21518;&#21464;&#25104;&#20102;&#32447;&#24615;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#26680;&#25216;&#24039;&#30340;&#38544;&#24335;&#26144;&#23556;&#20351;&#24471;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#38750;&#32447;&#24615;&#31639;&#27861;&#24471;&#20197;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#22312;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#20013;&#20351;&#29992;&#35823;&#24046;&#39033;&#30340;Froebenious&#33539;&#25968;&#30340;&#24179;&#26041;&#26102;&#65292;&#32447;&#24615;&#31639;&#27861;&#30340;&#26680;&#21270;&#25165;&#26159;&#21487;&#33021;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#24847;&#21619;&#30528;&#35823;&#24046;&#39033;&#30340;&#27491;&#24577;&#20998;&#24067;&#12290;&#36825;&#23545;&#20110;&#22914;&#31895;&#30095;&#25439;&#22351;&#36825;&#26679;&#30340;&#38750;&#39640;&#26031;&#35823;&#24046;&#26159;&#19981;&#21512;&#36866;&#30340;&#65292;&#32780;&#36825;&#20123;&#35823;&#24046;&#26159;&#30001;l1&#33539;&#25968;&#24314;&#27169;&#30340;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#20855;&#26377;&#31895;&#30095;&#25439;&#22351;&#30340;&#25968;&#25454;&#30340;&#22362;&#22266;&#26680;&#31232;&#30095;&#32858;&#31867;&#65288;RKSSC&#65289;&#31639;&#27861;&#12290;&#35813;&#27010;&#24565;&#21407;&#21017;&#19978;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;SC&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#27492;&#31867;&#25439;&#22351;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30693;&#21517;&#30340;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel methods are applied to many problems in pattern recognition, including subspace clustering (SC). That way, nonlinear problems in the input data space become linear in mapped high-dimensional feature space. Thereby, computationally tractable nonlinear algorithms are enabled through implicit mapping by the virtue of kernel trick. However, kernelization of linear algorithms is possible only if square of the Froebenious norm of the error term is used in related optimization problem. That, however, implies normal distribution of the error. That is not appropriate for non-Gaussian errors such as gross sparse corruptions that are modeled by -norm. Herein, to the best of our knowledge, we propose for the first time robust kernel sparse SC (RKSSC) algorithm for data with gross sparse corruptions. The concept, in principle, can be applied to other SC algorithms to achieve robustness to the presence of such type of corruption. We validated proposed approach on two well-known datasets with 
&lt;/p&gt;</description></item><item><title>M2CURL&#26159;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20174;&#35270;&#35302;&#35273;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#39640;&#25928;&#30340;&#34920;&#31034;&#65292;&#24182;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2401.17032</link><description>&lt;p&gt;
M2CURL: &#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation. (arXiv:2401.17032v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17032
&lt;/p&gt;
&lt;p&gt;
M2CURL&#26159;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20174;&#35270;&#35302;&#35273;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#39640;&#25928;&#30340;&#34920;&#31034;&#65292;&#24182;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#38754;&#20043;&#19968;&#26159;&#26377;&#25928;&#22320;&#25972;&#21512;&#19981;&#21516;&#30340;&#35266;&#27979;&#27169;&#24577;&#12290;&#20174;&#36825;&#20123;&#27169;&#24577;&#20013;&#24471;&#21040;&#31283;&#20581;&#20934;&#30830;&#30340;&#34920;&#31034;&#23545;&#20110;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35302;&#35273;&#25968;&#25454;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#34920;&#31034;&#38754;&#20020;&#30528;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#21644;&#23558;&#35270;&#35302;&#35273;&#36755;&#20837;&#19982;&#21160;&#24577;&#29615;&#22659;&#21644;&#20219;&#21153;&#30446;&#26631;&#36827;&#34892;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#23545;&#27604;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;M2CURL&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#23398;&#20064;&#20986;&#39640;&#25928;&#30340;&#34920;&#31034;&#24182;&#21152;&#36895;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#21487;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#25972;&#21512;&#12290;&#25105;&#20204;&#22312;Tactile Gym 2&#27169;&#25311;&#22120;&#19978;&#35780;&#20272;&#20102;M2CURL&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities. Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms. However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives. To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms. Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm. We evaluate M2CURL on the Tactile Gym 2 simulator 
&lt;/p&gt;</description></item><item><title>LADDER&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#23431;&#23449;&#30340;&#8220;&#36317;&#31163;&#26799;&#24230;&#8221;&#65292;&#23454;&#29616;&#20102;&#39044;&#27979;&#23431;&#23449;&#36317;&#31163;&#24182;&#25506;&#32034;&#20102;&#22810;&#20010;&#23431;&#23449;&#23398;&#24212;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#38656;&#35201;&#36827;&#34892;&#26377;&#36259;&#20294;&#35880;&#24910;&#30340;&#32771;&#34385;&#12290;</title><link>http://arxiv.org/abs/2401.17029</link><description>&lt;p&gt;
LADDER: &#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37325;&#26032;&#25506;&#32034;&#23431;&#23449;&#36317;&#31163;&#26799;&#24230;&#24182;&#25506;&#32034;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LADDER: Revisiting the Cosmic Distance Ladder with Deep Learning Approaches and Exploring its Applications. (arXiv:2401.17029v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17029
&lt;/p&gt;
&lt;p&gt;
LADDER&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#23431;&#23449;&#30340;&#8220;&#36317;&#31163;&#26799;&#24230;&#8221;&#65292;&#23454;&#29616;&#20102;&#39044;&#27979;&#23431;&#23449;&#36317;&#31163;&#24182;&#25506;&#32034;&#20102;&#22810;&#20010;&#23431;&#23449;&#23398;&#24212;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#38656;&#35201;&#36827;&#34892;&#26377;&#36259;&#20294;&#35880;&#24910;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;LADDER&#65288;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#29992;&#20110;&#36317;&#31163;&#20272;&#35745;&#21644;&#37325;&#24314;&#65289;&#30340;&#26032;&#39062;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;&#8220;&#23431;&#23449;&#36317;&#31163;&#26799;&#24230;&#8221;&#37325;&#24314;&#23431;&#23449;&#30340;&#21069;&#26223;&#12290;LADDER&#20351;&#29992;&#20102;&#26469;&#33258;Pantheon Type Ia&#36229;&#26032;&#26143;&#32534;&#35793;&#30340;&#35270;&#26143;&#31561;&#25968;&#25454;&#65292;&#24182;&#23558;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20840;&#37096;&#21327;&#26041;&#24046;&#20449;&#24687;&#36827;&#34892;&#20102;&#34701;&#21512;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#30456;&#24212;&#35823;&#24046;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#22810;&#20010;&#39564;&#35777;&#23454;&#39564;&#21518;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;LADDER&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23431;&#23449;&#23398;&#19978;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20316;&#20026;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#24037;&#20855;&#65292;&#29992;&#20110;&#20854;&#20182;&#25968;&#25454;&#38598;&#65288;&#22914;&#37325;&#23376;&#22768;&#23398;&#25391;&#33633;&#65289;&#30340;&#26657;&#20934;&#65292;&#29992;&#20110;&#39640;&#32418;&#31227;&#25968;&#25454;&#38598;&#65288;&#22914;&#20285;&#29595;&#23556;&#32447;&#26292;&#65289;&#30340;&#26657;&#20934;&#65292;&#20197;&#21450;&#29992;&#20316;&#26410;&#26469;&#25506;&#27979;&#30340;&#29420;&#31435;&#20110;&#27169;&#22411;&#30340;&#27169;&#25311;&#30446;&#24405;&#29983;&#25104;&#22120;&#31561;&#31561;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32473;&#20986;&#20102;&#20851;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#35201;&#36827;&#34892;&#26377;&#36259;&#32780;&#35880;&#24910;&#30340;&#32771;&#34385;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the prospect of reconstructing the ``cosmic distance ladder'' of the Universe using a novel deep learning framework called LADDER - Learning Algorithm for Deep Distance Estimation and Reconstruction. LADDER is trained on the apparent magnitude data from the Pantheon Type Ia supernovae compilation, incorporating the full covariance information among data points, to produce predictions along with corresponding errors. After employing several validation tests with a number of deep learning models, we pick LADDER as the best performing one. We then demonstrate applications of our method in the cosmological context, that include serving as a model-independent tool for consistency checks for other datasets like baryon acoustic oscillations, calibration of high-redshift datasets such as gamma ray bursts, use as a model-independent mock catalog generator for future probes, etc. Our analysis advocates for interesting yet cautious consideration of machine learning applications in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SubgroupTE&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#20122;&#32676;&#35782;&#21035;&#21644;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65292;&#36890;&#36807;&#32771;&#34385;&#27835;&#30103;&#21453;&#24212;&#30340;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#24314;&#35758;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.17027</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;&#20122;&#32676;&#35782;&#21035;&#30340;&#24322;&#36136;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65288;opioid use disorder&#20013;&#30340;&#20010;&#20154;&#21307;&#23398;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous treatment effect estimation with subpopulation identification for personalized medicine in opioid use disorder. (arXiv:2401.17027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SubgroupTE&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#20122;&#32676;&#35782;&#21035;&#21644;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65292;&#36890;&#36807;&#32771;&#34385;&#27835;&#30103;&#21453;&#24212;&#30340;&#24322;&#36136;&#24615;&#65292;&#25552;&#39640;&#20102;&#20010;&#24615;&#21270;&#27835;&#30103;&#24314;&#35758;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#24573;&#35270;&#20102;&#19981;&#21516;&#29305;&#24449;&#20122;&#32676;&#20043;&#38388;&#30340;&#27835;&#30103;&#32467;&#26524;&#21464;&#21270;&#12290;&#36825;&#20010;&#38480;&#21046;&#38459;&#30861;&#20102;&#23427;&#20204;&#25552;&#20379;&#20934;&#30830;&#20272;&#35745;&#21644;&#29305;&#23450;&#20122;&#32676;&#30340;&#27835;&#30103;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;SubgroupTE&#65292;&#23427;&#32467;&#21512;&#20102;&#20122;&#32676;&#35782;&#21035;&#21644;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;SubgroupTE&#23545;&#21508;&#20010;&#20122;&#32676;&#36827;&#34892;&#35782;&#21035;&#24182;&#21516;&#26102;&#20272;&#35745;&#27599;&#20010;&#20122;&#32676;&#30340;&#27835;&#30103;&#25928;&#26524;&#65292;&#36890;&#36807;&#32771;&#34385;&#27835;&#30103;&#21453;&#24212;&#30340;&#24322;&#36136;&#24615;&#25913;&#36827;&#20102;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#27604;&#36739;&#23454;&#39564;&#34920;&#26126;&#65292;SubgroupTE&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23545;&#19982;&#38463;&#29255;&#31867;&#33647;&#29289;&#20351;&#29992;&#38556;&#30861;&#65288;OUD&#65289;&#30456;&#20851;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#25552;&#39640;&#20010;&#24615;&#21270;&#27835;&#30103;&#24314;&#35758;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have demonstrated promising results in estimating treatment effects (TEE). However, most of them overlook the variations in treatment outcomes among subgroups with distinct characteristics. This limitation hinders their ability to provide accurate estimations and treatment recommendations for specific subgroups. In this study, we introduce a novel neural network-based framework, named SubgroupTE, which incorporates subgroup identification and treatment effect estimation. SubgroupTE identifies diverse subgroups and simultaneously estimates treatment effects for each subgroup, improving the treatment effect estimation by considering the heterogeneity of treatment responses. Comparative experiments on synthetic data show that SubgroupTE outperforms existing models in treatment effect estimation. Furthermore, experiments on a real-world dataset related to opioid use disorder (OUD) demonstrate the potential of our approach to enhance personalized treatment recommendatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#30340;&#36234;&#30028;&#26816;&#27979;&#24615;&#33021;&#65292;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;Mahalanobis&#36317;&#31163;&#26469;&#25298;&#32477;&#36755;&#20986;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#20998;&#31867;&#39118;&#38505;&#65292;&#21363;&#20351;&#24212;&#29992;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;</title><link>http://arxiv.org/abs/2401.17013</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#23545;&#20110;&#27169;&#22411;&#30340;&#36234;&#30028;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Out-of-Distribution Detection Performance on Autonomous Driving Datasets. (arXiv:2401.17013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#30340;&#36234;&#30028;&#26816;&#27979;&#24615;&#33021;&#65292;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;Mahalanobis&#36317;&#31163;&#26469;&#25298;&#32477;&#36755;&#20986;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#20998;&#31867;&#39118;&#38505;&#65292;&#21363;&#20351;&#24212;&#29992;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#35201;&#31995;&#32479;&#22320;&#30740;&#31350;&#23433;&#20840;&#25514;&#26045;&#65292;&#20197;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#39044;&#26399;&#24615;&#33021;&#31243;&#24230;&#12290;&#30001;&#20110;&#32570;&#20047;&#29992;&#20110;&#39640;&#32500;DNN&#30340;&#39564;&#35777;&#26041;&#27861;&#65292;&#38656;&#35201;&#22312;&#25509;&#21463;&#30340;&#24615;&#33021;&#21644;&#22788;&#29702;&#36234;&#30028;&#26679;&#26412;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#26368;&#21487;&#33021;&#30340;&#31867;&#26465;&#20214;&#39640;&#26031;&#20998;&#24067;&#30340;Mahalanobis&#36317;&#31163;&#65288;MD&#65289;&#20316;&#20026;&#36234;&#30028;&#35780;&#20998;&#65292;&#35780;&#20272;&#25298;&#32477;&#35821;&#20041;&#20998;&#21106;DNN&#30340;&#36755;&#20986;&#12290;&#35780;&#20272;&#36807;&#31243;&#28041;&#21450;&#19977;&#20010;&#22312;Cityscapes&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24182;&#22312;&#22235;&#20010;&#27773;&#36710;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#30340;DNN&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#24212;&#29992;&#22312;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#36890;&#36807;&#20943;&#23569;&#20687;&#32032;&#35206;&#30422;&#29575;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;&#20998;&#31867;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#30340;&#36866;&#29992;&#24615;&#23558;&#25903;&#25345;&#21512;&#27861;&#21270;&#23433;&#20840;&#25514;&#26045;&#24182;&#25512;&#21160;&#20854;&#22312;&#27773;&#36710;&#24863;&#30693;&#20013;&#30340;&#23433;&#20840;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety measures need to be systemically investigated to what extent they evaluate the intended performance of Deep Neural Networks (DNNs) for critical applications. Due to a lack of verification methods for high-dimensional DNNs, a trade-off is needed between accepted performance and handling of out-of-distribution (OOD) samples.  This work evaluates rejecting outputs from semantic segmentation DNNs by applying a Mahalanobis distance (MD) based on the most probable class-conditional Gaussian distribution for the predicted class as an OOD score. The evaluation follows three DNNs trained on the Cityscapes dataset and tested on four automotive datasets and finds that classification risk can drastically be reduced at the cost of pixel coverage, even when applied on unseen datasets. The applicability of our findings will support legitimizing safety measures and motivate their usage when arguing for safe usage of DNNs in automotive perception.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.17010</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;StarCoder&#30340;&#25913;&#36827;&#29256;&#26412;WizardCoder&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#24494;&#35843;&#23558;&#20854;&#36866;&#24212;&#20110;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;WizardCoder&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25506;&#31350;&#20102;&#26368;&#20339;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#38024;&#23545;&#36127;&#26679;&#26412;&#36828;&#22810;&#20110;&#27491;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;WizardCoder&#27169;&#22411;&#22312;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#30340;&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#22312;ROC AUC&#21644;F1&#24230;&#37327;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20854;&#35757;&#32451;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#23545;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#25588;&#21161;&#20998;&#37197;&#30340;&#24322;&#36136;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#20197;&#25903;&#25345;&#26377;&#25928;&#30340;&#25588;&#21161;&#20998;&#37197;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.16986</link><description>&lt;p&gt;
&#29992;&#20110;&#25104;&#26412;&#25928;&#30410;&#20248;&#21270;&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#21457;&#23637;&#25588;&#21161;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Causal Machine Learning for Cost-Effective Allocation of Development Aid. (arXiv:2401.16986v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#25588;&#21161;&#20998;&#37197;&#30340;&#24322;&#36136;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#20197;&#25903;&#25345;&#26377;&#25928;&#30340;&#25588;&#21161;&#20998;&#37197;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#22269;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#25552;&#20379;&#20102;&#8220;&#26080;&#20154;&#34987;&#36951;&#24323;&#8221;&#30340;&#26356;&#32654;&#22909;&#26410;&#26469;&#34013;&#22270;&#65292;&#20026;&#20102;&#22312;2030&#24180;&#20043;&#21069;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#36139;&#31351;&#22269;&#23478;&#38656;&#35201;&#22823;&#37327;&#30340;&#21457;&#23637;&#25588;&#21161;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#25588;&#21161;&#20998;&#37197;&#30340;&#24322;&#36136;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#20197;&#25903;&#25345;&#26377;&#25928;&#30340;&#25588;&#21161;&#20998;&#37197;&#20915;&#31574;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#19968;&#20010;&#24179;&#34913;&#33258;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#23558;&#39640;&#32500;&#22269;&#23478;&#29305;&#24449;&#23884;&#20837;&#65292;&#21516;&#26102;&#35299;&#20915;&#27835;&#30103;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65307;&#65288;ii&#65289;&#19968;&#20010;&#21453;&#20107;&#23454;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#35745;&#31639;&#22312;&#19981;&#21516;&#25588;&#21161;&#35268;&#27169;&#19979;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#65292;&#20197;&#35299;&#20915;&#23567;&#26679;&#26412;&#38382;&#39064;&#65307;&#65288;iii&#65289;&#19968;&#20010;&#25512;&#26029;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24322;&#36136;&#21270;&#30340;&#27835;&#30103;&#25928;&#26524;&#26354;&#32447;&#12290;&#25105;&#20204;&#20351;&#29992;105&#20010;&#22269;&#23478;&#25112;&#30053;&#24615;&#21457;&#23637;&#25588;&#21161;&#25968;&#25454;&#65288;&#24635;&#39069;&#36229;&#36807;52&#20159;&#32654;&#20803;&#65289;&#65292;&#20197;&#32467;&#26463;HIV/AIDS&#20026;&#30446;&#26631;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sustainable Development Goals (SDGs) of the United Nations provide a blueprint of a better future by 'leaving no one behind', and, to achieve the SDGs by 2030, poor countries require immense volumes of development aid. In this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. Specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. We demonstrate the effectiveness of our framework using data with official development aid earmarked to end HIV/AIDS in 105 countries, amounting to more than USD 5.2 billion. F
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21516;&#26102;&#25551;&#36848;&#22810;&#31181;&#25910;&#30410;&#29575;&#26354;&#32447;&#21160;&#24577;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#38750;&#21442;&#25968;&#20998;&#20301;&#25968;&#22238;&#24402;&#65292;&#29983;&#25104;&#26410;&#26469;&#25910;&#30410;&#29575;&#30340;&#28857;&#39044;&#27979;&#21644;&#21306;&#38388;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#28145;&#24230;&#38598;&#25104;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#25193;&#23637;&#21644;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.16985</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#22810;&#31181;&#25910;&#30410;&#29575;&#26354;&#32447;&#24314;&#27169;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multiple Yield Curve Modeling and Forecasting using Deep Learning. (arXiv:2401.16985v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21516;&#26102;&#25551;&#36848;&#22810;&#31181;&#25910;&#30410;&#29575;&#26354;&#32447;&#21160;&#24577;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#38750;&#21442;&#25968;&#20998;&#20301;&#25968;&#22238;&#24402;&#65292;&#29983;&#25104;&#26410;&#26469;&#25910;&#30410;&#29575;&#30340;&#28857;&#39044;&#27979;&#21644;&#21306;&#38388;&#39044;&#27979;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#28145;&#24230;&#38598;&#25104;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#25193;&#23637;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21516;&#26102;&#25551;&#36848;&#22810;&#31181;&#25910;&#30410;&#29575;&#26354;&#32447;&#21160;&#24577;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26088;&#22312;&#23398;&#20064;&#37329;&#34701;&#24066;&#22330;&#20840;&#29699;&#21270;&#24341;&#36215;&#30340;&#19981;&#21516;&#25910;&#30410;&#29575;&#26354;&#32447;&#20043;&#38388;&#30340;&#20381;&#36182;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#32467;&#21512;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#38750;&#21442;&#25968;&#20998;&#20301;&#25968;&#22238;&#24402;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26410;&#26469;&#25910;&#30410;&#29575;&#30340;&#28857;&#39044;&#27979;&#21644;&#21306;&#38388;&#39044;&#27979;&#12290;&#35813;&#26694;&#26550;&#30340;&#35774;&#35745;&#26088;&#22312;&#36991;&#20813;&#24433;&#21709;&#22810;&#20010;&#20998;&#20301;&#25968;&#22238;&#24402;&#27169;&#22411;&#30340;&#20998;&#20301;&#25968;&#20132;&#21449;&#38382;&#39064;&#12290;&#23545;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#21644;&#36801;&#31227;&#23398;&#20064;&#26426;&#21046;&#65292;&#25506;&#35752;&#20102;&#28508;&#22312;&#30340;&#25193;&#23637;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This manuscript introduces deep learning models that simultaneously describe the dynamics of several yield curves. We aim to learn the dependence structure among the different yield curves induced by the globalization of financial markets and exploit it to produce more accurate forecasts. By combining the self-attention mechanism and nonparametric quantile regression, our model generates both point and interval forecasts of future yields. The architecture is designed to avoid quantile crossing issues affecting multiple quantile regression models. Numerical experiments conducted on two different datasets confirm the effectiveness of our approach. Finally, we explore potential extensions and enhancements by incorporating deep ensemble methods and transfer learning mechanisms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;TAIGA-IACT&#30340;Monte Carlo&#65288;MC&#65289;&#22270;&#20687;&#19978;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20132;&#38169;&#35266;&#27979;&#27169;&#24335;&#21644;&#36866;&#24212;NN&#20998;&#26512;&#30340;&#22270;&#20687;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2401.16981</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20174;IACT&#22270;&#20687;&#20013;&#36873;&#25321;&#20285;&#29595;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
Selection of gamma events from IACT images with deep learning methods. (arXiv:2401.16981v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;TAIGA-IACT&#30340;Monte Carlo&#65288;MC&#65289;&#22270;&#20687;&#19978;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20132;&#38169;&#35266;&#27979;&#27169;&#24335;&#21644;&#36866;&#24212;NN&#20998;&#26512;&#30340;&#22270;&#20687;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20285;&#29595;&#23556;&#32447;&#35266;&#27979;&#31449;TAIGA&#30340;&#25104;&#20687;&#22823;&#27668;&#20999;&#20262;&#31185;&#22827;&#26395;&#36828;&#38236;&#65288;IACTs&#65289;&#25506;&#27979;&#28304;&#33258;&#23431;&#23449;&#23556;&#32447;&#25110;&#20285;&#29595;&#23556;&#32447;&#19982;&#22823;&#27668;&#30456;&#20114;&#20316;&#29992;&#30340;&#24191;&#22823;&#31354;&#27668;&#38453;&#23556;&#65288;EASs&#65289;&#12290;&#22240;&#27492;&#65292;&#26395;&#36828;&#38236;&#33719;&#21462;EASs&#30340;&#22270;&#20687;&#12290;&#23558;&#20285;&#29595;&#23556;&#32447;&#22270;&#20687;&#19982;&#23431;&#23449;&#23556;&#32447;&#32972;&#26223;&#36827;&#34892;&#20998;&#31163;&#26159;&#35813;&#31867;&#22411;&#25506;&#27979;&#22120;&#30340;&#20027;&#35201;&#29305;&#28857;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#30340;IACT&#35266;&#27979;&#20013;&#65292;&#38656;&#35201;&#21516;&#26102;&#35266;&#27979;&#32972;&#26223;&#21644;&#20285;&#29595;&#23556;&#32447;&#28304;&#12290;&#36825;&#31181;&#35266;&#27979;&#27169;&#24335;&#65288;&#31216;&#20026;&#20132;&#38169;&#35266;&#27979;&#65289;&#20250;&#25913;&#21464;&#20107;&#20214;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#36873;&#25321;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;TAIGA-IACT&#30340;Monte Carlo&#65288;MC&#65289;&#22270;&#20687;&#19978;&#24212;&#29992;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#21516;&#26102;&#32771;&#34385;&#20132;&#38169;&#35266;&#27979;&#27169;&#24335;&#21644;&#36866;&#24212;NN&#20998;&#26512;&#30340;&#22270;&#20687;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20960;&#31181;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#22270;&#20687;&#25110;&#36827;&#36807;&#39044;&#22788;&#29702;&#30340;&#22270;&#20687;&#20998;&#31867;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imaging Atmospheric Cherenkov Telescopes (IACTs) of gamma ray observatory TAIGA detect the Extesnive Air Showers (EASs) originating from the cosmic or gamma rays interactions with the atmosphere. Thereby, telescopes obtain images of the EASs. The ability to segregate gamma rays images from the hadronic cosmic ray background is one of the main features of this type of detectors. However, in actual IACT observations simultaneous observation of the background and the source of gamma ray is needed. This observation mode (called wobbling) modifies images of events, which affects the quality of selection by neural networks.  Thus, in this work, the results of the application of neural networks (NN) for image classification task on Monte Carlo (MC) images of TAIGA-IACTs are presented. The wobbling mode is considered together with the image adaptation for adequate analysis by NNs. Simultaneously, we explore several neural network structures that classify events both directly from images or thr
&lt;/p&gt;</description></item><item><title>CORE&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22240;&#26524;&#25512;&#26029;&#21644;&#24178;&#39044;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25581;&#31034;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#22312;&#32467;&#26500;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.16974</link><description>&lt;p&gt;
CORE: &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#25193;&#23637;&#39640;&#25928;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement Learning. (arXiv:2401.16974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16974
&lt;/p&gt;
&lt;p&gt;
CORE&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22240;&#26524;&#25512;&#26029;&#21644;&#24178;&#39044;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25581;&#31034;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#22312;&#32467;&#26500;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#21463;&#21040;Pearl&#30340;&#22240;&#26524;&#23618;&#27425;&#32467;&#26500;&#65288;Causal Hierarchy&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#35770;&#25991;&#21628;&#21505;&#23558;&#24178;&#39044;&#24341;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26041;&#20415;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#36825;&#31181;&#20027;&#21160;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CORE&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22240;&#26524;&#25512;&#26029;&#21644;&#24178;&#39044;&#35268;&#21010;&#26041;&#27861;&#12290;CORE&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#39034;&#24207;&#37325;&#24314;&#22240;&#26524;&#22270;&#65292;&#24182;&#23398;&#20064;&#25191;&#34892;&#20449;&#24687;&#20016;&#23500;&#30340;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CORE&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#22270;&#65292;&#24182;&#39640;&#25928;&#22320;&#25581;&#31034;&#22240;&#26524;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;CORE&#21487;&#20197;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#36798;10&#20010;&#21464;&#37327;&#30340;&#26356;&#22823;&#30340;&#22270;&#65292;&#24182;&#22312;&#32467;&#26500;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery is the challenging task of inferring causal structure from data. Motivated by Pearl's Causal Hierarchy (PCH), which tells us that passive observations alone are not enough to distinguish correlation from causation, there has been a recent push to incorporate interventions into machine learning research. Reinforcement learning provides a convenient framework for such an active approach to learning. This paper presents CORE, a deep reinforcement learning-based approach for causal discovery and intervention planning. CORE learns to sequentially reconstruct causal graphs from data while learning to perform informative interventions. Our results demonstrate that CORE generalizes to unseen graphs and efficiently uncovers causal structures. Furthermore, CORE scales to larger graphs with up to 10 variables and outperforms existing approaches in structure estimation accuracy and sample efficiency. All relevant code and supplementary material can be found at https://github.com/s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#30340;&#26032;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#39038;&#23458;&#21040;&#36798;&#21644;&#26410;&#30693;&#30340;&#28857;&#20987;&#29575;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38543;&#26426;&#19978;&#19979;&#25991;&#25671;&#33218;&#21644;&#20855;&#26377;&#23545;&#25239;&#24615;&#21040;&#36798;&#30340;&#22312;&#32447;&#21305;&#37197;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#22312;&#39038;&#23458;&#21040;&#36798;&#25509;&#36817;&#24179;&#31283;&#26102;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#22312;&#19968;&#33324;&#65288;&#38750;&#24179;&#31283;&#65289;&#39038;&#23458;&#21040;&#36798;&#20998;&#24067;&#19979;&#20139;&#21463;&#26368;&#20248;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#39038;&#23458;&#22330;&#26223;&#19979;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#30410;&#12290;</title><link>http://arxiv.org/abs/2401.16945</link><description>&lt;p&gt;
&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#19982;&#38750;&#24179;&#31283;&#39038;&#23458;
&lt;/p&gt;
&lt;p&gt;
Online Resource Allocation with Non-Stationary Customers. (arXiv:2401.16945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#30340;&#26032;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#39038;&#23458;&#21040;&#36798;&#21644;&#26410;&#30693;&#30340;&#28857;&#20987;&#29575;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#38543;&#26426;&#19978;&#19979;&#25991;&#25671;&#33218;&#21644;&#20855;&#26377;&#23545;&#25239;&#24615;&#21040;&#36798;&#30340;&#22312;&#32447;&#21305;&#37197;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#22312;&#39038;&#23458;&#21040;&#36798;&#25509;&#36817;&#24179;&#31283;&#26102;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#22312;&#19968;&#33324;&#65288;&#38750;&#24179;&#31283;&#65289;&#39038;&#23458;&#21040;&#36798;&#20998;&#24067;&#19979;&#20139;&#21463;&#26368;&#20248;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#39038;&#23458;&#22330;&#26223;&#19979;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#30340;&#26032;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#38750;&#24179;&#31283;&#39038;&#23458;&#21040;&#36798;&#21644;&#26410;&#30693;&#30340;&#28857;&#20987;&#29575;&#12290;&#25105;&#20204;&#20551;&#35774;&#22810;&#31181;&#31867;&#22411;&#30340;&#39038;&#23458;&#20197;&#38750;&#24179;&#31283;&#38543;&#26426;&#26041;&#24335;&#21040;&#36798;&#65292;&#27599;&#20010;&#26102;&#26399;&#37117;&#26377;&#26410;&#30693;&#30340;&#21040;&#36798;&#29575;&#65292;&#24182;&#19988;&#39038;&#23458;&#30340;&#28857;&#20987;&#29575;&#26410;&#30693;&#65292;&#21482;&#33021;&#22312;&#32447;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#38543;&#26426;&#19978;&#19979;&#25991;&#25671;&#33218;&#21644;&#20855;&#26377;&#23545;&#25239;&#24615;&#21040;&#36798;&#30340;&#22312;&#32447;&#21305;&#37197;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22312;&#32447;&#26041;&#26696;&#65292;&#23558;&#36164;&#28304;&#20998;&#37197;&#32473;&#38750;&#24179;&#31283;&#39038;&#23458;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#23454;&#29616;&#20102;&#8220;&#20004;&#20840;&#20854;&#32654;&#8221;&#30340;&#25928;&#26524;&#65306;&#24403;&#39038;&#23458;&#21040;&#36798;&#25509;&#36817;&#24179;&#31283;&#26102;&#65292;&#26041;&#26696;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#22312;&#19968;&#33324;&#65288;&#38750;&#24179;&#31283;&#65289;&#39038;&#23458;&#21040;&#36798;&#20998;&#24067;&#19979;&#20139;&#21463;&#26368;&#20248;&#30340;&#31454;&#20105;&#27604;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#19981;&#21516;&#39038;&#23458;&#22330;&#26223;&#19979;&#29983;&#25104;&#25509;&#36817;&#26368;&#20248;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel algorithm for online resource allocation with non-stationary customer arrivals and unknown click-through rates. We assume multiple types of customers arrive in a nonstationary stochastic fashion, with unknown arrival rates in each period, and that customers' click-through rates are unknown and can only be learned online. By leveraging results from the stochastic contextual bandit with knapsack and online matching with adversarial arrivals, we develop an online scheme to allocate the resources to nonstationary customers. We prove that under mild conditions, our scheme achieves a ``best-of-both-world'' result: the scheme has a sublinear regret when the customer arrivals are near-stationary, and enjoys an optimal competitive ratio under general (non-stationary) customer arrival distributions. Finally, we conduct extensive numerical experiments to show our approach generates near-optimal revenues for all different customer scenarios.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#28024;&#36719;&#32420;&#32500;&#21644;&#23548;&#31649;&#20998;&#21106;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#21462;&#24471;&#20102;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16937</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#28024;&#36719;&#32420;&#32500;&#21644;&#23548;&#31649;&#36827;&#34892;&#20998;&#21106;&#21644;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Segmentation and Characterization of Macerated Fibers and Vessels Using Deep Learning. (arXiv:2401.16937v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16937
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#28024;&#36719;&#32420;&#32500;&#21644;&#23548;&#31649;&#20998;&#21106;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#21462;&#24471;&#20102;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#26408;&#26448;&#30001;&#32420;&#32500;&#21644;&#23548;&#31649;&#31561;&#19981;&#21516;&#31181;&#31867;&#30340;&#32454;&#32990;&#32452;&#25104;&#65292;&#36825;&#20123;&#32454;&#32990;&#30340;&#24418;&#29366;&#12289;&#22823;&#23567;&#21644;&#25490;&#21015;&#23545;&#20110;&#29702;&#35299;&#26408;&#26448;&#26679;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#24120;&#65292;&#36825;&#28041;&#21450;&#23558;&#26679;&#26412;&#28024;&#27873;&#22312;&#28342;&#28082;&#20013;&#20197;&#20998;&#31163;&#32454;&#32990;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#20998;&#25955;&#22312;&#36733;&#29627;&#29255;&#19978;&#65292;&#29992;&#26174;&#24494;&#38236;&#36827;&#34892;&#24191;&#22495;&#25104;&#20687;&#65292;&#25429;&#25417;&#25968;&#21315;&#20010;&#32454;&#32990;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32454;&#32990;&#22312;&#22270;&#20687;&#20013;&#32463;&#24120;&#32858;&#38598;&#21644;&#37325;&#21472;&#65292;&#20351;&#29992;&#26631;&#20934;&#22270;&#20687;&#22788;&#29702;&#26041;&#27861;&#36827;&#34892;&#20998;&#21106;&#21464;&#24471;&#22256;&#38590;&#19988;&#32791;&#26102;&#12290;&#32467;&#26524;&#65306;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#28145;&#24230;&#23398;&#20064;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#19968;&#38454;YOLOv8&#27169;&#22411;&#26469;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#23545;&#26174;&#24494;&#38236;&#22270;&#20687;&#20013;&#30340;&#32420;&#32500;&#21644;&#23548;&#31649;&#36827;&#34892;&#20998;&#21106;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20998;&#26512;32640 x 25920&#20687;&#32032;&#30340;&#22270;&#20687;&#65292;&#24182;&#23637;&#31034;&#20986;&#26377;&#25928;&#30340;&#32454;&#32990;&#26816;&#27979;&#21644;&#20998;&#21106;&#65292;&#36798;&#21040;78%&#30340;mAP_0.5-0.95&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#23545;&#32463;&#36807;&#22522;&#22240;&#25913;&#36896;&#30340;&#32420;&#32500;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Wood comprises different cell types, such as fibers and vessels, defining its properties. Studying their shape, size, and arrangement in microscopic images is crucial for understanding wood samples. Typically, this involves macerating (soaking) samples in a solution to separate cells, then spreading them on slides for imaging with a microscope that covers a wide area, capturing thousands of cells. However, these cells often cluster and overlap in images, making the segmentation difficult and time-consuming using standard image-processing methods. Results: In this work, we develop an automatic deep learning segmentation approach that utilizes the one-stage YOLOv8 model for fast and accurate fiber and vessel segmentation and characterization in microscopy images. The model can analyze 32640 x 25920 pixels images and demonstrate effective cell detection and segmentation, achieving a mAP_0.5-0.95 of 78 %. To assess the model's robustness, we examined fibers from a genetically modi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31163;&#25955;&#20302;&#32500;&#25968;&#25454;&#20013;&#39044;&#27979;&#36830;&#32493;&#20998;&#36776;&#29575;&#39118;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25913;&#21892;&#25968;&#25454;&#20998;&#36776;&#29575;&#12289;&#38477;&#20302;&#25968;&#25454;&#32500;&#24230;&#12289;&#20197;&#21450;&#25512;&#26029;&#19981;&#21516;&#31354;&#38388;&#35268;&#26684;&#39118;&#25968;&#25454;&#30340;&#19977;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.16936</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#39044;&#27979;&#31163;&#25955;&#20302;&#32500;&#25968;&#25454;&#20013;&#36830;&#32493;&#27668;&#20505;&#27169;&#24335;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Representation Learning for Cross-modal Prediction of Continuous Weather Patterns from Discrete Low-Dimensional Data. (arXiv:2401.16936v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31163;&#25955;&#20302;&#32500;&#25968;&#25454;&#20013;&#39044;&#27979;&#36830;&#32493;&#20998;&#36776;&#29575;&#39118;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#25913;&#21892;&#25968;&#25454;&#20998;&#36776;&#29575;&#12289;&#38477;&#20302;&#25968;&#25454;&#32500;&#24230;&#12289;&#20197;&#21450;&#25512;&#26029;&#19981;&#21516;&#31354;&#38388;&#35268;&#26684;&#39118;&#25968;&#25454;&#30340;&#19977;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#27491;&#22312;&#23547;&#25214;&#19981;&#27745;&#26579;&#29615;&#22659;&#30340;&#28165;&#27905;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#20197;&#20943;&#23569;&#23545;&#20840;&#29699;&#21464;&#26262;&#36215;&#22240;&#20108;&#27687;&#21270;&#30899;&#25490;&#25918;&#30340;&#36129;&#29486;&#12290;&#39118;&#33021;&#19981;&#20165;&#26377;&#28508;&#21147;&#20943;&#23569;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#36824;&#21487;&#20197;&#28385;&#36275;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#28304;&#38656;&#27714;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#39118;&#33021;&#65292;&#35299;&#20915;&#39118;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20197;&#19979;&#19977;&#20010;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#12290;&#39318;&#20808;&#65292;&#22312;&#21508;&#31181;&#27668;&#20505;&#26465;&#20214;&#19979;&#25913;&#21892;&#25968;&#25454;&#20998;&#36776;&#29575;&#65292;&#20197;&#30830;&#20445;&#36275;&#22815;&#30340;&#20449;&#24687;&#29992;&#20110;&#35780;&#20272;&#28508;&#22312;&#33021;&#28304;&#36164;&#28304;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#38477;&#32500;&#25216;&#26415;&#23545;&#20174;&#20256;&#24863;&#22120;/&#27169;&#25311;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#31649;&#29702;&#21644;&#23384;&#20648;&#12290;&#31532;&#19977;&#65292;&#20174;&#19968;&#20010;&#31354;&#38388;&#35268;&#26684;&#25512;&#26029;&#39118;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#33719;&#21462;&#21487;&#33021;&#19981;&#23454;&#29992;&#25110;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20174;&#31163;&#25955;&#20302;&#32500;&#25968;&#25454;&#20013;&#22810;&#27169;&#24577;&#36830;&#32493;&#20998;&#36776;&#29575;&#39118;&#25968;&#25454;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
World is looking for clean and renewable energy sources that do not pollute the environment, in an attempt to reduce greenhouse gas emissions that contribute to global warming. Wind energy has significant potential to not only reduce greenhouse emission, but also meet the ever increasing demand for energy. To enable the effective utilization of wind energy, addressing the following three challenges in wind data analysis is crucial. Firstly, improving data resolution in various climate conditions to ensure an ample supply of information for assessing potential energy resources. Secondly, implementing dimensionality reduction techniques for data collected from sensors/simulations to efficiently manage and store large datasets. Thirdly, extrapolating wind data from one spatial specification to another, particularly in cases where data acquisition may be impractical or costly. We propose a deep learning based approach to achieve multi-modal continuous resolution wind data prediction from d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#22312;&#25345;&#32493;&#22270;&#21644;&#26223;&#35266;&#31354;&#38388;&#19978;&#24341;&#20837;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#19982;&#32858;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#26679;&#24066;&#22330;&#24773;&#26223;&#19979;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#30340;&#32489;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.16920</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#32858;&#31867;&#23454;&#29616;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Sparse Portfolio Selection via Topological Data Analysis based Clustering. (arXiv:2401.16920v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#22312;&#25345;&#32493;&#22270;&#21644;&#26223;&#35266;&#31354;&#38388;&#19978;&#24341;&#20837;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#19982;&#32858;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#26679;&#24066;&#22330;&#24773;&#26223;&#19979;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#30340;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#26500;&#24314;&#30340;&#25968;&#25454;&#39537;&#21160;&#32858;&#31867;&#22411;&#32929;&#31080;&#36873;&#25321;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#36164;&#20135;&#36873;&#25321;&#31574;&#30053;&#21033;&#29992;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#36873;&#25321;&#19968;&#32452;&#25299;&#25169;&#31867;&#20284;&#65288;&#19981;&#21516;&#65289;&#30340;&#36164;&#20135;&#29992;&#20110;&#31232;&#30095;&#25351;&#25968;&#36861;&#36394;&#65288;&#39532;&#31185;&#32500;&#33576;&#65289;&#25237;&#36164;&#32452;&#21512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#25345;&#32493;&#22270;&#21644;&#26223;&#35266;&#31354;&#38388;&#19978;&#32771;&#34385;&#26102;&#38388;&#25104;&#20998;&#30340;&#26032;&#36317;&#31163;&#24230;&#37327;&#65292;&#20316;&#20026;&#32858;&#31867;&#31639;&#27861;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#23545;2009&#24180;&#33267;2020&#24180;&#30340;S\&amp;P&#25351;&#25968;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21253;&#25324;&#23545;COVID-19&#25968;&#25454;&#30340;&#30740;&#31350;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#19982;&#32858;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#31574;&#30053;&#26174;&#33879;&#25552;&#21319;&#20102;&#19981;&#21516;&#24066;&#22330;&#24773;&#26223;&#19979;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#30340;&#32508;&#21512;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper uses topological data analysis (TDA) tools and introduces a data-driven clustering-based stock selection strategy tailored for sparse portfolio construction. Our asset selection strategy exploits the topological features of stock price movements to select a subset of topologically similar (different) assets for a sparse index tracking (Markowitz) portfolio. We introduce new distance measures, which serve as an input to the clustering algorithm, on the space of persistence diagrams and landscapes that consider the time component of a time series. We conduct an empirical analysis on the S\&amp;P index from 2009 to 2020, including a study on the COVID-19 data to validate the robustness of our methodology. Our strategy to integrate TDA with the clustering algorithm significantly enhanced the performance of sparse portfolios across various performance measures in diverse market scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#33021;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21608;&#26399;&#24615;&#25903;&#25745;&#32467;&#26500;&#30340;&#21018;&#24230;&#24352;&#37327;&#12290;&#36890;&#36807;&#32534;&#30721;&#30340;&#31561;&#21464;&#24615;&#21644;&#33021;&#37327;&#23432;&#24658;&#23450;&#24459;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#35757;&#32451;&#38656;&#27714;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.16914</link><description>&lt;p&gt;
&#33410;&#33021;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#26684;&#23376;&#32467;&#26500;&#30340;&#20122;&#27874;&#38271;&#22411;&#26448;&#26009;&#30340;&#24377;&#24615;&#24615;&#36136;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Energy-conserving equivariant GNN for elasticity of lattice architected metamaterials. (arXiv:2401.16914v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#33021;&#30340;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21608;&#26399;&#24615;&#25903;&#25745;&#32467;&#26500;&#30340;&#21018;&#24230;&#24352;&#37327;&#12290;&#36890;&#36807;&#32534;&#30721;&#30340;&#31561;&#21464;&#24615;&#21644;&#33021;&#37327;&#23432;&#24658;&#23450;&#24459;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#35757;&#32451;&#38656;&#27714;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26684;&#23376;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#20122;&#27874;&#38271;&#22411;&#26448;&#26009;&#65292;&#20854;&#24615;&#36136;&#24378;&#28872;&#20381;&#36182;&#20110;&#20854;&#20960;&#20309;&#35774;&#35745;&#12290;&#26684;&#23376;&#21644;&#22270;&#20043;&#38388;&#30340;&#31867;&#27604;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#20010;&#27604;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#26377;&#38480;&#20803;&#24314;&#27169;&#65289;&#26356;&#24555;&#36895;&#30340;&#26367;&#20195;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#38454;GNN&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#21608;&#26399;&#24615;&#25903;&#25745;&#32467;&#26500;&#30340;&#22235;&#38454;&#21018;&#24230;&#24352;&#37327;&#12290;&#35813;&#27169;&#22411;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#65288;i&#65289;SE&#65288;3&#65289;&#31561;&#21464;&#24615;&#21644;&#65288;ii&#65289;&#33021;&#37327;&#23432;&#24658;&#23450;&#24459;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#35823;&#24046;&#24230;&#37327;&#25351;&#26631;&#23558;&#35813;&#27169;&#22411;&#19982;&#38750;&#31561;&#21464;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#32534;&#30721;&#30340;&#31561;&#21464;&#24615;&#21644;&#33021;&#37327;&#23432;&#24658;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#20943;&#23569;&#35757;&#32451;&#38656;&#27714;&#26041;&#38754;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lattices are architected metamaterials whose properties strongly depend on their geometrical design. The analogy between lattices and graphs enables the use of graph neural networks (GNNs) as a faster surrogate model compared to traditional methods such as finite element modelling. In this work we present a higher-order GNN model trained to predict the fourth-order stiffness tensor of periodic strut-based lattices. The key features of the model are (i) SE(3) equivariance, and (ii) consistency with the thermodynamic law of conservation of energy. We compare the model to non-equivariant models based on a number of error metrics and demonstrate the benefits of the encoded equivariance and energy conservation in terms of predictive performance and reduced training requirements.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#32500;&#35745;&#31639;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23646;&#24615;&#32534;&#30721;&#22120;&#20869;&#20351;&#29992;&#31526;&#21495;-&#26679;&#20998;&#24067;&#34920;&#31034;&#30340;&#20108;&#36827;&#21046;&#32534;&#30721;&#34920;&#26469;&#32039;&#20945;&#22320;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#23646;&#24615;&#25552;&#21462;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.16876</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#32500;&#35745;&#31639;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Classification using Hyperdimensional Computing. (arXiv:2401.16876v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#32500;&#35745;&#31639;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23646;&#24615;&#32534;&#30721;&#22120;&#20869;&#20351;&#29992;&#31526;&#21495;-&#26679;&#20998;&#24067;&#34920;&#31034;&#30340;&#20108;&#36827;&#21046;&#32534;&#30721;&#34920;&#26469;&#32039;&#20945;&#22320;&#34920;&#31034;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#23646;&#24615;&#25552;&#21462;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;&#22522;&#20110;&#27169;&#22411;&#33021;&#22815;&#23558;&#36755;&#20837;&#20998;&#31867;&#21040;&#27169;&#22411;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#30340;&#26032;&#31867;&#21035;&#30340;&#33021;&#21147;&#12290;&#22312;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#24615;&#20219;&#21153;&#26102;&#65292;&#25552;&#20379;&#19968;&#20010;&#36741;&#21161;&#25551;&#36848;&#31526;&#65292;&#20197;&#23646;&#24615;&#38598;&#30340;&#24418;&#24335;&#25551;&#36848;ZSL&#20998;&#31867;&#20013;&#28041;&#21450;&#30340;&#26032;&#31867;&#21035;&#65292;&#26159;&#19968;&#31181;&#34987;&#38738;&#30544;&#30340;&#26041;&#27861;&#12290;&#21463;&#36229;&#32500;&#35745;&#31639;&#65288;HDC&#65289;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#23646;&#24615;&#32534;&#30721;&#22120;&#20869;&#20351;&#29992;&#31526;&#21495;-&#26679;&#20998;&#24067;&#34920;&#31034;&#30340;&#20108;&#36827;&#21046;&#32534;&#30721;&#34920;&#26469;&#32039;&#20945;&#22320;&#34920;&#31034;&#19968;&#20010;&#35745;&#31639;&#31616;&#21333;&#30340;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#36229;&#32500;&#35745;&#31639;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#65288;HDC-ZSC&#65289;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#22522;&#20110;HDC&#30340;&#23646;&#24615;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#30456;&#20284;&#24615;&#26680;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HDC-ZSC&#21487;&#20197;&#29992;&#20110;&#39318;&#20808;&#25191;&#34892;&#38646;&#26679;&#26412;&#23646;&#24615;&#25552;&#21462;&#20219;&#21153;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26368;&#23567;&#26550;&#26500;&#25913;&#21464;&#30340;&#24773;&#20917;&#19979;&#65292;&#34987;&#37325;&#26032;&#29992;&#20110;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification based on Zero-shot Learning (ZSL) is the ability of a model to classify inputs into novel classes on which the model has not previously seen any training examples. Providing an auxiliary descriptor in the form of a set of attributes describing the new classes involved in the ZSL-based classification is one of the favored approaches to solving this challenging task. In this work, inspired by Hyperdimensional Computing (HDC), we propose the use of stationary binary codebooks of symbol-like distributed representations inside an attribute encoder to compactly represent a computationally simple end-to-end trainable model, which we name Hyperdimensional Computing Zero-shot Classifier~(HDC-ZSC). It consists of a trainable image encoder, an attribute encoder based on HDC, and a similarity kernel. We show that HDC-ZSC can be used to first perform zero-shot attribute extraction tasks and, can later be repurposed for Zero-shot Classification tasks with minimal architectural changes
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#21644;MCTS&#30456;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#22312;&#22269;&#38469;&#35937;&#26827;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#19979;&#26827;&#27700;&#24179;&#65292;&#39564;&#35777;&#20102;&#38598;&#25104;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24182;&#23637;&#31034;&#20102;&#34701;&#20837;&#19987;&#23478;&#30693;&#35782;&#21644;&#25112;&#30053;&#21407;&#21017;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16852</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#65292;&#23558;&#28151;&#21512;&#19987;&#23478;&#19982;MCTS&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#22269;&#38469;&#35937;&#26827;&#20013;&#30340;&#26657;&#39564;
&lt;/p&gt;
&lt;p&gt;
Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to Improve in Chess. (arXiv:2401.16852v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16852
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#21644;MCTS&#30456;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#22312;&#22269;&#38469;&#35937;&#26827;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#19979;&#26827;&#27700;&#24179;&#65292;&#39564;&#35777;&#20102;&#38598;&#25104;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24182;&#23637;&#31034;&#20102;&#34701;&#20837;&#19987;&#23478;&#30693;&#35782;&#21644;&#25112;&#30053;&#21407;&#21017;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#35745;&#31639;&#26426;&#26827;&#30424;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20351;&#29992;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#22871;&#19987;&#38376;&#35774;&#35745;&#30340;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#38024;&#23545;&#28216;&#25103;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#23450;&#21464;&#21270;&#20570;&#20986;&#21709;&#24212;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#31232;&#30095;&#28608;&#27963;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#19982;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20351;&#20854;&#19982;&#22269;&#38469;&#35937;&#26827;&#30340;&#25112;&#30053;&#38454;&#27573;&#30456;&#19968;&#33268;&#65292;&#20174;&#32780;&#25670;&#33073;&#20256;&#32479;&#30340;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#19981;&#21516;&#30340;&#28216;&#25103;&#38454;&#27573;&#23450;&#20041;&#65292;&#23558;&#35745;&#31639;&#20219;&#21153;&#26377;&#25928;&#22320;&#20998;&#37197;&#32473;&#22810;&#20010;&#19987;&#23478;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#26174;&#31034;&#65292;&#22312;&#28216;&#25103;&#23454;&#21147;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#21333;&#27169;&#22411;&#26694;&#26550;&#12290;&#36825;&#35777;&#23454;&#20102;&#25105;&#20204;&#38598;&#25104;&#26041;&#27861;&#30340;&#21151;&#25928;&#65292;&#24182;&#20984;&#26174;&#20102;&#23558;&#19987;&#23478;&#30693;&#35782;&#21644;&#25112;&#30053;&#21407;&#21017;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new approach that integrates deep learning with computational chess, using both the Mixture of Experts (MoE) method and Monte-Carlo Tree Search (MCTS). Our methodology employs a suite of specialized models, each designed to respond to specific changes in the game's input data. This results in a framework with sparsely activated models, which provides significant computational benefits. Our framework combines the MoE method with MCTS, in order to align it with the strategic phases of chess, thus departing from the conventional ``one-for-all'' model. Instead, we utilize distinct game phase definitions to effectively distribute computational tasks across multiple expert neural networks. Our empirical research shows a substantial improvement in playing strength, surpassing the traditional single-model framework. This validates the efficacy of our integrated approach and highlights the potential of incorporating expert knowledge and strategic principles into neural net
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#22312;&#25968;&#25454;&#23436;&#25972;&#24615;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;Random Forest&#31639;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16843</link><description>&lt;p&gt;
&#35780;&#20272;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#22312;&#25968;&#25454;&#23436;&#25972;&#24615;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65306;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating ML-Based Anomaly Detection Across Datasets of Varied Integrity: A Case Study. (arXiv:2401.16843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#22312;&#25968;&#25454;&#23436;&#25972;&#24615;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;Random Forest&#31639;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#32593;&#32476;&#27969;&#37327;&#24322;&#24120;&#26816;&#27979;&#20013;&#25968;&#25454;&#23436;&#25972;&#24615;&#30340;&#26222;&#36941;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#32463;&#36807;&#25913;&#36827;&#30340;CICIDS-2017&#25968;&#25454;&#38598;&#30340;&#29256;&#26412;&#65292;NFS-2023-nTE&#21644;NFS-2023-TE&#65292;&#21033;&#29992;NFStream&#36827;&#34892;&#26041;&#27861;&#19978;&#21512;&#29702;&#30340;&#27969;&#37327;&#21040;&#26399;&#21644;&#26631;&#35760;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#27604;&#20102;Random Forest&#65288;RF&#65289;&#31639;&#27861;&#22312;&#21407;&#22987;&#30340;CICIDS-2017&#25968;&#25454;&#38598;&#12289;&#20854;&#25913;&#36827;&#29256;&#26412;WTMC-2021&#21644;CRiSIS-2022&#20197;&#21450;&#25105;&#20204;&#22522;&#20110;NFStream&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#22312;&#20108;&#20998;&#31867;&#21644;&#22810;&#20998;&#31867;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;RF&#27169;&#22411;&#34920;&#29616;&#20986;&#24322;&#24120;&#30340;&#31283;&#20581;&#24615;&#65292;&#26080;&#35770;&#24213;&#23618;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#22914;&#20309;&#65292;&#22312;&#24615;&#33021;&#25351;&#26631;&#19978;&#37117;&#33021;&#20445;&#25345;&#19968;&#33268;&#30340;&#39640;&#27700;&#24179;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#23454;&#38469;&#24433;&#21709;&#30340;&#37325;&#35201;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cybersecurity remains a critical challenge in the digital age, with network traffic flow anomaly detection being a key pivotal instrument in the fight against cyber threats. In this study, we address the prevalent issue of data integrity in network traffic datasets, which are instrumental in developing machine learning (ML) models for anomaly detection. We introduce two refined versions of the CICIDS-2017 dataset, NFS-2023-nTE and NFS-2023-TE, processed using NFStream to ensure methodologically sound flow expiration and labeling. Our research contrasts the performance of the Random Forest (RF) algorithm across the original CICIDS-2017, its refined counterparts WTMC-2021 and CRiSIS-2022, and our NFStream-generated datasets, in both binary and multi-class classification contexts. We observe that the RF model exhibits exceptional robustness, achieving consistent high-performance metrics irrespective of the underlying dataset quality, which prompts a critical discussion on the actual impac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;T-CUR&#20998;&#35299;&#30340;&#21487;&#20998;&#31163;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.16836</link><description>&lt;p&gt;
&#22522;&#20110;T-CUR&#20998;&#35299;&#30340;&#21487;&#20998;&#31163;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition. (arXiv:2401.16836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;T-CUR&#20998;&#35299;&#30340;&#21487;&#20998;&#31163;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#26694;&#26550;&#20869;&#35299;&#20915;NMF&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#21487;&#20998;&#31163;&#24615;&#20551;&#35774;&#65292;&#26368;&#36817;&#28436;&#21464;&#20026;&#21487;&#20998;&#31163;&#30340;&#27010;&#24565;&#12290;&#36825;&#19968;&#36827;&#23637;&#20026;&#21407;&#22987;&#25968;&#25454;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26680;&#24515;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25968;&#25454;&#26356;&#33258;&#28982;&#22320;&#34987;&#34920;&#31034;&#20026;&#22810;&#32500;&#25968;&#32452;&#65292;&#22914;&#22270;&#20687;&#25110;&#35270;&#39057;&#12290;&#23558;NMF&#24212;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#28041;&#21450;&#21521;&#37327;&#21270;&#65292;&#20250;&#23548;&#33268;&#20002;&#22833;&#20851;&#38190;&#30340;&#22810;&#32500;&#24230;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20445;&#30041;&#25968;&#25454;&#20013;&#36825;&#20123;&#22266;&#26377;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#36716;&#21521;&#24352;&#37327;(&#22810;&#32500;&#25968;&#32452;)&#24182;&#21033;&#29992;&#24352;&#37327;t&#20056;&#31215;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#21487;&#20998;&#31163;&#30340;NMF&#25193;&#23637;&#21040;&#24352;&#37327;&#35774;&#32622;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#25105;&#20204;&#25152;&#31216;&#30340;&#21487;&#20998;&#31163;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;(NTF)&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20132;&#26367;&#32034;&#24341;&#36873;&#25321;&#26041;&#27861;&#26469;&#36873;&#25321;cos
&lt;/p&gt;
&lt;p&gt;
Nonnegative Matrix Factorization (NMF) is an important unsupervised learning method to extract meaningful features from data. To address the NMF problem within a polynomial time framework, researchers have introduced a separability assumption, which has recently evolved into the concept of coseparability. This advancement offers a more efficient core representation for the original data. However, in the real world, the data is more natural to be represented as a multi-dimensional array, such as images or videos. The NMF's application to high-dimensional data involves vectorization, which risks losing essential multi-dimensional correlations. To retain these inherent correlations in the data, we turn to tensors (multidimensional arrays) and leverage the tensor t-product. This approach extends the coseparable NMF to the tensor setting, creating what we term coseparable Nonnegative Tensor Factorization (NTF). In this work, we provide an alternating index selection method to select the cos
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#30340;&#30693;&#35782;&#36861;&#36394;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16832</link><description>&lt;p&gt;
&#23545;&#21512;&#25104;&#23398;&#29983;&#25968;&#25454;&#30340;&#30693;&#35782;&#36861;&#36394;&#24615;&#33021;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Knowledge Tracing performance on synthesised student data. (arXiv:2401.16832v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16832
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#30340;&#30693;&#35782;&#36861;&#36394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#26088;&#22312;&#36890;&#36807;&#36319;&#36394;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#30340;&#21457;&#23637;&#26469;&#39044;&#27979;&#20182;&#20204;&#26410;&#26469;&#30340;&#34920;&#29616;&#12290;&#23613;&#31649;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#20445;&#25252;&#38382;&#39064;&#65292;KT&#27169;&#22411;&#22312;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#20173;&#21463;&#21040;&#25968;&#25454;&#38480;&#21046;&#65306;1&#65289;&#30001;&#20110;&#25968;&#25454;&#20445;&#25252;&#38382;&#39064;&#65292;&#26080;&#27861;&#33719;&#24471;&#29616;&#23454;&#29983;&#27963;&#25968;&#25454;&#65307;2&#65289;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#22810;&#26679;&#24615;&#65307;3&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#37325;&#22797;&#35760;&#24405;&#30340;&#22122;&#38899;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#22522;&#20110;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#31574;&#30053;&#27169;&#25311;&#20102;&#23398;&#29983;&#25968;&#25454;&#65292;&#24182;&#27979;&#35797;&#20102;&#23427;&#20204;&#22312;&#20004;&#20010;KT&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#25105;&#20204;&#35266;&#23519;&#21040;&#39069;&#22806;&#30340;&#21512;&#25104;&#25968;&#25454;&#21482;&#24102;&#26469;&#20102;&#36731;&#24494;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#19982;&#30495;&#23454;&#25968;&#25454;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Tracing (KT) aims to predict the future performance of students by tracking the development of their knowledge states. Despite all the recent progress made in this field, the application of KT models in education systems is still restricted from the data perspectives: 1) limited access to real life data due to data protection concerns, 2) lack of diversity in public datasets, 3) noises in benchmark datasets such as duplicate records. To resolve these problems, we simulated student data with three statistical strategies based on public datasets and tested their performance on two KT baselines. While we observe only minor performance improvement with additional synthetic data, our work shows that using only synthetic data for training can lead to similar performance as real data.
&lt;/p&gt;</description></item><item><title>H2O-Danube-1.8B &#26159;&#19968;&#20010;&#22312; 1T &#20010;&#26631;&#35760;&#19978;&#35757;&#32451;&#30340; 18 &#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#25351;&#26631;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#21644;&#20248;&#21270;&#35757;&#32451;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.16818</link><description>&lt;p&gt;
H2O-Danube-1.8B &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
H2O-Danube-1.8B Technical Report. (arXiv:2401.16818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16818
&lt;/p&gt;
&lt;p&gt;
H2O-Danube-1.8B &#26159;&#19968;&#20010;&#22312; 1T &#20010;&#26631;&#35760;&#19978;&#35757;&#32451;&#30340; 18 &#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#25351;&#26631;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#21644;&#20248;&#21270;&#35757;&#32451;&#30340;&#32842;&#22825;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#27982;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; H2O-Danube-1.8B&#65292;&#36825;&#26159;&#19968;&#20010;&#22312; 1T &#20010;&#26631;&#35760;&#19978;&#35757;&#32451;&#30340; 18 &#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#36981;&#24490; LLama 2 &#21644; Mistral &#30340;&#26680;&#24515;&#21407;&#21017;&#12290;&#25105;&#20204;&#21033;&#29992;&#21644;&#25913;&#36827;&#20102;&#21508;&#31181;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#27169;&#22411;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#24635;&#26631;&#35760;&#25968;&#37327;&#26126;&#26174;&#23569;&#20110;&#30456;&#20284;&#35268;&#27169;&#30340;&#21442;&#32771;&#27169;&#22411;&#65292;&#20294;&#23427;&#22312;&#20247;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20102;&#39640;&#24230;&#31454;&#20105;&#21147;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#21644;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#35757;&#32451;&#30340;&#32842;&#22825;&#27169;&#22411;&#12290;&#25105;&#20204;&#20197; Apache 2.0 &#35768;&#21487;&#35777;&#23558; H2O-Danube-1.8B &#24320;&#25918;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160; LLMs &#30340;&#32463;&#27982;&#27665;&#20027;&#21270;&#65292;&#35753;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present H2O-Danube-1.8B, a 1.8B language model trained on 1T tokens following the core principles of LLama 2 and Mistral. We leverage and refine various techniques for pre-training large language models. Although our model is trained on significantly fewer total tokens compared to reference models of similar size, it exhibits highly competitive metrics across a multitude of benchmarks. We additionally release a chat model trained with supervised fine-tuning followed by direct preference optimization. We make H2O-Danube-1.8B openly available under Apache 2.0 license further democratizing LLMs to a wider audience economically.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26041;&#24335;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#25512;&#24191;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#24615;&#36136;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.16808</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#26469;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Encoding Temporal Statistical-space Priors via Augmented Representation. (arXiv:2401.16808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16808
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26041;&#24335;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#25512;&#24191;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#24615;&#36136;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20173;&#28982;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#26102;&#38388;&#32500;&#24230;&#19982;&#35768;&#22810;&#39046;&#22495;&#23494;&#20999;&#30456;&#20851;&#12290;&#23613;&#31649;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#39640;&#22122;&#22768;&#20449;&#21495;&#27604;&#12289;&#38750;&#27491;&#24577;&#24615;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#25968;&#25454;&#32570;&#20047;&#20173;&#28982;&#26159;&#25361;&#25112;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#22686;&#24378;&#25216;&#26415;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22686;&#24378;&#34920;&#31034;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#20316;&#20026;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#36827;&#34892;&#32534;&#30721;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;&#32479;&#35745;&#31354;&#38388;&#22686;&#24378;&#34920;&#31034;&#65288;SSAR&#65289;&#12290;&#22522;&#20110;&#39640;&#32500;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#21551;&#21457;&#20102;&#25105;&#20204;&#30340;&#34920;&#31034;&#22686;&#24378;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#20010;&#19979;&#28216;&#26102;&#38388;&#23398;&#20064;&#31639;&#27861;&#30340;&#32463;&#39564;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#26816;&#26597;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20987;&#36133;&#20102;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#30340;&#24615;&#36136;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling time series data remains a pervasive issue as the temporal dimension is inherent to numerous domains. Despite significant strides in time series forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and lack of data continue challenging practitioners. In response, we leverage a simple representation augmentation technique to overcome these challenges. Our augmented representation acts as a statistical-space prior encoded at each time step. In response, we name our method Statistical-space Augmented Representation (SSAR). The underlying high-dimensional data-generating process inspires our representation augmentation. We rigorously examine the empirical generalization performance on two data sets with two downstream temporal learning algorithms. Our approach significantly beats all five up-to-date baselines. Moreover, the highly modular nature of our approach can easily be applied to various settings. Lastly, fully-fledged theoretical perspectives are availa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PBSCSR&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#38050;&#29748;&#20048;&#35889;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#21644;&#30456;&#20851;&#20803;&#25968;&#25454;&#65292;&#21487;&#20197;&#36827;&#34892;&#22810;&#20010;&#30740;&#31350;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.16803</link><description>&lt;p&gt;
PBSCSR&#65306;&#38050;&#29748;&#40657;&#24066;&#20048;&#35889;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset. (arXiv:2401.16803v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PBSCSR&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#38050;&#29748;&#20048;&#35889;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#21644;&#30456;&#20851;&#20803;&#25968;&#25454;&#65292;&#21487;&#20197;&#36827;&#34892;&#22810;&#20010;&#30740;&#31350;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;PBSCSR&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#38050;&#29748;&#20048;&#35889;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#30740;&#31350;&#20316;&#26354;&#23478;&#39118;&#26684;&#35782;&#21035;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#26082;&#20687;MNIST&#19968;&#26679;&#26131;&#20110;&#33719;&#21462;&#65292;&#21448;&#20687;ImageNet&#19968;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;IMSLP&#30340;&#38050;&#29748;&#20048;&#35889;&#22270;&#20687;&#20013;&#37319;&#26679;&#22266;&#23450;&#38271;&#24230;&#30340;&#30423;&#29256;&#20048;&#35889;&#29255;&#27573;&#12290;&#25968;&#25454;&#38598;&#26412;&#36523;&#21253;&#21547;40,000&#20010;62x64&#30340;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#65292;&#29992;&#20110;&#36827;&#34892;9&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;100,000&#20010;62x64&#30340;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#65292;&#29992;&#20110;&#36827;&#34892;100&#20998;&#31867;&#20219;&#21153;&#65292;&#36824;&#26377;29,310&#20010;&#26080;&#26631;&#31614;&#30340;&#21487;&#21464;&#38271;&#24230;&#30340;&#30423;&#29256;&#20048;&#35889;&#22270;&#20687;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;&#26631;&#35760;&#25968;&#25454;&#20197;&#19982;MNIST&#22270;&#20687;&#31867;&#20284;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20197;&#20415;&#26497;&#20854;&#26041;&#20415;&#22320;&#21487;&#35270;&#21270;&#12289;&#25805;&#20316;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21253;&#25324;&#30456;&#20851;&#30340;&#20803;&#25968;&#25454;&#65292;&#20197;&#20801;&#35768;&#35775;&#38382;IMSLP&#19978;&#30340;&#21407;&#22987;&#20048;&#35889;&#22270;&#20687;&#21644;&#20854;&#20182;&#30456;&#20851;&#25968;&#25454;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20960;&#20010;&#21487;&#20197;&#20351;&#29992;&#35813;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article motivates, describes, and presents the PBSCSR dataset for studying composer style recognition of piano sheet music. Our overarching goal was to create a dataset for studying composer style recognition that is "as accessible as MNIST and as challenging as ImageNet." To achieve this goal, we sample fixed-length bootleg score fragments from piano sheet music images on IMSLP. The dataset itself contains 40,000 62x64 bootleg score images for a 9-way classification task, 100,000 62x64 bootleg score images for a 100-way classification task, and 29,310 unlabeled variable-length bootleg score images for pretraining. The labeled data is presented in a form that mirrors MNIST images, in order to make it extremely easy to visualize, manipulate, and train models in an efficient manner. Additionally, we include relevant metadata to allow access to the underlying raw sheet music images and other related data on IMSLP. We describe several research tasks that could be studied with the data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;"mspace"&#65292;&#36866;&#29992;&#20110;&#39044;&#27979;&#26102;&#24577;&#22270;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;mspace&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16800</link><description>&lt;p&gt;
&#22312;&#26102;&#24577;&#22270;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#39044;&#27979;&#30340;&#22312;&#32447;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Algorithm for Node Feature Forecasting in Temporal Graphs. (arXiv:2401.16800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;"mspace"&#65292;&#36866;&#29992;&#20110;&#39044;&#27979;&#26102;&#24577;&#22270;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;mspace&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"mspace"&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#26102;&#24577;&#22270;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#28789;&#27963;&#22320;&#25429;&#25417;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#31354;&#38388;&#20132;&#21449;&#30456;&#20851;&#24615;&#20197;&#21450;&#33410;&#28857;&#20869;&#30340;&#26102;&#38388;&#33258;&#30456;&#20851;&#24615;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#27010;&#29575;&#21644;&#30830;&#23450;&#24615;&#30340;&#22810;&#27493;&#39044;&#27979;&#65292;&#36866;&#29992;&#20110;&#20272;&#35745;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#19982;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#32463;&#20856;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#21508;&#31181;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;mspace&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#27700;&#24179;&#30456;&#24403;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#23427;&#20204;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;mspace&#22312;&#20855;&#26377;&#19981;&#21516;&#35757;&#32451;&#26679;&#26412;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#19982;GNN&#26041;&#27861;&#30456;&#27604;&#30340;&#19968;&#20010;&#26174;&#33879;&#20248;&#21183;&#65292;&#21518;&#32773;&#38656;&#35201;&#20016;&#23500;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;mspace&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#29702;&#35770;&#27169;&#22411;&#26469;&#35777;&#26126;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;mspace&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an online algorithm "mspace" for forecasting node features in temporal graphs, which adeptly captures spatial cross-correlation among different nodes as well as the temporal autocorrelation within a node. The algorithm can be used for both probabilistic and deterministic multi-step forecasting, making it applicable for estimation and generation tasks. Comparative evaluations against various baselines, including graph neural network (GNN) based models and classical Kalman filters, demonstrate that mspace performs at par with the state-of-the-art and even surpasses them on some datasets. Importantly, mspace demonstrates consistent robustness across datasets with varying training sizes, a notable advantage over GNN-based methods requiring abundant training samples to learn the spatiotemporal trends in the data effectively. Therefore, employing mspace is advantageous in scenarios where the training sample availability is limited. Additionally, we establish theoret
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#21327;&#35758;&#65292;&#21487;&#23398;&#20064;&#25552;&#31034;&#20316;&#20026;&#20266;&#25554;&#34917;&#65288;PAI&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26469;&#27169;&#25311;&#19979;&#28216;&#27169;&#22411;&#23545;&#32570;&#22833;&#20540;&#30340;&#38544;&#21547;&#20559;&#22909;&#65292;&#26174;&#33879;&#25552;&#21319;&#25152;&#26377;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20998;&#26512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16796</link><description>&lt;p&gt;
&#21487;&#23398;&#20064;&#25552;&#31034;&#20316;&#20026;&#20266;&#25554;&#34917;&#26041;&#27861;&#65306;&#37325;&#26032;&#35780;&#20272;&#20256;&#32479;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#25554;&#34917;&#22312;&#19979;&#28216;&#20020;&#24202;&#39044;&#27979;&#20013;&#30340;&#24517;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learnable Prompt as Pseudo-Imputation: Reassessing the Necessity of Traditional EHR Data Imputation in Downstream Clinical Prediction. (arXiv:2401.16796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#21327;&#35758;&#65292;&#21487;&#23398;&#20064;&#25552;&#31034;&#20316;&#20026;&#20266;&#25554;&#34917;&#65288;PAI&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26469;&#27169;&#25311;&#19979;&#28216;&#27169;&#22411;&#23545;&#32570;&#22833;&#20540;&#30340;&#38544;&#21547;&#20559;&#22909;&#65292;&#26174;&#33879;&#25552;&#21319;&#25152;&#26377;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20998;&#26512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20998;&#26512;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#26159;&#21307;&#23398;&#20449;&#24687;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#30740;&#31350;&#38382;&#39064;&#12290;EHR&#20013;&#23384;&#22312;&#22823;&#37327;&#32570;&#22833;&#20540;&#65292;&#36825;&#20351;&#24471;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38590;&#20197;&#30452;&#25509;&#22522;&#20110;EHR&#27169;&#22411;&#21270;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#21327;&#35758;&#38656;&#35201;&#20351;&#29992;&#32479;&#35745;&#20449;&#24687;&#25110;&#25554;&#34917;&#27169;&#22411;&#26469;&#37325;&#26500;&#32570;&#22833;&#20540;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#21327;&#35758;&#20250;&#23558;&#38750;&#29616;&#23454;&#30340;&#25968;&#25454;&#27880;&#20837;&#21040;&#19979;&#28216;EHR&#20998;&#26512;&#27169;&#22411;&#20013;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#21327;&#35758;&#8212;&#8212;&#21487;&#23398;&#20064;&#25552;&#31034;&#20316;&#20026;&#20266;&#25554;&#34917;&#65288;PAI&#65289;&#12290;PAI&#19981;&#20877;&#24341;&#20837;&#20219;&#20309;&#25554;&#34917;&#25968;&#25454;&#65292;&#32780;&#26159;&#26500;&#24314;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26469;&#27169;&#25311;&#19979;&#28216;&#27169;&#22411;&#23545;&#32570;&#22833;&#20540;&#30340;&#38544;&#21547;&#20559;&#22909;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#25152;&#26377;EHR&#20998;&#26512;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25968;&#25454;&#19981;&#36275;&#21644;&#39640;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;PAI&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing the health status of patients based on Electronic Health Records (EHR) is a fundamental research problem in medical informatics. The presence of extensive missing values in EHR makes it challenging for deep neural networks to directly model the patient's health status based on EHR. Existing deep learning training protocols require the use of statistical information or imputation models to reconstruct missing values; however, the protocols inject non-realistic data into downstream EHR analysis models, significantly limiting model performance. This paper introduces Learnable Prompt as Pseudo Imputation (PAI) as a new training protocol. PAI no longer introduces any imputed data but constructs a learnable prompt to model the implicit preferences of the downstream model for missing values, resulting in a significant performance improvement for all EHR analysis models. Additionally, our experiments show that PAI exhibits higher robustness in situations of data insufficiency and hig
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#36275;&#29699;&#29699;&#21592;&#30340;&#36716;&#20250;&#36153;&#29992;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#20465;&#20048;&#37096;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#34920;&#29616;&#24182;&#22686;&#21152;&#20465;&#20048;&#37096;&#30340;&#39044;&#31639;&#12290;</title><link>http://arxiv.org/abs/2401.16795</link><description>&lt;p&gt;
&#22522;&#20110;&#24615;&#33021;&#27934;&#23519;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#36275;&#29699;&#36716;&#20250;&#36153;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performance Insights-based AI-driven Football Transfer Fee Prediction. (arXiv:2401.16795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#36275;&#29699;&#29699;&#21592;&#30340;&#36716;&#20250;&#36153;&#29992;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#20465;&#20048;&#37096;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#34920;&#29616;&#24182;&#22686;&#21152;&#20465;&#20048;&#37096;&#30340;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#39044;&#27979;&#36275;&#29699;&#29699;&#21592;&#30340;&#36716;&#20250;&#36153;&#29992;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20465;&#20048;&#37096;&#22312;&#36141;&#20080;&#21644;&#20986;&#21806;&#29699;&#21592;&#26102;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#25552;&#39640;&#34920;&#29616;&#21644;&#22686;&#21152;&#20465;&#20048;&#37096;&#39044;&#31639;&#12290;&#36890;&#36807;&#25910;&#38598;&#29699;&#21592;&#34920;&#29616;&#12289;&#36716;&#20250;&#36153;&#29992;&#21644;&#20854;&#20182;&#21487;&#33021;&#24433;&#21709;&#29699;&#21592;&#20215;&#20540;&#30340;&#22240;&#32032;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29699;&#21592;&#23545;&#27604;&#36187;&#24433;&#21709;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25152;&#24471;&#32467;&#26524;&#20316;&#20026;&#36716;&#20250;&#36153;&#29992;&#39044;&#27979;&#22120;&#30340;&#29305;&#24449;&#20043;&#19968;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20465;&#20048;&#37096;&#35782;&#21035;&#34987;&#20302;&#20272;&#30340;&#29699;&#21592;&#65292;&#24182;&#22312;&#20986;&#21806;&#26102;&#33719;&#24471;&#21033;&#28070;&#12290;&#23427;&#36824;&#21487;&#20197;&#24110;&#21161;&#20465;&#20048;&#37096;&#36991;&#20813;&#20026;&#29699;&#21592;&#25903;&#20184;&#36807;&#39640;&#36153;&#29992;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#36275;&#29699;&#20465;&#20048;&#37096;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#65292;&#20026;&#20182;&#20204;&#22312;&#29699;&#21592;&#25307;&#21215;&#21644;&#36716;&#20250;&#26041;&#38754;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
We developed an artificial intelligence approach to predict the transfer fee of a football player. This model can help clubs make better decisions about which players to buy and sell, which can lead to improved performance and increased club budgets. Having collected data on player performance, transfer fees, and other factors that might affect a player's value, we then used this data to train a machine learning model that can accurately predict a player's impact on the game. We further passed the obtained results as one of the features to the predictor of transfer fees. The model can help clubs identify players who are undervalued and who could be sold for a profit. It can also help clubs avoid overpaying for players. We believe that our model can be a valuable tool for football clubs. It can help them make better decisions about player recruitment and transfers.
&lt;/p&gt;</description></item><item><title>ACAI&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#20113;&#31471;&#30340;&#23384;&#20648;&#21644;&#32034;&#24341;&#30340;&#24212;&#29992;&#65292;&#25903;&#25345;&#27169;&#22411;&#31649;&#29702;&#65292;&#24110;&#21161;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#30340;&#29983;&#20135;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16791</link><description>&lt;p&gt;
&#21152;&#36895;&#20113;&#31471;&#20154;&#24037;&#26234;&#33021; (ACAI)
&lt;/p&gt;
&lt;p&gt;
Accelerated Cloud for Artificial Intelligence (ACAI). (arXiv:2401.16791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16791
&lt;/p&gt;
&lt;p&gt;
ACAI&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#20113;&#31471;&#30340;&#23384;&#20648;&#21644;&#32034;&#24341;&#30340;&#24212;&#29992;&#65292;&#25903;&#25345;&#27169;&#22411;&#31649;&#29702;&#65292;&#24110;&#21161;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#30340;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064; (ML) &#27169;&#22411;&#26159;&#19968;&#20010;&#38656;&#35201;&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#20184;&#20986;&#21162;&#21147;&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;&#32437;&#21521;&#19978;&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#31649;&#36947;&#36890;&#24120;&#21253;&#25324;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#21021;&#22987;ETL&#65288;&#25552;&#21462;&#12289;&#21464;&#25442;&#12289;&#21152;&#36733;&#65289;&#65292;&#27169;&#22411;&#35757;&#32451;&#38454;&#27573;&#20197;&#21450;&#20174;&#20013;&#33719;&#21462;&#27169;&#22411;&#24615;&#33021;&#32479;&#35745;&#30340;&#35780;&#20272;&#38454;&#27573;&#12290;&#27178;&#21521;&#19978;&#65292;&#22312;&#27169;&#22411;&#37197;&#32622;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#21487;&#33021;&#38656;&#35201;&#22810;&#20010;&#36825;&#26679;&#30340;&#31649;&#36947;&#26469;&#25214;&#21040;&#26368;&#20339;&#27169;&#22411;&#12290;&#35768;&#22810;&#20174;&#19994;&#32773;&#24120;&#24120;&#25163;&#21160;&#32500;&#25252;&#26085;&#24535;&#24182;&#32534;&#20889;&#31616;&#21333;&#30340;&#31896;&#21512;&#20195;&#30721;&#20197;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#20113;&#31471;&#36827;&#34892;&#36825;&#20010;&#36807;&#31243;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#36164;&#28304;&#20379;&#24212;&#12289;&#25968;&#25454;&#31649;&#29702;&#20197;&#21450;&#20316;&#19994;&#21382;&#21490;&#35760;&#24405;&#30340;&#35760;&#24405;&#65292;&#20197;&#30830;&#20445;&#32467;&#26524;&#21487;&#37325;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#20113;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#21152;&#36895;&#20113;&#31471;&#20154;&#24037;&#26234;&#33021;&#65288;ACAI&#65289;&#65292;&#20197;&#24110;&#21161;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20174;&#19994;&#32773;&#30340;&#29983;&#20135;&#21147;&#12290;ACAI&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#20113;&#31471;&#30340;&#23384;&#20648;&#21644;&#32034;&#24341;&#30340;&#24212;&#29992;&#65292;&#25903;&#25345;&#27169;&#22411;&#31649;&#29702;&#65292;&#31616;&#21270;&#20102;&#36825;&#20010;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training an effective Machine learning (ML) model is an iterative process that requires effort in multiple dimensions. Vertically, a single pipeline typically includes an initial ETL (Extract, Transform, Load) of raw datasets, a model training stage, and an evaluation stage where the practitioners obtain statistics of the model performance. Horizontally, many such pipelines may be required to find the best model within a search space of model configurations. Many practitioners resort to maintaining logs manually and writing simple glue code to automate the workflow. However, carrying out this process on the cloud is not a trivial task in terms of resource provisioning, data management, and bookkeeping of job histories to make sure the results are reproducible. We propose an end-to-end cloud-based machine learning platform, Accelerated Cloud for AI (ACAI), to help improve the productivity of ML practitioners. ACAI achieves this goal by enabling cloud-based storage of indexed, labeled, a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21517;&#20026;HawkEye&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#30340;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#22312;&#22788;&#29702;&#31163;&#32676;&#20540;&#21644;&#22122;&#22768;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16785</link><description>&lt;p&gt;
&#36890;&#36807;HawkEye Loss&#22312;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#20013;&#25552;&#39640;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Efficiency and Robustness in Support Vector Regression with HawkEye Loss. (arXiv:2401.16785v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16785
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21517;&#20026;HawkEye&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#30340;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#65292;&#26412;&#25991;&#35299;&#20915;&#20102;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#22312;&#22788;&#29702;&#31163;&#32676;&#20540;&#21644;&#22122;&#22768;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#30001;&#20110;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#20102;&#26174;&#33879;&#30340;&#20851;&#27880;&#65292;&#22312;&#38754;&#23545;&#31163;&#32676;&#20540;&#21644;&#22122;&#22768;&#26102;&#65292;SVR&#36935;&#21040;&#20102;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20351;&#29992;&#20102;&#949;-insensitive&#25439;&#22833;&#20989;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#20855;&#26377;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;SVR&#24050;&#25104;&#20026;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#22686;&#24378;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#35774;&#35745;&#20855;&#26377;&#24179;&#28369;&#29305;&#24615;&#30340;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#65292;&#20419;&#36827;&#20102;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#30340;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#24378;&#35843;&#30340;&#26159;&#65292;&#36825;&#20123;&#26377;&#30028;&#21644;&#24179;&#28369;&#30340;&#25439;&#22833;&#20989;&#25968;&#19981;&#20855;&#26377;&#19968;&#20010;&#19981;&#25935;&#24863;&#30340;&#21306;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;HawkEye&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#30340;&#23545;&#31216;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#19978;&#36848;&#32422;&#26463;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;HawkEye&#25439;&#22833;&#20989;&#25968;&#20316;&#20026;SVR&#20013;&#30340;&#31532;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#31361;&#20986;&#26174;&#31034;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support vector regression (SVR) has garnered significant popularity over the past two decades owing to its wide range of applications across various fields. Despite its versatility, SVR encounters challenges when confronted with outliers and noise, primarily due to the use of the $\varepsilon$-insensitive loss function. To address this limitation, SVR with bounded loss functions has emerged as an appealing alternative, offering enhanced generalization performance and robustness. Notably, recent developments focus on designing bounded loss functions with smooth characteristics, facilitating the adoption of gradient-based optimization algorithms. However, it's crucial to highlight that these bounded and smooth loss functions do not possess an insensitive zone. In this paper, we address the aforementioned constraints by introducing a novel symmetric loss function named the HawkEye loss function. It is worth noting that the HawkEye loss function stands out as the first loss function in SVR
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#22270;&#20844;&#24179;&#23398;&#20064;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#20915;&#23450;&#22270;&#20013;&#20559;&#24046;&#30340;&#22240;&#32032;&#65292;&#24182;&#25506;&#32034;&#20102;&#35757;&#32451;&#22270;&#21644;&#27979;&#35797;&#22270;&#20043;&#38388;&#34920;&#31034;&#36317;&#31163;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30830;&#20445;&#20844;&#24179;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.16784</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#22270;&#20844;&#24179;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Fairness Learning under Distribution Shifts. (arXiv:2401.16784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16784
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#22270;&#20844;&#24179;&#23398;&#20064;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#20915;&#23450;&#22270;&#20013;&#20559;&#24046;&#30340;&#22240;&#32032;&#65292;&#24182;&#25506;&#32034;&#20102;&#35757;&#32451;&#22270;&#21644;&#27979;&#35797;&#22270;&#20043;&#38388;&#34920;&#31034;&#36317;&#31163;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30830;&#20445;&#20844;&#24179;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;GNNs&#21487;&#33021;&#20250;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#32487;&#25215;&#20559;&#35265;&#65292;&#24182;&#26681;&#25454;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#21644;&#31181;&#26063;&#65289;&#20570;&#20986;&#27495;&#35270;&#24615;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#22312;GNNs&#19978;&#30830;&#20445;&#20844;&#24179;&#24615;&#65292;&#20294;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#22312;&#30456;&#21516;&#20998;&#24067;&#19979;&#30340;&#20551;&#35774;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#20010;&#22270;&#12290;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#65292;&#22270;&#30340;&#20844;&#24179;&#24615;&#24615;&#33021;&#26159;&#21542;&#20250;&#38477;&#20302;&#65311;&#20998;&#24067;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#22270;&#30340;&#20844;&#24179;&#23398;&#20064;&#65311;&#25152;&#26377;&#36825;&#20123;&#24320;&#25918;&#38382;&#39064;&#20174;&#29702;&#35770;&#19978;&#24456;&#22823;&#31243;&#24230;&#19978;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#29702;&#35770;&#19978;&#30830;&#23450;&#20102;&#20915;&#23450;&#22270;&#20013;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#24433;&#21709;&#27979;&#35797;&#22270;&#20844;&#24179;&#24615;&#30340;&#22240;&#32032;&#65292;&#20854;&#20013;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#22240;&#32032;&#26159;&#35757;&#32451;&#22270;&#21644;&#27979;&#35797;&#22270;&#20043;&#38388;&#26576;&#20123;&#32676;&#20307;&#30340;&#34920;&#31034;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have achieved remarkable performance on graph-structured data. However, GNNs may inherit prejudice from the training data and make discriminatory predictions based on sensitive attributes, such as gender and race. Recently, there has been an increasing interest in ensuring fairness on GNNs, but all of them are under the assumption that the training and testing data are under the same distribution, i.e., training data and testing data are from the same graph. Will graph fairness performance decrease under distribution shifts? How does distribution shifts affect graph fairness learning? All these open questions are largely unexplored from a theoretical perspective. To answer these questions, we first theoretically identify the factors that determine bias on a graph. Subsequently, we explore the factors influencing fairness on testing graphs, with a noteworthy factor being the representation distances of certain groups between the training and testing graph. M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#32974;&#20799;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#33041;&#36816;&#21160;&#26657;&#27491;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#26657;&#27491;&#26041;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#23545;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#25913;&#36827;&#30340;&#21512;&#29702;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.16782</link><description>&lt;p&gt;
&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#32974;&#20799;&#33041;&#36816;&#21160;&#26657;&#27491;&#30340;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Literature Review on Fetus Brain Motion Correction in MRI. (arXiv:2401.16782v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#32974;&#20799;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#33041;&#36816;&#21160;&#26657;&#27491;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#26657;&#27491;&#26041;&#27861;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#23545;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#25913;&#36827;&#30340;&#21512;&#29702;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#32974;&#20799;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#33041;&#36816;&#21160;&#26657;&#27491;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#21508;&#31181;&#24403;&#20195;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#36827;&#27493;&#65292;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#20854;&#20013;&#21253;&#25324;&#20256;&#32479;&#30340;&#19977;&#32500;&#32974;&#20799;&#30913;&#20849;&#25391;&#25104;&#20687;&#26657;&#27491;&#26041;&#27861;&#65292;&#22914;&#20999;&#29255;&#21040;&#20307;&#31215;&#37197;&#20934;&#65288;SVR&#65289;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#65292;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#65292;&#21464;&#21387;&#22120;&#65288;Transformers&#65289;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#20197;&#21450;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;&#20174;&#36825;&#31687;&#25991;&#29486;&#32508;&#36848;&#20013;&#24471;&#20986;&#30340;&#35265;&#35299;&#21453;&#26144;&#20102;&#23545;&#32974;&#20799;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#33041;&#36816;&#21160;&#25216;&#26415;&#22797;&#26434;&#24615;&#21644;&#23454;&#38469;&#24433;&#21709;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#21644;&#26410;&#26469;&#25913;&#36827;&#25552;&#20379;&#20102;&#21512;&#29702;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive review of the latest advancements in fetal motion correction in MRI. We delve into various contemporary methodologies and technological advancements aimed at overcoming these challenges. It includes traditional 3D fetal MRI correction methods like Slice to Volume Registration (SVR), deep learning-based techniques such as Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) Networks, Transformers, Generative Adversarial Networks (GANs) and most recent advancements of Diffusion Models. The insights derived from this literature review reflect a thorough understanding of both the technical intricacies and practical implications of fetal motion in MRI studies, offering a reasoned perspective on potential solutions and future improvements in this field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23454;&#20363;&#35268;&#33539;&#21270;&#27969;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#22266;&#23450;&#32479;&#35745;&#25968;&#25454;&#65292;&#20063;&#19981;&#38480;&#21046;&#20110;&#39044;&#27979;&#26550;&#26500;&#12290;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#36716;&#25442;&#21644;&#39044;&#27979;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#20363;&#35268;&#33539;&#21270;&#27969;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.16777</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#20363;&#35268;&#33539;&#21270;&#27969;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Distribution Shift in Time Series Forecasting with Instance Normalization Flows. (arXiv:2401.16777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23454;&#20363;&#35268;&#33539;&#21270;&#27969;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#22266;&#23450;&#32479;&#35745;&#25968;&#25454;&#65292;&#20063;&#19981;&#38480;&#21046;&#20110;&#39044;&#27979;&#26550;&#26500;&#12290;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#36716;&#25442;&#21644;&#39044;&#27979;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#20363;&#35268;&#33539;&#21270;&#27969;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#24456;&#22823;&#31243;&#24230;&#19978;&#38459;&#30861;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#26080;&#27861;&#22788;&#29702;&#36229;&#20986;&#31616;&#21333;&#32479;&#35745;&#30340;&#20559;&#31227;&#65292;&#35201;&#20040;&#19982;&#39044;&#27979;&#27169;&#22411;&#30340;&#20860;&#23481;&#24615;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#29992;&#35299;&#32806;&#20844;&#24335;&#65292;&#19981;&#20381;&#36182;&#20110;&#22266;&#23450;&#32479;&#35745;&#25968;&#25454;&#65292;&#20063;&#19981;&#38480;&#21046;&#20110;&#39044;&#27979;&#26550;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#20844;&#24335;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#36716;&#25442;&#65288;&#22806;&#24490;&#29615;&#65289;&#21644;&#39044;&#27979;&#65288;&#20869;&#24490;&#29615;&#65289;&#30340;&#32852;&#21512;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36716;&#25442;&#32780;&#35328;&#65292;&#23545;&#34920;&#36798;&#33021;&#21147;&#21644;&#21452;&#21521;&#24615;&#30340;&#29305;&#27530;&#35201;&#27714;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#20363;&#35268;&#33539;&#21270;&#27969;&#65288;IN-Flow&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to non-stationarity of time series, the distribution shift problem largely hinders the performance of time series forecasting. Existing solutions either fail for the shifts beyond simple statistics or the limited compatibility with forecasting models. In this paper, we propose a general decoupled formulation for time series forecasting, with no reliance on fixed statistics and no restriction on forecasting architectures. Then, we make such a formulation formalized into a bi-level optimization problem, to enable the joint learning of the transformation (outer loop) and forecasting (inner loop). Moreover, the special requirements of expressiveness and bi-direction for the transformation motivate us to propose instance normalization flows (IN-Flow), a novel invertible network for time series transformation. Extensive experiments demonstrate our method consistently outperforms state-of-the-art baselines on both synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#22871;APT&#26041;&#27861;&#26469;&#35299;&#20915;&#39034;&#24207;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#20013;&#30340;&#23884;&#22871;&#26399;&#26395;&#35745;&#31639;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.16776</link><description>&lt;p&gt;
&#21033;&#29992;&#23884;&#22871;MLMC&#23545;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#30340;&#39034;&#24207;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#36827;&#34892;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Leveraging Nested MLMC for Sequential Neural Posterior Estimation with Intractable Likelihoods. (arXiv:2401.16776v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#22871;APT&#26041;&#27861;&#26469;&#35299;&#20915;&#39034;&#24207;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#20013;&#30340;&#23884;&#22871;&#26399;&#26395;&#35745;&#31639;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25552;&#20986;&#20102;&#39034;&#24207;&#31070;&#32463;&#21518;&#39564;&#20272;&#35745;&#65288;SNPE&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#27169;&#22411;&#12290;&#23427;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#33258;&#36866;&#24212;&#22320;&#29983;&#25104;&#30340;&#27169;&#25311;&#26469;&#23398;&#20064;&#21518;&#39564;&#12290;&#20316;&#20026;&#19968;&#31181;SNPE&#25216;&#26415;&#65292;Greenberg&#31561;&#20154;&#65288;2019&#65289;&#25552;&#20986;&#30340;&#33258;&#21160;&#21518;&#39564;&#21464;&#25442;&#65288;APT&#65289;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;APT&#26041;&#27861;&#21253;&#21547;&#35745;&#31639;&#38590;&#20197;&#22788;&#29702;&#30340;&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#23545;&#25968;&#30340;&#26399;&#26395;&#65292;&#21363;&#23884;&#22871;&#26399;&#26395;&#12290;&#23613;&#31649;&#21407;&#23376;APT&#36890;&#36807;&#31163;&#25955;&#21270;&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20998;&#26512;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;APT&#26041;&#27861;&#26469;&#20272;&#35745;&#30456;&#20851;&#30340;&#23884;&#22871;&#26399;&#26395;&#12290;&#36825;&#26377;&#21161;&#20110;&#24314;&#31435;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;&#30001;&#20110;&#25439;&#22833;&#20989;&#25968;&#21450;&#20854;&#26799;&#24230;&#30340;&#23884;&#22871;&#20272;&#35745;&#26159;&#26377;&#20559;&#30340;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;
&lt;/p&gt;
&lt;p&gt;
Sequential neural posterior estimation (SNPE) techniques have been recently proposed for dealing with simulation-based models with intractable likelihoods. They are devoted to learning the posterior from adaptively proposed simulations using neural network-based conditional density estimators. As a SNPE technique, the automatic posterior transformation (APT) method proposed by Greenberg et al. (2019) performs notably and scales to high dimensional data. However, the APT method bears the computation of an expectation of the logarithm of an intractable normalizing constant, i.e., a nested expectation. Although atomic APT was proposed to solve this by discretizing the normalizing constant, it remains challenging to analyze the convergence of learning. In this paper, we propose a nested APT method to estimate the involved nested expectation instead. This facilitates establishing the convergence analysis. Since the nested estimators for the loss function and its gradient are biased, we make
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#20851;&#20110;&#32593;&#32476;&#30340;&#37325;&#35201;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27963;&#21160;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#22788;&#29702;&#22823;&#37327;&#26410;&#30693;&#21442;&#25968;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#22120;&#21644;&#21464;&#20998;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16775</link><description>&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#26410;&#30693;&#22823;&#35268;&#27169;&#34928;&#33853;&#12289;&#20449;&#36947;&#32479;&#35745;&#12289;&#22122;&#38899;&#26041;&#24046;&#21644;&#27963;&#21160;&#27010;&#29575;&#30340;&#26080;&#32447;&#21333;&#20803;&#32593;&#32476;&#20013;&#30340;&#22823;&#35268;&#27169;&#36830;&#25509;&#27963;&#21160;&#26816;&#27979;: &#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Activity Detection for Massive Connectivity in Cell-free Networks with Unknown Large-scale Fading, Channel Statistics, Noise Variance, and Activity Probability: A Bayesian Approach. (arXiv:2401.16775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#20851;&#20110;&#32593;&#32476;&#30340;&#37325;&#35201;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27963;&#21160;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#22788;&#29702;&#22823;&#37327;&#26410;&#30693;&#21442;&#25968;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#22823;&#21518;&#39564;&#20272;&#35745;&#22120;&#21644;&#21464;&#20998;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#21160;&#26816;&#27979;&#26159;&#19979;&#19968;&#20195;&#20813;&#25480;&#26435;&#22810;&#22336;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;&#29616;&#26377;&#30340;&#31639;&#27861;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#38656;&#35201;&#31934;&#30830;&#30340;&#32593;&#32476;&#20449;&#24687;&#65292;&#22914;&#22823;&#35268;&#27169;&#34928;&#33853;&#31995;&#25968;&#12289;&#23567;&#35268;&#27169;&#34928;&#33853;&#20449;&#36947;&#32479;&#35745;&#12289;&#35775;&#38382;&#28857;&#30340;&#22122;&#38899;&#26041;&#24046;&#21644;&#29992;&#25143;&#27963;&#21160;&#27010;&#29575;&#12290;&#33719;&#21462;&#36825;&#20123;&#20449;&#24687;&#23558;&#20250;&#32791;&#36153;&#22823;&#37327;&#24320;&#38144;&#65292;&#24182;&#19988;&#20854;&#20272;&#35745;&#20540;&#21487;&#33021;&#19981;&#20934;&#30830;&#12290;&#22312;&#26080;&#32447;&#21333;&#20803;&#32593;&#32476;&#20013;&#65292;&#36825;&#20010;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#23384;&#22312;&#35768;&#22810;&#36825;&#20123;&#21442;&#25968;&#38656;&#35201;&#33719;&#21462;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22312;&#27809;&#26377;&#19978;&#36848;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#30340;&#27963;&#21160;&#26816;&#27979;&#38382;&#39064;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20040;&#22810;&#26410;&#30693;&#21442;&#25968;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20854;&#20013;&#26410;&#30693;&#21464;&#37327;&#36171;&#20104;&#20102;&#20808;&#39564;&#20998;&#24067;&#65292;&#26377;&#25928;&#22320;&#36215;&#21040;&#20102;&#27491;&#21017;&#21270;&#30340;&#20316;&#29992;&#12290;&#32467;&#21512;&#20284;&#28982;&#20989;&#25968;&#65292;&#20351;&#29992;&#20102;&#26368;&#22823;&#21518;&#39564;&#65288;MAP&#65289;&#20272;&#35745;&#22120;&#21644;&#21464;&#20998;
&lt;/p&gt;
&lt;p&gt;
Activity detection is an important task in the next generation grant-free multiple access. While there are a number of existing algorithms designed for this purpose, they mostly require precise information about the network, such as large-scale fading coefficients, small-scale fading channel statistics, noise variance at the access points, and user activity probability. Acquiring these information would take a significant overhead and their estimated values might not be accurate. This problem is even more severe in cell-free networks as there are many of these parameters to be acquired. Therefore, this paper sets out to investigate the activity detection problem without the above-mentioned information. In order to handle so many unknown parameters, this paper employs the Bayesian approach, where the unknown variables are endowed with prior distributions which effectively act as regularizations. Together with the likelihood function, a maximum a posteriori (MAP) estimator and a variatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#37096;&#22870;&#21169;&#30340;&#37492;&#21035;&#22120;&#30340;&#36719;Q&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#23569;&#37327;&#19987;&#23478;&#25968;&#25454;&#21644;&#37319;&#26679;&#25968;&#25454;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#36890;&#36807;&#28155;&#21152;&#22522;&#20110;&#23545;&#25239;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#31639;&#27861;&#26356;&#21152;&#31283;&#20581;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.16772</link><description>&lt;p&gt;
&#22522;&#20110;&#22806;&#37096;&#22870;&#21169;&#30340;&#37492;&#21035;&#22120;&#30340;&#36719;Q&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator. (arXiv:2401.16772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#37096;&#22870;&#21169;&#30340;&#37492;&#21035;&#22120;&#30340;&#36719;Q&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#23569;&#37327;&#19987;&#23478;&#25968;&#25454;&#21644;&#37319;&#26679;&#25968;&#25454;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#36890;&#36807;&#28155;&#21152;&#22522;&#20110;&#23545;&#25239;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#31639;&#27861;&#26356;&#21152;&#31283;&#20581;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38590;&#20197;&#35774;&#35745;&#22870;&#21169;&#25110;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#65292;&#27169;&#20223;&#23398;&#20064;&#24120;&#24120;&#19982;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20351;&#29992;&#65292;&#20294;&#22312;&#23569;&#37327;&#19987;&#23478;&#25968;&#25454;&#21644;&#37319;&#26679;&#25968;&#25454;&#20013;&#24456;&#38590;&#22312;&#26410;&#30693;&#29366;&#24577;&#20013;&#33391;&#22909;&#22320;&#36827;&#34892;&#27169;&#20223;&#12290;&#34892;&#20026;&#20811;&#38534;&#31561;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19981;&#38656;&#35201;&#37319;&#26679;&#25968;&#25454;&#65292;&#20294;&#36890;&#24120;&#20250;&#21463;&#21040;&#20998;&#24067;&#20559;&#31227;&#30340;&#22256;&#25200;&#12290;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22914;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;GAIL&#65289;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#19987;&#23478;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36719;Q&#27169;&#20223;&#23398;&#20064;&#65288;SQIL&#65289;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#34892;&#20026;&#20811;&#38534;&#21644;&#24120;&#25968;&#22870;&#21169;&#30340;&#36719;Q&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#34920;&#26126;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#12290;&#20026;&#20102;&#20351;&#35813;&#31639;&#27861;&#23545;&#20998;&#24067;&#20559;&#31227;&#26356;&#21152;&#31283;&#20581;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#21644;&#26356;&#31283;&#20581;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35813;&#26041;&#27861;&#20013;&#28155;&#21152;&#22522;&#20110;&#23545;&#25239;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning is often used in addition to reinforcement learning in environments where reward design is difficult or where the reward is sparse, but it is difficult to be able to imitate well in unknown states from a small amount of expert data and sampling data. Supervised learning methods such as Behavioral Cloning do not require sampling data, but usually suffer from distribution shift. The methods based on reinforcement learning, such as inverse reinforcement learning and Generative Adversarial imitation learning (GAIL), can learn from only a few expert data. However, they often need to interact with the environment. Soft Q imitation learning (SQIL) addressed the problems, and it was shown that it could learn efficiently by combining Behavioral Cloning and soft Q-learning with constant rewards. In order to make this algorithm more robust to distribution shift, we propose more efficient and robust algorithm by adding to this method a reward function based on adversarial invers
&lt;/p&gt;</description></item><item><title>MolPLA&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#20998;&#23376;&#26680;&#24515;&#12289;R-&#22522;&#21644;&#36830;&#25509;&#28857;&#30340;&#20998;&#23376;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#25513;&#30721;&#22270;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#28145;&#20837;&#29702;&#35299;&#20998;&#23376;&#30340;&#21487;&#20998;&#35299;&#37096;&#20998;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#24110;&#21161;&#21270;&#23398;&#23478;&#22312;&#20808;&#23548;&#20248;&#21270;&#22330;&#26223;&#20013;&#25214;&#21040;&#21487;&#26367;&#20195;&#30340;R-&#22522;&#12290;</title><link>http://arxiv.org/abs/2401.16771</link><description>&lt;p&gt;
MolPLA: &#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#26680;&#24515;&#12289;R-&#22522;&#21644;&#36830;&#25509;&#28857;&#30340;&#20998;&#23376;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MolPLA: A Molecular Pretraining Framework for Learning Cores, R-Groups and their Linker Joints. (arXiv:2401.16771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16771
&lt;/p&gt;
&lt;p&gt;
MolPLA&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#20998;&#23376;&#26680;&#24515;&#12289;R-&#22522;&#21644;&#36830;&#25509;&#28857;&#30340;&#20998;&#23376;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#25513;&#30721;&#22270;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#28145;&#20837;&#29702;&#35299;&#20998;&#23376;&#30340;&#21487;&#20998;&#35299;&#37096;&#20998;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#24110;&#21161;&#21270;&#23398;&#23478;&#22312;&#20808;&#23548;&#20248;&#21270;&#22330;&#26223;&#20013;&#25214;&#21040;&#21487;&#26367;&#20195;&#30340;R-&#22522;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#26680;&#24515;&#32467;&#26500;&#21644;R-&#22522;&#26159;&#33647;&#29289;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#27010;&#24565;&#12290;&#23558;&#36825;&#20123;&#27010;&#24565;&#19982;&#20256;&#32479;&#30340;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20419;&#36827;&#23545;&#20998;&#23376;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MolPLA&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25513;&#30721;&#22270;&#23545;&#27604;&#23398;&#20064;&#26469;&#29702;&#35299;&#20998;&#23376;&#20013;&#30340;&#21487;&#20998;&#35299;&#37096;&#20998;&#65292;&#20197;&#25581;&#31034;&#20854;&#26680;&#24515;&#32467;&#26500;&#21644;&#22806;&#22260;R-&#22522;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21046;&#23450;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#26694;&#26550;&#65292;&#20351;MolPLA&#33021;&#22815;&#24110;&#21161;&#21270;&#23398;&#23478;&#22312;&#20808;&#23548;&#20248;&#21270;&#22330;&#26223;&#20013;&#25214;&#21040;&#21487;&#26367;&#20195;&#30340;R-&#22522;&#12290;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MolPLA&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;MolPLA&#33021;&#22815;&#21306;&#20998;&#26680;&#24515;&#21644;R-&#22522;&#20122;&#32467;&#26500;&#65292;&#35782;&#21035;&#20998;&#23376;&#20013;&#30340;&#21487;&#20998;&#35299;&#21306;&#22495;&#65292;&#24182;&#36890;&#36807;&#29702;&#24615;&#24314;&#35758;&#19981;&#21516;&#26597;&#35810;&#26680;&#24515;&#28201;&#24230;&#19979;&#30340;R-&#22522;&#26367;&#25442;&#65292;&#20026;&#20808;&#23548;&#20248;&#21270;&#22330;&#26223;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular core structures and R-groups are essential concepts in drug development. Integration of these concepts with conventional graph pre-training approaches can promote deeper understanding in molecules. We propose MolPLA, a novel pre-training framework that employs masked graph contrastive learning in understanding the underlying decomposable parts inmolecules that implicate their core structure and peripheral R-groups. Furthermore, we formulate an additional framework that grants MolPLA the ability to help chemists find replaceable R-groups in lead optimization scenarios. Experimental results on molecular property prediction show that MolPLA exhibits predictability comparable to current state-of-the-art models. Qualitative analysis implicate that MolPLA is capable of distinguishing core and R-group sub-structures, identifying decomposable regions in molecules and contributing to lead optimization scenarios by rationally suggesting R-group replacements given various query core tem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#26816;&#27979;&#21644;&#24674;&#22797;&#26694;&#26550;&#65288;CFDR&#65289;&#65292;&#36890;&#36807;&#23558;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#27969;&#31243;&#20013;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#24341;&#25806;&#65292;&#22312;&#21482;&#26377;&#19968;&#20010;&#25209;&#27425;&#30340;&#27979;&#35797;&#25968;&#25454;&#21644;&#23569;&#37327;&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24182;&#24555;&#36895;&#24674;&#22797;&#22810;&#31181;&#31867;&#22411;&#30340;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.16766</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#26816;&#27979;&#21644;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Detection and Recovery Against Deep Neural Network Fault Injection Attacks Based on Contrastive Learning. (arXiv:2401.16766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#26816;&#27979;&#21644;&#24674;&#22797;&#26694;&#26550;&#65288;CFDR&#65289;&#65292;&#36890;&#36807;&#23558;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#27969;&#31243;&#20013;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#24341;&#25806;&#65292;&#22312;&#21482;&#26377;&#19968;&#20010;&#25209;&#27425;&#30340;&#27979;&#35797;&#25968;&#25454;&#21644;&#23569;&#37327;&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24182;&#24555;&#36895;&#24674;&#22797;&#22810;&#31181;&#31867;&#22411;&#30340;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#25191;&#34892;&#35774;&#22791;&#19978;&#20316;&#20026;&#25512;&#29702;&#24341;&#25806;&#23454;&#26045;&#26102;&#23481;&#26131;&#21463;&#21040;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#25805;&#32437;&#27169;&#22411;&#21442;&#25968;&#20197;&#30772;&#22351;&#25512;&#29702;&#25191;&#34892;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23558;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#27969;&#31243;&#20013;&#65292;&#20197;&#23454;&#29616;&#20855;&#26377;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#24341;&#25806;&#65292;&#20197;&#24212;&#23545;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#26816;&#27979;&#21644;&#24674;&#22797;&#65288;CFDR&#65289;&#26694;&#26550;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;i&#65289;&#20165;&#38656;&#19968;&#20010;&#25209;&#27425;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#26816;&#27979;&#65292;&#65288;ii&#65289;&#21363;&#20351;&#20165;&#26377;&#23569;&#37327;&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#24555;&#36895;&#30340;&#24674;&#22797;&#25928;&#26524;&#12290;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#23545;&#22810;&#31181;&#31867;&#22411;&#30340;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;CFDR&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#26816;&#27979;&#21644;&#24674;&#22797;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Network (DNN) models when implemented on executing devices as the inference engines are susceptible to Fault Injection Attacks (FIAs) that manipulate model parameters to disrupt inference execution with disastrous performance. This work introduces Contrastive Learning (CL) of visual representations i.e., a self-supervised learning approach into the deep learning training and inference pipeline to implement DNN inference engines with self-resilience under FIAs. Our proposed CL based FIA Detection and Recovery (CFDR) framework features (i) real-time detection with only a single batch of testing data and (ii) fast recovery effective even with only a small amount of unlabeled testing data. Evaluated with the CIFAR-10 dataset on multiple types of FIAs, our CFDR shows promising detection and recovery effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#24863;&#30693;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27493;&#21521;&#21069;&#25628;&#32034;&#21644;&#22238;&#28335;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#26799;&#24230;&#19979;&#38477;&#20013;&#20986;&#29616;&#30340;&#26354;&#32447;&#34892;&#36827;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16760</link><description>&lt;p&gt;
&#19968;&#27493;&#21521;&#21069;&#21644;&#22238;&#28335;&#65306;&#20811;&#26381;&#25439;&#22833;&#24863;&#30693;&#37327;&#21270;&#35757;&#32451;&#20013;&#30340;&#26354;&#32447;&#34892;&#36827;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
One-Step Forward and Backtrack: Overcoming Zig-Zagging in Loss-Aware Quantization Training. (arXiv:2401.16760v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#24863;&#30693;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27493;&#21521;&#21069;&#25628;&#32034;&#21644;&#22238;&#28335;&#30340;&#26041;&#24335;&#35299;&#20915;&#20102;&#26799;&#24230;&#19979;&#38477;&#20013;&#20986;&#29616;&#30340;&#26354;&#32447;&#34892;&#36827;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#37327;&#21270;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#20256;&#32479;&#30340;&#25439;&#22833;&#24863;&#30693;&#37327;&#21270;&#26041;&#27861;&#24120;&#24120;&#20351;&#29992;&#37327;&#21270;&#26799;&#24230;&#26469;&#26367;&#20195;&#20840;&#31934;&#24230;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#26799;&#24230;&#35823;&#24046;&#20250;&#23548;&#33268;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#36807;&#31243;&#20013;&#20986;&#29616;&#24847;&#24819;&#19981;&#21040;&#30340;&#26354;&#32447;&#34892;&#36827;&#38382;&#39064;&#65292;&#20854;&#20013;&#26799;&#24230;&#26041;&#21521;&#36805;&#36895;&#25391;&#33633;&#25110;&#26354;&#32447;&#34892;&#36827;&#65292;&#36825;&#20010;&#38382;&#39064;&#20005;&#37325;&#20943;&#24930;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#27493;&#21521;&#21069;&#21644;&#22238;&#28335;&#30340;&#25439;&#22833;&#24863;&#30693;&#37327;&#21270;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#20934;&#30830;&#31283;&#23450;&#30340;&#26799;&#24230;&#26041;&#21521;&#26469;&#20811;&#26381;&#27492;&#38382;&#39064;&#12290;&#22312;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#27493;&#21521;&#21069;&#25628;&#32034;&#26469;&#23547;&#25214;&#19979;&#19968;&#27493;&#30340;&#35797;&#39564;&#26799;&#24230;&#65292;&#35813;&#26799;&#24230;&#34987;&#29992;&#26469;&#35843;&#25972;&#24403;&#21069;&#27493;&#39588;&#30340;&#26799;&#24230;&#65292;&#26397;&#30528;&#24555;&#36895;&#25910;&#25947;&#30340;&#26041;&#21521;&#35843;&#25972;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#22238;&#28335;&#24403;&#21069;&#27493;&#39588;&#65292;&#26356;&#26032;&#20840;&#31934;&#24230;&#21644;&#37327;&#21270;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight quantization is an effective technique to compress deep neural networks for their deployment on edge devices with limited resources. Traditional loss-aware quantization methods commonly use the quantized gradient to replace the full-precision gradient. However, we discover that the gradient error will lead to an unexpected zig-zagging-like issue in the gradient descent learning procedures, where the gradient directions rapidly oscillate or zig-zag, and such issue seriously slows down the model convergence. Accordingly, this paper proposes a one-step forward and backtrack way for loss-aware quantization to get more accurate and stable gradient direction to defy this issue. During the gradient descent learning, a one-step forward search is designed to find the trial gradient of the next-step, which is adopted to adjust the gradient of current step towards the direction of fast convergence. After that, we backtrack the current step to update the full-precision and quantized weights
&lt;/p&gt;</description></item><item><title>SwapNet&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;AI&#35774;&#22791;DNN&#22359;&#20132;&#25442;&#20013;&#38388;&#20214;&#65292;&#22312;&#36229;&#20986;&#20869;&#23384;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20998;&#35299;DNN&#20026;&#22359;&#24182;&#36827;&#34892;&#20132;&#25442;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;DNN&#30340;&#39640;&#25928;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2401.16757</link><description>&lt;p&gt;
SwapNet: &#36229;&#20986;&#20869;&#23384;&#39044;&#31639;&#30340;&#36793;&#32536;AI&#35774;&#22791;&#19978;&#36827;&#34892;DNN&#25512;&#29702;&#30340;&#39640;&#25928;&#20132;&#25442;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
SwapNet: Efficient Swapping for DNN Inference on Edge AI Devices Beyond the Memory Budget. (arXiv:2401.16757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16757
&lt;/p&gt;
&lt;p&gt;
SwapNet&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;AI&#35774;&#22791;DNN&#22359;&#20132;&#25442;&#20013;&#38388;&#20214;&#65292;&#22312;&#36229;&#20986;&#20869;&#23384;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20998;&#35299;DNN&#20026;&#22359;&#24182;&#36827;&#34892;&#20132;&#25442;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;DNN&#30340;&#39640;&#25928;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35774;&#22791;&#19978;&#25191;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21487;&#20197;&#23454;&#29616;&#21508;&#31181;&#33258;&#20027;&#31227;&#21160;&#35745;&#31639;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;AI&#35774;&#22791;&#30340;&#20869;&#23384;&#39044;&#31639;&#38480;&#21046;&#20102;&#36825;&#20123;&#24212;&#29992;&#20013;&#20801;&#35768;&#30340;DNN&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#27169;&#22411;&#21387;&#32553;&#25110;&#20113;&#21368;&#36733;&#65292;&#20943;&#23569;&#20102;DNN&#25512;&#29702;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#20294;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#25110;&#33258;&#20027;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#23558;DNN&#20998;&#35299;&#25104;&#22359;&#65292;&#24182;&#25353;&#39034;&#24207;&#20114;&#30456;&#20132;&#25442;&#65292;&#20197;&#20415;&#22312;&#36739;&#23567;&#30340;&#20869;&#23384;&#39044;&#31639;&#19979;&#25191;&#34892;&#22823;&#22411;DNN&#12290;&#28982;&#32780;&#65292;&#22312;&#36793;&#32536;AI&#35774;&#22791;&#19978;&#36827;&#34892;&#31616;&#21333;&#20132;&#25442;&#20250;&#24341;&#36215;&#26174;&#33879;&#30340;&#24310;&#36831;&#65292;&#22240;&#20026;&#22312;&#36793;&#32536;AI&#35774;&#22791;&#30340;DNN&#24320;&#21457;&#29983;&#24577;&#31995;&#32479;&#20013;&#23384;&#22312;&#20887;&#20313;&#30340;&#20869;&#23384;&#25805;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SwapNet&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;AI&#35774;&#22791;DNN&#22359;&#20132;&#25442;&#20013;&#38388;&#20214;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#28040;&#38500;&#20102;&#22359;&#20132;&#25442;&#36807;&#31243;&#20013;&#19981;&#24517;&#35201;&#30340;&#20869;&#23384;&#25805;&#20316;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;GPU&#21518;&#31471;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Executing deep neural networks (DNNs) on edge artificial intelligence (AI) devices enables various autonomous mobile computing applications. However, the memory budget of edge AI devices restricts the number and complexity of DNNs allowed in such applications. Existing solutions, such as model compression or cloud offloading, reduce the memory footprint of DNN inference at the cost of decreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN into blocks and swap them in and out in order, such that large DNNs can execute within a small memory budget. Nevertheless, naive swapping on edge AI devices induces significant delays due to the redundant memory operations in the DNN development ecosystem for edge AI devices. To this end, we develop SwapNet, an efficient DNN block swapping middleware for edge AI devices. We systematically eliminate the unnecessary memory operations during block swapping while retaining compatible with the deep learning frameworks, GPU backends,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffRI)&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#23398;&#20064;&#25512;&#26029;&#32452;&#20214;&#20043;&#38388;&#36830;&#25509;&#23384;&#22312;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#22320;&#38754;&#30495;&#23454;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16755</link><description>&lt;p&gt;
&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion model for relational inference. (arXiv:2401.16755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16755
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffRI)&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#23398;&#20064;&#25512;&#26029;&#32452;&#20214;&#20043;&#38388;&#36830;&#25509;&#23384;&#22312;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#22320;&#38754;&#30495;&#23454;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#30340;&#21160;&#24577;&#34892;&#20026;&#65292;&#21253;&#25324;&#22823;&#33041;&#27963;&#21160;&#12289;&#37329;&#34701;&#20215;&#26684;&#27874;&#21160;&#21644;&#29289;&#29702;&#38598;&#20307;&#29616;&#35937;&#65292;&#19982;&#31995;&#32479;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30456;&#20851;&#12290;&#21033;&#29992;&#21487;&#35266;&#27979;&#30340;&#21160;&#24577;&#26469;&#21457;&#29616;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#20851;&#31995;&#34987;&#31216;&#20026;&#20851;&#31995;&#25512;&#29702;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffRI)&#65292;&#23427;&#20511;&#37492;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#25554;&#20540;&#26041;&#27861;&#12290;DiffRI&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#23398;&#20064;&#25512;&#26029;&#32452;&#20214;&#20043;&#38388;&#36830;&#25509;&#23384;&#22312;&#30340;&#27010;&#29575;&#12290;&#23545;&#20110;&#27169;&#25311;&#21644;&#20934;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DiffRI&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#22320;&#38754;&#30495;&#23454;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamical behaviors of complex interacting systems, including brain activities, financial price movements, and physical collective phenomena, are associated with underlying interactions between the system's components. The issue of uncovering interaction relations in such systems using observable dynamics is called relational inference. In this study, we propose a Diffusion model for Relational Inference (DiffRI), inspired by a self-supervised method for probabilistic time series imputation. DiffRI learns to infer the probability of the presence of connections between components through conditional diffusion modeling. Experiments on both simulated and quasi-real datasets show that DiffRI is highly competent compared with other state-of-the-art models in discovering ground truth interactions in an unsupervised manner. Our code will be made public soon.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#32416;&#27491;&#20154;&#31867;&#38169;&#35823;&#26041;&#38754;&#36215;&#21040;&#20102;&#31215;&#26497;&#20316;&#29992;&#65292;&#20294;&#27492;&#20030;&#20063;&#28508;&#22312;&#23548;&#33268;&#24515;&#29702;&#25104;&#26412;&#65292;&#24182;&#24433;&#21709;&#20154;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#30740;&#31350;&#32593;&#29699;&#27604;&#36187;&#20013;&#30340;Hawk-Eye&#23457;&#26597;&#31995;&#32479;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#20837;AI&#30417;&#30563;&#21518;&#65292;&#35009;&#21028;&#21592;&#30340;&#38169;&#35823;&#29575;&#19979;&#38477;&#65292;&#24515;&#29702;&#25104;&#26412;&#23548;&#33268;&#20182;&#20204;&#26356;&#20542;&#21521;&#20110;&#23558;&#29699;&#21028;&#20026;&#36827;&#30028;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#31867;&#22411;&#38169;&#21028;&#30340;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2401.16754</link><description>&lt;p&gt;
AI&#30417;&#30563;&#21644;&#20154;&#31867;&#38169;&#35823;&#65306;&#26469;&#33258;&#20013;&#24515;&#27861;&#24237;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
AI Oversight and Human Mistakes: Evidence from Centre Court. (arXiv:2401.16754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16754
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#32416;&#27491;&#20154;&#31867;&#38169;&#35823;&#26041;&#38754;&#36215;&#21040;&#20102;&#31215;&#26497;&#20316;&#29992;&#65292;&#20294;&#27492;&#20030;&#20063;&#28508;&#22312;&#23548;&#33268;&#24515;&#29702;&#25104;&#26412;&#65292;&#24182;&#24433;&#21709;&#20154;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#30740;&#31350;&#32593;&#29699;&#27604;&#36187;&#20013;&#30340;Hawk-Eye&#23457;&#26597;&#31995;&#32479;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#20837;AI&#30417;&#30563;&#21518;&#65292;&#35009;&#21028;&#21592;&#30340;&#38169;&#35823;&#29575;&#19979;&#38477;&#65292;&#24515;&#29702;&#25104;&#26412;&#23548;&#33268;&#20182;&#20204;&#26356;&#20542;&#21521;&#20110;&#23558;&#29699;&#21028;&#20026;&#36827;&#30028;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#31867;&#22411;&#38169;&#21028;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19981;&#26029;&#25552;&#21319;&#30340;&#39537;&#21160;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#24050;&#32463;&#24320;&#22987;&#22312;&#35768;&#22810;&#22330;&#21512;&#29992;&#20110;&#32416;&#27491;&#20154;&#31867;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#23454;&#22320;&#35777;&#25454;&#65292;&#35777;&#26126;&#36825;&#31181;AI&#30417;&#30563;&#20250;&#20135;&#29983;&#24515;&#29702;&#25104;&#26412;&#65292;&#24433;&#21709;&#20154;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;AI&#30417;&#30563;&#21457;&#29983;&#30340;&#26368;&#39640;&#21487;&#35265;&#24615;&#22330;&#26223;&#20043;&#19968;&#65306;&#39030;&#32423;&#32593;&#29699;&#27604;&#36187;&#20013;&#35009;&#21028;&#30340;Hawk-Eye&#23457;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24341;&#20837;Hawk-Eye&#23457;&#26597;&#21518;&#65292;&#35009;&#21028;&#30340;&#25972;&#20307;&#38169;&#35823;&#29575;&#38477;&#20302;&#65292;&#31526;&#21512;&#24515;&#29702;&#25104;&#26412;&#34987;AI&#21542;&#23450;&#30340;&#21512;&#29702;&#24573;&#35270;&#29616;&#35937;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#35009;&#21028;&#22686;&#21152;&#20102;&#23545;&#29699;&#20837;&#20869;&#30340;&#21028;&#23450;&#29575;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#20174;II&#31867;&#38169;&#35823;&#65288;&#23558;&#29699;&#21028;&#20026;&#20986;&#30028;&#65292;&#23454;&#38469;&#19978;&#26159;&#36827;&#30028;&#65289;&#21040;I&#31867;&#38169;&#35823;&#65288;&#23558;&#29699;&#21028;&#20026;&#36827;&#30028;&#65292;&#23454;&#38469;&#19978;&#26159;&#20986;&#30028;&#65289;&#30340;&#36716;&#21464;&#12290;&#36890;&#36807;&#23545;&#29702;&#24615;&#19981;&#27880;&#24847;&#30340;&#35009;&#21028;&#27169;&#22411;&#36827;&#34892;&#24515;&#29702;&#25104;&#26412;&#30340;&#32467;&#26500;&#20272;&#35745;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;AI&#21542;&#23450;&#30340;&#24515;&#29702;&#25104;&#26412;&#65292;&#35009;&#21028;&#21592;&#38477;&#20302;&#20102;&#38169;&#35823;&#21028;&#23450;&#30340;&#39118;&#38505;&#24182;&#25552;&#39640;&#20102;&#29699;&#20837;&#20869;&#30340;&#21028;&#23450;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Powered by the increasing predictive capabilities of machine learning algorithms, artificial intelligence (AI) systems have begun to be used to overrule human mistakes in many settings. We provide the first field evidence this AI oversight carries psychological costs that can impact human decision-making. We investigate one of the highest visibility settings in which AI oversight has occurred: the Hawk-Eye review of umpires in top tennis tournaments. We find that umpires lowered their overall mistake rate after the introduction of Hawk-Eye review, in line with rational inattention given psychological costs of being overruled by AI. We also find that umpires increased the rate at which they called balls in, which produced a shift from making Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out). We structurally estimate the psychological costs of being overruled by AI using a model of rational inattentive umpires, and our results suggest that because 
&lt;/p&gt;</description></item><item><title>DecNefGAN&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38381;&#29615;fMRI&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#29983;&#25104;&#23545;&#25239;&#31995;&#32479;&#21644;&#31070;&#32463;&#24378;&#21270;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#31867;&#22914;&#20309;&#23545;&#25239;&#21644;&#24212;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.16742</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38381;&#29615;fMRI&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Generative AI-based closed-loop fMRI system. (arXiv:2401.16742v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16742
&lt;/p&gt;
&lt;p&gt;
DecNefGAN&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38381;&#29615;fMRI&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#29983;&#25104;&#23545;&#25239;&#31995;&#32479;&#21644;&#31070;&#32463;&#24378;&#21270;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#31867;&#22914;&#20309;&#23545;&#25239;&#21644;&#24212;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#29616;&#22312;&#22312;&#31038;&#20250;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#22823;&#30340;&#29992;&#22788;&#65292;&#20294;&#26159;&#23384;&#22312;&#28508;&#22312;&#30340;&#28389;&#29992;&#39118;&#38505;&#65292;&#20363;&#22914;&#65292;&#26080;&#24847;&#35782;&#22320;&#24433;&#21709;&#35748;&#30693;&#36807;&#31243;&#25110;&#20915;&#31574;&#12290;&#23613;&#31649;&#36825;&#22312;&#35748;&#30693;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#23433;&#20840;&#38382;&#39064;&#65292;&#20294;&#33267;&#20170;&#27809;&#26377;&#20851;&#20110;&#22312;&#20154;&#31867;&#36523;&#19978;&#23545;&#25239;&#24694;&#24847;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#30340;&#31070;&#32463;&#21644;&#35745;&#31639;&#26426;&#26426;&#21046;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DecNefGAN&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#29983;&#25104;&#23545;&#25239;&#31995;&#32479;&#21644;&#31070;&#32463;&#24378;&#21270;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DecNefGAN&#22312;&#19968;&#20010;&#38381;&#29615;&#31995;&#32479;&#20013;&#36830;&#25509;&#20102;&#20154;&#31867;&#21644;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#21019;&#24314;&#35825;&#21457;&#29305;&#23450;&#24515;&#29702;&#29366;&#24577;&#30340;&#21050;&#28608;&#65292;&#20174;&#32780;&#23545;&#31070;&#32463;&#27963;&#21160;&#26045;&#21152;&#22806;&#37096;&#25511;&#21046;&#12290;&#20154;&#31867;&#30340;&#30446;&#26631;&#30456;&#21453;&#65292;&#35201;&#31454;&#20105;&#24182;&#36798;&#21040;&#19968;&#20010;&#27491;&#20132;&#30340;&#24515;&#29702;&#29366;&#24577;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#26377;&#21161;&#20110;&#38416;&#26126;&#20154;&#33041;&#22914;&#20309;&#23545;&#25239;&#21644;&#24212;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
While generative AI is now widespread and useful in society, there are potential risks of misuse, e.g., unconsciously influencing cognitive processes or decision-making. Although this causes a security problem in the cognitive domain, there has been no research about neural and computational mechanisms counteracting the impact of malicious generative AI in humans. We propose DecNefGAN, a novel framework that combines a generative adversarial system and a neural reinforcement model. More specifically, DecNefGAN bridges human and generative AI in a closed-loop system, with the AI creating stimuli that induce specific mental states, thus exerting external control over neural activity. The objective of the human is the opposite, to compete and reach an orthogonal mental state. This framework can contribute to elucidating how the human brain responds to and counteracts the potential influence of generative AI.
&lt;/p&gt;</description></item><item><title>Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#20174;&#32780;&#20248;&#21270;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16736</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Engineering A Large Language Model From Scratch. (arXiv:2401.16736v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16736
&lt;/p&gt;
&lt;p&gt;
Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#65292;&#20174;&#32780;&#20248;&#21270;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#21019;&#26032;&#25216;&#26415;&#30340;&#24320;&#21457;&#21644;&#21457;&#24067;&#12290;Atinuke&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#29420;&#29305;&#30340;&#37197;&#32622;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#20248;&#21270;&#24615;&#33021;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#30340;&#23618;&#19982;&#27880;&#24847;&#26426;&#21046;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#24314;&#31435;&#26377;&#24847;&#20041;&#30340;&#20851;&#32852;&#12290;&#30001;&#20110;&#20854;&#25299;&#25169;&#32467;&#26500;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#37197;&#32622;&#65292;&#23427;&#21487;&#20197;&#25552;&#21462;&#29305;&#24449;&#24182;&#23398;&#20064;&#22797;&#26434;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#27169;&#20223;&#20154;&#31867;&#35821;&#35328;&#12290;Atinuke&#26159;&#27169;&#22359;&#21270;&#12289;&#21487;&#25193;&#23637;&#30340;&#65292;&#24182;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26080;&#32541;&#38598;&#25104;&#12290;softmax&#12289;&#23884;&#20837;&#21644;&#22810;&#22836;&#27880;&#24847;&#21147;&#31561;&#39640;&#32423;&#30697;&#38453;&#25805;&#20316;&#20351;&#24471;&#23545;&#25991;&#26412;&#12289;&#22768;&#38899;&#21644;&#35270;&#35273;&#20449;&#21495;&#30340;&#32454;&#33268;&#22788;&#29702;&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#23558;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#36719;&#20214;&#35774;&#35745;&#21407;&#21017;&#21644;&#25968;&#23398;&#26041;&#27861;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
The proliferation of deep learning in natural language processing (NLP) has led to the development and release of innovative technologies capable of understanding and generating human language with remarkable proficiency. Atinuke, a Transformer-based neural network, optimises performance across various language tasks by utilising a unique configuration. The architecture interweaves layers for processing sequential data with attention mechanisms to draw meaningful affinities between inputs and outputs. Due to the configuration of its topology and hyperparameter tuning, it can emulate human-like language by extracting features and learning complex mappings. Atinuke is modular, extensible, and integrates seamlessly with existing machine learning pipelines. Advanced matrix operations like softmax, embeddings, and multi-head attention enable nuanced handling of textual, acoustic, and visual signals. By unifying modern deep learning techniques with software design principles and mathematical
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#32447;&#24615;&#21305;&#37197;&#28388;&#27874;&#22120;&#65288;WLMF&#65289;&#33539;&#20363;&#26469;&#23454;&#29616;&#22797;&#26434;&#20540;CNN&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#20540;&#25968;&#25454;&#20013;&#21305;&#37197;&#28388;&#27874;&#30340;&#38590;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#24615;&#33021;&#12290;&#19982;&#26631;&#20934;&#32447;&#24615;&#23545;&#24212;&#29289;&#65288;SLMF&#65289;&#30456;&#27604;&#65292;WLMF&#22312;&#36755;&#20986;&#20449;&#22122;&#27604;&#26041;&#38754;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.16729</link><description>&lt;p&gt;
&#24191;&#20041;&#32447;&#24615;&#21305;&#37197;&#28388;&#27874;&#22120;&#65306;&#23454;&#29616;&#22797;&#26434;&#20540;CNN&#21487;&#35299;&#37322;&#24615;&#20043;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Widely Linear Matched Filter: A Lynchpin towards the Interpretability of Complex-valued CNNs. (arXiv:2401.16729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#32447;&#24615;&#21305;&#37197;&#28388;&#27874;&#22120;&#65288;WLMF&#65289;&#33539;&#20363;&#26469;&#23454;&#29616;&#22797;&#26434;&#20540;CNN&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#22797;&#26434;&#20540;&#25968;&#25454;&#20013;&#21305;&#37197;&#28388;&#27874;&#30340;&#38590;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#24615;&#33021;&#12290;&#19982;&#26631;&#20934;&#32447;&#24615;&#23545;&#24212;&#29289;&#65288;SLMF&#65289;&#30456;&#27604;&#65292;WLMF&#22312;&#36755;&#20986;&#20449;&#22122;&#27604;&#26041;&#38754;&#20855;&#26377;&#29702;&#35770;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#23454;&#20540;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#21487;&#35299;&#37322;&#24615;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36890;&#36807;&#21305;&#37197;&#28388;&#27874;&#22120;&#22312;&#25968;&#25454;&#20013;&#25214;&#21040;&#29305;&#24449;&#30340;&#20219;&#21153;&#19982;&#20854;&#30452;&#25509;&#21644;&#26377;&#29289;&#29702;&#21547;&#20041;&#30340;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#19968;&#33539;&#24335;&#24212;&#29992;&#20110;&#25581;&#31034;&#22797;&#26434;&#20540;CNNs&#21487;&#35299;&#37322;&#24615;&#36935;&#21040;&#20102;&#19968;&#20010;&#24040;&#22823;&#30340;&#38556;&#30861;&#65306;&#24191;&#20041;&#38750;&#24490;&#29615;&#22797;&#26434;&#20540;&#25968;&#25454;&#30340;&#21305;&#37197;&#28388;&#27874;&#22120;&#25193;&#23637;&#65292;&#21363;&#24191;&#20041;&#32447;&#24615;&#21305;&#37197;&#28388;&#27874;&#22120;&#65288;WLMF&#65289;&#65292;&#22312;&#25991;&#29486;&#20013;&#20165;&#20165;&#26159;&#38544;&#21547;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#22797;&#26434;&#20540;CNNs&#25805;&#20316;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24191;&#20041;WLMF&#33539;&#20363;&#65292;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#24182;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20026;&#20102;&#20445;&#35777;&#20005;&#35880;&#24615;&#65292;&#25105;&#20204;&#30340;WLMF&#35299;&#20915;&#26041;&#26696;&#19981;&#23545;&#22122;&#22768;&#30340;&#27010;&#29575;&#23494;&#24230;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;WLMF&#22312;&#36755;&#20986;&#20449;&#22122;&#27604;&#26041;&#38754;&#30456;&#23545;&#20110;&#20854;&#26631;&#20934;&#20005;&#26684;&#32447;&#24615;&#23545;&#24212;&#29289;&#65288;SLMF&#65289;&#30340;&#29702;&#35770;&#20248;&#21183;&#34987;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent study on the interpretability of real-valued convolutional neural networks (CNNs) \cite{Stankovic_Mandic_2023CNN} has revealed a direct and physically meaningful link with the task of finding features in data through matched filters. However, applying this paradigm to illuminate the interpretability of complex-valued CNNs meets a formidable obstacle: the extension of matched filtering to a general class of noncircular complex-valued data, referred to here as the widely linear matched filter (WLMF), has been only implicit in the literature. To this end, to establish the interpretability of the operation of complex-valued CNNs, we introduce a general WLMF paradigm, provide its solution and undertake analysis of its performance. For rigor, our WLMF solution is derived without imposing any assumption on the probability density of noise. The theoretical advantages of the WLMF over its standard strictly linear counterpart (SLMF) are provided in terms of their output signal-to-noise-
&lt;/p&gt;</description></item><item><title>SmartFRZ&#26159;&#19968;&#31181;&#36890;&#29992;&#32780;&#26234;&#33021;&#30340;&#23618;&#20923;&#32467;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#25191;&#34892;&#8220;&#29616;&#22330;&#8221;&#23618;&#20923;&#32467;&#65292;&#22312;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#30340;&#21516;&#26102;&#20860;&#39038;&#36890;&#29992;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16720</link><description>&lt;p&gt;
SmartFRZ:&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#23618;&#20923;&#32467;&#30340;&#39640;&#25928;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SmartFRZ: An Efficient Training Framework using Attention-Based Layer Freezing. (arXiv:2401.16720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16720
&lt;/p&gt;
&lt;p&gt;
SmartFRZ&#26159;&#19968;&#31181;&#36890;&#29992;&#32780;&#26234;&#33021;&#30340;&#23618;&#20923;&#32467;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#25191;&#34892;&#8220;&#29616;&#22330;&#8221;&#23618;&#20923;&#32467;&#65292;&#22312;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#30340;&#21516;&#26102;&#20860;&#39038;&#36890;&#29992;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#27491;&#19981;&#26029;&#22686;&#21152;&#65292;&#27169;&#22411;&#35757;&#32451;&#23545;&#20110;&#25552;&#20379;&#39640;&#36136;&#37327;&#26381;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#26082;&#32791;&#26102;&#21448;&#32791;&#33021;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#24433;&#21709;&#21040;&#29992;&#25143;&#23545;&#24212;&#29992;&#25928;&#29575;&#30340;&#38656;&#27714;&#12290;&#23618;&#20923;&#32467;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#25216;&#26415;&#34987;&#25552;&#20986;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#23618;&#20923;&#32467;&#26041;&#27861;&#23637;&#31034;&#20102;&#20943;&#23569;&#27169;&#22411;&#35757;&#32451;&#25104;&#26412;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#32570;&#28857;&#65292;&#22914;&#32570;&#20047;&#36890;&#29992;&#24615;&#21644;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#20363;&#22914;&#65292;&#29616;&#26377;&#30340;&#23618;&#20923;&#32467;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22312;&#35757;&#32451;&#20043;&#21069;&#25163;&#21160;&#23450;&#20041;&#20923;&#32467;&#37197;&#32622;&#65292;&#36825;&#19981;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#32593;&#32476;&#65292;&#35201;&#20040;&#20351;&#29992;&#21551;&#21457;&#24335;&#30340;&#20923;&#32467;&#26631;&#20934;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#24456;&#38590;&#20445;&#35777;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#32570;&#20047;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#25191;&#34892;&#8220;&#29616;&#22330;&#8221;&#23618;&#20923;&#32467;&#30340;&#36890;&#29992;&#32780;&#26234;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a proliferation of artificial intelligence applications, where model training is key to promising high-quality services for these applications. However, the model training process is both time-intensive and energy-intensive, inevitably affecting the user's demand for application efficiency. Layer freezing, an efficient model training technique, has been proposed to improve training efficiency. Although existing layer freezing methods demonstrate the great potential to reduce model training costs, they still remain shortcomings such as lacking generalizability and compromised accuracy. For instance, existing layer freezing methods either require the freeze configurations to be manually defined before training, which does not apply to different networks, or use heuristic freezing criteria that is hard to guarantee decent accuracy in different scenarios. Therefore, there lacks a generic and smart layer freezing method that can automatically perform ``in-situation'' layer fr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OptiState&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;Kalman&#28388;&#27874;&#12289;&#20248;&#21270;&#21644;&#23398;&#20064;&#27169;&#24335;&#30340;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#20449;&#24687;&#65292;&#20197;&#31934;&#30830;&#20272;&#35745;&#26426;&#22120;&#20154;&#20027;&#20307;&#30340;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20851;&#33410;&#32534;&#30721;&#22120;&#12289;IMU&#27979;&#37327;&#21644;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20248;&#21270;&#65292;&#36890;&#36807;Gate&#24490;&#29615;&#21333;&#20803;&#21644;Vision Transformer&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#20102;&#20272;&#35745;&#32467;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#65292;&#24182;&#20943;&#23567;&#20256;&#24863;&#22120;&#27979;&#37327;&#21644;&#27169;&#22411;&#31616;&#21270;&#24341;&#36215;&#30340;&#38750;&#32447;&#24615;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.16719</link><description>&lt;p&gt;
OptiState&#65306;&#22522;&#20110;&#38376;&#25511;&#32593;&#32476;&#12289;Transformer&#35270;&#35273;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
OptiState: State Estimation of Legged Robots using Gated Networks with Transformer-based Vision and Kalman Filtering. (arXiv:2401.16719v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OptiState&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;Kalman&#28388;&#27874;&#12289;&#20248;&#21270;&#21644;&#23398;&#20064;&#27169;&#24335;&#30340;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#20449;&#24687;&#65292;&#20197;&#31934;&#30830;&#20272;&#35745;&#26426;&#22120;&#20154;&#20027;&#20307;&#30340;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20851;&#33410;&#32534;&#30721;&#22120;&#12289;IMU&#27979;&#37327;&#21644;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20248;&#21270;&#65292;&#36890;&#36807;Gate&#24490;&#29615;&#21333;&#20803;&#21644;Vision Transformer&#33258;&#32534;&#30721;&#22120;&#25913;&#36827;&#20102;&#20272;&#35745;&#32467;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#20934;&#30830;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#65292;&#24182;&#20943;&#23567;&#20256;&#24863;&#22120;&#27979;&#37327;&#21644;&#27169;&#22411;&#31616;&#21270;&#24341;&#36215;&#30340;&#38750;&#32447;&#24615;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#39640;&#21160;&#24577;&#36816;&#21160;&#21644;&#20256;&#24863;&#22120;&#31934;&#24230;&#30340;&#23616;&#38480;&#24615;&#65292;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#29366;&#24577;&#20272;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#25972;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#12289;&#20248;&#21270;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#26412;&#20307;&#24863;&#21644;&#22806;&#24863;&#20449;&#24687;&#65292;&#29992;&#20110;&#20272;&#35745;&#26426;&#22120;&#20154;&#20027;&#20307;&#30340;&#29366;&#24577;&#12290;&#20511;&#21161;&#20851;&#33410;&#32534;&#30721;&#22120;&#21644;IMU&#27979;&#37327;&#65292;&#25105;&#20204;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#36890;&#36807;&#21333;&#21018;&#20307;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#65292;&#35813;&#27169;&#22411;&#36824;&#32467;&#21512;&#20102;&#22522;&#20110;&#20984;&#35268;&#21010;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#20248;&#21270;&#30340;&#25509;&#22320;&#21453;&#21147;&#25511;&#21046;&#36755;&#20986;&#12290;&#36890;&#36807;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#36827;&#19968;&#27493;&#25913;&#36827;&#20272;&#35745;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#36824;&#32771;&#34385;&#20102;&#20174;&#28145;&#24230;&#22270;&#20687;&#19978;&#24212;&#29992;&#35270;&#35273;Transformer&#33258;&#32534;&#30721;&#22120;&#33719;&#24471;&#30340;&#35821;&#20041;&#27934;&#23519;&#21644;&#26426;&#22120;&#20154;&#39640;&#24230;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#25552;&#20379;&#20934;&#30830;&#30340;&#26426;&#22120;&#20154;&#29366;&#24577;&#20272;&#35745;&#65292;&#21253;&#25324;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#26469;&#20943;&#23567;&#20256;&#24863;&#22120;&#27979;&#37327;&#21644;&#27169;&#22411;&#31616;&#21270;&#24341;&#36215;&#30340;&#38750;&#32447;&#24615;&#35823;&#24046;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32463;&#36807;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
State estimation for legged robots is challenging due to their highly dynamic motion and limitations imposed by sensor accuracy. By integrating Kalman filtering, optimization, and learning-based modalities, we propose a hybrid solution that combines proprioception and exteroceptive information for estimating the state of the robot's trunk. Leveraging joint encoder and IMU measurements, our Kalman filter is enhanced through a single-rigid body model that incorporates ground reaction force control outputs from convex Model Predictive Control optimization. The estimation is further refined through Gated Recurrent Units, which also considers semantic insights and robot height from a Vision Transformer autoencoder applied on depth images. This framework not only furnishes accurate robot state estimates, including uncertainty evaluations, but can minimize the nonlinear errors that arise from sensor measurements and model simplifications through learning. The proposed methodology is evaluated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65288;MBMM&#65289;&#30340;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#36719;&#32858;&#31867;&#12290;MBMM&#36890;&#36807;&#20854;&#28789;&#27963;&#30340;&#22810;&#20803;&#36125;&#22612;&#20998;&#24067;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#36866;&#24212;&#19981;&#21516;&#30340;&#32858;&#31867;&#24418;&#29366;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16708</link><description>&lt;p&gt;
&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65306;&#20855;&#26377;&#28789;&#27963;&#32858;&#31867;&#24418;&#29366;&#30340;&#27010;&#29575;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible Cluster Shapes. (arXiv:2401.16708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65288;MBMM&#65289;&#30340;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#36719;&#32858;&#31867;&#12290;MBMM&#36890;&#36807;&#20854;&#28789;&#27963;&#30340;&#22810;&#20803;&#36125;&#22612;&#20998;&#24067;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#36866;&#24212;&#19981;&#21516;&#30340;&#32858;&#31867;&#24418;&#29366;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65288;MBMM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#29992;&#20110;&#36719;&#32858;&#31867;&#12290;MBMM&#36890;&#36807;&#22810;&#20803;&#36125;&#22612;&#20998;&#24067;&#30340;&#28789;&#27963;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#36866;&#24212;&#19981;&#21516;&#30340;&#32858;&#31867;&#24418;&#29366;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MBMM&#30340;&#23646;&#24615;&#65292;&#25551;&#36848;&#20102;&#21442;&#25968;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36866;&#21512;&#21508;&#31181;&#32858;&#31867;&#24418;&#29366;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#20195;&#30721;&#21311;&#21517;&#21457;&#24067;&#22312;\url{https://github.com/hhchen1105/mbmm/}&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the multivariate beta mixture model (MBMM), a new probabilistic model for soft clustering. MBMM adapts to diverse cluster shapes because of the flexible probability density function of the multivariate beta distribution. We introduce the properties of MBMM, describe the parameter learning procedure, and present the experimental results, showing that MBMM fits diverse cluster shapes on synthetic and real datasets. The code is released anonymously at \url{https://github.com/hhchen1105/mbmm/}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.16694</link><description>&lt;p&gt;
EdgeOL: &#36793;&#32536;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#21407;&#20301;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EdgeOL: Efficient in-situ Online Learning on Edge Devices. (arXiv:2401.16694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#24212;&#29992;&#65292;&#22914;&#26426;&#22120;&#20154;&#36741;&#21161;&#20859;&#32769;&#21644;&#29289;&#20307;&#35782;&#21035;&#65292;&#36890;&#24120;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#19988;&#33258;&#28982;&#38656;&#35201;&#65306;i) &#22788;&#29702;&#23454;&#26102;&#25512;&#29702;&#35831;&#27714;&#21644;ii) &#36866;&#24212;&#21487;&#33021;&#30340;&#37096;&#32626;&#22330;&#26223;&#21464;&#21270;&#12290;&#22312;&#32447;&#27169;&#22411;&#24494;&#35843;&#34987;&#24191;&#27867;&#37319;&#29992;&#20197;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#33021;&#37327;&#28040;&#32791;&#65292;&#20351;&#20854;&#38590;&#20197;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EdgeOL&#65292;&#19968;&#31181;&#36793;&#32536;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#37096;&#21644;&#22806;&#37096;&#35843;&#20248;&#26469;&#20248;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12289;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#21644;&#33021;&#37327;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;EdgeOL&#24179;&#22343;&#20943;&#23569;&#20102;82%&#30340;&#24494;&#35843;&#25191;&#34892;&#26102;&#38388;&#65292;74%&#30340;&#33021;&#37327;&#28040;&#32791;&#65292;&#24182;&#25552;&#39640;&#20102;&#24179;&#22343;&#25512;&#29702;&#20934;&#30830;&#29575;1.70%&#65292;&#30456;&#23545;&#20110;&#21363;&#26102;&#22312;&#32447;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging applications, such as robot-assisted eldercare and object recognition, generally employ deep learning neural networks (DNNs) models and naturally require: i) handling streaming-in inference requests and ii) adapting to possible deployment scenario changes. Online model fine-tuning is widely adopted to satisfy these needs. However, fine-tuning involves significant energy consumption, making it challenging to deploy on edge devices. In this paper, we propose EdgeOL, an edge online learning framework that optimizes inference accuracy, fine-tuning execution time, and energy efficiency through both inter-tuning and intra-tuning optimizations. Experimental results show that, on average, EdgeOL reduces overall fine-tuning execution time by 82%, energy consumption by 74%, and improves average inference accuracy by 1.70% over the immediate online learning strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26694;&#26550;&#65292;&#36890;&#36807;&#20943;&#23569;&#26041;&#24046;&#26469;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#26469;&#26816;&#27979;&#26377;&#25928;&#24314;&#27169;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.16692</link><description>&lt;p&gt;
&#39044;&#26657;&#20934;&#21644;&#35745;&#31639;&#65306;&#28145;&#24230;&#28857;&#20987;&#29575;&#39044;&#27979;&#27169;&#22411;&#20013;&#19968;&#31181;&#26041;&#24046;&#20943;&#23569;&#30340;&#24230;&#37327;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Calibration-then-Calculation: A Variance Reduced Metric Framework in Deep Click-Through Rate Prediction Models. (arXiv:2401.16692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26694;&#26550;&#65292;&#36890;&#36807;&#20943;&#23569;&#26041;&#24046;&#26469;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#26469;&#26816;&#27979;&#26377;&#25928;&#24314;&#27169;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#30340;&#24615;&#33021;&#35780;&#20272;&#20851;&#27880;&#36739;&#23569;&#12290;&#38543;&#30528;&#22823;&#22411;&#25968;&#25454;&#38598;&#21644;&#22797;&#26434;&#27169;&#22411;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#36890;&#24120;&#21482;&#36816;&#34892;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#24182;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#25351;&#26631;&#30340;&#26041;&#24046;&#65292;&#36825;&#31181;&#36807;&#31243;&#21487;&#33021;&#23548;&#33268;&#19981;&#31934;&#30830;&#30340;&#27604;&#36739;&#12290;&#25351;&#26631;&#26041;&#24046;&#26469;&#33258;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#35757;&#32451;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#12290;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#22914;&#22810;&#27425;&#36816;&#34892;&#35757;&#32451;&#36807;&#31243;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#24448;&#24448;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#35745;&#31639;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26694;&#26550;&#65292;&#31216;&#20026;&#26657;&#20934;&#25439;&#22833;&#24230;&#37327;&#65292;&#36890;&#36807;&#20943;&#23569;&#20854;&#22522;&#20934;&#27169;&#22411;&#20013;&#30340;&#26041;&#24046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#32467;&#26524;&#65292;&#36825;&#20010;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#26469;&#26816;&#27979;&#26377;&#25928;&#24314;&#27169;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#21644;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been widely adopted across various fields, but there has been little focus on evaluating the performance of deep learning pipelines. With the increased use of large datasets and complex models, it has become common to run the training process only once and compare the result to previous benchmarks. However, this procedure can lead to imprecise comparisons due to the variance in neural network evaluation metrics. The metric variance comes from the randomness inherent in the training process of deep learning pipelines. Traditional solutions such as running the training process multiple times are usually not feasible in deep learning due to computational limitations. In this paper, we propose a new metric framework, Calibrated Loss Metric, that addresses this issue by reducing the variance in its vanilla counterpart. As a result, the new metric has a higher accuracy to detect effective modeling improvement. Our approach is supported by theoretical justifications and exte
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#21098;&#26525;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#21452;&#37325;&#26799;&#24230;&#21098;&#26525;&#65288;DGP&#65289;&#65292;&#21487;&#20197;&#25552;&#39640;&#21327;&#20316;&#23398;&#20064;&#30340;&#36890;&#20449;&#25928;&#29575;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.16687</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#26799;&#24230;&#21098;&#26525;&#65306;&#19968;&#31181;&#29992;&#20110;&#25269;&#24481;&#26799;&#24230;&#25915;&#20987;&#30340;&#21452;&#37325;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Revisiting Gradient Pruning: A Dual Realization for Defending against Gradient Attacks. (arXiv:2401.16687v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16687
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#21098;&#26525;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#21452;&#37325;&#26799;&#24230;&#21098;&#26525;&#65288;DGP&#65289;&#65292;&#21487;&#20197;&#25552;&#39640;&#21327;&#20316;&#23398;&#20064;&#30340;&#36890;&#20449;&#25928;&#29575;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20849;&#20139;&#26799;&#24230;&#26356;&#26032;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#21487;&#20197;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#24674;&#22797;&#29992;&#25143;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#23545;&#21327;&#20316;&#23398;&#20064;&#26500;&#25104;&#20005;&#37325;&#30340;&#38544;&#31169;&#23041;&#32961;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#37319;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#22914;&#24046;&#20998;&#38544;&#31169;&#12289;&#23494;&#30721;&#23398;&#21644;&#25200;&#21160;&#38450;&#24481;&#65292;&#20197;&#25269;&#24481;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#30340;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#20043;&#38388;&#23384;&#22312;&#36739;&#24046;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#32531;&#35299;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#24369;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#21363;&#21452;&#37325;&#26799;&#24230;&#21098;&#26525;&#65288;DGP&#65289;&#65292;&#22522;&#20110;&#26799;&#24230;&#21098;&#26525;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21516;&#26102;&#20445;&#25345;&#21327;&#20316;&#23398;&#20064;&#30340;&#25928;&#29992;&#21644;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DGP&#31245;&#24494;&#25913;&#21464;&#20102;&#26799;&#24230;&#21098;&#26525;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#21516;&#26102;&#65292;DGP&#36824;&#21487;&#20197;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#26174;&#33879;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative learning (CL) is a distributed learning framework that aims to protect user privacy by allowing users to jointly train a model by sharing their gradient updates only. However, gradient inversion attacks (GIAs), which recover users' training data from shared gradients, impose severe privacy threats to CL. Existing defense methods adopt different techniques, e.g., differential privacy, cryptography, and perturbation defenses, to defend against the GIAs. Nevertheless, all current defense methods suffer from a poor trade-off between privacy, utility, and efficiency. To mitigate the weaknesses of existing solutions, we propose a novel defense method, Dual Gradient Pruning (DGP), based on gradient pruning, which can improve communication efficiency while preserving the utility and privacy of CL. Specifically, DGP slightly changes gradient pruning with a stronger privacy guarantee. And DGP can also significantly improve communication efficiency with a theoretical analysis of its
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#27169;&#24577;&#21644;&#23458;&#25143;&#36873;&#25321;&#26469;&#35299;&#20915;&#22810;&#26679;&#30340;&#27169;&#24577;&#38598;&#21512;&#21644;&#36890;&#20449;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.16685</link><description>&lt;p&gt;
&#36890;&#20449;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65306;&#32852;&#21512;&#27169;&#24577;&#21644;&#23458;&#25143;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Multimodal Federated Learning: Joint Modality and Client Selection. (arXiv:2401.16685v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#27169;&#24577;&#21644;&#23458;&#25143;&#36873;&#25321;&#26469;&#35299;&#20915;&#22810;&#26679;&#30340;&#27169;&#24577;&#38598;&#21512;&#21644;&#36890;&#20449;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#20016;&#23500;&#22312;&#23458;&#25143;&#31471;&#25910;&#38598;&#22810;&#27169;&#24577;&#27979;&#37327;&#30340;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#19968;&#20123;&#23578;&#26410;&#35299;&#20915;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24322;&#26500;&#32593;&#32476;&#29615;&#22659;&#20013;&#65306;(i)&#27599;&#20010;&#23458;&#25143;&#31471;&#25910;&#38598;&#30340;&#27169;&#24577;&#38598;&#21512;&#23558;&#26159;&#22810;&#26679;&#30340;&#65292;(ii)&#36890;&#20449;&#38480;&#21046;&#38459;&#27490;&#23458;&#25143;&#31471;&#23558;&#20854;&#25152;&#26377;&#26412;&#22320;&#35757;&#32451;&#30340;&#27169;&#24577;&#27169;&#22411;&#19978;&#20256;&#21040;&#26381;&#21153;&#22120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#19982;&#32852;&#21512;&#27169;&#24577;&#21644;&#23458;&#25143;&#36873;&#25321;(mmFedMC)&#65292;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#30340;&#19978;&#36848;&#25361;&#25112;&#12290;&#32852;&#21512;&#36873;&#25321;&#31639;&#27861;&#21253;&#21547;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;(a)&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#35774;&#35745;&#30340;&#27169;&#24577;&#36873;&#25321;&#26041;&#27861;&#65292;&#26681;&#25454;Shapley&#20540;&#20998;&#26512;&#35780;&#20272;&#27169;&#24577;&#30340;&#24433;&#21709;&#65292;&#26681;&#25454;&#36890;&#20449;&#24320;&#38144;&#30340;&#27169;&#24577;&#27169;&#22411;&#22823;&#23567;&#65292;&#32467;&#21512;&#27169;&#24577;&#27169;&#22411;&#26356;&#26032;&#39057;&#29575;&#65288;&#31216;&#20026;&#26368;&#36817;&#26356;&#26032;&#65289;&#20316;&#20026;&#26435;&#37325;&#65292;&#20197;&#22686;&#24378;&#27169;&#24577;&#36873;&#25321;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal federated learning (FL) aims to enrich model training in FL settings where clients are collecting measurements across multiple modalities. However, key challenges to multimodal FL remain unaddressed, particularly in heterogeneous network settings where: (i) the set of modalities collected by each client will be diverse, and (ii) communication limitations prevent clients from uploading all their locally trained modality models to the server. In this paper, we propose multimodal Federated learning with joint Modality and Client selection (mmFedMC), a new FL methodology that can tackle the above-mentioned challenges in multimodal settings. The joint selection algorithm incorporates two main components: (a) A modality selection methodology for each client, which weighs (i) the impact of the modality, gauged by Shapley value analysis, (ii) the modality model size as a gauge of communication overhead, against (iii) the frequency of modality model updates, denoted recency, to enhan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#23398;&#20064;&#30340;&#20195;&#29702;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#32500;&#38543;&#26426;&#31995;&#32479;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#36890;&#36807;&#22312;Grassmann&#27969;&#24418;&#19978;&#36827;&#34892;&#20027;&#27979;&#22320;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;&#19968;&#32452;&#28508;&#22312;&#30340;&#20302;&#32500;&#25551;&#36848;&#31526;&#65292;&#28982;&#21518;&#21033;&#29992;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#26500;&#24314;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2401.16683</link><description>&lt;p&gt;
&#22312;&#20027;&#27979;&#22320;Grassmannian&#23376;&#27969;&#24418;&#19978;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#29992;&#20110;&#20195;&#29702;&#24314;&#27169;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Polynomial Chaos Expansions on Principal Geodesic Grassmannian Submanifolds for Surrogate Modeling and Uncertainty Quantification. (arXiv:2401.16683v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#23398;&#20064;&#30340;&#20195;&#29702;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#32500;&#38543;&#26426;&#31995;&#32479;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#36890;&#36807;&#22312;Grassmann&#27969;&#24418;&#19978;&#36827;&#34892;&#20027;&#27979;&#22320;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;&#19968;&#32452;&#28508;&#22312;&#30340;&#20302;&#32500;&#25551;&#36848;&#31526;&#65292;&#28982;&#21518;&#21033;&#29992;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#26500;&#24314;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27969;&#24418;&#23398;&#20064;&#30340;&#20195;&#29702;&#24314;&#27169;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#32500;&#38543;&#26426;&#31995;&#32479;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#39318;&#35201;&#30446;&#26631;&#26159;&#23545;&#21487;&#29992;&#30340;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#25366;&#25496;&#65292;&#20197;&#30830;&#23450;&#33021;&#22815;&#39640;&#25928;&#21442;&#25968;&#21270;&#39640;&#32500;&#35745;&#31639;&#27169;&#22411;&#21709;&#24212;&#30340;&#19968;&#32452;&#20302;&#32500;&#65288;&#28508;&#22312;&#65289;&#25551;&#36848;&#31526;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;Grassmann&#27969;&#24418;&#19978;&#30340;&#20027;&#27979;&#22320;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;&#19968;&#32452;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#32500;&#24230;&#30340;&#19981;&#30456;&#20132;&#20027;&#27979;&#22320;&#23376;&#27969;&#24418;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#21464;&#21270;&#12290;&#30001;&#20110;Grassmann&#19978;&#30340;&#25805;&#20316;&#38656;&#35201;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Riemanniann K-means&#21644;Grassmann&#27969;&#24418;&#19978;&#26679;&#26412;Frechet&#26041;&#24046;&#26368;&#23567;&#21270;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#20195;&#34920;&#21442;&#25968;&#31354;&#38388;&#20013;&#19981;&#21516;&#31995;&#32479;&#34892;&#20026;&#30340;&#8220;&#26412;&#22320;&#8221;&#20027;&#27979;&#22320;&#23376;&#27969;&#24418;&#12290;&#28982;&#21518;&#20351;&#29992;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#26500;&#24314;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
In this work we introduce a manifold learning-based surrogate modeling framework for uncertainty quantification in high-dimensional stochastic systems. Our first goal is to perform data mining on the available simulation data to identify a set of low-dimensional (latent) descriptors that efficiently parameterize the response of the high-dimensional computational model. To this end, we employ Principal Geodesic Analysis on the Grassmann manifold of the response to identify a set of disjoint principal geodesic submanifolds, of possibly different dimension, that captures the variation in the data. Since operations on the Grassmann require the data to be concentrated, we propose an adaptive algorithm based on Riemanniann K-means and the minimization of the sample Frechet variance on the Grassmann manifold to identify "local" principal geodesic submanifolds that represent different system behavior across the parameter space. Polynomial chaos expansion is then used to construct a mapping bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#34394;&#26500;&#35805;&#35821;&#26816;&#27979;&#30340;&#20998;&#31867;&#23454;&#39564;&#65292;&#21033;&#29992;&#20102;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#21644;&#26032;&#30340;&#29305;&#24449;&#38598;&#12290;&#34394;&#26500;&#35805;&#35821;&#30340;&#26816;&#27979;&#26377;&#21161;&#20110;&#20016;&#23500;&#22823;&#22411;&#25991;&#21270;&#36951;&#20135;&#26723;&#26696;&#65292;&#24182;&#24110;&#21161;&#26356;&#24191;&#27867;&#22320;&#29702;&#35299;&#34394;&#26500;&#21465;&#20107;&#30340;&#29305;&#36136;&#12290;</title><link>http://arxiv.org/abs/2401.16678</link><description>&lt;p&gt;
&#34394;&#26500;&#35805;&#35821;&#30340;&#26816;&#27979;&#19982;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
The Detection and Understanding of Fictional Discourse. (arXiv:2401.16678v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#34394;&#26500;&#35805;&#35821;&#26816;&#27979;&#30340;&#20998;&#31867;&#23454;&#39564;&#65292;&#21033;&#29992;&#20102;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#21644;&#26032;&#30340;&#29305;&#24449;&#38598;&#12290;&#34394;&#26500;&#35805;&#35821;&#30340;&#26816;&#27979;&#26377;&#21161;&#20110;&#20016;&#23500;&#22823;&#22411;&#25991;&#21270;&#36951;&#20135;&#26723;&#26696;&#65292;&#24182;&#24110;&#21161;&#26356;&#24191;&#27867;&#22320;&#29702;&#35299;&#34394;&#26500;&#21465;&#20107;&#30340;&#29305;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19982;&#34394;&#26500;&#35805;&#35821;&#26816;&#27979;&#20219;&#21153;&#30456;&#20851;&#30340;&#20998;&#31867;&#23454;&#39564;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#24403;&#20195;&#19987;&#19994;&#20986;&#29256;&#30340;&#23567;&#35828;&#12289;&#26469;&#33258;Hathi Trust&#30340;&#21382;&#21490;&#23567;&#35828;&#12289;&#31881;&#19997;&#25152;&#20889;&#25991;&#12289;&#26469;&#33258;Reddit&#30340;&#25925;&#20107;&#12289;&#27665;&#38388;&#20256;&#35828;&#12289;GPT&#29983;&#25104;&#30340;&#25925;&#20107;&#20197;&#21450;&#33521;&#35821;&#19990;&#30028;&#25991;&#23398;&#20316;&#21697;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#8220;&#36229;&#35789;&#20041;&#8221;&#29305;&#24449;&#38598;&#65292;&#20197;&#20419;&#36827;&#35821;&#20041;&#27010;&#25324;&#30340;&#30446;&#26631;&#12290;&#34394;&#26500;&#35805;&#35821;&#30340;&#26816;&#27979;&#21487;&#20197;&#24110;&#21161;&#20016;&#23500;&#25105;&#20204;&#23545;&#22823;&#22411;&#25991;&#21270;&#36951;&#20135;&#26723;&#26696;&#30340;&#20102;&#35299;&#65292;&#24182;&#26377;&#21161;&#20110;&#26356;&#24191;&#27867;&#22320;&#29702;&#35299;&#34394;&#26500;&#21465;&#20107;&#30340;&#29420;&#29305;&#29305;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a variety of classification experiments related to the task of fictional discourse detection. We utilize a diverse array of datasets, including contemporary professionally published fiction, historical fiction from the Hathi Trust, fanfiction, stories from Reddit, folk tales, GPT-generated stories, and anglophone world literature. Additionally, we introduce a new feature set of word "supersenses" that facilitate the goal of semantic generalization. The detection of fictional discourse can help enrich our knowledge of large cultural heritage archives and assist with the process of understanding the distinctive qualities of fictional storytelling more broadly.
&lt;/p&gt;</description></item><item><title>T3&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#36319;&#36394;&#21644;&#35302;&#21457;&#30340;&#30828;&#20214;-&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16677</link><description>&lt;p&gt;
T3&#65306;&#38754;&#21521;&#35745;&#31639;&#21644;&#38598;&#21512;&#32454;&#31890;&#24230;&#37325;&#21472;&#30340;&#36879;&#26126;&#36319;&#36394;&#21644;&#35302;&#21457;
&lt;/p&gt;
&lt;p&gt;
T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives. (arXiv:2401.16677v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16677
&lt;/p&gt;
&lt;p&gt;
T3&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#36319;&#36394;&#21644;&#35302;&#21457;&#30340;&#30828;&#20214;-&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#20013;&#30340;&#36890;&#20449;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#20110;&#20998;&#24067;&#24335;&#25216;&#26415;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#36825;&#20123;&#25216;&#26415;&#38656;&#35201;&#35774;&#22791;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#38543;&#30528;&#35774;&#22791;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36890;&#20449;&#20250;&#38477;&#20302;&#25193;&#23637;&#25928;&#29575;&#12290;&#34429;&#28982;&#19968;&#20123;&#20998;&#24067;&#24335;&#25216;&#26415;&#21487;&#20197;&#37325;&#21472;&#24182;&#38544;&#34255;&#36825;&#31181;&#36890;&#20449;&#19982;&#29420;&#31435;&#35745;&#31639;&#65292;&#20294;&#24352;&#37327;&#24182;&#34892;&#25216;&#26415;(TP)&#26412;&#36136;&#19978;&#23558;&#36890;&#20449;&#19982;&#27169;&#22411;&#25191;&#34892;&#20018;&#34892;&#21270;&#12290;&#38544;&#34255;&#36825;&#31181;&#20018;&#34892;&#36890;&#20449;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#23558;&#20854;&#19982;&#29983;&#20135;&#25805;&#20316;(&#36890;&#20449;&#25968;&#25454;)&#20132;&#26367;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#36719;&#20214;&#20013;&#30340;&#36825;&#31181;&#32454;&#31890;&#24230;&#36890;&#20449;&#21644;&#35745;&#31639;&#20132;&#38169;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#19982;&#20219;&#20309;&#24182;&#21457;&#25191;&#34892;&#19968;&#26679;&#65292;&#23427;&#38656;&#35201;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#22312;&#35745;&#31639;&#21644;&#36890;&#20449;&#20043;&#38388;&#20849;&#20139;&#65292;&#23548;&#33268;&#36164;&#28304;&#20105;&#29992;&#20943;&#23569;&#37325;&#21472;&#25928;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;T3&#65292;&#23427;&#24212;&#29992;&#30828;&#20214;-&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#26469;&#23454;&#29616;&#36879;&#26126;&#30340;&#36319;&#36394;&#21644;&#35302;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models increasingly rely on distributed techniques for their training and inference. These techniques require communication across devices which can reduce scaling efficiency as the number of devices increases. While some distributed techniques can overlap, and thus, hide this communication with independent computations, techniques such as Tensor Parallelism (TP) inherently serialize communication with model execution. One approach to hide this serialized communication is to interleave it with the producer operation (of the communicated data) in a fine-grained manner. However, this fine-grained interleaving of communication and computation in software can be difficult. Furthermore, as with any concurrent execution, it requires compute and memory resources to be shared between computation and communication, causing resource contention that reduces overlapping efficacy.  To overcome these challenges, we propose T3 which applies hardware-software co-design to transparently 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#65292;&#23427;&#20811;&#26381;&#20102;&#20256;&#32479;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#26377;&#28508;&#21147;&#24341;&#39046;&#22825;&#27668;&#39044;&#25253;&#30340;&#31532;&#20108;&#27425;&#38761;&#21629;&#12290;</title><link>http://arxiv.org/abs/2401.16669</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#20026;&#22825;&#27668;&#39044;&#25253;&#24102;&#26469;&#20102;&#31532;&#20108;&#27425;&#38761;&#21629;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Artificial Intelligence Providing the Second Revolution for Weather Forecasting?. (arXiv:2401.16669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16669
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#65292;&#23427;&#20811;&#26381;&#20102;&#20256;&#32479;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#26377;&#28508;&#21147;&#24341;&#39046;&#22825;&#27668;&#39044;&#25253;&#30340;&#31532;&#20108;&#27425;&#38761;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#36817;&#24180;&#26469;&#65292;&#23548;&#33268;&#20102;&#20960;&#31181;&#22823;&#21442;&#25968;&#20154;&#24037;&#26234;&#33021;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#34920;&#26126;&#20102;&#22825;&#27668;&#39044;&#25253;&#21487;&#33021;&#36814;&#26469;&#31532;&#20108;&#27425;&#38761;&#21629;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#39044;&#25253;&#27169;&#22411;&#30340;&#28436;&#21464;&#65292;&#24182;&#22312;&#30830;&#23450;&#30340;&#20849;&#21516;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#23427;&#20204;&#30340;&#21457;&#23637;&#30340;&#8220;&#19977;&#22823;&#35268;&#21017;&#8221;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#38761;&#21629;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#31616;&#35201;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#26410;&#26469;&#21457;&#23637;&#21069;&#26223;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#23558;&#25972;&#20010;&#25968;&#20540;&#39044;&#25253;&#36807;&#31243;&#36827;&#34892;&#25972;&#21512;&#12290;&#36890;&#36807;&#23558;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19982;&#20854;&#20182;&#20449;&#24687;&#32508;&#21512;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of artificial intelligence technologies, particularly in recent years, has led to the emergence of several large parameter artificial intelligence weather forecast models. These models represent a significant breakthrough, overcoming the limitations of traditional numerical weather prediction models and indicating a potential second revolution for weather forecast. This study explores the evolution of these advanced artificial intelligence forecast models, and based on the identified commonalities, proposes the "Three Large Rules" for their development. We discuss the potential of artificial intelligence in revolutionizing numerical weather prediction, briefly outlining the underlying reasons for this potential. Additionally, we explore key areas for future development prospects for large artificial intelligence weather forecast models, integrating the entire numerical prediction process. Through an example that combines a large artificial intelligence model with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21452;&#27491;&#21017;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#31232;&#30095;&#29983;&#29289;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#39044;&#27979;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#21644;&#33647;&#29289;&#30142;&#30149;&#20851;&#32852;&#26041;&#38754;&#20855;&#26377;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.16664</link><description>&lt;p&gt;
&#24555;&#36895;&#21452;&#27491;&#21017;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#31232;&#30095;&#29983;&#29289;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Fast Dual-Regularized Autoencoder for Sparse Biological Data. (arXiv:2401.16664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21452;&#27491;&#21017;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#31232;&#30095;&#29983;&#29289;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#39044;&#27979;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#21644;&#33647;&#29289;&#30142;&#30149;&#20851;&#32852;&#26041;&#38754;&#20855;&#26377;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31232;&#30095;&#25968;&#25454;&#20013;&#25512;&#26029;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#24212;&#29992;&#33539;&#22260;&#20174;&#20135;&#21697;&#25512;&#33616;&#21040;&#33647;&#29289;&#21457;&#29616;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#31232;&#30095;&#30697;&#38453;&#34917;&#20840;&#30340;&#32447;&#24615;&#27169;&#22411;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#30456;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#25512;&#33616;&#31995;&#32479;&#31639;&#27861;&#20855;&#26377;&#24778;&#20154;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#32447;&#24615;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#27973;&#23618;&#33258;&#32534;&#30721;&#22120;&#26469;&#35299;&#20915;&#21452;&#37051;&#22495;&#27491;&#21017;&#21270;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#21644;&#33647;&#29289;&#30142;&#30149;&#20851;&#32852;&#26041;&#38754;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relationship inference from sparse data is an important task with applications ranging from product recommendation to drug discovery. A recently proposed linear model for sparse matrix completion has demonstrated surprising advantage in speed and accuracy over more sophisticated recommender systems algorithms. Here we extend the linear model to develop a shallow autoencoder for the dual neighborhood-regularized matrix completion problem. We demonstrate the speed and accuracy advantage of our approach over the existing state-of-the-art in predicting drug-target interactions and drug-disease associations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiNGAM-MMI&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;LiNGAM&#27169;&#22411;&#20197;&#22788;&#29702;&#28151;&#28102;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;KL&#25955;&#24230;&#37327;&#21270;&#28151;&#28102;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#39640;&#25928;&#22320;&#30830;&#23450;&#21464;&#37327;&#39034;&#24207;&#65292;&#19981;&#35770;&#26159;&#21542;&#23384;&#22312;&#28151;&#28102;&#24773;&#20917;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LiNGAM-MMI&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#27491;&#30830;&#30340;&#21464;&#37327;&#39034;&#24207;&#12290;</title><link>http://arxiv.org/abs/2401.16661</link><description>&lt;p&gt;
&#20801;&#35768;&#28151;&#28102;&#30340;LiNGAM&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization of LiNGAM that allows confounding. (arXiv:2401.16661v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiNGAM-MMI&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;LiNGAM&#27169;&#22411;&#20197;&#22788;&#29702;&#28151;&#28102;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;KL&#25955;&#24230;&#37327;&#21270;&#28151;&#28102;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#39640;&#25928;&#22320;&#30830;&#23450;&#21464;&#37327;&#39034;&#24207;&#65292;&#19981;&#35770;&#26159;&#21542;&#23384;&#22312;&#28151;&#28102;&#24773;&#20917;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LiNGAM-MMI&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35782;&#21035;&#27491;&#30830;&#30340;&#21464;&#37327;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LiNGAM&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#26469;&#30830;&#23450;&#22240;&#26524;&#20851;&#31995;&#30340;&#21464;&#37327;&#39034;&#24207;&#65292;&#20294;&#22312;&#28151;&#28102;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;LiNGAM&#30340;&#22522;&#26412;&#32467;&#26500;&#30340;&#21516;&#26102;&#65292;&#35797;&#22270;&#35782;&#21035;&#21644;&#22788;&#29702;&#21463;&#28151;&#28102;&#24433;&#21709;&#30340;&#21464;&#37327;&#12290;&#32467;&#26524;&#26159;&#65292;&#19981;&#35770;&#26159;&#21542;&#23384;&#22312;&#28151;&#28102;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#19988;&#19981;&#33021;&#30830;&#20445;&#26816;&#27979;&#21040;&#25152;&#26377;&#30340;&#28151;&#28102;&#31867;&#22411;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;LiNGAM-MMI&#23545;LiNGAM&#36827;&#34892;&#20102;&#22686;&#24378;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;KL&#25955;&#24230;&#37327;&#21270;&#28151;&#28102;&#31243;&#24230;&#65292;&#24182;&#23433;&#25490;&#21464;&#37327;&#20197;&#26368;&#23567;&#21270;&#20854;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#30340;&#24418;&#24335;&#39640;&#25928;&#22320;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#30340;&#21464;&#37327;&#39034;&#24207;&#12290;&#22312;&#26080;&#28151;&#28102;&#30340;&#24773;&#20917;&#19979;&#65292;LiNGAM-MMI&#30340;&#22788;&#29702;&#25968;&#25454;&#25928;&#29575;&#19982;&#20256;&#32479;LiNGAM&#30456;&#24403;&#65292;&#21516;&#26102;&#26377;&#25928;&#22788;&#29702;&#28151;&#28102;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LiNGAM-MMI&#26356;&#20934;&#30830;&#22320;&#30830;&#23450;&#20102;&#27491;&#30830;&#30340;&#21464;&#37327;&#39034;&#24207;...
&lt;/p&gt;
&lt;p&gt;
LiNGAM determines the variable order from cause to effect using additive noise models, but it faces challenges with confounding. Previous methods maintained LiNGAM's fundamental structure while trying to identify and address variables affected by confounding. As a result, these methods required significant computational resources regardless of the presence of confounding, and they did not ensure the detection of all confounding types. In contrast, this paper enhances LiNGAM by introducing LiNGAM-MMI, a method that quantifies the magnitude of confounding using KL divergence and arranges the variables to minimize its impact. This method efficiently achieves a globally optimal variable order through the shortest path problem formulation. LiNGAM-MMI processes data as efficiently as traditional LiNGAM in scenarios without confounding while effectively addressing confounding situations. Our experimental results suggest that LiNGAM-MMI more accurately determines the correct variable order, bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Chen-Fliess&#24207;&#21015;&#23637;&#24320;&#23558;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;ODE&#27169;&#22411;&#36716;&#21270;&#20026;&#21333;&#23618;&#12289;&#26080;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#27492;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#23558;&#21021;&#22987;&#26465;&#20214;&#26144;&#23556;&#21040;&#26576;&#20010;&#32456;&#31471;&#26102;&#38388;&#30340;ODE&#27169;&#22411;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#32039;&#20945;&#34920;&#36798;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.16655</link><description>&lt;p&gt;
&#36890;&#36807;Chen-Fliess&#24207;&#21015;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;ODE&#27169;&#22411;&#26500;&#24314;&#20026;&#21333;&#23618;&#12289;&#26080;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rademacher Complexity of Neural ODEs via Chen-Fliess Series. (arXiv:2401.16655v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Chen-Fliess&#24207;&#21015;&#23637;&#24320;&#23558;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;ODE&#27169;&#22411;&#36716;&#21270;&#20026;&#21333;&#23618;&#12289;&#26080;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#27492;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#23558;&#21021;&#22987;&#26465;&#20214;&#26144;&#23556;&#21040;&#26576;&#20010;&#32456;&#31471;&#26102;&#38388;&#30340;ODE&#27169;&#22411;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#32039;&#20945;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#36830;&#32493;&#28145;&#24230;&#31070;&#32463;ODE&#27169;&#22411;&#20351;&#29992;Chen-Fliess&#24207;&#21015;&#23637;&#24320;&#20026;&#21333;&#23618;&#12289;&#26080;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#12290;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#65292;&#36755;&#20986;&#30340;&#8220;&#26435;&#37325;&#8221;&#26469;&#33258;&#25511;&#21046;&#36755;&#20837;&#30340;&#29305;&#24449;&#24207;&#21015;&#65292;&#23427;&#30001;&#25511;&#21046;&#36755;&#20837;&#22312;&#21333;&#32431;&#24418;&#19978;&#30340;&#36845;&#20195;&#31215;&#20998;&#26500;&#25104;&#12290;&#32780;&#8220;&#29305;&#24449;&#8221;&#21017;&#22522;&#20110;&#21463;&#25511;ODE&#27169;&#22411;&#20013;&#36755;&#20986;&#20989;&#25968;&#30456;&#23545;&#20110;&#21521;&#37327;&#22330;&#30340;&#36845;&#20195;&#26446;&#23548;&#25968;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#24212;&#29992;&#36825;&#20010;&#26694;&#26550;&#25512;&#23548;&#20986;&#20102;&#23558;&#21021;&#22987;&#26465;&#20214;&#26144;&#23556;&#21040;&#26576;&#20010;&#32456;&#31471;&#26102;&#38388;&#30340;ODE&#27169;&#22411;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#32039;&#20945;&#34920;&#36798;&#24335;&#12290;&#36825;&#19968;&#32467;&#26524;&#21033;&#29992;&#20102;&#21333;&#23618;&#32467;&#26500;&#25152;&#24102;&#26469;&#30340;&#30452;&#25509;&#20998;&#26512;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#20855;&#20307;&#31995;&#32479;&#30340;&#20363;&#23376;&#23454;&#20363;&#21270;&#35813;&#30028;&#65292;&#24182;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#21518;&#32493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show how continuous-depth neural ODE models can be framed as single-layer, infinite-width nets using the Chen--Fliess series expansion for nonlinear ODEs. In this net, the output ''weights'' are taken from the signature of the control input -- a tool used to represent infinite-dimensional paths as a sequence of tensors -- which comprises iterated integrals of the control input over a simplex. The ''features'' are taken to be iterated Lie derivatives of the output function with respect to the vector fields in the controlled ODE model. The main result of this work applies this framework to derive compact expressions for the Rademacher complexity of ODE models that map an initial condition to a scalar output at some terminal time. The result leverages the straightforward analysis afforded by single-layer architectures. We conclude with some examples instantiating the bound for some specific systems and discuss potential follow-up work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#24212;&#29992;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#26377;&#25928;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2401.16650</link><description>&lt;p&gt;
&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22238;&#25918;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Augmenting Replay in World Models for Continual Reinforcement Learning. (arXiv:2401.16650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#24212;&#29992;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#26377;&#25928;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#29615;&#22659;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#25104;&#21151;&#30340;&#31995;&#32479;&#24212;&#35813;&#36866;&#24403;&#24179;&#34913;&#20445;&#25345;&#24050;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#20195;&#29702;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#35201;&#27714;&#12290;&#39318;&#36827;&#20808;&#20986;&#32531;&#20914;&#21306;&#36890;&#24120;&#29992;&#20110;&#22686;&#24378;&#27492;&#31867;&#35774;&#32622;&#20013;&#30340;&#23398;&#20064;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22686;&#24378;&#26041;&#27861;&#24212;&#29992;&#20110;&#27492;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#32531;&#35299;&#20869;&#23384;&#38480;&#21046;&#65292;&#24182;&#19982;&#22522;&#20110;&#19990;&#30028;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#35780;&#20272;&#20854;&#22312;&#20419;&#36827;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;Procgen&#21644;Atari&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28508;&#22312;&#19990;&#30028;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#20998;&#24067;&#21305;&#37197;&#22686;&#24378;&#21487;&#20197;&#25104;&#21151;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24182;&#38750;&#23436;&#20840;&#26080;&#25032;&#21487;&#20987;&#65292;
&lt;/p&gt;
&lt;p&gt;
In continual RL, the environment of a reinforcement learning (RL) agent undergoes change. A successful system should appropriately balance the conflicting requirements of retaining agent performance on already learned tasks, stability, whilst learning new tasks, plasticity. The first-in-first-out buffer is commonly used to enhance learning in such settings but requires significant memory. We explore the application of an augmentation to this buffer which alleviates the memory constraints, and use it with a world model model-based reinforcement learning algorithm, to evaluate its effectiveness in facilitating continual learning. We evaluate the effectiveness of our method in Procgen and Atari RL benchmarks and show that the distribution matching augmentation to the replay-buffer used in the context of latent world models can successfully prevent catastrophic forgetting with significantly reduced computational overhead. Yet, we also find such a solution to not be entirely infallible, and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#20316;&#39044;&#27979;&#36827;&#34892;&#22522;&#20110;&#34892;&#20026;&#30340;&#34394;&#25311;&#29616;&#23454;&#35748;&#35777;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#29992;&#25143;&#30340;&#26410;&#26469;&#34892;&#20026;&#36712;&#36857;&#24182;&#36827;&#34892;&#35748;&#35777;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25216;&#26415;&#22312;&#20351;&#29992;&#36739;&#23567;&#36712;&#36857;&#27573;&#26102;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16649</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#20316;&#39044;&#27979;&#36827;&#34892;&#22522;&#20110;&#34892;&#20026;&#30340;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Using Motion Forecasting for Behavior-Based Virtual Reality (VR) Authentication. (arXiv:2401.16649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#20316;&#39044;&#27979;&#36827;&#34892;&#22522;&#20110;&#34892;&#20026;&#30340;&#34394;&#25311;&#29616;&#23454;&#35748;&#35777;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#29992;&#25143;&#30340;&#26410;&#26469;&#34892;&#20026;&#36712;&#36857;&#24182;&#36827;&#34892;&#35748;&#35777;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25216;&#26415;&#22312;&#20351;&#29992;&#36739;&#23567;&#36712;&#36857;&#27573;&#26102;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;&#20219;&#21153;&#30340;&#34892;&#20026;&#29983;&#29289;&#29305;&#24449;&#35748;&#35777;&#21487;&#20197;&#36890;&#36807;&#20165;&#20351;&#29992;&#20154;&#20307;&#30340;&#36816;&#21160;&#36712;&#36857;&#20316;&#20026;&#21807;&#19968;&#31614;&#21517;&#23454;&#29616;&#26080;&#32541;&#36830;&#32493;&#35748;&#35777;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34892;&#20026;&#29983;&#29289;&#29305;&#24449;&#35748;&#35777;&#26041;&#27861;&#22312;&#20351;&#29992;&#23436;&#25972;&#25110;&#25509;&#36817;&#23436;&#25972;&#30340;&#29992;&#25143;&#36712;&#36857;&#26102;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#20351;&#29992;&#20219;&#21153;&#24320;&#22987;&#26102;&#30340;&#36739;&#23567;&#27573;&#33853;&#26102;&#34920;&#29616;&#36739;&#24046;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#29616;&#26377;&#25216;&#26415;&#35774;&#35745;&#30340;&#20219;&#20309;&#31995;&#32479;&#22312;&#31561;&#24453;&#26410;&#26469;&#30340;&#36816;&#21160;&#36712;&#36857;&#27573;&#21487;&#29992;&#26102;&#37117;&#23384;&#22312;&#28431;&#27934;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#39044;&#27979;&#26041;&#27861;&#26469;&#39044;&#27979;&#26410;&#26469;&#29992;&#25143;&#34892;&#20026;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#30340;&#36712;&#36857;&#36827;&#34892;&#29992;&#25143;&#35748;&#35777;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21033;&#29992;&#20102;&#36825;&#26679;&#30340;&#35266;&#28857;&#65306;&#22312;&#20219;&#21153;&#29615;&#22659;&#20013;&#65292;&#26681;&#25454;&#29992;&#25143;&#24403;&#21069;&#30340;&#36712;&#36857;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#27979;&#29992;&#25143;&#30340;&#26410;&#26469;&#36712;&#36857;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#22826;&#21487;&#33021;&#22312;&#34892;&#20026;&#19978;&#21457;&#29983;&#21095;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-based behavioral biometric authentication of users interacting in virtual reality (VR) environments enables seamless continuous authentication by using only the motion trajectories of the person's body as a unique signature. Deep learning-based approaches for behavioral biometrics show high accuracy when using complete or near complete portions of the user trajectory, but show lower performance when using smaller segments from the start of the task. Thus, any systems designed with existing techniques are vulnerable while waiting for future segments of motion trajectories to become available. In this work, we present the first approach that predicts future user behavior using Transformer-based forecasting and using the forecasted trajectory to perform user authentication. Our work leverages the notion that given the current trajectory of a user in a task-based environment we can predict the future trajectory of the user as they are unlikely to dramatically shift their behavior sinc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#24182;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#65292;&#20294;&#23545;&#20110;PINNs&#21644;DeepONets&#31561;&#26041;&#27861;&#65292;&#21322;&#31934;&#24230;&#19981;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.16645</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#31934;&#24230;&#21152;&#36895;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#24182;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Speeding up and reducing memory usage for scientific machine learning via mixed precision. (arXiv:2401.16645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16645
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#24182;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#65292;&#20294;&#23545;&#20110;PINNs&#21644;DeepONets&#31561;&#26041;&#27861;&#65292;&#21322;&#31934;&#24230;&#19981;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#30340;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#38382;&#39064;&#30340;&#22810;&#21151;&#33021;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#28145;&#24230;&#31639;&#23376;&#32593;&#32476;&#65288;DeepONets&#65289;&#20197;&#20854;&#32467;&#21512;&#29289;&#29702;&#26041;&#31243;&#21644;&#23454;&#39564;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#33073;&#39062;&#32780;&#20986;&#65292;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20027;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;PINNs&#21644;DeepONets&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21253;&#25324;&#38271;&#26102;&#38388;&#30340;&#35745;&#31639;&#21644;&#22823;&#37327;&#30340;&#20869;&#23384;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#29992;&#21322;&#31934;&#24230;&#65288;float16&#65289;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#21333;&#31934;&#24230;&#65288;float32&#65289;&#25110;&#21452;&#31934;&#24230;&#65288;float64&#65289;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;float16&#19981;&#33021;&#24212;&#29992;&#20110;SciML&#26041;&#27861;&#65292;&#22240;&#20026;&#35757;&#32451;&#24320;&#22987;&#26102;&#26799;&#24230;&#21457;&#25955;&#65292;&#26435;&#37325;&#26356;&#26032;&#21464;&#20026;&#38646;&#65292;&#26080;&#27861;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific machine learning (SciML) has emerged as a versatile approach to address complex computational science and engineering problems. Within this field, physics-informed neural networks (PINNs) and deep operator networks (DeepONets) stand out as the leading techniques for solving partial differential equations by incorporating both physical equations and experimental data. However, training PINNs and DeepONets requires significant computational resources, including long computational times and large amounts of memory. In search of computational efficiency, training neural networks using half precision (float16) rather than the conventional single (float32) or double (float64) precision has gained substantial interest, given the inherent benefits of reduced computational time and memory consumed. However, we find that float16 cannot be applied to SciML methods, because of gradient divergence at the start of training, weight updates going to zero, and the inability to converge to a 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20026;&#20363;&#65292;&#21457;&#24067;&#22312;GitHub&#21644;Hugging Face&#19978;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.16640</link><description>&lt;p&gt;
TeenyTinyLlama&#65306;&#22522;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35757;&#32451;&#30340;&#24320;&#28304;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese. (arXiv:2401.16640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16640
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20026;&#20363;&#65292;&#21457;&#24067;&#22312;GitHub&#21644;Hugging Face&#19978;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#30340;&#36827;&#23637;&#36824;&#19981;&#24179;&#34913;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;LLMs&#26159;&#22312;&#20687;&#33521;&#35821;&#36825;&#26679;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#65292;&#20294;&#22810;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#31245;&#24046;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#22810;&#35821;&#35328;&#22522;&#30784;&#26377;&#26102;&#20250;&#38480;&#21046;&#23427;&#20204;&#20135;&#29983;&#30340;&#21103;&#20135;&#21697;&#65292;&#22914;&#35745;&#31639;&#38656;&#27714;&#21644;&#35768;&#21487;&#21046;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#20026;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20351;&#29992;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#36807;&#31243;&#12289;&#20854;&#23616;&#38480;&#24615;&#21644;&#20248;&#21183;&#12290;&#36825;&#23601;&#26159;TeenyTinyLlama&#65306;&#20004;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#29983;&#25104;&#30340;&#32039;&#20945;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;GitHub&#21644;Hugging Face&#19978;&#20197;&#23485;&#26494;&#30340;Apache 2.0&#35768;&#21487;&#35777;&#21457;&#24067;&#23427;&#20204;&#65292;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;&#35814;&#35265;https://github.com/Nkluge-correa/TeenyTinyLlama
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development. See https://github.com/Nkluge-correa/TeenyTinyLlama
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16635</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble. (arXiv:2401.16635v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;RLHF&#20381;&#36182;&#20110;&#36890;&#36807;&#26377;&#38480;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;RLHF&#21487;&#33021;&#20135;&#29983;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22870;&#21169;&#27169;&#22411;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#32771;&#34385;&#21040;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#21644;&#36164;&#28304;&#26114;&#36149;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21253;&#25324;&#32447;&#24615;&#23618;&#38598;&#25104;&#21644;&#22522;&#20110;LoRA&#30340;&#38598;&#25104;&#22312;&#20869;&#30340;&#39640;&#25928;&#38598;&#25104;&#26041;&#27861;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#38598;&#25104;&#22870;&#21169;&#27169;&#22411;&#36816;&#34892;Best-of-$n$&#21644;Proximal Policy Optimization&#65292;&#24182;&#39564;&#35777;&#25105;&#20204;&#30340;&#38598;&#25104;&#26041;&#27861;&#26377;&#21161;&#20110;&#25913;&#21892;RLHF&#36755;&#20986;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is a widely adopted approach for aligning large language models with human values. However, RLHF relies on a reward model that is trained with a limited amount of human preference data, which could lead to inaccurate predictions. As a result, RLHF may produce outputs that are misaligned with human values. To mitigate this issue, we contribute a reward ensemble method that allows the reward model to make more accurate predictions. As using an ensemble of large language model-based reward models can be computationally and resource-expensive, we explore efficient ensemble methods including linear-layer ensemble and LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy Optimization with our ensembled reward models, and verify that our ensemble methods help improve the alignment performance of RLHF outputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#33258;&#20027;&#39550;&#39542;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#29109;&#26597;&#35810;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#65292;&#21487;&#20197;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20943;&#23569;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#22810;&#26679;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#29109;&#26597;&#35810;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#36873;&#25321;&#22686;&#24378;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.16634</link><description>&lt;p&gt;
&#22312;&#20197;&#22823;&#25968;&#25454;&#20026;&#39537;&#21160;&#30340;&#33258;&#20027;&#39550;&#39542;&#20013;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;3D&#29289;&#20307;&#26816;&#27979;&#65306;&#19968;&#20010;&#23454;&#35777;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Why, When, and How to Use Active Learning in Large-Data-Driven 3D Object Detection for Safe Autonomous Driving: An Empirical Exploration. (arXiv:2401.16634v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#33258;&#20027;&#39550;&#39542;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#29109;&#26597;&#35810;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#65292;&#21487;&#20197;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20943;&#23569;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#22810;&#26679;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#29109;&#26597;&#35810;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#36873;&#25321;&#22686;&#24378;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#39550;&#39542;&#25968;&#25454;&#38598;&#20013;&#65292;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#36827;&#34892;3D&#29289;&#20307;&#26816;&#27979;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#12289;&#20887;&#20313;&#21644;&#39640;&#32500;&#25968;&#25454;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#29109;&#26597;&#35810;&#26469;&#36873;&#25321;&#20449;&#24687;&#26679;&#26412;&#65292;&#26088;&#22312;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;BEVFusion&#27169;&#22411;&#22312;nuScenes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#23558;&#20027;&#21160;&#23398;&#20064;&#19982;&#38543;&#26426;&#37319;&#26679;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#29109;&#26597;&#35810;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#24322;&#12290;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#22810;&#25968;&#31867;&#21644;&#23569;&#25968;&#31867;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#12290;&#31867;&#21035;&#29305;&#23450;&#20998;&#26512;&#25581;&#31034;&#20102;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#27169;&#22411;&#35757;&#32451;&#20998;&#37197;&#27880;&#37322;&#36164;&#28304;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#36873;&#25321;&#22810;&#26679;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29109;&#26597;&#35810;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#36873;&#25321;&#22686;&#24378;&#27169;&#22411;&#23398;&#20064;&#30340;&#25968;&#25454;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning strategies for 3D object detection in autonomous driving datasets may help to address challenges of data imbalance, redundancy, and high-dimensional data. We demonstrate the effectiveness of entropy querying to select informative samples, aiming to reduce annotation costs and improve model performance. We experiment using the BEVFusion model for 3D object detection on the nuScenes dataset, comparing active learning to random sampling and demonstrating that entropy querying outperforms in most cases. The method is particularly effective in reducing the performance gap between majority and minority classes. Class-specific analysis reveals efficient allocation of annotated resources for limited data budgets, emphasizing the importance of selecting diverse and informative data for model training. Our findings suggest that entropy querying is a promising strategy for selecting data that enhances model learning in resource-constrained environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#32500;&#28388;&#27874;&#22120;&#21644;&#20219;&#24847;&#27493;&#24133;&#30340;&#32447;&#24615;&#21367;&#31215;&#32593;&#32476;&#30340;&#20195;&#25968;&#22797;&#26434;&#24230;&#21644;&#31070;&#32463;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#36882;&#24402;&#31639;&#27861;&#21644;&#24230;&#37327;&#20195;&#25968;&#20960;&#20309;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#32593;&#32476;&#20248;&#21270;&#20013;&#30340;&#20851;&#38190;&#28857;&#25968;&#37327;&#26174;&#33879;&#36229;&#36807;&#20102;&#23436;&#20840;&#36830;&#25509;&#30340;&#32447;&#24615;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.16613</link><description>&lt;p&gt;
&#32447;&#24615;&#21367;&#31215;&#32593;&#32476;&#30340;&#20195;&#25968;&#22797;&#26434;&#24230;&#21644;&#31070;&#32463;&#22810;&#26679;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Algebraic Complexity and Neurovariety of Linear Convolutional Networks. (arXiv:2401.16613v1 [math.AG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#32500;&#28388;&#27874;&#22120;&#21644;&#20219;&#24847;&#27493;&#24133;&#30340;&#32447;&#24615;&#21367;&#31215;&#32593;&#32476;&#30340;&#20195;&#25968;&#22797;&#26434;&#24230;&#21644;&#31070;&#32463;&#22810;&#26679;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#36882;&#24402;&#31639;&#27861;&#21644;&#24230;&#37327;&#20195;&#25968;&#20960;&#20309;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#32593;&#32476;&#20248;&#21270;&#20013;&#30340;&#20851;&#38190;&#28857;&#25968;&#37327;&#26174;&#33879;&#36229;&#36807;&#20102;&#23436;&#20840;&#36830;&#25509;&#30340;&#32447;&#24615;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#32500;&#28388;&#27874;&#22120;&#21644;&#20219;&#24847;&#27493;&#24133;&#30340;&#32447;&#24615;&#21367;&#31215;&#32593;&#32476;&#12290;&#36825;&#31181;&#32593;&#32476;&#30340;&#31070;&#32463;&#27969;&#24418;&#26159;&#19968;&#20010;&#21322;&#20195;&#25968;&#38598;&#65292;&#30001;&#20855;&#26377;&#29305;&#23450;&#20998;&#35299;&#30340;&#22810;&#39033;&#24335;&#31354;&#38388;&#34920;&#31034;&#12290;&#36890;&#36807;&#24341;&#20837;&#36882;&#24402;&#31639;&#27861;&#65292;&#25105;&#20204;&#29983;&#25104;&#22810;&#39033;&#24335;&#26041;&#31243;&#65292;&#20854;&#20849;&#21516;&#38646;&#28857;&#23545;&#24212;&#20110;&#30456;&#24212;&#31070;&#32463;&#27969;&#24418;&#30340;Zariski&#38381;&#21253;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#24230;&#37327;&#20195;&#25968;&#20960;&#20309;&#30340;&#24037;&#20855;&#25506;&#32034;&#20102;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#30340;&#20195;&#25968;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#65292;&#20248;&#21270;&#36825;&#31181;&#32593;&#32476;&#26102;&#25152;&#26377;&#22797;&#26434;&#20851;&#38190;&#28857;&#30340;&#25968;&#37327;&#31561;&#20110;&#19968;&#20010;Segre&#31751;&#30340;&#36890;&#29992;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#24230;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;&#25968;&#37327;&#26174;&#33879;&#36229;&#36807;&#20102;&#20351;&#29992;&#30456;&#21516;&#21442;&#25968;&#20010;&#25968;&#35757;&#32451;&#23436;&#20840;&#36830;&#25509;&#30340;&#32447;&#24615;&#32593;&#32476;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#28857;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study linear convolutional networks with one-dimensional filters and arbitrary strides. The neuromanifold of such a network is a semialgebraic set, represented by a space of polynomials admitting specific factorizations. Introducing a recursive algorithm, we generate polynomial equations whose common zero locus corresponds to the Zariski closure of the corresponding neuromanifold. Furthermore, we explore the algebraic complexity of training these networks employing tools from metric algebraic geometry. Our findings reveal that the number of all complex critical points in the optimization of such a network is equal to the generic Euclidean distance degree of a Segre variety. Notably, this count significantly surpasses the number of critical points encountered in the training of a fully connected linear network with the same number of parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36125;&#21494;&#26031;&#20272;&#35745;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36870;&#38382;&#39064;&#20013;&#30340;&#31232;&#30095;&#24314;&#27169;&#21644;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16612</link><description>&lt;p&gt;
&#22312;&#36870;&#38382;&#39064;&#20013;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#29289;&#36827;&#34892;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning a Gaussian Mixture for Sparsity Regularization in Inverse Problems. (arXiv:2401.16612v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36125;&#21494;&#26031;&#20272;&#35745;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36870;&#38382;&#39064;&#20013;&#30340;&#31232;&#30095;&#24314;&#27169;&#21644;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36870;&#38382;&#39064;&#20013;&#65292;&#24191;&#27867;&#35748;&#20026;&#24341;&#20837;&#31232;&#30095;&#20808;&#39564;&#23545;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#27491;&#21017;&#21270;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#22522;&#20110;&#19968;&#20010;&#20808;&#39564;&#20551;&#35774;&#65292;&#21363;&#26410;&#30693;&#37327;&#21487;&#20197;&#22312;&#19968;&#20010;&#26377;&#38480;&#25968;&#37327;&#30340;&#26174;&#33879;&#25104;&#20998;&#30340;&#22522;&#30784;&#19978;&#36866;&#24403;&#34920;&#31034;&#65292;&#32780;&#22823;&#22810;&#25968;&#31995;&#25968;&#25509;&#36817;&#20110;&#38646;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#27604;&#22914;&#20998;&#27573;&#24179;&#28369;&#20449;&#21495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#39640;&#26031;&#36864;&#21270;&#28151;&#21512;&#29289;&#24418;&#24335;&#34920;&#36848;&#30340;&#27010;&#29575;&#31232;&#30095;&#20808;&#39564;&#65292;&#33021;&#22815;&#23545;&#20110;&#20219;&#24847;&#22522;&#36827;&#34892;&#31232;&#30095;&#24314;&#27169;&#12290;&#22312;&#36825;&#20010;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#20197;&#35299;&#37322;&#20026;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#20272;&#35745;&#36825;&#20010;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19982;&#24120;&#29992;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#25968;&#20540;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In inverse problems, it is widely recognized that the incorporation of a sparsity prior yields a regularization effect on the solution. This approach is grounded on the a priori assumption that the unknown can be appropriately represented in a basis with a limited number of significant components, while most coefficients are close to zero. This occurrence is frequently observed in real-world scenarios, such as with piecewise smooth signals. In this study, we propose a probabilistic sparsity prior formulated as a mixture of degenerate Gaussians, capable of modeling sparsity with respect to a generic basis. Under this premise, we design a neural network that can be interpreted as the Bayes estimator for linear inverse problems. Additionally, we put forth both a supervised and an unsupervised training strategy to estimate the parameters of this network. To evaluate the effectiveness of our approach, we conduct a numerical comparison with commonly employed sparsity-promoting regularization
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37319;&#29992;&#38750;&#20256;&#32479;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#32467;&#21512;&#35843;&#25511;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#30005;&#23376;&#22768;&#23376;&#35889;&#20989;&#25968;&#65292;&#24182;&#24471;&#21040;&#20102;&#36229;&#23548;&#26448;&#26009;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#21152;&#36895;&#20102;&#36229;&#23548;&#20307;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.16611</link><description>&lt;p&gt;
&#36890;&#36807;&#35843;&#25511;&#28145;&#24230;&#23398;&#20064;&#30340;&#30005;&#23376;&#22768;&#23376;&#35889;&#20989;&#25968;&#26469;&#21152;&#36895;&#36229;&#23548;&#20307;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Accelerating superconductor discovery through tempered deep learning of the electron-phonon spectral function. (arXiv:2401.16611v1 [cond-mat.supr-con])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;&#38750;&#20256;&#32479;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#32467;&#21512;&#35843;&#25511;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#30005;&#23376;&#22768;&#23376;&#35889;&#20989;&#25968;&#65292;&#24182;&#24471;&#21040;&#20102;&#36229;&#23548;&#26448;&#26009;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#21152;&#36895;&#20102;&#36229;&#23548;&#20307;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#23547;&#25214;&#26032;&#30340;&#30005;&#23376;&#22768;&#23376;&#36229;&#23548;&#20307;&#32467;&#21512;&#36215;&#26469;&#65292;&#26159;&#19968;&#20010;&#34028;&#21187;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#35745;&#31639;&#30005;&#23376;&#22768;&#23376;&#35889;&#20989;&#25968;$\alpha^2F(\omega)$&#30340;&#35745;&#31639;&#24378;&#24230;&#65292;&#36825;&#26159; Midgal-Eliashberg &#36229;&#23548;&#29702;&#35770;&#30340;&#22522;&#26412;&#22240;&#32032;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#27493;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;818&#20010;&#21160;&#24577;&#31283;&#23450;&#30340;&#26448;&#26009;&#35745;&#31639;&#20102;$\alpha^2F(\omega)$&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#38750;&#20256;&#32479;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#29992;&#28151;&#21512;&#27169;&#22411;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#39044;&#27979;$\alpha^2F(\omega)$&#65292;&#20197;&#35843;&#33410;&#27169;&#22411;&#30340;&#36807;&#24230;&#25311;&#21512;&#65292;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992; TEMNets&#65288;&#28201;&#24230;&#22343;&#34913;&#19979;&#30340; Bootstrapped Ensemble of Equivariant graph neural networks&#65289;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24471;&#21040;&#20102;&#26469;&#33258;$\alpha^2F(\omega)$&#30340; Eliashberg &#30697;&#30340; MAE&#65292;&#20998;&#21035;&#20026;0.21&#65292;45 K &#21644; 43 K: $\lambda$&#65292;$\omega_{\log}$ &#21644; $\omega_{2}$&#65292;&#23545;&#24212;&#30340;&#20020;&#30028;&#28201;&#24230;$T_c$&#30340; MAE &#20026;2.5 K&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#39046;&#22495;&#20449;&#24687;&#32467;&#21512;&#36827;&#26469;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating deep learning with the search for new electron-phonon superconductors represents a burgeoning field of research, where the primary challenge lies in the computational intensity of calculating the electron-phonon spectral function, $\alpha^2F(\omega)$, the essential ingredient of Midgal-Eliashberg theory of superconductivity. To overcome this challenge, we adopt a two-step approach. First, we compute $\alpha^2F(\omega)$ for 818 dynamically stable materials. We then train a deep-learning model to predict $\alpha^2F(\omega)$, using an unconventional training strategy to temper the model's overfitting, enhancing predictions. Specifically, we train a Bootstrapped Ensemble of Tempered Equivariant graph neural NETworks (BETE-NET), obtaining an MAE of 0.21, 45 K, and 43 K for the Eliashberg moments derived from $\alpha^2F(\omega)$: $\lambda$, $\omega_{\log}$, and $\omega_{2}$, respectively, yielding an MAE of 2.5 K for the critical temperature, $T_c$. Further, we incorporate domain
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#19968;&#33268;&#31639;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#23439;&#35266;at-$k$&#24230;&#37327;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20154;&#21475;&#25928;&#29992;&#26694;&#26550;&#19979;&#32771;&#34385;&#22797;&#26434;&#24615;&#33021;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#35813;&#31639;&#27861;&#22312;&#26497;&#31471;&#20998;&#31867;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.16594</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#19968;&#33268;&#31639;&#27861;: &#23439;&#35266;at-$k$&#24230;&#37327;.
&lt;/p&gt;
&lt;p&gt;
Consistent algorithms for multi-label classification with macro-at-$k$ metrics. (arXiv:2401.16594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16594
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#19968;&#33268;&#31639;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#23439;&#35266;at-$k$&#24230;&#37327;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#20154;&#21475;&#25928;&#29992;&#26694;&#26550;&#19979;&#32771;&#34385;&#22797;&#26434;&#24615;&#33021;&#24230;&#37327;&#30340;&#20248;&#21270;&#65292;&#35813;&#31639;&#27861;&#22312;&#26497;&#31471;&#20998;&#31867;&#38382;&#39064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#20154;&#21475;&#25928;&#29992;&#26694;&#26550;&#19979;&#32771;&#34385;&#20102;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#22797;&#26434;&#24615;&#33021;&#24230;&#37327;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#23558;&#24230;&#37327;&#32447;&#24615;&#20998;&#35299;&#20026;&#27599;&#20010;&#26631;&#31614;&#20998;&#21035;&#24212;&#29992;&#30340;&#20108;&#20998;&#31867;&#25928;&#29992;&#30340;&#24635;&#21644;&#65292;&#24182;&#23545;&#27599;&#20010;&#23454;&#20363;&#39044;&#27979;&#24688;&#22909;&#26377;$k$&#20010;&#26631;&#31614;&#30340;&#39069;&#22806;&#35201;&#27714;&#12290;&#8220;&#23439;&#35266;at-$k$&#8221;&#24230;&#37327;&#22312;&#20855;&#26377;&#38271;&#23614;&#26631;&#31614;&#30340;&#26497;&#31471;&#20998;&#31867;&#38382;&#39064;&#20013;&#20855;&#26377;&#29702;&#24819;&#30340;&#23646;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;at-$k$&#32422;&#26463;&#23558;&#21407;&#26412;&#29420;&#31435;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#27604;&#26631;&#20934;&#23439;&#24179;&#22343;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#35745;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#21644;&#24418;&#24335;&#65292;&#24182;&#22522;&#20110;Frank-Wolfe&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#19968;&#33268;&#19988;&#23454;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#36824;&#28041;&#21450;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#26356;&#19968;&#33324;&#24230;&#37327;&#65292;&#36825;&#20123;&#20989;&#25968;&#26159;&#25353;&#26631;&#31614;&#36827;&#34892;&#30340;&#28151;&#28102;&#30697;&#38453;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the optimization of complex performance metrics in multi-label classification under the population utility framework. We mainly focus on metrics linearly decomposable into a sum of binary classification utilities applied separately to each label with an additional requirement of exactly $k$ labels predicted for each instance. These "macro-at-$k$" metrics possess desired properties for extreme classification problems with long tail labels. Unfortunately, the at-$k$ constraint couples the otherwise independent binary classification tasks, leading to a much more challenging optimization problem than standard macro-averages. We provide a statistical framework to study this problem, prove the existence and the form of the optimal classifier, and propose a statistically consistent and practical learning algorithm based on the Frank-Wolfe method. Interestingly, our main results concern even more general metrics being non-linear functions of label-wise confusion matrices. Empirical
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#24565;&#31354;&#38388;&#30340;&#35821;&#20041;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#26080;&#27861;&#25429;&#25417;&#21644;&#37327;&#21270;&#8220;&#24847;&#20041;&#8221;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16569</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#27010;&#24565;&#31354;&#38388;&#35821;&#20041;&#36890;&#20449;&#30340;&#39046;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-Based Domain Learning for Semantic Communication with Conceptual Spaces. (arXiv:2401.16569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16569
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#24565;&#31354;&#38388;&#30340;&#35821;&#20041;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#26080;&#27861;&#25429;&#25417;&#21644;&#37327;&#21270;&#8220;&#24847;&#20041;&#8221;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20934;&#30830;&#20256;&#36882;&#31526;&#21495;&#30456;&#27604;&#65292;&#20197;&#20934;&#30830;&#20256;&#36882;&#24847;&#20041;&#20026;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#24050;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;&#36825;&#31181;&#33539;&#24335;&#36890;&#24120;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#29616;&#20195;&#21457;&#23637;&#65292;&#20197;&#25552;&#39640;&#36890;&#20449;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25429;&#25417;&#21644;&#37327;&#21270;&#8220;&#24847;&#20041;&#8221;&#30340;&#32454;&#33410;&#32570;&#20047;&#19968;&#20010;&#26631;&#20934;&#27169;&#22411;&#65292;&#35768;&#22810;&#39046;&#20808;&#30340;&#35821;&#20041;&#36890;&#20449;&#26041;&#27861;&#37319;&#29992;&#40657;&#30418;&#26694;&#26550;&#65292;&#23545;&#27169;&#22411;&#30340;&#20855;&#20307;&#23398;&#20064;&#20869;&#23481;&#30693;&#20043;&#29978;&#23569;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#27010;&#24565;&#31354;&#38388;&#26694;&#26550;&#65292;&#20197;&#20960;&#20309;&#26041;&#24335;&#26126;&#30830;&#24314;&#27169;&#24847;&#20041;&#12290;&#34429;&#28982;&#20197;&#21069;&#20351;&#29992;&#27010;&#24565;&#31354;&#38388;&#30740;&#31350;&#35821;&#20041;&#36890;&#20449;&#30340;&#24037;&#20316;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#20808;&#21069;&#30340;&#23581;&#35797;&#28041;&#21450;&#25163;&#24037;&#21046;&#20316;&#27010;&#24565;&#31354;&#38388;&#27169;&#22411;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#27010;&#24565;&#31354;&#38388;&#35821;&#20041;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication with the goal of accurately conveying meaning, rather than accurately transmitting symbols, has become an area of growing interest. This paradigm, termed semantic communication, typically leverages modern developments in artificial intelligence and machine learning to improve the efficiency and robustness of communication systems. However, a standard model for capturing and quantifying the details of "meaning" is lacking, with many leading approaches to semantic communication adopting a black-box framework with little understanding of what exactly the model is learning. One solution is to utilize the conceptual spaces framework, which models meaning explicitly in a geometric manner. Though prior work studying semantic communication with conceptual spaces has shown promising results, these previous attempts involve hand-crafting a conceptual space model, severely limiting the scalability and practicality of the approach. In this work, we develop a framework for learning a 
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#35843;&#30740;&#65292;&#26088;&#22312;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.16549</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Multi-Label Learning: A Comprehensive Survey. (arXiv:2401.16549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16549
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#35843;&#30740;&#65292;&#26088;&#22312;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#22312;&#35299;&#20915;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#23398;&#20064;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#20174;&#21333;&#20010;&#36755;&#20837;&#25968;&#25454;&#28857;&#20013;&#39044;&#27979;&#22810;&#20010;&#26631;&#31614;&#12290;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#28041;&#21450;&#22810;&#26631;&#31614;&#20998;&#31867;&#25110;&#25490;&#21517;&#30340;&#20219;&#21153;&#24102;&#26469;&#20102;&#37325;&#22823;&#32780;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#22810;&#26631;&#31614;&#20998;&#31867;&#38754;&#20020;&#30340;&#22256;&#38590;&#21253;&#25324;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#35299;&#20915;&#26631;&#31614;&#30456;&#20851;&#24615;&#21644;&#22788;&#29702;&#37096;&#20998;&#26631;&#31614;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#26356;&#26377;&#25928;&#22320;&#24212;&#23545;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#32508;&#21512;&#30740;&#31350;&#36824;&#27604;&#36739;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26412;&#35843;&#30740;&#26088;&#22312;&#20840;&#38754;&#23457;&#35270;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label learning is a rapidly growing research area that aims to predict multiple labels from a single input data point. In the era of big data, tasks involving multi-label classification (MLC) or ranking present significant and intricate challenges, capturing considerable attention in diverse domains. Inherent difficulties in MLC include dealing with high-dimensional data, addressing label correlations, and handling partial labels, for which conventional methods prove ineffective. Recent years have witnessed a notable increase in adopting deep learning (DL) techniques to address these challenges more effectively in MLC. Notably, there is a burgeoning effort to harness the robust learning capabilities of DL for improved modelling of label dependencies and other challenges in MLC. However, it is noteworthy that comprehensive studies specifically dedicated to DL for multi-label learning are limited. Thus, this survey aims to thoroughly review recent progress in DL for multi-label lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35266;&#23519;&#31383;&#21475;&#21010;&#20998;&#20026;&#26102;&#38388;&#27573;&#65292;&#36890;&#36807;&#20248;&#21270;&#39640;&#20248;&#20808;&#32423;&#29305;&#24449;&#30340;&#26102;&#38388;bin&#22823;&#23567;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31616;&#21333;&#12289;&#26356;&#24555;&#36895;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#36798;&#21040;&#19982;&#26356;&#22797;&#26434;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16537</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#34892;&#25919;&#25968;&#25454;&#26426;&#22120;&#23398;&#20064;&#35266;&#23519;&#26102;&#38388;&#31383;&#21475;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Efficient Observation Time Window Segmentation for Administrative Data Machine Learning. (arXiv:2401.16537v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35266;&#23519;&#31383;&#21475;&#21010;&#20998;&#20026;&#26102;&#38388;&#27573;&#65292;&#36890;&#36807;&#20248;&#21270;&#39640;&#20248;&#20808;&#32423;&#29305;&#24449;&#30340;&#26102;&#38388;bin&#22823;&#23567;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#31616;&#21333;&#12289;&#26356;&#24555;&#36895;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#36798;&#21040;&#19982;&#26356;&#22797;&#26434;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#34892;&#25919;&#25968;&#25454;&#39044;&#27979;&#32467;&#26524;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#39046;&#22495;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#12290;&#22823;&#22810;&#25968;&#34892;&#25919;&#25968;&#25454;&#35760;&#24405;&#37117;&#26377;&#26102;&#38388;&#25139;&#65292;&#35760;&#24405;&#38543;&#26102;&#38388;&#30340;&#27169;&#24335;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20851;&#38190;&#36755;&#20837;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35266;&#23519;&#31383;&#21475;&#21010;&#20998;&#20026;&#26102;&#38388;&#27573;&#25110;&#8220;bins&#8221;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#39640;&#25928;&#30340;&#36807;&#31243;&#65292;&#21487;&#20197;&#30830;&#23450;&#21738;&#20123;&#25968;&#25454;&#29305;&#24449;&#26368;&#36866;&#21512;&#36739;&#23567;&#30340;&#65292;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#26102;&#38388;&#27573;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#20303;&#25151;/&#26080;&#23478;&#21487;&#24402;&#30340;&#34892;&#25919;&#25968;&#25454;&#19978;&#20135;&#29983;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20854;&#20182;&#29305;&#24449;&#20351;&#29992;&#21333;&#20010;&#26102;&#38388;bin&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#21270;&#36825;&#20123;&#39640;&#20248;&#20808;&#32423;&#29305;&#24449;&#30340;&#26102;&#38388;bin&#22823;&#23567;&#21487;&#20197;&#23454;&#29616;&#26356;&#31616;&#21333;&#65292;&#26356;&#24555;&#36895;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#23454;&#29616;&#19982;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#27169;&#22411;&#40664;&#35748;&#23558;&#25152;&#26377;&#25968;&#25454;&#29305;&#24449;&#29992;&#30456;&#21516;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing administrative data to predict outcomes is an important application area of machine learning, particularly in healthcare. Most administrative data records are timestamped and the pattern of records over time is a key input for machine learning models. This paper explores how best to divide the observation window of a machine learning model into time segments or "bins". A computationally efficient process is presented that identifies which data features benefit most from smaller, higher resolution time segments. Results generated on healthcare and housing/homelessness administrative data demonstrate that optimizing the time bin size of these high priority features while using a single time bin for the other features achieves machine learning models that are simpler and quicker to train. This approach also achieves similar and sometimes better performance than more complex models that default to representing all data features with the same time resolution.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.16521</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#30340;&#39564;&#35777;&#12289;&#20581;&#22766;&#24615;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Validation, Robustness, and Accuracy of Perturbation-Based Sensitivity Analysis Methods for Time-Series Deep Learning Models. (arXiv:2401.16521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;&#25935;&#24863;&#24615;&#20998;&#26512;&#35780;&#20272;&#36755;&#20837;&#21464;&#21270;&#23545;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#26159;&#35299;&#37322;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#21518;&#26399;&#35299;&#37322;&#26041;&#27861;&#20013;&#65292;&#22914;&#21453;&#21521;&#20256;&#25773;&#12289;&#25200;&#21160;&#21644;&#36817;&#20284;&#27861;&#20013;&#65292;&#26412;&#30740;&#31350;&#23558;&#35843;&#26597;&#29616;&#20195;Transformer&#27169;&#22411;&#19978;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#22238;&#31572;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;1&#65289;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#26159;&#21542;&#20135;&#29983;&#21487;&#27604;&#36739;&#30340;&#36755;&#20986;&#21644;&#23646;&#24615;&#37325;&#35201;&#24615;&#25490;&#24207;&#65311;2&#65289;&#20351;&#29992;&#30456;&#21516;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#23545;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#36755;&#20986;&#20135;&#29983;&#24433;&#21709;&#65311;3&#65289;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#30340;&#32467;&#26524;&#19982;&#22522;&#26412;&#20107;&#23454;&#30340;&#19968;&#33268;&#24615;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
This work undertakes studies to evaluate Interpretability Methods for Time-Series Deep Learning. Sensitivity analysis assesses how input changes affect the output, constituting a key component of interpretation. Among the post-hoc interpretation methods such as back-propagation, perturbation, and approximation, my work will investigate perturbation-based sensitivity Analysis methods on modern Transformer models to benchmark their performances. Specifically, my work answers three research questions: 1) Do different sensitivity analysis (SA) methods yield comparable outputs and attribute importance rankings? 2) Using the same sensitivity analysis method, do different Deep Learning (DL) models impact the output of the sensitivity analysis? 3) How well do the results from sensitivity analysis methods align with the ground truth?
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MT-HCCAR&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20113;&#23646;&#24615;&#26816;&#32034;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#20113;&#23646;&#24615;&#26816;&#32034;&#20219;&#21153;&#20043;&#38388;&#30340;&#23618;&#32423;&#20851;&#31995;&#65292;&#24182;&#20855;&#26377;&#23545;&#19981;&#21516;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#20855;&#26377;&#20581;&#22766;&#27867;&#21270;&#33021;&#21147;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.16520</link><description>&lt;p&gt;
MT-HCCAR: &#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#19982;&#23618;&#32423;&#20998;&#31867;&#30340;&#27880;&#24847;&#21147;&#22238;&#24402;&#29992;&#20110;&#20113;&#23646;&#24615;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
MT-HCCAR: Multi-Task Deep Learning with Hierarchical Classification and Attention-based Regression for Cloud Property Retrieval. (arXiv:2401.16520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16520
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MT-HCCAR&#30340;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20113;&#23646;&#24615;&#26816;&#32034;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#20113;&#23646;&#24615;&#26816;&#32034;&#20219;&#21153;&#20043;&#38388;&#30340;&#23618;&#32423;&#20851;&#31995;&#65292;&#24182;&#20855;&#26377;&#23545;&#19981;&#21516;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#20855;&#26377;&#20581;&#22766;&#27867;&#21270;&#33021;&#21147;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#25928;&#30340;&#20113;&#23646;&#24615;&#26816;&#32034;&#21253;&#25324;&#20113;&#36974;&#34109;&#12289;&#20113;&#30456;&#20998;&#31867;&#21644;&#20113;&#20809;&#23398;&#21402;&#24230;&#65288;COT&#65289;&#39044;&#27979;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#20256;&#24863;&#22120;&#20202;&#22120;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#29420;&#29305;&#30340;&#20809;&#35889;&#29305;&#24449;&#12290;&#26368;&#36817;&#65292;&#22312;&#22320;&#29699;&#31185;&#23398;&#30740;&#31350;&#20013;&#37319;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#21355;&#26143;&#25968;&#25454;&#38598;&#30340;&#20809;&#35889;&#35266;&#27979;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#32570;&#20047;&#32771;&#34385;&#26816;&#32034;&#20219;&#21153;&#20043;&#38388;&#23618;&#32423;&#20851;&#31995;&#30340;&#21019;&#26032;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#29616;&#26377;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20809;&#35889;&#22810;&#26679;&#24615;&#65292;&#24320;&#21457;&#20855;&#26377;&#23545;&#19981;&#21516;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#20855;&#26377;&#20581;&#22766;&#27867;&#21270;&#33021;&#21147;&#30340;&#27169;&#22411;&#26159;&#24517;&#35201;&#30340;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#30446;&#21069;&#32570;&#20047;&#35299;&#20915;&#22810;&#26679;&#25968;&#25454;&#38598;&#19979;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;MT-HCCAR&#65292;&#36825;&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of Earth science, effective cloud property retrieval, encompassing cloud masking, cloud phase classification, and cloud optical thickness (COT) prediction, remains pivotal. Traditional methodologies necessitate distinct models for each sensor instrument due to their unique spectral characteristics. Recent strides in Earth Science research have embraced machine learning and deep learning techniques to extract features from satellite datasets' spectral observations. However, prevailing approaches lack novel architectures accounting for hierarchical relationships among retrieval tasks. Moreover, considering the spectral diversity among existing sensors, the development of models with robust generalization capabilities over different sensor datasets is imperative. Surprisingly, there is a dearth of methodologies addressing the selection of an optimal model for diverse datasets. In response, this paper introduces MT-HCCAR, an end-to-end deep learning model employing multi-task 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;AFSD-Physics&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#23454;&#39564;&#25968;&#25454;&#65292;&#24471;&#21040;&#20102;&#28155;&#21152;&#25705;&#25830;&#25605;&#25292;&#22534;&#31215;&#36807;&#31243;&#20013;&#28201;&#24230;&#28436;&#21464;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#12289;&#35745;&#31639;&#25104;&#26412;&#20302;&#19988;&#20934;&#30830;&#24230;&#39640;&#65292;&#19982;&#23454;&#38469;&#27979;&#37327;&#32467;&#26524;&#21563;&#21512;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.16501</link><description>&lt;p&gt;
AFSD-Physics&#65306;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#26041;&#27861;&#25506;&#32034;&#28155;&#21152;&#25705;&#25830;&#25605;&#25292;&#22534;&#31215;&#36807;&#31243;&#20013;&#28201;&#24230;&#28436;&#21464;&#30340;&#25511;&#21046;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
AFSD-Physics: Exploring the governing equations of temperature evolution during additive friction stir deposition by a human-AI teaming approach. (arXiv:2401.16501v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;AFSD-Physics&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#23454;&#39564;&#25968;&#25454;&#65292;&#24471;&#21040;&#20102;&#28155;&#21152;&#25705;&#25830;&#25605;&#25292;&#22534;&#31215;&#36807;&#31243;&#20013;&#28201;&#24230;&#28436;&#21464;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#12289;&#35745;&#31639;&#25104;&#26412;&#20302;&#19988;&#20934;&#30830;&#24230;&#39640;&#65292;&#19982;&#23454;&#38469;&#27979;&#37327;&#32467;&#26524;&#21563;&#21512;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#26041;&#27861;&#30740;&#31350;&#28155;&#21152;&#25705;&#25830;&#25605;&#25292;&#22534;&#31215;&#65288;AFSD&#65289;&#36807;&#31243;&#20013;&#28201;&#24230;&#28436;&#21464;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;AFSD&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#22266;&#24577;&#22686;&#26448;&#21046;&#36896;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#29076;&#34701;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26448;&#26009;&#22534;&#31215;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#35813;&#36807;&#31243;&#30340;&#24314;&#27169;&#20197;&#21450;AFSD&#24037;&#20855;&#30340;&#24314;&#27169;&#36824;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#12290;&#24471;&#21040;&#30340;&#20154;&#24037;&#26234;&#33021;&#23398;&#20064;&#26041;&#27861;&#34987;&#21629;&#21517;&#20026;AFSD-Physics&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#24037;&#20855;&#21644;&#22534;&#31215;&#36807;&#31243;&#20013;&#28201;&#24230;&#28436;&#21464;&#30340;&#25511;&#21046;&#26041;&#31243;&#65292;&#36890;&#36807;&#36807;&#31243;&#20013;&#30340;&#27979;&#37327;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#12290;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#37319;&#38598;&#20102;30&#23618;&#38109;7075&#26448;&#26009;&#22534;&#31215;&#36807;&#31243;&#20013;&#30340;&#27979;&#37327;&#25968;&#25454;&#12290;&#24471;&#21040;&#30340;&#25511;&#21046;&#26041;&#31243;&#26159;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#12289;&#35745;&#31639;&#25104;&#26412;&#20302;&#19988;&#20934;&#30830;&#24230;&#39640;&#30340;&#27169;&#22411;&#12290;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#19982;&#27979;&#37327;&#32467;&#26524;&#20855;&#26377;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a modeling effort to explore the underlying physics of temperature evolution during additive friction stir deposition (AFSD) by a human-AI teaming approach. AFSD is an emerging solid-state additive manufacturing technology that deposits materials without melting. However, both process modeling and modeling of the AFSD tool are at an early stage. In this paper, a human-AI teaming approach is proposed to combine models based on first principles with AI. The resulting human-informed machine learning method, denoted as AFSD-Physics, can effectively learn the governing equations of temperature evolution at the tool and the build from in-process measurements. Experiments are designed and conducted to collect in-process measurements for the deposition of aluminum 7075 with a total of 30 layers. The acquired governing equations are physically interpretable models with low computational cost and high accuracy. Model predictions show good agreement with the measurements. Expe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LDGD&#30340;&#21028;&#21035;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16497</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#21028;&#21035;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Discriminative Bayesian Gaussian Process Latent Variable Model for High-Dimensional Data. (arXiv:2401.16497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LDGD&#30340;&#21028;&#21035;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24314;&#27169;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#25968;&#25454;&#34987;&#22122;&#22768;&#24178;&#25200;&#25110;&#20197;&#19981;&#21516;&#30340;&#27169;&#24577;&#34920;&#31034;&#26102;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#23558;&#39640;&#32500;&#25968;&#25454;&#26144;&#23556;&#21040;&#28508;&#22312;&#30340;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;&#36825;&#20010;&#27169;&#22411;&#34987;&#21629;&#21517;&#20026;&#28508;&#22312;&#21028;&#21035;&#29983;&#25104;&#35299;&#30721;&#22120;&#65288;LDGD&#65289;&#65292;&#23427;&#22312;&#27969;&#24418;&#21457;&#29616;&#36807;&#31243;&#20013;&#21033;&#29992;&#20102;&#25968;&#25454;&#65288;&#25110;&#20854;&#29305;&#24449;&#65289;&#21644;&#30456;&#20851;&#26631;&#31614;&#65288;&#22914;&#31867;&#21035;&#25110;&#21050;&#28608;&#65289;&#12290;&#20026;&#20102;&#25512;&#26029;&#28508;&#22312;&#21464;&#37327;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#35299;&#65292;&#20351;&#24471;LDGD&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;LDGD&#30340;&#24212;&#29992;&#12290;LDGD&#19981;&#20165;&#33021;&#20934;&#30830;&#22320;&#25512;&#26029;&#27969;&#24418;&#65292;&#32780;&#19988;&#22312;&#39044;&#27979;&#26631;&#31614;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting meaningful information from high-dimensional data poses a formidable modeling challenge, particularly when the data is obscured by noise or represented through different modalities. In this research, we propose a novel non-parametric modeling approach, leveraging the Gaussian Process (GP), to characterize high-dimensional data by mapping it to a latent low-dimensional manifold. This model, named the Latent Discriminative Generative Decoder (LDGD), utilizes both the data (or its features) and associated labels (such as category or stimulus) in the manifold discovery process. To infer the latent variables, we derive a Bayesian solution, allowing LDGD to effectively capture inherent uncertainties in the data while enhancing the model's predictive accuracy and robustness. We demonstrate the application of LDGD on both synthetic and benchmark datasets. Not only does LDGD infer the manifold accurately, but its prediction accuracy in anticipating labels surpasses state-of-the-art a
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GPU&#38598;&#32676;&#35843;&#24230;&#22120;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#26681;&#25454;&#20219;&#21153;&#23545;&#36890;&#20449;&#32593;&#32476;&#24310;&#36831;&#30340;&#25935;&#24863;&#24615;&#36827;&#34892;GPU&#36164;&#28304;&#30340;&#37051;&#36817;&#22522;&#30784;&#19968;&#33268;&#24615;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#35843;&#24230;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#35843;&#24230;&#22120;&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;69&#65285;&#30340;&#31471;&#21040;&#31471;Makespan&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.16492</link><description>&lt;p&gt;
GPU&#38598;&#32676;&#35843;&#24230;&#23545;&#32593;&#32476;&#25935;&#24863;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GPU Cluster Scheduling for Network-Sensitive Deep Learning. (arXiv:2401.16492v1 [cs.PF])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16492
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GPU&#38598;&#32676;&#35843;&#24230;&#22120;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#65292;&#26681;&#25454;&#20219;&#21153;&#23545;&#36890;&#20449;&#32593;&#32476;&#24310;&#36831;&#30340;&#25935;&#24863;&#24615;&#36827;&#34892;GPU&#36164;&#28304;&#30340;&#37051;&#36817;&#22522;&#30784;&#19968;&#33268;&#24615;&#12290;&#30456;&#27604;&#20256;&#32479;&#30340;&#35843;&#24230;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#35843;&#24230;&#22120;&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;69&#65285;&#30340;&#31471;&#21040;&#31471;Makespan&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GPU&#38598;&#32676;&#35843;&#24230;&#22120;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;DL&#65288;DDL&#65289;&#24037;&#20316;&#36127;&#36733;&#65292;&#20197;&#22522;&#20110;DDL&#20316;&#19994;&#23545;&#39044;&#26399;&#36890;&#20449;&#32593;&#32476;&#24310;&#36831;&#30340;&#25935;&#24863;&#24615;&#36827;&#34892;GPU&#36164;&#28304;&#30340;&#37051;&#36817;&#22522;&#30784;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#35843;&#24230;&#22120;&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;i&#65289;&#19968;&#20010;&#32463;&#20856;&#30340;&#24310;&#36831;&#35843;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#20419;&#36827;&#20316;&#19994;&#25918;&#32622;&#21644;&#19968;&#33268;&#24615;&#65307;&#65288;ii&#65289;&#19968;&#20010;&#23545;&#32593;&#32476;&#25935;&#24863;&#30340;&#20316;&#19994;&#25250;&#21344;&#31574;&#30053;&#65307;&#21644;&#65288;iii&#65289;&#19968;&#31181;&#8220;&#33258;&#21160;&#35843;&#25972;&#22120;&#8221;&#26426;&#21046;&#65292;&#29992;&#20110;&#20248;&#21270;&#24310;&#36831;&#35745;&#26102;&#22120;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#24310;&#36831;&#35843;&#24230;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;DDL&#38598;&#32676;&#20223;&#30495;&#24179;&#21488;&#12290;&#36890;&#36807;&#20351;&#29992;&#20223;&#30495;&#24179;&#21488;&#65292;&#25105;&#20204;&#22312;&#23454;&#38469;&#24037;&#20316;&#36127;&#36733;&#36319;&#36394;&#20013;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#35774;&#35745;&#30340;&#20248;&#21183;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#19968;&#33268;&#24615;&#35843;&#24230;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#35843;&#24230;&#22120;&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;69&#65285;&#30340;&#31471;&#21040;&#31471;Makespan&#25552;&#21319;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#24179;&#22343;j
&lt;/p&gt;
&lt;p&gt;
We propose a novel GPU-cluster scheduler for distributed DL (DDL) workloads that enables proximity based consolidation of GPU resources based on the DDL jobs' sensitivities to the anticipated communication-network delays. Our scheduler consists of three major components: (i) a classical delay scheduling algorithm to facilitate job placement and consolidation; (ii) a network-sensitive job preemption strategy; and (iii) an "auto-tuner" mechanism to optimize delay timers for effective delay scheduling. Additionally, to enable a cost-effective methodology for large-scale experiments, we develop a data-driven DDL cluster simulation platform. Employing the simulation platform we compare against several state-of-the-art alternatives on real-world workload traces to demonstrate the benefits of our design. Our scheduler can provide improvement of up to 69% in end-to-end Makespan for training all jobs compared to the prevailing consolidation-based scheduling methods, while reducing the average j
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.16468</link><description>&lt;p&gt;
&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
High-Quality Image Restoration Following Human Instructions. (arXiv:2401.16468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#24674;&#22797;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#28041;&#21450;&#20174;&#36864;&#21270;&#35266;&#27979;&#20013;&#24674;&#22797;&#20986;&#39640;&#36136;&#37327;&#30340;&#24178;&#20928;&#22270;&#20687;&#12290;&#20840;&#33021;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36864;&#21270;&#31867;&#22411;&#30340;&#20449;&#24687;&#20316;&#20026;&#25552;&#31034;&#26469;&#26377;&#25928;&#22320;&#24674;&#22797;&#21508;&#31181;&#31867;&#22411;&#21644;&#32423;&#21035;&#30340;&#36864;&#21270;&#22270;&#20687;&#65292;&#24182;&#24341;&#23548;&#24674;&#22797;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#25351;&#20196;&#26469;&#25351;&#23548;&#22270;&#20687;&#24674;&#22797;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22312;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;&#36864;&#21270;&#22270;&#20687;&#20013;&#24674;&#22797;&#20986;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#24182;&#32771;&#34385;&#22810;&#31181;&#36864;&#21270;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;InstructIR&#22312;&#22270;&#20687;&#21435;&#22122;&#12289;&#38632;&#27700;&#21435;&#38500;&#12289;&#21435;&#27169;&#31946;&#12289;&#21435;&#38654;&#21644;(&#20302;&#20809;)&#22270;&#20687;&#22686;&#24378;&#31561;&#22810;&#20010;&#24674;&#22797;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;InstructIR&#22312;&#20043;&#21069;&#30340;&#20840;&#33021;&#24674;&#22797;&#26041;&#27861;&#19978;&#25552;&#39640;&#20102;1dB&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#32467;&#26524;&#20026;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#24674;&#22797;&#21644;&#22686;&#24378;&#30340;&#26032;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image restoration is a fundamental problem that involves recovering a high-quality clean image from its degraded observation. All-In-One image restoration models can effectively restore images from various types and levels of degradation using degradation-specific information as prompts to guide the restoration model. In this work, we present the first approach that uses human-written instructions to guide the image restoration model. Given natural language prompts, our model can recover high-quality images from their degraded counterparts, considering multiple degradation types. Our method, InstructIR, achieves state-of-the-art results on several restoration tasks including image denoising, deraining, deblurring, dehazing, and (low-light) image enhancement. InstructIR improves +1dB over previous all-in-one restoration methods. Moreover, our dataset and results represent a novel benchmark for new research on text-guided image restoration and enhancement. Our code, datasets and models a
&lt;/p&gt;</description></item><item><title>ReGAL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#20195;&#30721;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#20989;&#25968;&#24211;&#65292;&#21033;&#29992;&#36825;&#20123;&#20849;&#20139;&#20989;&#25968;&#24211;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2401.16467</link><description>&lt;p&gt;
ReGAL: &#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReGAL: Refactoring Programs to Discover Generalizable Abstractions. (arXiv:2401.16467v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16467
&lt;/p&gt;
&lt;p&gt;
ReGAL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#20195;&#30721;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#20989;&#25968;&#24211;&#65292;&#21033;&#29992;&#36825;&#20123;&#20849;&#20139;&#20989;&#25968;&#24211;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#31243;&#24207;&#21512;&#25104;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#24320;&#21457;&#26377;&#29992;&#25277;&#35937;&#25152;&#38656;&#30340;&#20840;&#23616;&#35270;&#35282;&#65307;&#23427;&#20204;&#36890;&#24120;&#19968;&#27425;&#39044;&#27979;&#19968;&#20010;&#31243;&#24207;&#65292;&#32463;&#24120;&#37325;&#22797;&#30456;&#21516;&#30340;&#21151;&#33021;&#12290;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#20887;&#20313;&#20195;&#30721;&#26082;&#20302;&#25928;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36890;&#29992;&#25277;&#35937;&#23398;&#20064;&#30340;&#37325;&#26500;&#26041;&#27861;&#65288;ReGAL&#65289;&#65292;&#36890;&#36807;&#20195;&#30721;&#37325;&#26500;&#26469;&#23398;&#20064;&#21487;&#37325;&#29992;&#20989;&#25968;&#24211;&#65292;&#21363;&#22312;&#19981;&#25913;&#21464;&#20195;&#30721;&#25191;&#34892;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#37325;&#32452;&#20195;&#30721;&#12290;ReGAL&#20174;&#19968;&#23567;&#32452;&#29616;&#26377;&#31243;&#24207;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#25191;&#34892;&#39564;&#35777;&#21644;&#32454;&#21270;&#25277;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ReGAL&#21457;&#29616;&#30340;&#20849;&#20139;&#20989;&#25968;&#24211;&#20351;&#24471;&#22312;&#19981;&#21516;&#39046;&#22495;&#39044;&#27979;&#31243;&#24207;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;LOGO&#22270;&#24418;&#29983;&#25104;&#12289;&#26085;&#26399;&#25512;&#29702;&#21644;&#22522;&#20110;Minecraft&#30340;&#25991;&#23383;&#28216;&#25103;TextCraft&#65289;&#19978;&#65292;&#24320;&#28304;&#21644;&#19987;&#26377;&#30340;LLMs&#22312;&#20351;&#29992;ReGAL&#20989;&#25968;&#24211;&#39044;&#27979;&#31243;&#24207;&#26102;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e. restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On three datasets (LOGO graphics generation, Date reasoning, and TextCraft, a Minecraft-based text game), both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL fun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#36951;&#25022;&#30340;&#24191;&#21578;&#29260;&#24191;&#21578;&#26102;&#27573;&#20998;&#37197;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#24191;&#21578;&#21830;&#19982;&#39038;&#23458;&#20043;&#38388;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#36890;&#36807;&#22235;&#31181;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#23569;&#36951;&#25022;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2401.16464</link><description>&lt;p&gt;
&#23454;&#29616;&#26080;&#36951;&#25022;&#30340;&#24191;&#21578;&#29260;&#24191;&#21578;&#26102;&#27573;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Towards Regret Free Slot Allocation in Billboard Advertisement. (arXiv:2401.16464v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#36951;&#25022;&#30340;&#24191;&#21578;&#29260;&#24191;&#21578;&#26102;&#27573;&#20998;&#37197;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#24191;&#21578;&#21830;&#19982;&#39038;&#23458;&#20043;&#38388;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#36890;&#36807;&#22235;&#31181;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20943;&#23569;&#36951;&#25022;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#21578;&#29260;&#24191;&#21578;&#20013;&#65292;&#20026;&#20102;&#21019;&#36896;&#21644;&#26368;&#22823;&#21270;&#23545;&#39038;&#23458;&#30340;&#24433;&#21709;&#21147;&#65292;&#24191;&#21578;&#21830;&#38656;&#35201;&#25214;&#21040;&#20855;&#26377;&#19968;&#23450;&#24433;&#21709;&#21147;&#30340;&#20154;&#25552;&#20379;&#19968;&#23450;&#25968;&#37327;&#30340;&#24191;&#21578;&#25773;&#25918;&#65292;&#24182;&#22522;&#20110;&#35266;&#30475;&#25968;&#37327;&#25910;&#36153;&#12290;&#26412;&#25991;&#38024;&#23545;&#24191;&#21578;&#25773;&#25918;&#32773;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#31163;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#22235;&#31181;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24615;&#12290;&#26368;&#32456;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#20915;&#31574;&#24102;&#26469;&#30340;&#25439;&#22833;&#65288;&#36951;&#25022;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating and maximizing influence among the customers is one of the central goals of an advertiser, and hence, remains an active area of research in recent times. In this advertisement technique, the advertisers approach an influence provider for a specific number of views of their content on a payment basis. Now, if the influence provider can provide the required number of views or more, he will receive the full, else a partial payment. In the context of an influence provider, it is a loss for him if he offers more or less views. This is formalized as 'Regret', and naturally, in the context of the influence provider, the goal will be to minimize this quantity. In this paper, we solve this problem in the context of billboard advertisement and pose it as a discrete optimization problem. We propose four efficient solution approaches for this problem and analyze them to understand their time and space complexity. We implement all the solution methodologies with real-life datasets and comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#21452;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#28789;&#27963;&#30340;&#29305;&#24449;&#34701;&#21512;&#21644;&#29305;&#24449;&#31354;&#38388;&#20840;&#23616;&#20851;&#31995;&#19981;&#21464;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16462</link><description>&lt;p&gt;
&#22522;&#20110;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#21452;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Supervised Contrastive Learning based Dual-Mixer Model for Remaining Useful Life Prediction. (arXiv:2401.16462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#21452;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#28789;&#27963;&#30340;&#29305;&#24449;&#34701;&#21512;&#21644;&#29305;&#24449;&#31354;&#38388;&#20840;&#23616;&#20851;&#31995;&#19981;&#21464;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#26088;&#22312;&#20934;&#30830;&#20272;&#35745;&#20174;&#24403;&#21069;&#39044;&#27979;&#26102;&#21051;&#21040;&#35774;&#22791;&#23436;&#20840;&#22833;&#25928;&#30340;&#21097;&#20313;&#26102;&#38388;&#12290;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;RUL&#39044;&#27979;&#26041;&#27861;&#20013;&#26102;&#38388;&#21644;&#31354;&#38388;&#29305;&#24449;&#21018;&#24615;&#32452;&#21512;&#30340;&#32570;&#28857;&#65292;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#28151;&#21512;&#27169;&#22411;&#30340;&#26102;&#31354;&#21516;&#36136;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#37319;&#29992;&#28789;&#27963;&#30340;&#36880;&#23618;&#36882;&#36827;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#30830;&#20445;&#26102;&#31354;&#29305;&#24449;&#30340;&#21516;&#36136;&#24615;&#24182;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#24341;&#20837;&#20102;&#29305;&#24449;&#31354;&#38388;&#20840;&#23616;&#20851;&#31995;&#19981;&#21464;&#24615;&#65288;FSGRI&#65289;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#32500;&#25345;&#26679;&#26412;&#29305;&#24449;&#19982;&#20854;&#36864;&#21270;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#19968;&#33268;&#24615;&#65292;&#31616;&#21270;&#20102;&#36755;&#20986;&#23618;&#20013;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of the Remaining Useful Life (RUL) prediction, aiming at providing an accurate estimate of the remaining time from the current predicting moment to the complete failure of the device, has gained significant attention from researchers in recent years. In this paper, to overcome the shortcomings of rigid combination for temporal and spatial features in most existing RUL prediction approaches, a spatial-temporal homogeneous feature extractor, named Dual-Mixer model, is firstly proposed. Flexible layer-wise progressive feature fusion is employed to ensure the homogeneity of spatial-temporal features and enhance the prediction accuracy. Secondly, the Feature Space Global Relationship Invariance (FSGRI) training method is introduced based on supervised contrastive learning. This method maintains the consistency of relationships among sample features with their degradation patterns during model training, simplifying the subsequently regression task in the output layer and improvin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#27969;&#25512;&#21160;&#21512;&#20316;&#24182;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.16461</link><description>&lt;p&gt;
&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65306;&#26356;&#24555;&#30340;&#20986;&#29616;&#65292;&#26356;&#24555;&#20048;&#30340;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents. (arXiv:2401.16461v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16461
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#27969;&#25512;&#21160;&#21512;&#20316;&#24182;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21487;&#35270;&#20026;&#19968;&#20010;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#31038;&#20250;&#65292;&#36890;&#36807;&#31038;&#20250;&#35268;&#33539;&#21487;&#20197;&#26377;&#25928;&#22320;&#35843;&#25511;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#19968;&#20010;&#31038;&#20250;&#30340;&#35268;&#33539;&#24182;&#19981;&#26159;&#30828;&#32534;&#30721;&#30340;&#65292;&#32780;&#26159;&#20174;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#19968;&#20010;&#31038;&#20250;&#20013;&#30340;&#26234;&#33021;&#20307;&#23545;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#20316;&#20986;&#30340;&#21453;&#24212;&#20197;&#21450;&#23545;&#20182;&#20154;&#21453;&#24212;&#30340;&#22238;&#24212;&#65292;&#20915;&#23450;&#20102;&#31038;&#20250;&#20013;&#20986;&#29616;&#21738;&#20123;&#35268;&#33539;&#12290;&#25105;&#20204;&#23558;&#19968;&#20010;&#26234;&#33021;&#20307;&#23545;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#28385;&#24847;&#25110;&#19981;&#28385;&#24847;&#34892;&#20026;&#30340;&#21453;&#24212;&#35270;&#20026;&#31532;&#19968;&#20010;&#26234;&#33021;&#20307;&#21521;&#31532;&#20108;&#20010;&#26234;&#33021;&#20307;&#30340;&#20132;&#27969;&#12290;&#29702;&#35299;&#36825;&#20123;&#20132;&#27969;&#26159;&#19968;&#31181;&#31038;&#20250;&#26234;&#33021;&#65306;&#36825;&#20123;&#20132;&#27969;&#36890;&#36807;&#25512;&#21160;&#26234;&#33021;&#20307;&#26397;&#30528;&#26576;&#20123;&#34892;&#20026;&#36827;&#34892;&#65292;&#20174;&#32780;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#24809;&#32602;&#21487;&#20197;&#23548;&#33268;&#35268;&#33539;&#30340;&#20986;&#29616;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#23485;&#27867;&#30340;&#31038;&#20250;&#26234;&#33021;&#21487;&#33021;&#22312;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;Ne&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiagent system can be viewed as a society of autonomous agents, whose interactions can be effectively regulated via social norms. In general, the norms of a society are not hardcoded but emerge from the agents' interactions. Specifically, how the agents in a society react to each other's behavior and respond to the reactions of others determines which norms emerge in the society. We think of these reactions by an agent to the satisfactory or unsatisfactory behaviors of another agent as communications from the first agent to the second agent. Understanding these communications is a kind of social intelligence: these communications provide natural drivers for norm emergence by pushing agents toward certain behaviors, which can become established as norms. Whereas it is well-known that sanctioning can lead to the emergence of norms, we posit that a broader kind of social intelligence can prove more effective in promoting cooperation in a multiagent system.  Accordingly, we develop Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16458</link><description>&lt;p&gt;
&#20449;&#29992;&#39118;&#38505;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65306;&#20174;P2P&#20511;&#36151;&#30340;&#36151;&#27454;&#25551;&#36848;&#20013;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending. (arXiv:2401.16458v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
P2P&#20511;&#36151;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#30340;&#34701;&#36164;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#32447;&#24179;&#21488;&#23558;&#20511;&#27454;&#20154;&#19982;&#25918;&#27454;&#20154;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;P2P&#20511;&#36151;&#38754;&#20020;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25918;&#27454;&#20154;&#24448;&#24448;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#20511;&#27454;&#20154;&#30340;&#20449;&#29992;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#20511;&#27454;&#20154;&#22312;&#36151;&#27454;&#30003;&#35831;&#36807;&#31243;&#20013;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22788;&#29702;&#36825;&#20123;&#25991;&#26412;&#25551;&#36848;&#65292;LLM&#26159;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#27169;&#24335;&#21644;&#35821;&#20041;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#23558;LLM&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;Lending Club&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;BERT&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#26174;&#33879;&#25552;&#39640;&#20102;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#22266;&#26377;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#20197;&#21450;&#28508;&#22312;&#20559;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism, linking borrowers with lenders through online platforms. However, P2P lending faces the challenge of information asymmetry, as lenders often lack sufficient data to assess the creditworthiness of borrowers. This paper proposes a novel approach to address this issue by leveraging the textual descriptions provided by borrowers during the loan application process. Our methodology involves processing these textual descriptions using a Large Language Model (LLM), a powerful tool capable of discerning patterns and semantics within the text. Transfer learning is applied to adapt the LLM to the specific task at hand.  Our results derived from the analysis of the Lending Club dataset show that the risk score generated by BERT, a widely used LLM, significantly improves the performance of credit risk classifiers. However, the inherent opacity of LLM-based systems, coupled with uncertainties about potential biases, unders
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#38376;&#36866;&#37197;&#22120;&#65288;ConGater&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#33410;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#26032;&#39062;&#27169;&#22359;&#21270;&#38376;&#26426;&#21046;&#65292;&#21487;&#22312;&#25512;&#29702;&#26102;&#36880;&#28176;&#36807;&#28193;&#20174;&#27169;&#22411;&#30340;&#20559;&#21521;&#29366;&#24577;&#21040;&#23436;&#20840;&#21435;&#20559;&#30340;&#29256;&#26412;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20998;&#31867;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16457</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#21487;&#25511;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#38376;&#36866;&#37197;&#22120;&#36827;&#34892;&#20998;&#31867;&#21644;&#26816;&#32034;&#12290;(arXiv:2401.16457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters. (arXiv:2401.16457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#38376;&#36866;&#37197;&#22120;&#65288;ConGater&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#33410;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#26032;&#39062;&#27169;&#22359;&#21270;&#38376;&#26426;&#21046;&#65292;&#21487;&#22312;&#25512;&#29702;&#26102;&#36880;&#28176;&#36807;&#28193;&#20174;&#27169;&#22411;&#30340;&#20559;&#21521;&#29366;&#24577;&#21040;&#23436;&#20840;&#21435;&#20559;&#30340;&#29256;&#26412;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20998;&#31867;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#24046;&#32531;&#35299;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#30340;&#20027;&#39064;&#65292;&#26368;&#36817;&#20851;&#27880;&#30340;&#28966;&#28857;&#26159;&#23398;&#20064;&#29420;&#31435;&#30340;&#27169;&#22359;&#65292;&#20363;&#22914;&#36866;&#37197;&#22120;&#36827;&#34892;&#25353;&#38656;&#21435;&#20559;&#12290;&#38500;&#20102;&#20248;&#21270;&#27169;&#22359;&#21270;&#21435;&#20559;&#27169;&#22411;&#22806;&#65292;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#38656;&#35201;&#22312;&#25512;&#29702;&#26102;&#25511;&#21046;&#20559;&#24046;&#20943;&#23569;&#30340;&#31243;&#24230;&#65292;&#20363;&#22914;&#65292;&#20026;&#20102;&#22312;&#25628;&#32034;&#32467;&#26524;&#20013;&#35843;&#25972;&#26399;&#26395;&#30340;&#24615;&#33021;-&#20844;&#24179;&#24615;&#26435;&#34913;&#25110;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#25511;&#21046;&#21435;&#20559;&#30340;&#24378;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#25511;&#38376;&#36866;&#37197;&#22120;&#65288;ConGater&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#33410;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#26032;&#39062;&#27169;&#22359;&#21270;&#38376;&#26426;&#21046;&#65292;&#20801;&#35768;&#22312;&#25512;&#29702;&#26102;&#20174;&#27169;&#22411;&#30340;&#20559;&#21521;&#29366;&#24577;&#36880;&#28176;&#36807;&#28193;&#21040;&#23436;&#20840;&#21435;&#20559;&#30340;&#29256;&#26412;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#19977;&#20010;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24615;&#21435;&#20559;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#20844;&#24179;&#24615;&#21015;&#34920;&#27491;&#21017;&#21270;&#26469;&#20943;&#23569;&#25628;&#32034;&#32467;&#26524;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ConGater&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias mitigation of Language Models has been the topic of many studies with a recent focus on learning separate modules like adapters for on-demand debiasing. Besides optimizing for a modularized debiased model, it is often critical in practice to control the degree of bias reduction at inference time, e.g., in order to tune for a desired performance-fairness trade-off in search results or to control the strength of debiasing in classification tasks. In this paper, we introduce Controllable Gate Adapter (ConGater), a novel modular gating mechanism with adjustable sensitivity parameters, which allows for a gradual transition from the biased state of the model to the fully debiased version at inference time. We demonstrate ConGater performance by (1) conducting adversarial debiasing experiments with three different models on three classification tasks with four protected attributes, and (2) reducing the bias of search results through fairness list-wise regularization to enable adjusting a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#28151;&#21512;Transformer&#21644;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;Chebyshev&#22810;&#39033;&#24335;&#22270;&#21367;&#31215;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23545;&#22797;&#26434;&#31354;&#38388;&#20381;&#36182;&#24615;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#26469;&#24314;&#27169;&#26102;&#31354;&#24322;&#36136;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16453</link><description>&lt;p&gt;
&#28151;&#21512;Transformer&#21644;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#38271;&#26399;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hybrid Transformer and Spatial-Temporal Self-Supervised Learning for Long-term Traffic Prediction. (arXiv:2401.16453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#28151;&#21512;Transformer&#21644;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;Chebyshev&#22810;&#39033;&#24335;&#22270;&#21367;&#31215;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23545;&#22797;&#26434;&#31354;&#38388;&#20381;&#36182;&#24615;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#26469;&#24314;&#27169;&#26102;&#31354;&#24322;&#36136;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20132;&#36890;&#39044;&#27979;&#19968;&#30452;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#21160;&#24577;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#22797;&#26434;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;Transformer&#21644;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#20132;&#36890;&#25968;&#25454;&#30340;&#24207;&#21015;&#32423;&#21644;&#22270;&#32423;&#24212;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;&#23427;&#21033;&#29992;Transformer&#20811;&#26381;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#25429;&#25417;&#38271;&#26399;&#24207;&#21015;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#37319;&#29992;Chebyshev&#22810;&#39033;&#24335;&#22270;&#21367;&#31215;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#26102;&#31354;&#24322;&#36136;&#24615;&#23545;&#20132;&#36890;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#26469;&#24314;&#27169;&#26102;&#31354;&#24322;&#36136;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;PeMS04&#21644;PeMS08&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#21487;&#35270;&#21270;&#21644;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term traffic prediction has always been a challenging task due to its dynamic temporal dependencies and complex spatial dependencies. In this paper, we propose a model that combines hybrid Transformer and spatio-temporal self-supervised learning. The model enhances its robustness by applying adaptive data augmentation techniques at the sequence-level and graph-level of the traffic data. It utilizes Transformer to overcome the limitations of recurrent neural networks in capturing long-term sequences, and employs Chebyshev polynomial graph convolution to capture complex spatial dependencies. Furthermore, considering the impact of spatio-temporal heterogeneity on traffic speed, we design two self-supervised learning tasks to model the temporal and spatial heterogeneity, thereby improving the accuracy and generalization ability of the model. Experimental evaluations are conducted on two real-world datasets, PeMS04 and PeMS08, and the results are visualized and analyzed, demonstrating 
&lt;/p&gt;</description></item><item><title>Context-Former&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#24182;&#25552;&#39640;&#20102;Decision Transformer&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16452</link><description>&lt;p&gt;
Context-Former&#65306;&#22522;&#20110;&#28508;&#22312;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#30340;&#25340;&#25509;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Context-Former: Stitching via Latent Conditioned Sequence Modeling. (arXiv:2401.16452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16452
&lt;/p&gt;
&lt;p&gt;
Context-Former&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#24182;&#25552;&#39640;&#20102;Decision Transformer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#20351;RL&#33021;&#22815;&#23398;&#20064;&#20248;&#20110;&#34892;&#20026;&#31574;&#30053;&#30340;&#31574;&#30053;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Decision Transformer&#65288;DT&#65289;&#23558;&#20915;&#31574;&#24314;&#27169;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#22312;&#31163;&#32447;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;DT&#32570;&#20047;&#25340;&#25509;&#33021;&#21147;&#65292;&#22240;&#27492;&#25552;&#39640;DT&#24615;&#33021;&#38656;&#35201;&#21033;&#29992;&#25340;&#25509;&#33021;&#21147;&#12290;&#20026;&#20102;&#36171;&#20104;DT&#25340;&#25509;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36712;&#36857;&#25340;&#25509;&#25277;&#35937;&#20026;&#19987;&#23478;&#21305;&#37197;&#65292;&#24182;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;ContextFormer&#65292;&#36890;&#36807;&#27169;&#25311;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#30340;&#34920;&#31034;&#26469;&#38598;&#25104;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21644;&#24207;&#21015;&#24314;&#27169;&#65292;&#20197;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) algorithms can improve the decision making via stitching sub-optimal trajectories to obtain more optimal ones. This capability is a crucial factor in enabling RL to learn policies that are superior to the behavioral policy. On the other hand, Decision Transformer (DT) abstracts the decision-making as sequence modeling, showcasing competitive performance on offline RL benchmarks, however, recent studies demonstrate that DT lacks of stitching capability, thus exploit stitching capability for DT is vital to further improve its performance. In order to endow stitching capability to DT, we abstract trajectory stitching as expert matching and introduce our approach, ContextFormer, which integrates contextual information-based imitation learning (IL) and sequence modeling to stitch sub-optimal trajectory fragments by emulating the representations of a limited number of expert trajectories. To validate our claim, we conduct experiments from two perspectives:
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#25968;&#23383;&#21452;&#32990;&#32974;&#27169;&#22411;&#65292;&#21033;&#29992;&#26102;&#31354;&#22270;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#26469;&#25903;&#25345;&#21160;&#24577;&#30340;&#26234;&#33021;&#22478;&#24066;&#29615;&#22659;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#12289;&#21516;&#27493;&#24615;&#12289;&#36164;&#28304;&#20248;&#21270;&#21644;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#22270;&#25968;&#25454;&#24211;&#20013;&#23454;&#29616;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#25968;&#25454;&#25429;&#33719;&#65292;&#38477;&#20302;&#24320;&#38144;&#21644;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2401.16449</link><description>&lt;p&gt;
&#33021;&#28304;&#25968;&#23383;&#21270;&#21452;&#32990;&#32974;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;: &#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#25968;&#23383;&#21452;&#32990;&#32974;&#27169;&#22411;&#29992;&#20110;&#32511;&#33394;&#22478;&#24066;
&lt;/p&gt;
&lt;p&gt;
AI in Energy Digital Twining: A Reinforcement Learning-based Adaptive Digital Twin Model for Green Cities. (arXiv:2401.16449v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#25968;&#23383;&#21452;&#32990;&#32974;&#27169;&#22411;&#65292;&#21033;&#29992;&#26102;&#31354;&#22270;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#26469;&#25903;&#25345;&#21160;&#24577;&#30340;&#26234;&#33021;&#22478;&#24066;&#29615;&#22659;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#12289;&#21516;&#27493;&#24615;&#12289;&#36164;&#28304;&#20248;&#21270;&#21644;&#33021;&#28304;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#22312;&#22270;&#25968;&#25454;&#24211;&#20013;&#23454;&#29616;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#25968;&#25454;&#25429;&#33719;&#65292;&#38477;&#20302;&#24320;&#38144;&#21644;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21452;&#32990;&#32974;(DT)&#23545;&#20110;&#23454;&#29616;&#21487;&#25345;&#32493;&#21644;&#26377;&#25928;&#30340;&#26234;&#33021;&#22478;&#24066;&#35299;&#20915;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;DT&#24314;&#27169;&#25216;&#26415;&#26080;&#27861;&#25903;&#25345;&#36825;&#20123;&#26234;&#33021;&#22478;&#24066;&#29615;&#22659;&#30340;&#21160;&#24577;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#20013;&#32570;&#20047;&#23454;&#26102;&#25968;&#25454;&#25429;&#33719;&#23548;&#33268;&#24314;&#27169;&#19981;&#20934;&#30830;&#20197;&#21450;&#36164;&#28304;&#21644;&#33021;&#28304;&#28040;&#32791;&#25361;&#25112;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26102;&#31354;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#21452;&#32990;&#32974;&#65288;RL-AT&#65289;&#26426;&#21046;&#19982;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#25512;&#36827;&#32511;&#33394;&#22478;&#24066;&#65292;&#24182;&#23637;&#31034;&#20934;&#30830;&#24615;&#12289;&#21516;&#27493;&#24615;&#12289;&#36164;&#28304;&#20248;&#21270;&#21644;&#33021;&#28304;&#25928;&#29575;&#31561;&#26041;&#38754;&#30340;&#23454;&#38469;&#22909;&#22788;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20351;&#29992;&#22270;&#25968;&#25454;&#24211;&#36827;&#34892;&#23454;&#29616;&#26102;&#65292;&#26102;&#31354;&#22270;&#33021;&#22815;&#25552;&#20379;&#19968;&#33268;&#30340;&#20934;&#30830;&#24615;&#21644;55%&#26356;&#39640;&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#23454;&#26102;&#25968;&#25454;&#25429;&#33719;&#33021;&#21147;&#65292;&#24320;&#38144;&#38477;&#20302;20%&#65292;&#33021;&#28304;&#28040;&#32791;&#38477;&#20302;25%&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital Twins (DT) have become crucial to achieve sustainable and effective smart urban solutions. However, current DT modelling techniques cannot support the dynamicity of these smart city environments. This is caused by the lack of right-time data capturing in traditional approaches, resulting in inaccurate modelling and high resource and energy consumption challenges. To fill this gap, we explore spatiotemporal graphs and propose the Reinforcement Learning-based Adaptive Twining (RL-AT) mechanism with Deep Q Networks (DQN). By doing so, our study contributes to advancing Green Cities and showcases tangible benefits in accuracy, synchronisation, resource optimization, and energy efficiency. As a result, we note the spatiotemporal graphs are able to offer a consistent accuracy and 55% higher querying performance when implemented using graph databases. In addition, our model demonstrates right-time data capturing with 20% lower overhead and 25% lower energy consumption.
&lt;/p&gt;</description></item><item><title>OMPGPT&#26159;&#19968;&#31181;&#20026;&#20102;OpenMP pragma&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;chain-of-OMP&#12290;</title><link>http://arxiv.org/abs/2401.16445</link><description>&lt;p&gt;
OMPGPT: &#19968;&#31181;&#29992;&#20110;OpenMP&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OMPGPT: A Generative Pre-trained Transformer Model for OpenMP. (arXiv:2401.16445v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16445
&lt;/p&gt;
&lt;p&gt;
OMPGPT&#26159;&#19968;&#31181;&#20026;&#20102;OpenMP pragma&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;chain-of-OMP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#31561;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#38543;&#30528;&#36825;&#19968;&#36235;&#21183;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;StarCoder&#12289;WizardCoder&#21644;CodeLlama&#31561;&#65292;&#24050;&#32463;&#28044;&#29616;&#20986;&#26469;&#65292;&#22312;&#22823;&#37327;&#30340;&#20195;&#30721;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#35745;&#22266;&#26377;&#30340;&#21407;&#22240;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#23436;&#25104;&#21644;&#27880;&#37322;&#29983;&#25104;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#20197;&#21450;&#23545;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#19968;&#33324;&#25903;&#25345;&#12290;&#34429;&#28982;&#20195;&#30721;LLMs&#30340;&#36890;&#29992;&#33021;&#21147;&#23545;&#35768;&#22810;&#31243;&#24207;&#21592;&#26469;&#35828;&#24456;&#26377;&#29992;&#65292;&#20294;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#39046;&#22495;&#20855;&#26377;&#26356;&#31364;&#30340;&#38656;&#27714;&#38598;&#65292;&#20351;&#24471;&#26356;&#23567;&#12289;&#26356;&#20855;&#39046;&#22495;&#29305;&#23450;&#30340;LM&#25104;&#20026;&#19968;&#20010;&#26356;&#26126;&#26234;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OMPGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#22312;OpenMP pragma&#29983;&#25104;&#26041;&#38754;&#30340;&#22266;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#24182;&#25913;&#36827;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#21019;&#24314;&#20102;&#38142;&#24335;OMP&#65288;chain-of-OMP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), as epitomized by models like ChatGPT, have revolutionized the field of natural language processing (NLP). Along with this trend, code-based large language models such as StarCoder, WizardCoder, and CodeLlama have emerged, trained extensively on vast repositories of code data. Yet, inherent in their design, these models primarily focus on generative tasks like code generation, code completion, and comment generation, and general support for multiple programming languages. While the generic abilities of code LLMs are useful for many programmers, the area of high-performance computing (HPC) has a narrower set of requirements that make a smaller and more domain-specific LM a smarter choice. This paper introduces OMPGPT, a novel model meticulously designed to harness the inherent strengths of language models for OpenMP pragma generation. Furthermore, we adopt and adapt prompt engineering techniques from the NLP domain to create chain-of-OMP, an innovative strat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#20998;&#31867;&#22120;&#21644;&#25163;&#37096;&#36861;&#36394;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#29992;&#25143;&#23545;&#34394;&#25311;&#29616;&#23454;&#29087;&#24713;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#29992;&#25143;&#19981;&#29087;&#24713;&#34394;&#25311;&#29616;&#23454;&#26102;&#20026;&#20854;&#25552;&#20379;&#25353;&#38656;&#22521;&#35757;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#23436;&#25104;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.16443</link><description>&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#29992;&#20110;&#36890;&#36807;&#25163;&#37096;&#20132;&#20114;&#26816;&#27979;&#29992;&#25143;&#23545;&#34394;&#25311;&#29616;&#23454;&#30340;&#29087;&#24713;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Evaluating Deep Networks for Detecting User Familiarity with VR from Hand Interactions. (arXiv:2401.16443v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16443
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#20998;&#31867;&#22120;&#21644;&#25163;&#37096;&#36861;&#36394;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#29992;&#25143;&#23545;&#34394;&#25311;&#29616;&#23454;&#29087;&#24713;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#29992;&#25143;&#19981;&#29087;&#24713;&#34394;&#25311;&#29616;&#23454;&#26102;&#20026;&#20854;&#25552;&#20379;&#25353;&#38656;&#22521;&#35757;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#23436;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#34394;&#25311;&#29616;&#23454;&#35774;&#22791;&#22312;&#28040;&#36153;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#23545;&#20110;&#23545;&#34394;&#25311;&#29616;&#23454;&#19981;&#29087;&#24713;&#30340;&#29992;&#25143;&#32780;&#35328;&#65292;&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#30340;&#20351;&#29992;&#21487;&#33021;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#26816;&#27979;&#29992;&#25143;&#23545;&#34394;&#25311;&#29616;&#23454;&#30340;&#29087;&#24713;&#31243;&#24230;&#20316;&#20026;&#20132;&#20114;&#23186;&#20171;&#65292;&#20855;&#26377;&#36890;&#36807;&#25552;&#20379;&#25353;&#38656;&#22521;&#35757;&#36827;&#34892;&#36866;&#24212;&#21644;&#38450;&#27490;&#29992;&#25143;&#22312;&#23436;&#25104;&#20219;&#21153;&#26102;&#34987;&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#25152;&#25302;&#32047;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#20998;&#31867;&#22120;&#36827;&#34892;&#33258;&#21160;&#26816;&#27979;&#29992;&#25143;&#23545;&#34394;&#25311;&#29616;&#23454;&#30340;&#29087;&#24713;&#31243;&#24230;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#36890;&#36807;&#29992;&#25143;&#20351;&#29992;&#25163;&#37096;&#19982;&#34394;&#25311;&#29616;&#23454;&#38376;&#38145;&#25968;&#23383;&#23494;&#30721;&#36755;&#20837;&#38754;&#26495;&#36827;&#34892;&#20132;&#20114;&#26469;&#35299;&#38145;&#34394;&#25311;&#29616;&#23454;&#38376;&#12290;&#25105;&#20204;&#23558;&#34394;&#25311;&#29616;&#23454;&#38376;&#20316;&#20026;&#20225;&#19994;&#34394;&#25311;&#31354;&#38388;&#30340;&#31532;&#19968;&#20837;&#21475;&#28857;&#65292;&#20363;&#22914;&#20250;&#35758;&#23460;&#12289;&#21150;&#20844;&#23460;&#25110;&#35786;&#25152;&#12290;&#23545;&#20110;&#19981;&#29087;&#24713;&#34394;&#25311;&#29616;&#23454;&#30340;&#29992;&#25143;&#32780;&#35328;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24050;&#32463;&#20351;&#29992;&#36807;&#25163;&#37096;&#25171;&#24320;&#24102;&#26377;&#23494;&#30721;&#36755;&#20837;&#38754;&#26495;&#30340;&#38376;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;&#29992;&#25143;&#21487;&#33021;&#23545;&#34394;&#25311;&#29616;&#23454;&#19981;&#29087;&#24713;&#65292;&#20294;&#20182;&#20204;&#23545;&#25171;&#24320;&#38376;&#30340;&#20219;&#21153;&#24212;&#35813;&#26159;&#29087;&#24713;&#30340;&#12290;&#20351;&#29992; pilot d
&lt;/p&gt;
&lt;p&gt;
As VR devices become more prevalent in the consumer space, VR applications are likely to be increasingly used by users unfamiliar with VR. Detecting the familiarity level of a user with VR as an interaction medium provides the potential of providing on-demand training for acclimatization and prevents the user from being burdened by the VR environment in accomplishing their tasks. In this work, we present preliminary results of using deep classifiers to conduct automatic detection of familiarity with VR by using hand tracking of the user as they interact with a numeric passcode entry panel to unlock a VR door. We use a VR door as we envision it to the first point of entry to collaborative virtual spaces, such as meeting rooms, offices, or clinics. Users who are unfamiliar with VR will have used their hands to open doors with passcode entry panels in the real world. Thus, while the user may not be familiar with VR, they would be familiar with the task of opening the door. Using a pilot d
&lt;/p&gt;</description></item><item><title>FaKnow&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31639;&#27861;&#24211;&#65292;&#21253;&#21547;&#22810;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#21644;&#24037;&#20855;&#65292;&#24182;&#35299;&#20915;&#20102;&#19981;&#21516;&#26694;&#26550;&#19979;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16441</link><description>&lt;p&gt;
FaKnow: &#19968;&#20010;&#29992;&#20110;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
FaKnow: A Unified Library for Fake News Detection. (arXiv:2401.16441v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16441
&lt;/p&gt;
&lt;p&gt;
FaKnow&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31639;&#27861;&#24211;&#65292;&#21253;&#21547;&#22810;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#21644;&#24037;&#20855;&#65292;&#24182;&#35299;&#20915;&#20102;&#19981;&#21516;&#26694;&#26550;&#19979;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22823;&#37327;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31639;&#27861;&#24212;&#36816;&#32780;&#29983;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#22312;&#19981;&#21516;&#30340;&#26694;&#26550;&#19979;&#24320;&#21457;&#65292;&#27599;&#20010;&#26694;&#26550;&#21448;&#35201;&#27714;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#22240;&#27492;&#38459;&#30861;&#20102;&#21487;&#37325;&#22797;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#30340;&#20195;&#30721;&#24320;&#21457;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#20887;&#20313;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FaKnow&#65292;&#19968;&#20010;&#32479;&#19968;&#19988;&#20840;&#38754;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31639;&#27861;&#24211;&#12290;&#23427;&#28085;&#30422;&#20102;&#22810;&#31181;&#24120;&#29992;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#31038;&#20250;&#29615;&#22659;&#30340;&#26041;&#27861;&#12290;&#35813;&#24211;&#28085;&#30422;&#20102;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#30340;&#23436;&#25972;&#33539;&#22260;&#65292;&#22312;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20869;&#26377;&#25928;&#32452;&#32455;&#20102;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#35757;&#32451;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#36741;&#21161;&#21151;&#33021;&#21644;&#24037;&#20855;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#21644;&#26085;&#24535;&#35760;&#24405;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21270;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past years, a large number of fake news detection algorithms based on deep learning have emerged. However, they are often developed under different frameworks, each mandating distinct utilization methodologies, consequently hindering reproducibility. Additionally, a substantial amount of redundancy characterizes the code development of such fake news detection models. To address these concerns, we propose FaKnow, a unified and comprehensive fake news detection algorithm library. It encompasses a variety of widely used fake news detection models, categorized as content-based and social context-based approaches. This library covers the full spectrum of the model training and evaluation process, effectively organizing the data, models, and training procedures within a unified framework. Furthermore, it furnishes a series of auxiliary functionalities and tools, including visualization, and logging. Our work contributes to the standardization and unification of fake news detection 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#24403;&#22320;&#26102;&#31354;&#20844;&#20849;&#35760;&#24405;&#26469;&#39044;&#27979;&#39537;&#36880;&#39118;&#38505;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#39044;&#27979;&#23545;&#20110;&#25351;&#23548;&#26377;&#38024;&#23545;&#24615;&#30340;&#22806;&#23637;&#25919;&#31574;&#26159;&#26377;&#29992;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.16440</link><description>&lt;p&gt;
&#36229;&#36234;&#39537;&#36880;&#39044;&#27979;&#65306;&#21033;&#29992;&#24403;&#22320;&#26102;&#31354;&#20844;&#20849;&#35760;&#24405;&#26469;&#25351;&#23548;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Beyond Eviction Prediction: Leveraging Local Spatiotemporal Public Records to Inform Action. (arXiv:2401.16440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#24403;&#22320;&#26102;&#31354;&#20844;&#20849;&#35760;&#24405;&#26469;&#39044;&#27979;&#39537;&#36880;&#39118;&#38505;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#39044;&#27979;&#23545;&#20110;&#25351;&#23548;&#26377;&#38024;&#23545;&#24615;&#30340;&#22806;&#23637;&#25919;&#31574;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#39537;&#36880;&#39118;&#38505;&#23545;&#25151;&#20135;&#36827;&#34892;&#35780;&#20998;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#39537;&#36880;&#39044;&#27979;&#26041;&#27861;&#30340;&#25104;&#21151;&#36890;&#24120;&#26159;&#36890;&#36807;&#19981;&#21516;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#25351;&#26631;&#26469;&#35780;&#20272;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39044;&#27979;&#30340;&#26681;&#26412;&#30446;&#26631;&#26159;&#20026;&#20102;&#21521;&#21487;&#33021;&#38754;&#20020;&#26356;&#22823;&#39118;&#38505;&#30340;&#23478;&#24237;&#25552;&#20379;&#36866;&#24403;&#30340;&#24110;&#21161;&#65292;&#20197;&#20445;&#25345;&#20303;&#25151;&#31283;&#23450;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#38382;&#19968;&#20010;&#38382;&#39064;&#65292;&#37027;&#23601;&#26159;&#36825;&#26679;&#30340;&#39044;&#27979;&#22312;&#25351;&#23548;&#22806;&#23637;&#34892;&#21160;&#26041;&#38754;&#26377;&#22810;&#22823;&#30340;&#29992;&#22788;&#12290;&#26412;&#25991;&#21033;&#29992;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#25151;&#20135;&#12289;&#39537;&#36880;&#21644;&#19994;&#20027;&#30340;&#20449;&#24687;&#36827;&#34892;&#21305;&#37197;&#65292;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#39537;&#36880;&#39044;&#27979;&#20219;&#21153;&#65292;&#29983;&#25104;&#39118;&#38505;&#24471;&#20998;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#39118;&#38505;&#24471;&#20998;&#26469;&#35268;&#21010;&#26377;&#38024;&#23545;&#24615;&#30340;&#22806;&#23637;&#25919;&#31574;&#12290;&#25105;&#20204;&#26174;&#31034;&#36825;&#20123;&#39118;&#38505;&#24471;&#20998;&#23454;&#38469;&#19978;&#26159;&#26377;&#29992;&#30340;&#65292;&#33021;&#22815;&#20351;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#24037;&#20316;&#20154;&#21592;&#22242;&#38431;&#22312;&#30456;&#21516;&#30340;&#26102;&#38388;&#20869;&#25509;&#35302;&#26356;&#22810;&#23481;&#26131;&#21457;&#29983;&#39537;&#36880;&#30340;&#25151;&#20135;&#65292;&#30456;&#27604;&#20110;&#20197;&#34903;&#21306;&#20026;&#22522;&#30784;&#25110;&#20851;&#27880;&#29305;&#23450;&#24314;&#31569;&#29289;&#30340;&#22806;&#23637;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been considerable recent interest in scoring properties on the basis of eviction risk. The success of methods for eviction prediction is typically evaluated using different measures of predictive accuracy. However, the underlying goal of such prediction is to direct appropriate assistance to households that may be at greater risk so they remain stably housed. Thus, we must ask the question of how useful such predictions are in targeting outreach efforts - informing action. In this paper, we investigate this question using a novel dataset that matches information on properties, evictions, and owners. We perform an eviction prediction task to produce risk scores and then use these risk scores to plan targeted outreach policies. We show that the risk scores are, in fact, useful, enabling a theoretical team of caseworkers to reach more eviction-prone properties in the same amount of time, compared to outreach policies that are either neighborhood-based or focus on buildings with 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32479;&#35745;&#23376;&#32452;&#19981;&#20844;&#24179;&#24615;&#27010;&#24565;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#23457;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#39640;&#26031;&#20998;&#24067;&#30340;&#23457;&#35745;&#32467;&#26524;&#12290;&#20182;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#21033;&#29992;&#26080;&#20559;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.16439</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#26102;&#38388;&#19979;&#23545;&#39640;&#26031;&#25968;&#25454;&#36827;&#34892;&#32479;&#35745;&#23376;&#32452;&#19981;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Polynomial time auditing of statistical subgroup fairness for Gaussian data. (arXiv:2401.16439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16439
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#32479;&#35745;&#23376;&#32452;&#19981;&#20844;&#24179;&#24615;&#27010;&#24565;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#23457;&#35745;&#30340;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#39640;&#26031;&#20998;&#24067;&#30340;&#23457;&#35745;&#32467;&#26524;&#12290;&#20182;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#21033;&#29992;&#26080;&#20559;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#32479;&#35745;&#23376;&#32452;&#19981;&#20844;&#24179;&#24615;&#27010;&#24565;&#23545;&#20998;&#31867;&#22120;&#36827;&#34892;&#23457;&#35745;&#30340;&#38382;&#39064;&#12290;Kearns&#31561;&#20154;&#65288;2018&#65289;&#24050;&#32463;&#34920;&#26126;&#65292;&#23457;&#35745;&#32452;&#21512;&#23376;&#32452;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#19982;&#26080;&#20559;&#23398;&#20064;&#19968;&#26679;&#22256;&#38590;&#12290;&#23613;&#31649;&#23545;&#20110;&#36825;&#20010;&#38382;&#39064;&#27809;&#26377;&#24050;&#30693;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#20294;&#20960;&#20046;&#25152;&#26377;&#35299;&#20915;&#23545;&#23376;&#32452;&#30340;&#32479;&#35745;&#27495;&#35270;&#24230;&#37327;&#30340;&#24037;&#20316;&#37117;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#27492;&#38382;&#39064;&#30340;&#39044;&#35328;&#26426;&#12290;&#22914;&#26524;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#39640;&#26031;&#20998;&#24067;&#65292;&#29978;&#33267;&#20165;&#26159;&#23547;&#24120;&#23545;&#25968;&#20985;&#26354;&#20998;&#24067;&#65292;&#37027;&#20040;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#24050;&#32463;&#21457;&#29616;&#20102;&#39640;&#25928;&#30340;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#21322;&#31354;&#38388;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;Kearns&#31561;&#20154;&#32473;&#20986;&#30340;&#25552;&#21319;&#39118;&#26684;&#30340;&#35268;&#32422;&#35201;&#27714;&#26080;&#20559;&#23398;&#20064;&#31639;&#27861;&#22312;&#21487;&#33021;&#19981;&#26159;&#23547;&#24120;&#23545;&#25968;&#20985;&#30340;&#37325;&#26032;&#21152;&#26435;&#20998;&#24067;&#19978;&#25104;&#21151;&#65292;&#21363;&#20351;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#26159;&#23547;&#24120;&#23545;&#25968;&#20985;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#39640;&#26031;&#20998;&#24067;&#30340;&#23457;&#35745;&#32467;&#26524;&#32473;&#20986;&#20102;&#27491;&#38754;&#21644;&#36127;&#38754;&#30340;&#32467;&#26524;&#65306;&#22312;&#27491;&#38754;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#26041;&#27861;&#26469;&#21033;&#29992;&#36825;&#20123;&#26080;&#20559;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of auditing classifiers with the notion of statistical subgroup fairness. Kearns et al. (2018) has shown that the problem of auditing combinatorial subgroups fairness is as hard as agnostic learning. Essentially all work on remedying statistical measures of discrimination against subgroups assumes access to an oracle for this problem, despite the fact that no efficient algorithms are known for it. If we assume the data distribution is Gaussian, or even merely log-concave, then a recent line of work has discovered efficient agnostic learning algorithms for halfspaces. Unfortunately, the boosting-style reductions given by Kearns et al. required the agnostic learning algorithm to succeed on reweighted distributions that may not be log-concave, even if the original data distribution was. In this work, we give positive and negative results on auditing for the Gaussian distribution: On the positive side, we an alternative approach to leverage these advances in agnostic l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26435;&#37325;&#30697;&#38453;&#30340;&#21015;&#31354;&#38388;&#21644;&#34892;&#31354;&#38388;&#30340;&#26032;&#27010;&#24565;&#65292;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16438</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#39640;&#25928;&#21033;&#29992;&#20102;&#26435;&#37325;&#31354;&#38388;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do deep neural networks utilize the weight space efficiently?. (arXiv:2401.16438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26435;&#37325;&#30697;&#38453;&#30340;&#21015;&#31354;&#38388;&#21644;&#34892;&#31354;&#38388;&#30340;&#26032;&#27010;&#24565;&#65292;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;Transformer&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#20294;&#26159;&#23427;&#20204;&#21442;&#25968;&#23494;&#38598;&#30340;&#29305;&#24615;&#38480;&#21046;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#26435;&#37325;&#30697;&#38453;&#30340;&#21015;&#31354;&#38388;&#21644;&#34892;&#31354;&#38388;&#65292;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#21033;&#29992;&#36825;&#31181;&#33539;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#29942;&#39048;&#23618;&#21644;&#27880;&#24847;&#21147;&#23618;&#65292;&#21487;&#20197;&#23558;&#21442;&#25968;&#20943;&#21322;&#65292;&#20165;&#24102;&#26469;&#36731;&#24494;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;ViT&#21644;ResNet50&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#19982;&#20256;&#32479;&#27169;&#22411;&#30340;&#27604;&#36739;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#35299;&#20915;&#20102;&#23545;&#21442;&#25968;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#32039;&#36843;&#38656;&#27714;&#65292;&#32780;&#19988;&#22312;&#30495;&#23454;&#22330;&#26223;&#30340;&#23454;&#38469;&#37096;&#32626;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models like Transformers and Convolutional Neural Networks (CNNs) have revolutionized various domains, but their parameter-intensive nature hampers deployment in resource-constrained settings. In this paper, we introduce a novel concept utilizes column space and row space of weight matrices, which allows for a substantial reduction in model parameters without compromising performance. Leveraging this paradigm, we achieve parameter-efficient deep learning models.. Our approach applies to both Bottleneck and Attention layers, effectively halving the parameters while incurring only minor performance degradation. Extensive experiments conducted on the ImageNet dataset with ViT and ResNet50 demonstrate the effectiveness of our method, showcasing competitive performance when compared to traditional models. This approach not only addresses the pressing demand for parameter efficient deep learning solutions but also holds great promise for practical deployment in real-world scena
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;TorNet&#65292;&#29992;&#20110;&#25903;&#25345;&#40857;&#21367;&#39118;&#26816;&#27979;&#21644;&#39044;&#27979;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#21313;&#24180;&#30340;&#20855;&#26377;&#20840;&#20998;&#36776;&#29575;&#21644;&#26497;&#21270;&#29305;&#24615;&#30340;&#22825;&#27668;&#38647;&#36798;&#25968;&#25454;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.16437</link><description>&lt;p&gt;
&#29992;&#20840;&#20998;&#36776;&#29575;&#26497;&#21270;&#22825;&#27668;&#38647;&#36798;&#25968;&#25454;&#36827;&#34892;&#40857;&#21367;&#39118;&#26816;&#27979;&#21644;&#39044;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Dataset for Tornado Detection and Prediction using Full-Resolution Polarimetric Weather Radar Data. (arXiv:2401.16437v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;TorNet&#65292;&#29992;&#20110;&#25903;&#25345;&#40857;&#21367;&#39118;&#26816;&#27979;&#21644;&#39044;&#27979;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#21313;&#24180;&#30340;&#20855;&#26377;&#20840;&#20998;&#36776;&#29575;&#21644;&#26497;&#21270;&#29305;&#24615;&#30340;&#22825;&#27668;&#38647;&#36798;&#25968;&#25454;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#38647;&#36798;&#26159;&#26816;&#27979;&#21644;&#39044;&#35686;&#40857;&#21367;&#39118;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#20026;&#20102;&#24110;&#21161;&#39044;&#25253;&#21592;&#35686;&#21578;&#20844;&#20247;&#65292;&#24050;&#24320;&#21457;&#20102;&#20960;&#31181;&#31639;&#27861;&#20197;&#33258;&#21160;&#26816;&#27979;&#22825;&#27668;&#38647;&#36798;&#35266;&#27979;&#20013;&#30340;&#40857;&#21367;&#39118;&#36857;&#35937;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#30452;&#25509;&#20174;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;&#27492;&#30446;&#30340;&#38750;&#24120;&#26377;&#25928;&#12290;&#30001;&#20110;&#40857;&#21367;&#39118;&#22312;&#25152;&#26377;&#21487;&#29992;&#38647;&#36798;&#35266;&#27979;&#25968;&#25454;&#20013;&#38750;&#24120;&#32597;&#35265;&#65292;&#22240;&#27492;&#36873;&#25321;&#21644;&#35774;&#35745;&#29992;&#20110;ML&#24212;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#20110;ML&#31639;&#27861;&#30340;&#24615;&#33021;&#12289;&#31283;&#20581;&#24615;&#21644;&#26368;&#32456;&#25509;&#21463;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;TorNet&#65292;&#20197;&#25903;&#25345;&#40857;&#21367;&#39118;&#26816;&#27979;&#21644;&#39044;&#27979;&#20013;&#30340;ML&#31639;&#27861;&#30340;&#24320;&#21457;&#12290;TorNet&#21253;&#21547;&#20174;10&#24180;&#25253;&#21578;&#30340;&#39118;&#26292;&#20107;&#20214;&#20013;&#37319;&#26679;&#30340;&#20840;&#20998;&#36776;&#29575;&#12289;&#26497;&#21270;&#30340;Level-II WSR-88D&#25968;&#25454;&#12290;&#24320;&#21457;&#20102;&#35768;&#22810;&#40857;&#21367;&#39118;&#26816;&#27979;&#30340;ML&#22522;&#20934;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather radar is the primary tool used by forecasters to detect and warn for tornadoes in near-real time. In order to assist forecasters in warning the public, several algorithms have been developed to automatically detect tornadic signatures in weather radar observations. Recently, Machine Learning (ML) algorithms, which learn directly from large amounts of labeled data, have been shown to be highly effective for this purpose. Since tornadoes are extremely rare events within the corpus of all available radar observations, the selection and design of training datasets for ML applications is critical for the performance, robustness, and ultimate acceptance of ML algorithms. This study introduces a new benchmark dataset, TorNet to support development of ML algorithms in tornado detection and prediction. TorNet contains full-resolution, polarimetric, Level-II WSR-88D data sampled from 10 years of reported storm events. A number of ML baselines for tornado detection are developed and compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ANROA&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24182;&#32593;&#22826;&#38451;&#33021;&#36716;&#25442;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#33258;&#36866;&#24212;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#21644;Rain&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#30005;&#21147;&#36136;&#37327;&#38382;&#39064;&#30340;&#36991;&#20813;&#21644;&#21333;&#20301;&#21151;&#29575;&#22240;&#25968;&#36816;&#34892;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.16434</link><description>&lt;p&gt;
&#22522;&#20110;ANROA&#30340;&#22810;&#21151;&#33021;&#24182;&#32593;&#22826;&#38451;&#33021;&#36716;&#25442;&#31995;&#32479;&#30340;&#26032;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel ANROA based control approach for grid-tied multi-functional solar energy conversion system. (arXiv:2401.16434v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ANROA&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24182;&#32593;&#22826;&#38451;&#33021;&#36716;&#25442;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#33258;&#36866;&#24212;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#21644;Rain&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#30005;&#21147;&#36136;&#37327;&#38382;&#39064;&#30340;&#36991;&#20813;&#21644;&#21333;&#20301;&#21151;&#29575;&#22240;&#25968;&#36816;&#34892;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#30340;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#19982;Rain&#20248;&#21270;&#31639;&#27861;&#65288;ANROA&#65289;&#26041;&#27861;&#30340;&#19977;&#30456;&#24182;&#32593;&#22826;&#38451;&#33021;&#20809;&#20239;&#31995;&#32479;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#33258;&#36866;&#24212;&#31070;&#32463;&#27169;&#31946;&#25512;&#29702;&#31995;&#32479;&#65288;ANFIS&#65289;&#19982;Rain&#20248;&#21270;&#31639;&#27861;&#65288;ROA&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#26368;&#22823;&#36319;&#36394;&#33021;&#21147;&#12290;ROA&#25216;&#26415;&#36127;&#36131;&#25511;&#21046;&#30005;&#21387;&#28304;&#21464;&#25442;&#22120;&#30340;&#24320;&#20851;&#65292;&#20197;&#36991;&#20813;&#30005;&#21387;&#27874;&#21160;&#12289;&#35856;&#27874;&#21644;&#38378;&#28865;&#31561;&#30005;&#21147;&#36136;&#37327;&#38382;&#39064;&#65292;&#21516;&#26102;&#23454;&#29616;&#36127;&#36733;&#24179;&#34913;&#21644;&#26080;&#21151;&#21151;&#29575;&#20351;&#29992;&#30340;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#38646;&#30005;&#21387;&#35843;&#33410;&#21644;&#21333;&#20301;&#21151;&#29575;&#22240;&#25968;&#27169;&#24335;&#19979;&#36816;&#34892;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#24314;&#27169;&#21644;&#20223;&#30495;&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#30340;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
An adaptive control approach for a three-phase grid-interfaced solar photovoltaic system based on the new Neuro-Fuzzy Inference System with Rain Optimization Algorithm (ANROA) methodology is proposed and discussed in this manuscript. This method incorporates an Adaptive Neuro-fuzzy Inference System (ANFIS) with a Rain Optimization Algorithm (ROA). The ANFIS controller has excellent maximum tracking capability because it includes features of both neural and fuzzy techniques. The ROA technique is in charge of controlling the voltage source converter switching. Avoiding power quality problems including voltage fluctuations, harmonics, and flickers as well as unbalanced loads and reactive power usage is the major goal. Besides, the proposed method performs at zero voltage regulation and unity power factor modes. The suggested control approach has been modeled and simulated, and its performance has been assessed using existing alternative methods. A statistical analysis of proposed and exis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#65288;NPA&#65289;&#30340;&#28145;&#24230;&#21830;&#21697;&#20851;&#32852;&#25366;&#25496;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#22320;&#24314;&#27169;&#36141;&#29289;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#29992;&#25143;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26597;&#25214;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.16433</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#36827;&#34892;&#31726;&#20869;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Within-basket Recommendation via Neural Pattern Associator. (arXiv:2401.16433v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#65288;NPA&#65289;&#30340;&#28145;&#24230;&#21830;&#21697;&#20851;&#32852;&#25366;&#25496;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#22320;&#24314;&#27169;&#36141;&#29289;&#36807;&#31243;&#20013;&#30340;&#22797;&#26434;&#29992;&#25143;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26597;&#25214;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31726;&#20869;&#25512;&#33616;&#65288;WBR&#65289;&#26159;&#25351;&#22312;&#36141;&#29289;&#36807;&#31243;&#20013;&#20026;&#20102;&#23436;&#25104;&#19968;&#20010;&#38750;&#31354;&#36141;&#29289;&#31726;&#32780;&#25512;&#33616;&#21830;&#21697;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#21019;&#26032;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#24573;&#35270;&#20102;&#23454;&#38469;&#29992;&#25143;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#65292;&#27604;&#22914;1&#65289;&#22810;&#20010;&#36141;&#29289;&#24847;&#22270;&#30340;&#20849;&#23384;&#65292;2&#65289;&#36825;&#20123;&#24847;&#22270;&#30340;&#22810;&#31890;&#24230;&#21644;3&#65289;&#36141;&#29289;&#36807;&#31243;&#20013;&#30340;&#20132;&#32455;&#34892;&#20026;&#65288;&#20999;&#25442;&#24847;&#22270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#27169;&#24335;&#20851;&#32852;&#22120;&#65288;NPA&#65289;&#30340;&#28145;&#24230;&#21830;&#21697;&#20851;&#32852;&#25366;&#25496;&#27169;&#22411;&#65292;&#26126;&#30830;&#22320;&#24314;&#27169;&#20102;&#19978;&#36848;&#22240;&#32032;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21463;&#21040;&#21521;&#37327;&#37327;&#21270;&#30340;&#21551;&#21457;&#65292;NPA&#27169;&#22411;&#23398;&#20064;&#23558;&#24120;&#35265;&#30340;&#29992;&#25143;&#24847;&#22270;&#65288;&#25110;&#21830;&#21697;&#32452;&#21512;&#27169;&#24335;&#65289;&#32534;&#30721;&#20026;&#37327;&#21270;&#34920;&#31034;&#65288;&#20063;&#31216;&#20026;&#30721;&#26412;&#65289;&#65292;&#36825;&#20801;&#35768;&#22312;&#25512;&#29702;&#38454;&#27573;&#36890;&#36807;&#27880;&#24847;&#21147;&#39537;&#21160;&#30340;&#26597;&#25214;&#26469;&#35782;&#21035;&#29992;&#25143;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;&#36825;&#26679;&#20135;&#29983;&#30340;&#25512;&#33616;&#32467;&#26524;&#36830;&#36143;&#19988;&#33258;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within-basket recommendation (WBR) refers to the task of recommending items to the end of completing a non-empty shopping basket during a shopping session. While the latest innovations in this space demonstrate remarkable performance improvement on benchmark datasets, they often overlook the complexity of user behaviors in practice, such as 1) co-existence of multiple shopping intentions, 2) multi-granularity of such intentions, and 3) interleaving behavior (switching intentions) in a shopping session. This paper presents Neural Pattern Associator (NPA), a deep item-association-mining model that explicitly models the aforementioned factors. Specifically, inspired by vector quantization, the NPA model learns to encode common user intentions (or item-combination patterns) as quantized representations (a.k.a. codebook), which permits identification of users's shopping intentions via attention-driven lookup during the reasoning phase. This yields coherent and self-interpretable recommendat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#36716;&#21270;&#29575;&#39044;&#27979;&#12290;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#25361;&#25112;&#65292;&#28155;&#21152;&#38750;&#28857;&#20987;&#24402;&#22240;&#30340;&#36716;&#21270;&#20250;&#25439;&#22351;&#27169;&#22411;&#30340;&#26657;&#20934;&#65292;&#32780;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#33021;&#22815;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16432</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#24191;&#21578;&#20013;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25913;&#36827;&#36716;&#21270;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving conversion rate prediction via self-supervised pre-training in online advertising. (arXiv:2401.16432v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16432
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#30340;&#36716;&#21270;&#29575;&#39044;&#27979;&#12290;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#25361;&#25112;&#65292;&#28155;&#21152;&#38750;&#28857;&#20987;&#24402;&#22240;&#30340;&#36716;&#21270;&#20250;&#25439;&#22351;&#27169;&#22411;&#30340;&#26657;&#20934;&#65292;&#32780;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#33021;&#22815;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36716;&#21270;&#29575;&#26159;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#20248;&#21270;&#25237;&#26631;&#20197;&#28385;&#36275;&#24191;&#21578;&#20027;&#24615;&#33021;&#35201;&#27714;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23835;&#36215;&#65292;&#20294;&#36825;&#20123;&#39044;&#27979;&#36890;&#24120;&#30001;&#20998;&#35299;&#26426;&#65288;FM&#65289;&#36827;&#34892;&#65292;&#29305;&#21035;&#26159;&#22312;&#25512;&#29702;&#24310;&#36831;&#33267;&#20851;&#37325;&#35201;&#30340;&#21830;&#19994;&#29615;&#22659;&#20013;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#36923;&#36753;&#22238;&#24402;&#26694;&#26550;&#35757;&#32451;&#65292;&#21033;&#29992;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#36807;&#21435;&#29992;&#25143;&#27963;&#21160;&#24418;&#25104;&#30340;&#26631;&#35760;&#34920;&#26684;&#25968;&#25454;&#12290;&#35768;&#22810;&#24191;&#21578;&#20027;&#21482;&#20851;&#24515;&#34987;&#28857;&#20987;&#23646;&#24615;&#30340;&#36716;&#21270;&#12290;&#39044;&#27979;&#32473;&#23450;&#28857;&#20987;&#30340;&#36716;&#21270;&#27169;&#22411;&#35757;&#32451;&#30340;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#25968;&#25454;&#31232;&#30095;&#24615; - &#28857;&#20987;&#24456;&#23569;&#65292;&#28857;&#20987;&#24402;&#22240;&#30340;&#36716;&#21270;&#26356;&#23569;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#38598;&#20013;&#28155;&#21152;&#38750;&#28857;&#20987;&#24402;&#22240;&#30340;&#36716;&#21270;&#26469;&#20943;&#36731;&#31232;&#30095;&#24615;&#20250;&#25439;&#22351;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;&#30001;&#20110;&#26657;&#20934;&#23545;&#23454;&#29616;&#24191;&#21578;&#20027;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#24605;&#24819;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of predicting conversion rates (CVR) lies at the heart of online advertising systems aiming to optimize bids to meet advertiser performance requirements. Even with the recent rise of deep neural networks, these predictions are often made by factorization machines (FM), especially in commercial settings where inference latency is key. These models are trained using the logistic regression framework on labeled tabular data formed from past user activity that is relevant to the task at hand.  Many advertisers only care about click-attributed conversions. A major challenge in training models that predict conversions-given-clicks comes from data sparsity - clicks are rare, conversions attributed to clicks are even rarer. However, mitigating sparsity by adding conversions that are not click-attributed to the training set impairs model calibration. Since calibration is critical to achieving advertiser goals, this is infeasible.  In this work we use the well-known idea of self-supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32467;&#21512;&#20027;&#39064;&#24314;&#27169;&#21644;&#24341;&#29992;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#26469;&#30740;&#31350;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20851;&#20110;&#23562;&#37325;&#31169;&#23494;&#21644;&#23478;&#24237;&#29983;&#27963;&#30340;&#26696;&#20363;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#21644;&#32452;&#32455;&#20855;&#26377;&#30456;&#20284;&#20027;&#39064;&#21644;&#24341;&#29992;&#27169;&#24335;&#30340;&#26696;&#20363;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#33021;&#22815;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16429</link><description>&lt;p&gt;
&#32467;&#21512;&#20027;&#39064;&#24314;&#27169;&#21644;&#24341;&#29992;&#32593;&#32476;&#20998;&#26512;&#30740;&#31350;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20851;&#20110;&#23562;&#37325;&#31169;&#20154;&#21644;&#23478;&#24237;&#29983;&#27963;&#26435;&#21033;&#30340;&#26696;&#20363;&#27861;
&lt;/p&gt;
&lt;p&gt;
Combining topic modelling and citation network analysis to study case law from the European Court on Human Rights on the right to respect for private and family life. (arXiv:2401.16429v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32467;&#21512;&#20027;&#39064;&#24314;&#27169;&#21644;&#24341;&#29992;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#29992;&#26469;&#30740;&#31350;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20851;&#20110;&#23562;&#37325;&#31169;&#23494;&#21644;&#23478;&#24237;&#29983;&#27963;&#30340;&#26696;&#20363;&#27861;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#21644;&#32452;&#32455;&#20855;&#26377;&#30456;&#20284;&#20027;&#39064;&#21644;&#24341;&#29992;&#27169;&#24335;&#30340;&#26696;&#20363;&#27861;&#65292;&#24182;&#19988;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#33021;&#22815;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;HUDOC&#31561;&#27861;&#24459;&#26696;&#20363;&#27861;&#25968;&#25454;&#24211;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#20026;&#20102;&#22788;&#29702;&#22914;&#27492;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#27861;&#24459;&#30740;&#31350;&#20154;&#21592;&#25214;&#21040;&#39640;&#25928;&#30340;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31181;&#26696;&#20363;&#27861;&#25968;&#25454;&#24211;&#36890;&#24120;&#21253;&#21547;&#26696;&#20214;&#30340;&#25991;&#26412;&#20869;&#23481;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#24341;&#29992;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#26469;&#33258;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20851;&#20110;&#27431;&#27954;&#20154;&#26435;&#20844;&#32422;&#31532;8&#26465;&#20851;&#20110;&#23562;&#37325;&#31169;&#20154;&#21644;&#23478;&#24237;&#29983;&#27963;&#12289;&#23478;&#24237;&#21644;&#36890;&#20449;&#26435;&#21033;&#30340;&#26696;&#20363;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#24182;&#27604;&#36739;&#20102;&#20027;&#39064;&#24314;&#27169;&#21644;&#24341;&#29992;&#32593;&#32476;&#22312;&#26681;&#25454;&#19968;&#33324;&#20027;&#39064;&#21644;&#24341;&#29992;&#27169;&#24335;&#25214;&#21040;&#21644;&#32452;&#32455;&#31532;8&#26465;&#26696;&#20363;&#27861;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#32467;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#26159;&#21542;&#27604;&#20165;&#24212;&#29992;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#25163;&#24037;&#25910;&#38598;&#21644;&#27880;&#37322;&#30340;&#20851;&#20110;&#39537;&#36880;&#30340;&#31532;8&#26465;&#26696;&#20363;&#27861;&#29420;&#29305;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#32452;&#21512;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;&#20351;&#29992;&#36825;&#20004;&#31181;&#26041;&#27861;&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As legal case law databases such as HUDOC continue to grow rapidly, it has become essential for legal researchers to find efficient methods to handle such large-scale data sets. Such case law databases usually consist of the textual content of cases together with the citations between them. This paper focuses on case law from the European Court of Human Rights on Article 8 of the European Convention of Human Rights, the right to respect private and family life, home and correspondence. In this study, we demonstrate and compare the potential of topic modelling and citation network to find and organize case law on Article 8 based on their general themes and citation patterns, respectively. Additionally, we explore whether combining these two techniques leads to better results compared to the application of only one of the methods. We evaluate the effectiveness of the combined method on a unique manually collected and annotated dataset of Aricle 8 case law on evictions. The results of our
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#37096;&#20998;&#27169;&#25311;&#20013;&#25512;&#26029;&#20986;&#30340;&#26041;&#24335;&#65292;&#20026;&#27169;&#25311;&#20248;&#21270;&#22120;&#25552;&#20379;&#38750;&#27491;&#24335;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2401.16426</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#37096;&#20998;&#27169;&#25311;&#20013;&#25512;&#26029;&#20986;&#30340;&#26041;&#24335;&#65292;&#20026;&#27169;&#25311;&#20248;&#21270;&#22120;&#25552;&#20379;&#38750;&#27491;&#24335;&#30340;&#23433;&#20840;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Informal Safety Guarantees for Simulated Optimizers Through Extrapolation from Partial Simulations. (arXiv:2401.16426v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16426
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#37096;&#20998;&#27169;&#25311;&#20013;&#25512;&#26029;&#20986;&#30340;&#26041;&#24335;&#65292;&#20026;&#27169;&#25311;&#20248;&#21270;&#22120;&#25552;&#20379;&#38750;&#27491;&#24335;&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26159;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#26377;&#20154;&#35748;&#20026;&#65292;&#22312;&#33258;&#25105;&#30417;&#30563;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39044;&#27979;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#20869;&#37096;&#34920;&#31034;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#21487;&#33021;&#37197;&#32622;&#30340;&#23454;&#20307;&#65292;&#21363;&#27169;&#25311;&#22120;&#12290;&#22312;&#36825;&#20010;&#20551;&#35774;&#19979;&#65292;&#22522;&#20110;&#23884;&#20837;&#24335;&#20195;&#29702;&#30340;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#27169;&#22411;&#26500;&#24314;&#20102;&#27169;&#25311;&#22120;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23558;&#20108;&#32500;&#22352;&#26631;&#31995;&#25193;&#23637;&#21040;&#20219;&#24847;&#32500;&#24230;&#26469;&#23454;&#29616;&#22810;&#20195;&#29702;&#19990;&#30028;&#65292;&#20808;&#21069;&#30340;&#25991;&#29486;&#36873;&#25321;&#20351;&#29992;&#22312;&#22352;&#26631;&#31995;&#19978;&#25805;&#20316;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#21033;&#29992;&#23610;&#24230;&#23610;&#23544;&#21464;&#21270;&#30340;&#21464;&#20307;&#34987;&#31216;&#20026;&#31515;&#21345;&#23572;&#23545;&#35937;&#65292;&#24182;&#29992;&#20110;&#34920;&#31034;&#27169;&#25311;&#65288;&#20854;&#20013;&#20010;&#20307;&#30340;&#27169;&#25311;&#26159;&#35813;&#23545;&#35937;&#20013;&#30340;&#20195;&#29702;&#21644;&#35774;&#22791;&#65289;&#12290;&#22312;&#31515;&#21345;&#23572;&#23545;&#35937;&#21608;&#22260;&#65292;&#36890;&#36807;&#32771;&#34385;&#20196;&#29260;&#36873;&#25321;&#21644;&#27169;&#25311;&#22797;&#26434;&#24615;&#65292;&#23545;&#27169;&#25311;&#22120;&#30340;&#34892;&#20026;&#36827;&#34892;&#24418;&#24335;&#21270;&#65292;&#24182;&#20351;&#29992;Lobian&#38556;&#30861;&#35777;&#26126;&#20102;&#36890;&#36807;&#26816;&#26597;&#27169;&#25311;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#30340;&#23545;&#40784;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is the backbone of state of the art language modeling. It has been argued that training with predictive loss on a self-supervised dataset causes simulators: entities that internally represent possible configurations of real-world systems. Under this assumption, a mathematical model for simulators is built based in the Cartesian frames model of embedded agents, which is extended to multi-agent worlds through scaling a two-dimensional frame to arbitrary dimensions, where literature prior chooses to instead use operations on frames. This variant leveraging scaling dimensionality is named the Cartesian object, and is used to represent simulations (where individual simulacra are the agents and devices in that object). Around the Cartesian object, functions like token selection and simulation complexity are accounted for in formalizing the behavior of a simulator, and used to show (through the L\"obian obstacle) that a proof of alignment between simulacra by inspecti
&lt;/p&gt;</description></item><item><title>cDVGAN&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#22810;&#31867;&#24341;&#21147;&#27874;&#20449;&#21495;&#21644;&#25506;&#27979;&#22120;&#25925;&#38556;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#37492;&#21035;&#22120;&#20998;&#26512;&#19968;&#38454;&#23548;&#25968;&#26102;&#38388;&#24207;&#21015;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.16356</link><description>&lt;p&gt;
cDVGAN: &#19968;&#20010;&#28789;&#27963;&#30340;&#27169;&#22411;&#29992;&#20110;&#22810;&#31867;&#24341;&#21147;&#27874;&#20449;&#21495;&#21644;&#25925;&#38556;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and Glitch Generation. (arXiv:2401.16356v2 [physics.ins-det] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16356
&lt;/p&gt;
&lt;p&gt;
cDVGAN&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#22810;&#31867;&#24341;&#21147;&#27874;&#20449;&#21495;&#21644;&#25506;&#27979;&#22120;&#25925;&#38556;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#37492;&#21035;&#22120;&#20998;&#26512;&#19968;&#38454;&#23548;&#25968;&#26102;&#38388;&#24207;&#21015;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#30495;&#23454;&#30340;&#26102;&#38388;&#22495;&#24341;&#21147;&#27874;&#65288;GWs&#65289;&#35266;&#27979;&#21644;GW&#25506;&#27979;&#22120;&#25925;&#38556;&#21487;&#20197;&#24110;&#21161;&#25512;&#36827;GW&#25968;&#25454;&#20998;&#26512;&#12290;&#27169;&#25311;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#29992;&#20110;&#20449;&#21495;&#25628;&#32034;&#30340;&#25968;&#25454;&#38598;&#65292;&#24179;&#34913;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#39564;&#35777;&#26816;&#27979;&#26041;&#26696;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;cDVGAN&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#30340;&#26032;&#22411;&#26465;&#20214;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#20195;&#34920;&#24341;&#21147;&#27874;&#65288;GWs&#65289;&#21644;&#25506;&#27979;&#22120;&#25925;&#38556;&#30340;&#22810;&#31181;&#31867;&#21035;&#30340;&#26102;&#38388;&#22495;&#35266;&#27979;&#12290;cDVGAN&#36824;&#21487;&#20197;&#36890;&#36807;&#22312;&#26465;&#20214;&#31867;&#21035;&#21521;&#37327;&#20013;&#36827;&#34892;&#25554;&#20540;&#29983;&#25104;&#36328;&#31867;&#21035;&#21464;&#21270;&#30340;&#24191;&#20041;&#28151;&#21512;&#26679;&#26412;&#12290;cDVGAN&#22312;&#20856;&#22411;&#30340;GANs&#30340;&#20108;&#20154;&#23545;&#25239;&#21338;&#24328;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#21442;&#19982;&#32773;&#65292;&#20854;&#20013;&#19968;&#20010;&#36741;&#21161;&#37492;&#21035;&#22120;&#20998;&#26512;&#19968;&#38454;&#23548;&#25968;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#25552;&#20379;&#20102;&#26356;&#22909;&#22320;&#25429;&#25417;&#21407;&#22987;&#25968;&#25454;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating realistic time-domain observations of gravitational waves (GWs) and GW detector glitches can help in advancing GW data analysis. Simulated data can be used in downstream tasks by augmenting datasets for signal searches, balancing data sets for machine learning, and validating detection schemes. In this work, we present Conditional Derivative GAN (cDVGAN), a novel conditional model in the Generative Adversarial Network framework for simulating multiple classes of time-domain observations that represent gravitational waves (GWs) and detector glitches. cDVGAN can also generate generalized hybrid samples that span the variation between classes through interpolation in the conditioned class vector. cDVGAN introduces an additional player into the typical 2-player adversarial game of GANs, where an auxiliary discriminator analyzes the first-order derivative time-series. Our results show that this provides synthetic data that better captures the features of the original data. cDVGAN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;rPDP-FL&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#8220;&#27169;&#25311;-&#26354;&#32447;&#25311;&#21512;&#8221;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#35760;&#24405;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.16251</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cross-silo Federated Learning with Record-level Personalized Differential Privacy. (arXiv:2401.16251v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;rPDP-FL&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#8220;&#27169;&#25311;-&#26354;&#32447;&#25311;&#21512;&#8221;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#35760;&#24405;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#22686;&#24378;&#30340;&#32852;&#21512;&#23398;&#20064;&#25104;&#20026;&#20102;&#20445;&#25252;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#26041;&#26696;&#36890;&#24120;&#20551;&#35774;&#25152;&#26377;&#35760;&#24405;&#30340;&#38544;&#31169;&#39044;&#31639;&#22343;&#30456;&#21516;&#65292;&#25552;&#20379;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#35760;&#24405;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#33021;&#26080;&#27861;&#28385;&#36275;&#27599;&#20010;&#35760;&#24405;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#26410;&#30693;&#39046;&#22495;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;rPDP-FL&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#37319;&#29992;&#20004;&#38454;&#27573;&#28151;&#21512;&#25277;&#26679;&#26041;&#26696;&#65292;&#26082;&#21253;&#25324;&#23458;&#25143;&#31471;&#32423;&#21035;&#25277;&#26679;&#65292;&#21448;&#21253;&#25324;&#38750;&#22343;&#21248;&#35760;&#24405;&#32423;&#21035;&#25277;&#26679;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;&#19968;&#20010;&#20851;&#38190;&#19988;&#38750;&#24179;&#20961;&#30340;&#38382;&#39064;&#26159;&#22312;&#32473;&#23450;&#20010;&#24615;&#21270;&#38544;&#31169;&#39044;&#31639;&#949;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#29702;&#24819;&#30340;&#27599;&#35760;&#24405;&#25277;&#26679;&#27010;&#29575;q&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#8220;&#27169;&#25311;-&#26354;&#32447;&#25311;&#21512;&#8221;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning enhanced by differential privacy has emerged as a popular approach to better safeguard the privacy of client-side data by protecting clients' contributions during the training process. Existing solutions typically assume a uniform privacy budget for all records and provide one-size-fits-all solutions that may not be adequate to meet each record's privacy requirement. In this paper, we explore the uncharted territory of cross-silo FL with record-level personalized differential privacy. We devise a novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme with both client-level sampling and non-uniform record-level sampling to accommodate varying privacy requirements. A critical and non-trivial problem is to select the ideal per-record sampling probability q given the personalized privacy budget {\epsilon}. We introduce a versatile solution named Simulation-CurveFitting, allowing us to uncover a significant insight into the nonlinear correlation betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#20026;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2401.15935</link><description>&lt;p&gt;
&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65306;&#29983;&#25104;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning. (arXiv:2401.15935v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#20026;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#33719;&#21462;&#20107;&#20214;&#24207;&#21015;&#34920;&#31034;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#36825;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#27169;&#24577;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#38134;&#34892;&#12289;&#30005;&#23376;&#21830;&#21153;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#25105;&#20204;&#23545;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20998;&#21035;&#24212;&#29992;&#20102;&#23427;&#20204;&#12290;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#32477;&#23545;&#20248;&#36234;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32467;&#21512;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#20316;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#65292;&#20174;&#24403;&#20195;&#22810;&#27169;&#24577;&#30740;&#31350;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#26041;&#27861;&#36890;&#24120;&#34987;&#35270;&#20026;&#20114;&#26021;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#23427;&#20204;&#30340;&#32852;&#21512;&#25506;&#32034;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#33267;&#23569;&#19982;&#29616;&#26377;&#26041;&#27861;&#25345;&#24179;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#26356;&#21152;&#26222;&#36866;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates self-supervised learning techniques to obtain representations of Event Sequences. It is a key modality in various applications, including but not limited to banking, e-commerce, and healthcare.  We perform a comprehensive study of generative and contrastive approaches in self-supervised learning, applying them both independently. We find that there is no single supreme method. Consequently, we explore the potential benefits of combining these approaches. To achieve this goal, we introduce a novel method that aligns generative and contrastive embeddings as distinct modalities, drawing inspiration from contemporary multimodal research.  Generative and contrastive approaches are often treated as mutually exclusive, leaving a gap for their combined exploration. Our results demonstrate that this aligned model performs at least on par with, and mostly surpasses, existing methods and is more universal across a variety of tasks. Furthermore, we demonstrate that self-sup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#19982;&#26368;&#26032;&#30340;&#20915;&#31574;&#29702;&#35770;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#20248;&#21270;&#20934;&#21017;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#33719;&#24471;&#26377;&#31283;&#23450;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.15771</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#19982;&#25968;&#25454;&#39537;&#21160;&#40065;&#26834;&#20248;&#21270;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Bayesian Nonparametrics meets Data-Driven Robust Optimization. (arXiv:2401.15771v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#19982;&#26368;&#26032;&#30340;&#20915;&#31574;&#29702;&#35770;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#20248;&#21270;&#20934;&#21017;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#20013;&#33719;&#24471;&#26377;&#31283;&#23450;&#24615;&#21644;&#20248;&#36234;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#27169;&#22411;&#36890;&#24120;&#28041;&#21450;&#20248;&#21270;&#25968;&#25454;&#39537;&#21160;&#30340;&#39118;&#38505;&#20934;&#21017;&#12290;&#39118;&#38505;&#36890;&#24120;&#26159;&#26681;&#25454;&#32463;&#39564;&#25968;&#25454;&#20998;&#24067;&#35745;&#31639;&#30340;&#65292;&#20294;&#30001;&#20110;&#20998;&#24067;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19981;&#31283;&#23450;&#21644;&#19981;&#22909;&#30340;&#26679;&#26412;&#22806;&#34920;&#29616;&#12290;&#22312;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#30340;&#31934;&#31070;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#40065;&#26834;&#20934;&#21017;&#65292;&#23558;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#65288;&#21363;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#65289;&#29702;&#35770;&#21644;&#26368;&#36817;&#30340;&#24179;&#28369;&#27169;&#31946;&#35268;&#36991;&#20559;&#22909;&#30340;&#20915;&#31574;&#29702;&#35770;&#27169;&#22411;&#30340;&#35265;&#35299;&#30456;&#32467;&#21512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19982;&#26631;&#20934;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#25216;&#26415;&#30340;&#26032;&#36830;&#25509;&#65292;&#20854;&#20013;&#21253;&#25324;&#23725;&#22238;&#24402;&#21644;&#22871;&#32034;&#22238;&#24402;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#40065;&#26834;&#20248;&#21270;&#36807;&#31243;&#22312;&#26377;&#38480;&#26679;&#26412;&#21644;&#28176;&#36817;&#32479;&#35745;&#20445;&#35777;&#26041;&#38754;&#30340;&#26377;&#21033;&#24615;&#23384;&#22312;&#12290;&#23545;&#20110;&#23454;&#38469;&#23454;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#34920;&#31034;&#30340;&#21487;&#34892;&#36817;&#20284;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning and statistical models often involves optimizing a data-driven risk criterion. The risk is usually computed with respect to the empirical data distribution, but this may result in poor and unstable out-of-sample performance due to distributional uncertainty. In the spirit of distributionally robust optimization, we propose a novel robust criterion by combining insights from Bayesian nonparametric (i.e., Dirichlet Process) theory and recent decision-theoretic models of smooth ambiguity-averse preferences. First, we highlight novel connections with standard regularized empirical risk minimization techniques, among which Ridge and LASSO regressions. Then, we theoretically demonstrate the existence of favorable finite-sample and asymptotic statistical guarantees on the performance of the robust optimization procedure. For practical implementation, we propose and study tractable approximations of the criterion based on well-known Dirichlet Process representations. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;T-Rex&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#20013;&#36873;&#25321;&#23569;&#25968;&#30456;&#20851;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#26368;&#36817;&#37051;&#24809;&#32602;&#26426;&#21046;&#65292;&#21487;&#38752;&#25511;&#21046;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36807;&#21435;20&#24180;&#20869;&#22522;&#20110;&#23569;&#37327;&#32929;&#31080;&#20934;&#30830;&#36319;&#36394;&#26631;&#20934;&#26222;&#23572;500&#25351;&#25968;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.15139</link><description>&lt;p&gt;
FDR&#25511;&#21046;&#30340;&#31232;&#30095;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
FDR-Controlled Portfolio Optimization for Sparse Financial Index Tracking. (arXiv:2401.15139v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;T-Rex&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#31232;&#30095;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#20013;&#36873;&#25321;&#23569;&#25968;&#30456;&#20851;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#26368;&#36817;&#37051;&#24809;&#32602;&#26426;&#21046;&#65292;&#21487;&#38752;&#25511;&#21046;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36807;&#21435;20&#24180;&#20869;&#22522;&#20110;&#23569;&#37327;&#32929;&#31080;&#20934;&#30830;&#36319;&#36394;&#26631;&#20934;&#26222;&#23572;500&#25351;&#25968;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#22914;&#37329;&#34701;&#25351;&#25968;&#36319;&#36394;&#25110;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#65292;&#20851;&#38190;&#26159;&#22312;&#20445;&#25345;&#23545;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#30340;&#25511;&#21046;&#30340;&#21516;&#26102;&#36873;&#25321;&#23569;&#25968;&#30456;&#20851;&#21464;&#37327;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#21464;&#37327;&#20043;&#38388;&#32463;&#24120;&#23384;&#22312;&#24378;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#32929;&#31080;&#25910;&#30410;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#21066;&#24369;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;&#27169;&#22411;X knockoff&#26041;&#27861;&#25110;T-Rex&#36873;&#25321;&#22120;&#65289;&#30340;FDR&#25511;&#21046;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;T-Rex&#26694;&#26550;&#65292;&#20197;&#36866;&#24212;&#39640;&#24230;&#30456;&#20851;&#21464;&#37327;&#30340;&#37325;&#21472;&#32452;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#26368;&#36817;&#37051;&#24809;&#32602;&#26426;&#21046;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#23454;&#29616;&#30340;&#65292;&#35813;&#26426;&#21046;&#33021;&#22815;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#27700;&#24179;&#19978;&#21487;&#38752;&#25511;&#21046;FDR&#12290;&#31232;&#30095;&#25351;&#25968;&#36319;&#36394;&#30340;&#23454;&#20363;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#36807;&#21435;20&#24180;&#20869;&#22522;&#20110;&#23569;&#37327;&#32929;&#31080;&#20934;&#30830;&#36319;&#36394;&#26631;&#20934;&#26222;&#23572;500&#25351;&#25968;&#30340;&#33021;&#21147;&#12290;&#22312;CRAN&#19978;&#25552;&#20379;&#20102;R&#21253;TRexSelector&#30340;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high-dimensional data analysis, such as financial index tracking or biomedical applications, it is crucial to select the few relevant variables while maintaining control over the false discovery rate (FDR). In these applications, strong dependencies often exist among the variables (e.g., stock returns), which can undermine the FDR control property of existing methods like the model-X knockoff method or the T-Rex selector. To address this issue, we have expanded the T-Rex framework to accommodate overlapping groups of highly correlated variables. This is achieved by integrating a nearest neighbors penalization mechanism into the framework, which provably controls the FDR at the user-defined target level. A real-world example of sparse index tracking demonstrates the proposed method's ability to accurately track the S&amp;P 500 index over the past 20 years based on a small number of stocks. An open-source implementation is provided within the R package TRexSelector on CRAN.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;MCTS&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;SR-GPT&#65292;&#22312;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#20844;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.14424</link><description>&lt;p&gt;
&#36890;&#36807;GPT&#24341;&#23548;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#25968;&#23398;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;MCTS&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;SR-GPT&#65292;&#22312;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#20844;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#25214;&#21040;&#19968;&#20010;&#31616;&#27905;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#20844;&#24335;&#26469;&#20934;&#30830;&#25551;&#36848;&#25968;&#25454;&#20013;&#27599;&#20010;&#21464;&#37327;&#19982;&#39044;&#27979;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20063;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#65292;&#26159;&#19968;&#20010;NP&#22256;&#38590;&#38382;&#39064;&#12290;&#21435;&#24180;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;sota&#12290;&#34429;&#28982;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#22312;&#24674;&#22797;&#30446;&#26631;&#34920;&#36798;&#24335;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#25913;&#36827;&#65292;&#20294;&#26159;&#22312;MCTS&#36807;&#31243;&#20013;&#32570;&#20047;&#24341;&#23548;&#20005;&#37325;&#38459;&#30861;&#20102;&#20854;&#25628;&#32034;&#25928;&#29575;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#31639;&#27861;&#22312;MCTS&#30340;&#25628;&#32034;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#20294;&#26159;&#36825;&#20010;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#24456;&#24046;&#12290;&#20026;&#20102;&#24179;&#34913;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SR-GPT&#65292;&#32467;&#21512;&#20102;AlphaZero&#30340;&#24605;&#24819;&#12290;SR-GPT&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#65292;&#23558;MCTS&#19982;&#19968;&#20010;&#36890;&#29992;&#24615;&#36739;&#22909;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. Last year, a symbolic regression method based on Monte Carlo Tree Search (MCTS) was proposed and sota was obtained on multiple datasets. While this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To balance efficiency and generality, we propose SR-GPT combining ideas from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines MCTS with a Gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#22522;&#26412;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.14423</link><description>&lt;p&gt;
&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#65306;&#20171;&#32461;&#19982;&#39640;&#32423;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompt Design and Engineering: Introduction and Advanced Methods. (arXiv:2401.14423v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#30340;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#22522;&#26412;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#22312;&#36807;&#21435;&#20960;&#20010;&#26376;&#20013;&#25104;&#20026;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#23398;&#31185;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20027;&#35201;&#27010;&#24565;&#65292;&#24182;&#22238;&#39038;&#20102;&#25552;&#31034;&#35774;&#35745;&#19982;&#24037;&#31243;&#30340;&#22522;&#26412;&#21644;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt design and engineering has become an important discipline in just the past few months. In this paper, we provide an introduction to the main concepts as well as review basic and more advanced approaches to prompt design and engineering.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.13802</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#20195;&#30721;&#29983;&#25104;&#12290;LLMs&#20027;&#35201;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;/&#23569;&#26679;&#26412;&#33539;&#24335;&#20013;&#34987;&#29992;&#20110;&#25351;&#23548;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#65288;CCD&#65289;&#36825;&#19968;&#38750;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#23478;&#26063;&#8212;&#8212;&#19978;&#19979;&#25991;&#35821;&#35328;&#23398;&#20064;&#65288;ICLL&#65289;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;&#12290;&#22312;ICLL&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#19982;&#32473;&#23450;&#24418;&#24335;&#35821;&#35328;&#30456;&#21516;&#30340;&#23383;&#31526;&#20018;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29702;&#35299;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20197;&#21450;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.12973</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#35821;&#35328;&#23398;&#20064;&#65306;&#26550;&#26500;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Language Learning: Architectures and Algorithms. (arXiv:2401.12973v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#23478;&#26063;&#8212;&#8212;&#19978;&#19979;&#25991;&#35821;&#35328;&#23398;&#20064;&#65288;ICLL&#65289;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#33021;&#21147;&#12290;&#22312;ICLL&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#19982;&#32473;&#23450;&#24418;&#24335;&#35821;&#35328;&#30456;&#21516;&#30340;&#23383;&#31526;&#20018;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#29702;&#35299;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20197;&#21450;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20196;&#20154;&#24778;&#21497;&#30340;&#33021;&#21147;&#65306;&#23427;&#20204;&#33021;&#22815;&#20174;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#26032;&#30340;&#20989;&#25968;&#12290;&#30446;&#21069;&#65292;&#25105;&#20204;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#21457;&#29983;&#30340;&#20102;&#35299;&#20027;&#35201;&#26469;&#33258;&#20110;&#22312;&#26497;&#20854;&#31616;&#21333;&#30340;&#23398;&#20064;&#38382;&#39064;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;&#32447;&#24615;&#22238;&#24402;&#21644;&#20851;&#32852;&#35760;&#24518;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#38382;&#39064;&#19982;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#30340;&#8220;&#30495;&#27491;&#8221;&#19978;&#19979;&#25991;&#23398;&#20064;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#21518;&#32773;&#19981;&#20165;&#28041;&#21450;&#26816;&#32034;&#21644;&#20989;&#25968;&#36817;&#20284;&#65292;&#36824;&#21253;&#25324;&#20102;&#33258;&#30001;&#29983;&#25104;&#35821;&#35328;&#21644;&#20854;&#20182;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#19968;&#20010;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#35821;&#35328;&#23398;&#20064;&#65288;ICLL&#65289;&#30340;&#26032;&#22411;&#38382;&#39064;&#23478;&#26063;&#65292;&#26469;&#25506;&#35752;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#22312;ICLL&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#34987;&#21576;&#29616;&#19968;&#32452;&#26469;&#33258;&#24418;&#24335;&#35821;&#35328;&#30340;&#23383;&#31526;&#20018;&#65292;&#24182;&#38656;&#35201;&#29983;&#25104;&#19982;&#35813;&#35821;&#35328;&#30456;&#21516;&#30340;&#20854;&#20182;&#23383;&#31526;&#20018;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#36890;&#36807;&#38543;&#26426;&#26377;&#38480;&#33258;&#21160;&#26426;&#29983;&#25104;&#30340;&#27491;&#21017;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#31181;&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#65288;&#21253;&#25324;&#20960;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale neural language models exhibit a remarkable capacity for in-context learning (ICL): they can infer novel functions from datasets provided as input. Most of our current understanding of when and how ICL arises comes from LMs trained on extremely simple learning problems like linear regression and associative recall. There remains a significant gap between these model problems and the "real" ICL exhibited by LMs trained on large text corpora, which involves not just retrieval and function approximation but free-form generation of language and other structured outputs. In this paper, we study ICL through the lens of a new family of model problems we term in context language learning (ICLL). In ICLL, LMs are presented with a set of strings from a formal language, and must generate additional strings from the same language. We focus on in-context learning of regular languages generated by random finite automata. We evaluate a diverse set of neural sequence models (including seve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#22686;&#24378;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65288;CCEC&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#36830;&#25509;&#22359;&#24182;&#20837;&#29305;&#24449;&#34920;&#31034;&#20013;&#65292;&#20197;&#20445;&#25345;&#22810;&#20010;&#35270;&#22270;&#38388;&#30340;&#19968;&#33268;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35889;&#32858;&#31867;&#25913;&#21892;&#32858;&#31867;&#30340;&#34920;&#31034;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12648</link><description>&lt;p&gt;
&#22522;&#20110;&#19968;&#33268;&#24615;&#22686;&#24378;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Consistency Enhancement-Based Deep Multiview Clustering via Contrastive Learning. (arXiv:2401.12648v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#22686;&#24378;&#30340;&#28145;&#24230;&#22810;&#35270;&#22270;&#32858;&#31867;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65288;CCEC&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#36830;&#25509;&#22359;&#24182;&#20837;&#29305;&#24449;&#34920;&#31034;&#20013;&#65292;&#20197;&#20445;&#25345;&#22810;&#20010;&#35270;&#22270;&#38388;&#30340;&#19968;&#33268;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#35889;&#32858;&#31867;&#25913;&#21892;&#32858;&#31867;&#30340;&#34920;&#31034;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#22270;&#32858;&#31867;&#65288;MVC&#65289;&#36890;&#36807;&#32508;&#21512;&#22810;&#20010;&#35270;&#22270;&#30340;&#20449;&#24687;&#65292;&#23558;&#25968;&#25454;&#26679;&#26412;&#20998;&#20026;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;&#32780;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;MVC&#22330;&#26223;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#27867;&#21270;&#29305;&#24449;&#34920;&#31034;&#24182;&#20445;&#25345;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#29616;&#26377;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#22312;&#32858;&#31867;&#36807;&#31243;&#20013;&#24573;&#30053;&#20102;&#32858;&#31867;&#34920;&#31034;&#30340;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#30340;&#19968;&#33268;&#22686;&#24378;&#22411;&#28145;&#24230;MVC&#26041;&#27861;&#65288;CCEC&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23558;&#35821;&#20041;&#36830;&#25509;&#22359;&#24182;&#20837;&#29305;&#24449;&#34920;&#31034;&#20013;&#65292;&#20197;&#20445;&#25345;&#22810;&#20010;&#35270;&#22270;&#38388;&#30340;&#19968;&#33268;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35889;&#32858;&#31867;&#25913;&#21892;&#32858;&#31867;&#30340;&#34920;&#31034;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#22810;&#20010;&#35270;&#22270;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multiview clustering (MVC) segregates data samples into meaningful clusters by synthesizing information across multiple views. Moreover, deep learning-based methods have demonstrated their strong feature learning capabilities in MVC scenarios. However, effectively generalizing feature representations while maintaining consistency is still an intractable problem. In addition, most existing deep clustering methods based on contrastive learning overlook the consistency of the clustering representations during the clustering process. In this paper, we show how the above problems can be overcome and propose a consistent enhancement-based deep MVC method via contrastive learning (CCEC). Specifically, semantic connection blocks are incorporated into a feature representation to preserve the consistent information among multiple views. Furthermore, the representation process for clustering is enhanced through spectral clustering, and the consistency across multiple views is improved. Experiment
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TTG-NN&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25345;&#20037;&#21516;&#35843;&#12289;&#22270;&#21367;&#31215;&#21644;&#24352;&#37327;&#36816;&#31639;&#65292;&#21516;&#26102;&#25429;&#25417;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#65288;TT&#65289;&#21644;Tensor&#35270;&#22270;&#22270;&#65288;TG&#65289;&#32467;&#26500;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.12007</link><description>&lt;p&gt;
Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tensor-view Topological Graph Neural Network. (arXiv:2401.12007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12007
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TTG-NN&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25345;&#20037;&#21516;&#35843;&#12289;&#22270;&#21367;&#31215;&#21644;&#24352;&#37327;&#36816;&#31639;&#65292;&#21516;&#26102;&#25429;&#25417;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#65288;TT&#65289;&#21644;Tensor&#35270;&#22270;&#22270;&#65288;TG&#65289;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#23398;&#20064;&#20219;&#21153;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36817;&#24180;&#26469;&#22312;&#22270;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#22270;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;GNNs&#22312;&#24615;&#33021;&#19978;&#22788;&#20110;&#26368;&#21069;&#27839;&#65292;&#20294;&#23427;&#20204;&#21482;&#20351;&#29992;&#20102;&#27599;&#20010;&#33410;&#28857;&#21608;&#22260;&#38750;&#24120;&#26377;&#38480;&#30340;&#37051;&#22495;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#20002;&#22833;&#21644;&#36807;&#22810;&#35745;&#31639;&#30340;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TTG-NN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25345;&#20037;&#21516;&#35843;&#12289;&#22270;&#21367;&#31215;&#21644;&#24352;&#37327;&#36816;&#31639;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#21516;&#26102;&#25429;&#25417;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#65288;TT&#65289;&#21644;Tensor&#35270;&#22270;&#22270;&#65288;TG&#65289;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#30340;&#25299;&#25169;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph classification is an important learning task for graph-structured data. Graph neural networks (GNNs) have recently gained growing attention in graph learning and have shown significant improvements in many important graph problems. Despite their state-of-the-art performances, existing GNNs only use local information from a very limited neighborhood around each node, suffering from loss of multi-modal information and overheads of excessive computation. To address these issues, we propose a novel Tensor-view Topological Graph Neural Network (TTG-NN), a class of simple yet effective topological deep learning built upon persistent homology, graph convolution, and tensor operations. This new method incorporates tensor learning to simultaneously capture Tensor-view Topological (TT), as well as Tensor-view Graph (TG) structural information on both local and global levels. Computationally, to fully exploit graph topology and structure, we propose two flexible TT and TG representation lea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.10746</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#33041;&#30005;&#35299;&#30721;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#32463;&#24120;&#29992;&#20110;&#21508;&#31181;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#22823;&#37327;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#65292;&#36801;&#31227;&#23398;&#20064;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;DL&#27169;&#22411;&#12290;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#26159;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#65288;EA&#65289;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#20351;&#29992;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#20302;&#24182;&#19988;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20854;&#23545;&#20849;&#20139;&#21644;&#20010;&#20307;DL&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;EA&#19982;DL&#30456;&#32467;&#21512;&#22312;&#35299;&#30721;BCI&#20449;&#21495;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;EA&#26469;&#35757;&#32451;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#20849;&#20139;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#26032;&#21463;&#35797;&#32773;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#23558;&#30446;&#26631;&#21463;&#35797;&#32773;&#30340;&#35299;&#30721;&#29575;&#25552;&#39640;&#20102;4.33&#65285;&#65292;&#24182;&#19988;&#25910;&#25947;&#26102;&#38388;&#32553;&#30701;&#20102;&#36229;&#36807;70&#65285;&#12290;&#25105;&#20204;&#36824;&#20026;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10337</link><description>&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#26159;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#22312;&#25991;&#26412;&#30693;&#35782;&#24211;&#20013;&#26377;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20889;&#20316;&#20013;&#35782;&#21035;TTPs&#65292;&#36890;&#24120;&#31216;&#20026;TTP&#26144;&#23556;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20197;&#32463;&#20856;&#30340;&#22810;&#31867;&#25110;&#22810;&#26631;&#31614;&#20998;&#31867;&#35774;&#32622;&#20026;&#30446;&#26631;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#31867;&#21035;&#65288;&#21363;TTPs&#65289;&#65292;&#26631;&#31614;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#22797;&#26434;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#31181;&#35774;&#32622;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#23558;&#25991;&#26412;&#19982;TTP&#26631;&#31614;&#20043;&#38388;&#30340;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#20026;&#25991;&#26412;&#20998;&#37197;&#32473;TTP&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20165;&#20165;&#22312;&#22823;&#22411;&#26631;&#31614;&#31354;&#38388;&#19978;&#31454;&#20105;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#25928;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#27604;&#36739;&#26426;&#21046;&#30340;&#31070;&#32463;&#21305;&#37197;&#26550;&#26500;&#65292;&#20419;&#36827;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#21644;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#25506;&#32034;&#19968;&#31181;&#26356;&#23569;&#20405;&#20837;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#21644;&#20415;&#25658;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2401.10107</link><description>&lt;p&gt;
&#26631;&#20934;&#22810;&#23548;&#30561;&#30496;&#22270;&#19982;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;&#27604;&#36739;&#20998;&#26512;&#65306;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparison analysis between standard polysomnographic data and in-ear-EEG signals: A preliminary study. (arXiv:2401.10107v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#26631;&#20934;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#21644;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;&#30456;&#20284;&#24615;&#65292;&#26088;&#22312;&#25506;&#32034;&#19968;&#31181;&#26356;&#23569;&#20405;&#20837;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#21644;&#20415;&#25658;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#30740;&#31350;&#30830;&#23450;&#20102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30446;&#30340;&#65306;&#22810;&#23548;&#30561;&#30496;&#22270;&#65288;PSG&#65289;&#30446;&#21069;&#34987;&#29992;&#20316;&#35780;&#20272;&#30561;&#30496;&#38556;&#30861;&#30340;&#22522;&#20934;&#12290;&#20854;&#19981;&#33298;&#36866;&#12289;&#19981;&#36866;&#21512;&#23478;&#24237;&#20351;&#29992;&#20197;&#21450;&#22312;&#30561;&#30496;&#36136;&#37327;&#35780;&#20272;&#20013;&#24341;&#20837;&#20559;&#24046;&#30340;&#38382;&#39064;&#38656;&#35201;&#25506;&#32034;&#26356;&#23569;&#20405;&#20837;&#24615;&#12289;&#25104;&#26412;&#25928;&#30410;&#39640;&#21644;&#20415;&#25658;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#26041;&#27861;&#26159;&#32819;&#20869;&#33041;&#30005;&#20256;&#24863;&#22120;&#65292;&#23427;&#22312;&#33298;&#36866;&#24615;&#12289;&#22266;&#23450;&#30005;&#26497;&#20301;&#32622;&#12289;&#25239;&#30005;&#30913;&#24178;&#25200;&#24615;&#21644;&#26131;&#20110;&#20351;&#29992;&#24615;&#26041;&#38754;&#22343;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24314;&#31435;&#19968;&#31181;&#35780;&#20272;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#19982;&#26631;&#20934;PSG&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#35780;&#20272;PSG&#21644;&#32819;&#20869;&#33041;&#30005;&#25512;&#23548;&#30340;&#30561;&#30496;&#22270;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#20174;PSG&#21644;&#32819;&#20869;&#33041;&#30005;&#20449;&#21495;&#30340;30&#31186;&#26102;&#22495;&#21644;&#39057;&#22495;&#25552;&#21462;&#29305;&#24449;&#12290;&#25105;&#20204;&#21482;&#32771;&#34385;&#22312;PSG&#35780;&#20998;&#21592;&#21644;&#32819;&#20869;&#33041;&#30005;&#35780;&#20998;&#21592;&#36798;&#25104;&#19968;&#33268;&#26102;&#30340;&#26102;&#27573;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#26469;&#37327;&#21270;PSG&#25512;&#23548;&#21644;&#21333;&#36890;&#36947;&#32819;&#20869;&#33041;&#30005;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Study Objectives: Polysomnography (PSG) currently serves as the benchmark for evaluating sleep disorders. Its discomfort, impracticality for home-use, and introduction of bias in sleep quality assessment necessitate the exploration of less invasive, cost-effective, and portable alternatives. One promising contender is the in-ear-EEG sensor, which offers advantages in terms of comfort, fixed electrode positions, resistance to electromagnetic interference, and user-friendliness. This study aims to establish a methodology to assess the similarity between the in-ear-EEG signal and standard PSG.  Methods: We assess the agreement between the PSG and in-ear-EEG derived hypnograms. We extract features in the time- and frequency- domain from PSG and in-ear-EEG 30-second epochs. We only consider the epochs where the PSG-scorers and the in-ear-EEG-scorers were in agreement. We introduce a methodology to quantify the similarity between PSG derivations and the single-channel in-ear-EEG. The approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09071</link><description>&lt;p&gt;
&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#37325;&#26032;&#24605;&#32771;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35770;&#19978;&#22312;&#35889;&#22495;&#20013;&#26377;&#24456;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#19982;&#31354;&#38388;&#22495;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#30001;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#20174;&#31354;&#38388;&#35282;&#24230;&#30740;&#31350;&#35889;&#22270;GNN&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#31354;&#38388;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65292;&#20363;&#22914;&#65292;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#22495;&#20013;&#23454;&#38469;&#19978;&#32534;&#30721;&#20102;&#21738;&#20123;&#20449;&#24687;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#30340;&#20869;&#22312;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#26126;&#30830;&#22320;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#36866;&#24212;&#24615;&#26032;&#22270;&#12290;&#29702;&#35770;&#21644;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26032;&#22270;&#19981;&#20165;&#34920;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#36824;&#33021;&#22815;&#23481;&#32435;&#26377;&#31526;&#21495;&#30340;&#36793;&#26435;&#37325;&#20197;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#21457;&#29616;&#23545;&#20110;&#31616;&#21333;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30340;&#20934;&#30830;&#24615;&#30456;&#24403;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#22270;&#20687;&#26102;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#12290;</title><link>http://arxiv.org/abs/2401.08876</link><description>&lt;p&gt;
&#35780;&#20272;&#29992;&#20110;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling. (arXiv:2401.08876v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#65292;&#21457;&#29616;&#23545;&#20110;&#31616;&#21333;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30340;&#20934;&#30830;&#24615;&#30456;&#24403;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#22270;&#20687;&#26102;&#29305;&#21035;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#65292;&#23427;&#20204;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20351;&#24471;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;&#34920;&#31034;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#19968;&#39033;&#22823;&#22411;&#39044;&#27880;&#20876;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#21644;&#26174;&#31034;Top-1&#21644;Top-k&#39044;&#27979;&#22312;AI&#36741;&#21161;&#22270;&#20687;&#26631;&#27880;&#20013;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#31616;&#21333;&#30340;&#22270;&#20687;&#65292;&#39044;&#27979;&#38598;&#30340;&#20934;&#30830;&#24615;&#19982;Top-1&#21644;Top-k&#26174;&#31034;&#30456;&#24403;&#25110;&#31245;&#20302;&#65292;&#20294;&#22312;&#26631;&#35760;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#22270;&#20687;&#26102;&#65292;&#23588;&#20854;&#26159;&#24403;&#38598;&#21512;&#22823;&#23567;&#36739;&#23567;&#26102;&#65292;&#39044;&#27979;&#38598;&#22312;&#36741;&#21161;&#20154;&#31867;&#26631;&#27880;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#23454;&#36341;&#20013;&#24378;&#35843;&#20102;&#31526;&#21512;&#39044;&#27979;&#38598;&#30340;&#23454;&#38469;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks are more commonly deployed in high-stakes domains, their lack of interpretability makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets$\unicode{x2013}$a method for generating valid confidence sets in distribution-free uncertainty quantification$\unicode{x2013}$to express uncertainty in AI-advised decision-making. Through a large pre-registered experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-k predictions for AI-advised image labeling. We find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-k displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images especially when the set size is small. Our results empirically pinpoint the practical challenges of conformal prediction sets and provide implications 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#24494;&#35843;&#20004;&#31181;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#31649;&#36947;&#21644;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.08406</link><description>&lt;p&gt;
RAG vs Fine-tuning: &#31649;&#36947;&#65292;&#26435;&#34913;&#20197;&#21450;&#22312;&#20892;&#19994;&#19978;&#30340;&#20010;&#26696;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. (arXiv:2401.08406v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#24494;&#35843;&#20004;&#31181;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#31649;&#36947;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#31243;&#24207;&#26102;&#65292;&#24320;&#21457;&#32773;&#36890;&#24120;&#26377;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#26469;&#25972;&#21512;&#19987;&#26377;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#65306;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#24494;&#35843;&#12290;RAG&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#22686;&#24378;&#25552;&#31034;&#20449;&#24687;&#65292;&#32780;&#24494;&#35843;&#21017;&#23558;&#38468;&#21152;&#30693;&#35782;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#24182;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24494;&#35843;&#21644;RAG&#30340;&#31649;&#36947;&#65292;&#24182;&#23545;&#22810;&#31181;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;Llama2-13B&#65292;GPT-3.5&#21644;GPT-4&#65289;&#36827;&#34892;&#20102;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#30001;&#22810;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#21253;&#25324;&#20174;PDF&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#29983;&#25104;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#65292;&#24182;&#21033;&#29992;GPT-4&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;RAG&#21644;&#24494;&#35843;&#31649;&#36947;&#19981;&#21516;&#38454;&#27573;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#23545;&#20892;&#19994;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#20316;&#20026;&#19968;&#20010;&#20135;&#19994;&#65292;&#20892;&#19994;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#26041;&#38754;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22823;&#30340;&#28183;&#36879;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, an
&lt;/p&gt;</description></item><item><title>TwinBooster&#32467;&#21512;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04478</link><description>&lt;p&gt;
TwinBooster: &#32467;&#21512;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#22686;&#24378;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction. (arXiv:2401.04478v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04478
&lt;/p&gt;
&lt;p&gt;
TwinBooster&#32467;&#21512;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#21644;&#24320;&#21457;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#23545;&#20998;&#23376;&#27963;&#24615;&#21644;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#35745;&#31639;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#20351;&#29992;&#36804;&#20170;&#20026;&#27490;&#20165;&#38480;&#20110;&#22823;&#37327;&#25968;&#25454;&#21487;&#29992;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;Siamese&#31070;&#32463;&#32593;&#32476;Barlow Twins&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#26816;&#27979;&#26041;&#27861;&#20449;&#24687;&#21644;&#20998;&#23376;&#25351;&#32441;&#25552;&#21462;&#30495;&#23454;&#30340;&#20998;&#23376;&#20449;&#24687;&#12290;TwinBooster&#36890;&#36807;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#30340;&#23646;&#24615;&#39044;&#27979;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#27969;&#27700;&#32447;&#22312;FS-Mol&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#31361;&#30772;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36890;&#24120;&#25968;&#25454;&#31232;&#32570;&#30340;&#20851;&#38190;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of drug discovery and development relies on the precise prediction of molecular activities and properties. While in silico molecular property prediction has shown remarkable potential, its use has been limited so far to assays for which large amounts of data are available. In this study, we use a fine-tuned large language model to integrate biological assays based on their textual information, coupled with Barlow Twins, a Siamese neural network using a novel self-supervised learning approach. This architecture uses both assay information and molecular fingerprints to extract the true molecular information. TwinBooster enables the prediction of properties of unseen bioassays and molecules by providing state-of-the-art zero-shot learning tasks. Remarkably, our artificial intelligence pipeline shows excellent performance on the FS-Mol benchmark. This breakthrough demonstrates the application of deep learning to critical property prediction tasks where data is typically scarce.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#26657;&#27491;&#26469;&#29983;&#25104;&#38750;&#24179;&#31283;&#32441;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#20462;&#25913;&#30340;&#21442;&#32771;&#32441;&#29702;&#32454;&#21270;&#20026;&#19968;&#31181;&#36830;&#36143;&#12289;&#26080;&#32541;&#30340;&#32441;&#29702;&#65292;&#24182;&#20445;&#30041;&#21442;&#32771;&#26679;&#26412;&#30340;&#29420;&#29305;&#35270;&#35273;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#23454;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#32441;&#29702;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#22312;&#32441;&#29702;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.02847</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#26657;&#27491;&#29983;&#25104;&#38750;&#24179;&#31283;&#32441;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating Non-Stationary Textures using Self-Rectification. (arXiv:2401.02847v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#26657;&#27491;&#26469;&#29983;&#25104;&#38750;&#24179;&#31283;&#32441;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#32593;&#32476;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#20462;&#25913;&#30340;&#21442;&#32771;&#32441;&#29702;&#32454;&#21270;&#20026;&#19968;&#31181;&#36830;&#36143;&#12289;&#26080;&#32541;&#30340;&#32441;&#29702;&#65292;&#24182;&#20445;&#30041;&#21442;&#32771;&#26679;&#26412;&#30340;&#29420;&#29305;&#35270;&#35273;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#23454;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#32441;&#29702;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#22312;&#32441;&#29702;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22522;&#20110;&#31034;&#20363;&#30340;&#38750;&#24179;&#31283;&#32441;&#29702;&#21512;&#25104;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#27493;&#26041;&#27861;&#65292;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;&#20462;&#25913;&#21442;&#32771;&#32441;&#29702;&#65292;&#24471;&#21040;&#21512;&#25104;&#30340;&#21021;&#22987;&#30446;&#26631;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#8220;&#33258;&#26657;&#27491;&#8221;&#33258;&#21160;&#23558;&#36825;&#20010;&#30446;&#26631;&#32454;&#21270;&#20026;&#19968;&#31181;&#36830;&#36143;&#12289;&#26080;&#32541;&#30340;&#32441;&#29702;&#65292;&#21516;&#26102;&#24544;&#23454;&#22320;&#20445;&#30041;&#20102;&#21442;&#32771;&#26679;&#26412;&#30340;&#29420;&#29305;&#35270;&#35273;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#36880;&#28176;&#23558;&#21512;&#25104;&#32441;&#29702;&#19982;&#21442;&#32771;&#23545;&#40784;&#65292;&#30830;&#20445;&#20445;&#30041;&#25152;&#25552;&#20379;&#30446;&#26631;&#20013;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#38750;&#24179;&#31283;&#32441;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#22312;&#32441;&#29702;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/xiaorongjun000/Self-Rectific&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of example-based non-stationary texture synthesis. We introduce a novel twostep approach wherein users first modify a reference texture using standard image editing tools, yielding an initial rough target for the synthesis. Subsequently, our proposed method, termed "self-rectification", automatically refines this target into a coherent, seamless texture, while faithfully preserving the distinct visual characteristics of the reference exemplar. Our method leverages a pre-trained diffusion network, and uses self-attention mechanisms, to gradually align the synthesized texture with the reference, ensuring the retention of the structures in the provided target. Through experimental validation, our approach exhibits exceptional proficiency in handling non-stationary textures, demonstrating significant advancements in texture synthesis when compared to existing state-of-the-art techniques. Code is available at https://github.com/xiaorongjun000/Self-Rectific
&lt;/p&gt;</description></item><item><title>Powerformer&#26159;&#19968;&#31181;&#36866;&#24212;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31283;&#20581;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#34920;&#31034;&#12290;&#23427;&#36890;&#36807;&#24320;&#21457;&#19987;&#29992;&#30340;&#21306;&#27573;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#21644;&#22810;&#22240;&#32032;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#20379;&#26356;&#21152;&#31283;&#20581;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#30005;&#21147;&#31995;&#32479;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02771</link><description>&lt;p&gt;
Powerformer&#65306;&#36866;&#24212;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#29992;&#20110;&#30005;&#21147;&#27969;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Powerformer: A Section-adaptive Transformer for Power Flow Adjustment. (arXiv:2401.02771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02771
&lt;/p&gt;
&lt;p&gt;
Powerformer&#26159;&#19968;&#31181;&#36866;&#24212;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31283;&#20581;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#34920;&#31034;&#12290;&#23427;&#36890;&#36807;&#24320;&#21457;&#19987;&#29992;&#30340;&#21306;&#27573;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#21644;&#22810;&#22240;&#32032;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#20379;&#26356;&#21152;&#31283;&#20581;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#30005;&#21147;&#31995;&#32479;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#23398;&#20064;&#31283;&#20581;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#34920;&#31034;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#20248;&#21270;&#36328;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#30005;&#21147;&#35843;&#24230;&#20197;&#36827;&#34892;&#30005;&#21147;&#27969;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;Powerformer&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#19987;&#29992;&#30340;&#21306;&#27573;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#65292;&#19982;&#20256;&#32479;&#21464;&#21387;&#22120;&#20013;&#20351;&#29992;&#30340;&#33258;&#27880;&#24847;&#20998;&#31163;&#24320;&#26469;&#12290;&#35813;&#26426;&#21046;&#26377;&#25928;&#22320;&#23558;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#19982;&#20256;&#36755;&#21306;&#27573;&#20449;&#24687;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#31283;&#20581;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32771;&#34385;&#30005;&#21147;&#31995;&#32479;&#30340;&#22270;&#25299;&#25169;&#21644;&#27597;&#32447;&#33410;&#28857;&#30340;&#30005;&#27668;&#23646;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#23450;&#21046;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#21644;&#22810;&#22240;&#32032;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30005;&#21147;&#31995;&#32479;&#22330;&#26223;&#65288;&#21253;&#25324;IEEE 118&#33410;&#28857;&#31995;&#32479;&#12289;&#20013;&#22269;&#23454;&#38469;300&#33410;&#28857;&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#22411;&#31995;&#32479;&#65289;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel transformer architecture tailored for learning robust power system state representations, which strives to optimize power dispatch for the power flow adjustment across different transmission sections. Specifically, our proposed approach, named Powerformer, develops a dedicated section-adaptive attention mechanism, separating itself from the self-attention used in conventional transformers. This mechanism effectively integrates power system states with transmission section information, which facilitates the development of robust state representations. Furthermore, by considering the graph topology of power system and the electrical attributes of bus nodes, we introduce two customized strategies to further enhance the expressiveness: graph neural network propagation and multi-factor attention mechanism. Extensive evaluations are conducted on three power system scenarios, including the IEEE 118-bus system, a realistic 300-bus system in China, and a large-
&lt;/p&gt;</description></item><item><title>HAAQI-Net&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#29992;&#25143;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;BLSTM&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;BEATs&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#33021;&#22815;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#39044;&#27979;&#38899;&#20048;&#30340;HAAQI&#24471;&#20998;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.01145</link><description>&lt;p&gt;
HAAQI-Net: &#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HAAQI-Net: A non-intrusive neural music quality assessment model for hearing aids. (arXiv:2401.01145v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01145
&lt;/p&gt;
&lt;p&gt;
HAAQI-Net&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#29992;&#25143;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;BLSTM&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;BEATs&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#33021;&#22815;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#39044;&#27979;&#38899;&#20048;&#30340;HAAQI&#24471;&#20998;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HAAQI-Net&#65292;&#19968;&#31181;&#38024;&#23545;&#21161;&#21548;&#22120;&#29992;&#25143;&#23450;&#21046;&#30340;&#38750;&#20405;&#20837;&#24615;&#28145;&#24230;&#23398;&#20064;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#22914;Hearing Aid Audio Quality Index (HAAQI) &#19981;&#21516;&#65292;HAAQI-Net&#37319;&#29992;&#20102;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(BLSTM)&#12290;&#35813;&#27169;&#22411;&#20197;&#35780;&#20272;&#30340;&#38899;&#20048;&#26679;&#26412;&#21644;&#21548;&#21147;&#25439;&#22833;&#27169;&#24335;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#39044;&#27979;&#30340;HAAQI&#24471;&#20998;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#26469;&#33258;&#38899;&#39057;&#21464;&#25442;&#22120;(BEATs)&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#12290;&#36890;&#36807;&#23558;&#39044;&#27979;&#20998;&#25968;&#19982;&#30495;&#23454;&#20998;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;HAAQI-Net&#36798;&#21040;&#20102;0.9257&#30340;&#38271;&#26399;&#19968;&#33268;&#24615;&#30456;&#20851;(LCC)&#65292;0.9394&#30340;&#26031;&#30382;&#23572;&#26364;&#31561;&#32423;&#30456;&#20851;&#31995;&#25968;(SRCC)&#65292;&#21644;0.0080&#30340;&#22343;&#26041;&#35823;&#24046;(MSE)&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#39640;&#24615;&#33021;&#20276;&#38543;&#30528;&#25512;&#29702;&#26102;&#38388;&#30340;&#22823;&#24133;&#20943;&#23569;&#65306;&#20174;62.52&#31186;(HAAQI)&#20943;&#23569;&#21040;2.71&#31186;(HAAQI-Net)&#65292;&#20026;&#21161;&#21548;&#22120;&#29992;&#25143;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces HAAQI-Net, a non-intrusive deep learning model for music quality assessment tailored to hearing aid users. In contrast to traditional methods like the Hearing Aid Audio Quality Index (HAAQI), HAAQI-Net utilizes a Bidirectional Long Short-Term Memory (BLSTM) with attention. It takes an assessed music sample and a hearing loss pattern as input, generating a predicted HAAQI score. The model employs the pre-trained Bidirectional Encoder representation from Audio Transformers (BEATs) for acoustic feature extraction. Comparing predicted scores with ground truth, HAAQI-Net achieves a Longitudinal Concordance Correlation (LCC) of 0.9257, Spearman's Rank Correlation Coefficient (SRCC) of 0.9394, and Mean Squared Error (MSE) of 0.0080. Notably, this high performance comes with a substantial reduction in inference time: from 62.52 seconds (by HAAQI) to 2.71 seconds (by HAAQI-Net), serving as an efficient music quality assessment model for hearing aid users.
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#35299;&#20915;&#29615;&#22659;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.15910</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Unlearning. (arXiv:2312.15910v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15910
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#35299;&#20915;&#29615;&#22659;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#23398;&#20064;&#25351;&#30340;&#26159;&#26681;&#25454;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#35831;&#27714;&#65292;&#38477;&#20302;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#28040;&#38500;&#23398;&#20064;&#30340;&#30740;&#31350;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#39046;&#22495;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#37027;&#23601;&#26159;&#24378;&#21270;&#23398;&#20064;&#12290;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#29615;&#22659;&#20013;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26234;&#33021;&#20307;&#24448;&#24448;&#20250;&#35760;&#24518;&#29615;&#22659;&#30340;&#29305;&#24449;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#22823;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#26681;&#25454;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#65292;&#29615;&#22659;&#30340;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#23637;&#19968;&#20010;&#26032;&#39062;&#19988;&#32039;&#36843;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21363;&#8220;&#24378;&#21270;&#28040;&#38500;&#23398;&#20064;&#8221;&#12290;&#24378;&#21270;&#28040;&#38500;&#23398;&#20064;&#20391;&#37325;&#20110;&#25764;&#38144;&#25972;&#20010;&#29615;&#22659;&#32780;&#19981;&#26159;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#36825;&#19968;&#29420;&#29305;&#29305;&#24449;&#24102;&#26469;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;1&#65289;&#22914;&#20309;&#25552;&#20986;&#28040;&#38500;&#23398;&#20064;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning refers to the process of mitigating the influence of specific training data on machine learning models based on removal requests from data owners. However, one important area that has been largely overlooked in the research of unlearning is reinforcement learning. Reinforcement learning focuses on training an agent to make optimal decisions within an environment to maximize its cumulative rewards. During the training, the agent tends to memorize the features of the environment, which raises a significant concern about privacy. As per data protection regulations, the owner of the environment holds the right to revoke access to the agent's training data, thus necessitating the development of a novel and pressing research field, known as \emph{reinforcement unlearning}. Reinforcement unlearning focuses on revoking entire environments rather than individual data samples. This unique characteristic presents three distinct challenges: 1) how to propose unlearning schemes f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23450;&#20215;&#29615;&#22659;&#19979;&#36827;&#34892;&#38656;&#27714;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22240;&#26524;&#25512;&#26029;&#30340;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23436;&#20840;&#25511;&#21046;&#30340;&#24773;&#20917;&#19979;&#26356;&#22909;&#22320;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#31163;&#32447;&#25919;&#31574;&#35774;&#32622;&#20013;&#20248;&#20110;&#20854;&#20182;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.15282</link><description>&lt;p&gt;
&#23450;&#20215;&#30340;&#22240;&#26524;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal Forecasting for Pricing. (arXiv:2312.15282v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23450;&#20215;&#29615;&#22659;&#19979;&#36827;&#34892;&#38656;&#27714;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22240;&#26524;&#25512;&#26029;&#30340;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23436;&#20840;&#25511;&#21046;&#30340;&#24773;&#20917;&#19979;&#26356;&#22909;&#22320;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#31163;&#32447;&#25919;&#31574;&#35774;&#32622;&#20013;&#20248;&#20110;&#20854;&#20182;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23450;&#20215;&#29615;&#22659;&#19979;&#36827;&#34892;&#38656;&#27714;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24314;&#27169;&#20215;&#26684;&#20316;&#20026;&#38656;&#27714;&#30340;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#38646;&#21806;&#21830;&#30340;&#30446;&#26631;&#26159;&#20197;&#65288;&#21033;&#28070;&#65289;&#26368;&#20339;&#26041;&#24335;&#35774;&#23450;&#20215;&#26684;&#65292;&#20197;&#35299;&#20915;&#19979;&#28216;&#20915;&#31574;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22240;&#26524;&#25512;&#26029;&#30340;&#21452;&#37325;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#39044;&#27979;&#27169;&#22411;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#19968;&#26041;&#38754;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23436;&#20840;&#25511;&#21046;&#30340;&#24773;&#20917;&#19979;&#23545;&#21512;&#25104;&#30340;&#12289;&#20294;&#29616;&#23454;&#30340;&#25968;&#25454;&#26356;&#22909;&#22320;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31163;&#32447;&#25919;&#31574;&#35774;&#32622;&#65288;&#21363;&#23450;&#20215;&#25919;&#31574;&#21457;&#29983;&#21464;&#21270;&#26102;&#65289;&#20013;&#20248;&#20110;&#20854;&#20182;&#39044;&#27979;&#26041;&#27861;&#65292;&#32780;&#22312;&#22312;&#32447;&#25919;&#31574;&#35774;&#32622;&#20013;&#30053;&#26377;&#33853;&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel method for demand forecasting in a pricing context. Here, modeling the causal relationship between price as an input variable to demand is crucial because retailers aim to set prices in a (profit) optimal manner in a downstream decision making problem. Our methods bring together the Double Machine Learning methodology for causal inference and state-of-the-art transformer-based forecasting models. In extensive empirical experiments, we show on the one hand that our method estimates the causal effect better in a fully controlled setting via synthetic, yet realistic data. On the other hand, we demonstrate on real-world data that our method outperforms forecasting methods in off-policy settings (i.e., when there's a change in the pricing policy) while only slightly trailing in the on-policy setting.
&lt;/p&gt;</description></item><item><title>Auto311&#26159;&#31532;&#19968;&#20010;&#22788;&#29702;&#38750;&#32039;&#24613;&#30005;&#35805;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#20943;&#36731;&#38750;&#32039;&#24613;&#30005;&#35805;&#36127;&#25285;&#65292;&#25552;&#20379;&#24555;&#36895;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#39044;&#27979;&#20107;&#20214;&#31867;&#22411;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#26696;&#20214;&#25253;&#21578;&#65292;&#24182;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#26469;&#23436;&#21892;&#25253;&#21578;&#65292;&#31995;&#32479;&#19982;&#20027;&#21483;&#20154;&#20043;&#38388;&#30340;&#23545;&#35805;&#32467;&#26500;&#24471;&#21040;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2312.14185</link><description>&lt;p&gt;
Auto311: &#19968;&#31181;&#22522;&#20110;&#20449;&#24515;&#25351;&#23548;&#30340;&#33258;&#21160;&#38750;&#32039;&#24613;&#36890;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Auto311: A Confidence-guided Automated System for Non-emergency Calls. (arXiv:2312.14185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14185
&lt;/p&gt;
&lt;p&gt;
Auto311&#26159;&#31532;&#19968;&#20010;&#22788;&#29702;&#38750;&#32039;&#24613;&#30005;&#35805;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#20943;&#36731;&#38750;&#32039;&#24613;&#30005;&#35805;&#36127;&#25285;&#65292;&#25552;&#20379;&#24555;&#36895;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#39044;&#27979;&#20107;&#20214;&#31867;&#22411;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#26696;&#20214;&#25253;&#21578;&#65292;&#24182;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#26469;&#23436;&#21892;&#25253;&#21578;&#65292;&#31995;&#32479;&#19982;&#20027;&#21483;&#20154;&#20043;&#38388;&#30340;&#23545;&#35805;&#32467;&#26500;&#24471;&#21040;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#24613;&#21644;&#38750;&#32039;&#24613;&#21709;&#24212;&#31995;&#32479;&#26159;&#22320;&#26041;&#25919;&#24220;&#25552;&#20379;&#30340;&#22522;&#26412;&#26381;&#21153;&#65292;&#23545;&#20110;&#20445;&#25252;&#29983;&#21629;&#12289;&#29615;&#22659;&#21644;&#36130;&#20135;&#33267;&#20851;&#37325;&#35201;&#12290;&#26377;&#25928;&#22788;&#29702;&#65288;&#38750;&#65289;&#32039;&#24613;&#30005;&#35805;&#23545;&#20844;&#20849;&#23433;&#20840;&#21644;&#31119;&#31049;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20943;&#36731;&#38750;&#32039;&#24613;&#30005;&#35805;&#30340;&#36127;&#25285;&#65292;&#20127;&#38656;911&#27714;&#21161;&#30340;&#23621;&#27665;&#23558;&#33719;&#24471;&#24555;&#36895;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#19982;&#32435;&#20160;&#32500;&#23572;&#32039;&#24613;&#36890;&#20449;&#37096;&#38376;&#21512;&#20316;&#65292;&#20998;&#26512;&#20102;11,796&#20010;&#38750;&#32039;&#24613;&#21628;&#21483;&#24405;&#38899;&#65292;&#24182;&#24320;&#21457;&#20102;Auto311&#65292;&#31532;&#19968;&#20010;&#22788;&#29702;311&#38750;&#32039;&#24613;&#21628;&#21483;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#65288;1&#65289;&#26377;&#25928;&#21160;&#24577;&#22320;&#39044;&#27979;&#27491;&#22312;&#36827;&#34892;&#30340;&#38750;&#32039;&#24613;&#20107;&#20214;&#31867;&#22411;&#65292;&#20197;&#22312;&#36890;&#35805;&#36807;&#31243;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#26696;&#20214;&#25253;&#21578;&#65307;&#65288;2&#65289;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#23436;&#25104;&#29983;&#25104;&#30340;&#25253;&#21578;&#65307;&#65288;3&#65289;&#20197;&#20248;&#21270;&#30340;&#20449;&#24515;&#27700;&#24179;&#23433;&#25490;&#31995;&#32479;&#21644;&#20027;&#21483;&#20154;&#20043;&#38388;&#30340;&#23545;&#35805;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#35780;&#20272;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emergency and non-emergency response systems are essential services provided by local governments and critical to protecting lives, the environment, and property. The effective handling of (non-)emergency calls is critical for public safety and well-being. By reducing the burden through non-emergency callers, residents in critical need of assistance through 911 will receive a fast and effective response. Collaborating with the Department of Emergency Communications (DEC) in Nashville, we analyzed 11,796 non-emergency call recordings and developed Auto311, the first automated system to handle 311 non-emergency calls, which (1) effectively and dynamically predicts ongoing non-emergency incident types to generate tailored case reports during the call; (2) itemizes essential information from dialogue contexts to complete the generated reports; and (3) strategically structures system-caller dialogues with optimized confidence. We used real-world data to evaluate the system's effectiveness a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#24191;&#20041;&#20307;&#31215;&#21462;&#26679;&#36827;&#34892;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24191;&#20041;&#29256;&#26412;&#30340;&#20307;&#31215;&#26631;&#20934;&#21270;&#21462;&#26679;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#26399;&#26395;&#19978;&#30340;&#20934;&#26368;&#20248;&#24615;&#20197;&#21450;&#22312;&#26576;&#20123;&#35268;&#33539;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.14057</link><description>&lt;p&gt;
&#22522;&#20110;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#24191;&#20041;&#20307;&#31215;&#21462;&#26679;&#30340;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Weighted least-squares approximation with determinantal point processes and generalized volume sampling. (arXiv:2312.14057v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#24191;&#20041;&#20307;&#31215;&#21462;&#26679;&#36827;&#34892;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24191;&#20041;&#29256;&#26412;&#30340;&#20307;&#31215;&#26631;&#20934;&#21270;&#21462;&#26679;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#26399;&#26395;&#19978;&#30340;&#20934;&#26368;&#20248;&#24615;&#20197;&#21450;&#22312;&#26576;&#20123;&#35268;&#33539;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#32473;&#23450;&#30340;m&#32500;&#31354;&#38388;V_m&#20013;&#30340;&#20803;&#32032;&#65292;&#20511;&#21161;&#20110;&#19968;&#20123;&#29305;&#24449;&#26144;&#23556;&#966;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#28857;x_1&#65292;...&#65292;x_n&#22788;&#30340;&#20989;&#25968;&#36827;&#34892;&#35780;&#20272;&#65292;&#26469;&#36924;&#36817;&#20989;&#25968;&#20174;L^2&#21040;&#20989;&#25968;&#12290;&#22312;&#22238;&#39038;&#19968;&#20123;&#20851;&#20110;&#20351;&#29992;&#29420;&#31435;&#21516;&#20998;&#24067;&#28857;&#30340;&#26368;&#20248;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#30340;&#32467;&#26524;&#20043;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25237;&#24433;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#25110;&#20307;&#31215;&#21462;&#26679;&#30340;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#12290;&#36825;&#20123;&#20998;&#24067;&#22312;&#36873;&#23450;&#30340;&#29305;&#24449;&#966;(x_i)&#20013;&#24341;&#20837;&#20102;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#65292;&#20197;&#20419;&#36827;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#24191;&#20041;&#29256;&#26412;&#30340;&#20307;&#31215;&#26631;&#20934;&#21270;&#21462;&#26679;&#65292;&#20351;&#29992;&#26679;&#26412;&#25968;n = O(mlog(m))&#24471;&#21040;&#20102;&#26399;&#26395;&#19978;&#30340;&#20934;&#26368;&#20248;&#32467;&#26524;&#65292;&#36825;&#24847;&#21619;&#30528;&#26399;&#26395;&#30340;L^2&#35823;&#24046;&#21463;&#21040;&#19968;&#20010;&#24120;&#25968;&#20056;&#20197;&#22312;L^2&#20013;&#30340;&#26368;&#20339;&#36924;&#36817;&#35823;&#24046;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#36827;&#19968;&#27493;&#20551;&#35774;&#20989;&#25968;&#22312;&#26576;&#20010;&#23884;&#20837;&#22312;L^2&#20013;&#30340;&#35268;&#33539;&#21521;&#37327;&#31354;&#38388;H&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#36924;&#36817;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of approximating a function from $L^2$ by an element of a given $m$-dimensional space $V_m$, associated with some feature map $\varphi$, using evaluations of the function at random points $x_1,\dots,x_n$. After recalling some results on optimal weighted least-squares using independent and identically distributed points, we consider weighted least-squares using projection determinantal point processes (DPP) or volume sampling. These distributions introduce dependence between the points that promotes diversity in the selected features $\varphi(x_i)$. We first provide a generalized version of volume-rescaled sampling yielding quasi-optimality results in expectation with a number of samples $n = O(m\log(m))$, that means that the expected $L^2$ error is bounded by a constant times the best approximation error in $L^2$. Also, further assuming that the function is in some normed vector space $H$ continuously embedded in $L^2$, we further prove that the approximation is
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#23450;&#20301;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#25214;&#21040;&#20102;&#23384;&#20648;&#20102;&#26377;&#20851;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#30340;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2312.12141</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23450;&#20301;&#20107;&#23454;&#30693;&#35782;&#65306;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Locating Factual Knowledge in Large Language Models: Exploring the Residual Stream and Analyzing Subvalues in Vocabulary Space. (arXiv:2312.12141v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12141
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#23450;&#20301;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#25214;&#21040;&#20102;&#23384;&#20648;&#20102;&#26377;&#20851;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#30340;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#21097;&#20313;&#27969;&#21644;&#20998;&#26512;&#35789;&#27719;&#31354;&#38388;&#20013;&#30340;&#23376;&#20540;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#26102;&#65292;&#23376;&#20540;&#20855;&#26377;&#21487;&#20154;&#31867;&#35299;&#37322;&#30340;&#27010;&#24565;&#30340;&#21407;&#22240;&#12290;&#23376;&#20540;&#30340;softmax&#20043;&#21069;&#30340;&#20540;&#36890;&#36807;&#19968;&#20010;&#21152;&#27861;&#20989;&#25968;&#30456;&#21152;&#65292;&#22240;&#27492;&#35789;&#27719;&#31354;&#38388;&#20013;&#21069;&#20960;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#20250;&#22686;&#21152;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#23545;&#25968;&#27010;&#29575;&#22686;&#21152;&#26469;&#35745;&#31639;&#23618;&#21644;&#23376;&#20540;&#30340;&#37325;&#35201;&#24615;&#27604;&#27010;&#29575;&#22686;&#21152;&#26356;&#22909;&#65292;&#22240;&#20026;&#23545;&#25968;&#27010;&#29575;&#22686;&#21152;&#30340;&#26354;&#32447;&#21576;&#32447;&#24615;&#21333;&#35843;&#22686;&#24418;&#29366;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35745;&#31639;&#20869;&#31215;&#26469;&#35780;&#20272;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#30340;&#23376;&#20540;&#34987;&#21069;&#38754;&#30340;&#23618;&#28608;&#27963;&#30340;&#31243;&#24230;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#20107;&#23454;&#30693;&#35782;&#8220;&#27861;&#22269;&#65292;&#39318;&#37117;&#65292;&#24052;&#40654;&#8221;&#23384;&#20648;&#30340;&#20301;&#32622;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#27880;&#24847;&#21147;&#23618;&#23384;&#20648;&#8220;&#24052;&#40654;&#19982;&#27861;&#22269;&#30456;&#20851;&#8221;&#12290;FFN&#23618;&#23384;&#20648;&#8220;&#24052;&#40654;&#26159;&#19968;&#20010;&#39318;&#37117;/&#22478;&#24066;&#8221;&#65292;&#30001;&#27880;&#24847;&#21147;&#23376;&#20540;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
We find the location of factual knowledge in large language models by exploring the residual stream and analyzing subvalues in vocabulary space. We find the reason why subvalues have human-interpretable concepts when projecting into vocabulary space. The before-softmax values of subvalues are added by an addition function, thus the probability of top tokens in vocabulary space will increase. Based on this, we find using log probability increase to compute the significance of layers and subvalues is better than probability increase, since the curve of log probability increase has a linear monotonically increasing shape. Moreover, we calculate the inner products to evaluate how much a feed-forward network (FFN) subvalue is activated by previous layers. Base on our methods, we find where factual knowledge &lt;France, capital, Paris&gt; is stored. Specifically, attention layers store "Paris is related to France". FFN layers store "Paris is a capital/city", activated by attention subvalues relate
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#35328;&#35821;&#19981;&#27969;&#30021;&#31243;&#24230;&#33258;&#21160;&#35843;&#25972;&#33647;&#29289;&#65292;&#36890;&#36807;&#23545;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.11509</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33647;&#29289;&#35843;&#25972;&#31995;&#32479;&#20197;&#20943;&#23569;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11509
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#35328;&#35821;&#19981;&#27969;&#30021;&#31243;&#24230;&#33258;&#21160;&#35843;&#25972;&#33647;&#29289;&#65292;&#36890;&#36807;&#23545;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#20026;&#24739;&#26377;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#34394;&#25311;&#24739;&#32773;&#24320;&#20855;&#33647;&#29289;&#22788;&#26041;&#65292;&#24182;&#26681;&#25454;&#38646;&#25104;&#26412;&#39057;&#32321;&#27979;&#37327;&#32467;&#26524;&#65292;&#35843;&#25972;&#33647;&#29289;&#21644;&#21058;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31995;&#32479;&#30340;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#22312;&#25105;&#20204;&#26500;&#24314;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#21644;&#35780;&#20272;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#33391;&#22909;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20004;&#20010;&#27169;&#22359;&#65292;&#25105;&#20204;&#20174;&#25991;&#29486;&#20013;&#25910;&#38598;&#20102;&#20851;&#20110;&#33647;&#29289;&#27835;&#30103;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#25928;&#26524;&#30340;&#25968;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#20449;&#30340;&#24739;&#32773;&#27169;&#25311;&#31995;&#32479;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#23545;&#21487;&#33021;&#23384;&#22312;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#20154;&#32676;&#36827;&#34892;&#20102;&#25968;&#25454;&#26631;&#27880;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;:
&lt;/p&gt;
&lt;p&gt;
We propose a Reinforcement-Learning-based system that would automatically prescribe a hypothetical patient medication that may help the patient with their mental-health-related speech disfluency, and adjust the medication and the dosages in response to zero-cost frequent measurement of the fluency of the patient. We demonstrate the components of the system: a module that detects and evaluates speech disfluency on a large dataset we built, and a Reinforcement Learning algorithm that automatically finds good combinations of medications. To support the two modules, we collect data on the effect of psychiatric medications for speech disfluency from the literature, and build a plausible patient simulation system. We demonstrate that the Reinforcement Learning system is, under some circumstances, able to converge to a good medication regime. We collect and label a dataset of people with possible speech disfluency and demonstrate our methods using that dataset. Our work is a proof of concept:
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36825;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;LLMs&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;LLMs&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#36739;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2312.05934</link><description>&lt;p&gt;
Fine-Tuning&#36824;&#26159;&#26816;&#32034;&#65311;&#27604;&#36739;&#22312;LLMs&#20013;&#30340;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. (arXiv:2312.05934v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36825;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;LLMs&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;LLMs&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#36739;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20013;&#23553;&#35013;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#27491;&#22914;&#23427;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22806;&#37096;&#25968;&#25454;&#38598;&#26469;&#25972;&#21512;&#26032;&#30340;&#20449;&#24687;&#25110;&#25913;&#36827;LLMs&#22312;&#24050;&#35265;&#20449;&#24687;&#19978;&#30340;&#33021;&#21147;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65306;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20027;&#39064;&#30340;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#33021;&#22815;&#25552;&#20379;&#19968;&#23450;&#30340;&#25913;&#36827;&#65292;&#20294;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#23436;&#20840;&#26032;&#30693;&#35782;&#19978;&#22987;&#32456;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#24456;&#38590;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#24182;&#19988;&#26292;&#38706;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiGPrompt&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2312.03731</link><description>&lt;p&gt;
&#22810;&#20010;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#22270;&#24418;&#25552;&#31034;&#30340;MultiGPrompt
&lt;/p&gt;
&lt;p&gt;
MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiGPrompt&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#21487;&#20197;&#22266;&#26377;&#22320;&#23545;Web&#19978;&#30456;&#20114;&#36830;&#25509;&#30340;&#23545;&#35937;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25903;&#25345;&#19968;&#31995;&#21015;Web&#24212;&#29992;&#65292;&#27604;&#22914;&#32593;&#32476;&#20998;&#26512;&#21644;&#20869;&#23481;&#25512;&#33616;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#27969;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#31471;&#21040;&#31471;&#30417;&#30563;&#26694;&#26550;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#19982;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#30340;&#21487;&#29992;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#20026;&#20102;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#24182;&#22686;&#24378;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#22522;&#20110;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#32780;&#25552;&#31034;&#21017;&#34987;&#25552;&#20986;&#26469;&#36827;&#19968;&#27493;&#32553;&#23567;&#39044;&#35757;&#32451;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#30446;&#26631;&#24046;&#36317;&#12290;&#34429;&#28982;&#24050;&#32463;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#24418;&#23398;&#20064;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25506;&#32034;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#21033;&#29992;&#21333;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23548;&#33268;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#33021;&#23398;&#20064;&#30340;&#36890;&#29992;&#30693;&#35782;&#30340;&#23376;&#38598;&#21463;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;MultiGPrompt&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#22270;&#24418;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs can inherently model interconnected objects on the Web, thereby facilitating a series of Web applications, such as web analyzing and content recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a mainstream technique for graph representation learning. However, their efficacy within an end-to-end supervised framework is significantly tied to the availabilityof task-specific labels. To mitigate labeling costs and enhance robustness in few-shot settings, pre-training on self-supervised tasks has emerged as a promising method, while prompting has been proposed to further narrow the objective gap between pretext and downstream tasks. Although there has been some initial exploration of prompt-based learning on graphs, they primarily leverage a single pretext task, resulting in a limited subset of general knowledge that could be learned from the pre-training data. Hence, in this paper, we propose MultiGPrompt, a novel multi-task pre-training and prompting framework to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#34917;&#26426;&#21046;&#23454;&#29616;&#20102;&#20013;&#26029;&#21644;&#24490;&#29615;&#26426;&#21046;&#65292;&#20351;&#20256;&#32479;&#35299;&#30721;&#36827;&#31243;&#21464;&#24471;&#38750;&#21333;&#35843;&#12290;&#21033;&#29992;&#20013;&#26029;&#26426;&#21046;&#21487;&#20197;&#25512;&#36831;&#29983;&#25104;&#20195;&#30721;&#65292;&#22686;&#24378;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#65307;&#21033;&#29992;&#24490;&#29615;&#26426;&#21046;&#21487;&#20197;&#24490;&#29615;&#26356;&#26032;&#21644;&#21516;&#27493;&#29983;&#25104;&#30340;&#27599;&#20010;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2311.17972</link><description>&lt;p&gt;
&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-Infilling Code Generation. (arXiv:2311.17972v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#33258;&#34917;&#26426;&#21046;&#23454;&#29616;&#20102;&#20013;&#26029;&#21644;&#24490;&#29615;&#26426;&#21046;&#65292;&#20351;&#20256;&#32479;&#35299;&#30721;&#36827;&#31243;&#21464;&#24471;&#38750;&#21333;&#35843;&#12290;&#21033;&#29992;&#20013;&#26029;&#26426;&#21046;&#21487;&#20197;&#25512;&#36831;&#29983;&#25104;&#20195;&#30721;&#65292;&#22686;&#24378;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#65307;&#21033;&#29992;&#24490;&#29615;&#26426;&#21046;&#21487;&#20197;&#24490;&#29615;&#26356;&#26032;&#21644;&#21516;&#27493;&#29983;&#25104;&#30340;&#27599;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#34917;&#20195;&#30721;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23427;&#23558;&#34917;&#20805;&#25805;&#20316;&#34701;&#20837;&#33258;&#22238;&#24402;&#35299;&#30721;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#33021;&#22815;&#36827;&#34892;&#22635;&#20805;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#36827;&#34892;&#22635;&#20805;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;&#34917;&#20805;&#25805;&#20316;&#26088;&#22312;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#21069;&#32512;&#21644;&#21518;&#32512;&#22635;&#20805;&#20013;&#38388;&#20869;&#23481;&#65292;&#32780;&#33258;&#34917;&#26426;&#21046;&#39034;&#24207;&#29983;&#25104;&#36825;&#20123;&#21608;&#22260;&#19978;&#19979;&#25991;&#21644;&#34987;&#22635;&#20805;&#20869;&#23481;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#22312;&#20256;&#32479;&#35299;&#30721;&#20013;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#20013;&#26029;&#21644;&#24490;&#29615;&#26426;&#21046;&#65292;&#20351;&#20854;&#36827;&#21270;&#20026;&#38750;&#21333;&#35843;&#36807;&#31243;&#12290;&#20013;&#26029;&#26426;&#21046;&#20801;&#35768;&#25512;&#36831;&#29983;&#25104;&#29305;&#23450;&#30340;&#20195;&#30721;&#65292;&#30452;&#21040;&#30830;&#23450;&#30340;&#21518;&#32512;&#24314;&#31435;&#65292;&#22686;&#24378;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#12290;&#21516;&#26102;&#65292;&#24490;&#29615;&#26426;&#21046;&#21033;&#29992;&#33258;&#34917;&#21644;&#20174;&#24038;&#21040;&#21491;&#35299;&#30721;&#30340;&#20114;&#34917;&#24615;&#65292;&#21487;&#20197;&#24490;&#29615;&#26356;&#26032;&#21644;&#21516;&#27493;&#27599;&#20010;&#29983;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces self-infilling code generation, a general framework that incorporates infilling operations into auto-regressive decoding. Our approach capitalizes on the observation that recent infilling-capable code language models can self-infill: whereas infilling operations aim to fill in the middle based on a predefined prefix and suffix, self-infilling sequentially generates both such surrounding context and the infilled content. We utilize this capability to introduce novel interruption and looping mechanisms in conventional decoding, evolving it into a non-monotonic process. Interruptions allow for postponing the generation of specific code until a definitive suffix is established, enhancing control over the output. Meanwhile, the looping mechanism, which leverages the complementary nature of self-infilling and left-to-right decoding, can iteratively update and synchronize each piece of generation cyclically. Extensive experiments are conducted to demonstrate that our prop
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#36830;&#32493;Galerkin&#26041;&#27861;&#20013;&#21152;&#20837;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#35745;&#31639;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.18897</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#22686;&#24378;&#20302;&#38454;&#19981;&#36830;&#32493;Galerkin&#26041;&#27861;&#22312;&#21487;&#21387;Navier-Stokes&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancing Low-Order Discontinuous Galerkin Methods with Neural Ordinary Differential Equations for Compressible Navier--Stokes Equations. (arXiv:2310.18897v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#36830;&#32493;Galerkin&#26041;&#27861;&#20013;&#21152;&#20837;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#35745;&#31639;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#38271;&#65292;&#27169;&#25311;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#21644;&#20934;&#30830;&#12290;&#28982;&#32780;&#65292;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#36890;&#24120;&#20250;&#36816;&#34892;&#19968;&#20010;&#20302;&#20445;&#30495;&#24230;&#27169;&#22411;&#24182;&#37319;&#29992;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#65292;&#20294;&#36873;&#25321;&#36866;&#24403;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#24182;&#23545;&#20854;&#36827;&#34892;&#35843;&#33410;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#22312;&#19981;&#36830;&#32493;Galerkin&#65288;DG&#65289;&#31354;&#38388;&#31163;&#25955;&#21270;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#25311;&#20013;&#24341;&#20837;&#31070;&#32463;&#24120;&#24494;&#20998;&#31639;&#23376;&#26469;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36830;&#32493;&#32423;&#21035;&#19978;&#23398;&#20064;&#20302;&#38454;DG&#27714;&#35299;&#22120;&#20013;&#32570;&#22833;&#30340;&#23610;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20302;&#38454;DG&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20197;&#19968;&#23450;&#31243;&#24230;&#30340;&#31934;&#24230;&#21152;&#36895;&#28388;&#27874;&#39640;&#38454;DG&#27169;&#25311;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing computing power over the years has enabled simulations to become more complex and accurate. While immensely valuable for scientific discovery and problem-solving, however, high-fidelity simulations come with significant computational demands. As a result, it is common to run a low-fidelity model with a subgrid-scale model to reduce the computational cost, but selecting the appropriate subgrid-scale models and tuning them are challenging. We propose a novel method for learning the subgrid-scale model effects when simulating partial differential equations augmented by neural ordinary differential operators in the context of discontinuous Galerkin (DG) spatial discretization. Our approach learns the missing scales of the low-order DG solver at a continuous level and hence improves the accuracy of the low-order DG approximations as well as accelerates the filtered high-order DG simulations with a certain degree of precision. We demonstrate the performance of our approach throug
&lt;/p&gt;</description></item><item><title>Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17807</link><description>&lt;p&gt;
Clover: &#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Clover: Closed-Loop Verifiable Code Generation. (arXiv:2310.17807v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17807
&lt;/p&gt;
&lt;p&gt;
Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#26159;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#20010;&#36235;&#21183;&#21487;&#33021;&#20250;&#23548;&#33268;&#35768;&#22810;&#19981;&#33391;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#24895;&#26223;&#65306;Clover&#33539;&#24335;&#65292;&#21363;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#65292;&#23427;&#23558;&#27491;&#30830;&#24615;&#26816;&#26597;&#31616;&#21270;&#20026;&#26356;&#21487;&#35775;&#38382;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#22312;Clover&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26816;&#26597;&#22120;&#65292;&#23427;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#12290;&#35813;&#26816;&#26597;&#22120;&#20351;&#29992;&#20102;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#38598;&#25104;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#28857;&#65292;&#21363;Clover&#22312;&#19968;&#33268;&#24615;&#26816;&#26597;&#26041;&#38754;&#24212;&#35813;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#30001;&#25163;&#24037;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65288;CloverBench&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35843;&#26597;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#27880;&#37322;&#30340;Dafny&#31243;&#24207;&#65292;&#38590;&#24230;&#27700;&#24179;&#19982;&#25945;&#31185;&#20070;&#30456;&#24403;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#35843;&#24230;&#31574;&#30053;&#26469;&#35299;&#20915;&#22823;&#22411;&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#35843;&#24230;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.15706</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#22810;&#26679;&#21270;&#35843;&#24230;&#31574;&#30053;&#26469;&#35299;&#20915;&#22823;&#22411;&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
Solving large flexible job shop scheduling instances by generating a diverse set of scheduling policies with deep reinforcement learning. (arXiv:2310.15706v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#35843;&#24230;&#31574;&#30053;&#26469;&#35299;&#20915;&#22823;&#22411;&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#35843;&#24230;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;FJSSP&#65289;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#21551;&#21457;&#24335;&#12289;&#31934;&#30830;&#21644;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24037;&#19994;&#23545;&#23454;&#26102;&#21709;&#24212;&#31361;&#21457;&#20107;&#20214;&#30340;&#38656;&#27714;&#20135;&#29983;&#20102;&#22312;&#20960;&#31186;&#20869;&#29983;&#25104;&#26032;&#35843;&#24230;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#35843;&#24230;&#35268;&#21017;&#65288;DRs&#65289;&#33021;&#22815;&#22312;&#32422;&#26463;&#19979;&#29983;&#25104;&#35843;&#24230;&#65292;&#23613;&#31649;&#20854;&#36136;&#37327;&#21487;&#20197;&#24471;&#21040;&#25913;&#36827;&#12290;&#20026;&#20102;&#25913;&#21892;&#32467;&#26524;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;FJSSP&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#19968;&#20010;&#31574;&#30053;&#65292;&#23558;&#25805;&#20316;&#20998;&#37197;&#21040;&#26426;&#22120;&#19978;&#29983;&#25104;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22411;&#30340;FJSSP&#23454;&#20363;&#20013;&#20173;&#28982;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#24456;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#33021;&#22815;&#31283;&#20581;&#35299;&#20915;&#22823;&#22411;FJSSP&#23454;&#20363;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Flexible Job Shop Scheduling Problem (FJSSP) has been extensively studied in the literature, and multiple approaches have been proposed within the heuristic, exact, and metaheuristic methods. However, the industry's demand to be able to respond in real-time to disruptive events has generated the necessity to be able to generate new schedules within a few seconds. Among these methods, under this constraint, only dispatching rules (DRs) are capable of generating schedules, even though their quality can be improved. To improve the results, recent methods have been proposed for modeling the FJSSP as a Markov Decision Process (MDP) and employing reinforcement learning to create a policy that generates an optimal solution assigning operations to machines. Nonetheless, there is still room for improvement, particularly in the larger FJSSP instances which are common in real-world scenarios. Therefore, the objective of this paper is to propose a method capable of robustly solving large insta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#29305;&#23450;&#23454;&#20363;&#19978;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#22810;&#34892;&#20026;&#35774;&#32622;&#21644;&#31163;&#25955;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.14526</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Zero Shot Learning in Restless Multi-armed Bandits. (arXiv:2310.14526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14526
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#29305;&#23450;&#23454;&#20363;&#19978;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#22810;&#34892;&#20026;&#35774;&#32622;&#21644;&#31163;&#25955;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#19968;&#31867;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#8212;&#8212;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;RMABs&#65289;&#65292;&#35813;&#38382;&#39064;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#22312;&#32447;&#24191;&#21578;&#21644;&#21453;&#30423;&#29454;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#20808;&#21069;&#30340;RMAB&#30740;&#31350;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#36830;&#32493;&#29366;&#24577;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#24403;&#36172;&#21338;&#26426;&#30340;&#20837;&#36873;&#21644;&#36864;&#20986;&#19981;&#26029;&#21457;&#29983;&#26102;&#65292;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PreFeRMAB&#65289;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#23545;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#24191;&#27867;RMAB&#38382;&#39064;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#27604;&#20174;&#22836;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#22320;&#23545;&#29305;&#23450;&#23454;&#20363;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#22810;&#34892;&#20026;&#35774;&#32622;&#21644;&#31163;&#25955;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;&#20026;&#20102;&#23454;&#29616;&#24555;&#36895;&#27867;&#21270;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#19968;&#31574;&#30053;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#29305;&#24449;&#20449;&#24687;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restless multi-arm bandits (RMABs), a class of resource allocation problems with broad application in areas such as healthcare, online advertising, and anti-poaching, have recently been studied from a multi-agent reinforcement learning perspective. Prior RMAB research suffers from several limitations, e.g., it fails to adequately address continuous states, and requires retraining from scratch when arms opt-in and opt-out over time, a common challenge in many real world applications. We address these limitations by developing a neural network-based pre-trained model (PreFeRMAB) that has general zero-shot ability on a wide range of previously unseen RMABs, and which can be fine-tuned on specific instances in a more sample-efficient way than retraining from scratch. Our model also accommodates general multi-action settings and discrete or continuous state spaces. To enable fast generalization, we learn a novel single policy network model that utilizes feature information and employs a tra
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RRL&#30340;&#26032;&#22411;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#38750;&#27169;&#31946;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#33391;&#22909;&#21487;&#25193;&#23637;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14336</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Rules for Scalable Data Representation and Classification. (arXiv:2310.14336v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14336
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RRL&#30340;&#26032;&#22411;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#38750;&#27169;&#31946;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#33391;&#22909;&#21487;&#25193;&#23637;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65288;&#22914;&#20915;&#31574;&#26641;&#65289;&#22312;&#38656;&#35201;&#39640;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#22330;&#26223;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#36879;&#26126;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#33391;&#22909;&#30340;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31163;&#25955;&#30340;&#21442;&#25968;&#21644;&#32467;&#26500;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#22312;&#20248;&#21270;&#26041;&#38754;&#24456;&#38590;&#24212;&#23545;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#38598;&#25104;&#26041;&#27861;&#21644;&#27169;&#31946;/&#36719;&#35268;&#21017;&#36890;&#24120;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20250;&#29306;&#29298;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#33719;&#24471;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#65292;&#31216;&#20026;&#22522;&#20110;&#35268;&#21017;&#30340;&#34920;&#31034;&#23398;&#20064;&#22120;&#65288;RRL&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#30340;&#38750;&#27169;&#31946;&#35268;&#21017;&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#19981;&#21487;&#24494;&#20998;&#30340;RRL&#65292;&#25105;&#20204;&#23558;&#20854;&#26144;&#23556;&#21040;&#36830;&#32493;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#26799;&#24230;&#23884;&#20837;&#30340;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#30452;&#25509;&#20248;&#21270;&#31163;&#25955;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36923;&#36753;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#22686;&#21152;RRL&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#36827;&#34892;&#21028;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. A novel design of logical activation functions is also devised to increase the scalability of RRL and enable it to discr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#34920;&#36798;GC2&#26597;&#35810;&#65292;&#19982;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#23384;&#22312;&#20998;&#31163;&#65292;&#36825;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.13139</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26377;&#38480;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks with polynomial activations have limited expressivity. (arXiv:2310.13139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26080;&#27861;&#34920;&#36798;GC2&#26597;&#35810;&#65292;&#19982;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#23384;&#22312;&#20998;&#31163;&#65292;&#36825;&#22238;&#31572;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#21487;&#20197;&#23436;&#20840;&#30001;&#36866;&#24403;&#30340;&#19968;&#38454;&#36923;&#36753;&#29255;&#27573;&#26469;&#25551;&#36848;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#20219;&#20309;&#22312;&#26631;&#35760;&#22270;&#19978;&#35299;&#37322;&#30340;&#20851;&#20110;&#20108;&#20803;&#36923;&#36753;&#29255;&#27573;&#65288;GC2&#65289;&#30340;&#26597;&#35810;&#37117;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#22823;&#23567;&#20165;&#21462;&#20915;&#20110;&#26597;&#35810;&#28145;&#24230;&#30340;GNN&#26469;&#34920;&#31034;&#12290;&#27491;&#22914;[Barcelo&#65286;Al&#12290;&#65292;2020&#65292;Grohe&#65292;2021]&#25351;&#20986;&#30340;&#37027;&#26679;&#65292;&#36825;&#20010;&#25551;&#36848;&#36866;&#29992;&#20110;&#19968;&#32452;&#28608;&#27963;&#20989;&#25968;&#30340;&#23478;&#26063;&#65292;&#36825;&#34920;&#26126;GNN&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#28608;&#27963;&#20989;&#25968;&#36873;&#25321;&#26469;&#34920;&#36798;&#19981;&#21516;&#30340;&#36923;&#36753;&#23618;&#27425;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#23618;&#27425;&#32467;&#26500;&#30340;&#23384;&#22312;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#30340;GNN&#26080;&#27861;&#34920;&#31034;GC2&#26597;&#35810;&#12290;&#36825;&#24847;&#21619;&#30528;&#22810;&#39033;&#24335;&#21644;&#24120;&#29992;&#30340;&#38750;&#22810;&#39033;&#24335;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;ReLU&#12289;sigmoid&#12289;&#21452;&#26354;&#27491;&#20999;&#31561;&#65289;&#20043;&#38388;&#23384;&#22312;&#19968;&#20010;&#20998;&#31163;&#65292;&#24182;&#22238;&#31572;&#20102;[Grohe&#65292;2021]&#25552;&#20986;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expressivity of Graph Neural Networks (GNNs) can be entirely characterized by appropriate fragments of the first order logic. Namely, any query of the two variable fragment of graded modal logic (GC2) interpreted over labelled graphs can be expressed using a GNN whose size depends only on the depth of the query. As pointed out by [Barcelo &amp; Al., 2020, Grohe, 2021 ], this description holds for a family of activation functions, leaving the possibibility for a hierarchy of logics expressible by GNNs depending on the chosen activation function. In this article, we show that such hierarchy indeed exists by proving that GC2 queries cannot be expressed by GNNs with polynomial activation functions. This implies a separation between polynomial and popular non polynomial activations (such as ReLUs, sigmoid and hyperbolic tan and others) and answers an open question formulated by [Grohe, 2021].
&lt;/p&gt;</description></item><item><title>&#30697;&#38453;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;MFNs&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#35299;&#26512;&#30697;&#38453;&#31561;&#21464;&#20989;&#25968;&#26469;&#21442;&#25968;&#21270;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10434</link><description>&lt;p&gt;
&#31561;&#21464;&#30697;&#38453;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Equivariant Matrix Function Neural Networks. (arXiv:2310.10434v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10434
&lt;/p&gt;
&lt;p&gt;
&#30697;&#38453;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;MFNs&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#35299;&#26512;&#30697;&#38453;&#31561;&#21464;&#20989;&#25968;&#26469;&#21442;&#25968;&#21270;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#23588;&#20854;&#26159;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#65292;&#24050;&#32463;&#25104;&#20026;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23398;&#20064;&#22270;&#24418;&#30340;&#24378;&#22823;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#24403;&#24314;&#27169;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#26102;&#65292;MPNNs&#22312;&#22823;&#20849;&#36717;&#20998;&#23376;&#65292;&#37329;&#23646;&#25110;&#38750;&#26230;&#24577;&#26448;&#26009;&#31561;&#31995;&#32479;&#20013;&#38754;&#20020;&#25361;&#25112;&#12290;&#23613;&#31649;&#35889;GNN&#21644;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;&#20363;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#65289;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#32570;&#20047;&#25193;&#23637;&#24615;&#65292;&#36866;&#24212;&#24615;&#65292;&#27867;&#21270;&#33021;&#21147;&#65292;&#35745;&#31639;&#25928;&#29575;&#65292;&#25110;&#32773;&#19981;&#33021;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#35814;&#32454;&#32467;&#26500;&#20851;&#31995;&#25110;&#23545;&#31216;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30697;&#38453;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;MFNs&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#35299;&#26512;&#30697;&#38453;&#31561;&#21464;&#20989;&#25968;&#26469;&#21442;&#25968;&#21270;&#38750;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#37319;&#29992;&#35299;&#26512;&#30697;&#38453;&#23637;&#24320;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#25509;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38543;&#31995;&#32479;&#22823;&#23567;&#32447;&#24615;&#25193;&#23637;&#30340;&#28508;&#21147;&#12290;&#35813;MFN&#26550;&#26500;&#22312;&#26631;&#20934;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs), especially message-passing neural networks (MPNNs), have emerged as powerful architectures for learning on graphs in diverse applications. However, MPNNs face challenges when modeling non-local interactions in systems such as large conjugated molecules, metals, or amorphous materials. Although Spectral GNNs and traditional neural networks such as recurrent neural networks and transformers mitigate these challenges, they often lack extensivity, adaptability, generalizability, computational efficiency, or fail to capture detailed structural relationships or symmetries in the data. To address these concerns, we introduce Matrix Function Neural Networks (MFNs), a novel architecture that parameterizes non-local interactions through analytic matrix equivariant functions. Employing resolvent expansions offers a straightforward implementation and the potential for linear scaling with system size. The MFN architecture achieves state-of-the-art performance in standa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#28389;&#29992;&#38382;&#39064;&#30340;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#26032;&#39062;&#27010;&#24565;&#26816;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Ring-A-Bell&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#32418;&#38431;&#24037;&#20855;&#65292;&#21487;&#20197;&#20107;&#20808;&#20934;&#22791;&#25972;&#20010;&#35780;&#20272;&#36807;&#31243;&#65292;&#32780;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.10012</link><description>&lt;p&gt;
&#21709;&#38083;&#65281;&#27010;&#24565;&#21435;&#38500;&#26041;&#27861;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#21487;&#38752;&#24615;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?. (arXiv:2310.10012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#28389;&#29992;&#38382;&#39064;&#30340;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#26032;&#39062;&#27010;&#24565;&#26816;&#32034;&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Ring-A-Bell&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#32418;&#38431;&#24037;&#20855;&#65292;&#21487;&#20197;&#20107;&#20808;&#20934;&#22791;&#25972;&#20010;&#35780;&#20272;&#36807;&#31243;&#65292;&#32780;&#26080;&#38656;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;(T2I)&#21512;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#30340;&#25193;&#25955;(SD)&#65292;&#26368;&#36817;&#23637;&#31034;&#20986;&#20102;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#28508;&#22312;&#28389;&#29992;&#30340;&#20960;&#20010;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;&#21019;&#24314;&#21463;&#29256;&#26435;&#38480;&#21046;&#12289;&#31105;&#27490;&#21644;&#21463;&#38480;&#20869;&#23481;&#65292;&#25110;&#32773;&#19981;&#36866;&#23452;&#24037;&#20316;&#30340;(NSFW)&#22270;&#29255;&#26041;&#38754;&#12290;&#34429;&#28982;&#24050;&#32463;&#37319;&#21462;&#20102;&#19968;&#20123;&#25514;&#26045;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#35780;&#20272;&#38454;&#27573;&#23454;&#26045;&#23433;&#20840;&#36807;&#28388;&#22120;&#25110;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#26469;&#28040;&#38500;&#19981;&#21463;&#27426;&#36814;&#30340;&#27010;&#24565;&#25110;&#39118;&#26684;&#65292;&#20294;&#36825;&#20123;&#23433;&#20840;&#25514;&#26045;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#30340;&#26032;&#39062;&#27010;&#24565;&#26816;&#32034;&#31639;&#27861;&#26469;&#30740;&#31350;&#36825;&#20123;&#23433;&#20840;&#26426;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Ring-A-Bell&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;T2I&#25193;&#25955;&#27169;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#32418;&#38431;&#24037;&#20855;&#65292;&#25972;&#20010;&#35780;&#20272;&#21487;&#20197;&#22312;&#27809;&#26377;&#30446;&#26631;&#27169;&#22411;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#21069;&#20934;&#22791;&#22909;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Ring-A-Bell&#39318;&#20808;&#23545;&#28608;&#21169;&#36827;&#34892;&#20998;&#35299;&#65292;&#28982;&#21518;&#36890;&#36807;&#21435;&#38500;&#20505;&#36873;&#27010;&#24565;&#21644;&#35745;&#31639;&#29305;&#23450;&#27010;&#24565;&#30340;&#30456;&#20851;&#24230;&#26469;&#35774;&#35745;&#31579;&#36873;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first pe
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20174;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#20840;&#38754;&#35780;&#20272;&#20102;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22238;&#31572;&#20102;&#20309;&#26102;&#20462;&#25913;&#22270;&#25968;&#25454;&#12289;&#22270;&#25968;&#25454;&#30340;&#21738;&#19968;&#37096;&#20998;&#38656;&#35201;&#20462;&#25913;&#20197;&#21450;&#22914;&#20309;&#20445;&#25252;&#22270;&#27169;&#22411;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04987</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#21270;&#22270;&#23398;&#20064;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data-centric Graph Learning: A Survey. (arXiv:2310.04987v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20174;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#20840;&#38754;&#35780;&#20272;&#20102;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22238;&#31572;&#20102;&#20309;&#26102;&#20462;&#25913;&#22270;&#25968;&#25454;&#12289;&#22270;&#25968;&#25454;&#30340;&#21738;&#19968;&#37096;&#20998;&#38656;&#35201;&#20462;&#25913;&#20197;&#21450;&#22914;&#20309;&#20445;&#25252;&#22270;&#27169;&#22411;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#21382;&#21490;&#35265;&#35777;&#20102;&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#22823;&#24433;&#21709;&#65292;&#20363;&#22914;AlexNet&#21644;ResNet&#30340;ImageNet&#12290;&#26368;&#36817;&#65292;&#19982;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#35774;&#35745;&#26356;&#22797;&#26434;&#30340;&#31070;&#32463;&#32467;&#26500;&#19981;&#21516;&#65292;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#20851;&#27880;&#37325;&#28857;&#36716;&#21521;&#20102;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20391;&#37325;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#25968;&#25454;&#20197;&#22686;&#24378;&#31070;&#32463;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#32780;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#65292;&#25805;&#20316;&#26222;&#36941;&#23384;&#22312;&#30340;&#25299;&#25169;&#25968;&#25454;&#30340;&#22270;&#23398;&#20064;&#20063;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#20013;&#24515;&#21270;&#30340;&#35282;&#24230;&#20840;&#38754;&#35780;&#20272;&#20102;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#26088;&#22312;&#22238;&#31572;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#20309;&#26102;&#20462;&#25913;&#22270;&#25968;&#25454;&#65292;&#65288;2&#65289;&#22270;&#25968;&#25454;&#30340;&#21738;&#19968;&#37096;&#20998;&#38656;&#35201;&#20462;&#25913;&#20197;&#37322;&#25918;&#21508;&#31181;&#22270;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#20445;&#25252;&#22270;&#27169;&#22411;&#20813;&#21463;&#26377;&#38382;&#39064;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#23398;&#20064;&#27969;&#31243;&#38454;&#27573;&#30340;&#21019;&#26032;&#20998;&#31867;&#27861;&#65292;&#24182;&#31361;&#20986;&#20102;&#20851;&#38190;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The history of artificial intelligence (AI) has witnessed the significant impact of high-quality data on various deep learning models, such as ImageNet for AlexNet and ResNet. Recently, instead of designing more complex neural architectures as model-centric approaches, the attention of AI community has shifted to data-centric ones, which focuses on better processing data to strengthen the ability of neural models. Graph learning, which operates on ubiquitous topological data, also plays an important role in the era of deep learning. In this survey, we comprehensively review graph learning approaches from the data-centric perspective, and aim to answer three crucial questions: (1) when to modify graph data, (2) what part of the graph data needs modification to unlock the potential of various graph models, and (3) how to safeguard graph models from problematic data influence. Accordingly, we propose a novel taxonomy based on the stages in the graph learning pipeline, and highlight the pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#27169;&#20223;&#23398;&#20064;&#20013;&#31070;&#32463;&#20195;&#29702;&#30340;&#21160;&#24577;DAG&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#21521;&#26080;&#29615;&#22240;&#26524;&#22270;&#23637;&#29616;&#20854;&#25429;&#33719;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#21152;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00489</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#27169;&#20223;&#23398;&#20064;&#30340;&#21160;&#24577;DAG&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Dynamic DAG Discovery for Interpretable Imitation Learning. (arXiv:2310.00489v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#27169;&#20223;&#23398;&#20064;&#20013;&#31070;&#32463;&#20195;&#29702;&#30340;&#21160;&#24577;DAG&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#21521;&#26080;&#29615;&#22240;&#26524;&#22270;&#23637;&#29616;&#20854;&#25429;&#33719;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#21152;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#36890;&#36807;&#27169;&#20223;&#19987;&#23478;&#30340;&#31034;&#33539;&#26469;&#23398;&#20064;&#20195;&#29702;&#31574;&#30053;&#65292;&#22312;&#21307;&#30103;&#27835;&#30103;&#26041;&#26696;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#20195;&#29702;&#23398;&#20064;&#21040;&#30340;&#25511;&#21046;&#31574;&#30053;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22256;&#38590;&#20027;&#35201;&#26469;&#33258;&#20004;&#20010;&#26041;&#38754;&#65306;1&#65289;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#36890;&#24120;&#23454;&#29616;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65307;2&#65289;&#20195;&#29702;&#20915;&#31574;&#32972;&#21518;&#30340;&#28508;&#22312;&#22240;&#26524;&#26426;&#21046;&#21487;&#33021;&#38543;&#30528;&#36712;&#36857;&#32780;&#21464;&#21270;&#65292;&#32780;&#19981;&#26159;&#22312;&#25972;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#20445;&#25345;&#38745;&#24577;&#19981;&#21464;&#12290;&#20026;&#20102;&#22686;&#21152;&#31070;&#32463;&#20195;&#29702;&#30340;&#36879;&#26126;&#24230;&#21644;&#25552;&#20379;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#26377;&#21521;&#26080;&#29615;&#22240;&#26524;&#22270;&#30340;&#24418;&#24335;&#23637;&#31034;&#20854;&#25152;&#25429;&#33719;&#30340;&#30693;&#35782;&#65292;&#20854;&#20013;&#33410;&#28857;&#26159;&#21160;&#20316;&#21644;&#29366;&#24577;&#21464;&#37327;&#65292;&#36793;&#34920;&#31034;&#39044;&#27979;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#36825;&#20010;&#22240;&#26524;&#21457;&#29616;&#36807;&#31243;&#26159;&#20381;&#36182;&#29366;&#24577;&#30340;&#65292;&#20351;&#20854;&#33021;&#22815;&#23545;&#28508;&#22312;&#22240;&#26524;&#22270;&#20013;&#30340;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning, which learns agent policy by mimicking expert demonstration, has shown promising results in many applications such as medical treatment regimes and self-driving vehicles. However, it remains a difficult task to interpret control policies learned by the agent. Difficulties mainly come from two aspects: 1) agents in imitation learning are usually implemented as deep neural networks, which are black-box models and lack interpretability; 2) the latent causal mechanism behind agents' decisions may vary along the trajectory, rather than staying static throughout time steps. To increase transparency and offer better interpretability of the neural agent, we propose to expose its captured knowledge in the form of a directed acyclic causal graph, with nodes being action and state variables and edges denoting the causal relations behind predictions. Furthermore, we design this causal discovery process to be state-dependent, enabling it to model the dynamics in latent causal gr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#34892;&#20026;&#30340;&#38887;&#24615;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#28508;&#22312;&#29305;&#24449;&#22312;&#30456;&#21516;&#20449;&#24687;&#22833;&#30495;&#27700;&#24179;&#19979;&#27604;&#36755;&#20837;&#34920;&#31034;&#26356;&#21152;&#38887;&#24615;&#65292;&#24182;&#19988;&#23545;&#25239;&#24615;&#38887;&#24615;&#30001;&#29305;&#24449;&#32500;&#24230;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#20849;&#21516;&#20915;&#23450;&#12290;</title><link>http://arxiv.org/abs/2309.17401</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversarial Machine Learning in Latent Representations of Neural Networks. (arXiv:2309.17401v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17401
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#34892;&#20026;&#30340;&#38887;&#24615;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#28508;&#22312;&#29305;&#24449;&#22312;&#30456;&#21516;&#20449;&#24687;&#22833;&#30495;&#27700;&#24179;&#19979;&#27604;&#36755;&#20837;&#34920;&#31034;&#26356;&#21152;&#38887;&#24615;&#65292;&#24182;&#19988;&#23545;&#25239;&#24615;&#38887;&#24615;&#30001;&#29305;&#24449;&#32500;&#24230;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#20849;&#21516;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20943;&#36731;&#31227;&#21160;&#35774;&#22791;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#38477;&#20302;&#36793;&#32536;&#35745;&#31639;&#22330;&#26223;&#20013;&#30340;&#31471;&#21040;&#31471;&#25512;&#29702;&#24310;&#36831;&#12290;&#23613;&#31649;&#24050;&#32463;&#23545;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#23545;&#25239;&#24615;&#34892;&#20026;&#30340;&#38887;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#20998;&#26512;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#34892;&#20026;&#30340;&#38887;&#24615;&#26469;&#22635;&#34917;&#29616;&#26377;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#32622;&#20110;&#20449;&#24687;&#35770;&#30340;&#32972;&#26223;&#19979;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#34913;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#22833;&#30495;&#21644;&#38887;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#34920;&#26126;&#65306;&#65288;i&#65289;&#22312;&#20551;&#35774;&#20855;&#26377;&#30456;&#21516;&#20449;&#24687;&#22833;&#30495;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#65292;&#28508;&#22312;&#29305;&#24449;&#22987;&#32456;&#27604;&#36755;&#20837;&#34920;&#31034;&#26356;&#21152;&#38887;&#24615;&#65307;&#65288;ii&#65289;&#23545;&#25239;&#24615;&#38887;&#24615;&#21516;&#26102;&#30001;&#29305;&#24449;&#32500;&#24230;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#20915;&#23450;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#32771;&#34385;&#20102;6&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed deep neural networks (DNNs) have been shown to reduce the computational burden of mobile devices and decrease the end-to-end inference latency in edge computing scenarios. While distributed DNNs have been studied, to the best of our knowledge the resilience of distributed DNNs to adversarial action still remains an open problem. In this paper, we fill the existing research gap by rigorously analyzing the robustness of distributed DNNs against adversarial action. We cast this problem in the context of information theory and introduce two new measurements for distortion and robustness. Our theoretical findings indicate that (i) assuming the same level of information distortion, latent features are always more robust than input representations; (ii) the adversarial robustness is jointly determined by the feature dimension and the generalization capability of the DNN. To test our theoretical findings, we perform extensive experimental analysis by considering 6 different DNN arc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#12289;&#21151;&#33021;&#24615;&#30340;&#20195;&#30721;&#32763;&#35793;&#27169;&#22411;&#27979;&#35797;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#23646;&#24615;&#20174;&#32431;&#35821;&#27861;&#21040;&#32431;&#35821;&#20041;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12813</link><description>&lt;p&gt;
&#33258;&#21160;&#27979;&#35797;&#20195;&#30721;&#32763;&#35793;&#27169;&#22411;&#30340;&#21151;&#33021;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Automatically Testing Functional Properties of Code Translation Models. (arXiv:2309.12813v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#12289;&#21151;&#33021;&#24615;&#30340;&#20195;&#30721;&#32763;&#35793;&#27169;&#22411;&#27979;&#35797;&#26041;&#27861;&#65292;&#33021;&#22815;&#25429;&#25417;&#21508;&#31181;&#23646;&#24615;&#20174;&#32431;&#35821;&#27861;&#21040;&#32431;&#35821;&#20041;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36328;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;$transpiling$&#65292;&#27491;&#21464;&#24471;&#26085;&#30410;&#23454;&#29992;&#12290;&#23613;&#31649;&#33258;&#21160;&#32763;&#35793;&#26174;&#33879;&#25552;&#39640;&#20102;&#24320;&#21457;&#32773;&#30340;&#29983;&#20135;&#21147;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#27491;&#30830;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#26368;&#21021;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#27979;&#35797;&#22871;&#20214;&#26469;&#27979;&#35797;&#23567;&#35268;&#27169;&#31243;&#24207;&#30340;&#32763;&#35793;&#65292;&#21518;&#26469;&#21448;&#23558;&#36825;&#20123;&#27979;&#35797;&#22871;&#20214;&#33258;&#21160;&#21270;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#21151;&#33021;&#24615;&#23646;&#24615;&#27979;&#35797;&#30340;&#20195;&#30721;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20851;&#20110;&#36716;&#25442;&#21518;&#20195;&#30721;&#30340;&#19968;&#33324;&#29992;&#25143;&#25552;&#20379;&#30340;&#35268;&#33539;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#23646;&#24615;&#65292;&#20174;&#32431;&#35821;&#27861;&#21040;&#32431;&#35821;&#20041;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#23454;&#39564;&#25152;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#26816;&#27979;&#27969;&#34892;&#30340;&#20195;&#30721;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#23646;&#24615;&#36829;&#35268;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#27492;&#21487;&#20197;&#26681;&#25454;&#32473;&#23450;&#30340;&#23646;&#24615;&#35780;&#20272;&#27169;&#22411;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#29992;&#25143;&#21482;&#38656;&#33719;&#24471;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming increasingly practical for translating code across programming languages, a process known as $transpiling$. Even though automated transpilation significantly boosts developer productivity, a key concern is whether the generated code is correct. Existing work initially used manually crafted test suites to test the translations of a small corpus of programs; these test suites were later automated. In contrast, we devise the first approach for automated, functional, property-based testing of code translation models. Our general, user-provided specifications about the transpiled code capture a range of properties, from purely syntactic to purely semantic ones. As shown by our experiments, this approach is very effective in detecting property violations in popular code translation models, and therefore, in evaluating model quality with respect to given properties. We also go a step further and explore the usage scenario where a user simply aims to obtain a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24046;&#20998;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#22312;&#21160;&#24577;&#21644;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#20063;&#20851;&#27880;&#20102;&#25935;&#24863;&#29992;&#25143;&#29305;&#24449;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2309.11515</link><description>&lt;p&gt;
&#36816;&#29992;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#30340;&#39034;&#24207;&#25512;&#33616;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Differential Privacy in Sequential Recommendation: A Noisy Graph Neural Network Approach. (arXiv:2309.11515v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11515
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#24046;&#20998;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#22312;&#21160;&#24577;&#21644;&#20381;&#36182;&#20851;&#31995;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#20063;&#20851;&#27880;&#20102;&#25935;&#24863;&#29992;&#25143;&#29305;&#24449;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#22312;&#32447;&#24179;&#21488;&#20013;&#39640;&#35843;&#30340;&#38544;&#31169;&#27844;&#38706;&#20107;&#20214;&#39057;&#32321;&#21457;&#29983;&#65292;&#29992;&#25143;&#23545;&#38544;&#31169;&#36234;&#26469;&#36234;&#20851;&#27880;&#12290;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#22312;&#32447;&#24179;&#21488;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20854;&#38544;&#31169;&#20445;&#25252;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24046;&#20998;&#38544;&#31169;&#25512;&#33616;&#31995;&#32479;&#21482;&#32771;&#34385;&#38745;&#24577;&#21644;&#29420;&#31435;&#30340;&#29992;&#25143;&#20132;&#20114;&#65292;&#22240;&#27492;&#26080;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#21160;&#24577;&#21644;&#20381;&#36182;&#20851;&#31995;&#30340;&#39034;&#24207;&#25512;&#33616;&#12290;&#21516;&#26102;&#65292;&#23545;&#20110;&#25935;&#24863;&#29992;&#25143;&#29305;&#24449;&#30340;&#38544;&#31169;&#39118;&#38505;&#20851;&#27880;&#36739;&#23569;&#65292;&#22823;&#22810;&#25968;&#21482;&#20445;&#25252;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#39034;&#24207;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22122;&#22768;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65288;&#31216;&#20026;DIPSGNN&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increasing frequency of high-profile privacy breaches in various online platforms, users are becoming more concerned about their privacy. And recommender system is the core component of online platforms for providing personalized service, consequently, its privacy preservation has attracted great attention. As the gold standard of privacy protection, differential privacy has been widely adopted to preserve privacy in recommender systems. However, existing differentially private recommender systems only consider static and independent interactions, so they cannot apply to sequential recommendation where behaviors are dynamic and dependent. Meanwhile, little attention has been paid on the privacy risk of sensitive user features, most of them only protect user feedbacks. In this work, we propose a novel DIfferentially Private Sequential recommendation framework with a noisy Graph Neural Network approach (denoted as DIPSGNN) to address these limitations. To the best of our knowledge, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#28040;&#34701;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#36335;&#24452;&#26469;&#21435;&#38500;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#33391;&#34892;&#20026;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20943;&#23569;GPT-2&#27602;&#24615;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#65292;&#20165;&#28040;&#34701;12&#26465;&#22240;&#26524;&#36793;&#20013;&#30340;11.6K&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#27602;&#24615;&#29983;&#25104;&#65292;&#24182;&#22312;&#20854;&#20182;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.05973</link><description>&lt;p&gt;
&#20999;&#26029;&#30005;&#36335;: &#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#28040;&#34701;&#21435;&#38500;&#27169;&#22411;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Circuit Breaking: Removing Model Behaviors with Targeted Ablation. (arXiv:2309.05973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#28040;&#34701;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#36335;&#24452;&#26469;&#21435;&#38500;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#33391;&#34892;&#20026;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#20943;&#23569;GPT-2&#27602;&#24615;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#65292;&#20165;&#28040;&#34701;12&#26465;&#22240;&#26524;&#36793;&#20013;&#30340;11.6K&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;&#27602;&#24615;&#29983;&#25104;&#65292;&#24182;&#22312;&#20854;&#20182;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#34920;&#29616;&#20986;&#22312;&#39044;&#35757;&#32451;&#30446;&#26631;&#19978;&#25552;&#39640;&#24615;&#33021;&#20294;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#38477;&#20302;&#24615;&#33021;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#34701;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#22240;&#26524;&#36335;&#24452;&#65292;&#20197;&#31105;&#29992;&#19982;&#19981;&#33391;&#34892;&#20026;&#26377;&#20851;&#30340;&#35745;&#31639;&#30005;&#36335;&#65292;&#20174;&#32780;&#21435;&#38500;&#19981;&#33391;&#34892;&#20026;&#12290;&#22312;&#25317;&#26377;&#27169;&#22411;&#34920;&#29616;&#24046;&#30340;&#23567;&#22411;&#36755;&#20837;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23398;&#20250;&#20102;&#28040;&#34701;&#19968;&#23567;&#37096;&#20998;&#37325;&#35201;&#30340;&#22240;&#26524;&#36335;&#24452;&#12290;&#22312;&#20943;&#23569;GPT-2&#27602;&#24615;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#28040;&#34701;&#20165;&#20165;12&#26465;&#22240;&#26524;&#36793;&#20013;&#30340;11.6K&#65292;&#21487;&#20197;&#20943;&#36731;&#27602;&#24615;&#29983;&#25104;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#19979;&#38477;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. We propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. Given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. In the setting of reducing GPT-2 toxic language generation, we find ablating just 12 of the 11.6K causal edges mitigates toxic generation with minimal degradation of performance on other inputs.
&lt;/p&gt;</description></item><item><title>&#23558;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#30340;&#36807;&#31243;&#35270;&#20026;&#30693;&#35782;&#26469;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#65292;&#23637;&#31034;&#20854;&#26377;&#36259;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04284</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#30340;&#36807;&#31243;&#35270;&#20026;&#30693;&#35782;&#26469;&#28304; - &#24212;&#29992;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Viewing the process of generating counterfactuals as a source of knowledge -- Application to the Naive Bayes classifier. (arXiv:2309.04284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04284
&lt;/p&gt;
&lt;p&gt;
&#23558;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#30340;&#36807;&#31243;&#35270;&#20026;&#30693;&#35782;&#26469;&#28304;&#65292;&#24182;&#24212;&#29992;&#20110;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#65292;&#23637;&#31034;&#20854;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#35768;&#22810;&#29702;&#35299;&#31639;&#27861;&#21487;&#20197;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20915;&#31574;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#29983;&#25104;&#23545;&#31435;&#20551;&#35774;&#31034;&#20363;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#36825;&#20010;&#29983;&#25104;&#36807;&#31243;&#35270;&#20026;&#19968;&#31181;&#21019;&#36896;&#19968;&#23450;&#37327;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#30693;&#35782;&#21487;&#20197;&#23384;&#20648;&#24182;&#22312;&#20197;&#21518;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#26412;&#25991;&#22312;&#21152;&#27861;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#22312;&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#27492;&#30446;&#30340;&#19978;&#30340;&#26377;&#36259;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are now many comprehension algorithms for understanding the decisions of a machine learning algorithm. Among these are those based on the generation of counterfactual examples. This article proposes to view this generation process as a source of creating a certain amount of knowledge that can be stored to be used, later, in different ways. This process is illustrated in the additive model and, more specifically, in the case of the naive Bayes classifier, whose interesting properties for this purpose are shown.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29992;&#27169;&#25311;&#22120;&#26356;&#26032;&#28508;&#22312;&#29366;&#24577;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#28151;&#21512;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25511;&#21046;&#39044;&#27979;&#24182;&#38450;&#27490;&#32047;&#31215;&#35823;&#24046;&#30340;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2309.02873</link><description>&lt;p&gt;
&#36890;&#36807;&#20855;&#22791;&#27169;&#25311;&#22120;&#20449;&#24687;&#30340;&#28508;&#22312;&#29366;&#24577;&#23398;&#20064;&#28151;&#21512;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Dynamics Models With Simulator-Informed Latent States. (arXiv:2309.02873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29992;&#27169;&#25311;&#22120;&#26356;&#26032;&#28508;&#22312;&#29366;&#24577;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#28151;&#21512;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#25511;&#21046;&#39044;&#27979;&#24182;&#38450;&#27490;&#32047;&#31215;&#35823;&#24046;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#21147;&#23398;&#27169;&#22411;&#23398;&#20064;&#30340;&#20219;&#21153;&#26159;&#20174;&#27979;&#37327;&#25968;&#25454;&#20013;&#25512;&#26029;&#26410;&#30693;&#30340;&#31995;&#32479;&#21160;&#21147;&#23398;&#65292;&#24182;&#39044;&#27979;&#31995;&#32479;&#26410;&#26469;&#30340;&#34892;&#20026;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#35757;&#32451;&#36882;&#24402;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#36890;&#24120;&#22312;&#29289;&#29702;&#24847;&#20041;&#19978;&#19981;&#21512;&#29702;&#65292;&#24182;&#19988;&#30001;&#20110;&#32047;&#31215;&#35823;&#24046;&#30340;&#23384;&#22312;&#65292;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#65292;&#23427;&#20204;&#30340;&#34892;&#20026;&#20250;&#24694;&#21270;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#26500;&#24314;&#30340;&#27169;&#25311;&#22120;&#26159;&#29289;&#29702;&#24847;&#20041;&#19978;&#21512;&#29702;&#30340;&#12290;&#28982;&#32780;&#65292;&#24314;&#27169;&#31616;&#21270;&#36890;&#24120;&#20250;&#23548;&#33268;&#36825;&#20123;&#27169;&#22411;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#28151;&#21512;&#24314;&#27169;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#36235;&#21183;&#65292;&#26088;&#22312;&#32467;&#21512;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#40657;&#30418;&#27169;&#25311;&#22120;&#23558;&#23398;&#20064;&#27169;&#22411;&#30340;&#28508;&#22312;&#29366;&#24577;&#36827;&#34892;&#26356;&#26032;&#12290;&#36890;&#36807;&#27169;&#25311;&#22120;&#26469;&#25511;&#21046;&#39044;&#27979;&#65292;&#38450;&#27490;&#32047;&#31215;&#35823;&#24046;&#30340;&#21457;&#29983;&#65292;&#36825;&#26159;&#19968;&#39033;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamics model learning deals with the task of inferring unknown dynamics from measurement data and predicting the future behavior of the system. A typical approach to address this problem is to train recurrent models. However, predictions with these models are often not physically meaningful. Further, they suffer from deteriorated behavior over time due to accumulating errors. Often, simulators building on first principles are available being physically meaningful by design. However, modeling simplifications typically cause inaccuracies in these models. Consequently, hybrid modeling is an emerging trend that aims to combine the best of both worlds. In this paper, we propose a new approach to hybrid modeling, where we inform the latent states of a learned model via a black-box simulator. This allows to control the predictions via the simulator preventing them from accumulating errors. This is especially challenging since, in contrast to previous approaches, access to the simulator's la
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36830;&#32493;&#26102;&#38388;&#39640;&#26031;&#36807;&#31243;&#21160;&#21147;&#23398;&#36827;&#34892;&#31934;&#30830;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#31163;&#25955;&#26102;&#38388;&#19979;&#36827;&#34892;&#39044;&#27979;&#21487;&#33021;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#39640;&#38454;&#25968;&#20540;&#31215;&#20998;&#22120;&#36827;&#34892;&#21160;&#21147;&#23398;&#20989;&#25968;&#30340;&#31163;&#25955;&#21270;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#36817;&#20284;&#25512;&#26029;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.02351</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#39640;&#26031;&#36807;&#31243;&#21160;&#21147;&#23398;&#30340;&#31934;&#30830;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Exact Inference for Continuous-Time Gaussian Process Dynamics. (arXiv:2309.02351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#36830;&#32493;&#26102;&#38388;&#39640;&#26031;&#36807;&#31243;&#21160;&#21147;&#23398;&#36827;&#34892;&#31934;&#30830;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#31163;&#25955;&#26102;&#38388;&#19979;&#36827;&#34892;&#39044;&#27979;&#21487;&#33021;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#39640;&#38454;&#25968;&#20540;&#31215;&#20998;&#22120;&#36827;&#34892;&#21160;&#21147;&#23398;&#20989;&#25968;&#30340;&#31163;&#25955;&#21270;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#36817;&#20284;&#25512;&#26029;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29289;&#29702;&#31995;&#32479;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#31995;&#32479;&#26469;&#25551;&#36848;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#30495;&#23454;&#31995;&#32479;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#38656;&#35201;&#20174;&#27979;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#30001;&#20110;&#25968;&#25454;&#36890;&#24120;&#20197;&#31163;&#25955;&#26102;&#38388;&#25910;&#38598;&#65292;&#20363;&#22914;&#36890;&#36807;&#20256;&#24863;&#22120;&#65292;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#21160;&#21147;&#27169;&#22411;&#23398;&#20064;&#20013;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#38024;&#23545;&#19968;&#27493;&#39044;&#27979;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22312;&#19968;&#20123;&#22330;&#26223;&#20013;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#38382;&#39064;&#65292;&#20363;&#22914;&#22914;&#26524;&#27979;&#37327;&#32467;&#26524;&#20197;&#19981;&#35268;&#21017;&#30340;&#26102;&#38388;&#27493;&#38271;&#25552;&#20379;&#65292;&#25110;&#32773;&#29289;&#29702;&#31995;&#32479;&#23646;&#24615;&#38656;&#35201;&#20445;&#25345;&#19981;&#21464;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#23545;&#30495;&#23454;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;GP&#27169;&#22411;&#12290;&#39640;&#38454;&#25968;&#20540;&#31215;&#20998;&#22120;&#25552;&#20379;&#20102;&#36890;&#36807;&#20219;&#24847;&#31934;&#24230;&#31163;&#25955;&#21270;&#21160;&#21147;&#23398;&#20989;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24037;&#20855;&#12290;&#35768;&#22810;&#39640;&#38454;&#31215;&#20998;&#22120;&#38656;&#35201;&#22312;&#20013;&#38388;&#26102;&#38388;&#27493;&#39588;&#36827;&#34892;&#21160;&#21147;&#23398;&#35780;&#20272;&#65292;&#36825;&#20351;&#24471;&#31934;&#30830;&#30340;GP&#25512;&#26029;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#12290;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#26469;&#36817;&#20284;GP&#21518;&#39564;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#31934;&#30830;&#30340;GP&#25512;&#26029;&#26159;&#24456;&#22256;&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physical systems can often be described via a continuous-time dynamical system. In practice, the true system is often unknown and has to be learned from measurement data. Since data is typically collected in discrete time, e.g. by sensors, most methods in Gaussian process (GP) dynamics model learning are trained on one-step ahead predictions. This can become problematic in several scenarios, e.g. if measurements are provided at irregularly-sampled time steps or physical system properties have to be conserved. Thus, we aim for a GP model of the true continuous-time dynamics. Higher-order numerical integrators provide the necessary tools to address this problem by discretizing the dynamics function with arbitrary accuracy. Many higher-order integrators require dynamics evaluations at intermediate time steps making exact GP inference intractable. In previous work, this problem is often tackled by approximating the GP posterior with variational inference. However, exact GP inference is pre
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27714;&#35299;&#22120;&#22312;&#27169;&#25311;&#28237;&#27969;&#27969;&#22330;&#26102;&#22914;&#20309;&#23454;&#29616;&#26102;&#38388;&#31283;&#23450;&#24615;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#28436;&#21270;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#21487;&#20197;&#36229;&#36234;&#20854;&#20182;&#27969;&#22330;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.01745</link><description>&lt;p&gt;
&#29992;&#20110;&#28237;&#27969;&#27969;&#22330;&#27169;&#25311;&#30340;&#33258;&#22238;&#24402;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Autoregressive Conditional Diffusion Models for Turbulent Flow Simulation. (arXiv:2309.01745v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01745
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#27714;&#35299;&#22120;&#22312;&#27169;&#25311;&#28237;&#27969;&#27969;&#22330;&#26102;&#22914;&#20309;&#23454;&#29616;&#26102;&#38388;&#31283;&#23450;&#24615;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#28436;&#21270;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#21487;&#20197;&#36229;&#36234;&#20854;&#20182;&#27969;&#22330;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#28237;&#27969;&#27969;&#22330;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27714;&#35299;&#22120;&#26085;&#30410;&#21463;&#21040;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23398;&#20064;&#30340;PDE&#27714;&#35299;&#22120;&#26469;&#35828;&#65292;&#22312;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#28436;&#21270;&#26102;&#38388;&#20013;&#23454;&#29616;&#26102;&#38388;&#31283;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#27969;&#20307;&#27714;&#35299;&#22120;&#26159;&#21542;&#21033;&#29992;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#28436;&#21270;&#26159;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#34892;&#36873;&#25321;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20934;&#30830;&#24615;&#12289;&#21518;&#39564;&#37319;&#26679;&#12289;&#35889;&#29305;&#24615;&#21644;&#26102;&#38388;&#31283;&#23450;&#24615;&#65292;&#24182;&#35201;&#27714;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25512;&#24191;&#21040;&#36229;&#20986;&#35757;&#32451;&#33539;&#22260;&#30340;&#27969;&#21160;&#21442;&#25968;&#12290;&#20026;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#22320;&#23545;&#19968;&#31995;&#21015;&#27969;&#22330;&#39044;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#21253;&#25324;&#19981;&#21487;&#21387;&#32553;&#27969;&#21160;&#12289;&#36328;&#38899;&#36895;&#27969;&#21160;&#21644;&#21508;&#21521;&#21516;&#24615;&#28237;&#27969;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Simulating turbulent flows is crucial for a wide range of applications, and machine learning-based solvers are gaining increasing relevance. However, achieving temporal stability when generalizing to longer rollout horizons remains a persistent challenge for learned PDE solvers. In this work, we analyze if fully data-driven fluid solvers that utilize an autoregressive rollout based on conditional diffusion models are a viable option to address this challenge. We investigate accuracy, posterior sampling, spectral behavior, and temporal stability, while requiring that methods generalize to flow parameters beyond the training regime. To quantitatively and qualitatively benchmark the performance of a range of flow prediction approaches, three challenging scenarios including incompressible and transonic flows, as well as isotropic turbulence are employed. We find that even simple diffusion-based approaches can outperform multiple established flow prediction methods in terms of accuracy and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#30697;&#38453;&#26469;&#38477;&#20302;&#39640;&#32500;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#32500;&#25968;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;&#22522;&#20110;&#8220;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#8221;&#65292;&#25552;&#20986;&#20102;&#27867;&#21270;&#20445;&#35777;&#30340;&#25968;&#25454;&#37327;&#19982;&#24615;&#33021;&#25351;&#26631;&#30340;&#20266;&#32500;&#24230;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2309.00203</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#32447;&#24615;&#35268;&#21010;&#38477;&#32500;&#26041;&#27861;&#65306;&#27867;&#21270;&#30028;&#38480;&#21644;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods. (arXiv:2309.00203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#30697;&#38453;&#26469;&#38477;&#20302;&#39640;&#32500;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#30340;&#32500;&#25968;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#27714;&#35299;&#36895;&#24230;&#12290;&#22522;&#20110;&#8220;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#8221;&#65292;&#25552;&#20986;&#20102;&#27867;&#21270;&#20445;&#35777;&#30340;&#25968;&#25454;&#37327;&#19982;&#24615;&#33021;&#25351;&#26631;&#30340;&#20266;&#32500;&#24230;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#22788;&#29702;&#39640;&#32500;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65288;LP&#65289;&#12290;&#32473;&#23450;&#36807;&#21435;&#30340;$n$&#32500;LP&#25968;&#25454;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;$n\times k$&#30340;&#8220;&#25237;&#24433;&#30697;&#38453;&#8221;&#65288;$n &gt; k$&#65289;&#65292;&#23558;&#32500;&#25968;&#20174;$n$&#38477;&#20302;&#21040;$k$&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;$k$&#32500;LP&#38382;&#39064;&#24182;&#36890;&#36807;&#20056;&#20197;&#25237;&#24433;&#30697;&#38453;&#26469;&#24674;&#22797;$n$&#32500;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22788;&#29702;&#26410;&#26469;&#30340;LP&#23454;&#20363;&#12290;&#36825;&#20010;&#24605;&#24819;&#19982;&#20219;&#20309;&#29992;&#25143;&#39318;&#36873;&#30340;LP&#27714;&#35299;&#22120;&#20860;&#23481;&#65292;&#22240;&#27492;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#21152;&#36895;LP&#27714;&#35299;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#65306;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#25165;&#33021;&#30830;&#20445;&#24674;&#22797;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#65311;&#25105;&#20204;&#22522;&#20110;&#8220;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#8221;&#30340;&#24605;&#24819;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#23558;&#36275;&#22815;&#36827;&#34892;&#27867;&#21270;&#20445;&#35777;&#30340;&#25968;&#25454;&#37327;&#19982;&#24615;&#33021;&#25351;&#26631;&#30340;&#8220;&#20266;&#32500;&#24230;&#8221;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20266;&#32500;&#24230;&#30340;$\tilde{\mathrm{O}}(nk^2)$&#19978;&#30028;&#65288;$\tilde{\mathrm{O}}$&#21387;&#32553;&#20102;&#23545;&#25968;&#22240;&#23376;&#65289;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;$\Omega(nk)$&#19979;&#30028;&#26469;&#34917;&#20805;&#23427;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper studies a simple data-driven approach to high-dimensional linear programs (LPs). Given data of past $n$-dimensional LPs, we learn an $n\times k$ \textit{projection matrix} ($n &gt; k$), which reduces the dimensionality from $n$ to $k$. Then, we address future LP instances by solving $k$-dimensional LPs and recovering $n$-dimensional solutions by multiplying the projection matrix. This idea is compatible with any user-preferred LP solvers, hence a versatile approach to faster LP solving. One natural question is: how much data is sufficient to ensure the recovered solutions' quality? We address this question based on the idea of \textit{data-driven algorithm design}, which relates the amount of data sufficient for generalization guarantees to the \textit{pseudo-dimension} of performance metrics. We present an $\tilde{\mathrm{O}}(nk^2)$ upper bound on the pseudo-dimension ($\tilde{\mathrm{O}}$ compresses logarithmic factors) and complement it by an $\Omega(nk)$ lower bound, hence 
&lt;/p&gt;</description></item><item><title>TransGNN&#26159;&#19968;&#31181;&#23558;Transformer&#21644;GNN&#23618;&#20132;&#26367;&#32467;&#21512;&#20197;&#30456;&#20114;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#24863;&#21463;&#22495;&#26377;&#38480;&#21644;&#23384;&#22312;&#22122;&#38899;&#36830;&#25509;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.14355</link><description>&lt;p&gt;
TransGNN: &#21033;&#29992;Transformer&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21327;&#21516;&#33021;&#21147;&#26469;&#20570;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
TransGNN: Harnessing the Collaborative Power of Transformers and Graph Neural Networks for Recommender Systems. (arXiv:2308.14355v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14355
&lt;/p&gt;
&lt;p&gt;
TransGNN&#26159;&#19968;&#31181;&#23558;Transformer&#21644;GNN&#23618;&#20132;&#26367;&#32467;&#21512;&#20197;&#30456;&#20114;&#22686;&#24378;&#20854;&#33021;&#21147;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#24863;&#21463;&#22495;&#26377;&#38480;&#21644;&#23384;&#22312;&#22122;&#38899;&#36830;&#25509;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23545;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#22270;&#36827;&#34892;&#24314;&#27169;&#26469;&#36827;&#34892;&#21327;&#21516;&#36807;&#28388;(CF)&#12290;&#29616;&#26377;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#22312;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#36793;&#19978;&#36827;&#34892;&#36882;&#24402;&#28040;&#24687;&#20256;&#36882;&#26469;&#25913;&#36827;&#32534;&#30721;&#23884;&#20837;&#12290;&#23613;&#31649;&#23427;&#20204;&#24050;&#32463;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#26159;&#24403;&#21069;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#24863;&#21463;&#22495;&#21644;&#23384;&#22312;&#22122;&#38899; "&#20852;&#36259;&#26080;&#20851;" &#36830;&#25509;&#30340;&#25361;&#25112;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#33258;&#36866;&#24212;&#21644;&#20840;&#23616;&#20449;&#24687;&#32858;&#21512;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25429;&#25417;&#22797;&#26434;&#12289;&#32416;&#32544;&#30340;&#32467;&#26500;&#20449;&#24687;&#26041;&#38754;&#22312;&#22823;&#35268;&#27169;&#20132;&#20114;&#22270;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransGNN&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20132;&#26367;&#22320;&#32467;&#21512;Transformer&#21644;GNN&#23618;&#26469;&#30456;&#20114;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as promising solutions for collaborative filtering (CF) through the modeling of user-item interaction graphs. The nucleus of existing GNN-based recommender systems involves recursive message passing along user-item interaction edges to refine encoded embeddings. Despite their demonstrated effectiveness, current GNN-based methods encounter challenges of limited receptive fields and the presence of noisy ``interest-irrelevant'' connections. In contrast, Transformer-based methods excel in aggregating information adaptively and globally. Nevertheless, their application to large-scale interaction graphs is hindered by inherent complexities and challenges in capturing intricate, entangled structural information. In this paper, we propose TransGNN, a novel model that integrates Transformer and GNN layers in an alternating fashion to mutually enhance their capabilities. Specifically, TransGNN leverages Transformer layers to broaden the receptive field 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20999;&#29255;Wasserstein&#25439;&#22833;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;SGD&#36712;&#36857;&#36924;&#36817;&#20102;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2307.11714</link><description>&lt;p&gt;
&#20351;&#29992;&#20999;&#29255;Wasserstein&#25439;&#22833;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;SGD&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence of SGD for Training Neural Networks with Sliced Wasserstein Losses. (arXiv:2307.11714v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20999;&#29255;Wasserstein&#25439;&#22833;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;SGD&#36712;&#36857;&#36924;&#36817;&#20102;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#36817;&#24180;&#26469;&#24341;&#21457;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;Wasserstein&#36317;&#31163;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#20960;&#20309;&#19978;&#21512;&#29702;&#21644;&#30452;&#35266;&#30340;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#30340;&#26041;&#27861;&#12290;&#20986;&#20110;&#35745;&#31639;&#21407;&#22240;&#65292;&#20999;&#29255;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#20316;&#20026;Wasserstein&#36317;&#31163;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#34987;&#24341;&#20837;&#65292;&#24182;&#19988;&#24050;&#32463;&#34987;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#24335;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#12290;&#34429;&#28982;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#20013;&#23454;&#38469;&#35266;&#23519;&#21040;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23545;&#20110;&#36825;&#19968;&#35266;&#23519;&#27809;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#20511;&#37492;Bianchi&#31561;&#20154;&#65288;2022&#65289;&#20851;&#20110;SGD&#22312;&#38750;&#20809;&#28369;&#21644;&#38750;&#20984;&#20989;&#25968;&#19978;&#25910;&#25947;&#24615;&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#30340;&#19978;&#19979;&#25991;&#65292;&#20351;&#24471;SW&#25439;&#22833;&#23545;NN&#21442;&#25968;&#30340;&#22266;&#23450;&#27493;&#38271;SGD&#36712;&#36857;&#25910;&#25947;&#21040;&#65288;&#27425;&#65289;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#38598;&#21512;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#30528;&#27493;&#38271;&#20943;&#23567;&#65292;&#36825;&#20123;&#36712;&#36857;&#36924;&#36817;&#20102;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport has sparked vivid interest in recent years, in particular thanks to the Wasserstein distance, which provides a geometrically sensible and intuitive way of comparing probability measures. For computational reasons, the Sliced Wasserstein (SW) distance was introduced as an alternative to the Wasserstein distance, and has seen uses for training generative Neural Networks (NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed practically in such a setting, there is to our knowledge no theoretical guarantee for this observation. Leveraging recent works on convergence of SGD on non-smooth and non-convex functions by Bianchi et al. (2022), we aim to bridge that knowledge gap, and provide a realistic context under which fixed-step SGD trajectories for the SW loss on NN parameters converge. More precisely, we show that the trajectories approach the set of (sub)-gradient flow equations as the step decreases. Under stricter assumptions, we show a much st
&lt;/p&gt;</description></item><item><title>ENN&#26159;&#19968;&#31181;&#20855;&#26377;DCT&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#28608;&#27963;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#39640;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#22312;&#35299;&#37322;&#32593;&#32476;&#25910;&#25947;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#27599;&#20010;&#28608;&#27963;&#20989;&#25968;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#21709;&#24212;&#65292;&#21363;&#8220;&#20984;&#36215;&#8221;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.00673</link><description>&lt;p&gt;
ENN: &#19968;&#31181;&#20855;&#26377;DCT&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ENN: A Neural Network with DCT Adaptive Activation Functions. (arXiv:2307.00673v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00673
&lt;/p&gt;
&lt;p&gt;
ENN&#26159;&#19968;&#31181;&#20855;&#26377;DCT&#33258;&#36866;&#24212;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#28608;&#27963;&#20989;&#25968;&#65292;&#25552;&#20379;&#20102;&#39640;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#22312;&#35299;&#37322;&#32593;&#32476;&#25910;&#25947;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#27599;&#20010;&#28608;&#27963;&#20989;&#25968;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#21709;&#24212;&#65292;&#21363;&#8220;&#20984;&#36215;&#8221;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#39640;&#24230;&#21462;&#20915;&#20110;&#28608;&#27963;&#20989;&#25968;&#30340;&#24615;&#36136;&#65292;&#23613;&#31649;&#36825;&#20123;&#36890;&#24120;&#22312;&#35757;&#32451;&#38454;&#27573;&#34987;&#20551;&#23450;&#20026;&#39044;&#23450;&#20041;&#21644;&#22266;&#23450;&#30340;&#12290;&#22312;&#20449;&#21495;&#22788;&#29702;&#30340;&#35270;&#35282;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#8212;&#8212;&#34920;&#36798;&#31070;&#32463;&#32593;&#32476;(ENN)&#65292;&#20854;&#20013;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#20351;&#29992;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;(DCT)&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#36825;&#31181;&#21442;&#25968;&#21270;&#26041;&#27861;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#20445;&#25345;&#36739;&#20302;&#65292;&#36866;&#21512;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#26696;&#65292;&#24182;&#33021;&#36866;&#24212;&#19981;&#21516;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#28608;&#27963;&#20989;&#25968;&#26041;&#38754;&#20381;&#36182;&#20110;&#20449;&#21495;&#22788;&#29702;&#35270;&#35282;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#65292;&#20026;&#32593;&#32476;&#25552;&#20379;&#20102;&#39640;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#8220;&#20984;&#36215;&#8221;&#30340;&#27010;&#24565;&#26469;&#20026;&#32593;&#32476;&#22312;&#25910;&#25947;&#26102;&#30340;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#21363;&#27599;&#20010;&#28608;&#27963;&#20989;&#25968;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#21709;&#24212;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35814;&#23613;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The expressiveness of neural networks highly depends on the nature of the activation function, although these are usually assumed predefined and fixed during the training stage. Under a signal processing perspective, in this paper we present Expressive Neural Network (ENN), a novel model in which the non-linear activation functions are modeled using the Discrete Cosine Transform (DCT) and adapted using backpropagation during training. This parametrization keeps the number of trainable parameters low, is appropriate for gradient-based schemes, and adapts to different learning tasks. This is the first non-linear model for activation functions that relies on a signal processing perspective, providing high flexibility and expressiveness to the network. We contribute with insights in the explainability of the network at convergence by recovering the concept of bump, this is, the response of each activation function in the output space. Finally, through exhaustive experiments we show that th
&lt;/p&gt;</description></item><item><title>UTrans&#26159;&#19968;&#31181;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#33021;&#26816;&#27979;&#21487;&#36716;&#31227;&#21464;&#37327;&#21644;&#28304;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.00238</link><description>&lt;p&gt;
&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#30340;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unified Transfer Learning Models for High-Dimensional Linear Regression. (arXiv:2307.00238v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00238
&lt;/p&gt;
&lt;p&gt;
UTrans&#26159;&#19968;&#31181;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#33021;&#26816;&#27979;&#21487;&#36716;&#31227;&#21464;&#37327;&#21644;&#28304;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#24403;&#30446;&#26631;&#25968;&#25454;&#31232;&#32570;&#32780;&#28304;&#25968;&#25454;&#20805;&#36275;&#65292;&#25110;&#32773;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#36716;&#31227;&#23398;&#20064;&#22312;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#32479;&#19968;&#36716;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;UTrans&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#21487;&#36716;&#31227;&#21464;&#37327;&#21644;&#28304;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20272;&#35745;&#35823;&#24046;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#30028;&#38480;&#20302;&#20110;&#20165;&#26377;&#30446;&#26631;&#25968;&#25454;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#28304;&#25968;&#25454;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#25490;&#38500;&#19981;&#21487;&#36716;&#31227;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;UTrans&#19982;&#29616;&#26377;&#31639;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;UTrans&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#21516;&#26102;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#20272;&#35745;&#21644;&#39044;&#27979;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#32654;&#22269;&#20195;&#38469;&#27969;&#21160;&#25968;&#25454;&#65292;&#24182;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#19982;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning plays a key role in modern data analysis when: (1) the target data are scarce but the source data are sufficient; (2) the distributions of the source and target data are heterogeneous. This paper develops an interpretable unified transfer learning model, termed as UTrans, which can detect both transferable variables and source data. More specifically, we establish the estimation error bounds and prove that our bounds are lower than those with target data only. Besides, we propose a source detection algorithm based on hypothesis testing to exclude the nontransferable data. We evaluate and compare UTrans to the existing algorithms in multiple experiments. It is shown that UTrans attains much lower estimation and prediction errors than the existing methods, while preserving interpretability. We finally apply it to the US intergenerational mobility data and compare our proposed algorithms to the classical machine learning algorithms.
&lt;/p&gt;</description></item><item><title>MILD&#27169;&#22411;&#21270;&#20102;&#23398;&#20064;&#21160;&#24577;&#65292;&#36890;&#36807;&#22522;&#20110;Weibull&#28151;&#21512;&#27169;&#22411;&#30340;&#36845;&#20195;&#36873;&#25321;&#26041;&#27861;&#65292;&#35782;&#21035;&#24178;&#20928;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#20197;&#20943;&#23569;&#32593;&#32476;&#23545;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#30340;&#35760;&#24518;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.11560</link><description>&lt;p&gt;
MILD: &#27169;&#22411;&#21270;&#23398;&#20064;&#21160;&#24577;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
MILD: Modeling the Instance Learning Dynamics for Learning with Noisy Labels. (arXiv:2306.11560v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11560
&lt;/p&gt;
&lt;p&gt;
MILD&#27169;&#22411;&#21270;&#20102;&#23398;&#20064;&#21160;&#24577;&#65292;&#36890;&#36807;&#22522;&#20110;Weibull&#28151;&#21512;&#27169;&#22411;&#30340;&#36845;&#20195;&#36873;&#25321;&#26041;&#27861;&#65292;&#35782;&#21035;&#24178;&#20928;&#30340;&#25968;&#25454;&#23454;&#20363;&#65292;&#20197;&#20943;&#23569;&#32593;&#32476;&#23545;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#30340;&#35760;&#24518;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#24102;&#26377;&#20934;&#30830;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#30340;&#25910;&#38598;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#38477;&#20302;&#25104;&#26412;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#21521;&#26159;&#23398;&#20064;&#24102;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#20943;&#23569;&#32593;&#32476;&#23545;&#38169;&#35823;&#26631;&#31614;&#25968;&#25454;&#30340;&#35760;&#24518;&#25928;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Weibull&#28151;&#21512;&#27169;&#22411;&#30340;&#36845;&#20195;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#30340;&#25972;&#20307;&#23398;&#20064;&#21160;&#24577;&#26469;&#35782;&#21035;&#24178;&#20928;&#30340;&#25968;&#25454;&#12290;&#19982;&#20808;&#21069;&#30340;&#23567;&#25439;&#22833;&#21551;&#21457;&#24335;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#28145;&#24230;&#32593;&#32476;&#23481;&#26131;&#35760;&#24518;&#21644;&#38590;&#20197;&#24536;&#35760;&#24178;&#20928;&#25968;&#25454;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#26399;&#38388;&#34987;&#38169;&#35823;&#20998;&#31867;&#21644;&#34987;&#35760;&#24518;&#20043;&#38388;&#30340;&#36716;&#25442;&#27425;&#25968;&#26469;&#27979;&#37327;&#27599;&#20010;&#23454;&#20363;&#30340;&#35760;&#24518;&#21644;&#36951;&#24536;&#38590;&#24230;&#65292;&#24182;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#19968;&#20010;&#26032;&#30340;&#36873;&#25321;&#25351;&#26631;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite deep learning has achieved great success, it often relies on a large amount of training data with accurate labels, which are expensive and time-consuming to collect. A prominent direction to reduce the cost is to learn with noisy labels, which are ubiquitous in the real-world applications. A critical challenge for such a learning task is to reduce the effect of network memorization on the falsely-labeled data. In this work, we propose an iterative selection approach based on the Weibull mixture model, which identifies clean data by considering the overall learning dynamics of each data instance. In contrast to the previous small-loss heuristics, we leverage the observation that deep network is easy to memorize and hard to forget clean data. In particular, we measure the difficulty of memorization and forgetting for each instance via the transition times between being misclassified and being memorized in training, and integrate them into a novel metric for selection. Based on th
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23835;&#36215;&#24341;&#21457;&#20102;&#29256;&#26435;&#21644;&#21019;&#26032;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;&#24314;&#35758;&#23545;&#24320;&#28304;&#20195;&#30721;&#35768;&#21487;&#35777;&#36827;&#34892;&#26356;&#25913;&#65292;&#38480;&#21046;AI&#31995;&#32479;&#23545;&#20195;&#30721;&#30340;&#35775;&#38382;&#21644;&#20351;&#29992;&#65292;&#24182;&#25506;&#35752;&#19982;AI&#21644;&#29256;&#26435;&#20043;&#38388;&#30340;&#20851;&#31995;&#24341;&#21457;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09267</link><description>&lt;p&gt;
ChatGPT&#21644;&#20854;&#20182;&#31867;&#20284;&#31995;&#32479;&#26159;AI&#30340;&#29616;&#20195;&#21202;&#32435;&#24681;&#20061;&#22836;&#34503;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?. (arXiv:2306.09267v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09267
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23835;&#36215;&#24341;&#21457;&#20102;&#29256;&#26435;&#21644;&#21019;&#26032;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;&#24314;&#35758;&#23545;&#24320;&#28304;&#20195;&#30721;&#35768;&#21487;&#35777;&#36827;&#34892;&#26356;&#25913;&#65292;&#38480;&#21046;AI&#31995;&#32479;&#23545;&#20195;&#30721;&#30340;&#35775;&#38382;&#21644;&#20351;&#29992;&#65292;&#24182;&#25506;&#35752;&#19982;AI&#21644;&#29256;&#26435;&#20043;&#38388;&#30340;&#20851;&#31995;&#24341;&#21457;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23835;&#36215;&#24341;&#21457;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#31038;&#20132;&#21442;&#19982;&#12290;AI&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#36890;&#36807;&#35775;&#38382;&#36807;&#21435;&#20960;&#21313;&#24180;&#24320;&#21457;&#20154;&#21592;&#21019;&#24314;&#30340;&#22823;&#37327;&#24320;&#28304;&#20195;&#30721;&#26469;&#25552;&#20379;&#22238;&#31572;&#25110;&#21709;&#24212;&#38382;&#39064;&#25110;&#35831;&#27714;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#34987;&#25351;&#25511;&#31363;&#21462;&#23384;&#20648;&#22312;&#34394;&#25311;&#24211;&#20013;&#30340;&#24320;&#28304;&#20195;&#30721;&#12290;&#26412;&#25991;&#30528;&#37325;&#35752;&#35770;&#27492;&#31867;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#26159;&#21542;&#26377;&#35299;&#20915;&#26041;&#26696;&#26469;&#20445;&#25252;&#21019;&#26032;&#24182;&#36991;&#20813;&#22810;&#24180;&#30340;&#35785;&#35772;&#12290;&#23545;AI&#21644;&#29256;&#26435;&#20043;&#38388;&#30340;&#20851;&#31995;&#24341;&#21457;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#20063;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#19979;&#24314;&#35758;&#65306;(a)&#23545;&#24320;&#21457;&#20154;&#21592;&#21019;&#24314;&#30340;&#24320;&#28304;&#20195;&#30721;&#30340;&#35768;&#21487;&#35777;&#36827;&#34892;&#21363;&#26102;&#26356;&#25913;&#65292;&#38480;&#21046;&#23545;&#20219;&#20309;&#24320;&#28304;&#20195;&#30721;&#30340;&#35775;&#38382;&#21644;/&#25110;&#20351;&#29992;&#20165;&#38480;&#20110;&#20154;&#31867;&#65307;(b)&#24314;&#35758;&#23545;&#40635;&#30465;&#29702;&#24037;&#23398;&#38498;&#65288;MIT&#65289;&#35768;&#21487;&#35777;&#36827;&#34892;&#20462;&#35746;&#65292;&#35201;&#27714;AI&#31995;&#32479;&#20174;&#24320;&#28304;&#20195;&#30721;&#37096;&#20998;&#33719;&#21462;&#36866;&#24403;&#30340;&#35768;&#21487;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of Generative Artificial Intelligence systems ("AI systems") has created unprecedented social engagement. AI code generation systems provide responses (output) to questions or requests by accessing the vast library of open-source code created by developers over the past few decades. However, they do so by allegedly stealing the open-source code stored in virtual libraries, known as repositories. This Article focuses on how this happens and whether there is a solution that protects innovation and avoids years of litigation. We also touch upon the array of issues raised by the relationship between AI and copyright. Looking ahead, we propose the following: (a) immediate changes to the licenses for open-source code created by developers that will limit access and/or use of any open-source code to humans only; (b) we suggest revisions to the Massachusetts Institute of Technology ("MIT") license so that AI systems are required to procure appropriate licenses from open-source code de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#22270;&#20998;&#24067;&#30340;&#32622;&#25442;&#31561;&#21464;&#31163;&#25955;&#33258;&#32534;&#30721;&#22120;(VQ-GAE)&#65292;&#23427;&#37319;&#29992;&#21521;&#37327;&#37327;&#21270;&#38450;&#27490;&#23558;&#31163;&#25955;&#23545;&#35937;&#26144;&#23556;&#21040;&#36830;&#32493;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#25429;&#33719;&#20102;&#22270;&#30340;&#20840;&#23616;&#32467;&#26500;&#65292;&#23454;&#39564;&#34920;&#26126;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07735</link><description>&lt;p&gt;
&#21521;&#37327;&#37327;&#21270;&#22270;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Vector-Quantized Graph Auto-Encoder. (arXiv:2306.07735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#22270;&#20998;&#24067;&#30340;&#32622;&#25442;&#31561;&#21464;&#31163;&#25955;&#33258;&#32534;&#30721;&#22120;(VQ-GAE)&#65292;&#23427;&#37319;&#29992;&#21521;&#37327;&#37327;&#21270;&#38450;&#27490;&#23558;&#31163;&#25955;&#23545;&#35937;&#26144;&#23556;&#21040;&#36830;&#32493;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#25429;&#33719;&#20102;&#22270;&#30340;&#20840;&#23616;&#32467;&#26500;&#65292;&#23454;&#39564;&#34920;&#26126;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24314;&#27169;&#22270;&#30340;&#20998;&#24067;&#30340;&#38382;&#39064;&#12290;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#32622;&#25442;&#31561;&#21464;&#31163;&#25955;&#33258;&#32534;&#30721;&#22120;(VQ-GAE)&#65292;&#26088;&#22312;&#35774;&#35745;&#29992;&#20110;&#24314;&#27169;&#22270;&#30340;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#32622;&#25442;&#31561;&#21464;&#24615;&#65292;&#25105;&#20204;&#30340;&#33258;&#32534;&#30721;&#22120;&#32469;&#36807;&#20102;&#22270;&#34920;&#31034;&#30340;&#25490;&#24207;&#38382;&#39064;&#12290;&#25105;&#20204;&#21033;&#29992;GNN&#25429;&#25417;&#22270;&#30340;&#23616;&#37096;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#37319;&#29992;&#21521;&#37327;&#37327;&#21270;&#26469;&#38450;&#27490;&#23558;&#31163;&#25955;&#23545;&#35937;&#26144;&#23556;&#21040;&#36830;&#32493;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#27492;&#22806;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#20351;&#29992;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#28508;&#22312;&#34920;&#31034;&#25429;&#33719;&#22270;&#30340;&#20840;&#23616;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#29992;&#20110;&#22270;&#29983;&#25104;&#30340;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19968;&#20123;&#26368;&#31361;&#20986;&#30340;&#35780;&#20272;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we addresses the problem of modeling distributions of graphs. We introduce the Vector-Quantized Graph Auto-Encoder (VQ-GAE), a permutation-equivariant discrete auto-encoder and designed to model the distribution of graphs. By exploiting the permutation-equivariance of graph neural networks (GNNs), our autoencoder circumvents the problem of the ordering of the graph representation. We leverage the capability of GNNs to capture local structures of graphs while employing vector-quantization to prevent the mapping of discrete objects to a continuous latent space. Furthermore, the use of autoregressive models enables us to capture the global structure of graphs via the latent representation. We evaluate our model on standard datasets used for graph generation and observe that it achieves excellent performance on some of the most salient evaluation metrics compared to the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MusicGen&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05284</link><description>&lt;p&gt;
&#31616;&#21333;&#19988;&#21487;&#25511;&#30340;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Simple and Controllable Music Generation. (arXiv:2306.05284v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MusicGen&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#26465;&#20214;&#38899;&#20048;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MusicGen&#65292;&#23427;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#25805;&#20316;&#22810;&#20010;&#21387;&#32553;&#31163;&#25955;&#38899;&#20048;&#34920;&#31034;&#27969;&#65292;&#21363;&#20196;&#29260;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;MusicGen&#30001;&#19968;&#20010;&#21333;&#19968;&#38454;&#27573;&#30340;Transformer LM&#21644;&#39640;&#25928;&#30340;&#20196;&#29260;&#20132;&#38169;&#27169;&#24335;&#32452;&#25104;&#65292;&#28040;&#38500;&#20102;&#32423;&#32852;&#22810;&#20010;&#27169;&#22411;&#30340;&#38656;&#35201;&#65292;&#20363;&#22914;&#20998;&#23618;&#25110;&#19978;&#37319;&#26679;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MusicGen&#22914;&#20309;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#30340;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#33258;&#21160;&#21644;&#20154;&#20026;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#26631;&#20934;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#35780;&#20272;&#30340;&#22522;&#32447;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;MusicGen&#25152;&#21253;&#21547;&#32452;&#20214;&#30340;&#37325;&#35201;&#24615;&#12290;&#38899;&#20048;&#26679;&#26412;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312;https://github.com/fac&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/fac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BABE&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30450;&#38899;&#39057;&#24102;&#23485;&#25193;&#23637;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;&#30450;&#24102;&#23485;&#25193;&#23637;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01433</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#30450;&#38899;&#39057;&#24102;&#23485;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Blind Audio Bandwidth Extension. (arXiv:2306.01433v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BABE&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30450;&#38899;&#39057;&#24102;&#23485;&#25193;&#23637;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;&#30450;&#24102;&#23485;&#25193;&#23637;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#24102;&#23485;&#25193;&#23637;&#28041;&#21450;&#20174;&#24102;&#23485;&#21463;&#38480;&#30340;&#35266;&#27979;&#20449;&#21495;&#20013;&#23454;&#29616;&#39640;&#39057;&#35889;&#30340;&#36924;&#30495;&#37325;&#24314;&#12290;&#22312;&#20302;&#36890;&#20449;&#21495;&#36864;&#21270;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#22914;&#23545;&#21382;&#21490;&#38899;&#39057;&#35760;&#24405;&#30340;&#24674;&#22797;&#65292;&#36825;&#20415;&#25104;&#20102;&#19968;&#20010;&#30450;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BABE&#30340;&#26032;&#26041;&#27861;&#65288;Blind Audio Bandwidth Extension&#65289;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#30450;&#38382;&#39064;&#65292;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#26080;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#20808;&#39564;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;BABE&#21033;&#29992;&#20102;&#19968;&#20010;&#24191;&#20041;&#29256;&#26412;&#30340;&#25193;&#25955;&#21518;&#39564;&#37319;&#26679;&#65292;&#22312;&#20854;&#20013;&#36864;&#21270;&#31639;&#23376;&#26159;&#26410;&#30693;&#30340;&#65292;&#20294;&#26159;&#36890;&#36807;&#36845;&#20195;&#36827;&#34892;&#21442;&#25968;&#21270;&#21644;&#25512;&#26029;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#26159;&#29992;&#23458;&#35266;&#21644;&#20027;&#35266;&#25351;&#26631;&#35780;&#20272;&#30340;&#65292;&#32467;&#26524;&#34920;&#26126;BABE&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#30450;&#24102;&#23485;&#25193;&#23637;&#22522;&#32447;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#21512;&#25104;&#25968;&#25454;&#26102;&#19982;&#38750;&#30450;&#28388;&#27874;&#22120;&#30693;&#24773;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;BABE&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio bandwidth extension involves the realistic reconstruction of high-frequency spectra from bandlimited observations. In cases where the lowpass degradation is unknown, such as in restoring historical audio recordings, this becomes a blind problem. This paper introduces a novel method called BABE (Blind Audio Bandwidth Extension) that addresses the blind problem in a zero-shot setting, leveraging the generative priors of a pre-trained unconditional diffusion model. During the inference process, BABE utilizes a generalized version of diffusion posterior sampling, where the degradation operator is unknown but parametrized and inferred iteratively. The performance of the proposed method is evaluated using objective and subjective metrics, and the results show that BABE surpasses state-of-the-art blind bandwidth extension baselines and achieves competitive performance compared to non-blind filter-informed methods when tested with synthetic data. Moreover, BABE exhibits robust generaliza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoWG&#30340;&#26080;&#21442;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#26082;&#39640;&#25928;&#21448;&#36890;&#29992;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#20110;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#38656;&#22238;&#28335;&#25628;&#32034;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.16284</link><description>&lt;p&gt;
DoWG&#23637;&#31034;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#36890;&#29992;&#26080;&#21442;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method. (arXiv:2305.16284v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DoWG&#30340;&#26080;&#21442;&#25968;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#26082;&#39640;&#25928;&#21448;&#36890;&#29992;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#20110;&#24179;&#31283;&#21644;&#38750;&#24179;&#31283;&#38382;&#39064;&#65292;&#24182;&#19988;&#26080;&#38656;&#22238;&#28335;&#25628;&#32034;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26131;&#20110;&#23454;&#29616;&#30340;&#26080;&#21442;&#25968;&#26799;&#24230;&#20248;&#21270;&#22120;&#65306;DoWG&#65288;Weighted Gradients&#30340;&#36317;&#31163;&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#26159;&#39640;&#25928;&#30340;&#8212;&#8212;&#22312;&#19981;&#35843;&#25972;&#20219;&#20309;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#21305;&#37197;&#20248;&#21270;&#20984;&#20248;&#21270;&#20013;&#26368;&#20248;&#35843;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#30452;&#21040;&#23545;&#25968;&#22240;&#23376;&#65292;&#24182;&#19988;&#26159;&#36890;&#29992;&#30340;&#8212;&#8212;&#33258;&#21160;&#36866;&#24212;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#38382;&#39064;&#12290;&#19982;AdaGrad&#65292;Adam&#25110;DoG&#31561;&#27969;&#34892;&#31639;&#27861;&#35745;&#31639;&#24179;&#26041;&#26799;&#24230;&#30340;&#36816;&#34892;&#24179;&#22343;&#20540;&#19981;&#21516;&#65292;DoWG&#20445;&#25345;&#36816;&#34892;&#24179;&#22343;&#20540;&#30340;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#21152;&#26435;&#29256;&#26412;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;&#25152;&#38656;&#30340;&#24615;&#36136;&#33267;&#20851;&#37325;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;DoWG&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#22238;&#28335;&#25628;&#32034;&#36807;&#31243;&#30340;&#26080;&#21442;&#25968;&#65292;&#39640;&#25928;&#21644;&#36890;&#29992;&#31639;&#27861;&#12290;&#23427;&#36824;&#26159;&#31532;&#19968;&#20010;&#36866;&#24212;&#20110;&#24179;&#31283;&#20248;&#21270;&#30340;&#26080;&#21442;&#25968;AdaGrad&#26679;&#24335;&#31639;&#27861;&#12290;&#20026;&#20102;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;DoWG&#22312;&#31283;&#23450;&#30340;&#36793;&#32536;&#35757;&#32451;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new easy-to-implement parameter-free gradient-based optimizer: DoWG (Distance over Weighted Gradients). We prove that DoWG is efficient -- matching the convergence rate of optimally tuned gradient descent in convex optimization up to a logarithmic factor without tuning any parameters, and universal -- automatically adapting to both smooth and nonsmooth problems. While popular algorithms such as AdaGrad, Adam, or DoG compute a running average of the squared gradients, DoWG maintains a new distance-based weighted version of the running average, which is crucial to achieve the desired properties. To our best knowledge, DoWG is the first parameter-free, efficient, and universal algorithm that does not require backtracking search procedures. It is also the first parameter-free AdaGrad style algorithm that adapts to smooth optimization. To complement our theory, we also show empirically that DoWG trains at the edge of stability, and validate its effectiveness on practic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27714;&#35299;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#35299;&#20915;&#25193;&#25955;ODE&#38382;&#39064;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#31283;&#23450;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#37319;&#26679;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.15357</link><description>&lt;p&gt;
&#36890;&#36807;&#27714;&#35299;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#35299;&#20915;&#25193;&#25955;ODE&#38382;&#39064;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution. (arXiv:2305.15357v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27714;&#35299;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#35299;&#20915;&#25193;&#25955;ODE&#38382;&#39064;&#30340;&#26377;&#25928;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#31283;&#23450;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#37319;&#26679;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;&#21453;&#21521;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#38543;&#26426;&#24615;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#22312;&#27599;&#27425;&#37319;&#26679;&#26102;&#24615;&#33021;&#27874;&#21160;&#24456;&#22823;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#23569;&#37327;&#37325;&#26032;&#37319;&#26679;&#27493;&#39588;&#30340;&#37319;&#26679;&#22120;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#36825;&#31181;&#22266;&#26377;&#38543;&#26426;&#24615;&#23548;&#33268;&#20854;&#26080;&#25928;&#21644;&#19981;&#31283;&#23450;&#65292;&#20351;&#29992;&#25143;&#38590;&#20197;&#20445;&#35777;&#36229;&#20998;&#36776;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#36825;&#31181;&#38543;&#26426;&#24615;&#35270;&#20026;&#19968;&#31181;&#26426;&#36935;&#65306;&#20840;&#38754;&#20998;&#26512;&#21644;&#21033;&#29992;&#23427;&#23548;&#33268;&#20102;&#26500;&#24314;&#19968;&#31181;&#26377;&#25928;&#30340;&#21363;&#25554;&#21363;&#29992;&#37319;&#26679;&#26041;&#27861;&#65292;&#20855;&#26377;&#28508;&#21147;&#20351;&#19968;&#31995;&#21015;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#21463;&#30410;&#12290;&#26356;&#35814;&#32454;&#22320;&#35828;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#27714;&#35299;&#25193;&#25955;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;&#25193;&#25955;ODE&#65289;&#21644;&#26368;&#20248;&#36793;&#30028;&#26465;&#20214;&#65288;BC&#65289;&#65292;&#31283;&#23450;&#22320;&#20174;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#20013;&#37319;&#26679;&#39640;&#36136;&#37327;&#30340;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#24182;&#20998;&#26512;&#20854;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models, as a kind of powerful generative model, have given impressive results on image super-resolution (SR) tasks. However, due to the randomness introduced in the reverse process of diffusion models, the performances of diffusion-based SR models are fluctuating at every time of sampling, especially for samplers with few resampled steps. This inherent randomness of diffusion models results in ineffectiveness and instability, making it challenging for users to guarantee the quality of SR results. However, our work takes this randomness as an opportunity: fully analyzing and leveraging it leads to the construction of an effective plug-and-play sampling method that owns the potential to benefit a series of diffusion-based SR methods. More in detail, we propose to steadily sample high-quality SR images from pretrained diffusion-based SR models by solving diffusion ordinary differential equations (diffusion ODEs) with optimal boundary conditions (BCs) and analyze the characterist
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#26041;&#27861;&#19982;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;QD-RL&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#21534;&#21520;&#37327;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#19979;&#35757;&#32451;&#33021;&#22815;&#22312;&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.13795</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36817;&#31471;&#31574;&#30053;&#26799;&#24230;&#26641;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning. (arXiv:2305.13795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#26041;&#27861;&#19982;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;QD-RL&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#21534;&#21520;&#37327;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#19979;&#35757;&#32451;&#33021;&#22815;&#22312;&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22521;&#35757;&#36890;&#24120;&#33021;&#22815;&#22312;&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26234;&#33021;&#20307;&#26159;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#36136;&#37327;&#22810;&#26679;&#24615;&#24378;&#21270;&#23398;&#20064;(QD-RL)&#26159;&#19968;&#31867;&#26032;&#20852;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#21644;RL&#30340;&#35265;&#35299;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#19968;&#31995;&#21015;&#20851;&#20110;&#34892;&#20026;&#23884;&#20837;&#30340;&#39640;&#24615;&#33021;&#21644;&#34892;&#20026;&#22810;&#26679;&#24615;&#30340;&#31574;&#30053;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;QD-RL&#26041;&#27861;&#36804;&#20170;&#20026;&#27490;&#21033;&#29992;&#20102;&#26679;&#26412;&#26377;&#25928;&#30340;&#31163;&#31574;&#30053;RL&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#39640;&#21534;&#21520;&#37327;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#30340;&#36827;&#27493;&#24050;&#32463;&#25171;&#24320;&#20102;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#31639;&#27861;&#30340;&#22823;&#38376;&#65292;&#32780;&#23558;&#29616;&#26377;&#30340;&#31163;&#31574;&#30053;QD-RL&#26041;&#27861;&#25193;&#23637;&#21040;&#36825;&#20123;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#37319;&#29992;&#20102;&#33021;&#22815;&#21033;&#29992;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#31561;&#31574;&#30053;&#26041;&#27861;&#19982;QD&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QD-RL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generally capable agents that perform well in unseen dynamic environments is a long-term goal of robot learning. Quality Diversity Reinforcement Learning (QD-RL) is an emerging class of reinforcement learning (RL) algorithms that blend insights from Quality Diversity (QD) and RL to produce a collection of high performing and behaviorally diverse policies with respect to a behavioral embedding. Existing QD-RL approaches have thus far taken advantage of sample-efficient off-policy RL algorithms. However, recent advances in high-throughput, massively parallelized robotic simulators have opened the door for algorithms that can take advantage of such parallelism, and it is unclear how to scale existing off-policy QD-RL methods to these new data-rich regimes. In this work, we take the first steps to combine on-policy RL methods, specifically Proximal Policy Optimization (PPO), that can leverage massive parallelism, with QD, and propose a new QD-RL method with these high-throughput s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#32463;&#39564;&#24615;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#26368;&#22810;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;Spoken&#29256;&#26412;&#30340;&#25991;&#26412;&#22522;&#20934;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#27169;&#22411;&#35780;&#20272;&#21644;&#25512;&#21160;&#26410;&#26469;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.13009</link><description>&lt;p&gt;
&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Textually Pretrained Speech Language Models. (arXiv:2305.13009v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#32463;&#39564;&#24615;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#26368;&#22810;&#30340;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;Spoken&#29256;&#26412;&#30340;&#25991;&#26412;&#22522;&#20934;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#27169;&#22411;&#35780;&#20272;&#21644;&#25512;&#21160;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#65288;SpeechLMs&#65289;&#20165;&#22788;&#29702;&#21644;&#29983;&#25104;&#38899;&#39057;&#25968;&#25454;&#65292;&#27809;&#26377;&#25991;&#23383;&#30417;&#30563;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TWIST&#65292;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;SpeechLMs&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;TWIST&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#20248;&#20110;&#20919;&#21551;&#21160;&#30340;SpeechLM&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#35774;&#35745;&#36873;&#25321;&#65288;&#22914;&#35821;&#38899;&#20998;&#35789;&#22120;&#12289;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#22312;&#26500;&#24314;&#24615;&#33021;&#26356;&#22909;&#30340;SpeechLMs&#26041;&#38754;&#37117;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36804;&#20170;&#20026;&#27490;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#26368;&#22810;&#30340;SpeechLM&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;Spoken&#29256;&#26412;&#30340;StoryCloze&#25991;&#26412;&#22522;&#20934;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#27169;&#22411;&#35780;&#20272;&#24182;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;&#25105;&#20204;&#20844;&#24320;&#25552;&#20379;&#35821;&#38899;&#26679;&#26412;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#65306;https://pages.cs.huji.ac.il/
&lt;/p&gt;
&lt;p&gt;
Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21024;&#38500;&#26631;&#20934;&#26469;&#35780;&#20272;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#65292;&#35813;&#26041;&#27861;&#38543;&#26426;&#36974;&#30422;&#26631;&#35760;&#30340;&#37096;&#20998;&#21521;&#37327;&#34920;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#30828;&#21024;&#38500;&#26631;&#20934;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.10496</link><description>&lt;p&gt;
&#34701;&#21512;&#24402;&#22240;&#37325;&#35201;&#24615;&#20197;&#25552;&#39640;&#24544;&#23454;&#24230;&#35780;&#20272;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incorporating Attribution Importance for Improving Faithfulness Metrics. (arXiv:2305.10496v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36719;&#21024;&#38500;&#26631;&#20934;&#26469;&#35780;&#20272;&#24402;&#22240;&#26041;&#27861;&#30340;&#24544;&#23454;&#24230;&#65292;&#35813;&#26041;&#27861;&#38543;&#26426;&#36974;&#30422;&#26631;&#35760;&#30340;&#37096;&#20998;&#21521;&#37327;&#34920;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#30828;&#21024;&#38500;&#26631;&#20934;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#26159;&#25552;&#20379;&#23545;&#27169;&#22411;&#25512;&#29702;&#36807;&#31243;&#36827;&#34892;&#39044;&#27979;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#19968;&#20010;&#26356;&#21152;&#20934;&#30830;&#30340;&#24402;&#22240;&#26041;&#27861;&#26631;&#24535;&#30528;&#23427;&#26356;&#21152;&#24544;&#23454;&#65292;&#23427;&#21487;&#20197;&#26356;&#21152;&#20934;&#30830;&#22320;&#21453;&#26144;&#21738;&#20123;&#37096;&#20998;&#30340;&#36755;&#20837;&#23545;&#39044;&#27979;&#26356;&#21152;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24544;&#23454;&#24230;&#35780;&#20272;&#26041;&#27861;&#65292;&#22914;&#20805;&#20998;&#24615;&#21644;&#20840;&#38754;&#24615;&#65292;&#21482;&#20351;&#29992;&#19968;&#31181;&#30828;&#21024;&#38500;&#26631;&#20934;&#65292;&#21363;&#23436;&#20840;&#21024;&#38500;&#25110;&#20445;&#30041;&#30001;&#32473;&#23450;&#24402;&#22240;&#26041;&#27861;&#25490;&#21517;&#26368;&#39640;&#30340;&#39030;&#37096;&#26631;&#35760;&#65292;&#24182;&#35266;&#23519;&#39044;&#27979;&#21487;&#33021;&#24615;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#30828;&#21024;&#38500;&#26631;&#20934;&#24573;&#30053;&#20102;&#27599;&#20010;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#65292;&#25226;&#23427;&#20204;&#20840;&#37096;&#31561;&#21516;&#22320;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36719;&#21024;&#38500;&#26631;&#20934;&#12290;&#25105;&#20204;&#19981;&#20250;&#23436;&#20840;&#21024;&#38500;&#25110;&#20445;&#30041;&#36755;&#20837;&#20013;&#30340;&#26631;&#35760;&#65292;&#32780;&#26159;&#38543;&#26426;&#22320;&#36974;&#30422;&#20195;&#34920;&#24402;&#22240;&#26041;&#27861;&#37325;&#35201;&#24615;&#30340;&#37096;&#20998;&#26631;&#35760;&#21521;&#37327;&#34920;&#31034;&#12290;&#22522;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;&#19981;&#21516;&#30340;&#24402;&#22240;&#26041;&#27861;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution methods (FAs) are popular approaches for providing insights into the model reasoning process of making predictions. The more faithful a FA is, the more accurately it reflects which parts of the input are more important for the prediction. Widely used faithfulness metrics, such as sufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely removing or retaining the top most important tokens ranked by a given FA and observing the changes in predictive likelihood. However, this hard criterion ignores the importance of each individual token, treating them all equally for computing sufficiency and comprehensiveness. In this paper, we propose a simple yet effective soft erasure criterion. Instead of entirely removing or retaining tokens from the input, we randomly mask parts of the token vector representations proportionately to their FA importance. Extensive experiments across various natural language processing tasks and different FAs show that our sof
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedPDD&#30340;&#38544;&#31169;&#20445;&#25252;&#21452;&#37325;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25945;&#24072;&#33976;&#39311;&#21644;&#23398;&#29983;&#33976;&#39311;&#20004;&#20010;&#38454;&#27573;&#65292;&#22312;&#19981;&#20256;&#36755;&#27169;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#36716;&#31227;&#30693;&#35782;&#21644;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06272</link><description>&lt;p&gt;
FedPDD&#65306;&#29992;&#20110;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#30340;&#38544;&#31169;&#20445;&#25252;&#21452;&#37325;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedPDD: A Privacy-preserving Double Distillation Framework for Cross-silo Federated Recommendation. (arXiv:2305.06272v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FedPDD&#30340;&#38544;&#31169;&#20445;&#25252;&#21452;&#37325;&#33976;&#39311;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#25945;&#24072;&#33976;&#39311;&#21644;&#23398;&#29983;&#33976;&#39311;&#20004;&#20010;&#38454;&#27573;&#65292;&#22312;&#19981;&#20256;&#36755;&#27169;&#22411;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#36716;&#31227;&#30693;&#35782;&#21644;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24179;&#21488;&#25512;&#33616;&#26088;&#22312;&#36890;&#36807;&#20174;&#19981;&#21516;&#24179;&#21488;&#25910;&#38598;&#24322;&#26500;&#29305;&#24449;&#26469;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#25252;&#27861;&#35268;&#38480;&#21046;&#20102;&#36825;&#31181;&#24179;&#21488;&#38388;&#30340;&#36328;&#30028;&#21327;&#20316;&#65292;&#22240;&#27492;&#19981;&#33021;&#32858;&#21512;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#25512;&#33616;&#22330;&#26223;&#20013;&#22788;&#29702;&#25968;&#25454;&#23396;&#23707;&#38382;&#39064;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#29616;&#26377;&#30340;&#36328;&#24179;&#21488;FL&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#37325;&#21472;&#29992;&#25143;&#30340;&#25968;&#25454;&#21327;&#21516;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#65292;&#20256;&#36755;&#27169;&#22411;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#37325;&#21472;&#29992;&#25143;&#30340;&#25968;&#37327;&#24448;&#24448;&#38750;&#24120;&#23567;&#65292;&#20174;&#32780;&#22823;&#22823;&#38480;&#21046;&#20102;&#27492;&#31867;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#26399;&#38388;&#20256;&#36755;&#27169;&#22411;&#20449;&#24687;&#38656;&#35201;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#21487;&#33021;&#20250;&#36896;&#25104;&#20005;&#37325;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#20445;&#25252;&#21452;&#37325;&#33976;&#39311;&#26694;&#26550; FedPDD &#29992;&#20110;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26377;&#25928;&#22320;&#36716;&#31227;&#30693;&#35782;&#26469;&#20445;&#25252;&#38544;&#31169;&#12290;FedPDD &#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#25945;&#24072;&#33976;&#39311;&#21644;&#23398;&#29983;&#33976;&#39311;&#12290;&#22312;&#25945;&#24072;&#33976;&#39311;&#38454;&#27573;&#65292;&#27599;&#20010;&#24179;&#21488;&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#24182;&#23558;&#26469;&#33258;&#36825;&#20123;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#19968;&#20010;&#23567;&#30340;&#12289;&#24102;&#26377;&#22122;&#22768;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#12290;&#28982;&#21518;&#65292;&#22312;&#23398;&#29983;&#33976;&#39311;&#38454;&#27573;&#65292;&#27599;&#20010;&#24179;&#21488;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#65292;&#21516;&#26102;&#20174;&#25945;&#24072;&#27169;&#22411;&#21644;&#26412;&#22320;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#35757;&#32451;&#33258;&#24049;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;FedPDD &#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36328;&#24179;&#21488;&#32852;&#37030;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-platform recommendation aims to improve recommendation accuracy by gathering heterogeneous features from different platforms. However, such cross-silo collaborations between platforms are restricted by increasingly stringent privacy protection regulations, thus data cannot be aggregated for training. Federated learning (FL) is a practical solution to deal with the data silo problem in recommendation scenarios. Existing cross-silo FL methods transmit model information to collaboratively build a global model by leveraging the data of overlapped users. However, in reality, the number of overlapped users is often very small, thus largely limiting the performance of such approaches. Moreover, transmitting model information during training requires high communication costs and may cause serious privacy leakage. In this paper, we propose a novel privacy-preserving double distillation framework named FedPDD for cross-silo federated recommendation, which efficiently transfers knowledge wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#30340;&#20027;&#21160;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#22810;&#31181;&#20027;&#21160;&#21644;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36951;&#24536;-&#23398;&#20064;&#26354;&#32447;&#26041;&#27861;&#26469;&#24179;&#34913;&#19981;&#24536;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#23398;&#20064;&#30340;&#20004;&#20010;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.03923</link><description>&lt;p&gt;
&#20027;&#21160;&#30340;&#36830;&#32493;&#23398;&#20064;&#65306;&#22312;&#20219;&#21153;&#24207;&#21015;&#20013;&#26631;&#35760;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active Continual Learning: Labelling Queries in a Sequence of Tasks. (arXiv:2305.03923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#20027;&#21160;&#23398;&#20064;&#20219;&#21153;&#30340;&#20027;&#21160;&#36830;&#32493;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#22330;&#26223;&#19979;&#22810;&#31181;&#20027;&#21160;&#21644;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#20043;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#36951;&#24536;-&#23398;&#20064;&#26354;&#32447;&#26041;&#27861;&#26469;&#24179;&#34913;&#19981;&#24536;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#23398;&#20064;&#30340;&#20004;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20013;&#65292;&#33719;&#21462;&#26032;&#30693;&#35782;&#24182;&#19981;&#24536;&#35760;&#24050;&#23398;&#20869;&#23481;&#26159;&#20854;&#26680;&#24515;&#12290;&#32780;&#20219;&#21153;&#26159;&#25353;&#39034;&#24207;&#20986;&#29616;&#30340;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#20934;&#22791;&#21644;&#27880;&#37322;&#21017;&#36890;&#24120;&#26159;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#36830;&#32493;&#23398;&#20064;&#26469;&#36866;&#24212;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#20219;&#21153;&#30340;&#20027;&#21160;&#36830;&#32493;&#23398;&#20064;&#65288;ACL&#65289;&#20013;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#20219;&#21153;&#21253;&#25324;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#27744;&#21644;&#19968;&#20010;&#27880;&#37322;&#39044;&#31639;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;AL&#21644;CL&#31639;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#65292;&#31867;&#21035;&#21644;&#20219;&#21153;&#22686;&#37327;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#23454;&#39564;&#25581;&#31034;&#20102;&#19981;&#24536;&#26087;&#30693;&#35782;&#21644;&#24555;&#36895;&#23398;&#20064;&#22312;CL&#21644;AL&#20013;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#23613;&#31649;&#22312;&#20197;&#21069;&#20219;&#21153;&#30340;&#27880;&#37322;&#25910;&#38598;&#19978;&#26465;&#20214;&#26597;&#35810;&#31574;&#30053;&#20250;&#25552;&#39640;&#39046;&#22495;&#21644;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#30340;&#20219;&#21153;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#36951;&#24536;-&#23398;&#20064;&#26354;&#32447;&#21017;&#26356;&#22909;&#22320;&#24179;&#34913;&#20102;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring new knowledge without forgetting what has been learned in a sequence of tasks is the central focus of continual learning (CL). While tasks arrive sequentially, the training data are often prepared and annotated independently, leading to CL of incoming supervised learning tasks. This paper considers the under-explored problem of active continual learning (ACL) for a sequence of active learning (AL) tasks, where each incoming task includes a pool of unlabelled data and an annotation budget. We investigate the effectiveness and interplay between several AL and CL algorithms in the domain, class and task-incremental scenarios. Our experiments reveal the trade-off between two contrasting goals of not forgetting the old knowledge and the ability to quickly learn in CL and AL. While conditioning the query strategy on the annotations collected for the previous tasks leads to improved task performance on the domain and task incremental learning, our proposed forgetting-learning profil
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20808;&#39564;&#30340;&#30697;&#27861;&#36712;&#36947;&#24674;&#22797;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#21442;&#29031;&#38754;&#23545;&#40784;&#21644;&#21333;&#39063;&#31890;&#20919;&#20923;&#30005;&#38236;&#24314;&#27169;&#31561;&#38382;&#39064;&#65292;&#20855;&#26377;&#25233;&#21046;&#22122;&#22768;&#30340;&#20248;&#21183;.</title><link>http://arxiv.org/abs/2304.14604</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20808;&#39564;&#30340;&#30697;&#27861;&#36712;&#36947;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Deep Neural-network Prior for Orbit Recovery from Method of Moments. (arXiv:2304.14604v1 [stat.ME] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14604
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20808;&#39564;&#30340;&#30697;&#27861;&#36712;&#36947;&#24674;&#22797;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#22810;&#21442;&#29031;&#38754;&#23545;&#40784;&#21644;&#21333;&#39063;&#31890;&#20919;&#20923;&#30005;&#38236;&#24314;&#27169;&#31561;&#38382;&#39064;&#65292;&#20855;&#26377;&#25233;&#21046;&#22122;&#22768;&#30340;&#20248;&#21183;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36947;&#24674;&#22797;&#38382;&#39064;&#26159;&#19968;&#31867;&#32463;&#24120;&#20986;&#29616;&#30340;&#38382;&#39064;&#65292;&#26377;&#22810;&#31181;&#24418;&#24335;&#12290;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#32463;&#36807;&#32676;&#20316;&#29992;&#25197;&#26354;&#24182;&#36890;&#36807;&#24050;&#30693;&#31639;&#23376;&#35266;&#23519;&#21518;&#65292;&#20272;&#35745;&#26410;&#30693;&#20989;&#25968;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#35266;&#27979;&#20540;&#20250;&#21463;&#21040;&#38750;&#24179;&#20961;&#27700;&#24179;&#30340;&#22122;&#22768;&#27745;&#26579;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#29305;&#23450;&#30340;&#36712;&#36947;&#24674;&#22797;&#38382;&#39064;&#65292;&#21363;&#22810;&#21442;&#29031;&#38754;&#23545;&#40784;&#21644;&#21333;&#39063;&#31890;&#20919;&#20923;&#30005;&#38236;&#24314;&#27169;&#12290;&#20026;&#20102;&#25233;&#21046;&#22122;&#22768;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20004;&#20010;&#38382;&#39064;&#20013;&#37117;&#20351;&#29992;&#30697;&#27861;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20808;&#39564;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#24212;&#36755;&#20986;&#20449;&#21495;&#21644;&#32676;&#20803;&#32032;&#30340;&#20998;&#24067;&#65292;&#32780;&#30697;&#21017;&#20026;&#36755;&#20837;&#12290;&#22312;&#22810;&#21442;&#29031;&#38754;&#23545;&#40784;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20174;&#30697;&#20013;&#37325;&#24314;&#20449;&#21495;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#37325;&#24314;&#20102;&#20919;&#20923;&#30005;&#38236;&#27169;&#25311;&#21644;&#29983;&#29289;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orbit recovery problems are a class of problems that often arise in practice and in various forms. In these problems, we aim to estimate an unknown function after being distorted by a group action and observed via a known operator. Typically, the observations are contaminated with a non-trivial level of noise. Two particular orbit recovery problems of interest in this paper are multireference alignment and single-particle cryo-EM modeling. In order to suppress the noise, we suggest using the method of moments approach for both problems while introducing deep neural network priors. In particular, our neural networks should output the signals and the distribution of group elements, with moments being the input. In the multireference alignment case, we demonstrate the advantage of using the NN to accelerate the convergence for the reconstruction of signals from the moments. Finally, we use our method to reconstruct simulated and biological volumes in the cryo-EM setting.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#25506;&#32034;&#20102;&#20855;&#26377; $U(1)$ &#21619;&#36947;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#30340;&#21619;&#36947;&#32467;&#26500;&#65292;&#25214;&#21040;&#20102;21&#20010;&#19982;&#23454;&#39564;&#27979;&#37327;&#20540;&#19968;&#33268;&#30340;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#26080;&#20013;&#24494;&#23376;&#21452;&#36125;&#22612;&#34928;&#21464;&#30340;&#26377;&#25928;&#36136;&#37327;&#21644;&#21487;&#35266;&#30340;&#36731;&#23376; CP &#30772;&#22351;&#12290;</title><link>http://arxiv.org/abs/2304.14176</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#22840;&#20811;&#21644;&#36731;&#23376;&#30340;&#21619;&#36947;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Exploring the flavor structure of quarks and leptons with reinforcement learning. (arXiv:2304.14176v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14176
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#25506;&#32034;&#20102;&#20855;&#26377; $U(1)$ &#21619;&#36947;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#30340;&#21619;&#36947;&#32467;&#26500;&#65292;&#25214;&#21040;&#20102;21&#20010;&#19982;&#23454;&#39564;&#27979;&#37327;&#20540;&#19968;&#33268;&#30340;&#27169;&#22411;&#65292;&#39044;&#27979;&#20102;&#26080;&#20013;&#24494;&#23376;&#21452;&#36125;&#22612;&#34928;&#21464;&#30340;&#26377;&#25928;&#36136;&#37327;&#21644;&#21487;&#35266;&#30340;&#36731;&#23376; CP &#30772;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25506;&#32034;&#22840;&#20811;&#21644;&#36731;&#23376;&#21619;&#36947;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#20316;&#20026;&#20855;&#20307;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22522;&#26412;&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#65292;&#38024;&#23545;&#20855;&#26377; $U(1)$ &#21619;&#36947;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#12290;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23545;&#22840;&#20811;&#21644;&#36731;&#23376;&#30340; $U(1)$ &#33655;&#36827;&#34892;&#23398;&#20064;&#65292;&#20195;&#29702;&#26041;&#26696;&#25214;&#21040;&#20102;21&#20010;&#19982;&#22840;&#20811;&#21644;&#36731;&#23376;&#30340;&#23454;&#39564;&#27979;&#37327;&#36136;&#37327;&#21644;&#28151;&#21512;&#35282;&#19968;&#33268;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#27491;&#24207;&#30340;&#22266;&#26377;&#20540;&#24448;&#24448;&#22823;&#20110;&#21453;&#24207;&#65292;&#27491;&#24207;&#19982;&#30446;&#21069;&#30340;&#23454;&#39564;&#25968;&#25454;&#30456;&#27604;&#26356;&#21152;&#31526;&#21512;&#12290;&#20195;&#29702;&#30340;&#33258;&#20027;&#34892;&#20026;&#26681;&#25454;&#26080;&#20013;&#24494;&#23376;&#21452;&#36125;&#22612;&#34928;&#21464;&#30340;&#26377;&#25928;&#36136;&#37327;&#21644;&#21494;&#23376;&#22330;&#30340;&#35282;&#25104;&#20998;&#24341;&#36215;&#30340;&#21487;&#35266;&#30340;&#36731;&#23376; CP &#30772;&#22351;&#26469;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method to explore the flavor structure of quarks and leptons with reinforcement learning. As a concrete model, we utilize a basic policy-based algorithm for models with $U(1)$ flavor symmetry. By training neural networks on the $U(1)$ charges of quarks and leptons, the agent finds 21 models to be consistent with experimentally measured masses and mixing angles of quarks and leptons. In particular, an intrinsic value of normal ordering tends to be larger than that of inverted ordering, and the normal ordering is well fitted with the current experimental data in contrast to the inverted ordering. A specific value of effective mass for the neutrinoless double beta decay and a sizable leptonic CP violation induced by an angular component of flavon field are predicted by autonomous behavior of the agent.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#26234;&#33021;&#30005;&#32593;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#31361;&#20986;&#20102;&#30446;&#21069;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.18136</link><description>&lt;p&gt;
&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Machine-learned Adversarial Attacks against Fault Prediction Systems in Smart Electrical Grids. (arXiv:2303.18136v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#26234;&#33021;&#30005;&#32593;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#31361;&#20986;&#20102;&#30446;&#21069;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#65292;&#30001;&#20110;&#32463;&#27982;&#21644;&#20851;&#38190;&#24615;&#30340;&#21407;&#22240;&#65292;&#25925;&#38556;&#26816;&#27979;&#20219;&#21153;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#26234;&#33021;&#30005;&#32593;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#32570;&#38519;&#26816;&#27979;&#21644;&#36127;&#36733;&#39044;&#27979;&#65292;&#24050;&#32463;&#37319;&#29992;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#30740;&#31350;&#26234;&#33021;&#30005;&#32593;&#24773;&#20917;&#19979;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#23433;&#20840;&#24615;&#25361;&#25112;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#23578;&#26410;&#19982;&#25152;&#26377;&#30005;&#32593;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#22320;&#36827;&#34892;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26234;&#33021;&#30005;&#32593;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#31361;&#20986;&#23637;&#31034;&#20102;&#25925;&#38556;&#23450;&#20301;&#21644;&#31867;&#22411;&#20998;&#31867;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#35828;&#26126;&#20102;&#30446;&#21069;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In smart electrical grids, fault detection tasks may have a high impact on society due to their economic and critical implications. In the recent years, numerous smart grid applications, such as defect detection and load forecasting, have embraced data-driven methodologies. The purpose of this study is to investigate the challenges associated with the security of machine learning (ML) applications in the smart grid scenario. Indeed, the robustness and security of these data-driven algorithms have not been extensively studied in relation to all power grid applications. We demonstrate first that the deep neural network method used in the smart grid is susceptible to adversarial perturbation. Then, we highlight how studies on fault localization and type classification illustrate the weaknesses of present ML algorithms in smart grids to various adversarial attacks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#38543;&#26426;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65292;&#38024;&#23545;&#26410;&#30693;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#36890;&#36807;&#25191;&#34892;&#29305;&#24449;&#21521;&#37327;&#36716;&#25442;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.17043</link><description>&lt;p&gt;
&#26080;&#35266;&#27979;&#19978;&#19979;&#25991;&#30340;&#32852;&#37030;&#38543;&#26426;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Stochastic Bandit Learning with Unobserved Context. (arXiv:2303.17043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#38543;&#26426;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65292;&#38024;&#23545;&#26410;&#30693;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#36890;&#36807;&#25191;&#34892;&#29305;&#24449;&#21521;&#37327;&#36716;&#25442;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26410;&#30693;&#19978;&#19979;&#25991;&#30340;&#32852;&#37030;&#38543;&#26426;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#65292;&#20854;&#20013;M&#20010;&#20195;&#29702;&#38754;&#20020;&#19981;&#21516;&#30340;&#36172;&#21338;&#26426;&#24182;&#21327;&#20316;&#23398;&#20064;&#12290;&#36890;&#20449;&#27169;&#22411;&#30001;&#20013;&#22830;&#26381;&#21153;&#22120;&#32452;&#25104;&#65292;&#24182;&#19988;&#20195;&#29702;&#20250;&#23450;&#26399;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20849;&#20139;&#20854;&#20272;&#35745;&#32467;&#26524;&#65292;&#20197;&#20415;&#36873;&#25321;&#26368;&#20248;&#21160;&#20316;&#20197;&#26368;&#23567;&#21270;&#24635;&#21518;&#24724;&#12290;&#25105;&#20204;&#20551;&#35774;&#31934;&#30830;&#30340;&#19978;&#19979;&#25991;&#19981;&#21487;&#35266;&#23519;&#65292;&#20195;&#29702;&#20165;&#35266;&#27979;&#19978;&#19979;&#25991;&#30340;&#20998;&#24067;&#12290;&#20363;&#22914;&#65292;&#24403;&#19978;&#19979;&#25991;&#26412;&#36523;&#26159;&#22122;&#22768;&#27979;&#37327;&#25110;&#22522;&#20110;&#39044;&#27979;&#26426;&#21046;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#31181;&#20998;&#24067;&#24335;&#32852;&#37030;&#31639;&#27861;&#65292;&#20419;&#36827;&#20195;&#29702;&#20043;&#38388;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#36873;&#25321;&#19968;&#31995;&#21015;&#26368;&#20248;&#21160;&#20316;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#36890;&#36807;&#25191;&#34892;&#29305;&#24449;&#21521;&#37327;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#38500;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#32447;&#24615;&#21442;&#25968;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of federated stochastic multi-arm contextual bandits with unknown contexts, in which M agents are faced with different bandits and collaborate to learn. The communication model consists of a central server and the agents share their estimates with the central server periodically to learn to choose optimal actions in order to minimize the total regret. We assume that the exact contexts are not observable and the agents observe only a distribution of the contexts. Such a situation arises, for instance, when the context itself is a noisy measurement or based on a prediction mechanism. Our goal is to develop a distributed and federated algorithm that facilitates collaborative learning among the agents to select a sequence of optimal actions so as to maximize the cumulative reward. By performing a feature vector transformation, we propose an elimination-based algorithm and prove the regret bound for linearly parametrized reward functions. Finally, we validated the perfo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#36136;&#22810;&#22771;&#25193;&#25955;&#21152;&#26435;MRI&#20272;&#35745;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#20989;&#25968;&#30340;&#21333;&#38454;&#27573;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#25195;&#25551;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16376</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#24322;&#36136;&#22810;&#22771;&#25193;&#25955;&#21152;&#26435;MRI&#20272;&#35745;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#20989;&#25968;&#30340;&#21333;&#38454;&#27573;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Unified Single-stage Learning Model for Estimating Fiber Orientation Distribution Functions on Heterogeneous Multi-shell Diffusion-weighted MRI. (arXiv:2303.16376v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#36136;&#22810;&#22771;&#25193;&#25955;&#21152;&#26435;MRI&#20272;&#35745;&#32420;&#32500;&#23450;&#21521;&#20998;&#24067;&#20989;&#25968;&#30340;&#21333;&#38454;&#27573;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#25195;&#25551;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#21152;&#26435;MRI&#36890;&#36807;&#20854;&#22312;q&#31354;&#38388;&#20013;&#30340;&#39057;&#35889;&#27979;&#37327;&#27599;&#20010;&#20307;&#32032;&#20013;&#26412;&#22320;&#25193;&#25955;&#36807;&#31243;&#30340;&#26041;&#21521;&#21644;&#23610;&#24230;&#65292;&#36890;&#24120;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;shell&#20013;&#33719;&#21462;&#12290; &#26368;&#36817;&#22312;&#24494;&#32467;&#26500;&#25104;&#20687;&#21644;&#22810;&#32452;&#32455;&#20998;&#35299;&#26041;&#38754;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20449;&#21495;&#30340;&#24452;&#21521;b&#20540;&#20381;&#36182;&#24615;&#30340;&#20851;&#27880;&#12290; &#22240;&#27492;&#65292;&#22312;&#32452;&#32455;&#20998;&#31867;&#21644;&#24494;&#35266;&#32467;&#26500;&#20272;&#35745;&#26041;&#38754;&#65292;&#38656;&#35201;&#25193;&#23637;&#24452;&#21521;&#21644;&#35282;&#21521;&#22495;&#30340;&#20449;&#21495;&#34920;&#31034;&#12290; &#25552;&#20986;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#27169;&#25311;DW-MRI&#20449;&#21495;&#19982;&#29983;&#29289;&#24494;&#32467;&#26500;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290; &#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22810;&#22771;&#22810;&#32452;&#32455;&#32422;&#26463;&#29699;&#24418;&#21435;&#21367;&#31215;&#65289;&#20855;&#26377;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#21644;&#26356;&#39640;&#30340;&#25195;&#25551;&#19968;&#33268;&#24615;&#12290; &#28982;&#32780;&#65292;&#30001;&#20110;&#23398;&#20064;&#36807;&#31243;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25193;&#25955;&#36807;&#31243;&#21644;&#32452;&#32455;&#24494;&#32467;&#26500;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#22240;&#27492;&#36890;&#24120;&#38656;&#35201;&#22810;&#38454;&#27573;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-weighted (DW) MRI measures the direction and scale of the local diffusion process in every voxel through its spectrum in q-space, typically acquired in one or more shells. Recent developments in micro-structure imaging and multi-tissue decomposition have sparked renewed attention to the radial b-value dependence of the signal. Applications in tissue classification and micro-architecture estimation, therefore, require a signal representation that extends over the radial as well as angular domain. Multiple approaches have been proposed that can model the non-linear relationship between the DW-MRI signal and biological microstructure. In the past few years, many deep learning-based methods have been developed towards faster inference speed and higher inter-scan consistency compared with traditional model-based methods (e.g., multi-shell multi-tissue constrained spherical deconvolution). However, a multi-stage learning strategy is typically required since the learning process rel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31616;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#30340;&#29366;&#24577;&#20998;&#24067;&#26469;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#23376;&#20363;&#31243;&#30340;&#20840;&#23616;&#25506;&#32034;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#30340;&#21152;&#36895;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14623</link><description>&lt;p&gt;
&#26080;&#38656;&#24378;&#21270;&#23398;&#20064;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning without Reinforcement Learning. (arXiv:2303.14623v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14623
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#31616;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#30340;&#29366;&#24577;&#20998;&#24067;&#26469;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#23376;&#20363;&#31243;&#30340;&#20840;&#23616;&#25506;&#32034;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#30340;&#21152;&#36895;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064; (IRL) &#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#23398;&#20064;&#21512;&#20046;&#36923;&#36753;&#30340;&#19987;&#23478;&#28436;&#31034;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;IRL&#26041;&#27861;&#23384;&#22312;&#35745;&#31639;&#19978;&#30340;&#24369;&#28857;&#65306;&#23427;&#20204;&#38656;&#35201;&#23558;&#35299;&#20915;&#38590;&#24230;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#20316;&#20026;&#23376;&#20363;&#31243;&#36827;&#34892;&#21453;&#22797;&#27714;&#35299;&#12290;&#36825;&#19982;&#24402;&#32422;&#30340;&#35266;&#28857;&#30456;&#30683;&#30462;&#65306;&#25105;&#20204;&#24050;&#23558;&#27169;&#20223;&#23398;&#20064;&#30340;&#36739;&#26131;&#38382;&#39064;&#24402;&#32422;&#20026;&#21453;&#22797;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#30340;&#26356;&#38590;&#38382;&#39064;&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#24037;&#20316;&#35777;&#26126;&#65292;&#35775;&#38382;&#24378;&#31574;&#30053;&#33457;&#36153;&#26102;&#38388;&#30340;&#29366;&#24577;&#20998;&#24067;&#30340;&#20391;&#38754;&#20449;&#24687;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#35299;&#20915;RL&#38382;&#39064;&#30340;&#26679;&#26412;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#21152;&#26126;&#26234;&#30340;&#27169;&#20223;&#23398;&#20064;&#31616;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#30340;&#29366;&#24577;&#20998;&#24067;&#26469;&#32531;&#35299;RL&#23376;&#20363;&#31243;&#30340;&#20840;&#23616;&#25506;&#32034;&#37096;&#20998;&#65292;&#29702;&#35770;&#19978;&#25552;&#20379;&#20102;&#25351;&#25968;&#32423;&#30340;&#21152;&#36895;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#20013;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#26041;&#38754;&#37117;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;IRL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) is a powerful set of techniques for imitation learning that aims to learn a reward function that rationalizes expert demonstrations. Unfortunately, traditional IRL methods suffer from a computational weakness: they require repeatedly solving a hard reinforcement learning (RL) problem as a subroutine. This is counter-intuitive from the viewpoint of reductions: we have reduced the easier problem of imitation learning to repeatedly solving the harder problem of RL. Another thread of work has proved that access to the side-information of the distribution of states where a strong policy spends time can dramatically reduce the sample and computational complexities of solving an RL problem. In this work, we demonstrate for the first time a more informed imitation learning reduction where we utilize the state distribution of the expert to alleviate the global exploration component of the RL subroutine, providing an exponential speedup in theory. In practice
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21487;&#21464;&#22823;&#23567;&#21387;&#32553;&#24615;&#26694;&#26550;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#30456;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#23558;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#19982;&#20854;&#36755;&#20837;&#25968;&#25454;&#30340;&#21487;&#21464;&#22823;&#23567;&#21387;&#32553;&#29575;&#30456;&#20851;&#32852;&#65292;&#24182;&#25552;&#20379;&#20102;&#20381;&#36182;&#20110;&#32463;&#39564;&#20998;&#24067;&#32780;&#38750;&#26410;&#30693;&#20998;&#24067;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#25512;&#23548;&#20986;&#36755;&#20837;&#25968;&#25454;&#21644;&#36755;&#20986;&#20551;&#35774;&#38543;&#26426;&#21464;&#37327;&#30340;&#20219;&#20309;&#20989;&#25968;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#21253;&#21547;&#24182;&#21487;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;PAC-Bayes&#21644;&#25968;&#25454;&#30456;&#20851;&#20869;&#22312;&#32500;&#24230;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2303.05369</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#21464;&#22823;&#23567;&#30340;&#21387;&#32553;&#24615;&#24314;&#31435;&#25968;&#25454;&#30456;&#20851;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Data-dependent Generalization Bounds via Variable-Size Compressibility. (arXiv:2303.05369v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#21487;&#21464;&#22823;&#23567;&#21387;&#32553;&#24615;&#26694;&#26550;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#30456;&#20851;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#23558;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#19982;&#20854;&#36755;&#20837;&#25968;&#25454;&#30340;&#21487;&#21464;&#22823;&#23567;&#21387;&#32553;&#29575;&#30456;&#20851;&#32852;&#65292;&#24182;&#25552;&#20379;&#20102;&#20381;&#36182;&#20110;&#32463;&#39564;&#20998;&#24067;&#32780;&#38750;&#26410;&#30693;&#20998;&#24067;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#25512;&#23548;&#20986;&#36755;&#20837;&#25968;&#25454;&#21644;&#36755;&#20986;&#20551;&#35774;&#38543;&#26426;&#21464;&#37327;&#30340;&#20219;&#20309;&#20989;&#25968;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#21253;&#21547;&#24182;&#21487;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;PAC-Bayes&#21644;&#25968;&#25454;&#30456;&#20851;&#20869;&#22312;&#32500;&#24230;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#8220;&#21487;&#21464;&#22823;&#23567;&#21387;&#32553;&#24615;&#8221;&#26694;&#26550;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#30456;&#20851;&#27867;&#21270;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#19982;&#20854;&#36755;&#20837;&#25968;&#25454;&#30340;&#21487;&#21464;&#22823;&#23567;&#8220;&#21387;&#32553;&#29575;&#8221;&#30456;&#20851;&#32852;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;&#30028;&#38480;&#20381;&#36182;&#20110;&#25163;&#22836;&#32473;&#23450;&#36755;&#20837;&#25968;&#25454;&#30340;&#32463;&#39564;&#20998;&#24067;&#65292;&#32780;&#19981;&#26159;&#20854;&#26410;&#30693;&#20998;&#24067;&#12290;&#25105;&#20204;&#24314;&#31435;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#21253;&#25324;&#23614;&#37096;&#30028;&#38480;&#12289;&#26399;&#26395;&#20540;&#30340;&#23614;&#37096;&#30028;&#38480;&#21644;&#26399;&#26395;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#21487;&#20197;&#25512;&#23548;&#20986;&#23545;&#36755;&#20837;&#25968;&#25454;&#21644;&#36755;&#20986;&#20551;&#35774;&#38543;&#26426;&#21464;&#37327;&#30340;&#20219;&#20309;&#20989;&#25968;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20123;&#27867;&#21270;&#30028;&#38480;&#21253;&#21547;&#24182;&#21487;&#33021;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#30340;&#22522;&#20110;PAC-Bayes&#21644;&#25968;&#25454;&#30456;&#20851;&#20869;&#22312;&#32500;&#24230;&#30340;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#24471;&#21040;&#22797;&#21407;&#65292;&#20174;&#32780;&#25581;&#31034;&#20986;&#25105;&#20204;&#26041;&#27861;&#30340;&#32479;&#19968;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we establish novel data-dependent upper bounds on the generalization error through the lens of a "variable-size compressibility" framework that we introduce newly here. In this framework, the generalization error of an algorithm is linked to a variable-size 'compression rate' of its input data. This is shown to yield bounds that depend on the empirical measure of the given input data at hand, rather than its unknown distribution. Our new generalization bounds that we establish are tail bounds, tail bounds on the expectation, and in-expectations bounds. Moreover, it is shown that our framework also allows to derive general bounds on any function of the input data and output hypothesis random variables. In particular, these general bounds are shown to subsume and possibly improve over several existing PAC-Bayes and data-dependent intrinsic dimension-based bounds that are recovered as special cases, thus unveiling a unifying character of our approach. For instance, a new da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#21442;&#25968;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;Shapley&#20540;&#36817;&#20284;&#31639;&#27861;SVARM&#21644;Stratified SVARM&#65292;&#23427;&#20204;&#22522;&#20110;&#19968;&#31181;&#19982;&#36793;&#38469;&#36129;&#29486;&#27010;&#24565;&#33073;&#38057;&#30340;Shapley&#20540;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#23558;&#20854;&#19982;&#21512;&#25104;&#28216;&#25103;&#21644;&#24120;&#35265;&#21487;&#35299;&#37322;&#24615;&#29992;&#20363;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2302.00736</link><description>&lt;p&gt;
&#19981;&#20351;&#29992;&#36793;&#38469;&#36129;&#29486;&#36817;&#20284;&#35745;&#31639;Shapley&#20540;
&lt;/p&gt;
&lt;p&gt;
Approximating the Shapley Value without Marginal Contributions. (arXiv:2302.00736v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26080;&#21442;&#25968;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;Shapley&#20540;&#36817;&#20284;&#31639;&#27861;SVARM&#21644;Stratified SVARM&#65292;&#23427;&#20204;&#22522;&#20110;&#19968;&#31181;&#19982;&#36793;&#38469;&#36129;&#29486;&#27010;&#24565;&#33073;&#38057;&#30340;Shapley&#20540;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#23558;&#20854;&#19982;&#21512;&#25104;&#28216;&#25103;&#21644;&#24120;&#35265;&#21487;&#35299;&#37322;&#24615;&#29992;&#20363;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#26159;&#20026;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#29609;&#23478;&#20998;&#37197;&#26377;&#24847;&#20041;&#30340;&#36129;&#29486;&#20540;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#65292;&#26368;&#36817;&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;Shapley&#20540;&#30340;&#26377;&#24847;&#20041;&#24615;&#28304;&#20110;&#20165;&#26377;Shapley&#20540;&#28385;&#36275;&#30340;&#20844;&#29702;&#23646;&#24615;&#65292;&#28982;&#32780;&#65292;&#30830;&#20999;&#35745;&#31639;&#30340;&#20195;&#20215;&#26159;&#38543;&#30528;&#29609;&#23478;&#25968;&#37327;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#39640;&#25928;&#36817;&#20284;Shapley&#20540;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#22260;&#32469;&#30528;&#29609;&#23478;&#30340;&#36793;&#38469;&#36129;&#29486;&#30340;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#19982;&#36793;&#38469;&#36129;&#29486;&#27010;&#24565;&#33073;&#38057;&#30340;Shapley&#20540;&#34920;&#31034;&#30340;&#26080;&#21442;&#25968;&#12289;&#39046;&#22495;&#26080;&#20851;&#30340;&#36817;&#20284;&#31639;&#27861;SVARM&#21644;Stratified SVARM&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#30340;&#26080;&#19982;&#20262;&#27604;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#21253;&#25324;&#21512;&#25104;&#28216;&#25103;&#21644;&#24120;&#29992;&#21487;&#35299;&#37322;&#24615;&#29992;&#20363;&#30340;&#23454;&#35777;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Shapley value is arguably the most popular approach for assigning a meaningful contribution value to players in a cooperative game, which has recently been used intensively in explainable artificial intelligence. The meaningfulness is due to axiomatic properties that only the Shapley value satisfies, which, however, comes at the expense of an exact computation growing exponentially with the number of agents. Accordingly, a number of works are devoted to the efficient approximation of the Shapley values, most of them revolve around the notion of an agent's marginal contribution. In this paper, we propose with SVARM and Stratified SVARM two parameter-free and domain-independent approximation algorithms based on a representation of the Shapley value detached from the notion of marginal contributions. We prove unmatched theoretical guarantees regarding their approximation quality and provide empirical results including synthetic games as well as common explainability use cases comparin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MoleculeSTM&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#32467;&#26500;-&#25991;&#26412;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21270;&#23398;&#32467;&#26500;&#21644;&#25991;&#26412;&#25551;&#36848;&#65292;&#21487;&#20197;&#23454;&#29616;&#22522;&#20110;&#25991;&#26412;&#30340;&#26816;&#32034;&#21644;&#32534;&#36753;&#12290;&#36890;&#36807;&#26500;&#24314;&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#36827;&#34892;&#39564;&#35777;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#24320;&#25918;&#35789;&#27719;&#21644;&#32452;&#21512;&#24615;&#30340;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10789</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#20998;&#23376;&#32467;&#26500;-&#25991;&#26412;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#26816;&#32034;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing. (arXiv:2212.10789v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10789
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MoleculeSTM&#30340;&#22810;&#27169;&#24577;&#20998;&#23376;&#32467;&#26500;-&#25991;&#26412;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#21270;&#23398;&#32467;&#26500;&#21644;&#25991;&#26412;&#25551;&#36848;&#65292;&#21487;&#20197;&#23454;&#29616;&#22522;&#20110;&#25991;&#26412;&#30340;&#26816;&#32034;&#21644;&#32534;&#36753;&#12290;&#36890;&#36807;&#26500;&#24314;&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#36827;&#34892;&#39564;&#35777;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#24320;&#25918;&#35789;&#27719;&#21644;&#32452;&#21512;&#24615;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#20013;&#27491;&#22312;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#37319;&#29992;&#20154;&#24037;&#26234;&#33021;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#21033;&#29992;&#20998;&#23376;&#30340;&#21270;&#23398;&#32467;&#26500;&#65292;&#24573;&#35270;&#20102;&#21270;&#23398;&#39046;&#22495;&#20013;&#21487;&#29992;&#30340;&#20016;&#23500;&#25991;&#26412;&#30693;&#35782;&#12290;&#23558;&#25991;&#26412;&#30693;&#35782;&#32435;&#20837;&#32771;&#34385;&#21487;&#20197;&#23454;&#29616;&#26032;&#30340;&#33647;&#29289;&#35774;&#35745;&#30446;&#26631;&#65292;&#36866;&#24212;&#22522;&#20110;&#25991;&#26412;&#30340;&#25351;&#23548;&#21644;&#39044;&#27979;&#22797;&#26434;&#30340;&#29983;&#29289;&#27963;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#30340;&#20998;&#23376;&#32467;&#26500;-&#25991;&#26412;&#27169;&#22411;MoleculeSTM&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#20998;&#23376;&#30340;&#21270;&#23398;&#32467;&#26500;&#21644;&#25991;&#26412;&#25551;&#36848;&#26469;&#23454;&#29616;&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#12290;&#20026;&#20102;&#35757;&#32451;MoleculeSTM&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;PubChemSTM&#65292;&#21253;&#21547;&#36229;&#36807;28&#19975;&#20010;&#21270;&#23398;&#32467;&#26500;-&#25991;&#26412;&#23545;&#12290;&#20026;&#20102;&#23637;&#31034;MoleculeSTM&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#22522;&#20110;&#25991;&#26412;&#25351;&#20196;&#30340;&#25361;&#25112;&#24615;&#38646;&#26679;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#32467;&#26500;-&#25991;&#26412;&#26816;&#32034;&#21644;&#20998;&#23376;&#32534;&#36753;&#12290;MoleculeSTM&#20855;&#26377;&#20004;&#20010;&#20027;&#35201;&#29305;&#24615;&#65306;&#24320;&#25918;&#35789;&#27719;&#21644;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23454;&#29616;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing adoption of artificial intelligence in drug discovery. However, existing studies use machine learning to mainly utilize the chemical structures of molecules but ignore the vast textual knowledge available in chemistry. Incorporating textual knowledge enables us to realize new drug design objectives, adapt to text-based instructions and predict complex biological activities. Here we present a multi-modal molecule structure-text model, MoleculeSTM, by jointly learning molecules' chemical structures and textual descriptions via a contrastive learning strategy. To train MoleculeSTM, we construct a large multi-modal dataset, namely, PubChemSTM, with over 280,000 chemical structure-text pairs. To demonstrate the effectiveness and utility of MoleculeSTM, we design two challenging zero-shot tasks based on text instructions, including structure-text retrieval and molecule editing. MoleculeSTM has two main properties: open vocabulary and compositionality via natural language.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#20013;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#21452;&#37325;&#31283;&#20581;&#26368;&#36817;&#37051;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#33268;&#30340;&#20272;&#35745;&#65292;&#24182;&#22312;&#23384;&#22312;&#33391;&#22909;&#30340;&#34892;&#21644;&#21015;&#37051;&#23621;&#26102;&#25552;&#20379;&#65288;&#36817;&#20284;&#65289;&#20108;&#27425;&#25913;&#36827;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.14297</link><description>&lt;p&gt;
&#22240;&#23376;&#27169;&#22411;&#20013;&#30340;&#21452;&#37325;&#31283;&#20581;&#26368;&#36817;&#37051;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Doubly robust nearest neighbors in factor models. (arXiv:2211.14297v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14297
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#20013;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#21452;&#37325;&#31283;&#20581;&#26368;&#36817;&#37051;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#33268;&#30340;&#20272;&#35745;&#65292;&#24182;&#22312;&#23384;&#22312;&#33391;&#22909;&#30340;&#34892;&#21644;&#21015;&#37051;&#23621;&#26102;&#25552;&#20379;&#65288;&#36817;&#20284;&#65289;&#20108;&#27425;&#25913;&#36827;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#22312;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#20013;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#25913;&#36827;&#26368;&#36817;&#37051;&#65288;NN&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#24102;&#26377;&#32570;&#22833;&#25968;&#25454;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#20854;&#20013;&#24403;&#34987;&#35266;&#23519;&#21040;&#26102;&#65292;&#31532;$(i, t)$&#20010;&#26465;&#30446;&#30001;&#20854;&#22343;&#20540;$f(u_i, v_t)$&#21152;&#19978;&#22343;&#20540;&#20026;&#38646;&#30340;&#22122;&#22768;&#32473;&#20986;&#65292;&#20854;&#20013;$f$&#20026;&#26410;&#30693;&#20989;&#25968;&#65292;$u_i$&#21644;$v_t$&#20026;&#28508;&#22312;&#22240;&#23376;&#12290;&#20043;&#21069;&#30340;NN&#31574;&#30053;&#65292;&#22914;&#21333;&#20803;-&#21333;&#20803;NN&#65292;&#29992;&#20110;&#20272;&#35745;&#22343;&#20540;$f(u_i, v_t)$&#65292;&#20381;&#36182;&#20110;&#23384;&#22312;&#20854;&#20182;&#34892;$j$&#20351;&#24471;$u_j \approx u_i$&#12290;&#31867;&#20284;&#22320;&#65292;&#26102;&#38388;-&#26102;&#38388;NN&#31574;&#30053;&#20381;&#36182;&#20110;&#23384;&#22312;&#21015;$t'$&#20351;&#24471;$v_{t'} \approx v_t$&#12290;&#24403;&#30456;&#20284;&#34892;&#25110;&#30456;&#20284;&#21015;&#19981;&#21487;&#29992;&#26102;&#65292;&#36825;&#20123;&#31574;&#30053;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22312;&#20004;&#20010;&#26041;&#38754;&#23545;&#36825;&#31181;&#19981;&#36275;&#26159;&#21452;&#37325;&#31283;&#20581;&#30340;&#65306;(1) &#21482;&#35201;&#23384;&#22312;&#33391;&#22909;&#30340;&#34892;&#25110;&#21015;&#37051;&#23621;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#25552;&#20379;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290; (2) &#27492;&#22806;&#65292;&#22914;&#26524;&#23384;&#22312;&#33391;&#22909;&#30340;&#34892;&#21644;&#21015;&#37051;&#23621;&#65292;&#23427;&#25552;&#20379;&#20102;&#65288;&#36817;&#20284;&#65289;&#20108;&#27425;&#25913;&#36827;&#38750;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and analyze an improved variant of nearest neighbors (NN) for estimation with missing data in latent factor models. We consider a matrix completion problem with missing data, where the $(i, t)$-th entry, when observed, is given by its mean $f(u_i, v_t)$ plus mean-zero noise for an unknown function $f$ and latent factors $u_i$ and $v_t$. Prior NN strategies, like unit-unit NN, for estimating the mean $f(u_i, v_t)$ relies on existence of other rows $j$ with $u_j \approx u_i$. Similarly, time-time NN strategy relies on existence of columns $t'$ with $v_{t'} \approx v_t$. These strategies provide poor performance respectively when similar rows or similar columns are not available. Our estimate is doubly robust to this deficit in two ways: (1) As long as there exist either good row or good column neighbors, our estimate provides a consistent estimate. (2) Furthermore, if both good row and good column neighbors exist, it provides a (near-)quadratic improvement in the non-asympto
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#20272;&#35745;&#24178;&#39044;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#20851;&#31995;&#21644;&#21327;&#21464;&#37327;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.01900</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#20272;&#35745;&#21453;&#20107;&#23454;&#27835;&#30103;&#32467;&#26524;&#30340;&#26102;&#38388;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios. (arXiv:2206.01900v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#20272;&#35745;&#24178;&#39044;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#20851;&#31995;&#21644;&#21327;&#21464;&#37327;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24037;&#31243;&#21644;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#24178;&#39044;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#20154;&#31867;&#20309;&#26102;&#24212;&#35813;&#24178;&#39044;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#20309;&#26102;&#29699;&#21592;&#24212;&#35813;&#20256;&#32473;&#38431;&#21451;&#36827;&#34892;&#22909;&#23556;&#38376;&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20351;&#29992;&#21453;&#20107;&#23454;&#30340;&#38271;&#26399;&#39044;&#27979;&#26469;&#20272;&#35745;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65288;ITE&#65289;&#26159;&#35780;&#20272;&#27492;&#31867;&#24178;&#39044;&#25514;&#26045;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20256;&#32479;&#26694;&#26550;&#27809;&#26377;&#32771;&#34385;&#21040;&#22810;&#26234;&#33021;&#20307;&#20851;&#31995;&#30340;&#26102;&#38388;&#21464;&#21270;&#21644;&#21327;&#21464;&#37327;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;ITE&#30340;&#38169;&#35823;&#35780;&#20272;&#21644;&#35299;&#37322;&#22256;&#38590;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20272;&#35745;&#24178;&#39044;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#22270;&#24418;&#21464;&#20998;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#35745;&#31639;&#26469;&#36827;&#34892;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#30340;&#38271;&#26399;&#39044;&#27979;&#30340;ITE&#20272;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#30830;&#35748;&#24490;&#29615;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of intervention in a multi-agent system, e.g., when humans should intervene in autonomous driving systems and when a player should pass to teammates for a good shot, is challenging in various engineering and scientific fields. Estimating the individual treatment effect (ITE) using counterfactual long-term prediction is practical to evaluate such interventions. However, most of the conventional frameworks did not consider the time-varying complex structure of multi-agent relationships and covariate counterfactual prediction. This may lead to erroneous assessments of ITE and difficulty in interpretation. Here we propose an interpretable, counterfactual recurrent network in multi-agent systems to estimate the effect of the intervention. Our model leverages graph variational recurrent neural networks and theory-based computation with domain knowledge for the ITE estimation framework based on long-term prediction of multi-agent covariates and outcomes, which can confirm the circu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;TracInAD&#65292;&#29992;&#20110;&#26681;&#25454;&#24433;&#21709;&#21147;&#27979;&#37327;&#26631;&#35760;&#24322;&#24120;&#65292;&#24182;&#19988;&#22312;&#21307;&#30103;&#21644;&#32593;&#32476;&#23433;&#20840;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.01362</link><description>&lt;p&gt;
TracInAD&#65306;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#24433;&#21709;&#21147;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TracInAD: Measuring Influence for Anomaly Detection. (arXiv:2205.01362v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;TracInAD&#65292;&#29992;&#20110;&#26681;&#25454;&#24433;&#21709;&#21147;&#27979;&#37327;&#26631;&#35760;&#24322;&#24120;&#65292;&#24182;&#19988;&#22312;&#21307;&#30103;&#21644;&#32593;&#32476;&#23433;&#20840;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35768;&#22810;&#20854;&#20182;&#20219;&#21153;&#19968;&#26679;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20063;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#26816;&#27979;&#34920;&#26684;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#24120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26681;&#25454;TracIn&#65288;&#19968;&#31181;&#26368;&#21021;&#29992;&#20110;&#35299;&#37322;&#24615;&#30446;&#30340;&#30340;&#24433;&#21709;&#21147;&#27979;&#37327;&#65289;&#26469;&#26631;&#35760;&#24322;&#24120;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#22686;&#24378;&#20219;&#20309;&#26080;&#30417;&#30563;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#34920;&#26126;&#35757;&#32451;&#28857;&#23545;&#27979;&#35797;&#28857;&#30340;&#24179;&#22343;&#24433;&#21709;&#21147;&#21487;&#20197;&#29992;&#20316;&#24322;&#24120;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21307;&#30103;&#21644;&#32593;&#32476;&#23433;&#20840;&#34920;&#26684;&#22522;&#20934;&#25968;&#25454;&#19978;&#22312;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As with many other tasks, neural networks prove very effective for anomaly detection purposes. However, very few deep-learning models are suited for detecting anomalies on tabular datasets. This paper proposes a novel methodology to flag anomalies based on TracIn, an influence measure initially introduced for explicability purposes. The proposed methods can serve to augment any unsupervised deep anomaly detection method. We test our approach using Variational Autoencoders and show that the average influence of a subsample of training points on a test point can serve as a proxy for abnormality. Our model proves to be competitive in comparison with state-of-the-art approaches: it achieves comparable or better performance in terms of detection accuracy on medical and cyber-security tabular benchmark data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#20581;&#24247;&#20449;&#24687;&#31995;&#32479;&#20013;&#32771;&#34385;&#29992;&#25143;&#20215;&#20540;&#20849;&#21019;&#30340;&#26368;&#20339;&#26381;&#21153;&#36164;&#28304;&#31649;&#29702;&#31574;&#30053;&#65292;&#36890;&#36807;&#23884;&#20837;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26381;&#21153;&#36164;&#28304;&#30340;&#20248;&#21270;&#20998;&#37197;&#21644;&#29992;&#25143;&#21442;&#19982;&#34892;&#20026;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2204.02521</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#20581;&#24247;&#20449;&#24687;&#31995;&#32479;&#20013;&#32771;&#34385;&#29992;&#25143;&#20215;&#20540;&#20849;&#21019;&#30340;&#26368;&#20339;&#26381;&#21153;&#36164;&#28304;&#31649;&#29702;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Optimal service resource management strategy for IoT-based health information system considering value co-creation of users. (arXiv:2204.02521v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#20581;&#24247;&#20449;&#24687;&#31995;&#32479;&#20013;&#32771;&#34385;&#29992;&#25143;&#20215;&#20540;&#20849;&#21019;&#30340;&#26368;&#20339;&#26381;&#21153;&#36164;&#28304;&#31649;&#29702;&#31574;&#30053;&#65292;&#36890;&#36807;&#23884;&#20837;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26381;&#21153;&#36164;&#28304;&#30340;&#20248;&#21270;&#20998;&#37197;&#21644;&#29992;&#25143;&#21442;&#19982;&#34892;&#20026;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#26381;&#21153;&#36164;&#28304;&#31649;&#29702;&#31574;&#30053;&#65292;&#29992;&#20110;&#22686;&#24378;&#20581;&#24247;&#20449;&#24687;&#26381;&#21153;&#30340;&#24615;&#33021;&#65292;&#20248;&#21270;&#26381;&#21153;&#36164;&#28304;&#21033;&#29992;&#65292;&#24182;&#25552;&#20379;&#20114;&#21160;&#24615;&#20581;&#24247;&#20449;&#24687;&#26381;&#21153;&#12290;&#32771;&#34385;&#21040;&#20581;&#24247;&#20449;&#24687;&#26381;&#21153;&#20013;&#30340;&#20215;&#20540;&#20849;&#21019;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#26368;&#20339;&#26381;&#21153;&#36164;&#28304;&#31649;&#29702;&#31574;&#30053;&#65292;&#37325;&#28857;&#20851;&#27880;&#19982;&#29992;&#25143;&#30340;&#21327;&#20316;&#21644;&#20114;&#21160;&#12290;&#22312;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#20581;&#24247;&#20449;&#24687;&#26381;&#21153;&#31995;&#32479;&#65288;I-HISS&#65289;&#20013;&#23884;&#20837;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25511;&#21046;&#26381;&#21153;&#25552;&#20379;&#21644;&#26381;&#21153;&#36866;&#24212;&#26469;&#20998;&#37197;&#26381;&#21153;&#36164;&#28304;&#65292;&#22522;&#20110;&#29992;&#25143;&#21442;&#19982;&#34892;&#20026;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#22312;&#19981;&#21516;&#29992;&#25143;&#23545;&#20581;&#24247;&#20449;&#24687;&#26381;&#21153;&#30340;&#21453;&#24212;&#19979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores optimal service resource management strategy, a continuous challenge for health information service to enhance service performance, optimise service resource utilisation and deliver interactive health information service. An adaptive optimal service resource management strategy was developed considering a value co-creation model in health information service with a focus on collaborative and interactive with users. The deep reinforcement learning algorithm was embedded in the Internet of Things (IoT)-based health information service system (I-HISS) to allocate service resources by controlling service provision and service adaptation based on user engagement behaviour. The simulation experiments were conducted to evaluate the significance of the proposed algorithm under different user reactions to the health information service.
&lt;/p&gt;</description></item></channel></rss>