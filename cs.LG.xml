<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#28145;&#24230;&#27169;&#22411;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#38543;&#26426;&#20004;&#28857;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#20256;&#36882;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30340;&#39640;&#25928;&#24615;&#24182;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01621</link><description>&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#27169;&#22411;&#38646;&#38454;&#20248;&#21270;&#30340;&#38543;&#26426;&#20004;&#28857;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Two Points Method for Deep Model Zeroth-order Optimization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#28145;&#24230;&#27169;&#22411;&#30340;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#38543;&#26426;&#20004;&#28857;&#27861;&#65292;&#36890;&#36807;&#21069;&#21521;&#20256;&#36882;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20248;&#21270;&#30446;&#26631;&#19978;&#30340;&#39640;&#25928;&#24615;&#24182;&#36229;&#36234;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#30001;&#20110;&#30828;&#20214;&#39044;&#31639;&#25110;&#32570;&#20047;&#21453;&#21521;&#20256;&#25773;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#26500;&#24314;&#25110;&#23436;&#20840;&#24494;&#35843;&#36825;&#26679;&#30340;&#22823;&#27169;&#22411;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#38646;&#38454;&#26041;&#27861;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#65292;&#23427;&#21482;&#38656;&#35201;&#21069;&#21521;&#20256;&#36882;&#26469;&#26356;&#26032;&#27169;&#22411;&#12290;&#26412;&#25991;&#22312;&#26080;&#26799;&#24230;&#24773;&#24418;&#19979;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#38543;&#26426;&#20004;&#28857;&#65288;S2P&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#19968;&#33324;&#21644;&#25918;&#26494;&#30340;&#24179;&#28369;&#24615;&#20551;&#35774;&#19979;&#25552;&#20986;&#20102;S2P&#30340;&#29702;&#35770;&#25910;&#25947;&#24615;&#36136;&#12290;&#29702;&#35770;&#24615;&#36136;&#36824;&#25581;&#31034;&#20102;&#26356;&#24555;&#12289;&#26356;&#31283;&#23450;&#30340;S2P&#21464;&#20307;&#8212;&#8212;&#21152;&#36895;S2P&#65288;AS2P&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;&#26032;&#25910;&#25947;&#24615;&#36136;&#65292;&#26356;&#22909;&#22320;&#34920;&#31034;&#20102;&#28145;&#24230;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;AS2P&#22312;&#20248;&#21270;&#22823;&#22411;&#28145;&#24230;&#27169;&#22411;&#65288;&#21253;&#25324;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#30446;&#26631;&#19978;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#19988;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large foundation models, such as large language models, have performed exceptionally well in various application scenarios. Building or fully fine-tuning such large models is usually prohibitive due to either hardware budget or lack of access to backpropagation. The zeroth-order methods offer a promising direction for tackling this challenge, where only forward passes are needed to update the model. This paper introduces an efficient Stochastic Two-Point (S2P) approach within the gradient-free regime. We present the theoretical convergence properties of S2P under the general and relaxed smoothness assumptions. The theoretical properties also shed light on a faster and more stable S2P variant, Accelerated S2P (AS2P), through exploiting our new convergence properties that better represent the dynamics of deep models in training. Our comprehensive empirical results show that AS2P is highly effective in optimizing objectives for large deep models, including language models, and outperforms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01454</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#19968;&#31181;&#32479;&#35745;&#22240;&#26524;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#65288;SCD&#65289;&#20013;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#20316;&#20026;&#32422;&#26463;&#23884;&#20837;&#21040;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#25509;&#21463;&#65292;&#22240;&#20026;&#36825;&#23545;&#20110;&#21019;&#24314;&#19968;&#33268;&#26377;&#24847;&#20041;&#30340;&#22240;&#26524;&#27169;&#22411;&#26159;&#37325;&#35201;&#30340;&#65292;&#23613;&#31649;&#35782;&#21035;&#32972;&#26223;&#30693;&#35782;&#30340;&#25361;&#25112;&#34987;&#35748;&#21487;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;LLM&#30340;&#8220;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#65288;SCP&#65289;&#8221;&#19982;SCD&#26041;&#27861;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#22240;&#26524;&#25512;&#26029;&#65288;KBCI&#65289;&#30456;&#32467;&#21512;&#65292;&#23545;SCD&#36827;&#34892;&#20808;&#39564;&#30693;&#35782;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4&#21487;&#20197;&#20351;LLM-KBCI&#30340;&#36755;&#20986;&#19982;&#24102;&#26377;LLM-KBCI&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;SCD&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#65292;&#22914;&#26524;GPT-4&#32463;&#21382;&#20102;SCP&#65292;&#37027;&#20040;SCD&#30340;&#32467;&#26524;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;LLM&#19981;&#21547;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;LLM&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20854;&#32972;&#26223;&#30693;&#35782;&#26469;&#25913;&#36827;SCD&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32423;&#32852;&#32553;&#25918;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#36793;&#38469;&#25233;&#21046;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#65292;&#24182;&#38477;&#20302;&#36807;&#21435;&#20219;&#21153;&#30340;&#36951;&#24536;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01262</link><description>&lt;p&gt;
&#32423;&#32852;&#32553;&#25918;&#20998;&#31867;&#22120;&#65306;&#36890;&#36807;&#27010;&#29575;&#32553;&#25918;&#36827;&#34892;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cascaded Scaling Classifier: class incremental learning with probability scaling
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01262
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32423;&#32852;&#32553;&#25918;&#20998;&#31867;&#22120;&#65292;&#32467;&#21512;&#36793;&#38469;&#25233;&#21046;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#65292;&#24182;&#38477;&#20302;&#36807;&#21435;&#20219;&#21153;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#33719;&#21462;&#26032;&#30693;&#35782;&#24182;&#23558;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#20165;&#26377;&#36731;&#24494;&#30340;&#36951;&#24536;&#12290;&#21516;&#26679;&#30340;&#33021;&#21147;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#36830;&#32493;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#24433;&#21709;&#21040;&#36807;&#21435;&#23398;&#20064;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#36951;&#24536;&#21487;&#20197;&#36890;&#36807;&#22238;&#25918;&#23384;&#20648;&#30340;&#36807;&#21435;&#20219;&#21153;&#26679;&#26412;&#26469;&#32531;&#35299;&#65292;&#20294;&#26159;&#23545;&#20110;&#38271;&#24207;&#21015;&#20219;&#21153;&#21487;&#33021;&#38656;&#35201;&#36739;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#65307;&#27492;&#22806;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#20445;&#23384;&#26679;&#26412;&#30340;&#36807;&#25311;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22686;&#37327;&#20998;&#31867;&#22120;&#65292;&#20998;&#21035;&#31216;&#20026;&#36793;&#38469;&#25233;&#21046;&#21644;&#32423;&#32852;&#32553;&#25918;&#20998;&#31867;&#22120;&#12290;&#21069;&#32773;&#32467;&#21512;&#20102;&#36719;&#32422;&#26463;&#21644;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#20445;&#30041;&#36807;&#21435;&#23398;&#20064;&#30340;&#30693;&#35782;&#21516;&#26102;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#30340;&#27169;&#24335;&#12290;&#21518;&#32773;&#26159;&#19968;&#31181;&#24102;&#26377;&#38376;&#25511;&#30340;&#22686;&#37327;&#20998;&#31867;&#22120;&#65292;&#24110;&#21161;&#27169;&#22411;&#20462;&#25913;&#36807;&#21435;&#30340;&#39044;&#27979;&#32780;&#19981;&#30452;&#25509;&#24178;&#25200;&#23427;&#20204;&#12290;&#36825;&#26159;&#36890;&#36807;...
&lt;/p&gt;
&lt;p&gt;
Humans are capable of acquiring new knowledge and transferring learned knowledge into different domains, incurring a small forgetting. The same ability, called Continual Learning, is challenging to achieve when operating with neural networks due to the forgetting affecting past learned tasks when learning new ones. This forgetting can be mitigated by replaying stored samples from past tasks, but a large memory size may be needed for long sequences of tasks; moreover, this could lead to overfitting on saved samples. In this paper, we propose a novel regularisation approach and a novel incremental classifier called, respectively, Margin Dampening and Cascaded Scaling Classifier. The first combines a soft constraint and a knowledge distillation approach to preserve past learned knowledge while allowing the model to learn new patterns effectively. The latter is a gated incremental classifier, helping the model modify past predictions without directly interfering with them. This is achieved
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35782;&#21035;&#26410;&#31934;&#32454;&#35299;&#26512;&#30340;PDEs&#20013;&#30340;&#38381;&#21512;&#39033;&#65292;&#36890;&#36807;&#37096;&#32626;&#20013;&#22830;&#31574;&#30053;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21644;&#21152;&#36895;&#27169;&#25311;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00972</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#35782;&#21035;&#31895;&#31890;&#24230;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38381;&#21512;&#39033;
&lt;/p&gt;
&lt;p&gt;
Closure Discovery for Coarse-Grained Partial Differential Equations using Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00972
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35782;&#21035;&#26410;&#31934;&#32454;&#35299;&#26512;&#30340;PDEs&#20013;&#30340;&#38381;&#21512;&#39033;&#65292;&#36890;&#36807;&#37096;&#32626;&#20013;&#22830;&#31574;&#30053;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#21644;&#21152;&#36895;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#22320;&#39044;&#27979;&#22825;&#27668;&#12289;&#37326;&#28779;&#21644;&#27969;&#34892;&#30149;&#31561;&#20851;&#38190;&#29616;&#35937;&#36890;&#24120;&#22522;&#20110;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#25551;&#36848;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25429;&#25417;&#36825;&#31181;PDEs&#20013;&#20840;&#38754;&#30340;&#26102;&#31354;&#23610;&#24230;&#33539;&#22260;&#30340;&#27169;&#25311;&#36890;&#24120;&#26159;&#20195;&#20215;&#39640;&#26114;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#20250;&#20351;&#29992;&#21033;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#32463;&#39564;&#38381;&#21512;&#39033;&#30340;&#31895;&#31890;&#24230;&#27169;&#25311;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35782;&#21035;&#26410;&#31934;&#32454;&#35299;&#26512;&#30340;PDEs&#20013;&#38381;&#21512;&#39033;&#30340;&#26032;&#39062;&#21644;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;MARL&#30340;&#24418;&#24335;&#21270;&#32467;&#21512;&#20102;&#24402;&#32435;&#20559;&#24046;&#65292;&#24182;&#21033;&#29992;&#37096;&#32626;&#20102;&#30001;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#39640;&#25928;&#34920;&#31034;&#30340;&#20013;&#22830;&#31574;&#30053;&#26469;&#21033;&#29992;&#23616;&#37096;&#24615;&#12290;&#36890;&#36807;&#23545;&#23545;&#27969;&#26041;&#31243;&#21644;Burgers&#26041;&#31243;&#30340;&#25968;&#20540;&#35299;&#36827;&#34892;&#28436;&#31034;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MARL&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;MARL&#23545;&#20110;&#20869;&#22806;&#20998;&#24067;&#30340;&#27979;&#35797;&#26696;&#20363;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#65292;&#24182;&#19988;&#19982;&#31934;&#32454;&#35299;&#26512;&#30456;&#27604;&#26377;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable predictions of critical phenomena, such as weather, wildfires and epidemics are often founded on models described by Partial Differential Equations (PDEs). However, simulations that capture the full range of spatio-temporal scales in such PDEs are often prohibitively expensive. Consequently, coarse-grained simulations that employ heuristics and empirical closure terms are frequently utilized as an alternative. We propose a novel and systematic approach for identifying closures in under-resolved PDEs using Multi-Agent Reinforcement Learning (MARL). The MARL formulation incorporates inductive bias and exploits locality by deploying a central policy represented efficiently by Convolutional Neural Networks (CNN). We demonstrate the capabilities and limitations of MARL through numerical solutions of the advection equation and the Burgers' equation. Our results show accurate predictions for in- and out-of-distribution test cases as well as a significant speedup compared to resolving
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGDF&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#32500;&#32435;&#28388;&#27874;&#29702;&#35770;&#21644;&#24341;&#20837;&#26102;&#21464;&#33258;&#36866;&#24212;&#26435;&#37325;&#65292;&#21152;&#36895;&#20102;SGD&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;SGDF&#22312;&#25910;&#25947;&#21644;&#27867;&#21270;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;</title><link>https://rss.arxiv.org/abs/2311.02818</link><description>&lt;p&gt;
&#20449;&#21495;&#22788;&#29702;&#19982;SGD&#30456;&#36935;&#65306;&#20174;&#21160;&#37327;&#21040;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
Signal Processing Meets SGD: From Momentum to Filter
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.02818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SGDF&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#32500;&#32435;&#28388;&#27874;&#29702;&#35770;&#21644;&#24341;&#20837;&#26102;&#21464;&#33258;&#36866;&#24212;&#26435;&#37325;&#65292;&#21152;&#36895;&#20102;SGD&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20854;&#20182;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;SGDF&#22312;&#25910;&#25947;&#21644;&#27867;&#21270;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21450;&#20854;&#22522;&#20110;&#21160;&#37327;&#30340;&#21464;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#20204;&#36890;&#24120;&#38754;&#20020;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#29616;&#26377;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#20248;&#21270;&#22120;&#21152;&#36895;&#25910;&#25947;&#65292;&#20294;&#24120;&#24120;&#20197;&#27867;&#21270;&#33021;&#21147;&#20026;&#20195;&#20215;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#23646;&#24615;&#20250;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30683;&#30462;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#21152;&#36895;SGD&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#19981;&#21464;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20943;&#23567;&#21382;&#21490;&#26799;&#24230;&#30340;&#26041;&#24046;&#30340;&#24605;&#24819;&#65292;&#36890;&#36807;&#24212;&#29992;&#32500;&#32435;&#28388;&#27874;&#29702;&#35770;&#22686;&#24378;SGD&#30340;&#19968;&#38454;&#30697;&#20272;&#35745;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#26102;&#21464;&#33258;&#36866;&#24212;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#20248;&#21270;&#22120;&#30456;&#27604;&#65292;SGDF&#22312;&#25910;&#25947;&#21644;&#27867;&#21270;&#20043;&#38388;&#25214;&#21040;&#20102;&#19968;&#20010;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, stochastic gradient descent (SGD) and its momentum-based variants are widely used in optimization algorithms, they usually face the problem of slow convergence. Meanwhile, existing adaptive learning rate optimizers accelerate convergence but often at the expense of generalization ability. We demonstrate that the adaptive learning rate property impairs generalization. To address this contradiction, we propose a novel optimization method that aims to accelerate the convergence rate of SGD without loss of generalization. This approach is based on the idea of reducing the variance of the historical gradient, enhancing the first-order moment estimation of the SGD by applying Wiener filtering theory, and introducing a time-varying adaptive weight. Experimental results show that SGDF achieves a trade-off between convergence and generalization compared to state-of-the-art optimizers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#32447;&#24615;&#21270;Laplace&#36817;&#20284;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#21407;&#22987;DNN&#30340;&#39044;&#27979;&#22343;&#20540;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#35757;&#32451;&#25104;&#26412;&#19982;&#35757;&#32451;&#28857;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;</title><link>https://rss.arxiv.org/abs/2302.12565</link><description>&lt;p&gt;
&#21464;&#20998;&#32447;&#24615;&#21270;Laplace&#36817;&#20284;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Variational Linearized Laplace Approximation for Bayesian Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2302.12565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#32447;&#24615;&#21270;Laplace&#36817;&#20284;&#22312;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#21407;&#22987;DNN&#30340;&#39044;&#27979;&#22343;&#20540;&#65292;&#24182;&#20855;&#26377;&#39640;&#25928;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#35757;&#32451;&#25104;&#26412;&#19982;&#35757;&#32451;&#28857;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#32447;&#24615;&#21270;Laplace&#36817;&#20284;&#65288;LLA&#65289;&#34987;&#29992;&#26469;&#23545;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#39044;&#27979;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#28857;&#25110;DNN&#21442;&#25968;&#36739;&#22810;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#20854;&#20182;LLA&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#22914;Kronecker&#20998;&#35299;&#25110;&#23545;&#35282;&#32447;GGN&#30697;&#38453;&#30340;&#36817;&#20284;&#65292;&#34987;&#20351;&#29992;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#31232;&#30095;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#30340;LLA&#36817;&#20284;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;GP&#30340;&#23545;&#20598;RKHS&#20844;&#24335;&#65292;&#24182;&#20445;&#30041;&#20102;&#21407;&#22987;DNN&#30340;&#39044;&#27979;&#22343;&#20540;&#12290;&#27492;&#22806;&#65292;&#23427;&#20801;&#35768;&#26377;&#25928;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20013;&#23454;&#29616;&#23376;&#32447;&#24615;&#35757;&#32451;&#26102;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20854;&#35757;&#32451;&#25104;&#26412;&#19982;&#35757;&#32451;&#28857;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#36817;&#20284;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Linearized Laplace Approximation (LLA) has been recently used to perform uncertainty estimation on the predictions of pre-trained deep neural networks (DNNs). However, its widespread application is hindered by significant computational costs, particularly in scenarios with a large number of training points or DNN parameters. Consequently, additional approximations of LLA, such as Kronecker-factored or diagonal approximate GGN matrices, are utilized, potentially compromising the model's performance. To address these challenges, we propose a new method for approximating LLA using a variational sparse Gaussian Process (GP). Our method is based on the dual RKHS formulation of GPs and retains as the predictive mean the output of the original DNN. Furthermore, it allows for efficient stochastic optimization, which results in sub-linear training time in the size of the training dataset. Specifically, its training cost is independent of the number of training points. We compare our propose
&lt;/p&gt;</description></item><item><title>BAdam&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#25910;&#25947;&#34892;&#20026;&#20197;&#21450;&#22312;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2404.02827</link><description>&lt;p&gt;
BAdam&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#39640;&#25928;&#20840;&#21442;&#25968;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02827
&lt;/p&gt;
&lt;p&gt;
BAdam&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#25910;&#25947;&#34892;&#20026;&#20197;&#21450;&#22312;&#24615;&#33021;&#35780;&#20272;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;BAdam&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;Adam&#20316;&#20026;&#20869;&#37096;&#27714;&#35299;&#22120;&#30340;&#22359;&#22352;&#26631;&#20248;&#21270;&#26694;&#26550;&#30340;&#20248;&#21270;&#22120;&#12290;BAdam&#25552;&#20379;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#24182;&#19988;&#30001;&#20110;&#38142;&#24335;&#35268;&#21017;&#23646;&#24615;&#20943;&#23569;&#20102;&#21453;&#21521;&#36807;&#31243;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;BAdam&#24212;&#29992;&#20110;&#22312;Alpaca-GPT4&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21333;&#20010;RTX3090-24GB GPU&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#30340;Llama 2-7B&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;LoRA&#21644;LOMO&#30456;&#27604;&#65292;BAdam&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;MT-bench&#23545;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#19979;&#28216;&#24615;&#33021;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;BAdam&#22312;&#36866;&#24230;&#36229;&#36234;LoRA&#30340;&#22522;&#30784;&#19978;&#26356;&#26174;&#33879;&#22320;&#20248;&#20110;LOMO&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;BAdam&#19982;Adam&#22312;&#20013;&#31561;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21363;&#22312;SuperGLUE&#22522;&#20934;&#19978;&#23545;RoBERTa-large&#36827;&#34892;&#24494;&#35843;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;BAdam&#33021;&#22815;&#32553;&#23567;&#19982;Adam&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02827v1 Announce Type: new  Abstract: This work presents BAdam, an optimizer that leverages the block coordinate optimization framework with Adam as the inner solver. BAdam offers a memory efficient approach to the full parameter finetuning of large language models and reduces running time of the backward process thanks to the chain rule property. Experimentally, we apply BAdam to instruction-tune the Llama 2-7B model on the Alpaca-GPT4 dataset using a single RTX3090-24GB GPU. The results indicate that BAdam exhibits superior convergence behavior in comparison to LoRA and LOMO. Furthermore, our downstream performance evaluation of the instruction-tuned models using the MT-bench shows that BAdam modestly surpasses LoRA and more substantially outperforms LOMO. Finally, we compare BAdam with Adam on a medium-sized task, i.e., finetuning RoBERTa-large on the SuperGLUE benchmark. The results demonstrate that BAdam is capable of narrowing the performance gap with Adam. Our code is
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02491</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Norms of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02491
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35201;&#27714;&#20855;&#26377;&#35299;&#20915;&#31038;&#20250;&#35268;&#33539;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26368;&#22823;&#30340;&#31038;&#20250;&#35268;&#33539;&#25216;&#33021;&#38598;&#65292;&#21253;&#25324;402&#39033;&#25216;&#33021;&#21644;12,383&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#35266;&#28857;&#21644;&#35770;&#28857;&#21040;&#25991;&#21270;&#21644;&#27861;&#24459;&#31561;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#29702;&#35299;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#19982;&#23567;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20960;&#20046;&#38543;&#26426;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5-Turbo&#21644;LLaMA2-Chat&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20165;&#30053;&#20302;&#20110;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
&lt;/p&gt;</description></item><item><title>SOMson&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#38899;&#39057;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#22686;&#24378;Kohonen&#22320;&#22270;&#19979;&#25968;&#25454;&#30340;&#20449;&#24687;&#37327;&#65292;&#35299;&#20915;SOM&#22312;&#25552;&#20379;&#25972;&#20307;&#22270;&#29255;&#26102;&#30340;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2404.00016</link><description>&lt;p&gt;
SOMson -- &#22312;Kohonen&#22320;&#22270;&#20013;&#23545;&#22810;&#32500;&#25968;&#25454;&#36827;&#34892;&#38899;&#39057;&#21270;
&lt;/p&gt;
&lt;p&gt;
SOMson -- Sonification of Multidimensional Data in Kohonen Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00016
&lt;/p&gt;
&lt;p&gt;
SOMson&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#38899;&#39057;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#22686;&#24378;Kohonen&#22320;&#22270;&#19979;&#25968;&#25454;&#30340;&#20449;&#24687;&#37327;&#65292;&#35299;&#20915;SOM&#22312;&#25552;&#20379;&#25972;&#20307;&#22270;&#29255;&#26102;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kohonen Maps&#65292;&#21448;&#31216;&#33258;&#32452;&#32455;&#26144;&#23556;&#65288;SOMs&#65289;&#65292;&#26159;&#19968;&#31181;&#21487;&#20197;&#23558;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#21487;&#35270;&#21270;&#21040;&#20302;&#32500;&#22320;&#22270;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#34429;&#28982;SOMs&#26159;&#25968;&#25454;&#23457;&#26597;&#21644;&#25506;&#32034;&#30340;&#32477;&#20339;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#22266;&#26377;&#22320;&#20250;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#12290;&#22320;&#22270;&#19979;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#24182;&#19981;&#23436;&#20840;&#25972;&#21512;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20840;&#23616;&#22270;&#29255;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;SOMson&#65292;&#19968;&#31181;&#23545;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#38899;&#39057;&#21270;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#38899;&#39057;&#21270;&#22686;&#21152;&#20102;SOM&#21516;&#26102;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#27809;&#26377;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#65292;&#32780;&#26159;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#22312;&#32447;&#31034;&#20363;&#65292;&#35753;&#35835;&#32773;&#21487;&#20197;&#33258;&#34892;&#25506;&#32034;SOMson&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20854;&#20248;&#21183;&#12289;&#21155;&#21183;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00016v1 Announce Type: cross  Abstract: Kohonen Maps, aka. Self-organizing maps (SOMs) are neural networks that visualize a high-dimensional feature space on a low-dimensional map. While SOMs are an excellent tool for data examination and exploration, they inherently cause a loss of detail. Visualizations of the underlying data do not integrate well and, therefore, fail to provide an overall picture. Consequently, we suggest SOMson, an interactive sonification of the underlying data, as a data augmentation technique. The sonification increases the amount of information provided simultaneously by the SOM. Instead of a user study, we present an interactive online example, so readers can explore SOMson themselves. Its strengths, weaknesses, and prospects are discussed.
&lt;/p&gt;</description></item><item><title>MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.19888</link><description>&lt;p&gt;
MambaMixer&#65306;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#21644;&#36890;&#36947;&#36873;&#25321;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19888
&lt;/p&gt;
&lt;p&gt;
MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;Transformers&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#24615;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#22823;&#35268;&#27169;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#23637;&#29616;&#20986;&#36755;&#20837;&#22823;&#23567;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#23581;&#35797;&#20026;&#22810;&#32500;&#25968;&#25454;&#35774;&#35745;&#39640;&#25928;&#26377;&#25928;&#30340;&#26550;&#26500;&#20027;&#24178;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#35201;&#20040;&#26159;&#25968;&#25454;&#29420;&#31435;&#30340;&#65292;&#35201;&#20040;&#26080;&#27861;&#20801;&#35768;&#36328;&#32500;&#24230;&#21644;&#20869;&#37096;&#32500;&#24230;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#24863;&#30693;&#23454;&#29616;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#20102;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#21463;&#21040;SSMs&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MambaMixer&#65292;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#36328;&#26631;&#35760;&#21644;&#36890;&#36947;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19888v1 Announce Type: cross  Abstract: Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. M
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;</title><link>https://arxiv.org/abs/2403.19871</link><description>&lt;p&gt;
&#36890;&#36807;&#32531;&#24930;&#21464;&#21270;&#30340;&#24207;&#21015;&#23454;&#29616;&#31283;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19871
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#36138;&#23146;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#32771;&#34385;&#36890;&#36807;&#19981;&#21516;&#30340;&#37325;&#26032;&#35757;&#32451;&#28436;&#21464;&#26469;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36890;&#36807;&#19981;&#21516;&#30340;&#25968;&#25454;&#25209;&#27425;&#26356;&#26032;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20445;&#30041;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265; - &#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#23454;&#26045;&#31616;&#26131;&#24615;&#21644;&#19982;&#29992;&#25143;&#24314;&#31435;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201; - &#36890;&#36807;&#20351;&#29992;&#21487;&#20197;&#30452;&#25509;&#32435;&#20837;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#23450;&#20041;&#23450;&#20041;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#29983;&#20135;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#27604;&#36138;&#23146;&#35757;&#32451;&#27169;&#22411;&#26356;&#24378;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19871v1 Announce Type: cross  Abstract: Retraining machine learning models remains an important task for real-world machine learning model deployment. Existing methods focus largely on greedy approaches to find the best-performing model without considering the stability of trained model structures across different retraining evolutions. In this study, we develop a mixed integer optimization algorithm that holistically considers the problem of retraining machine learning models across different data batch updates. Our method focuses on retaining consistent analytical insights - which is important to model interpretability, ease of implementation, and fostering trust with users - by using custom-defined distance metrics that can be directly incorporated into the optimization problem. Importantly, our method shows stronger stability than greedily trained models with a small, controllable sacrifice in model performance in a real-world production case study. Finally, important an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22686;&#24378;&#19987;&#23478;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.17601</link><description>&lt;p&gt;
LASIL&#65306;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#38271;&#26399;&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17601
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22686;&#24378;&#19987;&#23478;&#29366;&#24577;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35266;&#20132;&#36890;&#20223;&#30495;&#22312;&#20132;&#36890;&#24037;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#21333;&#20010;&#36710;&#36742;&#34892;&#20026;&#21644;&#25972;&#20307;&#20132;&#36890;&#27969;&#30340;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#19968;&#20010;&#30495;&#23454;&#30340;&#27169;&#25311;&#22120;&#65292;&#31934;&#30830;&#22797;&#21046;&#21508;&#31181;&#20132;&#36890;&#26465;&#20214;&#19979;&#30340;&#20154;&#31867;&#39550;&#39542;&#34892;&#20026;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#20381;&#36182;&#21551;&#21457;&#24335;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#24448;&#24448;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20132;&#36890;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#32780;&#26080;&#27861;&#25552;&#20379;&#20934;&#30830;&#30340;&#27169;&#25311;&#12290;&#30001;&#20110;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#27169;&#25311;&#22120;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#31283;&#23450;&#30340;&#38271;&#26399;&#27169;&#25311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23398;&#20064;&#32773;&#24863;&#30693;&#30340;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26234;&#20307;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#21516;&#26102;&#24314;&#27169;&#19987;&#23478;&#21644;&#23398;&#20064;&#32773;&#29366;&#24577;&#20998;&#24067;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#19987;&#23478;&#29366;&#24577;&#65292;&#20174;&#32780;&#20351;&#22686;&#24378;&#29366;&#24577;&#24847;&#35782;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17601v1 Announce Type: new  Abstract: Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#22312;&#21463;&#32422;&#26463;&#35774;&#32622;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19987;&#23478;&#34892;&#20026;&#21463;&#38480;&#21046;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.17456</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Imitating Cost-Constrained Behaviors in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#20223;&#21463;&#25104;&#26412;&#32422;&#26463;&#30340;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;&#27169;&#20223;&#23398;&#20064;&#22312;&#21463;&#32422;&#26463;&#35774;&#32622;&#19979;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#23454;&#38469;&#39046;&#22495;&#20013;&#19987;&#23478;&#34892;&#20026;&#21463;&#38480;&#21046;&#22240;&#32032;&#24433;&#21709;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#22797;&#26434;&#30340;&#35745;&#21010;&#21644;&#35843;&#24230;&#38382;&#39064;&#19968;&#30452;&#36890;&#36807;&#21508;&#31181;&#20248;&#21270;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#30340;&#27169;&#20223;&#23398;&#20064;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#21487;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#35266;&#23519;&#19987;&#23478;&#30340;&#34892;&#20026;&#26469;&#23398;&#20064;&#22870;&#21169;&#65288;&#25110;&#20559;&#22909;&#65289;&#27169;&#22411;&#25110;&#30452;&#25509;&#34892;&#20026;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26080;&#38480;&#21046;&#35774;&#32622;&#19979;&#30340;&#27169;&#20223;&#65288;&#20363;&#22914;&#65292;&#36710;&#36742;&#28040;&#32791;&#30340;&#29123;&#27833;&#37327;&#27809;&#26377;&#38480;&#21046;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#19987;&#23478;&#30340;&#34892;&#20026;&#19981;&#20165;&#21463;&#22870;&#21169;&#65288;&#25110;&#20559;&#22909;&#65289;&#30340;&#24433;&#21709;&#65292;&#36824;&#21463;&#32422;&#26463;&#30340;&#24433;&#21709;&#12290;&#20363;&#22914;&#65292;&#33258;&#21160;&#39550;&#39542;&#36865;&#36135;&#36710;&#30340;&#20915;&#31574;&#19981;&#20165;&#21462;&#20915;&#20110;&#36335;&#24452;&#20559;&#22909;/&#22870;&#21169;&#65288;&#26681;&#25454;&#36807;&#21435;&#30340;&#38656;&#27714;&#25968;&#25454;&#65289;&#65292;&#36824;&#21462;&#20915;&#20110;&#36710;&#36742;&#20869;&#30340;&#29123;&#27833;&#21644;&#36865;&#36798;&#26102;&#38388;&#31561;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17456v1 Announce Type: cross  Abstract: Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the ti
&lt;/p&gt;</description></item><item><title>&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15855</link><description>&lt;p&gt;
&#21021;&#22987;&#20540;&#21644;&#25299;&#25169;&#32467;&#26500;&#22312;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Initialisation and Topology Effects in Decentralised Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15855
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#23436;&#20840;&#20998;&#25955;&#24335;&#29305;&#24449;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22312;&#32593;&#32476;&#19978;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#23545;&#20010;&#20307;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#26412;&#22320;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#25968;&#25454;&#38544;&#31169;&#24615;&#65292;&#28040;&#38500;&#20102;&#21333;&#28857;&#25925;&#38556;&#21644;&#20013;&#22830;&#21327;&#35843;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#19968;&#20010;&#31616;&#21270;&#30340;&#25968;&#20540;&#27169;&#22411;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#31995;&#32479;&#30340;&#26089;&#26399;&#34892;&#20026;&#65292;&#20351;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#20540;&#31574;&#30053;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#19979;&#30340;&#27604;&#20363;&#34892;&#20026;&#21644;&#29615;&#22659;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26356;&#22810;&#30740;&#31350;&#25171;&#24320;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15855v1 Announce Type: cross  Abstract: Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices. A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for mor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#21270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#26045;&#21152;&#32422;&#26463;&#65292;&#20197;&#25913;&#21892;&#26679;&#26412;&#19982;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#12290;</title><link>https://arxiv.org/abs/2403.14404</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14404
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#21270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#26045;&#21152;&#32422;&#26463;&#65292;&#20197;&#25913;&#21892;&#26679;&#26412;&#19982;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22914;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#27491;&#24555;&#36895;&#25552;&#21319;&#20854;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#20063;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#36816;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#39044;&#26399;&#20174;&#38544;&#21547;&#25968;&#25454;&#20998;&#24067;&#20013;&#21462;&#26679;&#30340;&#26679;&#26412;&#23558;&#36981;&#23432;&#29305;&#23450;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#30340;&#22522;&#30784;&#32422;&#26463;&#36827;&#34892;&#20449;&#24687;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29983;&#25104;&#26679;&#26412;&#19982;&#26045;&#21152;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#32780;&#19981;&#24433;&#21709;&#25512;&#29702;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#36825;&#20123;&#32422;&#26463;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#38450;&#27490;&#36807;&#25311;&#21512;&#30340;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#65292;&#21487;&#29992;&#20110;&#26045;&#21152;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#20197;&#21450;&#36741;&#21161;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14404v1 Announce Type: new  Abstract: Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework to inform denoising diffusion models on underlying constraints on such generated samples during model training. Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed. Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting. Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.
&lt;/p&gt;</description></item><item><title>DL2Fence&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#24103;&#34701;&#21512;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;NoCs&#20013;&#26816;&#27979;&#21644;&#23450;&#20301;&#32454;&#21270;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#65292;&#20197;&#20986;&#33394;&#30340;&#26816;&#27979;&#24615;&#33021;&#21644;&#26497;&#20302;&#30340;&#30828;&#20214;&#24320;&#38144;&#33879;&#31216;&#12290;</title><link>https://arxiv.org/abs/2403.13563</link><description>&lt;p&gt;
DL2Fence: &#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#24103;&#34701;&#21512;&#38598;&#25104;&#65292;&#22686;&#24378;&#22823;&#35268;&#27169;NoCs&#20013;&#32454;&#21270;&#25298;&#32477;&#26381;&#21153;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
DL2Fence: Integrating Deep Learning and Frame Fusion for Enhanced Detection and Localization of Refined Denial-of-Service in Large-Scale NoCs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13563
&lt;/p&gt;
&lt;p&gt;
DL2Fence&#26159;&#19968;&#20010;&#26032;&#26694;&#26550;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#24103;&#34701;&#21512;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;NoCs&#20013;&#26816;&#27979;&#21644;&#23450;&#20301;&#32454;&#21270;&#30340;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#65292;&#20197;&#20986;&#33394;&#30340;&#26816;&#27979;&#24615;&#33021;&#21644;&#26497;&#20302;&#30340;&#30828;&#20214;&#24320;&#38144;&#33879;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#31934;&#32454;&#30340;&#27867;&#27946;&#27880;&#20837;&#36895;&#29575;&#21487;&#35843;&#30340;&#32593;&#32476;&#33455;&#29255;&#65288;NoC&#65289;&#25298;&#32477;&#26381;&#21153;&#65288;DoS&#65289;&#27169;&#22411;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#25552;&#20986;&#20102;DL2Fence&#65292;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;&#24103;&#34701;&#21512;&#65288;2F&#65289;&#36827;&#34892;DoS&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#26032;&#39062;&#26694;&#26550;&#12290;&#24320;&#21457;&#20102;&#20004;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#29992;&#20110;DoS&#30340;&#20998;&#31867;&#21644;&#20998;&#21106;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;95.8&#65285;&#21644;91.7&#65285;&#30340;&#26816;&#27979;&#21644;&#23450;&#20301;&#20934;&#30830;&#29575;&#65292;&#22312;16x16&#32593;&#26684;NoC&#20013;&#30340;&#31934;&#24230;&#29575;&#20998;&#21035;&#20026;98.5&#65285;&#21644;99.3&#65285;&#12290;&#24403;&#20174;8x8&#25193;&#23637;&#21040;16x16 NoCs&#26102;&#65292;&#35813;&#26694;&#26550;&#30340;&#30828;&#20214;&#24320;&#38144;&#26174;&#30528;&#20943;&#23569;&#20102;76.3&#65285;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#20854;&#30828;&#20214;&#35201;&#27714;&#23569;&#20102;42.4&#65285;&#12290;&#36825;&#19968;&#36827;&#23637;&#34920;&#26126;DL2Fence&#22312;&#24179;&#34913;&#22823;&#35268;&#27169;NoCs&#20013;&#20986;&#33394;&#30340;&#26816;&#27979;&#24615;&#33021;&#19982;&#26497;&#20302;&#30828;&#20214;&#24320;&#38144;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13563v1 Announce Type: cross  Abstract: This study introduces a refined Flooding Injection Rate-adjustable Denial-of-Service (DoS) model for Network-on-Chips (NoCs) and more importantly presents DL2Fence, a novel framework utilizing Deep Learning (DL) and Frame Fusion (2F) for DoS detection and localization. Two Convolutional Neural Networks models for classification and segmentation were developed to detect and localize DoS respectively. It achieves detection and localization accuracies of 95.8\% and 91.7\%, and precision rates of 98.5\% and 99.3\% in a 16x16 mesh NoC. The framework's hardware overhead notably decreases by 76.3\% when scaling from 8x8 to 16x16 NoCs, and it requires 42.4\% less hardware compared to state-of-the-arts. This advancement demonstrates DL2Fence's effectiveness in balancing outstanding detection performance in large-scale NoCs with extremely low hardware overhead.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AdaptSFL&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13101</link><description>&lt;p&gt;
AdaptSFL&#65306;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#32593;&#32476;&#20013;&#30340;&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AdaptSFL: Adaptive Split Federated Learning in Resource-constrained Edge Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13101
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AdaptSFL&#33258;&#36866;&#24212;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26085;&#30410;&#22797;&#26434;&#20351;&#24471;&#23558;&#20854;&#27665;&#20027;&#21270;&#21040;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#38754;&#20020;&#37325;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#27169;&#22411;&#20998;&#21306;&#23558;&#20027;&#35201;&#35757;&#32451;&#24037;&#20316;&#36127;&#33655;&#36716;&#31227;&#21040;&#26381;&#21153;&#22120;&#19978;&#65292;&#24182;&#22312;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#23454;&#29616;&#24182;&#34892;&#35757;&#32451;&#30340;&#20998;&#21106;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#31995;&#32479;&#20248;&#21270;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#36164;&#28304;&#21463;&#38480;&#31995;&#32479;&#19979;SFL&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#24456;&#22823;&#31243;&#24230;&#19978;&#27809;&#26377;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;SFL&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#37327;&#21270;&#20102;&#27169;&#22411;&#20998;&#21106;&#65288;MS&#65289;&#21644;&#23458;&#25143;&#31471;&#27169;&#22411;&#32858;&#21512;&#65288;MA&#65289;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20316;&#20026;&#29702;&#35770;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AdaptSFL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#33258;&#36866;&#24212;SFL&#26694;&#26550;&#65292;&#20197;&#21152;&#36895;&#36164;&#28304;&#21463;&#38480;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#19979;&#30340;SFL&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AdaptSFL&#33258;&#36866;&#24212;&#22320;&#25511;&#21046;&#23458;&#25143;&#31471;MA&#21644;MS&#65292;&#20197;&#24179;&#34913;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13101v1 Announce Type: new  Abstract: The increasing complexity of deep neural networks poses significant barriers to democratizing them to resource-limited edge devices. To address this challenge, split federated learning (SFL) has emerged as a promising solution by of floading the primary training workload to a server via model partitioning while enabling parallel training among edge devices. However, although system optimization substantially influences the performance of SFL under resource-constrained systems, the problem remains largely uncharted. In this paper, we provide a convergence analysis of SFL which quantifies the impact of model splitting (MS) and client-side model aggregation (MA) on the learning performance, serving as a theoretical foundation. Then, we propose AdaptSFL, a novel resource-adaptive SFL framework, to expedite SFL under resource-constrained edge computing systems. Specifically, AdaptSFL adaptively controls client-side MA and MS to balance commun
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#23618;&#27425;&#65288;&#33410;&#28857;&#32423;&#12289;&#37051;&#22495;&#32423;&#21644;&#22270;&#32423;&#65289;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.12529</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#20449;&#24687;&#25552;&#21319;&#20102;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Contextualized Messages Boost Graph Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12529
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#23618;&#27425;&#65288;&#33410;&#28857;&#32423;&#12289;&#37051;&#22495;&#32423;&#21644;&#22270;&#32423;&#65289;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22240;&#20854;&#22788;&#29702;&#20197;&#22270;&#34920;&#31034;&#30340;&#20219;&#24847;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;GNN&#36890;&#24120;&#36981;&#24490;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26469;&#26412;&#22320;&#26356;&#26032;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#20351;&#29992;&#22270;&#35835;&#20986;&#20989;&#25968;&#21019;&#24314;&#25972;&#20010;&#22270;&#30340;&#34920;&#31034;&#12290;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#20462;&#25913;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#32858;&#21512;&#21644;&#32452;&#21512;&#31574;&#30053;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;GNN&#65292;&#24120;&#24120;&#21463;&#21551;&#21457;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#20174;&#22522;&#20110;&#22270;&#21516;&#26500;&#38382;&#39064;&#30340;&#29702;&#35770;&#35282;&#24230;&#25506;&#32034;GNN&#65292;&#35813;&#38382;&#39064;&#22266;&#26377;&#22320;&#20551;&#35774;&#21487;&#25968;&#30340;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#29702;&#35770;&#24037;&#20316;&#25506;&#32034;&#20102;&#20855;&#26377;&#19981;&#21487;&#25968;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#30340;GNN&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;GNN&#22312;&#33410;&#28857;&#32423;&#12289;&#37051;&#22495;&#32423;&#21644;&#22270;&#32423;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12529v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have gained significant interest in recent years due to their ability to handle arbitrarily structured data represented as graphs. GNNs generally follow the message-passing scheme to locally update node feature representations. A graph readout function is then employed to create a representation for the entire graph. Several studies proposed different GNNs by modifying the aggregation and combination strategies of the message-passing framework, often inspired by heuristics. Nevertheless, several studies have begun exploring GNNs from a theoretical perspective based on the graph isomorphism problem which inherently assumes countable node feature representations. Yet, there are only a few theoretical works exploring GNNs with uncountable node feature representations. This paper presents a new perspective on the representational capabilities of GNNs across all levels - node-level, neighborhood-level, and graph-l
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#22240;&#26524;&#21457;&#29616;&#21151;&#33021;&#30340;&#22270;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#37096;&#20998;&#26631;&#35760;&#23398;&#20064;&#29615;&#22659;&#20013;&#26377;&#25928;&#23398;&#20064;&#21306;&#20998;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.11449</link><description>&lt;p&gt;
&#20855;&#26377;&#28508;&#22312;&#22240;&#26524;&#21457;&#29616;&#21151;&#33021;&#30340;&#22270;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Partial Label Learning with Potential Cause Discovering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11449
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#28508;&#22312;&#22240;&#26524;&#21457;&#29616;&#21151;&#33021;&#30340;&#22270;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#37096;&#20998;&#26631;&#35760;&#23398;&#20064;&#29615;&#22659;&#20013;&#26377;&#25928;&#23398;&#20064;&#21306;&#20998;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22240;&#20854;&#22312;&#35299;&#20915;&#21508;&#39046;&#22495;&#22797;&#26434;&#22270;&#32467;&#26500;&#25968;&#25454;&#25361;&#25112;&#20013;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#26631;&#27880;&#22270;&#25968;&#25454;&#20197;&#36827;&#34892;&#35757;&#32451;&#30001;&#20110;&#22270;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#30456;&#20114;&#20851;&#32852;&#24615;&#32780;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#24471;GNN&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#21306;&#20998;&#20449;&#24687;&#65292;&#21363;&#20351;&#22312;&#37096;&#20998;&#26631;&#35760;&#23398;&#20064;&#65288;PLL&#65289;&#30340;&#29615;&#22659;&#20013;&#23384;&#22312;&#22122;&#22768;&#26631;&#31614;&#12290; PLL&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#19982;&#19968;&#32452;&#20505;&#36873;&#26631;&#31614;&#30456;&#20851;&#32852;&#65292;&#21253;&#25324;&#30495;&#23454;&#26631;&#31614;&#21644;&#39069;&#22806;&#30340;&#22122;&#22768;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#28508;&#22312;&#22240;&#26524;&#25552;&#21462;&#26469;&#33719;&#21462;&#20855;&#26377;&#26356;&#39640;&#22240;&#26524;&#20851;&#31995;&#21487;&#33021;&#24615;&#30340;&#22270;&#25968;&#25454;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#25552;&#21462;&#30340;&#22270;&#30340;&#36741;&#21161;&#35757;&#32451;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11449v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have gained considerable attention for their potential in addressing challenges posed by complex graph-structured data in diverse domains. However, accurately annotating graph data for training is difficult due to the inherent complexity and interconnectedness of graphs. To tackle this issue, we propose a novel graph representation learning method that enables GNN models to effectively learn discriminative information even in the presence of noisy labels within the context of Partially Labeled Learning (PLL). PLL is a critical weakly supervised learning problem, where each training instance is associated with a set of candidate labels, including both the true label and additional noisy labels. Our approach leverages potential cause extraction to obtain graph data that exhibit a higher likelihood of possessing a causal relationship with the labels. By incorporating auxiliary training based on the extracted gra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;1D&#23376;&#27169;&#22411;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#20197;&#21450;&#21462;&#20915;&#20110;&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#30340;&#20960;&#31181;&#36890;&#29992;&#36817;&#20284;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#20445;&#35777;&#36817;&#20284;&#35823;&#24046;&#20219;&#24847;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.10089</link><description>&lt;p&gt;
&#29992;&#20110;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Approximation and bounding techniques for the Fisher-Rao distances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;1D&#23376;&#27169;&#22411;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#20197;&#21450;&#21462;&#20915;&#20110;&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#30340;&#20960;&#31181;&#36890;&#29992;&#36817;&#20284;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#20445;&#35777;&#36817;&#20284;&#35823;&#24046;&#20219;&#24847;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#27169;&#22411;&#30340;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#34987;&#23450;&#20041;&#20026;Fisher&#20449;&#24687;&#24230;&#37327;&#35825;&#23548;&#30340;Riemannian&#27979;&#22320;&#36317;&#31163;&#12290;&#20026;&#20102;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;Fisher-Rao&#36317;&#31163;&#65292;&#25105;&#20204;&#38656;&#35201;&#65288;1&#65289;&#25512;&#23548;&#20986;Fisher-Rao&#27979;&#22320;&#32447;&#30340;&#20844;&#24335;&#65292;&#20197;&#21450;&#65288;2&#65289;&#27839;&#30528;&#36825;&#20123;&#27979;&#22320;&#32447;&#31215;&#20998;Fisher&#38271;&#24230;&#20803;&#32032;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#23376;&#27169;&#22411;&#30340;&#38381;&#21512;&#24418;&#24335;1D Fisher-Rao&#36317;&#31163;&#25253;&#21578;&#20102;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20960;&#31181;&#36890;&#29992;&#30340;&#36817;&#20284;&#26041;&#26696;&#65292;&#21462;&#20915;&#20110;Fisher-Rao&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#33021;&#20197;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#25552;&#20379;Fisher-Rao&#39044;&#27979;&#27979;&#22320;&#32447;&#21644;&#20005;&#26684;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#26102;&#36817;&#20284;&#20135;&#29983;&#20219;&#24847;&#23567;&#30340;&#38468;&#21152;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10089v1 Announce Type: cross  Abstract: The Fisher-Rao distance between two probability distributions of a statistical model is defined as the Riemannian geodesic distance induced by the Fisher information metric. In order to calculate the Fisher-Rao distance in closed-form, we need (1) to elicit a formula for the Fisher-Rao geodesics, and (2) to integrate the Fisher length element along those geodesics. We consider several numerically robust approximation and bounding techniques for the Fisher-Rao distances: First, we report generic upper bounds on Fisher-Rao distances based on closed-form 1D Fisher-Rao distances of submodels. Second, we describe several generic approximation schemes depending on whether the Fisher-Rao geodesics or pregeodesics are available in closed-form or not. In particular, we obtain a generic method to guarantee an arbitrarily small additive error on the approximation provided that Fisher-Rao pregeodesics and tight lower and upper bounds are available
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#20116;&#31181;&#26174;&#33879;&#30340;&#24037;&#20214;&#65292;&#24182;&#24212;&#29992;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.07743</link><description>&lt;p&gt;
&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#37197;&#22791;&#24037;&#20214;&#22788;&#29702;&#27969;&#27700;&#32447;&#65306;&#35745;&#31639;&#19982;&#24615;&#33021;&#26435;&#34913;&#30340;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07743
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#20116;&#31181;&#26174;&#33879;&#30340;&#24037;&#20214;&#65292;&#24182;&#24212;&#29992;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#26159;&#30284;&#30151;&#35786;&#26029;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#22312;&#26174;&#24494;&#38236;&#19979;&#36827;&#34892;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#32452;&#32455;&#30149;&#29702;&#23398;&#22788;&#29702;&#36807;&#31243;&#20250;&#20135;&#29983;&#19968;&#20123;&#24037;&#20214;&#65292;&#26368;&#32456;&#20250;&#36716;&#31227;&#21040;&#29627;&#29827;&#36733;&#29627;&#29255;&#30340;&#25968;&#23383;&#21270;&#29256;&#26412;&#65292;&#21363;&#20840;&#29627;&#24187;&#28783;&#29255;&#12290;&#24037;&#20214;&#26159;&#35786;&#26029;&#26080;&#20851;&#30340;&#21306;&#22495;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#65288;CPATH&#65289;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#24037;&#20214;&#23545;&#20110;&#21487;&#38752;&#30340;&#33258;&#21160;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26041;&#26696;&#65292;&#29992;&#20110;&#26816;&#27979;&#21253;&#25324;&#25439;&#22351;&#32452;&#32455;&#12289;&#27169;&#31946;&#12289;&#35126;&#30385;&#32452;&#32455;&#12289;&#27668;&#27873;&#21644;&#22312;WSIs&#20013;&#30340;&#32452;&#32455;&#23398;&#26080;&#20851;&#34880;&#28082;&#31561;&#20116;&#31181;&#26174;&#33879;&#24037;&#20214;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#29420;&#31435;&#30340;&#20108;&#20803;DL&#27169;&#22411;&#20316;&#20026;&#19987;&#23478;&#26469;&#25429;&#25417;&#29305;&#23450;&#30340;&#24037;&#20214;&#24418;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#34701;&#21512;&#26426;&#21046;&#26469;&#38598;&#25104;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23545;&#26368;&#32456;&#30340;&#27010;&#29575;&#36827;&#34892;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07743v1 Announce Type: cross  Abstract: Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probabilit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#21644;LLMs&#23545;&#40784;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#30693;&#35782;&#65292;&#20805;&#20998;&#37322;&#25918;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;</title><link>https://arxiv.org/abs/2403.07300</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#25511;&#21046;&#39044;&#35757;&#32451;LLMs&#36827;&#34892;&#24191;&#20041;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#21644;LLMs&#23545;&#40784;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#30693;&#35782;&#65292;&#20805;&#20998;&#37322;&#25918;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26368;&#36817;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26377;&#38480;&#30340;&#26102;&#38388;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28608;&#22686;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#23558;LLMs&#24341;&#20837;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30452;&#25509;&#23558;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#26102;&#38388;&#21644;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#22266;&#26377;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#26102;&#38388;&#24207;&#21015;&#23545;&#40784;&#26694;&#26550;&#65292;&#31216;&#20026;LLaTA&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#20013;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#36755;&#20837;&#26080;&#20851;&#38745;&#24577;&#30693;&#35782;&#21644;&#36755;&#20837;&#30456;&#20851;&#21160;&#24577;&#30693;&#35782;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#20026;&#39044;&#27979;&#27169;&#22411;&#36171;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07300v1 Announce Type: cross  Abstract: Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models. However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization. Recently, with the surge of the Large Language Models (LLMs), several works have attempted to introduce LLMs into time series forecasting. Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data. In this work, we propose a novel Large Language Models and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time series forecasting challenge. Based on cross-modal knowledge distillation, the proposed method exploits both input-agnostic static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers the forecasting model with f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#37325;&#22797;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#38024;&#23545;&#19981;&#21516;&#24773;&#24418;&#25552;&#20986;&#20102;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#24322;&#36136;&#20195;&#29702;&#12289;&#21516;&#36136;&#20195;&#29702;&#21644;&#38750;&#21333;&#32431;&#35270;&#35282;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.07143</link><description>&lt;p&gt;
&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#30340;&#26032;&#35270;&#35282;&#65306;&#24322;&#36136;&#12289;&#21516;&#36136;&#12289;&#38750;&#21333;&#32431;&#35270;&#35282;&#20195;&#29702;&#21644;&#22242;&#38431;&#29983;&#20135;
&lt;/p&gt;
&lt;p&gt;
New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#37325;&#22797;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#38024;&#23545;&#19981;&#21516;&#24773;&#24418;&#25552;&#20986;&#20102;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#24322;&#36136;&#20195;&#29702;&#12289;&#21516;&#36136;&#20195;&#29702;&#21644;&#38750;&#21333;&#32431;&#35270;&#35282;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#37325;&#22797;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#12290; &#22996;&#25176;&#26041;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#37325;&#22797;&#20114;&#21160;&#23398;&#20064;&#26368;&#22823;&#21270;&#20854;&#25928;&#29992;&#30340;&#26368;&#20339;&#21512;&#21516;&#65292;&#32780;&#27809;&#26377;&#20851;&#20110;&#20195;&#29702;&#26041;&#31867;&#22411;&#65288;&#21363;&#20195;&#29702;&#26041;&#30340;&#25104;&#26412;&#21644;&#29983;&#20135;&#20989;&#25968;&#65289;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290; &#25105;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#24773;&#22659;&#65292;&#22996;&#25176;&#26041;&#22312;&#27599;&#19968;&#36718;&#19982;$\textit{&#21333;&#20010;}$&#20195;&#29702;&#26041;&#31614;&#35746;&#21512;&#21516;&#26102;&#65306;1. &#20195;&#29702;&#26041;&#26159;&#24322;&#36136;&#30340;&#65307;2. &#20195;&#29702;&#26041;&#26159;&#21516;&#36136;&#30340;&#65307;3. &#22996;&#25176;&#26041;&#19982;&#30456;&#21516;&#30340;&#20195;&#29702;&#26041;&#20114;&#21160;&#19988;&#35813;&#20195;&#29702;&#26041;&#26159;&#38750;&#21333;&#32431;&#30340;&#12290; &#25105;&#25552;&#20986;&#19981;&#21516;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#26469;&#35774;&#35745;&#27599;&#31181;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290; &#23545;&#20110;&#24322;&#36136;&#20195;&#29702;&#31867;&#22411;&#65292;&#25105;&#30830;&#23450;&#20102;&#19968;&#20010;&#26465;&#20214;&#65292;&#20801;&#35768;&#23558;&#38382;&#39064;&#30452;&#25509;&#31616;&#21270;&#20026;Lipschitz&#32769;&#34382;&#26426;&#38382;&#39064;&#12290; &#23545;&#20110;&#30456;&#21516;&#20195;&#29702;&#26041;&#65292;&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36870;&#21338;&#24328;&#35770;&#30340;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#26696;&#26469;&#23398;&#20064;&#26368;&#20339;&#21512;&#21516;&#12290; &#23545;&#20110;&#25112;&#30053;&#24615;&#38750;&#21333;&#32431;&#20195;&#29702;&#65292;&#25105;&#35774;&#35745;&#20102;&#19968;&#20010;&#20302;&#25112;&#30053;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07143v1 Announce Type: cross  Abstract: This work studies the repeated principal-agent problem from an online learning perspective. The principal's goal is to learn the optimal contract that maximizes her utility through repeated interactions, without prior knowledge of the agent's type (i.e., the agent's cost and production functions).   I study three different settings when the principal contracts with a $\textit{single}$ agent each round: 1. The agents are heterogeneous; 2. the agents are homogenous; 3. the principal interacts with the same agent and the agent is non-myopic. I present different approaches and techniques for designing learning algorithms in each setting. For heterogeneous agent types, I identify a condition that allows the problem to be reduced to Lipschitz bandits directly. For identical agents, I give a polynomial sample complexity scheme to learn the optimal contract based on inverse game theory. For strategic non-myopic agents, I design a low strategic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#27969;&#34433;&#32676;&#37319;&#26679;&#22120;&#65288;GFACS&#65289;&#65292;&#19968;&#31181;&#32467;&#21512;&#29983;&#25104;&#27969;&#32593;&#32476;&#19982;&#34433;&#32676;&#20248;&#21270;&#26041;&#27861;&#30340;&#31070;&#32463;&#24341;&#23548;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;ACO&#31639;&#27861;&#24182;&#19982;&#29305;&#23450;&#38382;&#39064;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07041</link><description>&lt;p&gt;
&#20351;&#29992;GFlowNets&#30340;&#34433;&#32676;&#37319;&#26679;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Ant Colony Sampling with GFlowNets for Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#27969;&#34433;&#32676;&#37319;&#26679;&#22120;&#65288;GFACS&#65289;&#65292;&#19968;&#31181;&#32467;&#21512;&#29983;&#25104;&#27969;&#32593;&#32476;&#19982;&#34433;&#32676;&#20248;&#21270;&#26041;&#27861;&#30340;&#31070;&#32463;&#24341;&#23548;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;ACO&#31639;&#27861;&#24182;&#19982;&#29305;&#23450;&#38382;&#39064;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#27969;&#34433;&#32676;&#37319;&#26679;&#22120;&#65288;GFACS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#31070;&#32463;&#24341;&#23548;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;GFACS &#23558;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#19982;&#34433;&#32676;&#20248;&#21270;&#65288;ACO&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;GFlowNets &#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#22312;&#32452;&#21512;&#31354;&#38388;&#20013;&#23398;&#20064;&#26500;&#36896;&#24615;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#22270;&#23454;&#20363;&#19978;&#25552;&#20379;&#20915;&#31574;&#21464;&#37327;&#30340;&#30693;&#24773;&#20808;&#39564;&#20998;&#24067;&#26469;&#22686;&#24378; ACO&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#25216;&#24039;&#32452;&#21512;&#65292;&#21253;&#25324;&#25628;&#32034;&#24341;&#23548;&#30340;&#23616;&#37096;&#25506;&#32034;&#12289;&#33021;&#37327;&#24402;&#19968;&#21270;&#21644;&#33021;&#37327;&#22609;&#24418;&#65292;&#20197;&#25552;&#39640; GFACS &#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GFACS &#22312;&#19971;&#20010;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447; ACO &#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#38382;&#39064;&#29305;&#23450;&#21551;&#21457;&#24335;&#26041;&#27861;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312; \url{https://github.com/ai4co/gfacs} &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07041v1 Announce Type: new  Abstract: This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS integrates generative flow networks (GFlowNets) with the ant colony optimization (ACO) methodology. GFlowNets, a generative model that learns a constructive policy in combinatorial spaces, enhance ACO by providing an informed prior distribution of decision variables conditioned on input graph instances. Furthermore, we introduce a novel combination of training tricks, including search-guided local exploration, energy normalization, and energy shaping to improve GFACS. Our experimental results demonstrate that GFACS outperforms baseline ACO algorithms in seven CO tasks and is competitive with problem-specific heuristics for vehicle routing problems. The source code is available at \url{https://github.com/ai4co/gfacs}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20445;&#35777;&#23433;&#20840;&#30340;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.05571</link><description>&lt;p&gt;
&#20855;&#26377;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20445;&#35777;&#23433;&#20840;&#30340;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Efficient and Guaranteed-Safe Non-Convex Trajectory Optimization with Constrained Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#20445;&#35777;&#23433;&#20840;&#30340;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#25193;&#25955;&#27169;&#22411;&#21644;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#36712;&#36857;&#20248;&#21270;&#38754;&#20020;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#21644;&#29615;&#22659;&#35774;&#32622;&#36896;&#25104;&#30340;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#26694;&#26550;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#21644;&#25968;&#20540;&#27714;&#35299;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#38750;&#20984;&#36712;&#36857;&#20248;&#21270;&#65292;&#30830;&#20445;&#35745;&#31639;&#25928;&#29575;&#21644;&#32422;&#26463;&#28385;&#36275;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#39069;&#22806;&#32422;&#26463;&#36829;&#21453;&#25439;&#22833;&#30340;&#32422;&#26463;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#23427;&#26088;&#22312;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#36817;&#20284;&#23616;&#37096;&#26368;&#20248;&#35299;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32422;&#26463;&#36829;&#21453;&#12290;&#28982;&#21518;&#29992;&#26679;&#26412;&#20316;&#20026;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#21021;&#22987;&#29468;&#27979;&#65292;&#26469;&#20248;&#21270;&#24182;&#24471;&#20986;&#26368;&#32456;&#35299;&#65292;&#24182;&#39564;&#35777;&#21487;&#34892;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05571v1 Announce Type: cross  Abstract: Trajectory optimization in robotics poses a challenging non-convex problem due to complex dynamics and environmental settings. Traditional numerical optimization methods are time-consuming in finding feasible solutions, whereas data-driven approaches lack safety guarantees for the output trajectories. In this paper, we introduce a general and fully parallelizable framework that combines diffusion models and numerical solvers for non-convex trajectory optimization, ensuring both computational efficiency and constraint satisfaction. A novel constrained diffusion model is proposed with an additional constraint violation loss for training. It aims to approximate the distribution of locally optimal solutions while minimizing constraint violations during sampling. The samples are then used as initial guesses for a numerical solver to refine and derive final solutions with formal verification of feasibility and optimality. Experimental evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#26576;&#20123;&#21464;&#37327;&#30001;&#23427;&#20204;&#30340;&#29238;&#33410;&#28857;&#21151;&#33021;&#20915;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21487;&#20197;&#20351;&#24471;&#19968;&#20123;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#21464;&#24471;&#21487;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#22240;&#26524;&#25928;&#24212;&#21487;&#35782;&#21035;&#24615;&#30340;&#24773;&#20917;&#19979;&#25490;&#38500;&#35266;&#27979;&#21040;&#30340;&#21151;&#33021;&#24615;&#21464;&#37327;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.04919</link><description>&lt;p&gt;
&#37492;&#21035;&#21151;&#33021;&#20381;&#36182;&#19979;&#30340;&#22240;&#26524;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Identifying Causal Effects Under Functional Dependencies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#26576;&#20123;&#21464;&#37327;&#30001;&#23427;&#20204;&#30340;&#29238;&#33410;&#28857;&#21151;&#33021;&#20915;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21487;&#20197;&#20351;&#24471;&#19968;&#20123;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#21464;&#24471;&#21487;&#35782;&#21035;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#22240;&#26524;&#25928;&#24212;&#21487;&#35782;&#21035;&#24615;&#30340;&#24773;&#20917;&#19979;&#25490;&#38500;&#35266;&#27979;&#21040;&#30340;&#21151;&#33021;&#24615;&#21464;&#37327;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#25928;&#24212;&#30340;&#35782;&#21035;&#65292;&#21463;&#20004;&#20010;&#25913;&#36827;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#22312;&#24050;&#30693;&#22240;&#26524;&#22270;&#20013;&#26576;&#20123;&#21464;&#37327;&#26159;&#30001;&#23427;&#20204;&#30340;&#29238;&#33410;&#28857;&#21151;&#33021;&#20915;&#23450;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#12290;&#31532;&#19968;&#65292;&#24403;&#26576;&#20123;&#21464;&#37327;&#26159;&#21151;&#33021;&#30340;&#26102;&#65292;&#19968;&#20010;&#19981;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#25928;&#24212;&#21487;&#33021;&#21464;&#24471;&#21487;&#35782;&#21035;&#12290;&#31532;&#20108;&#65292;&#21487;&#20197;&#25490;&#38500;&#35266;&#27979;&#26576;&#20123;&#21151;&#33021;&#21464;&#37327;&#32780;&#19981;&#24433;&#21709;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#30340;&#35266;&#27979;&#25968;&#25454;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#22522;&#20110;&#19968;&#20010;&#25490;&#38500;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#20174;&#22240;&#26524;&#22270;&#20013;&#21024;&#38500;&#21151;&#33021;&#21464;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#32467;&#26524;&#22240;&#26524;&#22270;&#20013;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#21253;&#25324;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04919v1 Announce Type: new  Abstract: We study the identification of causal effects, motivated by two improvements to identifiability which can be attained if one knows that some variables in a causal graph are functionally determined by their parents (without needing to know the specific functions). First, an unidentifiable causal effect may become identifiable when certain variables are functional. Second, certain functional variables can be excluded from being observed without affecting the identifiability of a causal effect, which may significantly reduce the number of needed variables in observational data. Our results are largely based on an elimination procedure which removes functional variables from a causal graph while preserving key properties in the resulting causal graph, including the identifiability of causal effects.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#23558;&#28176;&#36817;&#20110;&#19968;&#20010;&#24120;&#25968;&#20989;&#25968;&#65292;&#24182;&#38480;&#21046;&#20102;&#36825;&#20123;&#20998;&#31867;&#22120;&#30340;&#32479;&#19968;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03880</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#20960;&#20046;&#32943;&#23450;&#26159;&#28176;&#36817;&#24120;&#25968;
&lt;/p&gt;
&lt;p&gt;
Graph neural network outputs are almost surely asymptotically constant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03880
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#23558;&#28176;&#36817;&#20110;&#19968;&#20010;&#24120;&#25968;&#20989;&#25968;&#65292;&#24182;&#38480;&#21046;&#20102;&#36825;&#20123;&#20998;&#31867;&#22120;&#30340;&#32479;&#19968;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20027;&#35201;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;GNN&#30340;&#27010;&#29575;&#20998;&#31867;&#22120;&#22312;&#20174;&#26576;&#20010;&#38543;&#26426;&#22270;&#27169;&#22411;&#20013;&#32472;&#21046;&#30340;&#26356;&#22823;&#22270;&#19978;&#24212;&#29992;&#26102;&#39044;&#27979;&#22914;&#20309;&#28436;&#21464;&#65292;&#25552;&#20986;&#20102;GNN&#34920;&#36798;&#33021;&#21147;&#30340;&#26032;&#35282;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36755;&#20986;&#25910;&#25947;&#21040;&#19968;&#20010;&#24120;&#25968;&#20989;&#25968;&#65292;&#36825;&#20010;&#20989;&#25968;&#19978;&#38480;&#20102;&#36825;&#20123;&#20998;&#31867;&#22120;&#21487;&#20197;&#32479;&#19968;&#34920;&#36798;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#25910;&#25947;&#29616;&#35937;&#36866;&#29992;&#20110;&#38750;&#24120;&#24191;&#27867;&#30340;GNN&#31867;&#21035;&#65292;&#21253;&#25324;&#20808;&#36827;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;&#32858;&#21512;&#21253;&#25324;&#24179;&#22343;&#20540;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#36716;&#25442;&#22120;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#21508;&#31181;&#38543;&#26426;&#22270;&#27169;&#22411;&#65292;&#21253;&#25324;&#65288;&#31232;&#30095;&#30340;&#65289;Erd\H{o}s-R\'enyi&#27169;&#22411;&#21644;&#38543;&#26426;&#22359;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#39564;&#35777;&#36825;&#20123;&#21457;&#29616;&#65292;&#35266;&#23519;&#21040;&#25910;&#25947;&#29616;&#35937;&#24050;&#32463;&#22312;&#30456;&#23545;&#36866;&#20013;&#35268;&#27169;&#30340;&#22270;&#20013;&#26174;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03880v1 Announce Type: new  Abstract: Graph neural networks (GNNs) are the predominant architectures for a variety of learning tasks on graphs. We present a new angle on the expressive power of GNNs by studying how the predictions of a GNN probabilistic classifier evolve as we apply it on larger graphs drawn from some random graph model. We show that the output converges to a constant function, which upper-bounds what these classifiers can express uniformly. This convergence phenomenon applies to a very wide class of GNNs, including state of the art models, with aggregates including mean and the attention-based mechanism of graph transformers. Our results apply to a broad class of random graph models, including the (sparse) Erd\H{o}s-R\'enyi model and the stochastic block model. We empirically validate these findings, observing that the convergence phenomenon already manifests itself on graphs of relatively modest size.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{MultiDimSPCI}$&#30340;&#39034;&#24207;CP&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#26500;&#24314;&#39044;&#27979;&#21306;&#22495;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#39044;&#27979;&#21306;&#22495;&#21644;&#26377;&#25928;&#30340;&#35206;&#30422;&#12290;</title><link>https://arxiv.org/abs/2403.03850</link><description>&lt;p&gt;
&#21033;&#29992;&#26925;&#29699;&#38598;&#36827;&#34892;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#30340;&#21512;&#35268;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction for multi-dimensional time series by ellipsoidal sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03850
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{MultiDimSPCI}$&#30340;&#39034;&#24207;CP&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20013;&#26500;&#24314;&#39044;&#27979;&#21306;&#22495;&#65292;&#20855;&#26377;&#26356;&#23567;&#30340;&#39044;&#27979;&#21306;&#22495;&#21644;&#26377;&#25928;&#30340;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#35268;&#39044;&#27979;&#65288;CP&#65289;&#22240;&#20854;&#26080;&#38656;&#20551;&#35774;&#20998;&#24067;&#12289;&#19981;&#21463;&#27169;&#22411;&#38480;&#21046;&#19988;&#22312;&#29702;&#35770;&#19978;&#21487;&#38752;&#32780;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#12290;&#23545;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;CP&#26041;&#27861;&#19987;&#27880;&#20110;&#20026;&#21333;&#21464;&#37327;&#21709;&#24212;&#26500;&#24314;&#39044;&#27979;&#21306;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{MultiDimSPCI}$&#30340;&#39034;&#24207;CP&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#22810;&#20803;&#21709;&#24212;&#26500;&#24314;&#39044;&#27979;&#21306;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#21487;&#20132;&#25442;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#29615;&#22659;&#20013;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#26465;&#20214;&#35206;&#30422;&#38388;&#38553;&#30340;&#26377;&#38480;&#26679;&#26412;&#39640;&#27010;&#29575;&#30028;&#38480;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;$\texttt{MultiDimSPCI}$&#22312;&#21508;&#31181;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#19978;&#20445;&#25345;&#26377;&#25928;&#35206;&#30422;&#65292;&#21516;&#26102;&#20135;&#29983;&#27604;CP&#21644;&#38750;CP&#22522;&#32447;&#26356;&#23567;&#30340;&#39044;&#27979;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03850v1 Announce Type: cross  Abstract: Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound. For forecasting problems in supervised learning, most CP methods focus on building prediction intervals for univariate responses. In this work, we develop a sequential CP method called $\texttt{MultiDimSPCI}$ that builds prediction regions for a multivariate response, especially in the context of multivariate time series, which are not exchangeable. Theoretically, we estimate finite-sample high-probability bounds on the conditional coverage gap. Empirically, we demonstrate that $\texttt{MultiDimSPCI}$ maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#19968;&#20010;&#29305;&#23450;&#30340;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#31361;&#20986;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#65292;&#21516;&#26102;&#20855;&#26377;&#24378;&#22823;&#29983;&#25104;&#22120;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.02957</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#28176;&#36817;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#19968;&#20010;&#29305;&#23450;&#30340;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#31361;&#20986;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#65292;&#21516;&#26102;&#20855;&#26377;&#24378;&#22823;&#29983;&#25104;&#22120;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#21435;&#22122;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#35770;&#29702;&#35299;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#29305;&#23450;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65288;CME&#65289;&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;&#30740;&#31350;&#30340;&#22522;&#20110;DPM&#30340;&#21435;&#22122;&#22120;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19982;DPMs&#20849;&#20139;&#65292;&#20294;&#22312;&#35757;&#32451;&#21518;&#30340;&#36870;&#25512;&#29702;&#36807;&#31243;&#20013;&#20165;&#20256;&#36882;&#26465;&#20214;&#22343;&#20540;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#30340;&#29420;&#29305;&#35270;&#35282;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#36870;&#36807;&#31243;&#20013;&#20999;&#25442;&#37325;&#26032;&#37319;&#26679;&#30340;&#26041;&#24335;&#32487;&#25215;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#22120;&#12290;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#39564;&#35777;&#20102;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02957v1 Announce Type: new  Abstract: Diffusion probabilistic models (DPMs) have recently shown great potential for denoising tasks. Despite their practical utility, there is a notable gap in their theoretical understanding. This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps. The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training. We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off. The theoretical findings are validated by numerical results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#21387;&#32553;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#24863;&#30693;&#21644;&#20687;&#32032;&#32423;&#20934;&#30830;&#24230;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#33258;&#36866;&#24212;&#32534;&#30721;&#21644;&#32852;&#21512;&#22270;&#20687;-&#25991;&#26412;&#25439;&#22833;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#20687;&#32032;&#32423;&#20934;&#30830;&#24230;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02944</link><description>&lt;p&gt;
&#29992;&#25991;&#26412;&#24341;&#23548;&#32534;&#30721;&#30340;&#31070;&#32463;&#22270;&#20687;&#21387;&#32553;&#25216;&#26415;&#23454;&#29616;&#20687;&#32032;&#32423;&#21644;&#24863;&#30693;&#20934;&#30830;&#24230;&#30340;&#21452;&#37325;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#21387;&#32553;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#24863;&#30693;&#21644;&#20687;&#32032;&#32423;&#20934;&#30830;&#24230;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#33258;&#36866;&#24212;&#32534;&#30721;&#21644;&#32852;&#21512;&#22270;&#20687;-&#25991;&#26412;&#25439;&#22833;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#20687;&#32032;&#32423;&#20934;&#30830;&#24230;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#20102;&#25552;&#39640;&#37325;&#24314;&#22270;&#20687;&#24863;&#30693;&#36136;&#37327;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#20687;&#32032;&#32423;&#20934;&#30830;&#24230;&#26126;&#26174;&#38477;&#20302;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#21387;&#32553;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#24863;&#30693;&#21644;&#20687;&#32032;&#32423;&#20934;&#30830;&#24230;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#26694;&#26550;&#65292;&#20027;&#35201;&#36890;&#36807;&#25991;&#26412;&#33258;&#36866;&#24212;&#32534;&#30721;&#21644;&#32852;&#21512;&#22270;&#20687;-&#25991;&#26412;&#25439;&#22833;&#35757;&#32451;&#26469;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;&#36825;&#26679;&#19968;&#26469;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35299;&#30721;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#39640;&#29983;&#25104;&#22810;&#26679;&#24615;&#32780;&#38395;&#21517;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#20102;&#25991;&#26412;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#20687;&#32032;&#32423;&#21644;&#24863;&#30693;&#36136;&#37327;&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#29983;&#25104;&#30340;&#26631;&#39064;&#36824;&#26159;&#26426;&#22120;&#29983;&#25104;&#30340;&#26631;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02944v1 Announce Type: cross  Abstract: Recent advances in text-guided image compression have shown great potential to enhance the perceptual quality of reconstructed images. These methods, however, tend to have significantly degraded pixel-wise fidelity, limiting their practicality. To fill this gap, we develop a new text-guided image compression algorithm that achieves both high perceptual and pixel-wise fidelity. In particular, we propose a compression framework that leverages text information mainly by text-adaptive encoding and training with joint image-text loss. By doing so, we avoid decoding based on text-guided generative models -- known for high generative diversity -- and effectively utilize the semantic information of text at a global level. Experimental results on various datasets show that our method can achieve high pixel-level and perceptual quality, with either human- or machine-generated captions. In particular, our method outperforms all baselines in terms
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MMDA&#65289;&#35774;&#32622;&#65292;&#20801;&#35768;&#22810;&#26679;&#21270;&#30340;&#28304;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#28304;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.01582</link><description>&lt;p&gt;
&#22312;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#19978;
&lt;/p&gt;
&lt;p&gt;
On the Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MMDA&#65289;&#35774;&#32622;&#65292;&#20801;&#35768;&#22810;&#26679;&#21270;&#30340;&#28304;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#28304;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSFDA&#65289;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#28304;&#27169;&#22411;&#32780;&#38750;&#28304;&#25968;&#25454;&#65292;&#20174;&#22810;&#20010;&#33391;&#22909;&#26631;&#35760;&#30340;&#28304;&#22495;&#20256;&#36882;&#30693;&#35782;&#21040;&#19968;&#20010;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#12290;&#29616;&#26377;&#30340;MSFDA&#26041;&#27861;&#23616;&#38480;&#20110;&#27599;&#20010;&#28304;&#22495;&#20165;&#25552;&#20379;&#21333;&#19968;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#32479;&#19968;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;MSFDA&#35774;&#32622;&#65306;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#22810;&#28304;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MMDA&#65289;&#65292;&#20801;&#35768;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;&#22810;&#26679;&#21270;&#28304;&#27169;&#22411;&#65292;&#32780;&#19981;&#21463;&#23450;&#37327;&#38480;&#21046;&#12290;&#34429;&#28982;MMDA&#20855;&#26377;&#33391;&#22909;&#30340;&#28508;&#21147;&#65292;&#20294;&#21512;&#24182;&#22823;&#37327;&#28304;&#27169;&#22411;&#20250;&#22686;&#21152;&#21253;&#21547;&#19981;&#24819;&#35201;&#30340;&#27169;&#22411;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#31361;&#26174;&#20986;&#28304;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#35813;&#38382;&#39064;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20004;&#20010;&#22522;&#26412;&#36873;&#25321;&#21407;&#21017;&#65306;&#21487;&#36716;&#31227;&#24615;&#21407;&#21017;&#21644;&#22810;&#26679;&#24615;&#21407;&#21017;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#25972;&#21512;&#23427;&#20204;&#30340;&#36873;&#25321;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01582v1 Announce Type: new  Abstract: Multi-Source-Free Unsupervised Domain Adaptation (MSFDA) aims to transfer knowledge from multiple well-labeled source domains to an unlabeled target domain, using source models instead of source data. Existing MSFDA methods limited that each source domain provides only a single model, with a uniform structure. This paper introduces a new MSFDA setting: Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation (MMDA), allowing diverse source models with varying architectures, without quantitative restrictions. While MMDA holds promising potential, incorporating numerous source models poses a high risk of including undesired models, which highlights the source model selection problem. To address it, we first provide a theoretical analysis of this problem. We reveal two fundamental selection principles: transferability principle and diversity principle, and introduce a selection algorithm to integrate them. Then, considering the measu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;SUB-SAMPLE-Q&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#23616;&#37096;&#20195;&#29702;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.00222</link><description>&lt;p&gt;
&#23384;&#22312;&#22823;&#35268;&#27169;&#23616;&#37096;&#20195;&#29702;&#30340;&#20840;&#23616;&#20915;&#31574;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00222
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;SUB-SAMPLE-Q&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#23616;&#37096;&#20195;&#29702;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#22312;&#25351;&#25968;&#32423;&#21035;&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23384;&#22312;&#35768;&#22810;&#23616;&#37096;&#20195;&#29702;&#30340;&#20840;&#23616;&#20915;&#31574;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20840;&#23616;&#20915;&#31574;&#32773;&#20570;&#20986;&#24433;&#21709;&#25152;&#26377;&#23616;&#37096;&#20195;&#29702;&#30340;&#20915;&#31574;&#65292;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#26368;&#22823;&#21270;&#20840;&#23616;&#21644;&#23616;&#37096;&#20195;&#29702;&#22870;&#21169;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#25193;&#23637;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#29366;&#24577;/&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#21487;&#33021;&#20250;&#38543;&#20195;&#29702;&#25968;&#37327;&#25351;&#25968;&#22686;&#38271;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SUB-SAMPLE-Q&#31639;&#27861;&#65292;&#22312;&#27492;&#31639;&#27861;&#20013;&#65292;&#20840;&#23616;&#20195;&#29702;&#23545;$k\leq n$&#20010;&#23616;&#37096;&#20195;&#29702;&#36827;&#34892;&#23376;&#37319;&#26679;&#20197;&#22312;&#20165;&#25351;&#25968;&#20110;$k$&#30340;&#26102;&#38388;&#20869;&#35745;&#31639;&#20986;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19982;&#25351;&#25968;&#20110;$n$&#30340;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38543;&#30528;&#23376;&#37319;&#26679;&#20195;&#29702;&#25968;$k$&#30340;&#22686;&#21152;&#65292;&#23398;&#21040;&#30340;&#31574;&#30053;&#23558;&#25910;&#25947;&#20110;&#39034;&#24207;&#20026;$\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00222v1 Announce Type: new  Abstract: We study reinforcement learning for global decision-making in the presence of many local agents, where the global decision-maker makes decisions affecting all local agents, and the objective is to learn a policy that maximizes the rewards of both the global and the local agents. Such problems find many applications, e.g. demand response, EV charging, queueing, etc. In this setting, scalability has been a long-standing challenge due to the size of the state/action space which can be exponential in the number of agents. This work proposes the SUB-SAMPLE-Q algorithm where the global agent subsamples $k\leq n$ local agents to compute an optimal policy in time that is only exponential in $k$, providing an exponential speedup from standard methods that are exponential in $n$. We show that the learned policy converges to the optimal policy in the order of $\tilde{O}(1/\sqrt{k}+\epsilon_{k,m})$ as the number of sub-sampled agents $k$ increases, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.17826</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Prediction-Powered Ranking of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#26681;&#25454;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#36827;&#34892;&#25490;&#21517;--&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#21463;&#20154;&#31867;&#20559;&#22909;&#65292;&#37027;&#20040;&#23427;&#23601;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#26469;&#24357;&#21512;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
&lt;/p&gt;</description></item><item><title>TorchMD-Net 2.0&#26159;&#22312;&#31070;&#32463;&#32593;&#32476;&#21183;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;TensorNet&#31561;&#23574;&#31471;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#24471;&#22312;&#35745;&#31639;&#33021;&#37327;&#21644;&#21147;&#26102;&#33719;&#24471;&#20102;2&#21040;10&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.17660</link><description>&lt;p&gt;
TorchMD-Net 2.0: &#20998;&#23376;&#27169;&#25311;&#20013;&#30340;&#24555;&#36895;&#31070;&#32463;&#32593;&#32476;&#21183;
&lt;/p&gt;
&lt;p&gt;
TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17660
&lt;/p&gt;
&lt;p&gt;
TorchMD-Net 2.0&#26159;&#22312;&#31070;&#32463;&#32593;&#32476;&#21183;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#24341;&#20837;TensorNet&#31561;&#23574;&#31471;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#24471;&#22312;&#35745;&#31639;&#33021;&#37327;&#21644;&#21147;&#26102;&#33719;&#24471;&#20102;2&#21040;10&#20493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17660v1 &#22768;&#26126;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#22312;&#20998;&#23376;&#27169;&#25311;&#20013;&#23454;&#29616;&#35745;&#31639;&#36895;&#24230;&#12289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#36890;&#29992;&#36866;&#29992;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TorchMD-Net&#36719;&#20214;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#36825;&#26159;&#20174;&#20256;&#32479;&#21147;&#22330;&#36716;&#21521;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21183;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;TorchMD-Net&#28436;&#21464;&#25104;&#19968;&#20010;&#26356;&#20840;&#38754;&#21644;&#26356;&#22810;&#26679;&#21270;&#30340;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;TensorNet&#31561;&#23574;&#31471;&#20307;&#31995;&#32467;&#26500;&#12290;&#36890;&#36807;&#27169;&#22359;&#21270;&#35774;&#35745;&#26041;&#27861;&#23454;&#29616;&#20102;&#36825;&#31181;&#36716;&#21464;&#65292;&#40723;&#21169;&#31185;&#23398;&#30028;&#20869;&#37096;&#30340;&#23450;&#21046;&#24212;&#29992;&#12290;&#26368;&#26174;&#30528;&#30340;&#22686;&#24378;&#26159;&#22312;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#25913;&#36827;&#65292;&#22312;TensorNet&#27169;&#22411;&#30340;&#33021;&#37327;&#21644;&#21147;&#35745;&#31639;&#20013;&#23454;&#29616;&#20102;&#38750;&#24120;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#24615;&#33021;&#25552;&#21319;&#33539;&#22260;&#20174;&#21069;&#20960;&#20010;&#29256;&#26412;&#30340;2&#20493;&#21040;10&#20493;&#12290;&#20854;&#20182;&#22686;&#24378;&#21151;&#33021;&#21253;&#25324;&#39640;&#24230;&#20248;&#21270;&#30340;&#37051;&#23621;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17660v1 Announce Type: new  Abstract: Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular simulations has been a persistent challenge. This paper presents substantial advancements in the TorchMD-Net software, a pivotal step forward in the shift from conventional force fields to neural network-based potentials. The evolution of TorchMD-Net into a more comprehensive and versatile framework is highlighted, incorporating cutting-edge architectures such as TensorNet. This transformation is achieved through a modular design approach, encouraging customized applications within the scientific community. The most notable enhancement is a significant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2-fold to 10-fold over previous iterations. Other enhancements include highly optimized neighbor sear
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;</title><link>https://arxiv.org/abs/2402.17205</link><description>&lt;p&gt;
&#27979;&#37327;&#31070;&#32463;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;STEM&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Measuring Vision-Language STEM Skills of Neural Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#32467;&#21512;STEM&#65288;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#65289;&#30693;&#35782;&#26469;&#35299;&#20915;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#25361;&#25112;&#24615;&#38382;&#39064;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#23427;&#21253;&#25324;448&#39033;&#25216;&#33021;&#21644;1,073,146&#20010;&#36328;&#36234;&#25152;&#26377;STEM&#31185;&#30446;&#30340;&#38382;&#39064;&#12290;&#19982;&#36890;&#24120;&#20391;&#37325;&#20110;&#26816;&#39564;&#19987;&#23478;&#27700;&#24179;&#33021;&#21147;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#22522;&#30784;&#25216;&#33021;&#21644;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;GPT-3.5-Turbo&#65292;&#28155;&#21152;&#21040;&#25105;&#20204;&#30340;&#22522;&#20934;&#20013;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#36817;&#30340;&#27169;&#22411;&#36827;&#23637;&#21482;&#26377;&#21161;&#20110;&#25484;&#25569;&#25968;&#25454;&#38598;&#20013;&#38750;&#24120;&#26377;&#38480;&#25968;&#37327;&#30340;&#20302;&#24180;&#32423;&#25216;&#33021;&#65288;&#19977;&#24180;&#32423;&#20013;&#30340;2.5%&#65289;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#36828;&#27809;&#26377;&#23436;&#20840;&#25484;&#25569;&#23398;&#21069;&#25945;&#32946;&#38454;&#27573;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
&lt;/p&gt;</description></item><item><title>HyperCube&#32593;&#32476;&#36890;&#36807;&#29420;&#29305;&#30340;&#22240;&#24335;&#20998;&#35299;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#65292;&#25104;&#21151;&#23398;&#20064;&#20102;&#23545;&#31216;&#32676;&#30340;&#25805;&#20316;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24674;&#22797;&#23436;&#25972;&#25805;&#20316;&#34920;&#65292;&#24182;&#24418;&#25104;&#24191;&#20041;&#20613;&#37324;&#21494;&#22522;&#36827;&#34892;&#32676;&#21367;&#31215;&#12290;</title><link>https://arxiv.org/abs/2402.17002</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#21547;&#27491;&#20132;&#20559;&#32622;&#21457;&#29616;&#23545;&#31216;&#32676;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Discovering Symmetry Group Structures via Implicit Orthogonality Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17002
&lt;/p&gt;
&lt;p&gt;
HyperCube&#32593;&#32476;&#36890;&#36807;&#29420;&#29305;&#30340;&#22240;&#24335;&#20998;&#35299;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#65292;&#25104;&#21151;&#23398;&#20064;&#20102;&#23545;&#31216;&#32676;&#30340;&#25805;&#20316;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24674;&#22797;&#23436;&#25972;&#25805;&#20316;&#34920;&#65292;&#24182;&#24418;&#25104;&#24191;&#20041;&#20613;&#37324;&#21494;&#22522;&#36827;&#34892;&#32676;&#21367;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;HyperCube&#32593;&#32476;&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#25968;&#25454;&#20013;&#23545;&#31216;&#32676;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;&#12290;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#29420;&#29305;&#30340;&#22240;&#24335;&#20998;&#35299;&#26550;&#26500;&#65292;&#32467;&#21512;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#21521;&#23398;&#20064;&#27491;&#20132;&#34920;&#31034;&#28748;&#36755;&#20102;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#21033;&#29992;&#20102;&#34920;&#31034;&#29702;&#35770;&#30340;&#19968;&#20010;&#22522;&#26412;&#23450;&#29702;&#65292;&#21363;&#25152;&#26377;&#32039;&#33268;/&#26377;&#38480;&#32676;&#37117;&#21487;&#20197;&#30001;&#27491;&#20132;&#30697;&#38453;&#34920;&#31034;&#12290;HyperCube&#33021;&#22815;&#39640;&#25928;&#22320;&#20174;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#36890;&#29992;&#32676;&#25805;&#20316;&#65292;&#25104;&#21151;&#24674;&#22797;&#23436;&#25972;&#30340;&#25805;&#20316;&#34920;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#23398;&#20064;&#20986;&#30340;&#22240;&#32032;&#30452;&#25509;&#23545;&#24212;&#20110;&#24213;&#23618;&#32676;&#30340;&#31934;&#30830;&#30697;&#38453;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#22240;&#32032;&#25429;&#25417;&#21040;&#20102;&#32676;&#30340;&#23436;&#25972;&#19981;&#21487;&#32422;&#34920;&#31034;&#38598;&#21512;&#65292;&#24418;&#25104;&#20102;&#25191;&#34892;&#32676;&#21367;&#31215;&#30340;&#24191;&#20041;&#20613;&#37324;&#21494;&#22522;&#12290;&#22312;&#23545;&#32676;&#21644;&#38750;&#32676;&#31526;&#21495;&#25805;&#20316;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;HyperCube&#23637;&#31034;&#20102;10
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17002v1 Announce Type: new  Abstract: We introduce the HyperCube network, a novel approach for autonomously discovering symmetry group structures within data. The key innovation is a unique factorization architecture coupled with a novel regularizer that instills a powerful inductive bias towards learning orthogonal representations. This leverages a fundamental theorem of representation theory that all compact/finite groups can be represented by orthogonal matrices. HyperCube efficiently learns general group operations from partially observed data, successfully recovering complete operation tables. Remarkably, the learned factors correspond directly to exact matrix representations of the underlying group. Moreover, these factors capture the group's complete set of irreducible representations, forming the generalized Fourier basis for performing group convolutions. In extensive experiments with both group and non-group symbolic operations, HyperCube demonstrates a dramatic 10
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#24179;&#31283;&#36716;&#31227;&#21160;&#24577;&#30340;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;Dirichlet Markov&#38142;&#21644;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26469;&#35299;&#20915;&#21407;&#26377;&#27169;&#22411;&#25429;&#25417;&#26102;&#21464;&#36716;&#31227;&#21160;&#24577;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.16297</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#24179;&#31283;&#36716;&#31227;&#21160;&#24577;&#30340;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Poisson-Gamma Dynamical Systems with Non-Stationary Transition Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16297
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38750;&#24179;&#31283;&#36716;&#31227;&#21160;&#24577;&#30340;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;Dirichlet Markov&#38142;&#21644;&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#26469;&#35299;&#20915;&#21407;&#26377;&#27169;&#22411;&#25429;&#25417;&#26102;&#21464;&#36716;&#31227;&#21160;&#24577;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#35745;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#22240;&#20854;&#33021;&#22815;&#25512;&#26029;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#32467;&#26500;&#21644;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#32780;&#22791;&#21463;&#37325;&#35270;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22788;&#29702;&#22024;&#26434;&#21644;&#19981;&#23436;&#25972;&#30340;&#35745;&#25968;&#25968;&#25454;&#12290;&#22312;&#36825;&#20123;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#65292;&#27850;&#26494;-&#20285;&#39532;&#21160;&#21147;&#31995;&#32479;&#65288;PGDSs&#65289;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#35266;&#23519;&#21040;&#30340;&#35745;&#25968;&#24207;&#21015;&#24213;&#23618;&#21160;&#24577;&#30340;&#28436;&#21464;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#26368;&#26032;&#30340;PGDS&#22312;&#25429;&#25417;&#24120;&#35265;&#20110;&#23454;&#38469;&#35745;&#25968;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#21464;&#36716;&#31227;&#21160;&#24577;&#26041;&#38754;&#20173;&#26377;&#19981;&#36275;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24179;&#31283;PGDS&#65292;&#20801;&#35768;&#22522;&#30784;&#36716;&#31227;&#30697;&#38453;&#38543;&#26102;&#38388;&#28436;&#21464;&#65292;&#28436;&#21464;&#30340;&#36716;&#31227;&#30697;&#38453;&#30001;&#31934;&#24515;&#35774;&#35745;&#30340;Dirichlet Markov&#38142;&#24314;&#27169;&#12290;&#21033;&#29992;Dirichlet-Multinomial-Beta&#25968;&#25454;&#22686;&#24191;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#20849;&#36717;&#19988;&#39640;&#25928;&#30340;Gibbs&#37319;&#26679;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16297v1 Announce Type: cross  Abstract: Bayesian methodologies for handling count-valued time series have gained prominence due to their ability to infer interpretable latent structures and to estimate uncertainties, and thus are especially suitable for dealing with noisy and incomplete count data. Among these Bayesian models, Poisson-Gamma Dynamical Systems (PGDSs) are proven to be effective in capturing the evolving dynamics underlying observed count sequences. However, the state-of-the-art PGDS still falls short in capturing the time-varying transition dynamics that are commonly observed in real-world count time series. To mitigate this limitation, a non-stationary PGDS is proposed to allow the underlying transition matrices to evolve over time, and the evolving transition matrices are modeled by sophisticatedly-designed Dirichlet Markov chains. Leveraging Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed t
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.16075</link><description>&lt;p&gt;
&#22522;&#20110;&#25554;&#20540;&#30340;&#31574;&#30053;&#25193;&#25955;&#30340;&#34892;&#20026;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Behavioral Refinement via Interpolant-based Policy Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16075
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#21551;&#21160;&#25193;&#25955;&#26041;&#27861;&#26377;&#21161;&#20110;&#20811;&#26381;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#36890;&#36807;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26469;&#27169;&#20223;&#34892;&#20026;&#12290;&#26368;&#36817;&#65292;&#25317;&#26377;&#24314;&#27169;&#39640;&#32500;&#24230;&#21644;&#22810;&#27169;&#24577;&#20998;&#24067;&#33021;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#27169;&#20223;&#23398;&#20064;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23558;&#21160;&#20316;&#65288;&#25110;&#29366;&#24577;&#65289;&#20174;&#26631;&#20934;&#39640;&#26031;&#22122;&#22768;&#20013;&#25193;&#25955;&#26469;&#22609;&#36896;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35201;&#23398;&#20064;&#30340;&#30446;&#26631;&#31574;&#30053;&#36890;&#24120;&#19982;&#39640;&#26031;&#20998;&#24067;&#26174;&#33879;&#19981;&#21516;&#65292;&#36825;&#31181;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#22312;&#20351;&#29992;&#23569;&#37327;&#25193;&#25955;&#27493;&#39588;&#65288;&#20197;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65289;&#21644;&#26377;&#38480;&#25968;&#25454;&#19979;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#20174;&#27604;&#39640;&#26031;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#28304;&#22836;&#24320;&#22987;&#65292;&#21487;&#20197;&#20351;&#25193;&#25955;&#26041;&#27861;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#12289;&#19968;&#31181;&#26032;&#26041;&#27861;&#21644;&#23454;&#35777;&#21457;&#29616;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#20449;&#24687;&#37327;&#20016;&#23500;&#30340;&#28304;&#31574;&#30053;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;BRIDGER&#65292;&#21033;&#29992;&#20102;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16075v1 Announce Type: cross  Abstract: Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to overcome the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochast
&lt;/p&gt;</description></item><item><title>ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15898</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Information-based Transductive Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15898
&lt;/p&gt;
&lt;p&gt;
ITL&#26159;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#19990;&#30028;&#35774;&#32622;&#20013;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20197;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#24212;&#29992;&#20013;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20027;&#21160;&#23398;&#20064;&#25512;&#24191;&#21040;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#37319;&#26679;&#21463;&#38480;&#20110;&#21487;&#35775;&#38382;&#22495;&#30340;&#24773;&#20917;&#65292;&#32780;&#39044;&#27979;&#30446;&#26631;&#21487;&#33021;&#20301;&#20110;&#36825;&#20010;&#22495;&#20043;&#22806;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ITL&#65292;&#21363;&#22522;&#20110;&#20449;&#24687;&#30340;&#36716;&#23548;&#24335;&#23398;&#20064;&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20851;&#20110;&#25351;&#23450;&#39044;&#27979;&#30446;&#26631;&#30340;&#20449;&#24687;&#33719;&#21462;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ITL&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#20013;&#33719;&#24471;&#30340;&#26368;&#23567;&#21487;&#33021;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;ITL&#65306;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#21644;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;ITL&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15898v1 Announce Type: cross  Abstract: We generalize active learning to address real-world settings where sampling is restricted to an accessible region of the domain, while prediction targets may lie outside this region. To this end, we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified prediction targets. We show, under general regularity assumptions, that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. We demonstrate ITL in two key applications: Few-shot fine-tuning of large neural networks and safe Bayesian optimization, and in both cases, ITL significantly outperforms the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15163</link><description>&lt;p&gt;
&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#20013;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Studying the Impact of Stochasticity on the Evaluation of Deep Neural Networks for Forest-Fire Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#29992;&#20110;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#37326;&#28779;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#20004;&#31867;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#20998;&#31867;&#30340;&#25351;&#26631;&#65292;&#35780;&#20272;&#23545;&#35266;&#23519;&#22320;&#38754;&#30495;&#30456;&#65288;GT&#65289;&#30340;&#24544;&#23454;&#24230;&#65292;&#20197;&#21450;&#36866;&#24403;&#30340;&#24471;&#20998;&#35268;&#21017;&#65292;&#27979;&#35797;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#24230;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26862;&#26519;&#28779;&#28798;&#25968;&#25454;&#65292;&#31361;&#26174;&#20102;&#20256;&#32479;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#21487;&#35299;&#37322;&#30340;&#36866;&#29992;&#20110;&#38543;&#26426;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15163v1 Announce Type: cross  Abstract: This paper presents the first systematic study of the evaluation of Deep Neural Networks (DNNs) for discrete dynamical systems under stochastic assumptions, with a focus on wildfire prediction. We develop a framework to study the impact of stochasticity on two classes of evaluation metrics: classification-based metrics, which assess fidelity to observed ground truth (GT), and proper scoring rules, which test fidelity-to-statistic. Our findings reveal that evaluating for fidelity-to-statistic is a reliable alternative in highly stochastic scenarios. We extend our analysis to real-world wildfire data, highlighting limitations in traditional wildfire prediction evaluation methods, and suggest interpretable stochasticity-compatible alternatives.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#25928;&#38480;&#21046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26032;&#30340;&#29702;&#35770;&#19979;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.14878</link><description>&lt;p&gt;
&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#30340;&#33021;&#25928;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Energy-efficiency Limits on Training AI Systems using Learning-in-Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#25928;&#38480;&#21046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26032;&#30340;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14878v1 &#20844;&#21578;&#31867;&#22411;: cross &#25688;&#35201;: &#20869;&#23384;&#20013;&#23398;&#20064;&#65288;LIM&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#33539;Paradigm&#65292;&#26088;&#22312;&#20811;&#26381;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#20869;&#23384;&#29942;&#39048;&#12290;&#34429;&#28982;&#35745;&#31639;&#20110;&#20869;&#23384;&#65288;CIM&#65289;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25152;&#35859;&#30340;&#20869;&#23384;&#22681;&#38382;&#39064;&#65288;&#21363;&#30001;&#20110;&#37325;&#22797;&#20869;&#23384;&#35835;&#21462;&#35775;&#38382;&#32780;&#28040;&#32791;&#30340;&#33021;&#37327;&#65289;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#20197;&#35757;&#32451;&#25152;&#38656;&#30340;&#31934;&#24230;&#37325;&#22797;&#20869;&#23384;&#20889;&#20837;&#26102;&#28040;&#32791;&#30340;&#33021;&#37327;&#65288;&#26356;&#26032;&#22681;&#65289;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#32771;&#34385;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#20043;&#38388;&#20256;&#36755;&#20449;&#24687;&#26102;&#25152;&#28040;&#32791;&#30340;&#33021;&#37327;&#65288;&#25972;&#21512;&#22681;&#65289;&#12290;LIM&#33539;&#24335;&#25552;&#20986;&#65292;&#22914;&#26524;&#29289;&#29702;&#20869;&#23384;&#30340;&#33021;&#37327;&#23631;&#38556;&#34987;&#33258;&#36866;&#24212;&#35843;&#21046;&#65292;&#20351;&#24471;&#23384;&#20648;&#22120;&#26356;&#26032;&#21644;&#25972;&#21512;&#30340;&#21160;&#24577;&#19982;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;AI&#27169;&#22411;&#30340;Lyapunov&#21160;&#24577;&#30456;&#21305;&#37197;&#65292;&#37027;&#20040;&#36825;&#20123;&#29942;&#39048;&#20063;&#21487;&#20197;&#34987;&#20811;&#26381;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20351;&#29992;&#19981;&#21516;LIM&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#32791;&#30340;&#26032;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14878v1 Announce Type: cross  Abstract: Learning-in-memory (LIM) is a recently proposed paradigm to overcome fundamental memory bottlenecks in training machine learning systems. While compute-in-memory (CIM) approaches can address the so-called memory-wall (i.e. energy dissipated due to repeated memory read access) they are agnostic to the energy dissipated due to repeated memory writes at the precision required for training (the update-wall), and they don't account for the energy dissipated when transferring information between short-term and long-term memories (the consolidation-wall). The LIM paradigm proposes that these bottlenecks, too, can be overcome if the energy barrier of physical memories is adaptively modulated such that the dynamics of memory updates and consolidation match the Lyapunov dynamics of gradient-descent training of an AI model. In this paper, we derive new theoretical lower bounds on energy dissipation when training AI systems using different LIM app
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.14744</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22478;&#24066;&#23621;&#27665;&#65306;&#29992;&#20110;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#30340;LLM&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#25972;&#21512;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#23558;LLMs&#19982;&#30495;&#23454;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#26041;&#27861;&#21644;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#20195;&#29702;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#28789;&#27963;&#39640;&#25928;&#30340;&#20010;&#20154;&#31227;&#21160;&#29983;&#25104;&#12290;LLMs&#36890;&#36807;&#39640;&#25928;&#22788;&#29702;&#35821;&#20041;&#25968;&#25454;&#24182;&#22312;&#24314;&#27169;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#20379;&#22810;&#21151;&#33021;&#24615;, &#20811;&#26381;&#20102;&#20197;&#24448;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#22478;&#24066;&#27969;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#36843;&#20999;&#38656;&#27714;, &#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;: &#23558;LLMs&#19982;&#20016;&#23500;&#30340;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;, &#24320;&#21457;&#21487;&#38752;&#30340;&#27963;&#21160;&#29983;&#25104;&#31574;&#30053;, &#20197;&#21450;&#25506;&#32034;LLMs&#22312;&#22478;&#24066;&#31227;&#21160;&#20013;&#30340;&#24212;&#29992;&#12290;&#20854;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20195;&#29702;&#26694;&#26550;, &#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#20010;&#20307;&#27963;&#21160;&#27169;&#24335;&#21644;&#21160;&#26426;, &#21253;&#25324;&#23558;LLMs&#19982;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#25968;&#25454;&#23545;&#40784;&#30340;&#33258;&#27965;&#26041;&#27861;&#21644;&#21487;&#35299;&#37322;&#27963;&#21160;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#31574;&#30053;&#12290;&#22312;&#23454;&#39564;&#30740;&#31350;&#20013;, &#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#20840;&#38754;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14744v1 Announce Type: new  Abstract: This paper introduces a novel approach using Large Language Models (LLMs) integrated into an agent framework for flexible and efficient personal mobility generation. LLMs overcome the limitations of previous models by efficiently processing semantic data and offering versatility in modeling various tasks. Our approach addresses the critical need to align LLMs with real-world urban mobility data, focusing on three research questions: aligning LLMs with rich activity data, developing reliable activity generation strategies, and exploring LLM applications in urban mobility. The key technical contribution is a novel LLM agent framework that accounts for individual activity patterns and motivations, including a self-consistency approach to align LLMs with real-world activity data and a retrieval-augmented strategy for interpretable activity generation. In experimental studies, comprehensive validation is performed using real-world data. This 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.14528</link><description>&lt;p&gt;
ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24573;&#35270;&#20102;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#21407;&#22987;&#34892;&#20026;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#21160;&#20316;&#32500;&#24230;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35780;&#20272;&#35757;&#32451;&#36807;&#31243;&#20013;&#21508;&#31181;&#21407;&#22987;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#24863;&#30693;&#29109;&#39033;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24182;&#20248;&#20808;&#22788;&#29702;&#20855;&#26377;&#39640;&#28508;&#22312;&#24433;&#21709;&#30340;&#34892;&#21160;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38450;&#27490;&#23545;&#29305;&#23450;&#21407;&#22987;&#34892;&#20026;&#36807;&#24230;&#20851;&#27880;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65292;&#22312;&#36328;7&#20010;&#39046;&#22495;&#30340;29&#20010;&#19981;&#21516;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;&#30456;&#36739;&#20110;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#65292;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14528v1 Announce Type: cross  Abstract: The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which un
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#39318;&#27425;&#37319;&#29992;&#21464;&#21387;&#22120;&#39044;&#27979;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;CPLEX&#21644;LSTM&#12290;</title><link>https://arxiv.org/abs/2402.13380</link><description>&lt;p&gt;
&#36808;&#21521;&#21464;&#21387;&#22120;&#65306;&#29992;&#21464;&#21387;&#22120;&#24443;&#24213;&#25913;&#21464;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#39318;&#27425;&#37319;&#29992;&#21464;&#21387;&#22120;&#39044;&#27979;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;CPLEX&#21644;LSTM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#23481;&#37327;&#38480;&#21046;&#25209;&#37327;&#29983;&#20135;&#38382;&#39064;&#65288;CLSP&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#21033;&#29992;&#21464;&#21387;&#22120;&#26469;&#39044;&#27979;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#22788;&#29702;&#39034;&#24207;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#38750;&#24120;&#36866;&#21512;&#39044;&#27979;&#27599;&#20010;CLSP&#21608;&#26399;&#20013;&#34920;&#31034;&#29983;&#20135;&#35774;&#32622;&#20915;&#31574;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#12290;&#36825;&#20010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#21160;&#24577;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;CLSP&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#21518;&#22788;&#29702;&#21464;&#21387;&#22120;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;CPLEX&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13380v1 Announce Type: new  Abstract: In this study, we introduce an innovative deep learning framework that employs a transformer model to address the challenges of mixed-integer programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP). Our approach, to our knowledge, is the first to utilize transformers to predict the binary variables of a mixed-integer programming (MIP) problem. Specifically, our approach harnesses the encoder decoder transformer's ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the CLSP. This problem is inherently dynamic, and we need to handle sequential decision making under constraints. We present an efficient algorithm in which CLSP solutions are learned through a transformer neural network. The proposed post-processed transformer algorithm surpasses the state-of-the-art solver, CPLEX and Long Short-Term Memory (LSTM) in solution time
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;&#25361;&#25112;&#20102;&#31163;&#32447; IL &#20013;&#26377;&#38480;&#25903;&#25345;&#19987;&#23478;&#28436;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#21305;&#37197;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#30340;&#21344;&#29992;&#20998;&#24067;</title><link>https://arxiv.org/abs/2402.13147</link><description>&lt;p&gt;
SubIQ: &#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13147
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#36719; Q &#23398;&#20064;&#29992;&#20110;&#33719;&#24471;&#27425;&#20248;&#28436;&#31034;&#30340;&#31163;&#32447;&#27169;&#20223;&#25361;&#25112;&#20102;&#31163;&#32447; IL &#20013;&#26377;&#38480;&#25903;&#25345;&#19987;&#23478;&#28436;&#31034;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#20197;&#21305;&#37197;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#30340;&#21344;&#29992;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#65292;&#26088;&#22312;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#27169;&#20223;&#19987;&#23478;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#36827;&#19968;&#27493;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447; IL &#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22788;&#29702;&#20165;&#28085;&#30422;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#19968;&#23567;&#37096;&#20998;&#30340;&#19987;&#23478;&#28436;&#31034;&#30340;&#26377;&#38480;&#25903;&#25345;&#12290;&#25105;&#20204;&#32771;&#34385;&#31163;&#32447; IL&#65292;&#20854;&#20013;&#19987;&#23478;&#28436;&#31034;&#21463;&#21040;&#38480;&#21046;&#65292;&#20294;&#26159;&#30001;&#26356;&#22823;&#35268;&#27169;&#30340;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#34917;&#20805;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#29992;&#20110;&#27492;&#35774;&#32622;&#30340;&#31163;&#32447; IL &#26041;&#27861;&#22522;&#20110;&#34892;&#20026;&#20811;&#38534;&#25110;&#20998;&#24067;&#21305;&#37197;&#65292;&#20854;&#30446;&#30340;&#26159;&#23558;&#27169;&#20223;&#31574;&#30053;&#30340;&#21344;&#29992;&#20998;&#24067;&#19982;&#19987;&#23478;&#31574;&#30053;&#30340;&#21344;&#29992;&#20998;&#24067;&#21305;&#37197;&#12290;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#22240;&#20026;&#19987;&#23478;&#28436;&#31034;&#26377;&#38480;&#65292;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#20219;&#20309;&#21344;&#29992;&#20998;&#24067;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#27425;&#20248;&#28436;&#31034;&#38598;&#21512;&#35268;&#27169;&#26356;&#22823;&#65292;&#26377;&#24456;&#39640;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13147v1 Announce Type: cross  Abstract: We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that 
&lt;/p&gt;</description></item><item><title>&#24605;&#32500;&#38142;&#36171;&#20104;&#21464;&#21387;&#22120;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12875</link><description>&lt;p&gt;
&#24605;&#32500;&#38142;&#28608;&#21457;&#21464;&#21387;&#22120;&#35299;&#20915;&#22266;&#26377;&#20018;&#34892;&#38382;&#39064;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought Empowers Transformers to Solve Inherently Serial Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12875
&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#36171;&#20104;&#21464;&#21387;&#22120;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#19968;&#31995;&#21015;&#20013;&#38388;&#27493;&#39588;&#65292;&#21363;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31639;&#26415;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#20934;&#30830;&#24615;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;CoT&#32972;&#21518;&#30340;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#34920;&#36798;&#24615;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#23545;&#35299;&#30721;&#22120;&#19987;&#29992;&#21464;&#21387;&#22120;&#30340;CoT&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#22312;&#27010;&#24565;&#19978;&#65292;CoT&#36171;&#20104;&#27169;&#22411;&#25191;&#34892;&#22266;&#26377;&#20018;&#34892;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#31181;&#33021;&#21147;&#22312;&#21464;&#21387;&#22120;&#20013;&#32570;&#20047;&#65292;&#29305;&#21035;&#26159;&#24403;&#28145;&#24230;&#36739;&#20302;&#26102;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#24050;&#32463;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;CoT&#30340;&#24773;&#20917;&#19979;&#65292;&#20855;&#26377;&#26377;&#38480;&#31934;&#24230;$\mathsf{poly}(n)$&#23884;&#20837;&#23610;&#23544;&#30340;&#24658;&#23450;&#28145;&#24230;&#21464;&#21387;&#22120;&#21482;&#33021;&#22312;$\mathsf{TC}^0$&#20013;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20855;&#26377;&#24120;&#25968;&#20301;&#31934;&#24230;&#30340;&#24658;&#23450;&#28145;&#24230;&#21464;&#21387;&#22120;&#30340;&#26356;&#32039;&#23494;&#30340;&#34920;&#36798;&#24615;&#19978;&#30028;&#65292;&#23427;&#21482;&#33021;&#35299;&#20915;$\mathsf{AC}^0$&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12875v1 Announce Type: new  Abstract: Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\mathsf{poly}(n)$ embedding size can only solve problems in $\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\mathsf{AC}^0$, a 
&lt;/p&gt;</description></item><item><title>UniST&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#24615;&#12289;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20197;&#21450;&#20016;&#23500;&#30340;&#25513;&#30721;&#31574;&#30053;&#25104;&#21151;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.11838</link><description>&lt;p&gt;
UniST&#65306;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#25552;&#31034;&#22686;&#24378;&#22411;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11838
&lt;/p&gt;
&lt;p&gt;
UniST&#26159;&#19968;&#31181;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#35774;&#35745;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#36890;&#36807;&#28789;&#27963;&#24615;&#12289;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#20197;&#21450;&#20016;&#23500;&#30340;&#25513;&#30721;&#31574;&#30053;&#25104;&#21151;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11838v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#23545;&#20110;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#20132;&#36890;&#31649;&#29702;&#12289;&#36164;&#28304;&#20248;&#21270;&#21644;&#22478;&#24066;&#35268;&#21010;&#12290;&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#20854;&#20013;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#36328;&#22810;&#20010;&#39046;&#22495;&#30340;&#22810;&#20010;&#20219;&#21153;&#65292;&#20294;&#22478;&#24066;&#26102;&#31354;&#24314;&#27169;&#33853;&#21518;&#12290;&#29616;&#26377;&#30340;&#22478;&#24066;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#30340;&#26102;&#31354;&#22330;&#26223;&#36827;&#34892;&#23450;&#21046;&#65292;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#22823;&#37327;&#22495;&#20869;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#36890;&#29992;&#27169;&#22411;UniST&#12290;&#20511;&#37492;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;UniST&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#21462;&#24471;&#25104;&#21151;&#65306;(i) &#23545;&#19981;&#21516;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#29305;&#24449;&#30340;&#28789;&#27963;&#24615;&#65292;(ii) &#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#65292;&#37319;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#25513;&#30721;&#31574;&#30053;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#31354;&#38388;&#26102;&#38388;&#20851;&#31995;&#65292;(iii) &#26102;&#31354;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11838v1 Announce Type: new  Abstract: Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning. Although pretrained foundation models for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind. Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data. In this work, we propose a universal model, UniST, for urban spatio-temporal prediction. Drawing inspiration from large language models, UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal know
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11472</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#65306;DDIPrompt
&lt;/p&gt;
&lt;p&gt;
DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11472
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#33647;&#29289;&#20998;&#23376;&#20869;&#37096;&#21644;&#20043;&#38388;&#21407;&#23376;&#21644;&#21151;&#33021;&#22242;&#20043;&#38388;&#22797;&#26434;&#20851;&#32852;&#26041;&#38754;&#30340;&#29087;&#32451;&#34920;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#65288;DDI&#65289;&#26041;&#38754;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#21046;&#32422;&#65306;&#65288;1&#65289;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#20294;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#26576;&#20123;&#30456;&#20114;&#20316;&#29992;&#34987;&#24191;&#27867;&#22320;&#20302;&#20272;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#23545;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;DDI&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#65288;2&#65289;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26159;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#65292;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#24448;&#24448;&#24573;&#35270;&#25110;&#30740;&#31350;&#19981;&#36275;&#30340;&#32597;&#35265;&#20294;&#28508;&#22312;&#20851;&#38190;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DDIPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#26368;&#36817;&#22270;&#25552;&#31034;&#23398;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#33391;&#26041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#26631;&#27880;&#20989;&#25968;&#30340;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#22686;&#24378;&#20551;&#35774;&#31867;&#19978;&#30340;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10445</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#26631;&#27880;&#20989;&#25968;&#30340;&#21327;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning with Different Labeling Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10445
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#26631;&#27880;&#20989;&#25968;&#30340;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#22686;&#24378;&#20551;&#35774;&#31867;&#19978;&#30340;&#39640;&#25928;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181; Collaborative PAC Learning &#30340;&#21464;&#20307;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26088;&#22312;&#23398;&#20064;&#27599;&#20010;$n$&#20010;&#25968;&#25454;&#20998;&#24067;&#30340;&#20934;&#30830;&#20998;&#31867;&#22120;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20174;&#23427;&#20204;&#24635;&#20849;&#25277;&#21462;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#19982;&#36890;&#24120;&#30340;&#21327;&#20316;&#23398;&#20064;&#35774;&#32622;&#19981;&#21516;&#65292;&#19981;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#21516;&#26102;&#23545;&#25152;&#26377;&#20998;&#24067;&#20934;&#30830;&#30340;&#21333;&#19968;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#25968;&#25454;&#20998;&#24067;&#28385;&#36275;&#36739;&#24369;&#30340;&#21487;&#23454;&#29616;&#24615;&#20551;&#35774;&#26102;&#65292;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#20551;&#35774;&#31867;&#30340;&#19968;&#20010;&#33258;&#28982;&#22686;&#24378;&#65292;&#20998;&#26512;&#20381;&#36182;&#20110;&#23545;&#35813;&#22686;&#24378;&#31867;&#30340;VC&#32500;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10445v1 Announce Type: new  Abstract: We study a variant of Collaborative PAC Learning, in which we aim to learn an accurate classifier for each of the $n$ data distributions, while minimizing the number of samples drawn from them in total. Unlike in the usual collaborative learning setup, it is not assumed that there exists a single classifier that is simultaneously accurate for all distributions.   We show that, when the data distributions satisfy a weaker realizability assumption, sample-efficient learning is still feasible. We give a learning algorithm based on Empirical Risk Minimization (ERM) on a natural augmentation of the hypothesis class, and the analysis relies on an upper bound on the VC dimension of this augmented class.   In terms of the computational efficiency, we show that ERM on the augmented hypothesis class is NP-hard, which gives evidence against the existence of computationally efficient learners in general. On the positive side, for two special cases, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.09345</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#22870;&#21169;&#24314;&#27169;&#26469;&#20943;&#36731;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reward Hacking via Information-Theoretic Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20013;&#30340;&#25104;&#21151;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#38754;&#65292;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#20110;&#22870;&#21169;&#24314;&#27169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;&#22870;&#21169;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#21644;&#40065;&#26834;&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#31216;&#20026;InfoRM&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#26469;&#36807;&#28388;&#20986;&#19981;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#36807;&#24230;&#20248;&#21270;&#19982;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23558;InfoRM&#20316;&#20026;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#36807;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09345v1 Announce Type: cross Abstract: Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quant
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.08957</link><description>&lt;p&gt;
MUSTARD&#65306;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#23398;&#25512;&#29702;&#21644;&#23450;&#29702;&#35777;&#26126;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#20005;&#26684;&#21644;&#24418;&#24335;&#21270;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#23427;&#20204;&#26159;&#25506;&#32034;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#21560;&#24341;&#39046;&#22495;&#65292;&#20294;&#20173;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22914;Chain-of-Thought&#65288;CoT&#65289;&#25581;&#31034;&#20102;&#20013;&#38388;&#27493;&#39588;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36880;&#27493;&#27880;&#37322;&#38656;&#35201;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#65292;&#23548;&#33268;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#30340;&#35757;&#32451;&#27493;&#39588;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20027;&#23548;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;&#12290;MUSTARD&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#21512;&#25104;&#25968;&#25454;&#65306;&#65288;1&#65289;&#23427;&#38543;&#26426;&#36873;&#25321;&#20960;&#20010;&#25968;&#23398;&#27010;&#24565;&#20316;&#20026;&#38382;&#39064;&#30340;&#31867;&#21035;&#12290;&#65288;2&#65289;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#36873;&#23450;&#30340;&#27010;&#24565;&#25552;&#31034;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#38382;&#39064;&#21644;&#23427;&#20204;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20391;&#20449;&#24687;&#20013;&#30340;Stackelberg&#21338;&#24328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#20013;&#29609;&#23478;&#20043;&#38388;&#20449;&#24687;&#20132;&#27969;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21518;&#24724;&#26368;&#23567;&#21270;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.08576</link><description>&lt;p&gt;
&#20391;&#20449;&#24687;&#20013;&#30340;Stackelberg&#21338;&#24328;&#20013;&#30340;&#21518;&#24724;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Regret Minimization in Stackelberg Games with Side Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08576
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20391;&#20449;&#24687;&#20013;&#30340;Stackelberg&#21338;&#24328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#29616;&#23454;&#20013;&#29609;&#23478;&#20043;&#38388;&#20449;&#24687;&#20132;&#27969;&#19981;&#20805;&#20998;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#21518;&#24724;&#26368;&#23567;&#21270;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#22522;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;Stackelberg&#21338;&#24328;&#26159;&#19968;&#20010;&#21452;&#20154;&#21338;&#24328;&#65292;&#20854;&#20013;&#39046;&#23548;&#32773;&#25215;&#35834;&#19968;&#31181;&#65288;&#28151;&#21512;&#65289;&#31574;&#30053;&#65292;&#36861;&#38543;&#32773;&#20570;&#20986;&#26368;&#20339;&#21453;&#24212;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;Stackelberg&#21338;&#24328;&#31639;&#27861;&#26159;&#31639;&#27861;&#21338;&#24328;&#35770;&#30340;&#26368;&#22823;&#25104;&#21151;&#20043;&#19968;&#65292;&#22240;&#20026;Stackelberg&#21338;&#24328;&#30340;&#31639;&#27861;&#24050;&#32463;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#39046;&#22495;&#20013;&#34987;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22330;&#23433;&#20840;&#12289;&#21453;&#30423;&#29454;&#21644;&#32593;&#32476;&#29359;&#32618;&#39044;&#38450;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#26410;&#33021;&#32771;&#34385;&#21040;&#27599;&#20010;&#29609;&#23478;&#21487;&#29992;&#30340;&#39069;&#22806;&#20449;&#24687;&#65288;&#20363;&#22914;&#20132;&#36890;&#27169;&#24335;&#65292;&#22825;&#27668;&#26465;&#20214;&#65292;&#32593;&#32476;&#25317;&#22622;&#65289;&#65292;&#36825;&#26159;&#29616;&#23454;&#30340;&#26174;&#33879;&#29305;&#24449;&#65292;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;&#21040;&#20004;&#20010;&#29609;&#23478;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#24773;&#20917;&#24418;&#24335;&#21270;&#20026;&#24102;&#26377;&#20391;&#20449;&#24687;&#30340;Stackelberg&#21338;&#24328;&#65292;&#20854;&#20013;&#20004;&#20010;&#29609;&#23478;&#22312;&#36827;&#34892;&#28216;&#25103;&#20043;&#21069;&#37117;&#35266;&#23519;&#21040;&#19968;&#20010;&#22806;&#37096;&#29615;&#22659;&#12290;&#28982;&#21518;&#65292;&#39046;&#23548;&#32773;&#25215;&#35834;&#19968;&#31181;&#65288;&#21487;&#33021;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#65289;&#31574;&#30053;&#65292;&#36861;&#38543;&#32773;&#23545;&#39046;&#23548;&#32773;&#30340;&#31574;&#30053;&#21644;&#19978;&#19979;&#25991;&#37117;&#20570;&#20986;&#26368;&#20339;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In its most basic form, a Stackelberg game is a two-player game in which a leader commits to a (mixed) strategy, and a follower best-responds. Stackelberg games are perhaps one of the biggest success stories of algorithmic game theory over the last decade, as algorithms for playing in Stackelberg games have been deployed in many real-world domains including airport security, anti-poaching efforts, and cyber-crime prevention. However, these algorithms often fail to take into consideration the additional information available to each player (e.g. traffic patterns, weather conditions, network congestion), a salient feature of reality which may significantly affect both players' optimal strategies. We formalize such settings as Stackelberg games with side information, in which both players observe an external context before playing. The leader then commits to a (possibly context-dependent) strategy, and the follower best-responds to both the leader's strategy and the context. We focus on t
&lt;/p&gt;</description></item><item><title>&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.08193</link><description>&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08193
&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#65288;GEnBP&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#32500;&#31995;&#32479;&#20013;&#39640;&#25928;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#31561;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#33021;&#26377;&#25928;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#25512;&#26029;&#20173;&#28982;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#26031;&#27169;&#22411;&#38598;&#25104;&#32622;&#20449;&#20256;&#25773;&#65288;GEnBP&#65289;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#38598;&#25104;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21644;&#39640;&#26031;&#32622;&#20449;&#20256;&#25773;&#65288;GaBP&#65289;&#26041;&#27861;&#30340;&#32467;&#21512;&#12290;GEnBP&#36890;&#36807;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20256;&#36882;&#20302;&#31209;&#26412;&#22320;&#20449;&#24687;&#26469;&#26356;&#26032;&#38598;&#25104;&#27169;&#22411;&#12290;&#36825;&#31181;&#32452;&#21512;&#32487;&#25215;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26377;&#21033;&#29305;&#24615;&#12290;&#38598;&#25104;&#25216;&#26415;&#20351;&#24471;GEnBP&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#29366;&#24577;&#12289;&#21442;&#25968;&#21644;&#22797;&#26434;&#30340;&#12289;&#22024;&#26434;&#30340;&#40657;&#31665;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#22270;&#27169;&#22411;&#32467;&#26500;&#20013;&#20351;&#29992;&#26412;&#22320;&#20449;&#24687;&#30830;&#20445;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#35745;&#31639;&#65292;&#24182;&#33021;&#39640;&#25928;&#22320;&#22788;&#29702;&#22797;&#26434;&#30340;&#20381;&#36182;&#32467;&#26500;&#12290;&#24403;&#38598;&#25104;&#22823;&#23567;&#36828;&#23567;&#20110;&#25512;&#26029;&#32500;&#24230;&#26102;&#65292;GEnBP&#29305;&#21035;&#26377;&#20248;&#21183;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#31354;&#26102;&#24314;&#27169;&#12289;&#22270;&#20687;&#22788;&#29702;&#21644;&#29289;&#29702;&#27169;&#22411;&#21453;&#28436;&#31561;&#39046;&#22495;&#32463;&#24120;&#20986;&#29616;&#12290;GEnBP&#21487;&#20197;&#24212;&#29992;&#20110;&#19968;&#33324;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient inference in high-dimensional models remains a central challenge in machine learning. This paper introduces the Gaussian Ensemble Belief Propagation (GEnBP) algorithm, a fusion of the Ensemble Kalman filter and Gaussian belief propagation (GaBP) methods. GEnBP updates ensembles by passing low-rank local messages in a graphical model structure. This combination inherits favourable qualities from each method. Ensemble techniques allow GEnBP to handle high-dimensional states, parameters and intricate, noisy, black-box generation processes. The use of local messages in a graphical model structure ensures that the approach is suited to distributed computing and can efficiently handle complex dependence structures. GEnBP is particularly advantageous when the ensemble size is considerably smaller than the inference dimension. This scenario often arises in fields such as spatiotemporal modelling, image processing and physical model inversion. GEnBP can be applied to general problem s
&lt;/p&gt;</description></item><item><title>ClusterTabNet&#26159;&#19968;&#31181;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#34920;&#26684;&#32467;&#26500;&#20026;&#21333;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#22270;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20854;&#37051;&#25509;&#30697;&#38453;&#65292;&#23454;&#29616;&#23545;&#34920;&#26684;&#30340;&#26816;&#27979;&#21644;&#32467;&#26500;&#35782;&#21035;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;ClusterTabNet&#20855;&#26377;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07502</link><description>&lt;p&gt;
ClusterTabNet: &#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#26816;&#27979;&#21644;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#30340;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ClusterTabNet: Supervised clustering method for table detection and table structure recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07502
&lt;/p&gt;
&lt;p&gt;
ClusterTabNet&#26159;&#19968;&#31181;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#34920;&#26684;&#32467;&#26500;&#20026;&#21333;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#22270;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20854;&#37051;&#25509;&#30697;&#38453;&#65292;&#23454;&#29616;&#23545;&#34920;&#26684;&#30340;&#26816;&#27979;&#21644;&#32467;&#26500;&#35782;&#21035;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;ClusterTabNet&#20855;&#26377;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#24212;&#29992;&#20110;&#26816;&#27979;&#21644;&#35782;&#21035;OCR&#36755;&#20986;&#20013;&#30340;&#34920;&#26684;&#12290;&#25105;&#20204;&#23558;&#34920;&#26684;&#32467;&#26500;&#33258;&#19979;&#32780;&#19978;&#35299;&#37322;&#20026;&#19968;&#32452;&#21333;&#35789;&#23545;&#20043;&#38388;&#30340;&#20851;&#31995;&#22270;&#65288;&#23646;&#20110;&#21516;&#19968;&#34892;&#12289;&#21015;&#12289;&#26631;&#39064;&#20197;&#21450;&#21516;&#19968;&#34920;&#26684;&#65289;&#65292;&#24182;&#20351;&#29992;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#39044;&#27979;&#20854;&#37051;&#25509;&#30697;&#38453;&#12290;&#25105;&#20204;&#22312;PubTables-1M&#25968;&#25454;&#38598;&#20197;&#21450;PubTabNet&#21644;FinTabNet&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#27861;&#65288;&#22914;DETR&#21644;Faster R-CNN&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38656;&#35201;&#26356;&#23567;&#30340;&#27169;&#22411;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel deep-learning-based method to cluster words in documents which we apply to detect and recognize tables given the OCR output. We interpret table structure bottom-up as a graph of relations between pairs of words (belonging to the same row, column, header, as well as to the same table) and use a transformer encoder model to predict its adjacency matrix. We demonstrate the performance of our method on the PubTables-1M dataset as well as PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art detection methods such as DETR and Faster R-CNN, our method achieves similar or better accuracy, while requiring a significantly smaller model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07204</link><description>&lt;p&gt;
&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#22478;&#24066;&#34892;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#12290;OUIP&#19982;&#20256;&#32479;&#34892;&#31243;&#35268;&#21010;&#19981;&#21516;&#65292;&#20256;&#32479;&#35268;&#21010;&#38480;&#21046;&#20102;&#29992;&#25143;&#34920;&#36798;&#26356;&#35814;&#32454;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#23454;&#26102;&#20449;&#24687;&#12289;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#21644;&#19981;&#36275;&#30340;&#31354;&#38388;&#24847;&#35782;&#65292;&#23427;&#20204;&#26080;&#27861;&#29420;&#31435;&#22320;&#25552;&#20379;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ItiNera&#30340;OUIP&#31995;&#32479;&#65292;&#23558;&#31354;&#38388;&#20248;&#21270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#26356;&#26032;&#20852;&#36259;&#28857;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#28857;&#25968;&#25454;&#24211;&#12290;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#35831;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#21327;&#21516;&#23454;&#29616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21512;&#20316;&#21338;&#24328;&#20013;&#20005;&#26684;&#20984;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#39044;&#26399;&#26680;&#24515;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{Common-Points-Picking}&#30340;&#31639;&#27861;&#65292;&#22312;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#32473;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#19968;&#20010;&#31283;&#23450;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.07067</link><description>&lt;p&gt;
&#23398;&#20064;&#20005;&#26684;&#20984;&#30340;&#38543;&#26426;&#21512;&#20316;&#21338;&#24328;&#30340;&#39044;&#26399;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Learning the Expected Core of Strictly Convex Stochastic Cooperative Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21512;&#20316;&#21338;&#24328;&#20013;&#20005;&#26684;&#20984;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#39044;&#26399;&#26680;&#24515;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{Common-Points-Picking}&#30340;&#31639;&#27861;&#65292;&#22312;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#32473;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#19968;&#20010;&#31283;&#23450;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#20998;&#37197;&#65292;&#20063;&#31216;&#20026;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#65292;&#26159;&#32463;&#27982;&#23398;&#12289;&#24037;&#31243;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#20027;&#39064;&#12290;&#20449;&#29992;&#20998;&#37197;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#26159;&#26680;&#24515;&#65292;&#23427;&#26159;&#31283;&#23450;&#20998;&#37197;&#30340;&#38598;&#21512;&#65292;&#20854;&#20013;&#27809;&#26377;&#20195;&#29702;&#26377;&#21160;&#26426;&#20174;&#22823;&#32852;&#30431;&#20013;&#20559;&#31163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#38543;&#26426;&#21512;&#20316;&#21338;&#24328;&#30340;&#31283;&#23450;&#20998;&#37197;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22870;&#21169;&#20989;&#25968;&#34987;&#25551;&#36848;&#20026;&#20855;&#26377;&#26410;&#30693;&#20998;&#24067;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#36820;&#22238;&#26597;&#35810;&#32852;&#30431;&#30340;&#38543;&#26426;&#22870;&#21169;&#30340;oracle&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#39044;&#26399;&#26680;&#24515;&#65292;&#21363;&#22312;&#26399;&#26395;&#19978;&#31283;&#23450;&#30340;&#20998;&#37197;&#38598;&#21512;&#12290;&#22312;&#20005;&#26684;&#20984;&#21338;&#24328;&#31867;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{Common-Points-Picking}&#30340;&#31639;&#27861;&#65292;&#23427;&#22312;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#32473;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#19968;&#20010;&#31283;&#23450;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20998;&#26512;&#28041;&#21450;&#21040;&#20984;&#20960;&#20309;&#20013;&#30340;&#20960;&#20010;&#26032;&#32467;&#26524;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Reward allocation, also known as the credit assignment problem, has been an important topic in economics, engineering, and machine learning. An important concept in credit assignment is the core, which is the set of stable allocations where no agent has the motivation to deviate from the grand coalition. In this paper, we consider the stable allocation learning problem of stochastic cooperative games, where the reward function is characterised as a random variable with an unknown distribution. Given an oracle that returns a stochastic reward for an enquired coalition each round, our goal is to learn the expected core, that is, the set of allocations that are stable in expectation. Within the class of strictly convex games, we present an algorithm named \texttt{Common-Points-Picking} that returns a stable allocation given a polynomial number of samples, with high probability. The analysis of our algorithm involves the development of several new results in convex geometry, including an e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#26465;&#20214;&#27491;&#23450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#12290;&#36825;&#20026;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#24102;&#26469;&#20102;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.06955</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Training dynamics in Physics-Informed Neural Networks with feature mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#26465;&#20214;&#27491;&#23450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#12290;&#36825;&#20026;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#24102;&#26469;&#20102;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26631;&#24535;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#21464;&#20307;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26469;&#33258;&#26356;&#24191;&#27867;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30740;&#31350;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#32463;&#39564;&#24615;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#26469;&#30740;&#31350;&#24102;&#26377;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;PINNs&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#26465;&#20214;&#27491;&#23450;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#20316;&#20026;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#38598;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#23454;&#29616;&#65292;&#24182;&#21463;&#30410;&#20110;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine learning approach for solving Partial Differential Equations (PDEs). Although its variants have achieved significant progress, the empirical success of utilising feature mapping from the wider Implicit Neural Representations studies has been substantially neglected. We investigate the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel, which sheds light on the convergence and generalisation of the model. We also show the inadequacy of commonly used Fourier-based feature mapping in some scenarios and propose the conditional positive definite Radial Basis Function as a better alternative. The empirical results reveal the efficacy of our method in diverse forward and inverse problem sets. This simple technique can be easily implemented in coordinate input networks and benefits the broad PINNs research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#25968;&#25454;&#28246;&#20013;&#30340;&#25968;&#25454;&#21457;&#29616;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#30528;&#37325;&#20110;&#34920;&#26684;&#22686;&#24378;&#65292;&#25552;&#20986;&#20102;&#20934;&#30830;&#26816;&#32034;&#36830;&#25509;&#20505;&#36873;&#20154;&#30340;&#37325;&#35201;&#24615;&#21644;&#31616;&#21333;&#21512;&#24182;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06282</link><description>&lt;p&gt;
&#33719;&#21462;&#12289;&#21512;&#24182;&#12289;&#39044;&#27979;&#65306;&#36890;&#36807;&#25968;&#25454;&#28246;&#22686;&#24378;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
Retrieve, Merge, Predict: Augmenting Tables with Data Lakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#25968;&#25454;&#28246;&#20013;&#30340;&#25968;&#25454;&#21457;&#29616;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#30528;&#37325;&#20110;&#34920;&#26684;&#22686;&#24378;&#65292;&#25552;&#20986;&#20102;&#20934;&#30830;&#26816;&#32034;&#36830;&#25509;&#20505;&#36873;&#20154;&#30340;&#37325;&#35201;&#24615;&#21644;&#31616;&#21333;&#21512;&#24182;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#25968;&#25454;&#28246;&#20013;&#30340;&#25968;&#25454;&#21457;&#29616;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#37325;&#28857;&#26159;&#32473;&#23450;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#26684;&#22686;&#24378;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#20010;&#20027;&#35201;&#27493;&#39588;&#20013;&#20351;&#29992;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#26816;&#32034;&#21487;&#36830;&#25509;&#30340;&#34920;&#26684;&#12289;&#21512;&#24182;&#20449;&#24687;&#21644;&#39044;&#27979;&#32467;&#26524;&#34920;&#26684;&#12290;&#20316;&#20026;&#25968;&#25454;&#28246;&#65292;&#26412;&#25991;&#20351;&#29992;&#20102;YADL&#65288;&#21478;&#19968;&#20010;&#25968;&#25454;&#28246;&#65289;-&#25105;&#20204;&#24320;&#21457;&#30340;&#19968;&#31181;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#27492;&#25968;&#25454;&#21457;&#29616;&#20219;&#21153;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;-&#21644;Open Data US&#65292;&#19968;&#20010;&#34987;&#24341;&#29992;&#30340;&#30495;&#23454;&#25968;&#25454;&#28246;&#12290;&#36890;&#36807;&#23545;&#36825;&#20004;&#20010;&#25968;&#25454;&#28246;&#30340;&#31995;&#32479;&#24615;&#25506;&#32034;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#27010;&#36848;&#20102;&#20934;&#30830;&#26816;&#32034;&#36830;&#25509;&#20505;&#36873;&#20154;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#31616;&#21333;&#21512;&#24182;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#65292;&#26088;&#22312;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an in-depth analysis of data discovery in data lakes, focusing on table augmentation for given machine learning tasks. We analyze alternative methods used in the three main steps: retrieving joinable tables, merging information, and predicting with the resultant table. As data lakes, the paper uses YADL (Yet Another Data Lake) -- a novel dataset we developed as a tool for benchmarking this data discovery task -- and Open Data US, a well-referenced real data lake. Through systematic exploration on both lakes, our study outlines the importance of accurately retrieving join candidates and the efficiency of simple merging methods. We report new insights on the benefits of existing solutions and on their limitations, aiming at guiding future research in this space.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06104</link><description>&lt;p&gt;
&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65306;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#26126;&#30830;&#23398;&#20064;&#20989;&#25968;&#23548;&#25968;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#22238;&#24402;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#25439;&#22833;&#20989;&#25968;&#26469;&#23558;&#27169;&#22411;&#39044;&#27979;&#19982;&#27599;&#20010;&#20010;&#20307;&#25968;&#25454;&#26679;&#26412;&#30340;&#30495;&#23454;&#20540;&#23545;&#40784;&#65292;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#20851;&#31995;&#30340;&#39044;&#27979;&#19981;&#22815;&#20248;&#21270;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#24037;&#20316;&#24341;&#20837;&#20102;&#26631;&#31614;&#30456;&#20284;&#24615;&#20449;&#24687;&#26469;&#25913;&#36827;&#22238;&#24402;&#26041;&#27861;&#65292;&#20294;&#22312;&#23436;&#20840;&#25429;&#25417;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FAR&#65288;&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65289;&#20316;&#20026;&#19968;&#31181;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#39046;&#22495;&#30340;&#20843;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample, which, as we show, can result in sub-optimal prediction of the relationships between the different samples. Recent research endeavors have introduced novel perspectives by incorporating label similarity information to regression. However, a notable gap persists in these approaches when it comes to fully capturing the intricacies of the underlying ground truth function. In this work, we propose FAR (Function Aligned Regression) as a arguably better and more efficient solution to fit the underlying function of ground truth by capturing functional derivatives. We demonstrate the effectiveness of the proposed method practically on 2 synthetic datasets and on 8 extensive real-world tasks from 6 b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05569</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Node Classification With Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#25104;&#23545;&#20132;&#20114;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21457;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#30340;&#24819;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#30340;&#21457;&#23637;&#12290;GNNs&#21644;HyperGNNs&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#20960;&#20309;&#25299;&#25169;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#22823;&#22810;&#25968;HyperGNNs&#21487;&#20197;&#20351;&#29992;&#24102;&#26377;&#36229;&#22270;&#30340;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;GNN&#26469;&#36817;&#20284;&#12290;&#36825;&#23548;&#33268;&#20102;WCE-GNN&#65292;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;GNN&#21644;&#19968;&#20010;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#65288;WCE&#65289;&#65292;&#29992;&#20110;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23545;&#20110;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WCE-GNN&#19981;&#20165;&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#20248;&#21270;&#38544;&#21547;&#20998;&#24067;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21333;&#20010;&#24490;&#29615;&#20013;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#21644;&#37319;&#26679;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05468</link><description>&lt;p&gt;
&#38544;&#24335;&#25193;&#25955;: &#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit Diffusion: Efficient Optimization through Stochastic Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#20248;&#21270;&#38544;&#21547;&#20998;&#24067;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21333;&#20010;&#24490;&#29615;&#20013;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#21644;&#37319;&#26679;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21442;&#25968;&#21270;&#38543;&#26426;&#25193;&#25955;&#38544;&#24335;&#23450;&#20041;&#30340;&#20998;&#24067;&#26469;&#36827;&#34892;&#20248;&#21270;&#30340;&#26032;&#31639;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#36825;&#20123;&#21442;&#25968;&#65292;&#21487;&#20197;&#20462;&#25913;&#37319;&#26679;&#36807;&#31243;&#30340;&#32467;&#26524;&#20998;&#24067;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#36825;&#20123;&#36807;&#31243;&#30340;&#19968;&#38454;&#20248;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;&#24490;&#29615;&#20013;&#36827;&#34892;&#20248;&#21270;&#21644;&#37319;&#26679;&#27493;&#39588;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#21463;&#21040;&#21452;&#23618;&#20248;&#21270;&#21644;&#33258;&#21160;&#38544;&#24335;&#24494;&#20998;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#37319;&#26679;&#20316;&#20026;&#22312;&#27010;&#29575;&#20998;&#24067;&#31354;&#38388;&#19978;&#36827;&#34892;&#20248;&#21270;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#26041;&#27861;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#21450;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new algorithm to optimize distributions defined implicitly by parameterized stochastic diffusions. Doing so allows us to modify the outcome distribution of sampling processes by optimizing over their parameters. We introduce a general framework for first-order optimization of these processes, that performs jointly, in a single loop, optimization and sampling steps. This approach is inspired by recent advances in bilevel optimization and automatic implicit differentiation, leveraging the point of view of sampling as optimization over the space of probability distributions. We provide theoretical guarantees on the performance of our method, as well as experimental results demonstrating its effectiveness in real-world settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;CATE&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#23567;&#21306;&#38388;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.04906</link><description>&lt;p&gt;
&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conformal Monte Carlo Meta-learners for Predictive Inference of Individual Treatment Effects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931;&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644;CATE&#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#23567;&#21306;&#38388;&#23485;&#24230;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21487;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#35782;&#24178;&#39044;&#25928;&#26524;&#65292;&#21363;&#27835;&#30103;&#25928;&#26524;&#65292;&#23545;&#20110;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524; (CATE) &#20272;&#35745;&#31561;&#26041;&#27861;&#36890;&#24120;&#21482;&#25552;&#20379;&#27835;&#30103;&#25928;&#26524;&#30340;&#28857;&#20272;&#35745;&#65292;&#32780;&#24120;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65292;&#21363;&#19968;&#33268;&#24615;&#33945;&#29305;&#21345;&#27931; (CMC) &#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33268;&#24615;&#39044;&#27979;&#31995;&#32479;&#12289;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#21644; CATE &#20803;&#23398;&#20064;&#27169;&#22411;&#65292;&#26469;&#20135;&#29983;&#21487;&#29992;&#20110;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32467;&#26524;&#22122;&#22768;&#20998;&#24067;&#30340;&#29305;&#23450;&#20551;&#35774;&#22914;&#20309;&#20005;&#37325;&#24433;&#21709;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;CMC&#26694;&#26550;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#23454;&#39564;&#35206;&#30422;&#33539;&#22260;&#65292;&#21516;&#26102;&#20445;&#25345;&#36739;&#23567;&#30340;&#21306;&#38388;&#23485;&#24230;&#65292;&#20197;&#25552;&#20379;&#30495;&#23454;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge of the effect of interventions, called the treatment effect, is paramount for decision-making. Approaches to estimating this treatment effect, e.g. by using Conditional Average Treatment Effect (CATE) estimators, often only provide a point estimate of this treatment effect, while additional uncertainty quantification is frequently desired instead. Therefore, we present a novel method, the Conformal Monte Carlo (CMC) meta-learners, leveraging conformal predictive systems, Monte Carlo sampling, and CATE meta-learners, to instead produce a predictive distribution usable in individualized decision-making. Furthermore, we show how specific assumptions on the noise distribution of the outcome heavily affect these uncertainty predictions. Nonetheless, the CMC framework shows strong experimental coverage while retaining small interval widths to provide estimates of the true individual treatment effect.
&lt;/p&gt;</description></item><item><title>L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04902</link><description>&lt;p&gt;
L4Q: &#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#37327;&#21270;&#35757;&#32451;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04902
&lt;/p&gt;
&lt;p&gt;
L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#26041;&#27861;&#27491;&#22312;&#27969;&#34892;&#36215;&#26469;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25152;&#24102;&#26469;&#30340;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#20943;&#23569;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#36890;&#24120;&#39318;&#36873;&#21518;&#35757;&#32451;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#24182;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#37327;&#21270;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#37327;&#21270;&#27169;&#22411;&#30340;&#37197;&#32622;&#12290;&#30001;&#38750;&#32447;&#24615;&#37327;&#21270;&#25110;&#28151;&#21512;&#31934;&#24230;&#26435;&#37325;&#24341;&#36215;&#30340;&#25928;&#26524;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#24182;&#19988;&#37325;&#26032;&#35757;&#32451;&#29305;&#23450;&#37327;&#21270;&#21442;&#25968;&#21487;&#33021;&#20250;&#24433;&#21709;&#26368;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L4Q&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#12290;L4Q&#21033;&#29992;&#20102;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
&lt;/p&gt;</description></item><item><title>EvoSeed&#26159;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#25628;&#32034;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#65292;&#26088;&#22312;&#25581;&#31034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#23041;&#32961;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#40657;&#30418;&#35774;&#32622;&#20013;&#25805;&#20316;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.04699</link><description>&lt;p&gt;
EvoSeed&#65306;&#25581;&#31034;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#24187;&#35273;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
EvoSeed: Unveiling the Threat on Deep Neural Networks with Real-World Illusions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04699
&lt;/p&gt;
&lt;p&gt;
EvoSeed&#26159;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#25628;&#32034;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#65292;&#26088;&#22312;&#25581;&#31034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#23041;&#32961;&#12290;&#35813;&#26694;&#26550;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#40657;&#30418;&#35774;&#32622;&#20013;&#25805;&#20316;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#25152;&#21033;&#29992;&#65292;&#36825;&#20123;&#26679;&#26412;&#23545;&#20154;&#31867;&#24863;&#30693;&#27809;&#26377;&#24433;&#21709;&#65292;&#20294;&#20250;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30333;&#30418;&#24615;&#36136;&#26469;&#29983;&#25104;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#65292;&#25110;&#32773;&#25913;&#21464;&#23545;&#25239;&#26679;&#26412;&#19982;&#35757;&#32451;&#20998;&#24067;&#30340;&#20998;&#24067;&#12290;&#20026;&#20102;&#32531;&#35299;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EvoSeed&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#30340;&#25628;&#32034;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;EvoSeed&#26694;&#26550;&#20351;&#29992;&#36741;&#21161;&#25193;&#25955;&#21644;&#20998;&#31867;&#22120;&#27169;&#22411;&#22312;&#27169;&#22411;&#26080;&#20851;&#30340;&#40657;&#30418;&#35774;&#32622;&#20013;&#36816;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;CMA-ES&#26469;&#20248;&#21270;&#23545;&#23545;&#25239;&#31181;&#23376;&#21521;&#37327;&#30340;&#25628;&#32034;&#65292;&#35813;&#21521;&#37327;&#22312;&#32463;&#36807;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22788;&#29702;&#21518;&#65292;&#23548;&#33268;&#20998;&#31867;&#22120;&#27169;&#22411;&#38169;&#35823;&#20998;&#31867;&#26080;&#38480;&#21046;&#30340;&#33258;&#28982;&#23545;&#25239;&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#29983;&#25104;&#30340;&#23545;&#25239;&#22270;&#20687;&#20855;&#26377;&#39640;&#36136;&#37327;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are exploited using natural adversarial samples, which have no impact on human perception but are misclassified. Current approaches often rely on the white-box nature of deep neural networks to generate these adversarial samples or alter the distribution of adversarial samples compared to training distribution. To alleviate the limitations of current approaches, we propose EvoSeed, a novel evolutionary strategy-based search algorithmic framework to generate natural adversarial samples. Our EvoSeed framework uses auxiliary Diffusion and Classifier models to operate in a model-agnostic black-box setting. We employ CMA-ES to optimize the search for an adversarial seed vector, which, when processed by the Conditional Diffusion Model, results in an unrestricted natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality and are transferable to different classifiers. Our approach demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#20851;&#31995;&#36229;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26694;&#26550;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04062</link><description>&lt;p&gt;
&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Link Prediction with Relational Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#20851;&#31995;&#36229;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26694;&#26550;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#24050;&#32463;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#23548;&#33268;&#20102;&#20855;&#26377;&#25104;&#21151;&#24212;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#20016;&#23500;&#26223;&#35266;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#25104;&#21151;&#36716;&#31227;&#21040;&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20851;&#31995;&#36229;&#36793;&#30340;&#23384;&#22312;&#20351;&#24471;&#38142;&#25509;&#39044;&#27979;&#25104;&#20026;&#22312;&#19981;&#21516;&#36873;&#25321;&#30340;k&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20219;&#21153;&#65292;&#36825;&#27604;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#35201;&#22256;&#38590;&#24471;&#22810;&#65292;&#22240;&#20026;&#27599;&#20010;&#20851;&#31995;&#37117;&#26159;&#20108;&#36827;&#21046;&#30340;&#65288;k=2&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#30456;&#24212;&#30340;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#20197;&#21450;&#19968;&#20123;&#33258;&#28982;&#36923;&#36753;&#24418;&#24335;&#23545;&#29983;&#25104;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs. The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$). In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms. Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model archit
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24615;&#28431;&#27934;&#65292;&#36890;&#36807;&#36793;&#37325;&#26500;&#25915;&#20987;&#21487;&#20197;&#25512;&#26029;&#20986;&#25935;&#24863;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#25506;&#35752;&#20102;&#22122;&#22768;&#32858;&#21512;&#26426;&#21046;&#20135;&#29983;&#30340;&#38544;&#31169;&#22270;&#34920;&#31034;&#23545;&#35813;&#25915;&#20987;&#30340;&#38887;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04033</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#24418;&#34920;&#31034;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
On provable privacy vulnerabilities of graph representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04033
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#22270;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24615;&#28431;&#27934;&#65292;&#36890;&#36807;&#36793;&#37325;&#26500;&#25915;&#20987;&#21487;&#20197;&#25512;&#26029;&#20986;&#25935;&#24863;&#30340;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#25506;&#35752;&#20102;&#22122;&#22768;&#32858;&#21512;&#26426;&#21046;&#20135;&#29983;&#30340;&#38544;&#31169;&#22270;&#34920;&#31034;&#23545;&#35813;&#25915;&#20987;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;(GRL)&#23545;&#20110;&#20174;&#22797;&#26434;&#30340;&#32593;&#32476;&#32467;&#26500;&#20013;&#25552;&#21462;&#27934;&#35265;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#34920;&#31034;&#20013;&#21487;&#33021;&#23384;&#22312;&#38544;&#31169;&#28431;&#27934;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24615;&#28431;&#27934;&#65292;&#21487;&#20197;&#36890;&#36807;&#36793;&#37325;&#26500;&#25915;&#20987;&#25512;&#26029;&#20986;&#25935;&#24863;&#30340;&#25299;&#25169;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#35299;&#20915;&#20102;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#36793;&#37325;&#26500;&#25915;&#20987;(COSERA)&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#35777;&#26126;&#38543;&#30528;&#22270;&#30340;&#35268;&#27169;&#22686;&#21152;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#23436;&#32654;&#22320;&#37325;&#26500;&#31232;&#30095;&#30340;Erdos Renyi&#22270;&#19982;&#29420;&#31435;&#38543;&#26426;&#29305;&#24449;&#12290;&#21453;&#20043;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31232;&#30095;&#24615;&#23545;COSERA&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#22359;&#27169;&#22411;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#22122;&#22768;&#32858;&#21512;(NAG)&#26426;&#21046;&#20135;&#29983;&#30340;(&#21487;&#35777;&#26126;&#30340;)&#38544;&#31169;&#22270;&#34920;&#31034;&#23545;COSERA&#25915;&#20987;&#30340;&#38887;&#24615;&#12290;&#25105;&#20204;&#23454;&#35777;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Graph representation learning (GRL) is critical for extracting insights from complex network structures, but it also raises security concerns due to potential privacy vulnerabilities in these representations. This paper investigates the structural vulnerabilities in graph neural models where sensitive topological information can be inferred through edge reconstruction attacks. Our research primarily addresses the theoretical underpinnings of cosine-similarity-based edge reconstruction attacks (COSERA), providing theoretical and empirical evidence that such attacks can perfectly reconstruct sparse Erdos Renyi graphs with independent random features as graph size increases. Conversely, we establish that sparsity is a critical factor for COSERA's effectiveness, as demonstrated through analysis and experiments on stochastic block models. Finally, we explore the resilience of (provably) private graph representations produced via noisy aggregation (NAG) mechanism against COSERA. We empirical
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#65292;&#22686;&#21152;&#20102;&#23545;&#20854;&#29702;&#35770;&#29702;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#20110;&#39640;&#26041;&#24046;&#30340;&#19979;&#28216;&#39044;&#27979;&#22120;&#29305;&#21035;&#26377;&#30410;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32463;&#39564;&#27861;&#21017;&#29992;&#20110;&#36873;&#25321;&#36866;&#24403;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03985</link><description>&lt;p&gt;
&#23545;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#38598;&#25104;&#36827;&#34892;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#65292;&#22686;&#21152;&#20102;&#23545;&#20854;&#29702;&#35770;&#29702;&#35299;&#12290;&#23454;&#39564;&#35777;&#26126;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#23545;&#20110;&#39640;&#26041;&#24046;&#30340;&#19979;&#28216;&#39044;&#27979;&#22120;&#29305;&#21035;&#26377;&#30410;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32463;&#39564;&#27861;&#21017;&#29992;&#20110;&#36873;&#25321;&#36866;&#24403;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20026;&#30417;&#30563;&#23398;&#20064;&#29983;&#25104;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#22686;&#21152;&#20934;&#30830;&#24615;&#12289;&#26356;&#26377;&#25928;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36825;&#20123;&#22909;&#22788;&#22312;&#32463;&#39564;&#19978;&#26377;&#26126;&#30830;&#30340;&#25903;&#25345;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#29702;&#35770;&#29702;&#35299;&#30446;&#21069;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#20351;&#29992;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20960;&#31181;&#35774;&#32622;&#30340;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#65292;&#26469;&#22686;&#21152;&#29702;&#35770;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#39044;&#27979;&#65292;&#23545;&#20110;&#39640;&#26041;&#24046;&#30340;&#19979;&#28216;&#39044;&#27979;&#22120;&#65292;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#23558;&#29305;&#21035;&#26377;&#30410;&#65292;&#24182;&#20026;&#22343;&#26041;&#35823;&#24046;&#21644;Brier&#20998;&#25968;&#30340;&#24773;&#20917;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#32463;&#39564;&#27861;&#21017;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#25968;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#19968;&#20010;&#38598;&#25104;&#22312;&#22810;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20960;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#20197;&#21450;&#19979;&#28216;&#39044;&#27979;&#22120;&#19978;&#30340;&#24615;&#33021;&#26469;&#30740;&#31350;&#25105;&#20204;&#30340;&#29702;&#35770;&#22312;&#23454;&#36341;&#20013;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#27934;&#23519;&#20063;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have highlighted the benefits of generating multiple synthetic datasets for supervised learning, from increased accuracy to more effective model selection and uncertainty estimation. These benefits have clear empirical support, but the theoretical understanding of them is currently very light. We seek to increase the theoretical understanding by deriving bias-variance decompositions for several settings of using multiple synthetic datasets. Our theory predicts multiple synthetic datasets to be especially beneficial for high-variance downstream predictors, and yields a simple rule of thumb to select the appropriate number of synthetic datasets in the case of mean-squared error and Brier score. We investigate how our theory works in practice by evaluating the performance of an ensemble over many synthetic datasets for several real datasets and downstream predictors. The results follow our theory, showing that our insights are also practically relevant.
&lt;/p&gt;</description></item><item><title>PARD&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20013;&#30340;&#37096;&#20998;&#39034;&#24207;&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.03687</link><description>&lt;p&gt;
Pard: &#20855;&#26377;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#29992;&#20110;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03687
&lt;/p&gt;
&lt;p&gt;
PARD&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20013;&#30340;&#37096;&#20998;&#39034;&#24207;&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#22238;&#24402;&#27169;&#22411;&#23545;&#20110;&#22270;&#30340;&#39034;&#24207;&#25935;&#24863;&#65292;&#20294;&#20854;&#31616;&#21333;&#26377;&#25928;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#19968;&#30452;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#32622;&#25442;&#19981;&#21464;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22270;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#22270;&#65292;&#20294;&#38656;&#35201;&#39069;&#22806;&#30340;&#29305;&#24449;&#21644;&#25104;&#21315;&#19978;&#19975;&#27493;&#30340;&#21435;&#22122;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;PARD&#65292;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#12290;PARD&#21033;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#32622;&#25442;&#19981;&#21464;&#24615;&#65292;&#26080;&#38656;&#20851;&#27880;&#22270;&#30340;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#38598;&#21512;&#19981;&#21516;&#65292;&#22270;&#20013;&#30340;&#20803;&#32032;&#24182;&#19981;&#26159;&#23436;&#20840;&#26080;&#24207;&#30340;&#65292;&#33410;&#28857;&#21644;&#36793;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#37096;&#20998;&#39034;&#24207;&#12290;&#21033;&#29992;&#36825;&#20010;&#37096;&#20998;&#39034;&#24207;&#65292;PARD&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#30340;&#27010;&#29575;&#20026;c&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#20989;&#25968;&#29992;&#20110;&#20027;&#21160;&#30456;&#20851;&#32858;&#31867;&#65292;&#20998;&#21035;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#27010;&#24565;&#21644;&#20449;&#24687;&#35770;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03587</link><description>&lt;p&gt;
&#20027;&#21160;&#30456;&#20851;&#32858;&#31867;&#30340;&#26377;&#25928;&#33719;&#21462;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Effective Acquisition Functions for Active Correlation Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#20989;&#25968;&#29992;&#20110;&#20027;&#21160;&#30456;&#20851;&#32858;&#31867;&#65292;&#20998;&#21035;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#27010;&#24565;&#21644;&#20449;&#24687;&#35770;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#32858;&#31867;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#65292;&#25903;&#25345;&#27491;&#21644;&#36127;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#20551;&#35774;&#30456;&#20284;&#24615;&#20107;&#20808;&#26410;&#30693;&#65292;&#32780;&#26159;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#20197;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#24335;&#36845;&#20195;&#22320;&#26597;&#35810;&#30456;&#20284;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#20989;&#25968;&#29992;&#20110;&#22312;&#27492;&#35774;&#32622;&#19979;&#20351;&#29992;&#12290;&#20854;&#20013;&#19968;&#31181;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#27010;&#24565;&#65288;&#21363;&#24403;&#30456;&#20284;&#24615;&#36829;&#21453;&#20256;&#36882;&#24615;&#26102;&#65289;&#12290;&#20854;&#20313;&#20004;&#20010;&#22522;&#20110;&#20449;&#24687;&#35770;&#37327;&#65292;&#21363;&#29109;&#21644;&#20449;&#24687;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correlation clustering is a powerful unsupervised learning paradigm that supports positive and negative similarities. In this paper, we assume the similarities are not known in advance. Instead, we employ active learning to iteratively query similarities in a cost-efficient way. In particular, we develop three effective acquisition functions to be used in this setting. One is based on the notion of inconsistency (i.e., when similarities violate the transitive property). The remaining two are based on information-theoretic quantities, i.e., entropy and information gain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#23545;&#29305;&#23450;&#26465;&#20214;&#26377;&#20005;&#26684;&#35201;&#27714;&#30340;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#31526;&#21512;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;PGDM&#22312;&#22797;&#26434;&#30340;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21512;&#25104;&#20986;&#31526;&#21512;&#35201;&#27714;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.03559</link><description>&lt;p&gt;
&#29992;&#20110;&#32422;&#26463;&#28385;&#36275;&#30340;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Projected Generative Diffusion Models for Constraint Satisfaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#23558;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36866;&#29992;&#20110;&#23545;&#29305;&#23450;&#26465;&#20214;&#26377;&#20005;&#26684;&#35201;&#27714;&#30340;&#22330;&#26223;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#31526;&#21512;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;PGDM&#22312;&#22797;&#26434;&#30340;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21512;&#25104;&#20986;&#31526;&#21512;&#35201;&#27714;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#19968;&#20010;&#39034;&#24207;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#21407;&#22987;&#22122;&#22768;&#20013;&#21512;&#25104;&#20986;&#36830;&#36143;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#22312;&#38656;&#35201;&#36755;&#20986;&#31526;&#21512;&#29305;&#23450;&#20005;&#26684;&#26465;&#20214;&#30340;&#22330;&#26223;&#20013;&#30452;&#25509;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30528;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#20171;&#32461;&#20102;&#25237;&#24433;&#24335;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65288;PGDM&#65289;&#65292;&#35813;&#26041;&#27861;&#23558;&#20256;&#32479;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#36845;&#20195;&#25237;&#24433;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#25968;&#25454;&#24544;&#23454;&#22320;&#36981;&#24490;&#25351;&#23450;&#30340;&#32422;&#26463;&#25110;&#29289;&#29702;&#21407;&#29702;&#12290;&#26412;&#25991;&#22312;&#21463;&#38480;&#21046;&#30340;&#32422;&#26463;&#31867;&#21035;&#19979;&#65292;&#23545;PGDM&#33021;&#22815;&#20174;&#21487;&#34892;&#23376;&#20998;&#24067;&#20013;&#21512;&#25104;&#36755;&#20986;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#38750;&#20984;&#32422;&#26463;&#21644;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#26696;&#20363;&#20013;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#36825;&#20123;&#33021;&#21147;&#36890;&#36807;&#22312;&#35270;&#39057;&#29983;&#25104;&#20013;&#20307;&#29616;&#20102;&#20855;&#26377;&#29289;&#29702;&#23398;&#20449;&#24687;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models excel at robustly synthesizing coherent content from raw noise through a sequential process. However, their direct application in scenarios requiring outputs to adhere to specific, stringent criteria faces several severe challenges. This paper aims at overcome these challenges and introduces Projected Generative Diffusion Models (PGDM), an approach that recast traditional diffusion models sampling into a constrained-optimization problem. This enables the application of an iterative projections method to ensure that generated data faithfully adheres to specified constraints or physical principles. This paper provides theoretical support for the ability of PGDM to synthesize outputs from a feasible subdistribution under a restricted class of constraints while also providing large empirical evidence in the case of complex non-convex constraints and ordinary differential equations. These capabilities are demonstrated by physics-informed motion in video generatio
&lt;/p&gt;</description></item><item><title>MobilityGPT&#26159;&#19968;&#31181;&#22522;&#20110;GPT&#27169;&#22411;&#30340;&#22686;&#24378;&#22411;&#20154;&#31867;&#31227;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#31227;&#21160;&#24314;&#27169;&#36716;&#25442;&#20026;&#33258;&#22238;&#24402;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#22320;&#29702;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#37325;&#21147;&#30340;&#37319;&#26679;&#26041;&#27861;&#21644;&#36947;&#36335;&#36830;&#25509;&#30697;&#38453;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#30340;&#22320;&#29702;&#31354;&#38388;&#31227;&#21160;&#25968;&#25454;&#30340;&#35821;&#20041;&#30495;&#23454;&#24615;&#21644;&#21508;&#20010;&#29305;&#24449;&#30340;&#20445;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.03264</link><description>&lt;p&gt;
MobilityGPT: &#22522;&#20110;GPT&#27169;&#22411;&#30340;&#22686;&#24378;&#22411;&#20154;&#31867;&#31227;&#21160;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MobilityGPT: Enhanced Human Mobility Modeling with a GPT model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03264
&lt;/p&gt;
&lt;p&gt;
MobilityGPT&#26159;&#19968;&#31181;&#22522;&#20110;GPT&#27169;&#22411;&#30340;&#22686;&#24378;&#22411;&#20154;&#31867;&#31227;&#21160;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#31227;&#21160;&#24314;&#27169;&#36716;&#25442;&#20026;&#33258;&#22238;&#24402;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#22320;&#29702;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;&#20197;&#21450;&#22522;&#20110;&#37325;&#21147;&#30340;&#37319;&#26679;&#26041;&#27861;&#21644;&#36947;&#36335;&#36830;&#25509;&#30697;&#38453;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#30340;&#22320;&#29702;&#31354;&#38388;&#31227;&#21160;&#25968;&#25454;&#30340;&#35821;&#20041;&#30495;&#23454;&#24615;&#21644;&#21508;&#20010;&#29305;&#24449;&#30340;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#29305;&#24449;&#21644;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#22320;&#29702;&#31354;&#38388;&#31227;&#21160;&#25968;&#25454;&#22312;&#35821;&#20041;&#19978;&#26159;&#30495;&#23454;&#30340;&#65292;&#21253;&#25324;&#19968;&#33268;&#30340;&#20301;&#32622;&#24207;&#21015;&#20197;&#21450;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#30340;&#29305;&#24449;&#65292;&#22914;&#23545;&#22320;&#29702;&#38480;&#21046;&#30340;&#32422;&#26463;&#65292;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#31227;&#21160;&#24314;&#27169;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#33258;&#22238;&#24402;&#29983;&#25104;&#20219;&#21153;&#65292;&#21033;&#29992;&#20102;Generative Pre-trained Transformer (GPT)&#12290;&#20026;&#20102;&#30830;&#20445;&#20854;&#21487;&#25511;&#30340;&#29983;&#25104;&#26469;&#32531;&#35299;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22320;&#29702;&#24863;&#30693;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;MobilityGPT&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#37325;&#21147;&#30340;&#37319;&#26679;&#26041;&#27861;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;transformer&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#36947;&#36335;&#36830;&#25509;&#30697;&#38453;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#32422;&#26463;&#65292;&#35813;&#30697;&#38453;&#32473;&#20986;&#20102;&#36712;&#36857;&#29983;&#25104;&#20013;&#24207;&#21015;&#20043;&#38388;&#30340;&#36830;&#25509;&#24615;&#65292;&#20174;&#32780;&#20445;&#25345;&#29983;&#25104;&#30340;&#36712;&#36857;&#22312;&#22320;&#29702;&#33539;&#22260;&#20869;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#24341;&#23548;&#30340;&#25913;&#36827;&#30446;&#26631;&#20989;&#25968;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories. However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits. To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained Transformer (GPT). To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT. We propose a gravity-based sampling method to train a transformer for semantic sequence similarity. Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits. Lastly, we constructed a Reinforcement L
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuseMoE&#30340;&#19987;&#23478;&#28151;&#21512;Transformer&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#38376;&#25511;&#20989;&#25968;&#23454;&#29616;&#28789;&#27963;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#65292;&#21516;&#26102;&#25913;&#21892;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03226</link><description>&lt;p&gt;
FuseMoE&#65306;&#29992;&#20110;&#28789;&#27963;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#19987;&#23478;&#28151;&#21512;Transformer
&lt;/p&gt;
&lt;p&gt;
FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuseMoE&#30340;&#19987;&#23478;&#28151;&#21512;Transformer&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#38376;&#25511;&#20989;&#25968;&#23454;&#29616;&#28789;&#27963;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#65292;&#21516;&#26102;&#25913;&#21892;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#39046;&#22495;&#36234;&#26469;&#36234;&#22810;&#22320;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#23427;&#20204;&#38754;&#20020;&#22788;&#29702;&#22810;&#31181;&#27169;&#24577;&#30340;&#21452;&#37325;&#25361;&#25112;&#65292;&#36825;&#20123;&#27169;&#24577;&#32463;&#24120;&#22240;&#32570;&#22833;&#20803;&#32032;&#32780;&#19981;&#23436;&#25972;&#65292;&#20197;&#21450;&#25910;&#38598;&#26679;&#26412;&#30340;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#21644;&#31232;&#30095;&#24615;&#12290;&#25104;&#21151;&#21033;&#29992;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#65292;&#21516;&#26102;&#20811;&#26381;&#39640;&#36136;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#31232;&#32570;&#24615;&#65292;&#26159;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;``FuseMoE''&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#21019;&#26032;&#38376;&#25511;&#20989;&#25968;&#30340;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#12290;FuseMoE&#26088;&#22312;&#25972;&#21512;&#22810;&#31181;&#27169;&#24577;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#38750;&#24120;&#26377;&#25928;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#29420;&#29305;&#30340;&#38376;&#25511;&#20989;&#25968;&#26377;&#21161;&#20110;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;FuseMoE&#30340;&#23454;&#38469;&#23454;&#29992;&#24615;&#36890;&#36807;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32452;&#21512;&#29305;&#24449;&#23545;&#40784;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#32452;&#21512;&#36890;&#29992;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;-&#31867;&#21035;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.02851</link><description>&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#29305;&#24449;&#23545;&#40784;&#22686;&#24378;&#32452;&#21512;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Compositional Generalization via Compositional Feature Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02851
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#29305;&#24449;&#23545;&#40784;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#32452;&#21512;&#36890;&#29992;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;-&#31867;&#21035;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#32463;&#24120;&#38754;&#20020;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#22312;&#24120;&#35265;&#30340;&#22810;&#39046;&#22495;&#22810;&#31867;&#21035;&#35774;&#32622;&#20013;&#65292;&#38543;&#30528;&#31867;&#21035;&#21644;&#39046;&#22495;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#24456;&#38590;&#20026;&#27599;&#20010;&#39046;&#22495;-&#31867;&#21035;&#32452;&#21512;&#25910;&#38598;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20010;&#25361;&#25112;&#33258;&#28982;&#22320;&#24341;&#21457;&#20102;&#23545;&#20855;&#22791;&#32452;&#21512;&#36890;&#29992;&#24615;&#65288;CG&#65289;&#33021;&#21147;&#30340;&#27169;&#22411;&#30340;&#25506;&#32034;&#65292;&#21363;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;-&#31867;&#21035;&#32452;&#21512;&#12290;&#20026;&#20102;&#28145;&#20837;&#30740;&#31350;CG&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;CG-Bench&#65292;&#36825;&#26159;&#19968;&#22871;&#20174;&#29616;&#26377;&#23454;&#38469;&#22270;&#20687;&#25968;&#25454;&#38598;&#27966;&#29983;&#30340;CG&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#30446;&#21069;&#22312;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;DINOv2&#65289;&#19978;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#36825;&#20010;&#25361;&#25112;&#20013;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32452;&#21512;&#29305;&#24449;&#23545;&#40784;&#65288;CFA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#24494;&#35843;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#19978;&#23398;&#20064;&#20004;&#20010;&#27491;&#20132;&#32447;&#24615;&#22836;&#37096;&#26469;&#23545;&#40784;&#31867;&#21035;&#21644;&#39046;&#22495;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain lab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;zkSNARKs&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27169;&#22411;&#25512;&#29702;&#21644;&#38646;&#30693;&#35782;&#35745;&#31639;&#35777;&#26126;&#65292;&#21487;&#20197;&#25552;&#20379;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#65292;&#39564;&#35777;&#27169;&#22411;&#22312;&#20844;&#24320;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#38381;&#28304;&#21830;&#19994;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21487;&#20449;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02675</link><description>&lt;p&gt;
&#20351;&#29992;zkSNARKs&#36827;&#34892;&#21487;&#39564;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Verifiable evaluations of machine learning models using zkSNARKs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;zkSNARKs&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#27169;&#22411;&#25512;&#29702;&#21644;&#38646;&#30693;&#35782;&#35745;&#31639;&#35777;&#26126;&#65292;&#21487;&#20197;&#25552;&#20379;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#65292;&#39564;&#35777;&#27169;&#22411;&#22312;&#20844;&#24320;&#36755;&#20837;&#19978;&#30340;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#38381;&#28304;&#21830;&#19994;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21487;&#20449;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36234;&#26469;&#36234;&#22810;&#38381;&#28304;&#21830;&#19994;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19990;&#30028;&#20013;&#65292;&#24320;&#21457;&#32773;&#30340;&#27169;&#22411;&#35780;&#20272;&#24517;&#39035;&#34987;&#24403;&#20316;&#38754;&#20540;&#25509;&#21463;&#12290;&#36825;&#20123;&#35780;&#20272;&#32467;&#26524;&#65292;&#26080;&#35770;&#26159;&#20219;&#21153;&#20934;&#30830;&#24615;&#12289;&#20559;&#24046;&#35780;&#20272;&#36824;&#26159;&#23433;&#20840;&#26816;&#26597;&#65292;&#20256;&#32479;&#19978;&#26080;&#27861;&#36890;&#36807;&#37325;&#26032;&#25191;&#34892;&#40657;&#31665;&#27169;&#22411;&#36755;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;zkSNARKs&#36827;&#34892;&#21487;&#39564;&#35777;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;zkSNARKs&#36827;&#34892;&#27169;&#22411;&#25512;&#29702;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#36755;&#20986;&#30340;&#38646;&#30693;&#35782;&#35745;&#31639;&#35777;&#26126;&#21487;&#20197;&#25171;&#21253;&#25104;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#65292;&#26174;&#31034;&#20855;&#26377;&#22266;&#23450;&#31169;&#26377;&#26435;&#37325;&#30340;&#27169;&#22411;&#22312;&#20844;&#24320;&#36755;&#20837;&#19978;&#36798;&#21040;&#20102;&#25152;&#36848;&#30340;&#24615;&#33021;&#25110;&#20844;&#24179;&#24615;&#25351;&#26631;&#12290;&#36825;&#20123;&#21487;&#39564;&#35777;&#30340;&#35780;&#20272;&#35777;&#26126;&#21487;&#20197;&#22312;&#20219;&#20309;&#26631;&#20934;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#36827;&#34892;&#65292;&#35745;&#31639;&#35201;&#27714;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#36825;&#19968;&#28857;&#65292;&#24182;&#31361;&#20986;&#20102;&#20851;&#38190;&#25361;&#25112;&#21644;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a world of increasing closed-source commercial machine learning models, model evaluations from developers must be taken at face value. These benchmark results, whether over task accuracy, bias evaluations, or safety checks, are traditionally impossible to verify by a model end-user without the costly or impossible process of re-performing the benchmark on black-box model outputs. This work presents a method of verifiable model evaluation using model inference through zkSNARKs. The resulting zero-knowledge computational proofs of model outputs over datasets can be packaged into verifiable evaluation attestations showing that models with fixed private weights achieve stated performance or fairness metrics over public inputs. These verifiable attestations can be performed on any standard neural network model with varying compute requirements. For the first time, we demonstrate this across a sample of real-world models and highlight key challenges and design solutions. This presents a n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19990;&#30028;&#30693;&#35782;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;&#36890;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02651</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models Provide Promptable Representations for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#25552;&#31034;&#30340;&#34920;&#31034;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#19990;&#30028;&#30693;&#35782;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#24555;&#22320;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#35757;&#32451;&#30340;&#31574;&#30053;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20248;&#20110;&#36890;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#32972;&#26223;&#19990;&#30028;&#30693;&#35782;&#24555;&#36895;&#23398;&#20064;&#26032;&#30340;&#34892;&#20026;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#20195;&#29702;&#36890;&#24120;&#38656;&#35201;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20013;&#32534;&#30721;&#30340;&#22823;&#37327;&#36890;&#29992;&#21644;&#21487;&#32034;&#24341;&#30340;&#19990;&#30028;&#30693;&#35782;&#26469;&#36827;&#34892;&#20855;&#35937;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;VLMs&#29992;&#20316;&#21487;&#25552;&#31034;&#34920;&#31034;&#26469;&#21021;&#22987;&#21270;&#31574;&#30053;&#65306;&#36825;&#20123;&#23884;&#20837;&#22312;&#35270;&#35273;&#35266;&#23519;&#20013;&#20855;&#26377;&#22522;&#30784;&#65292;&#24182;&#26681;&#25454;VLM&#30340;&#20869;&#37096;&#30693;&#35782;&#32534;&#30721;&#35821;&#20041;&#29305;&#24449;&#65292;&#36890;&#36807;&#25552;&#20379;&#20219;&#21153;&#19978;&#19979;&#25991;&#21644;&#36741;&#21161;&#20449;&#24687;&#26469;&#35302;&#21457;&#12290;&#25105;&#20204;&#22312;Minecraft&#21644;Habitat&#20013;&#30340;&#35270;&#35273;&#22797;&#26434;&#12289;&#38271;&#26399;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36890;&#29992;&#22411;VLMs&#25552;&#21462;&#30340;&#23884;&#20837;&#35757;&#32451;&#30340;&#31574;&#30053;&#32988;&#36807;&#20351;&#29992;&#36890;&#29992;&#30340;&#12289;&#19981;&#21487;&#25552;&#31034;&#30340;&#22270;&#20687;&#23884;&#20837;&#35757;&#32451;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#36981;&#24490;&#25351;&#31034;&#30340;&#20803;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following met
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#26550;&#26500;&#65292;&#21517;&#20026;Moirai&#65292;&#20197;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36328;&#39057;&#29575;&#23398;&#20064;&#12289;&#36866;&#24212;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#21516;&#20998;&#24067;&#29305;&#24615;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#35757;&#32451;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer&#12290;</title><link>https://arxiv.org/abs/2402.02592</link><description>&lt;p&gt;
&#32479;&#19968;&#35757;&#32451;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer
&lt;/p&gt;
&lt;p&gt;
Unified Training of Universal Time Series Forecasting Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#26550;&#26500;&#65292;&#21517;&#20026;Moirai&#65292;&#20197;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#36328;&#39057;&#29575;&#23398;&#20064;&#12289;&#36866;&#24212;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20197;&#21450;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#21516;&#20998;&#24067;&#29305;&#24615;&#31561;&#25361;&#25112;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#32479;&#19968;&#35757;&#32451;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#19968;&#27169;&#22411;&#30340;&#26694;&#26550;&#19979;&#36816;&#20316;&#65292;&#38480;&#21046;&#20102;&#20854;&#33021;&#22815;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31361;&#30772;&#24615;&#24433;&#21709;&#30340;&#28508;&#21147;&#12290;&#36890;&#29992;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#28304;&#20110;&#22312;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35774;&#24819;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#30340;&#21333;&#19968;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;i) &#36328;&#39057;&#29575;&#23398;&#20064;&#65292;ii) &#36866;&#24212;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#20219;&#24847;&#25968;&#37327;&#30340;&#21464;&#37327;&#65292;&#20197;&#21450;iii) &#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#22266;&#26377;&#30340;&#19981;&#21516;&#20998;&#24067;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23545;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#26550;&#26500;&#36827;&#34892;&#20102;&#26032;&#39062;&#30340;&#22686;&#24378;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Masked Encoder&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;Transformer&#65288;Moirai&#65289;&#12290;&#22312;&#25105;&#20204;&#26032;&#24341;&#20837;&#30340;&#22823;&#35268;&#27169;&#24320;&#25918;&#26102;&#38388;&#24207;&#21015;&#23384;&#26723;&#65288;LOTSA&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) fea
&lt;/p&gt;</description></item><item><title>DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02563</link><description>&lt;p&gt;
DefInt&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DefInt: A Default-interventionist Framework for Efficient Reasoning with Hybrid Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02563
&lt;/p&gt;
&lt;p&gt;
DefInt&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65292;&#36890;&#36807;&#40664;&#35748;&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#24605;&#36335;&#65292;&#28982;&#21518;&#36890;&#36807;&#21453;&#24605;&#25512;&#29702;&#24178;&#39044;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#28151;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#33021;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22914;&#36830;&#38145;&#25512;&#29702;&#65288;CoT&#65289;&#21644;&#24605;&#32500;&#26641;&#65288;ToT&#65289;&#20027;&#35201;&#20851;&#27880;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24573;&#35270;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#26631;&#35760;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#20855;&#26377;&#24040;&#22823;&#35299;&#31354;&#38388;&#30340;&#24320;&#25918;&#24615;&#23454;&#38469;&#20219;&#21153;&#26469;&#35828;&#21487;&#33021;&#29305;&#21035;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#30340;&#21452;&#36807;&#31243;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40664;&#35748;&#24178;&#39044;&#26694;&#26550;&#65288;DefInt&#65289;&#65292;&#20197;&#37322;&#25918;&#28151;&#21512;LLMs&#30340;&#21327;&#21516;&#28508;&#21147;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;DefInt&#20351;&#29992;&#36739;&#23567;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20302;&#25104;&#26412;&#30340;&#25512;&#29702;&#24605;&#36335;&#65292;&#31867;&#20284;&#20110;&#8220;&#31995;&#32479;1&#8221;&#20135;&#29983;&#30340;&#24555;&#36895;&#30452;&#35273;&#12290;&#22914;&#26524;&#36825;&#20123;&#30452;&#35273;&#34987;&#35748;&#20026;&#20302;&#32622;&#20449;&#24230;&#65292;&#21017;DefInt&#23558;&#35843;&#29992;&#25918;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#24605;&#25512;&#29702;&#20316;&#20026;&#8220;&#31995;&#32479;2&#8221;&#30340;&#24178;&#39044;&#65292;&#21487;&#20197;&#35206;&#30422;&#40664;&#35748;&#24605;&#32771;&#24182;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#23454;&#39564;&#22312;&#20116;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;DefInt&#35770;&#25991;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive emergent abilities in a wide range of tasks, but still face challenges in handling complex reasoning problems. Previous works like chain-of-thought (CoT) and tree-of-thoughts(ToT) have predominately focused on enhancing accuracy, but overlook the rapidly increasing token cost, which could be particularly problematic for open-ended real-world tasks with huge solution spaces. Motivated by the dual process theory of human cognition, we propose a Default-Interventionist framework (DefInt) to unleash the synergistic potential of hybrid LLMs. By default, DefInt uses smaller-scale language models to generate low-cost reasoning thoughts, which resembles the fast intuitions produced by System 1. If the intuitions are considered with low confidence, DefInt will invoke the reflective reasoning of scaled-up language models as the intervention of System 2, which can override the default thoughts and rectify the reasoning process. Experiments on fiv
&lt;/p&gt;</description></item><item><title>AutoTimes&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#25442;&#33021;&#21147;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#19982;&#20808;&#21069;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02370</link><description>&lt;p&gt;
AutoTimes: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoTimes: Autoregressive Time Series Forecasters via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02370
&lt;/p&gt;
&lt;p&gt;
AutoTimes&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#36716;&#25442;&#33021;&#21147;&#26469;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#19982;&#20808;&#21069;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35268;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#23578;&#26410;&#23436;&#20840;&#21457;&#23637;&#12290;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#30456;&#20284;&#39034;&#24207;&#32467;&#26500;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#24573;&#35270;&#20102;&#26102;&#38388;&#24207;&#21015;&#21644;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#30340;&#19968;&#33268;&#24615;&#65292;&#23548;&#33268;&#23545;LLM&#28508;&#21147;&#30340;&#21033;&#29992;&#19981;&#36275;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#20174;&#35821;&#35328;&#24314;&#27169;&#20013;&#23398;&#21040;&#30340;&#36890;&#29992;&#20196;&#29260;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoTimes&#65292;&#23558;LLM&#37325;&#26032;&#29992;&#20316;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22120;&#65292;&#36825;&#19982;LLM&#30340;&#33719;&#21462;&#21644;&#21033;&#29992;&#19968;&#33268;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#21442;&#25968;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#39044;&#27979;&#22120;&#21487;&#20197;&#22788;&#29702;&#28789;&#27963;&#30340;&#31995;&#21015;&#38271;&#24230;&#65292;&#24182;&#23454;&#29616;&#19982;&#27969;&#34892;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20196;&#29260;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#24212;&#30340;&#26102;&#38388;&#25139;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models of time series have not been fully developed due to the limited availability of large-scale time series and the underexploration of scalable pre-training. Based on the similar sequential structure of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, prior methods may overlook the consistency in aligning time series and natural language, resulting in insufficient utilization of the LLM potentials. To fully exploit the general-purpose token transitions learned from language modeling, we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which is consistent with the acquisition and utilization of LLMs without updating the parameters. The consequent forecasters can handle flexible series lengths and achieve competitive performance as prevalent models. Further, we present token-wise prompting that utilizes corresponding timestamps to make ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20998;&#25903;&#23450;&#30028;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02328</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#21450;&#20854;&#22312;&#20998;&#25903;&#23450;&#30028;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Data-driven algorithm design using neural networks with applications to branch-and-cut
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20998;&#25903;&#23450;&#30028;&#26694;&#26550;&#20013;&#65292;&#29992;&#20110;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#26159;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#19968;&#31867;&#31639;&#27861;&#20013;&#36873;&#25321;&#22312;&#26576;&#20010;&#65288;&#26410;&#30693;&#65289;&#38382;&#39064;&#23454;&#20363;&#20998;&#24067;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#31639;&#27861;&#30340;&#33539;&#20363;&#12290;&#26412;&#25991;&#22312;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#26368;&#26032;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#24605;&#36335;&#65292;&#21363;&#19981;&#20165;&#20165;&#36873;&#25321;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#30340;&#21333;&#19968;&#31639;&#27861;&#65292;&#32780;&#26159;&#20801;&#35768;&#26681;&#25454;&#38382;&#39064;&#23454;&#20363;&#36873;&#25321;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#38382;&#39064;&#23454;&#20363;&#26679;&#26412;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#38382;&#39064;&#23454;&#20363;&#26144;&#23556;&#21040;&#26368;&#21512;&#36866;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#24605;&#36335;&#24418;&#24335;&#21270;&#65292;&#24182;&#26681;&#25454;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#35774;&#35745;&#30340;&#24037;&#20316;&#65292;&#25512;&#23548;&#20986;&#20102;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#30340;&#20005;&#26684;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#21040;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#30340;&#20998;&#25903;&#23450;&#30028;&#26694;&#26550;&#20013;&#65292;&#20197;&#20570;&#20986;&#33391;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven algorithm design is a paradigm that uses statistical and machine learning techniques to select from a class of algorithms for a computational problem an algorithm that has the best expected performance with respect to some (unknown) distribution on the instances of the problem. We build upon recent work in this line of research by introducing the idea where, instead of selecting a single algorithm that has the best performance, we allow the possibility of selecting an algorithm based on the instance to be solved. In particular, given a representative sample of instances, we learn a neural network that maps an instance of the problem to the most appropriate algorithm {\em for that instance}. We formalize this idea and derive rigorous sample complexity bounds for this learning problem, in the spirit of recent work in data-driven algorithm design. We then apply this approach to the problem of making good decisions in the branch-and-cut framework for mixed-integer optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#24182;&#23558;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#22240;&#26524;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02277</link><description>&lt;p&gt;
&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#22806;&#28304;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Bayesian Optimization via Exogenous Distribution Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#65292;&#24182;&#23558;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#22240;&#26524;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#23558;&#30446;&#26631;&#21464;&#37327;&#26368;&#22823;&#21270;&#20316;&#20026;&#25805;&#20316;&#30446;&#26631;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;CBO&#65289;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#25913;&#21464;&#22240;&#26524;&#32467;&#26500;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#30828;&#24178;&#39044;&#65292;&#35201;&#20040;&#24341;&#20837;&#21160;&#20316;&#33410;&#28857;&#21040;&#20869;&#29983;&#21464;&#37327;&#20013;&#65292;&#20197;&#35843;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#22806;&#28304;&#21464;&#37327;&#30340;&#20998;&#24067;&#65292;&#36825;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#36890;&#24120;&#34987;&#24573;&#30053;&#25110;&#36890;&#36807;&#26399;&#26395;&#36827;&#34892;&#36793;&#32536;&#21270;&#12290;&#22806;&#28304;&#20998;&#24067;&#23398;&#20064;&#25552;&#39640;&#20102;&#36890;&#24120;&#36890;&#36807;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#35757;&#32451;&#30340;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#21270;&#22240;&#26524;&#27169;&#22411;&#30340;&#36817;&#20284;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#23398;&#20064;&#21040;&#30340;&#22806;&#28304;&#20998;&#24067;&#23558;&#29616;&#26377;&#30340;CBO&#25193;&#23637;&#21040;&#36229;&#20986;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#30340;&#19968;&#33324;&#22240;&#26524;&#26041;&#26696;&#12290;&#24674;&#22797;&#22806;&#28304;&#21464;&#37327;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#22122;&#22768;&#25110;&#26410;&#35266;&#27979;&#21040;&#30340;&#38544;&#34255;&#21464;&#37327;&#20351;&#29992;&#26356;&#28789;&#27963;&#30340;&#20808;&#39564;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;CBO&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods.   Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#24182;&#22238;&#39038;&#20102;&#21253;&#25324;&#26550;&#26500;&#20462;&#25913;&#22312;&#20869;&#30340;&#22810;&#31181;&#25216;&#26415;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#38271;&#19978;&#19979;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.02244</link><description>&lt;p&gt;
&#36229;&#36234;&#26497;&#38480;&#65306;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#26399;&#20026;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#24182;&#22238;&#39038;&#20102;&#21253;&#25324;&#26550;&#26500;&#20462;&#25913;&#22312;&#20869;&#30340;&#22810;&#31181;&#25216;&#26415;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#29702;&#35299;&#38271;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#24778;&#24322;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#29702;&#35299;&#19978;&#19979;&#25991;&#12289;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#21644;&#29983;&#25104;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#20197;&#20005;&#26684;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#20026;&#20195;&#20215;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#26377;&#25928;&#25903;&#25345;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#33021;&#21147;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26368;&#36817;&#20026;&#25193;&#23637;LLMs&#24207;&#21015;&#38271;&#24230;&#32780;&#35774;&#35745;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#23545;&#38271;&#19978;&#19979;&#25991;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22238;&#39038;&#21644;&#20998;&#31867;&#20102;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#20462;&#25913;&#20301;&#32622;&#32534;&#30721;&#21644;&#20462;&#25913;&#27880;&#24847;&#26426;&#21046;&#31561;&#26550;&#26500;&#20462;&#25913;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#26356;&#38271;&#24207;&#21015;&#30340;&#22788;&#29702;&#65292;&#21516;&#26102;&#36991;&#20813;&#35745;&#31639;&#38656;&#27714;&#30340;&#25104;&#27604;&#20363;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#30340;&#22810;&#26679;&#26041;&#27861;&#21487;&#20197;&#22312;LLMs&#30340;&#19981;&#21516;&#38454;&#27573;&#65288;&#21363;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#65289;&#20013;&#21033;&#29992;&#12290;&#36825;&#20351;&#24471;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#24207;&#21015;&#24182;&#25552;&#21319;&#23545;&#38271;&#19978;&#19979;&#25991;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to effic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#32422;&#31616;&#26041;&#27861;&#65292;&#21033;&#29992;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#25237;&#24433;&#32479;&#19968;&#20102;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02239</link><description>&lt;p&gt;
&#20998;&#24067;&#32422;&#31616;&#65306;&#29992;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#25237;&#24433;&#32479;&#19968;&#38477;&#32500;&#21644;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Distributional Reduction: Unifying Dimensionality Reduction and Clustering with Gromov-Wasserstein Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#32422;&#31616;&#26041;&#27861;&#65292;&#21033;&#29992;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#25237;&#24433;&#32479;&#19968;&#20102;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#25429;&#25417;&#28508;&#22312;&#30340;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#28041;&#21450;&#20351;&#29992;&#38477;&#32500;&#26041;&#27861;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#30340;&#31354;&#38388;&#19978;&#65292;&#25110;&#23558;&#25968;&#25454;&#28857;&#32452;&#32455;&#25104;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26159;&#25353;&#39034;&#24207;&#20351;&#29992;&#30340;&#65292;&#32780;&#19981;&#33021;&#20445;&#35777;&#32858;&#31867;&#19982;&#38477;&#32500;&#30456;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65306;&#20351;&#29992;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65292;&#25105;&#20204;&#23558;&#32858;&#31867;&#21644;&#38477;&#32500;&#32479;&#19968;&#20026;&#19968;&#20010;&#31216;&#20026;&#20998;&#24067;&#32422;&#31616;&#30340;&#21333;&#19968;&#26694;&#26550;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#32858;&#31867;&#21644;&#38477;&#32500;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning aims to capture the underlying structure of potentially large and high-dimensional datasets. Traditionally, this involves using dimensionality reduction methods to project data onto interpretable spaces or organizing points into meaningful clusters. In practice, these methods are used sequentially, without guaranteeing that the clustering aligns well with the conducted dimensionality reduction. In this work, we offer a fresh perspective: that of distributions. Leveraging tools from optimal transport, particularly the Gromov-Wasserstein distance, we unify clustering and dimensionality reduction into a single framework called distributional reduction. This allows us to jointly address clustering and dimensionality reduction with a single optimization problem. Through comprehensive experiments, we highlight the versatility and interpretability of our method and show that it outperforms existing approaches across a variety of image and genomics datasets.
&lt;/p&gt;</description></item><item><title>&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#30340;&#23384;&#22312;&#65292;&#36825;&#31181;&#25200;&#21160;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#23545;&#21407;&#22987;&#22270;&#20687;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36755;&#20837;&#32500;&#24230;&#36229;&#36807;&#36755;&#20986;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#36830;&#32493;&#30340;&#26080;&#23475;&#25200;&#21160;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#19968;&#26063;&#36890;&#29992;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#19968;&#33268;&#22320;&#24433;&#21709;&#32593;&#32476;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21363;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#25200;&#21160;&#21487;&#33021;&#19981;&#20250;&#24433;&#21709;&#20854;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02095</link><description>&lt;p&gt;
&#30524;&#35265;&#26410;&#24517;&#20026;&#23454;&#65306;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Seeing is not always believing: The Space of Harmless Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02095
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#30340;&#23384;&#22312;&#65292;&#36825;&#31181;&#25200;&#21160;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#23545;&#21407;&#22987;&#22270;&#20687;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36755;&#20837;&#32500;&#24230;&#36229;&#36807;&#36755;&#20986;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#36830;&#32493;&#30340;&#26080;&#23475;&#25200;&#21160;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#19968;&#26063;&#36890;&#29992;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#19968;&#33268;&#22320;&#24433;&#21709;&#32593;&#32476;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21363;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#25200;&#21160;&#21487;&#33021;&#19981;&#20250;&#24433;&#21709;&#20854;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#30340;&#23384;&#22312;&#65292;&#21363;&#25200;&#21160;&#20250;&#20351;&#32593;&#32476;&#36755;&#20986;&#23436;&#20840;&#19981;&#21464;&#12290;&#26080;&#35770;&#36825;&#20123;&#25200;&#21160;&#22312;&#24212;&#29992;&#20110;&#22270;&#20687;&#26102;&#30340;&#22823;&#23567;&#22914;&#20309;&#65292;&#21482;&#35201;&#23427;&#20204;&#20301;&#20110;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#20869;&#65292;&#23601;&#19981;&#20250;&#23545;&#21407;&#22987;&#22270;&#20687;&#30340;&#32593;&#32476;&#36755;&#20986;&#20135;&#29983;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#32593;&#32476;&#20013;&#30340;&#20219;&#20309;&#32447;&#24615;&#23618;&#65292;&#36755;&#20837;&#32500;&#24230;$n$&#36229;&#36807;&#36755;&#20986;&#32500;&#24230;$m$&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36830;&#32493;&#26080;&#23475;&#25200;&#21160;&#23376;&#31354;&#38388;&#30340;&#23384;&#22312;&#65292;&#20854;&#32500;&#24230;&#20026;$(n-m)$&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#26063;&#19968;&#33268;&#24433;&#21709;&#32593;&#32476;&#36755;&#20986;&#30340;&#36890;&#29992;&#25200;&#21160;&#65292;&#32780;&#19981;&#35770;&#23427;&#20204;&#30340;&#22823;&#23567;&#22914;&#20309;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#23475;&#25200;&#21160;&#22312;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#20351;&#29992;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21363;&#34987;&#20154;&#31867;&#25429;&#25417;&#21040;&#30340;&#37325;&#35201;&#25200;&#21160;&#21487;&#33021;&#19981;&#20250;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of deep neural networks, we expose the existence of a harmless perturbation space, where perturbations leave the network output entirely unaltered. Perturbations within this harmless perturbation space, regardless of their magnitude when applied to images, exhibit no impact on the network's outputs of the original images. Specifically, given any linear layer within the network, where the input dimension $n$ exceeds the output dimension $m$, we demonstrate the existence of a continuous harmless perturbation subspace with a dimension of $(n-m)$. Inspired by this, we solve for a family of general perturbations that consistently influence the network output, irrespective of their magnitudes. With these theoretical findings, we explore the application of harmless perturbations for privacy-preserving data usage. Our work reveals the difference between DNNs and human perception that the significant perturbations captured by humans may not affect the recognition of DNNs. As a re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#21453;&#21521;&#24037;&#31243;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#20381;&#36182;&#20869;&#37096;&#29305;&#24449;&#22270;&#26469;&#26816;&#27979;&#21644;&#21453;&#21521;&#24037;&#31243;&#21518;&#38376;&#65292;&#24182;&#35782;&#21035;&#20854;&#30446;&#26631;&#31867;&#21035;&#65292;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.02034</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#38024;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#21453;&#21521;&#24037;&#31243;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Post-Training Reverse-Engineering Defense Against Backdoors in Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#21453;&#21521;&#24037;&#31243;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#20381;&#36182;&#20869;&#37096;&#29305;&#24449;&#22270;&#26469;&#26816;&#27979;&#21644;&#21453;&#21521;&#24037;&#31243;&#21518;&#38376;&#65292;&#24182;&#35782;&#21035;&#20854;&#30446;&#26631;&#31867;&#21035;&#65292;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#26041;&#27861;&#12290;&#36890;&#29992;&#26041;&#27861;&#26088;&#22312;&#21487;&#38752;&#22320;&#26816;&#27979;&#21644;/&#25110;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;&#65292;&#32780;&#21453;&#21521;&#24037;&#31243;&#26041;&#27861;&#36890;&#24120;&#26126;&#30830;&#20551;&#35774;&#20854;&#20013;&#19968;&#31181;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#22120;&#65292;&#23427;&#20381;&#36182;&#20110;&#34987;&#38450;&#23432;&#30340;DNN&#30340;&#20869;&#37096;&#29305;&#24449;&#22270;&#26469;&#26816;&#27979;&#21644;&#21453;&#21521;&#24037;&#31243;&#21518;&#38376;&#65292;&#24182;&#35782;&#21035;&#20854;&#30446;&#26631;&#31867;&#21035;&#65307;&#23427;&#21487;&#20197;&#22312;&#21518;&#35757;&#32451;&#26102;&#25805;&#20316;&#65288;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#38598;&#65289;&#65307;&#23545;&#20110;&#19981;&#21516;&#30340;&#23884;&#20837;&#26426;&#21046;&#65288;&#21363;&#36890;&#29992;&#30340;&#65289;&#38750;&#24120;&#26377;&#25928;&#65307;&#24182;&#19988;&#20855;&#26377;&#20302;&#35745;&#31639;&#24320;&#38144;&#65292;&#22240;&#27492;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;CIFAR-10&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#19981;&#21516;&#25915;&#20987;&#36827;&#34892;&#20102;&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A variety of defenses have been proposed against backdoors attacks on deep neural network (DNN) classifiers. Universal methods seek to reliably detect and/or mitigate backdoors irrespective of the incorporation mechanism used by the attacker, while reverse-engineering methods often explicitly assume one. In this paper, we describe a new detector that: relies on internal feature map of the defended DNN to detect and reverse-engineer the backdoor and identify its target class; can operate post-training (without access to the training dataset); is highly effective for various incorporation mechanisms (i.e., is universal); and which has low computational overhead and so is scalable. Our detection approach is evaluated for different attacks on a benchmark CIFAR-10 image classifier.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#12290;&#36890;&#36807;&#31454;&#20105;&#27604;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#21644;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.01995</link><description>&lt;p&gt;
&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#65306;&#31532;&#19968;&#27425;&#36817;&#20284;&#31639;&#27861;&#65292;&#20855;&#26377;&#20840;&#32622;&#20449;&#21306;&#38388;&#38598;&#25104;&#30340;&#23398;&#20064;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Online Uniform Risk Times Sampling: First Approximation Algorithms, Learning Augmentation with Full Confidence Interval Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#24341;&#20837;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#12290;&#36890;&#36807;&#31454;&#20105;&#27604;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#23454;&#39564;&#21644;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#65292;&#23558;&#26377;&#38480;&#30340;&#27835;&#30103;&#39044;&#31639;&#20998;&#37197;&#21040;&#21487;&#29992;&#30340;&#39118;&#38505;&#26102;&#38388;&#19978;&#26159;&#20943;&#23569;&#29992;&#25143;&#30130;&#21171;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26410;&#30693;&#30340;&#23454;&#38469;&#39118;&#38505;&#26102;&#38388;&#25968;&#37327;&#65292;&#36825;&#19968;&#31574;&#30053;&#36935;&#21040;&#20102;&#26174;&#33879;&#30340;&#38556;&#30861;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#29702;&#35770;&#20445;&#35777;&#26041;&#38754;&#36824;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#22312;&#32447;&#22343;&#21248;&#39118;&#38505;&#26102;&#38388;&#25277;&#26679;&#38382;&#39064;&#24341;&#20837;&#36817;&#20284;&#31639;&#27861;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#32447;&#36817;&#20284;&#31639;&#27861;&#65292;&#19968;&#31181;&#24102;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#19968;&#31181;&#27809;&#26377;&#23398;&#20064;&#22686;&#24378;&#65292;&#24182;&#20351;&#29992;&#31454;&#20105;&#27604;&#20998;&#26512;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#23454;&#39564;&#21644;HeartSteps&#31227;&#21160;&#24212;&#29992;&#30340;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In digital health, the strategy of allocating a limited treatment budget across available risk times is crucial to reduce user fatigue. This strategy, however, encounters a significant obstacle due to the unknown actual number of risk times, a factor not adequately addressed by existing methods lacking theoretical guarantees. This paper introduces, for the first time, the online uniform risk times sampling problem within the approximation algorithm framework. We propose two online approximation algorithms for this problem, one with and one without learning augmentation, and provide rigorous theoretical performance guarantees for them using competitive ratio analysis. We assess the performance of our algorithms using both synthetic experiments and a real-world case study on HeartSteps mobile applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20248;&#21270;&#26041;&#27861;&#20998;&#26512;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#38750;&#28176;&#36817;&#35774;&#32622;&#19979;&#30340;&#31934;&#30830;&#39044;&#27979;&#20998;&#25968;&#20989;&#25968;&#21644;&#25910;&#25947;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01965</link><description>&lt;p&gt;
&#36890;&#36807;&#20984;&#20248;&#21270;&#23545;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analyzing Neural Network-Based Generative Diffusion Models through Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20984;&#20248;&#21270;&#26041;&#27861;&#20998;&#26512;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#38750;&#28176;&#36817;&#35774;&#32622;&#19979;&#30340;&#31934;&#30830;&#39044;&#27979;&#20998;&#25968;&#20989;&#25968;&#21644;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#20013;&#21464;&#24471;&#24191;&#27867;&#20351;&#29992;&#12290;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#33073;&#39062;&#32780;&#20986;&#65292;&#38656;&#35201;&#20272;&#35745;&#36755;&#20837;&#25968;&#25454;&#20998;&#24067;&#30340;&#20998;&#25968;&#20989;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20998;&#25968;&#21305;&#37197;&#21644;&#21435;&#22122;&#20998;&#25968;&#21305;&#37197;&#37325;&#26032;&#26500;&#24314;&#20026;&#20984;&#20248;&#21270;&#30340;&#24418;&#24335;&#65292;&#26469;&#20998;&#26512;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#25193;&#25955;&#29702;&#35770;&#20027;&#35201;&#26159;&#28176;&#36817;&#30340;&#65292;&#20294;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#32473;&#20986;&#20102;&#31934;&#30830;&#30340;&#39044;&#27979;&#20998;&#25968;&#20989;&#25968;&#65292;&#24182;&#24314;&#31435;&#20102;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#28176;&#36817;&#35774;&#32622;&#20013;&#23398;&#20064;&#21040;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are becoming widely used in state-of-the-art image, video and audio generation. Score-based diffusion models stand out among these methods, necessitating the estimation of score function of the input data distribution. In this study, we present a theoretical framework to analyze two-layer neural network-based diffusion models by reframing score matching and denoising score matching as convex optimization. Though existing diffusion theory is mainly asymptotic, we characterize the exact predicted score function and establish the convergence result for neural network-based diffusion models with finite data. This work contributes to understanding what neural network-based diffusion model learns in non-asymptotic settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#32463;&#20856;&#21457;&#29616;&#31639;&#27861;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26356;&#22909;&#30340;&#34920;&#29616;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.01929</link><description>&lt;p&gt;
&#26679;&#26412;&#12289;&#20272;&#35745;&#12289;&#32858;&#21512;&#65306;&#22240;&#26524;&#21457;&#29616;&#22522;&#30784;&#27169;&#22411;&#30340;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample, estimate, aggregate: A recipe for causal discovery foundation models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#35757;&#32451;&#19982;&#32463;&#20856;&#21457;&#29616;&#31639;&#27861;&#30340;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#20934;&#30830;&#22320;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#26356;&#22909;&#30340;&#34920;&#29616;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26159;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#30340;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#31185;&#23398;&#30740;&#31350;&#12289;&#25351;&#23548;&#20915;&#31574;&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#30340;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#29305;&#24615;&#20351;&#23427;&#20204;&#21464;&#24471;&#32531;&#24930;&#12289;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#24182;&#19988;&#33030;&#24369;&#12290;&#21463;&#22522;&#30784;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#26694;&#26550;&#65292;&#20854;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#35757;&#32451;&#29992;&#20110;&#22788;&#29702;&#22312;&#36739;&#23567;&#30340;&#21464;&#37327;&#23376;&#38598;&#19978;&#36816;&#34892;&#30340;&#32463;&#20856;&#21457;&#29616;&#31639;&#27861;&#30340;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#20197;&#19979;&#35266;&#23519;&#32467;&#26524;&#65306;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#22312;&#23567;&#38382;&#39064;&#19978;&#35745;&#31639;&#36895;&#24230;&#24555;&#65292;&#23545;&#65288;&#36793;&#38469;&#65289;&#25968;&#25454;&#32467;&#26500;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#19988;&#23427;&#20204;&#30340;&#36755;&#20986;&#32467;&#26500;&#20316;&#20026;&#23545;&#35937;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#65292;&#24182;&#19988;&#25552;&#20379;&#27604;&#29616;&#26377;&#27169;&#22411;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery, the task of inferring causal structure from data, promises to accelerate scientific research, inform policy making, and more. However, the per-dataset nature of existing causal discovery algorithms renders them slow, data hungry, and brittle. Inspired by foundation models, we propose a causal discovery framework where a deep learning model is pretrained to resolve predictions from classical discovery algorithms run over smaller subsets of variables. This method is enabled by the observations that the outputs from classical algorithms are fast to compute for small problems, informative of (marginal) data structure, and their structure outputs as objects remain comparable across datasets. Our method achieves state-of-the-art performance on synthetic and realistic datasets, generalizes to data generating mechanisms not seen during training, and offers inference speeds that are orders of magnitude faster than existing models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiPO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#25490;&#21517;&#21015;&#34920;&#20013;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#31574;&#30053;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#21487;&#34892;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.01878</link><description>&lt;p&gt;
LiPO: &#36890;&#36807;&#23398;&#20064;&#25490;&#24207;&#36827;&#34892;&#21015;&#34920;&#22411;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
LiPO: Listwise Preference Optimization through Learning-to-Rank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiPO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#25490;&#21517;&#21015;&#34920;&#20013;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#31574;&#30053;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#21487;&#34892;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#26159;&#25511;&#21046;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34892;&#20026;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;DPO&#21644;SLiC&#65292;&#25104;&#20026;&#20256;&#32479;&#30340;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#24037;&#21453;&#39304;&#36890;&#24120;&#20197;&#23545;&#22810;&#20010;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#30340;&#26684;&#24335;&#25552;&#20379;&#65292;&#20197;&#25674;&#38144;&#38405;&#35835;&#25552;&#31034;&#30340;&#25104;&#26412;&#12290;&#22810;&#20010;&#21709;&#24212;&#20063;&#21487;&#20197;&#36890;&#36807;&#22870;&#21169;&#27169;&#22411;&#25110;AI&#21453;&#39304;&#36827;&#34892;&#25490;&#24207;&#12290;&#32570;&#23569;&#20851;&#20110;&#30452;&#25509;&#36866;&#24212;&#21709;&#24212;&#21015;&#34920;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#21015;&#34920;&#22411;&#20559;&#22909;&#20248;&#21270;&#65288;LiPO&#65289;&#26694;&#26550;&#65292;&#22312;&#32473;&#23450;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#31574;&#30053;&#21487;&#20197;&#20174;&#19968;&#20010;&#25490;&#21517;&#21015;&#34920;&#20013;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21487;&#34892;&#21709;&#24212;&#12290;&#36825;&#31181;&#35266;&#28857;&#19982;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#24418;&#25104;&#26126;&#30830;&#30340;&#32852;&#31995;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20559;&#22909;&#20248;&#21270;&#24037;&#20316;&#21487;&#20197;&#26144;&#23556;&#21040;&#29616;&#26377;&#30340;&#25490;&#21517;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the Listwise Preference Optimization (LiPO) framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives, esp
&lt;/p&gt;</description></item><item><title>LTAU-FF&#26159;&#19968;&#31181;&#21033;&#29992;&#25439;&#22833;&#36712;&#36857;&#20998;&#26512;&#26469;&#20272;&#35745;&#21407;&#23376;&#21147;&#22330;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#21644;&#27169;&#22411;&#28508;&#31354;&#38388;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#38598;&#21512;&#34920;&#31034;&#21644;&#19981;&#30830;&#23450;&#24230;&#37327;&#65292;&#26080;&#38656;&#35780;&#20272;&#22810;&#20010;&#27169;&#22411;&#65292;&#33021;&#20934;&#30830;&#39044;&#27979;&#27979;&#35797;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.00853</link><description>&lt;p&gt;
LTAU-FF: &#21407;&#23376;&#21147;&#22330;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#25439;&#22833;&#36712;&#36857;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
LTAU-FF: Loss Trajectory Analysis for Uncertainty in Atomistic Force Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00853
&lt;/p&gt;
&lt;p&gt;
LTAU-FF&#26159;&#19968;&#31181;&#21033;&#29992;&#25439;&#22833;&#36712;&#36857;&#20998;&#26512;&#26469;&#20272;&#35745;&#21407;&#23376;&#21147;&#22330;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#21644;&#27169;&#22411;&#28508;&#31354;&#38388;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27169;&#22411;&#38598;&#21512;&#34920;&#31034;&#21644;&#19981;&#30830;&#23450;&#24230;&#37327;&#65292;&#26080;&#38656;&#35780;&#20272;&#22810;&#20010;&#27169;&#22411;&#65292;&#33021;&#20934;&#30830;&#39044;&#27979;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#38598;&#21512;&#26159;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#21407;&#23376;&#21147;&#22330;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20351;&#29992;&#22522;&#20110;&#38598;&#21512;&#30340;&#19981;&#30830;&#23450;&#24230;&#37327;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#38598;&#21512;&#20135;&#29983;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#36880;&#26679;&#26412;&#35823;&#24046;&#30340;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#65288;CDF&#65289;&#26469;&#39640;&#25928;&#34920;&#31034;&#27169;&#22411;&#38598;&#21512;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#22522;&#20110;&#36317;&#31163;&#30340;&#27169;&#22411;&#28508;&#31354;&#38388;&#20013;&#30340;&#30456;&#20284;&#24615;&#25628;&#32034;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#19981;&#30830;&#23450;&#24230;&#37327;&#25351;&#26631;&#65288;&#31216;&#20026;LTAU&#65289;&#65292;&#23427;&#22312;&#35757;&#32451;&#25110;&#25512;&#29702;&#36807;&#31243;&#20013;&#26080;&#38656;&#35780;&#20272;&#22810;&#20010;&#27169;&#22411;&#65292;&#21516;&#26102;&#21457;&#25381;&#20102;&#38598;&#21512;&#25216;&#26415;&#30340;&#20248;&#21183;&#12290;&#20316;&#20026;&#21021;&#22987;&#27979;&#35797;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20272;&#35745;&#21407;&#23376;&#21147;&#22330;&#20013;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65288;LTAU-FF&#65289;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#34987;&#36731;&#26494;&#22320;&#26657;&#20934;&#20197;&#20934;&#30830;&#39044;&#27979;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model ensembles are simple and effective tools for estimating the prediction uncertainty of deep learning atomistic force fields. Despite this, widespread adoption of ensemble-based uncertainty quantification (UQ) techniques is limited by the high computational costs incurred by ensembles during both training and inference. In this work we leverage the cumulative distribution functions (CDFs) of per-sample errors obtained over the course of training to efficiently represent the model ensemble, and couple them with a distance-based similarity search in the model latent space. Using these tools, we develop a simple UQ metric (which we call LTAU) that leverages the strengths of ensemble-based techniques without requiring the evaluation of multiple models during either training or inference. As an initial test, we apply our method towards estimating the epistemic uncertainty in atomistic force fields (LTAU-FF) and demonstrate that it can be easily calibrated to accurately predict test erro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#20013;&#65292;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#21306;&#20998;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.00793</link><description>&lt;p&gt;
&#26080;&#27861;&#21306;&#20998;&#30340;&#21306;&#20998;&#65306;&#31639;&#27861;&#39044;&#27979;&#20013;&#30340;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#20013;&#65292;&#37325;&#28857;&#22312;&#20110;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#21306;&#20998;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#32435;&#20837;&#31639;&#27861;&#39044;&#27979;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21033;&#29992;&#20154;&#30340;&#21028;&#26029;&#21147;&#26469;&#21306;&#20998;&#37027;&#20123;&#23545;&#20110;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#31639;&#27861;&#26469;&#35828;&#8220;&#30475;&#36215;&#26469;&#30456;&#21516;&#8221;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#26694;&#26550;&#33021;&#22815;&#28548;&#28165;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#19987;&#23478;&#36890;&#24120;&#20855;&#26377;&#20449;&#24687;&#30340;&#35775;&#38382;&#26435;&#38480;&#8212;&#8212;&#29305;&#21035;&#26159;&#20027;&#35266;&#20449;&#24687;&#8212;&#8212;&#32780;&#36825;&#20123;&#20449;&#24687;&#26159;&#31639;&#27861;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#32534;&#30721;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#35748;&#35782;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#32452;&#26377;&#21407;&#21017;&#30340;&#31639;&#27861;&#65292;&#20165;&#22312;&#20219;&#20309;&#21487;&#34892;&#30340;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#26377;&#25152;&#25913;&#21892;&#26102;&#25165;&#36873;&#25321;&#24615;&#22320;&#32435;&#20837;&#20154;&#31867;&#21453;&#39304;&#12290;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#31639;&#27861;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#24448;&#24448;&#20248;&#20110;&#20154;&#31867;&#23545;&#24212;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#31867;&#21028;&#26029;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65288;&#21487;&#20197;&#39044;&#20808;&#30830;&#23450;&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;X&#23556;&#32447;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#23376;&#38598;&#22312;&#24739;&#32773;&#32676;&#20307;&#20013;&#21344;&#25454;&#20102;&#36817;30%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#24335;&#65292;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#27010;&#29575;&#30005;&#36335;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35828;&#26126;&#20102;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#25104;&#21151;&#22320;&#26500;&#24314;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#21644;&#28151;&#21512;&#27010;&#29575;&#30005;&#36335;&#30740;&#31350;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00759</link><description>&lt;p&gt;
&#26500;&#24314;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Building Expressive and Tractable Probabilistic Generative Models: A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#20102;&#27010;&#29575;&#30005;&#36335;&#12290;&#25991;&#31456;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#24182;&#35828;&#26126;&#20102;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#25104;&#21151;&#22320;&#26500;&#24314;&#20102;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#27010;&#29575;&#30005;&#36335;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#28145;&#24230;&#21644;&#28151;&#21512;&#27010;&#29575;&#30005;&#36335;&#30740;&#31350;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#20013;&#30340;&#36827;&#23637;&#21644;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#27010;&#29575;&#30005;&#36335;&#65288;PCs&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#22266;&#26377;&#26435;&#34913;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#31361;&#20986;&#20102;&#20351;PCs&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#39640;&#25928;&#30340;&#35774;&#35745;&#21407;&#21017;&#21644;&#31639;&#27861;&#25193;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#39046;&#22495;&#30340;&#20998;&#31867;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26368;&#36817;&#36890;&#36807;&#34701;&#21512;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#27010;&#24565;&#26469;&#26500;&#24314;&#28145;&#24230;&#21644;&#28151;&#21512;PCs&#30340;&#21162;&#21147;&#65292;&#24182;&#27010;&#36848;&#20102;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#32570;&#22833;&#20027;&#35201;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26367;&#20195;&#32467;&#26524;&#26469;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32435;&#20837;&#26367;&#20195;&#32467;&#26524;&#24182;&#36991;&#20813;&#36873;&#25321;&#20559;&#35823;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#20272;&#35745;&#20540;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#22312;&#26041;&#24046;&#26041;&#38754;&#21487;&#33021;&#27604;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#33391;&#22909;&#23454;&#35777;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00168</link><description>&lt;p&gt;
&#20351;&#29992;&#26367;&#20195;&#32467;&#26524;&#36827;&#34892;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Continuous Treatment Effects with Surrogate Outcomes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#32570;&#22833;&#20027;&#35201;&#32467;&#26524;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26367;&#20195;&#32467;&#26524;&#26469;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#32435;&#20837;&#26367;&#20195;&#32467;&#26524;&#24182;&#36991;&#20813;&#36873;&#25321;&#20559;&#35823;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#20272;&#35745;&#20540;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#22312;&#26041;&#24046;&#26041;&#38754;&#21487;&#33021;&#27604;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;&#27169;&#25311;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#33391;&#22909;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22240;&#26524;&#25512;&#26029;&#24212;&#29992;&#20013;&#65292;&#20027;&#35201;&#32467;&#26524;&#65288;&#26631;&#31614;&#65289;&#24120;&#24120;&#26159;&#37096;&#20998;&#32570;&#22833;&#30340;&#65292;&#29305;&#21035;&#26159;&#22914;&#26524;&#23427;&#20204;&#24456;&#26114;&#36149;&#25110;&#24456;&#38590;&#25910;&#38598;&#12290;&#22914;&#26524;&#32570;&#22833;&#20381;&#36182;&#20110;&#21327;&#21464;&#37327;&#65288;&#21363;&#32570;&#22833;&#19981;&#23436;&#20840;&#38543;&#26426;&#65289;&#65292;&#20165;&#22522;&#20110;&#23436;&#20840;&#35266;&#27979;&#26679;&#26412;&#30340;&#20998;&#26512;&#21487;&#33021;&#23384;&#22312;&#20559;&#35823;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32467;&#21512;&#19982;&#20027;&#35201;&#32467;&#26524;&#30456;&#20851;&#30340;&#23436;&#20840;&#35266;&#27979;&#30340;&#27835;&#30103;&#21518;&#21464;&#37327;&#65288;&#26367;&#20195;&#32467;&#26524;&#65289;&#21487;&#20197;&#25913;&#36827;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26367;&#20195;&#32467;&#26524;&#22312;&#20272;&#35745;&#36830;&#32493;&#27835;&#30103;&#25928;&#26524;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#26041;&#27861;&#65292;&#20197;&#39640;&#25928;&#22320;&#23558;&#26367;&#20195;&#32467;&#26524;&#32435;&#20837;&#20998;&#26512;&#20013;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#20250;&#21463;&#21040;&#19978;&#36848;&#36873;&#25321;&#20559;&#35823;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25152;&#25552;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26041;&#24046;&#30340;&#21487;&#33021;&#25913;&#36827;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#32463;&#39564;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world causal inference applications, the primary outcomes (labels) are often partially missing, especially if they are expensive or difficult to collect. If the missingness depends on covariates (i.e., missingness is not completely at random), analyses based on fully-observed samples alone may be biased. Incorporating surrogates, which are fully observed post-treatment variables related to the primary outcome, can improve estimation in this case. In this paper, we study the role of surrogates in estimating continuous treatment effects and propose a doubly robust method to efficiently incorporate surrogates in the analysis, which uses both labeled and unlabeled data and does not suffer from the above selection bias problem. Importantly, we establish asymptotic normality of the proposed estimator and show possible improvements on the variance compared with methods that solely use labeled data. Extensive simulations show our methods enjoy appealing empirical performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.15964</link><description>&lt;p&gt;
&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Pre-training for Localized Instruction Generation of Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sieve-&amp;-Swap&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#24182;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#65292;&#20174;&#32780;&#23454;&#29616;&#35270;&#39057;&#26412;&#22320;&#21270;&#25351;&#20196;&#29983;&#25104;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35270;&#39057;&#23637;&#31034;&#20102;&#35832;&#22914;&#39135;&#35889;&#20934;&#22791;&#31561;&#20219;&#21153;&#30340;&#36880;&#27493;&#28436;&#31034;&#12290;&#29702;&#35299;&#27492;&#31867;&#35270;&#39057;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#23545;&#27493;&#39588;&#36827;&#34892;&#31934;&#30830;&#23450;&#20301;&#24182;&#29983;&#25104;&#25991;&#23383;&#35828;&#26126;&#12290;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#24182;&#32534;&#20889;&#35828;&#26126;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#38480;&#21046;&#20102;&#24403;&#21069;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#24182;&#38459;&#30861;&#20102;&#26377;&#25928;&#23398;&#20064;&#12290;&#21033;&#29992;&#22823;&#35268;&#27169;&#20294;&#22024;&#26434;&#30340;&#35270;&#39057;-&#25991;&#26412;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#21319;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#36716;&#24405;&#21253;&#21547;&#26080;&#20851;&#20869;&#23481;&#65292;&#19982;&#20154;&#31867;&#27880;&#37322;&#21592;&#32534;&#20889;&#30340;&#35828;&#26126;&#30456;&#27604;&#23384;&#22312;&#39118;&#26684;&#21464;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;Sieve-&amp;-Swap&#65292;&#36890;&#36807;&#33258;&#21160;&#31579;&#36873;&#20986;&#19981;&#30456;&#20851;&#25991;&#26412;&#21644;&#20351;&#29992;&#25991;&#26412;&#39135;&#35889;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#32534;&#20889;&#30340;&#35828;&#26126;&#33258;&#21160;&#26367;&#25442;&#25991;&#26412;&#36716;&#24405;&#20197;&#22686;&#24378;&#25991;&#23383;&#25351;&#20196;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&amp;-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;Thompson&#25277;&#26679;&#31639;&#27861;&#65292;&#24378;&#35843;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#65292;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#27492;&#20462;&#25913;&#20165;&#23545;&#32047;&#31215;&#36951;&#25022;&#20135;&#29983;&#24658;&#23450;&#30340;&#24809;&#32602;&#12290;</title><link>https://arxiv.org/abs/2311.09483</link><description>&lt;p&gt;
&#20855;&#26377;&#29992;&#25143;&#23450;&#20041;&#30446;&#26631;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#29992;&#20110;&#20581;&#24247;&#34892;&#20026;&#25913;&#21464;
&lt;/p&gt;
&lt;p&gt;
Adaptive Interventions with User-Defined Goals for Health Behavior Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;Thompson&#25277;&#26679;&#31639;&#27861;&#65292;&#24378;&#35843;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#65292;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#27492;&#20462;&#25913;&#20165;&#23545;&#32047;&#31215;&#36951;&#25022;&#20135;&#29983;&#24658;&#23450;&#30340;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#27963;&#21160;&#19981;&#36275;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#20844;&#20849;&#20581;&#24247;&#38382;&#39064;&#65292;&#19982;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;2&#22411;&#31958;&#23615;&#30149;&#31561;&#19981;&#33391;&#20581;&#24247;&#32467;&#26524;&#30456;&#20851;&#12290;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#31243;&#24207;&#20026;&#20302;&#25104;&#26412;&#12289;&#21487;&#25193;&#23637;&#30340;&#36523;&#20307;&#27963;&#21160;&#20419;&#36827;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#65292;&#28982;&#32780;&#36890;&#24120;&#25928;&#26524;&#36739;&#23567;&#65292;&#31896;&#38468;&#29575;&#20302;&#65292;&#29305;&#21035;&#26159;&#19982;&#20154;&#31867;&#36741;&#23548;&#30456;&#27604;&#12290;&#30446;&#26631;&#35774;&#23450;&#26159;&#20581;&#24247;&#36741;&#23548;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#20013;&#19968;&#30452;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;Thompson&#25277;&#26679;&#31639;&#27861;&#30340;&#20462;&#25913;&#65292;&#37325;&#28857;&#25918;&#22312;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#12290;&#20316;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#30340;&#19968;&#27493;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#21516;&#26102;&#20248;&#21270;&#20010;&#20154;&#20559;&#22909;&#21644;&#30446;&#26631;&#30340;&#24179;&#34913;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#25913;&#21482;&#23545;&#32047;&#31215;&#36951;&#25022;&#36896;&#25104;&#19968;&#20010;&#24120;&#25968;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09483v2 Announce Type: replace-cross  Abstract: Physical inactivity remains a major public health concern, having associations with adverse health outcomes such as cardiovascular disease and type-2 diabetes. Mobile health applications present a promising avenue for low-cost, scalable physical activity promotion, yet often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. Goal-setting is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. This paper introduces a modification to the Thompson sampling algorithm that places emphasis on individualized goal-setting by optimizing personalized reward functions. As a step towards supporting goal-setting, this paper offers a balanced approach that can leverage shared structure while optimizing individual preferences and goals. We prove that our modification incurs only a constant penalty on the cumulative regret 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#38750;&#21442;&#25968;&#38544;&#24615;&#31867;&#21035;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#28508;&#22312;&#31867;&#21035;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#20173;&#28982;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;</title><link>https://arxiv.org/abs/2311.07454</link><description>&lt;p&gt;
&#31163;&#25955;&#38750;&#21442;&#25968;&#38544;&#24615;&#31867;&#21035;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Discrete Nonparametric Causal Discovery Under Latent Class Confounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#38750;&#21442;&#25968;&#38544;&#24615;&#31867;&#21035;&#28151;&#28102;&#19979;&#30340;&#22240;&#26524;&#21457;&#29616;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#28508;&#22312;&#31867;&#21035;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#20173;&#28982;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#21521;&#26080;&#29615;&#22270;&#29992;&#20110;&#24314;&#27169;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;"&#22240;&#26524;&#21457;&#29616;"&#25551;&#36848;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36825;&#31181;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#24403;&#25968;&#25454;&#26159;&#26469;&#33258;&#22810;&#20010;&#28304;&#65288;&#32676;&#20307;&#25110;&#29615;&#22659;&#65289;&#30340;&#32858;&#21512;&#29289;&#26102;&#65292;&#20840;&#23616;&#28151;&#28102;&#20351;&#39537;&#21160;&#35768;&#22810;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#29305;&#24615;&#21464;&#24471;&#27169;&#31946;&#12290;&#36825;&#31181;&#24773;&#20917;&#26377;&#26102;&#34987;&#31216;&#20026;&#28151;&#21512;&#27169;&#22411;&#25110;&#28508;&#22312;&#31867;&#21035;&#12290;&#34429;&#28982;&#19968;&#20123;&#29616;&#20195;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#33021;&#22815;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#22788;&#29702;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#65292;&#20294;&#26159;&#30446;&#21069;&#25152;&#30693;&#30340;&#22788;&#29702;&#20840;&#23616;&#28151;&#28102;&#30340;&#26041;&#27861;&#37117;&#28041;&#21450;&#19981;&#36866;&#29992;&#20110;&#31163;&#25955;&#20998;&#24067;&#30340;&#21442;&#25968;&#20551;&#35774;&#12290;&#20197;&#31163;&#25955;&#21644;&#38750;&#21442;&#25968;&#35266;&#23519;&#21464;&#37327;&#20026;&#37325;&#28857;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26377;&#38480;&#30340;&#28508;&#22312;&#31867;&#21035;&#19979;&#65292;&#22240;&#26524;&#21457;&#29616;&#20173;&#28982;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#36825;&#20010;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#30001;&#20840;&#23616;&#28151;&#28102;&#30340;&#22522;&#25968;&#12289;&#35266;&#23519;&#21464;&#37327;&#30340;&#22522;&#25968;&#31561;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07454v2 Announce Type: replace Abstract: Directed acyclic graphs are used to model the causal structure of a system. ``Causal discovery'' describes the problem of learning this structure from data. When data is an aggregate from multiple sources (populations or environments), global confounding obscures conditional independence properties that drive many causal discovery algorithms. This setting is sometimes known as a mixture model or a latent class. While some modern methods for causal discovery are able to work around unobserved confounding in specific cases, the only known ways to deal with a global confounder involve parametric assumptions. that are unsuitable for discrete distributions.Focusing on discrete and non-parametric observed variables, we demonstrate that causal discovery can still be identifiable under bounded latent classes. The feasibility of this problem is governed by a trade-off between the cardinality of the global confounder, the cardinalities of the o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25152;&#35859;&#30340;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#24182;&#21033;&#29992;&#24726;&#35770;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#36827;&#34892;&#27491;&#30830;&#30340;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#32467;&#35770;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.06840</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#36951;&#28431;&#26631;&#31614;: &#19968;&#39033;&#20851;&#20110;&#24726;&#35770;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Omitted Labels in Causality: A Study of Paradoxes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25152;&#35859;&#30340;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#24182;&#21033;&#29992;&#24726;&#35770;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#36827;&#34892;&#27491;&#30830;&#30340;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#32467;&#35770;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#25152;&#31216;&#20043;&#20026;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#30340;&#27010;&#24565;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#19987;&#19994;&#20154;&#22763;&#25110;&#29305;&#23450;&#30340;&#19987;&#27880;&#30740;&#31350;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#25105;&#20204;&#21033;&#29992;&#24050;&#24191;&#27867;&#30740;&#31350;&#30340;&#24726;&#35770;&#65288;&#36763;&#26222;&#26862;&#24726;&#35770;&#21644;&#24247;&#22810;&#22622;&#24726;&#35770;&#65289;&#26469;&#35828;&#26126;&#22312;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#26356;&#26222;&#36941;&#22256;&#38590;&#12290;&#19982;&#22240;&#26524;&#25512;&#26029;&#22522;&#26412;&#21407;&#29702;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#27491;&#30830;&#8221;&#30340;&#26657;&#27491;&#26377;&#26102;&#38656;&#35201;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#12290;&#36825;&#20123;&#38519;&#38449;&#24341;&#23548;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#32593;&#32476;&#21644;&#20854;&#24418;&#25104;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#36825;&#20123;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore what we call ``omitted label contexts,'' in which training data is limited to a subset of the possible labels. This setting is common among specialized human experts or specific focused studies. We lean on well-studied paradoxes (Simpson's and Condorcet) to illustrate the more general difficulties of causal inference in omitted label contexts. Contrary to the fundamental principles on which much of causal inference is built, we show that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. These pitfalls lead us to the study networks of conclusions drawn from different contexts and the structures the form, proving an interesting connection between these networks and social choice theory.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;Meta-Continual Active Learning&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#35299;&#20915;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.03732</link><description>&lt;p&gt;
&#23398;&#20064;&#23398;&#20064;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#25345;&#20037;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn for Few-shot Continual Active Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;Meta-Continual Active Learning&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#35299;&#20915;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#30830;&#20445;&#35299;&#20915;&#20808;&#21069;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#23637;&#31034;&#23545;&#26032;&#39046;&#22495;&#30340;&#21487;&#22609;&#24615;&#12290;&#26368;&#36817;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#23616;&#38480;&#20110;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#23569;&#26679;&#26412;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#65288;CAL&#65289;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#65292;&#20294;&#26410;&#26631;&#35760;&#25968;&#25454;&#20805;&#36275;&#65292;&#20294;&#26377;&#38480;&#30340;&#27880;&#37322;&#39044;&#31639;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20803;&#25345;&#32493;&#20027;&#21160;&#23398;&#20064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#20803;&#23398;&#20064;&#21644;&#32463;&#39564;&#37325;&#25773;&#26469;&#35299;&#20915;&#20219;&#21153;&#20043;&#38388;&#30340;&#28151;&#28102;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32467;&#21512;&#25991;&#26412;&#22686;&#24378;&#26469;&#30830;&#20445;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;&#23569;&#26679;&#26412;CAL&#35774;&#32622;&#20013;&#19981;&#21516;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03732v2 Announce Type: replace-cross  Abstract: Continual learning strives to ensure stability in solving previously seen tasks while demonstrating plasticity in a novel domain. Recent advances in CL are mostly confined to a supervised learning setting, especially in NLP domain. In this work, we consider a few-shot continual active learning (CAL) setting where labeled data are inadequate, and unlabeled data are abundant but with a limited annotation budget. We propose a simple but efficient method, called Meta-Continual Active Learning. Specifically, we employ meta-learning and experience replay to address inter-task confusion and catastrophic forgetting. We further incorporate textual augmentations to ensure generalization. We conduct extensive experiments on benchmark text classification datasets to validate the effectiveness of the proposed method and analyze the effect of different active learning strategies in few-shot CAL setting. Our experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#24418;&#29366;&#65292;&#33021;&#22815;&#20248;&#21270;&#22320;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#65292;&#23454;&#29616;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#19979;&#36798;&#21040;&#30446;&#26631;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.03233</link><description>&lt;p&gt;
&#23548;&#33322;&#35268;&#27169;&#23450;&#24459;&#65306;&#33258;&#36866;&#24212;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#35745;&#31639;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Navigating Scaling Laws: Compute Optimality in Adaptive Model Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#35843;&#25972;&#24418;&#29366;&#65292;&#33021;&#22815;&#20248;&#21270;&#22320;&#20351;&#29992;&#35745;&#31639;&#36164;&#28304;&#65292;&#23454;&#29616;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#37327;&#19979;&#36798;&#21040;&#30446;&#26631;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#25216;&#26415;&#20027;&#35201;&#30001;&#32463;&#36807;&#22823;&#37327;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#38750;&#24120;&#24222;&#22823;&#27169;&#22411;&#20027;&#23548;&#12290;&#36825;&#19968;&#33539;&#24335;&#38750;&#24120;&#31616;&#21333;&#65306;&#25237;&#20837;&#26356;&#22810;&#30340;&#35745;&#31639;&#36164;&#28304;&#65288;&#26368;&#20248;&#22320;&#65289;&#20250;&#25552;&#39640;&#24615;&#33021;&#65292;&#32780;&#19988;&#29978;&#33267;&#33021;&#22815;&#21487;&#39044;&#27979;&#24615;&#22320;&#20570;&#21040;&#65307;&#24050;&#32463;&#25512;&#23548;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#20934;&#30830;&#39044;&#27979;&#20102;&#32593;&#32476;&#22312;&#25152;&#38656;&#35745;&#31639;&#27700;&#24179;&#19979;&#30340;&#24615;&#33021;&#12290;&#36825;&#24341;&#20986;&#20102;&#8220;&#35745;&#31639;&#20248;&#21270;&#8221;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20998;&#37197;&#32473;&#23450;&#35745;&#31639;&#27700;&#24179;&#20197;&#26368;&#22823;&#21270;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20801;&#35768;&#8220;&#33258;&#36866;&#24212;&#8221;&#27169;&#22411;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#25913;&#21464;&#24418;&#29366;&#30340;&#27169;&#22411;&#65292;&#26469;&#25193;&#23637;&#20248;&#21270;&#27010;&#24565;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#35774;&#35745;&#20986;&#33021;&#22815;&#26368;&#20248;&#22320;&#22312;&#22522;&#26412;&#23450;&#24459;&#20043;&#38388;&#31359;&#34892;&#24182;&#36229;&#36234;&#23427;&#20204;&#30340;&#8220;&#38745;&#24577;&#8221;&#23545;&#24212;&#29289;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#36798;&#21040;&#32473;&#23450;&#30446;&#26631;&#24615;&#33021;&#25152;&#38656;&#30340;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03233v2 Announce Type: replace  Abstract: In recent years, the state-of-the-art in deep learning has been dominated by very large models that have been pre-trained on vast amounts of data. The paradigm is very simple: investing more computational resources (optimally) leads to better performance, and even predictably so; neural scaling laws have been derived that accurately forecast the performance of a network for a desired level of compute. This leads to the notion of a `compute-optimal' model, i.e. a model that allocates a given level of compute during training optimally to maximize performance. In this work, we extend the concept of optimality by allowing for an `adaptive' model, i.e. a model that can change its shape during training. By doing so, we can design adaptive models that optimally traverse between the underlying scaling laws and outpace their `static' counterparts, leading to a significant reduction in the required compute to reach a given target performance. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepInception&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#26500;&#24314;&#26032;&#39062;&#30340;&#23884;&#22871;&#22330;&#26223;&#65292;&#25104;&#21151;&#20652;&#30496;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#30772;&#35299;&#32773;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;DeepInception&#22312;&#30772;&#35299;&#25104;&#21151;&#29575;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.03191</link><description>&lt;p&gt;
DeepInception: &#20652;&#30496;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#30772;&#35299;&#32773;
&lt;/p&gt;
&lt;p&gt;
DeepInception: Hypnotize Large Language Model to Be Jailbreaker
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepInception&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#26500;&#24314;&#26032;&#39062;&#30340;&#23884;&#22871;&#22330;&#26223;&#65292;&#25104;&#21151;&#20652;&#30496;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#30772;&#35299;&#32773;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;DeepInception&#22312;&#30772;&#35299;&#25104;&#21151;&#29575;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#30772;&#35299;&#25915;&#20987;&#65292;&#20351;&#24471;&#23433;&#20840;&#25514;&#26045;&#26080;&#25928;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30772;&#35299;&#30740;&#31350;&#36890;&#24120;&#37319;&#29992;&#26292;&#21147;&#20248;&#21270;&#25110;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#22806;&#25512;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#23454;&#38469;&#25110;&#26377;&#25928;&#12290;&#26412;&#25991;&#21463;&#21040;&#20197;&#31859;&#23572;&#26684;&#25289;&#22982;&#23454;&#39564;&#20026;&#28789;&#24863;&#65292;&#20851;&#20110;&#26435;&#23041;&#21147;&#37327;&#23545;&#20110;&#24341;&#21457;&#26377;&#23475;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;DeepInception&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#20652;&#30496;LLM&#25104;&#20026;&#30772;&#35299;&#32773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DeepInception&#21033;&#29992;LLM&#30340;&#35282;&#33394;&#25198;&#28436;&#33021;&#21147;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23884;&#22871;&#22330;&#26223;&#26469;&#34892;&#20026;&#65292;&#23454;&#29616;&#20102;&#22312;&#27491;&#24120;&#22330;&#26223;&#19979;&#36867;&#36991;&#20351;&#29992;&#25511;&#21046;&#30340;&#33258;&#36866;&#24212;&#26041;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;DeepInception&#22312;&#30772;&#35299;&#25104;&#21151;&#29575;&#26041;&#38754;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#31454;&#20105;&#21147;&#30456;&#24403;&#65292;&#24182;&#21487;&#20197;&#22312;&#21518;&#32493;&#20132;&#20114;&#20013;&#23454;&#29616;&#25345;&#32493;&#30340;&#30772;&#35299;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;LLM&#30340;&#33258;&#22833;&#20851;&#38190;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable success in various applications, large language models (LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails void. However, previous studies for jailbreaks usually resort to brute-force optimization or extrapolations of a high computation cost, which might not be practical or effective. In this paper, inspired by the Milgram experiment w.r.t. the authority power for inciting harmfulness, we disclose a lightweight method, termed DeepInception, which can easily hypnotize LLM to be a jailbreaker. Specifically, DeepInception leverages the personification ability of LLM to construct a novel nested scene to behave, which realizes an adaptive way to escape the usage control in a normal scenario. Empirically, our DeepInception can achieve competitive jailbreak success rates with previous counterparts and realize a continuous jailbreak in subsequent interactions, which reveals the critical weakness of self-losing on both open and closed-source LLMs l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#35780;&#20272;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#23545;&#31918;&#39135;&#21361;&#26426;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#31918;&#39135;&#23433;&#20840;&#31995;&#32479;&#20869;&#65292;&#20154;&#36947;&#20027;&#20041;&#24178;&#39044;&#23545;&#33829;&#20859;&#19981;&#33391;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2310.11287</link><description>&lt;p&gt;
&#35780;&#20272;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#23545;&#31918;&#39135;&#23433;&#20840;&#30340;&#22240;&#26524;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the Causal Impact of Humanitarian Aid on Food Security
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.11287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#35780;&#20272;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#23545;&#31918;&#39135;&#21361;&#26426;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#31918;&#39135;&#23433;&#20840;&#31995;&#32479;&#20869;&#65292;&#20154;&#36947;&#20027;&#20041;&#24178;&#39044;&#23545;&#33829;&#20859;&#19981;&#33391;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27668;&#20505;&#21464;&#21270;&#24341;&#21457;&#24178;&#26097;&#30340;&#24773;&#20917;&#19979;&#65292;&#26131;&#21463;&#23041;&#32961;&#30340;&#22320;&#21306;&#38754;&#20020;&#20005;&#37325;&#30340;&#31918;&#39135;&#23433;&#20840;&#23041;&#32961;&#65292;&#38656;&#35201;&#32039;&#24613;&#30340;&#20154;&#36947;&#20027;&#20041;&#25588;&#21161;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#38750;&#27954;&#20043;&#35282;&#30340;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#22522;&#20110;&#29616;&#37329;&#30340;&#24178;&#39044;&#23545;&#31918;&#39135;&#21361;&#26426;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#22312;&#31918;&#39135;&#23433;&#20840;&#31995;&#32479;&#20869;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#65292;&#21327;&#35843;&#21253;&#25324;&#31038;&#20250;&#32463;&#27982;&#12289;&#22825;&#27668;&#21644;&#36965;&#24863;&#25968;&#25454;&#22312;&#20869;&#30340;&#20840;&#38754;&#25968;&#25454;&#24211;&#65292;&#20197;&#21450;&#20272;&#35745;&#20154;&#36947;&#20027;&#20041;&#24178;&#39044;&#23545;&#33829;&#20859;&#19981;&#33391;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#23601;&#22269;&#23478;&#32423;&#21035;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#26679;&#26412;&#37327;&#26377;&#38480;&#12289;&#25968;&#25454;&#36136;&#37327;&#19981;&#20339;&#65292;&#20197;&#21450;&#30001;&#20110;&#25105;&#20204;&#23545;&#31918;&#39135;&#23433;&#20840;&#31561;&#36328;&#23398;&#31185;&#31995;&#32479;&#30340;&#26377;&#38480;&#29702;&#35299;&#32780;&#23548;&#33268;&#30340;&#19981;&#23436;&#32654;&#22240;&#26524;&#22270;&#12290;&#30456;&#21453;&#65292;&#22312;&#21306;&#19968;&#32423;&#21035;&#19978;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#26174;&#33879;&#24433;&#21709;&#65292;&#36827;&#19968;&#27493;&#26263;&#31034;&#20102;&#31995;&#32479;&#29305;&#23450;&#32972;&#26223;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.11287v2 Announce Type: replace  Abstract: In the face of climate change-induced droughts, vulnerable regions encounter severe threats to food security, demanding urgent humanitarian assistance. This paper introduces a causal inference framework for the Horn of Africa, aiming to assess the impact of cash-based interventions on food crises. Our contributions include identifying causal relationships within the food security system, harmonizing a comprehensive database including socio-economic, weather and remote sensing data, and estimating the causal effect of humanitarian interventions on malnutrition. On a country level, our results revealed no significant effects, likely due to limited sample size, suboptimal data quality, and an imperfect causal graph resulting from our limited understanding of multidisciplinary systems like food security. Instead, on a district level, results revealed significant effects, further implying the context-specific nature of the system. This un
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#32852;&#21512;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#26469;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#20844;&#20849;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#20849;&#20139;&#30828;&#26631;&#31614;&#20195;&#26367;&#27169;&#22411;&#21442;&#25968;&#65292;&#24418;&#25104;&#20266;&#26631;&#31614;&#20197;&#32467;&#21512;&#31169;&#26377;&#25968;&#25454;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#24182;&#33719;&#24471;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#23218;&#32654;&#30340;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2310.05696</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#21327;&#21516;&#35757;&#32451;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Protecting Sensitive Data through Federated Co-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05696
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#32852;&#21512;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#26469;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;&#65292;&#36890;&#36807;&#22312;&#20844;&#20849;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#20849;&#20139;&#30828;&#26631;&#31614;&#20195;&#26367;&#27169;&#22411;&#21442;&#25968;&#65292;&#24418;&#25104;&#20266;&#26631;&#31614;&#20197;&#32467;&#21512;&#31169;&#26377;&#25968;&#25454;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#65292;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#25928;&#26524;&#24182;&#33719;&#24471;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#23218;&#32654;&#30340;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#25935;&#24863;&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#20998;&#24067;&#30340;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#21487;&#33021;&#26080;&#27861;&#27719;&#24635;&#12290;&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#36845;&#20195;&#22320;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#30340;&#21442;&#25968;&#26469;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#21512;&#24182;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21487;&#20197;&#36890;&#36807;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#25512;&#26029;&#20986;&#25935;&#24863;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32852;&#21512;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#23458;&#25143;&#31471;&#20998;&#20139;&#20844;&#20849;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#30340;&#30828;&#26631;&#31614;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#21442;&#25968;&#12290;&#23545;&#20849;&#20139;&#26631;&#31614;&#30340;&#19968;&#33268;&#24615;&#24418;&#25104;&#20102;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#20266;&#26631;&#31614;&#65292;&#23458;&#25143;&#31471;&#23558;&#20854;&#19982;&#31169;&#26377;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#26469;&#35757;&#32451;&#26412;&#22320;&#27169;&#22411;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20849;&#20139;&#30828;&#26631;&#31614;&#22823;&#22823;&#25552;&#39640;&#20102;&#19982;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#30456;&#27604;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#21516;&#26102;&#65292;&#32852;&#21512;&#21327;&#21516;&#35757;&#32451;&#23454;&#29616;&#20102;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#23218;&#32654;&#30340;&#27169;&#22411;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#20687;(&#26799;&#24230;&#25552;&#21319;)&#20915;&#31574;&#26641;&#12289;&#35268;&#21017;&#38598;&#21512;&#31561;&#26412;&#22320;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05696v2 Announce Type: replace  Abstract: In many applications, sensitive data is inherently distributed and may not be pooled due to privacy concerns. Federated learning allows us to collaboratively train a model without pooling the data by iteratively aggregating the parameters of local models. It is possible, though, to infer upon the sensitive data from the shared model parameters. We propose to use a federated co-training approach where clients share hard labels on a public unlabeled dataset instead of model parameters. A consensus on the shared labels forms a pseudo labeling for the unlabeled dataset that clients use in combination with their private data to train local models. We show that sharing hard labels substantially improves privacy over sharing model parameters. At the same time, federated co-training achieves a model quality comparable to federated learning. Moreover, it allows us to use local models such as (gradient boosted) decision trees, rule ensembles, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.01945</link><description>&lt;p&gt;
OHQ: &#33455;&#29255;&#19978;&#30340;&#30828;&#20214;&#24863;&#30693;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OHQ: On-chip Hardware-aware Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#37327;&#21270;&#24863;&#30693;&#27969;&#27700;&#32447;&#21644;&#24341;&#20837;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#36827;&#34892;&#39640;&#25928;&#37327;&#21270;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#21644;&#23454;&#38469;&#37096;&#32626;&#24046;&#36317;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#37096;&#32626;&#20808;&#36827;&#28145;&#24230;&#27169;&#22411;&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#21033;&#29992;&#22810;&#20301;&#23485;&#26550;&#26500;&#26469;&#37322;&#25918;&#37327;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#23384;&#22312;&#25628;&#32034;&#31354;&#38388;&#36807;&#22823;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#37327;&#21270;&#36807;&#31243;&#20381;&#36182;&#20110;&#29420;&#31435;&#30340;&#39640;&#24615;&#33021;&#35774;&#22791;&#65292;&#32780;&#19981;&#26159;&#26412;&#22320;&#36827;&#34892;&#65292;&#36825;&#20063;&#23548;&#33268;&#20102;&#32771;&#34385;&#30340;&#30828;&#20214;&#25351;&#26631;&#19982;&#23454;&#38469;&#37096;&#32626;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33455;&#29255;&#19978;&#36827;&#34892;&#30828;&#20214;&#24863;&#30693;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#30340;&#26694;&#26550;&#65288;OHQ&#65289;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#22312;&#32447;&#35774;&#22791;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#33455;&#29255;&#19978;&#30340;&#37327;&#21270;&#24863;&#30693;&#65288;OQA&#65289;&#27969;&#27700;&#32447;&#65292;&#33021;&#22815;&#24863;&#30693;&#37327;&#21270;&#31639;&#23376;&#22312;&#30828;&#20214;&#19978;&#30340;&#23454;&#38469;&#25928;&#29575;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25513;&#30721;&#24341;&#23548;&#30340;&#37327;&#21270;&#20272;&#35745;&#65288;MQE&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization emerges as one of the most promising approaches for deploying advanced deep models on resource-constrained hardware. Mixed-precision quantization leverages multiple bit-width architectures to unleash the accuracy and efficiency potential of quantized models. However, existing mixed-precision quantization suffers exhaustive search space that causes immense computational overhead. The quantization process thus relies on separate high-performance devices rather than locally, which also leads to a significant gap between the considered hardware metrics and the real deployment.In this paper, we propose an On-chip Hardware-aware Quantization (OHQ) framework that performs hardware-aware mixed-precision quantization without accessing online devices. First, we construct the On-chip Quantization Awareness (OQA) pipeline, enabling perceive the actual efficiency metrics of the quantization operator on the hardware.Second, we propose Mask-guided Quantization Estimation (MQE) technique 
&lt;/p&gt;</description></item><item><title>DistriBlock&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#30340;&#26377;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#12289;&#29109;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.17000</link><description>&lt;p&gt;
DistriBlock: &#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
DistriBlock: Identifying adversarial audio samples by leveraging characteristics of the output distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.17000
&lt;/p&gt;
&lt;p&gt;
DistriBlock&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#30340;&#26377;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#36755;&#20986;&#20998;&#24067;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#12289;&#29109;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#33021;&#35823;&#23548;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#20351;&#20854;&#39044;&#27979;&#20219;&#24847;&#30446;&#26631;&#25991;&#26412;&#65292;&#20174;&#32780;&#26500;&#25104;&#26126;&#26174;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistriBlock&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;ASR&#31995;&#32479;&#30340;&#39640;&#25928;&#26816;&#27979;&#31574;&#30053;&#65292;&#35813;&#31995;&#32479;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#39044;&#27979;&#36755;&#20986;&#26631;&#35760;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#23545;&#35813;&#20998;&#24067;&#30340;&#19968;&#32452;&#29305;&#24449;&#36827;&#34892;&#27979;&#37327;&#65306;&#36755;&#20986;&#27010;&#29575;&#30340;&#20013;&#20301;&#25968;&#12289;&#26368;&#22823;&#20540;&#21644;&#26368;&#23567;&#20540;&#65292;&#20998;&#24067;&#30340;&#29109;&#65292;&#20197;&#21450;&#19982;&#21518;&#32493;&#26102;&#38388;&#27493;&#39588;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;Kullback-Leibler&#21644;Jensen-Shannon&#25955;&#24230;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#21033;&#29992;&#23545;&#33391;&#24615;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#24212;&#29992;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#20998;&#31867;&#12289;&#36825;&#31181;&#20998;&#31867;&#22120;&#30340;&#38598;&#21512;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#26368;&#20808;&#36827;&#30340;ASR&#31995;&#32479;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DistriBlock&#22312;&#35782;&#21035;&#23545;&#25239;&#24615;&#38899;&#39057;&#26679;&#26412;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.17000v2 Announce Type: replace-cross  Abstract: Adversarial attacks can mislead automatic speech recognition (ASR) systems into predicting an arbitrary target text, thus posing a clear security threat. To prevent such attacks, we propose DistriBlock, an efficient detection strategy applicable to any ASR system that predicts a probability distribution over output tokens in each time step. We measure a set of characteristics of this distribution: the median, maximum, and minimum over the output probabilities, the entropy of the distribution, as well as the Kullback-Leibler and the Jensen-Shannon divergence with respect to the distributions of the subsequent time step. Then, by leveraging the characteristics observed for both benign and adversarial data, we apply binary classifiers, including simple threshold-based classification, ensembles of such classifiers, and neural networks. Through extensive analysis across different state-of-the-art ASR systems and language data sets, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#25429;&#25417;&#20102;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2204.04510</link><description>&lt;p&gt;
&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#35753;&#31616;&#21333;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#19978;&#26356;&#24378;&#22823;&#21644;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
Translating Subgraphs to Nodes Makes Simple GNNs Strong and Efficient for Subgraph Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.04510
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23376;&#22270;&#36716;&#21270;&#20026;&#33410;&#28857;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#25429;&#25417;&#20102;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#24120;&#20351;&#29992;&#19987;&#38376;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#22823;&#22411;&#20840;&#23616;&#22270;&#12290;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#20294;&#25361;&#25112;&#23376;&#22270;&#30340;&#23618;&#27425;&#32467;&#26500;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23376;&#22270;&#21040;&#33410;&#28857;&#65288;S2N&#65289;&#36716;&#25442;&#30340;&#26032;&#39062;&#20844;&#24335;&#65292;&#29992;&#20110;&#23398;&#20064;&#23376;&#22270;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20840;&#23616;&#22270;&#20013;&#30340;&#19968;&#32452;&#23376;&#22270;&#65292;&#25105;&#20204;&#36890;&#36807;&#31895;&#30053;&#22320;&#23558;&#23376;&#22270;&#36716;&#25442;&#25104;&#33410;&#28857;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#22270;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;S2N&#19981;&#20165;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26174;&#33879;&#20943;&#23569;&#20102;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#19988;&#36890;&#36807;&#25429;&#25417;&#23376;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#20063;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#23427;&#20204;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#31895;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#20063;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35843;&#25972;&#27169;&#22411;&#21518;&#25928;&#26524;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraph representation learning has emerged as an important problem, but it is by default approached with specialized graph neural networks on a large global graph. These models demand extensive memory and computational resources but challenge modeling hierarchical structures of subgraphs. In this paper, we propose Subgraph-To-Node (S2N) translation, a novel formulation for learning representations of subgraphs. Specifically, given a set of subgraphs in the global graph, we construct a new graph by coarsely transforming subgraphs into nodes. Demonstrating both theoretical and empirical evidence, S2N not only significantly reduces memory and computational costs compared to state-of-the-art models but also outperforms them by capturing both local and global structures of the subgraph. By leveraging graph coarsening methods, our method outperforms baselines even in a data-scarce setting with insufficient subgraphs. Our experiments on eight benchmarks demonstrate that fined-tuned models w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;"mspace"&#65292;&#36866;&#29992;&#20110;&#39044;&#27979;&#26102;&#24577;&#22270;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;mspace&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16800</link><description>&lt;p&gt;
&#22312;&#26102;&#24577;&#22270;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#39044;&#27979;&#30340;&#22312;&#32447;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Algorithm for Node Feature Forecasting in Temporal Graphs. (arXiv:2401.16800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;"mspace"&#65292;&#36866;&#29992;&#20110;&#39044;&#27979;&#26102;&#24577;&#22270;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#19982;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;mspace&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20381;&#28982;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"mspace"&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#26102;&#24577;&#22270;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#28789;&#27963;&#22320;&#25429;&#25417;&#19981;&#21516;&#33410;&#28857;&#20043;&#38388;&#30340;&#31354;&#38388;&#20132;&#21449;&#30456;&#20851;&#24615;&#20197;&#21450;&#33410;&#28857;&#20869;&#30340;&#26102;&#38388;&#33258;&#30456;&#20851;&#24615;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#27010;&#29575;&#21644;&#30830;&#23450;&#24615;&#30340;&#22810;&#27493;&#39044;&#27979;&#65292;&#36866;&#29992;&#20110;&#20272;&#35745;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#19982;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#32463;&#20856;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#21508;&#31181;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;mspace&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#27700;&#24179;&#30456;&#24403;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#23427;&#20204;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;mspace&#22312;&#20855;&#26377;&#19981;&#21516;&#35757;&#32451;&#26679;&#26412;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#19982;GNN&#26041;&#27861;&#30456;&#27604;&#30340;&#19968;&#20010;&#26174;&#33879;&#20248;&#21183;&#65292;&#21518;&#32773;&#38656;&#35201;&#20016;&#23500;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;mspace&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#29702;&#35770;&#27169;&#22411;&#26469;&#35777;&#26126;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#20445;&#35777;&#65292;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;mspace&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an online algorithm "mspace" for forecasting node features in temporal graphs, which adeptly captures spatial cross-correlation among different nodes as well as the temporal autocorrelation within a node. The algorithm can be used for both probabilistic and deterministic multi-step forecasting, making it applicable for estimation and generation tasks. Comparative evaluations against various baselines, including graph neural network (GNN) based models and classical Kalman filters, demonstrate that mspace performs at par with the state-of-the-art and even surpasses them on some datasets. Importantly, mspace demonstrates consistent robustness across datasets with varying training sizes, a notable advantage over GNN-based methods requiring abundant training samples to learn the spatiotemporal trends in the data effectively. Therefore, employing mspace is advantageous in scenarios where the training sample availability is limited. Additionally, we establish theoret
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16635</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble. (arXiv:2401.16635v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;RLHF&#20381;&#36182;&#20110;&#36890;&#36807;&#26377;&#38480;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;RLHF&#21487;&#33021;&#20135;&#29983;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22870;&#21169;&#27169;&#22411;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#32771;&#34385;&#21040;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#21644;&#36164;&#28304;&#26114;&#36149;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21253;&#25324;&#32447;&#24615;&#23618;&#38598;&#25104;&#21644;&#22522;&#20110;LoRA&#30340;&#38598;&#25104;&#22312;&#20869;&#30340;&#39640;&#25928;&#38598;&#25104;&#26041;&#27861;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#38598;&#25104;&#22870;&#21169;&#27169;&#22411;&#36816;&#34892;Best-of-$n$&#21644;Proximal Policy Optimization&#65292;&#24182;&#39564;&#35777;&#25105;&#20204;&#30340;&#38598;&#25104;&#26041;&#27861;&#26377;&#21161;&#20110;&#25913;&#21892;RLHF&#36755;&#20986;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is a widely adopted approach for aligning large language models with human values. However, RLHF relies on a reward model that is trained with a limited amount of human preference data, which could lead to inaccurate predictions. As a result, RLHF may produce outputs that are misaligned with human values. To mitigate this issue, we contribute a reward ensemble method that allows the reward model to make more accurate predictions. As using an ensemble of large language model-based reward models can be computationally and resource-expensive, we explore efficient ensemble methods including linear-layer ensemble and LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy Optimization with our ensembled reward models, and verify that our ensemble methods help improve the alignment performance of RLHF outputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;L-AutoDA&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36845;&#20195;&#20132;&#20114;&#65292;L-AutoDA&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#31454;&#20105;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#26174;&#31034;&#20986;&#22312;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.15335</link><description>&lt;p&gt;
L-AutoDA: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks. (arXiv:2401.15335v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;L-AutoDA&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35774;&#35745;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36845;&#20195;&#20132;&#20114;&#65292;L-AutoDA&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#31454;&#20105;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#26174;&#31034;&#20986;&#22312;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#23545;&#25239;&#25915;&#20987;&#23545;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#21644;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#26174;&#33879;&#25361;&#25112;&#12290;&#20915;&#31574;&#22411;&#25915;&#20987;&#21482;&#38656;&#35201;&#27169;&#22411;&#30340;&#20915;&#31574;&#21453;&#39304;&#65292;&#32780;&#19981;&#38656;&#35201;&#35814;&#32454;&#30340;&#27010;&#29575;&#25110;&#20998;&#25968;&#65292;&#22240;&#27492;&#29305;&#21035;&#38590;&#20197;&#38450;&#24481;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;L-AutoDA&#65288;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20915;&#31574;&#22411;&#23545;&#25239;&#25915;&#20987;&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#33258;&#21160;&#35774;&#35745;&#36825;&#20123;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#36827;&#21270;&#26694;&#26550;&#20013;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36845;&#20195;&#20132;&#20114;&#65292;L-AutoDA&#33021;&#22815;&#39640;&#25928;&#22320;&#33258;&#21160;&#35774;&#35745;&#20986;&#31454;&#20105;&#24615;&#30340;&#25915;&#20987;&#31639;&#27861;&#65292;&#20943;&#23569;&#20154;&#24037;&#24037;&#20316;&#37327;&#12290;&#25105;&#20204;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;L-AutoDA&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;&#22312;&#25104;&#21151;&#29575;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30456;&#27604;&#22522;&#20934;&#26041;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#25239;&#25915;&#20987;&#29983;&#25104;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of machine learning, adversarial attacks present a significant challenge to model robustness and security. Decision-based attacks, which only require feedback on the decision of a model rather than detailed probabilities or scores, are particularly insidious and difficult to defend against. This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks. By iteratively interacting with LLMs in an evolutionary framework, L-AutoDA automatically designs competitive attack algorithms efficiently without much human effort. We demonstrate the efficacy of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline methods in both success rate and computational efficiency. Our findings underscore the potential of language models as tools for adversarial attack generation and highli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#36793;&#30028;&#27861;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#25214;&#21040;&#21487;&#35777;&#26126;&#26368;&#20248;&#31232;&#30095;&#29983;&#23384;&#26641;&#27169;&#22411;&#65292;&#23545;&#20110;&#28041;&#21450;&#20154;&#31867;&#20581;&#24247;&#30340;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#20998;&#26512;&#21644;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.15330</link><description>&lt;p&gt;
&#26368;&#20248;&#31232;&#30095;&#29983;&#23384;&#26641;
&lt;/p&gt;
&lt;p&gt;
Optimal Sparse Survival Trees. (arXiv:2401.15330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#36793;&#30028;&#27861;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#25214;&#21040;&#21487;&#35777;&#26126;&#26368;&#20248;&#31232;&#30095;&#29983;&#23384;&#26641;&#27169;&#22411;&#65292;&#23545;&#20110;&#28041;&#21450;&#20154;&#31867;&#20581;&#24247;&#30340;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#20998;&#26512;&#21644;&#20915;&#31574;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28041;&#21450;&#20154;&#31867;&#20581;&#24247;&#30340;&#39640;&#39118;&#38505;&#38382;&#39064;&#30340;&#20998;&#26512;&#21644;&#20915;&#31574;&#20013;&#65292;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#21307;&#29983;&#12289;&#21307;&#38498;&#12289;&#21046;&#33647;&#20844;&#21496;&#21644;&#29983;&#29289;&#25216;&#26415;&#20844;&#21496;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#20854;&#21560;&#24341;&#20154;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25429;&#25417;&#22797;&#26434;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#26641;&#30340;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29983;&#25104;&#29983;&#23384;&#26641;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#65288;&#25110;&#36138;&#23146;&#65289;&#31639;&#27861;&#65292;&#23384;&#22312;&#29983;&#25104;&#27425;&#20248;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35268;&#21010;&#36793;&#30028;&#27861;&#65292;&#21487;&#20197;&#22312;&#20960;&#31186;&#38047;&#20869;&#25214;&#21040;&#21487;&#35777;&#26126;&#26368;&#20248;&#31232;&#30095;&#29983;&#23384;&#26641;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is crucial for doctors, hospitals, pharmaceutical companies and biotechnology corporations to analyze and make decisions for high stakes problems that involve human health. Tree-based methods have been widely adopted for \textit{survival analysis} due to their appealing interpretablility and their ability to capture complex relationships. However, most existing methods to produce survival trees rely on heuristic (or greedy) algorithms, which risk producing sub-optimal models. We present a dynamic-programming-with-bounds approach that finds provably-optimal sparse survival tree models, frequently in only a few seconds.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.14521</link><description>&lt;p&gt;
&#20197;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#29289;&#29702;-&#27010;&#24565;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological Modeling using the Mass-Conserving-Perceptron. (arXiv:2401.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#26500;&#24314;&#22522;&#20110;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#27700;&#25991;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22312;&#20445;&#25345;&#31616;&#27905;&#24615;&#30340;&#21516;&#26102;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21508;&#31181;&#27969;&#37327;&#21160;&#21147;&#23398;&#34892;&#20026;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24320;&#21457;&#31616;&#27905;&#21487;&#35299;&#37322;&#30340;&#38598;&#27700;&#21306;&#23610;&#24230;&#27700;&#25991;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#37319;&#29992;&#22522;&#20110;&#36136;&#37327;&#23432;&#24658;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#20316;&#20026;&#22522;&#26412;&#35745;&#31639;&#21333;&#20803;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#21333;&#20010;&#20301;&#32622;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#65288;&#28145;&#24230;&#65289;&#65292;&#32780;&#19981;&#26159;&#23545;&#22823;&#26679;&#26412;&#38598;&#27700;&#21306;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#24191;&#24230;&#12290;&#30446;&#26631;&#26159;&#21457;&#29616;&#19968;&#20010;&#26368;&#23567;&#30340;&#34920;&#31034;&#65288;&#21333;&#20803;&#29366;&#24577;&#25968;&#21644;&#27969;&#37327;&#36335;&#24452;&#25968;&#65289;&#65292;&#29992;&#20110;&#34920;&#31034;&#33021;&#22815;&#35299;&#37322;&#32473;&#23450;&#38598;&#27700;&#21306;&#36755;&#20837;&#29366;&#24577;&#21644;&#36755;&#20986;&#34892;&#20026;&#30340;&#20027;&#35201;&#36807;&#31243;&#65292;&#29305;&#21035;&#24378;&#35843;&#27169;&#25311;&#20840;&#33539;&#22260;&#65288;&#39640;&#12289;&#20013;&#12289;&#20302;&#65289;&#30340;&#27969;&#37327;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#21306;&#22495;&#65292;&#37319;&#29992;&#31867;&#20284;HyMod&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;3&#20010;&#21333;&#20803;&#29366;&#24577;&#21644;2&#20010;&#20027;&#35201;&#27969;&#21160;&#36335;&#24452;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#26679;&#30340;&#34920;&#31034;&#65292;&#20294;&#24341;&#20837;&#36755;&#20837;&#26049;&#36335;&#26426;&#21046;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#27700;&#25991;&#22270;&#30340;&#26102;&#38388;&#21644;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the applicability of machine learning technologies to the development of parsimonious, interpretable, catchment-scale hydrologic models using directed-graph architectures based on the mass-conserving perceptron (MCP) as the fundamental computational unit. Here, we focus on architectural complexity (depth) at a single location, rather than universal applicability (breadth) across large samples of catchments. The goal is to discover a minimal representation (numbers of cell-states and flow paths) that represents the dominant processes that can explain the input-state-output behaviors of a given catchment, with particular emphasis given to simulating the full range (high, medium, and low) of flow dynamics. We find that a HyMod-like architecture with three cell-states and two major flow pathways achieves such a representation at our study location, but that the additional incorporation of an input-bypass mechanism significantly improves the timing and shape of the hydrograph
&lt;/p&gt;</description></item><item><title>LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13920</link><description>&lt;p&gt;
LocMoE: &#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20302;&#24320;&#38144;MoE
&lt;/p&gt;
&lt;p&gt;
LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13920
&lt;/p&gt;
&lt;p&gt;
LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;MoE&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#20998;&#24067;&#24335;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#26377;&#25928;&#31232;&#30095;&#21644;&#25193;&#23637;&#27169;&#22411;&#65292;&#22240;&#27492;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#24615;&#33021;&#21463;&#21040;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20840;&#23545;&#20840;&#36890;&#20449;&#30340;&#39640;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#30001;&#20110;&#22823;&#37327;&#30340;&#19987;&#23478;&#23481;&#37327;&#23548;&#33268;&#30456;&#23545;&#20887;&#20313;&#30340;&#35745;&#31639;&#12290;&#36127;&#36733;&#19981;&#24179;&#34913;&#21487;&#33021;&#26159;&#30001;&#20110;&#29616;&#26377;&#36335;&#30001;&#31574;&#30053;&#22987;&#32456;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#30340;&#19987;&#23478;&#23548;&#33268;&#30340;&#12290;&#20840;&#23545;&#20840;&#36807;&#31243;&#20013;&#39057;&#32321;&#30340;&#33410;&#28857;&#38388;&#36890;&#20449;&#20063;&#26174;&#33879;&#24310;&#38271;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#24615;&#33021;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#19987;&#23478;&#23481;&#37327;&#30340;&#26368;&#23567;&#38408;&#20540;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30340;&#38376;&#25511;&#26435;&#37325;&#19982;&#20998;&#37197;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#26368;&#22823;&#35282;&#20559;&#24046;&#35745;&#31639;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35780;&#20272;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#27969;&#34892;&#24230;&#25351;&#26631;&#25552;&#20986;&#36136;&#30097;&#65292;&#35748;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#32676;&#32452;&#20869;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#23548;&#33268;&#30340;&#39044;&#27979;&#26631;&#31614;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#29616;&#23454;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.13391</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65306;&#20572;&#27490;&#20165;&#26681;&#25454;&#32676;&#32452;&#38388;&#25351;&#26631;&#35780;&#20272;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely on between-group metrics. (arXiv:2401.13391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35780;&#20272;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#27969;&#34892;&#24230;&#25351;&#26631;&#25552;&#20986;&#36136;&#30097;&#65292;&#35748;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#32676;&#32452;&#20869;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#23548;&#33268;&#30340;&#39044;&#27979;&#26631;&#31614;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#29616;&#23454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;&#20844;&#24179;&#24615;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#20844;&#24179;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#26680;&#24515;&#20851;&#27880;&#28857;&#65292;&#20294;&#30446;&#21069;&#30340;&#35752;&#35770;&#24448;&#24448;&#24378;&#35843;&#22522;&#20110;&#32467;&#26524;&#30340;&#25351;&#26631;&#65292;&#32780;&#27809;&#26377;&#23545;&#23376;&#32676;&#20307;&#20013;&#30340;&#24046;&#24322;&#24433;&#21709;&#36827;&#34892;&#32454;&#33268;&#32771;&#34385;&#12290;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#19981;&#20165;&#24433;&#21709;&#25935;&#24863;&#32676;&#32452;&#20043;&#38388;&#30340;&#23454;&#20363;&#25490;&#24207;&#65292;&#32780;&#19988;&#36890;&#24120;&#36824;&#26174;&#33879;&#24433;&#21709;&#36825;&#20123;&#32676;&#32452;&#20869;&#23454;&#20363;&#30340;&#25490;&#24207;&#12290;&#36825;&#20123;&#21464;&#21270;&#38590;&#20197;&#35299;&#37322;&#65292;&#24182;&#24341;&#36215;&#20102;&#23545;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#30340;&#25285;&#24551;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#25928;&#24212;&#22312;&#36890;&#24120;&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;-&#20844;&#24179;&#24615;&#35780;&#20272;&#26694;&#26550;&#20013;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#23545;&#35780;&#20272;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#27969;&#34892;&#24230;&#25351;&#26631;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#35748;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#32676;&#32452;&#20869;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#23548;&#33268;&#30340;&#39044;&#27979;&#26631;&#31614;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#29616;&#23454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) finds widespread applications across various domains, sparking concerns about fairness in its deployment. While fairness in AI remains a central concern, the prevailing discourse often emphasizes outcome-based metrics without a nuanced consideration of the differential impacts within subgroups. Bias mitigation techniques do not only affect the ranking of pairs of instances across sensitive groups, but often also significantly affect the ranking of instances within these groups. Such changes are hard to explain and raise concerns regarding the validity of the intervention. Unfortunately, these effects largely remain under the radar in the accuracy-fairness evaluation framework that is usually applied. This paper challenges the prevailing metrics for assessing bias mitigation techniques, arguing that they do not take into account the changes within-groups and that the resulting prediction labels fall short of reflecting real-world scenarios. We propose a para
&lt;/p&gt;</description></item><item><title>&#25910;&#32553;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;CDPMs&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#25910;&#32553;&#21518;&#21521;&#37319;&#26679;&#24182;&#20811;&#26381;&#20998;&#25968;&#21305;&#37197;&#35823;&#24046;&#21644;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#25910;&#32553;&#23376;&#26041;&#24046;&#20445;&#25345;&#65288;sub-VP&#65289;&#26159;&#34920;&#29616;&#26368;&#20339;&#30340;&#19968;&#31181;CDPMs&#12290;</title><link>http://arxiv.org/abs/2401.13115</link><description>&lt;p&gt;
&#25910;&#32553;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contractive Diffusion Probabilistic Models. (arXiv:2401.13115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13115
&lt;/p&gt;
&lt;p&gt;
&#25910;&#32553;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;CDPMs&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#36890;&#36807;&#25910;&#32553;&#21518;&#21521;&#37319;&#26679;&#24182;&#20811;&#26381;&#20998;&#25968;&#21305;&#37197;&#35823;&#24046;&#21644;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#25910;&#32553;&#23376;&#26041;&#24046;&#20445;&#25345;&#65288;sub-VP&#65289;&#26159;&#34920;&#29616;&#26368;&#20339;&#30340;&#19968;&#31181;CDPMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#32553;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#24314;&#27169;&#20013;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;DPMs&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#20004;&#20010;&#35201;&#32032;&#65306;&#39532;&#23572;&#31185;&#22827;&#25193;&#25955;&#36807;&#31243;&#30340;&#26102;&#38388;&#21453;&#28436;&#21644;&#20998;&#25968;&#21305;&#37197;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#38544;&#21547;&#22320;&#20551;&#35774;&#20998;&#25968;&#21305;&#37197;&#26159;&#25509;&#36817;&#23436;&#32654;&#30340;&#65292;&#32780;&#36825;&#20010;&#20551;&#35774;&#26159;&#20540;&#24471;&#24576;&#30097;&#30340;&#12290;&#37492;&#20110;&#21487;&#33021;&#26080;&#27861;&#20445;&#35777;&#30340;&#20998;&#25968;&#21305;&#37197;&#65292;&#25105;&#20204;&#22312;DPMs&#30340;&#35774;&#35745;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20934;&#21017;&#8212;&#8212;&#25910;&#32553;&#21518;&#21521;&#37319;&#26679;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#32553;DPMs&#65288;CDPMs&#65289;&#31867;&#65292;&#21253;&#25324;&#25910;&#32553;&#22885;&#24681;&#26031;&#22374;-&#20044;&#20262;&#36125;&#20811;&#65288;OU&#65289;&#36807;&#31243;&#21644;&#25910;&#32553;&#23376;&#26041;&#24046;&#20445;&#25345;&#65288;sub-VP&#65289;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#12290;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#21518;&#21521;&#36807;&#31243;&#30340;&#25910;&#32553;&#33021;&#22815;&#32553;&#23567;&#20998;&#25968;&#21305;&#37197;&#35823;&#24046;&#21644;&#31163;&#25955;&#21270;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;CDPMs&#23545;&#20110;&#36825;&#20004;&#31181;&#35823;&#24046;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#24471;&#21040;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#25903;&#25345;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25910;&#32553;&#23376;&#26041;&#24046;&#20445;&#25345;&#22312;&#34920;&#29616;&#19978;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models (DPMs) have emerged as a promising technology in generative modeling. The success of DPMs relies on two ingredients: time reversal of Markov diffusion processes and score matching. Most existing work implicitly assumes that score matching is close to perfect, while this assumption is questionable. In view of possibly unguaranteed score matching, we propose a new criterion -- the contraction of backward sampling in the design of DPMs. This leads to a novel class of contractive DPMs (CDPMs), including contractive Ornstein-Uhlenbeck (OU) processes and contractive sub-variance preserving (sub-VP) stochastic differential equations (SDEs). The key insight is that the contraction in the backward process narrows score matching errors, as well as discretization error. Thus, the proposed CDPMs are robust to both sources of error. Our proposal is supported by theoretical results, and is corroborated by experiments. Notably, contractive sub-VP shows the best performa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#39057;&#29575;&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.12425</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#24573;&#35270;&#30340;&#23614;&#37096;
&lt;/p&gt;
&lt;p&gt;
The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#34913;&#37327;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#24565;&#30340;&#39057;&#29575;&#65292;&#24182;&#21457;&#29616;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#38646;&#26679;&#26412;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35270;&#35273;&#27010;&#24565;&#19978;&#30340;&#34920;&#29616;&#26497;&#19981;&#22343;&#34913;&#12290;&#20363;&#22914;&#65292;&#23613;&#31649;CLIP&#22312;ImageNet&#19978;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24179;&#22343;&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#65288;72.7&#65285;&#65289;&#65292;&#20294;&#22312;&#21313;&#20010;&#27010;&#24565;&#65288;&#22914;gyromitra&#21644;night snake&#65289;&#19978;&#30340;&#20934;&#30830;&#29575;&#19981;&#21040;10&#65285;&#65292;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#36825;&#20123;&#27010;&#24565;&#22312;VLM&#30340;&#38750;&#22343;&#34913;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#34920;&#31034;&#19981;&#36275;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#31181;&#19981;&#24179;&#34913;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;VLM&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#35745;&#31639;&#29305;&#23450;&#27010;&#24565;&#30340;&#39057;&#29575;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#20998;&#26512;&#39044;&#35757;&#32451;&#25991;&#26412;&#26469;&#27979;&#37327;&#27010;&#24565;&#39057;&#29575;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#24110;&#21161;&#35745;&#31639;&#21253;&#21547;&#32473;&#23450;&#27010;&#24565;&#30340;&#21516;&#20041;&#35789;&#30340;&#30456;&#20851;&#25991;&#26412;&#65292;&#24182;&#35299;&#20915;&#35821;&#35328;&#27495;&#20041;&#12290;&#25105;&#20204;&#30830;&#35748;&#20687;LAION&#36825;&#26679;&#30340;&#27969;&#34892;&#30340;VLM&#25968;&#25454;&#38598;&#30830;&#23454;&#23637;&#31034;&#20102;&#38271;&#23614;&#27010;&#24565;&#20998;&#24067;&#65292;&#24182;&#19988;&#36825;&#19982;&#25353;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#24378;&#28872;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#24403;&#20195;&#30340;&#22810;&#27169;&#24335;&#31995;&#32479;&#65292;&#22914;&#35270;&#35273;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25991;&#26412;-&#35270;&#35273;&#25512;&#29702;&#27169;&#22411;&#65292;&#22312;&#36825;&#31181;&#38271;&#23614;&#20998;&#24067;&#19979;&#32463;&#24120;&#38590;&#20197;&#36798;&#21040;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) excel in zero-shot recognition but exhibit drastically imbalanced performance across visual concepts. For example, CLIP, despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields $&lt;$10% on ten concepts (e.g., gyromitra and night snake), presumably, because these concepts are under-represented in VLMs' imbalanced pretraining data. Yet, assessing this imbalance is challenging as it is non-trivial to calculate the frequency of specific concepts within VLMs' large-scale pretraining data. Our work makes the first attempt to measure the concept frequency by analyzing pretraining texts. We use off-the-shelf language models to help count relevant texts that contain synonyms of the given concepts and resolve linguistic ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit long-tailed concept distributions, which strongly correlate with per-class accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and text-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#65292;&#29305;&#21035;&#20851;&#27880;&#21322;&#20107;&#23454;&#30340;&#35299;&#37322;&#65292;&#24182;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#39318;&#36873;&#39033;&#20010;&#24615;&#21270;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10938</link><description>&lt;p&gt;
&#21363;&#20351;&#35299;&#37322;&#65306;&#24418;&#24335;&#22522;&#30784;&#65292;&#20248;&#20808;&#32423;&#21644;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Even-if Explanations: Formal Foundations, Priorities and Complexity. (arXiv:2401.10938v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#65292;&#29305;&#21035;&#20851;&#27880;&#21322;&#20107;&#23454;&#30340;&#35299;&#37322;&#65292;&#24182;&#23545;&#32447;&#24615;&#27169;&#22411;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#37322;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#39318;&#36873;&#39033;&#20010;&#24615;&#21270;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20316;&#20026;&#40657;&#30418;&#23376;&#36816;&#34892;&#65292;&#32570;&#20047;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#65292;&#32780;&#21448;&#25903;&#25345;&#20915;&#31574;&#36807;&#31243;&#12290;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#35797;&#22270;&#22238;&#31572;&#20026;&#20160;&#20040;&#32473;&#23450;&#27169;&#22411;&#22914;&#20309;&#23545;&#20010;&#20307;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#20851;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#24050;&#32463;&#36827;&#34892;&#20102;&#37325;&#35201;&#24037;&#20316;&#65292;&#20294;&#23545;&#21322;&#20107;&#23454;&#35299;&#37322;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#21322;&#20107;&#23454;&#30340;&#23616;&#37096;&#20107;&#21518;&#35299;&#37322;&#24615;&#26597;&#35810;&#20197;&#21450;&#19981;&#21516;&#27169;&#22411;&#31867;&#21035;&#20013;&#20854;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#34920;&#26126;&#32447;&#24615;&#21644;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#27604;&#31070;&#32463;&#32593;&#32476;&#26356;&#26131;&#20110;&#35299;&#37322;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#22909;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#33258;&#24049;&#30340;&#20559;&#22909;&#20010;&#24615;&#21270;&#35299;&#37322;&#65292;&#26080;&#35770;&#26159;&#22312;&#21322;&#20107;&#23454;&#36824;&#26159;&#21453;&#20107;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#35299;&#37322;&#33021;&#21147;&#21644;&#29992;&#25143;&#20013;&#24515;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
EXplainable AI has received significant attention in recent years. Machine learning models often operate as black boxes, lacking explainability and transparency while supporting decision-making processes. Local post-hoc explainability queries attempt to answer why individual inputs are classified in a certain way by a given model. While there has been important work on counterfactual explanations, less attention has been devoted to semifactual ones. In this paper, we focus on local post-hoc explainability queries within the semifactual `even-if' thinking and their computational complexity among different classes of models, and show that both linear and tree-based models are strictly more interpretable than neural networks. After this, we introduce a preference-based framework that enables users to personalize explanations based on their preferences, both in the case of semifactuals and counterfactuals, enhancing interpretability and user-centricity. Finally, we explore the complexity o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.10746</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#33041;&#30005;&#35299;&#30721;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#32463;&#24120;&#29992;&#20110;&#21508;&#31181;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#22823;&#37327;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#65292;&#36801;&#31227;&#23398;&#20064;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;DL&#27169;&#22411;&#12290;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#26159;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#65288;EA&#65289;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#20351;&#29992;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#20302;&#24182;&#19988;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20854;&#23545;&#20849;&#20139;&#21644;&#20010;&#20307;DL&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;EA&#19982;DL&#30456;&#32467;&#21512;&#22312;&#35299;&#30721;BCI&#20449;&#21495;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;EA&#26469;&#35757;&#32451;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#20849;&#20139;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#26032;&#21463;&#35797;&#32773;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#23558;&#30446;&#26631;&#21463;&#35797;&#32773;&#30340;&#35299;&#30721;&#29575;&#25552;&#39640;&#20102;4.33&#65285;&#65292;&#24182;&#19988;&#25910;&#25947;&#26102;&#38388;&#32553;&#30701;&#20102;&#36229;&#36807;70&#65285;&#12290;&#25105;&#20204;&#36824;&#20026;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10463</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#36825;&#26159;&#20174;&#24555;&#36895;&#35760;&#24518;&#21040;&#32531;&#24930;&#27867;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#36716;&#21464;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30456;&#21464;&#24418;&#24335;&#21270;&#20026;Grokking&#37197;&#32622;&#19979;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#65292;&#24182;&#30830;&#23450;&#20102;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#20805;&#36275;&#21644;&#36807;&#21097;&#38454;&#27573;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#21021;&#22987;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;Grokking&#37197;&#32622;&#65292;&#31283;&#23450;&#22320;&#22312;&#31616;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#37325;&#29616;&#20102;Grokking&#12290;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26679;&#26412;&#32423;&#21644;&#27169;&#22411;&#32423;&#30340;Grokking&#65292;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#25968;&#25454;&#38598;&#22823;&#23567;&#22788;&#21457;&#29983;&#30340;&#26356;&#24179;&#28369;&#30340;&#30456;&#21464;&#12290;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#36825;&#20010;&#20020;&#30028;&#28857;&#20063;&#21464;&#24471;&#26356;&#22823;&#65292;&#36825;&#34920;&#26126;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21152;&#28145;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#29702;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
&lt;/p&gt;</description></item><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09071</link><description>&lt;p&gt;
&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#37325;&#26032;&#24605;&#32771;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35770;&#19978;&#22312;&#35889;&#22495;&#20013;&#26377;&#24456;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#19982;&#31354;&#38388;&#22495;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#30001;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#20174;&#31354;&#38388;&#35282;&#24230;&#30740;&#31350;&#35889;&#22270;GNN&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#31354;&#38388;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65292;&#20363;&#22914;&#65292;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#22495;&#20013;&#23454;&#38469;&#19978;&#32534;&#30721;&#20102;&#21738;&#20123;&#20449;&#24687;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#30340;&#20869;&#22312;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#26126;&#30830;&#22320;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#36866;&#24212;&#24615;&#26032;&#22270;&#12290;&#29702;&#35770;&#21644;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26032;&#22270;&#19981;&#20165;&#34920;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#36824;&#33021;&#22815;&#23481;&#32435;&#26377;&#31526;&#21495;&#30340;&#36793;&#26435;&#37325;&#20197;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#36817;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#27169;&#22411;&#25512;&#26029;&#20986;&#20195;&#29702;&#21464;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.06687</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#36817;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Proximal Causal Inference With Text Data. (arXiv:2401.06687v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#36817;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#27169;&#22411;&#25512;&#26029;&#20986;&#20195;&#29702;&#21464;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#22240;&#26524;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#20542;&#21521;&#20110;&#21253;&#21547;&#37096;&#20998;&#25110;&#19981;&#23436;&#20840;&#27979;&#37327;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#20195;&#29702;&#26469;&#20943;&#36731;&#28151;&#28102;&#20559;&#24046;&#12290;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#20998;&#26512;&#20154;&#21592;&#22312;&#19968;&#37096;&#20998;&#23454;&#20363;&#30340;&#25991;&#26412;&#20013;&#20855;&#26377;&#26377;&#30417;&#30563;&#30340;&#28151;&#28102;&#21464;&#37327;&#26631;&#31614;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#25104;&#26412;&#65292;&#36825;&#31181;&#32422;&#26463;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#23558;&#22788;&#29702;&#21069;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#38646;&#26679;&#26412;&#27169;&#22411;&#20174;&#20998;&#21106;&#30340;&#20004;&#20010;&#37096;&#20998;&#25512;&#26029;&#20986;&#20004;&#20010;&#20195;&#29702;&#65292;&#24182;&#23558;&#36825;&#20123;&#20195;&#29702;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#30340;&#20195;&#29702;&#26041;&#27861;&#28385;&#36275;&#36817;&#37051; g-formula&#25152;&#38656;&#30340;&#35782;&#21035;&#26465;&#20214;&#65292;&#32780;&#20854;&#20182;&#30475;&#20284;&#21512;&#29702;&#30340;&#25552;&#35758;&#21017;&#19981;&#28385;&#36275;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-based causal methods attempt to mitigate confounding bias by including unstructured text data as proxies of confounding variables that are partially or imperfectly measured. These approaches assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is not always feasible due to data privacy or cost. Here, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that splits pre-treatment text data, infers two proxies from two zero-shot models on the separate splits, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. This combination of proximal causal inference and zero-sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22270;&#31232;&#30095;&#21270;&#30340;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#28151;&#21512;&#30697;&#38453;&#65292;&#20197;&#25552;&#39640;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#33021;&#25928;&#12290;&#22312;&#29305;&#27530;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#26377;&#20445;&#35777;&#24615;&#33021;&#21644;&#36138;&#24515;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03083</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31232;&#30095;&#21270;&#30340;&#33021;&#25928;&#20248;&#21270;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Energy-efficient Decentralized Learning via Graph Sparsification. (arXiv:2401.03083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22270;&#31232;&#30095;&#21270;&#30340;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#28151;&#21512;&#30697;&#38453;&#65292;&#20197;&#25552;&#39640;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#33021;&#25928;&#12290;&#22312;&#29305;&#27530;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#26377;&#20445;&#35777;&#24615;&#33021;&#21644;&#36138;&#24515;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#28151;&#21512;&#30697;&#38453;&#26469;&#25552;&#39640;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#33021;&#25928;&#65292;&#28151;&#21512;&#30697;&#38453;&#25511;&#21046;&#20102;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20005;&#26684;&#20998;&#26512;&#65292;&#23558;&#38382;&#39064;&#34920;&#31034;&#20026;&#21452;&#23618;&#20248;&#21270;&#65292;&#20854;&#20013;&#24213;&#23618;&#36890;&#36807;&#22270;&#31232;&#30095;&#21270;&#35299;&#20915;&#12290;&#22312;&#23436;&#20840;&#36830;&#25509;&#30340;&#22522;&#30784;&#25299;&#25169;&#32467;&#26500;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#38024;&#23545;&#19968;&#33324;&#24773;&#20917;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#22522;&#20110;&#30495;&#23454;&#25299;&#25169;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#30340;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#23558;&#26368;&#32321;&#24537;&#33410;&#28857;&#30340;&#33021;&#32791;&#38477;&#20302;54%-76%&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims at improving the energy efficiency of decentralized learning by optimizing the mixing matrix, which controls the communication demands during the learning process. Through rigorous analysis based on a state-of-the-art decentralized learning algorithm, the problem is formulated as a bi-level optimization, with the lower level solved by graph sparsification. A solution with guaranteed performance is proposed for the special case of fully-connected base topology and a greedy heuristic is proposed for the general case. Simulations based on real topology and dataset show that the proposed solution can lower the energy consumption at the busiest node by 54%-76% while maintaining the quality of the trained model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#36830;&#25509;&#20114;&#32852;&#32593;&#19978;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#20010;&#20154;&#35745;&#31639;&#26426;&#65292;&#21033;&#29992;&#35745;&#31639;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#21033;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;Ravnest&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#35745;&#31639;&#33410;&#28857;&#32452;&#32455;&#25104;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38598;&#32676;&#65292;&#23454;&#29616;&#20102;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#27599;&#20010;&#33410;&#28857;&#25215;&#36733;&#25972;&#20010;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.01728</link><description>&lt;p&gt;
Ravnest: &#24322;&#26500;&#35774;&#22791;&#19978;&#30340;&#20998;&#25955;&#24335;&#24322;&#27493;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices. (arXiv:2401.01728v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#36890;&#36807;&#36830;&#25509;&#20114;&#32852;&#32593;&#19978;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#20010;&#20154;&#35745;&#31639;&#26426;&#65292;&#21033;&#29992;&#35745;&#31639;&#33021;&#21147;&#26469;&#23454;&#29616;&#26377;&#21033;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;Ravnest&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#35745;&#31639;&#33410;&#28857;&#32452;&#32455;&#25104;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38598;&#32676;&#65292;&#23454;&#29616;&#20102;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#27599;&#20010;&#33410;&#28857;&#25215;&#36733;&#25972;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#12289;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#24322;&#24120;&#30340;&#27867;&#21270;&#21644;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#36235;&#21183;&#39044;&#35745;&#23558;&#32487;&#32493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#22686;&#22823;&#20351;&#24471;&#20256;&#32479;&#30340;&#38598;&#20013;&#24335;&#26041;&#27861;&#22312;&#36825;&#31181;&#35268;&#27169;&#19978;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24322;&#27493;&#20998;&#25955;&#24335;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#20102;&#36830;&#25509;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#36164;&#28304;&#21463;&#38480;&#30340;&#24322;&#26500;&#20010;&#20154;&#35745;&#31639;&#26426;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#26377;&#21033;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;Ravnest&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#35745;&#31639;&#33410;&#28857;&#32452;&#32455;&#25104;&#20855;&#26377;&#31867;&#20284;&#25968;&#25454;&#20256;&#36755;&#36895;&#29575;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#38598;&#32676;&#26469;&#23454;&#29616;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#27599;&#20010;&#33410;&#28857;&#25215;&#36733;&#25972;&#20010;&#27169;&#22411;&#12290;&#36825;&#20123;&#38598;&#32676;&#21442;&#19982;$\textit{&#38646;&#27668;&#27873;&#24322;&#27493;&#27169;&#22411;&#24182;&#34892;}$&#35757;&#32451;&#65292;&#21033;&#29992;$\textit{&#24182;&#34892;&#22810;&#29615;&#20840;&#23616;&#27719;&#32858;}$&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#36890;&#20449;&#21644;&#27169;&#22411;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets. This trend is expected to continue. However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales. This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics. Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model. These clusters engage in $\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;Vapnik-Chervonenkis (VC)&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#19979;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#21644;&#29702;&#35770;&#36824;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#26368;&#32456;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;&#12290;</title><link>http://arxiv.org/abs/2312.05134</link><description>&lt;p&gt;
&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Multi-Distribution Learning. (arXiv:2312.05134v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;Vapnik-Chervonenkis (VC)&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#19979;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#21644;&#29702;&#35770;&#36824;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#26368;&#32456;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20998;&#24067;&#23398;&#20064;&#65288;MDL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;k&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#39118;&#38505;&#65292;&#24050;&#25104;&#20026;&#36866;&#24212;&#20581;&#22766;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#22810;&#32452;&#21512;&#20316;&#31561;&#38656;&#27714;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;MDL&#38656;&#35201;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20063;&#31216;&#20026;&#25353;&#38656;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#19979;&#30028;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#38024;&#23545;Vapnik-Chervonenkis&#65288;VC&#65289;&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#25509;&#36817;&#20110;&#65288;d+k&#65289;/&#949;^2&#65288;&#22312;&#26576;&#20123;&#23545;&#25968;&#22240;&#23376;&#20013;&#65289;&#65292;&#19982;&#24050;&#30693;&#30340;&#26368;&#20339;&#19979;&#30028;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#24605;&#24819;&#21644;&#29702;&#35770;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#65292;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc. Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we propose a novel algorithm that yields an $varepsilon$-optimal randomized hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$ (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory have been further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#19981;&#21464;&#21644;&#31561;&#21464;&#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#25552;&#20379;&#24555;&#36895;&#32780;&#39640;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#36890;&#36807;&#20351;&#29992;&#19981;&#21464;&#36755;&#20837;&#21644;&#31561;&#21464;&#23618;&#65292;&#21487;&#20197;&#22686;&#24378;&#28145;&#24230;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.18672</link><description>&lt;p&gt;
&#19981;&#21464;&#21644;&#31561;&#21464;&#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison Between Invariant and Equivariant Classical and Quantum Graph Neural Networks. (arXiv:2311.18672v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#19981;&#21464;&#21644;&#31561;&#21464;&#30340;&#32463;&#20856;&#21644;&#37327;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#25552;&#20379;&#24555;&#36895;&#32780;&#39640;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#25351;&#20986;&#36890;&#36807;&#20351;&#29992;&#19981;&#21464;&#36755;&#20837;&#21644;&#31561;&#21464;&#23618;&#65292;&#21487;&#20197;&#22686;&#24378;&#28145;&#24230;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#29702;&#35299;CERN&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;(LHC)&#19978;&#20135;&#29983;&#30340;&#22823;&#37327;&#39640;&#33021;&#31890;&#23376;&#30896;&#25758;&#25968;&#25454;&#26102;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36825;&#20123;&#30896;&#25758;&#20107;&#20214;&#30340;&#25968;&#25454;&#21487;&#20197;&#33258;&#28982;&#22320;&#29992;&#22270;&#32467;&#26500;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#28145;&#24230;&#20960;&#20309;&#26041;&#27861;&#65292;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#65292;&#24050;&#32463;&#22312;&#39640;&#33021;&#29289;&#29702;&#25968;&#25454;&#20998;&#26512;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20219;&#21153;&#26159;&#21943;&#27880;&#26631;&#35760;&#65292;&#20854;&#20013;&#21943;&#27880;&#34987;&#35270;&#20026;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#21644;&#20854;&#32452;&#25104;&#31890;&#23376;&#20043;&#38388;&#30340;&#36793;&#36830;&#25509;&#30340;&#28857;&#20113;&#12290;LHC&#31890;&#23376;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#20197;&#21450;&#29992;&#20110;&#20854;&#20998;&#26512;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#22823;&#22823;&#20419;&#36827;&#20102;&#24320;&#21457;&#26367;&#20195;&#24555;&#36895;&#19988;&#39640;&#25928;&#30340;&#35745;&#31639;&#33539;&#24335;&#65292;&#22914;&#37327;&#23376;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22686;&#24378;&#28145;&#24230;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#19981;&#21464;&#36755;&#20837;&#21644;&#31561;&#21464;&#23618;&#26469;&#21033;&#29992;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22522;&#26412;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are heavily relied on to understand the vast amounts of data from high-energy particle collisions at the CERN Large Hadron Collider (LHC). The data from such collision events can naturally be represented with graph structures. Therefore, deep geometric methods, such as graph neural networks (GNNs), have been leveraged for various data analysis tasks in high-energy physics. One typical task is jet tagging, where jets are viewed as point clouds with distinct features and edge connections between their constituent particles. The increasing size and complexity of the LHC particle datasets, as well as the computational models used for their analysis, greatly motivate the development of alternative fast and efficient computational paradigms such as quantum computation. In addition, to enhance the validity and robustness of deep networks, one can leverage the fundamental symmetries present in the data through the use of invariant inputs and equivariant layers. In t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#20449;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#30693;&#35782;&#33976;&#39311;&#65288;CC-BAKD&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#26426;&#21046;&#23558;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#19982;&#21387;&#32553;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22312;&#23398;&#20064;&#32773;&#19982;&#25945;&#24072;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#26102;&#20851;&#20110;&#25209;&#27425;&#36873;&#25321;&#21644;&#25209;&#27425;&#32534;&#30721;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.08053</link><description>&lt;p&gt;
&#36890;&#20449;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Communication-Constrained Bayesian Active Knowledge Distillation. (arXiv:2311.08053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#20449;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#30693;&#35782;&#33976;&#39311;&#65288;CC-BAKD&#65289;&#30340;&#26032;&#21327;&#35758;&#65292;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#28151;&#21512;&#26426;&#21046;&#23558;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#19982;&#21387;&#32553;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22312;&#23398;&#20064;&#32773;&#19982;&#25945;&#24072;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#26102;&#20851;&#20110;&#25209;&#27425;&#36873;&#25321;&#21644;&#25209;&#27425;&#32534;&#30721;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#37325;&#20256;&#65288;ARQ&#65289;&#21327;&#35758;&#26088;&#22312;&#30830;&#20445;&#25509;&#25910;&#26041;&#27491;&#30830;&#25509;&#25910;&#21040;&#21457;&#23556;&#26041;&#30340;&#25152;&#26377;&#20998;&#32452;&#12290;&#24403;&#21457;&#23556;&#26041;&#26159;&#19968;&#20010;&#23398;&#20064;&#32773;&#19982;&#19968;&#20010;&#25945;&#24072;&#36827;&#34892;&#36890;&#20449;&#26102;&#65292;&#36825;&#20010;&#30446;&#26631;&#19982;&#23398;&#20064;&#32773;&#30340;&#23454;&#38469;&#30446;&#26631;&#30456;&#20914;&#31361;&#65292;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#20174;&#25945;&#24072;&#37027;&#37324;&#33719;&#21462;&#26368;&#30456;&#20851;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#20174;&#20027;&#21160;&#23398;&#20064;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26412;&#25991;&#35299;&#20915;&#20197;&#19979;&#20851;&#38190;&#21327;&#35758;&#35774;&#35745;&#38382;&#39064;&#65306;(i)&#20027;&#21160;&#25209;&#27425;&#36873;&#25321;&#65306;&#24212;&#35813;&#21457;&#36865;&#21738;&#20010;&#25209;&#27425;&#30340;&#36755;&#20837;&#32473;&#25945;&#24072;&#20197;&#33719;&#21462;&#26368;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#36890;&#20449;&#36718;&#27425;&#30340;&#25968;&#37327;&#65311;(ii)&#25209;&#27425;&#32534;&#30721;&#65306;&#26159;&#21542;&#21487;&#20197;&#32452;&#21512;&#25968;&#25454;&#28857;&#30340;&#25209;&#27425;&#20197;&#20943;&#23569;&#27599;&#20010;&#36890;&#20449;&#36718;&#27425;&#25152;&#38656;&#30340;&#36890;&#20449;&#36164;&#28304;&#65311;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#36890;&#20449;&#21463;&#38480;&#30340;&#36125;&#21494;&#26031;&#20027;&#21160;&#30693;&#35782;&#33976;&#39311;&#65288;CC-BAKD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#32447;&#24615;&#28151;&#21512;&#26426;&#21046;&#23558;&#36125;&#21494;&#26031;&#20027;&#21160;&#23398;&#20064;&#19982;&#21387;&#32553;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional retransmission (ARQ) protocols are designed with the goal of ensuring the correct reception of all the individual transmitter's packets at the receiver. When the transmitter is a learner communicating with a teacher, this goal is at odds with the actual aim of the learner, which is that of eliciting the most relevant label information from the teacher. Taking an active learning perspective, this paper addresses the following key protocol design questions: (i) Active batch selection: Which batch of inputs should be sent to the teacher to acquire the most useful information and thus reduce the number of required communication rounds? (ii) Batch encoding: Can batches of data points be combined to reduce the communication resources required at each communication round? Specifically, this work introduces Communication-Constrained Bayesian Active Knowledge Distillation (CC-BAKD), a novel protocol that integrates Bayesian active learning with compression via a linear mix-up mecha
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#32676;&#20869;&#29305;&#24449;&#32858;&#38598;&#21644;&#32676;&#22806;&#29305;&#24449;&#31163;&#25955;&#30340;&#24615;&#36136;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21644;&#26435;&#37325;&#21521;&#37327;&#25509;&#36817;&#31243;&#24230;&#30340;&#31070;&#32463;&#22349;&#22604;&#65288;NC-OOD&#65289;&#26816;&#27979;&#22120;&#26469;&#25552;&#39640;OAD&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.01479</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#22349;&#22604;&#30340;&#35270;&#35282;&#26816;&#27979;&#21040;&#32676;&#22806;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Detecting Out-of-Distribution Through the Lens of Neural Collapse. (arXiv:2311.01479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01479
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#32676;&#20869;&#29305;&#24449;&#32858;&#38598;&#21644;&#32676;&#22806;&#29305;&#24449;&#31163;&#25955;&#30340;&#24615;&#36136;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21644;&#26435;&#37325;&#21521;&#37327;&#25509;&#36817;&#31243;&#24230;&#30340;&#31070;&#32463;&#22349;&#22604;&#65288;NC-OOD&#65289;&#26816;&#27979;&#22120;&#26469;&#25552;&#39640;OAD&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#22806;&#65288;OOD&#65289;&#26816;&#27979;&#23545;&#20110;&#23433;&#20840;&#37096;&#32626;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;OOD&#26816;&#27979;&#22120;&#24212;&#35813;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#26377;&#25928;&#22320;&#27867;&#21270;&#12290;&#20026;&#20102;&#25913;&#36827;&#29616;&#26377;OOD&#26816;&#27979;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#24230;&#28789;&#27963;&#30340;OOD&#26816;&#27979;&#22120;&#65292;&#31216;&#20026;&#31070;&#32463;&#22349;&#22604;&#65288;NC-OOD&#65289;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26222;&#36941;&#35266;&#23519;&#21040;&#30340;&#32676;&#20869;&#65288;ID&#65289;&#29305;&#24449;&#20542;&#21521;&#20110;&#24418;&#25104;&#31751;&#65292;&#32780;&#32676;&#22806;&#29305;&#24449;&#21017;&#36828;&#31163;&#30340;&#35266;&#23519;&#12290;&#29305;&#21035;&#26159;&#22522;&#20110;&#26368;&#36817;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#31070;&#32463;&#22349;&#22604;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;ID&#29305;&#24449;&#20542;&#21521;&#20110;&#22312;&#25509;&#36817;&#26435;&#37325;&#21521;&#37327;&#30340;&#20301;&#32622;&#32858;&#38598;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#25193;&#23637;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#19982;&#26435;&#37325;&#21521;&#37327;&#30340;&#25509;&#36817;&#31243;&#24230;&#26469;&#26816;&#27979;OOD&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25490;&#38500;OOD&#26679;&#26412;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;OOD&#29305;&#24449;&#20542;&#21521;&#20110;&#27604;ID&#29305;&#24449;&#26356;&#25509;&#36817;&#21407;&#28857;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;OOD&#26816;&#27979;&#26041;&#38754;&#22987;&#32456;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is essential for the safe deployment of AI. Particularly, OOD detectors should generalize effectively across diverse scenarios. To improve upon the generalizability of existing OOD detectors, we introduce a highly versatile OOD detector, called Neural Collapse inspired OOD detector (NC-OOD). We extend the prevalent observation that in-distribution (ID) features tend to form clusters, whereas OOD features are far away. Particularly, based on the recent observation, Neural Collapse, we further demonstrate that ID features tend to cluster in proximity to weight vectors. From our extended observation, we propose to detect OOD based on feature proximity to weight vectors. To further rule out OOD samples, we leverage the observation that OOD features tend to reside closer to the origin than ID features. Extensive experiments show that our approach enhances the generalizability of existing work and can consistently achieve state-of-the-art OOD detection per
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#32479;&#19968;&#35270;&#35282;&#19979;&#24555;&#36895;&#35745;&#31639;Shapley&#20540;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#25674;&#38144;&#20272;&#35745;&#22120;SimSHAP&#65292;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#25216;&#26415;&#26174;&#33879;&#21152;&#36895;&#20102;&#20934;&#30830;Shapley&#20540;&#30340;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2311.01010</link><description>&lt;p&gt;
&#25506;&#32034;&#24555;&#36895;Shapley&#20540;&#20272;&#35745;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Exploring Unified Perspective For Fast Shapley Value Estimation. (arXiv:2311.01010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01010
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#32479;&#19968;&#35270;&#35282;&#19979;&#24555;&#36895;&#35745;&#31639;Shapley&#20540;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#25674;&#38144;&#20272;&#35745;&#22120;SimSHAP&#65292;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#25216;&#26415;&#26174;&#33879;&#21152;&#36895;&#20102;&#20934;&#30830;Shapley&#20540;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#34987;&#24191;&#27867;&#25509;&#21463;&#21644;&#21487;&#38752;&#30340;&#24037;&#20855;&#65292;&#23427;&#20197;&#29702;&#35770;&#20844;&#29702;&#20026;&#22522;&#30784;&#65292;&#29992;&#20110;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#40657;&#30418;&#27169;&#22411;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22312;&#29305;&#24449;&#25968;&#30446;&#19978;&#65292;&#35745;&#31639;Shapley&#20540;&#20250;&#36935;&#21040;&#25351;&#25968;&#32423;&#22797;&#26434;&#24615;&#12290;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;ApproSemivalue&#12289;KernelSHAP&#21644;FastSHAP&#65292;&#20197;&#21152;&#36895;&#35745;&#31639;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21363;&#38543;&#26426;&#20272;&#35745;&#22120;&#21487;&#20197;&#32479;&#19968;&#20026;&#29305;&#24449;&#23376;&#38598;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#32447;&#24615;&#21464;&#25442;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35774;&#35745;&#31616;&#21333;&#25674;&#38144;&#20272;&#35745;&#22120;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#21644;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;SimSHAP&#65292;&#36890;&#36807;&#28040;&#38500;&#20887;&#20313;&#25216;&#26415;&#12290;&#22312;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;SimSHAP&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#26174;&#30528;&#21152;&#36895;&#20102;&#20934;&#30830;Shapley&#20540;&#30340;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapley values have emerged as a widely accepted and trustworthy tool, grounded in theoretical axioms, for addressing challenges posed by black-box models like deep neural networks. However, computing Shapley values encounters exponential complexity in the number of features. Various approaches, including ApproSemivalue, KernelSHAP, and FastSHAP, have been explored to expedite the computation. We analyze the consistency of existing works and conclude that stochastic estimators can be unified as the linear transformation of importance sampling of feature subsets. Based on this, we investigate the possibility of designing simple amortized estimators and propose a straightforward and efficient one, SimSHAP, by eliminating redundant techniques. Extensive experiments conducted on tabular and image datasets validate the effectiveness of our SimSHAP, which significantly accelerates the computation of accurate Shapley values.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#23454;&#29616;&#20102;&#21452;&#21521;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#38544;&#34255;&#32422;&#26463;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18449</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#20855;&#26377;&#38544;&#34255;&#32422;&#26463;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Hidden Constraints via Latent Decision Models. (arXiv:2310.18449v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#23454;&#29616;&#20102;&#21452;&#21521;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#38544;&#34255;&#32422;&#26463;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#23588;&#20854;&#22312;&#20844;&#20849;&#25919;&#31574;&#39046;&#22495;&#22914;&#35686;&#23519;&#21010;&#21306;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23450;&#20041;&#21487;&#34892;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#20915;&#31574;&#30340;&#39640;&#32500;&#24230;&#65292;&#20854;&#22312;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#38544;&#34255;&#32422;&#26463;&#28508;&#22312;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;HC-LSBO&#65289;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#21407;&#22987;&#20915;&#31574;&#31354;&#38388;&#19982;&#36739;&#20302;&#32500;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#21452;&#21521;&#26144;&#23556;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;HC-LSBO&#25429;&#25417;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#22266;&#26377;&#30340;&#38544;&#34255;&#32422;&#26463;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#30340;&#21516;&#26102;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#35780;&#20272;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#35268;&#27169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) has emerged as a potent tool for addressing intricate decision-making challenges, especially in public policy domains such as police districting. However, its broader application in public policymaking is hindered by the complexity of defining feasible regions and the high-dimensionality of decisions. This paper introduces the Hidden-Constrained Latent Space Bayesian Optimization (HC-LSBO), a novel BO method integrated with a latent decision model. This approach leverages a variational autoencoder to learn the distribution of feasible decisions, enabling a two-way mapping between the original decision space and a lower-dimensional latent space. By doing so, HC-LSBO captures the nuances of hidden constraints inherent in public policymaking, allowing for optimization in the latent space while evaluating objectives in the original space. We validate our method through numerical experiments on both synthetic and real data sets, with a specific focus on large-scal
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;</title><link>http://arxiv.org/abs/2310.17688</link><description>&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#26102;&#20195;&#31649;&#29702;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17688
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#24555;&#36895;&#36827;&#23637;&#30340;&#26102;&#20195;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31649;&#29702;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#20849;&#35782;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#21363;&#23558;&#21040;&#26469;&#30340;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#22823;&#35268;&#27169;&#30340;&#31038;&#20250;&#21361;&#23475;&#21644;&#24694;&#24847;&#20351;&#29992;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22833;&#21435;&#25511;&#21046;&#30340;&#19981;&#21487;&#36870;&#36716;&#30340;&#25439;&#22833;&#12290;&#37492;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21644;&#25345;&#32493;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#30740;&#21457;&#21644;&#27835;&#29702;&#30340;&#20248;&#20808;&#20107;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&amp;D and governance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#24072;&#29983;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36739;&#23569;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#12290;&#36890;&#36807;&#24212;&#29992;&#35813;&#26041;&#27861;&#20110;SAM&#21644;CLIP&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;SAM-CLIP&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;</title><link>http://arxiv.org/abs/2310.15308</link><description>&lt;p&gt;
SAM-CLIP: &#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21512;&#24182;&#20026;&#35821;&#20041;&#21644;&#31354;&#38388;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding. (arXiv:2310.15308v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#24072;&#29983;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36739;&#23569;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#12290;&#36890;&#36807;&#24212;&#29992;&#35813;&#26041;&#27861;&#20110;SAM&#21644;CLIP&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;SAM-CLIP&#65292;&#23558;&#20004;&#32773;&#30340;&#20248;&#21183;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24320;&#21487;&#29992;&#30340;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65288;VFMs&#65289;&#30340;&#39046;&#22495;&#65292;&#22914;CLIP&#21644;Segment Anything Model&#65288;SAM&#65289;&#65292;&#27491;&#22312;&#36805;&#36895;&#25193;&#22823;&#12290;VFMs&#20855;&#26377;&#28304;&#33258;&#23427;&#20204;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#19981;&#21516;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;CLIP&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;SAM&#19987;&#27880;&#20110;&#20998;&#21106;&#30340;&#31354;&#38388;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;VFMs&#39640;&#25928;&#21512;&#24182;&#20026;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#21560;&#25910;&#23427;&#20204;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#38598;&#25104;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#25216;&#26415;&#21644;&#24072;&#29983;&#33976;&#39311;&#12290;&#19982;&#20256;&#32479;&#30340;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#30456;&#27604;&#65292;&#36825;&#31181;&#31574;&#30053;&#20855;&#26377;&#26174;&#33879;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#23427;&#21482;&#38656;&#35201;&#26368;&#21021;&#29992;&#20110;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;SAM&#21644;CLIP&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;SAM-CLIP&#65306;&#23558;SAM&#21644;CLIP&#30340;&#20248;&#21183;&#34701;&#21512;&#20026;&#21333;&#19968;&#20027;&#24178;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that assimilates their expertise. Our proposed method integrates multi-task learning, continual learning techniques, and teacher-student distillation. This strategy entails significantly less computational cost compared to traditional multi-task training from scratch. Additionally, it only demands a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we derive SAM-CLIP: a unified model that amalgamates the strengths of SAM and CLIP into a single backbone, making it apt for ed
&lt;/p&gt;</description></item><item><title>&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#21644;&#20005;&#26684;&#26410;&#26469;&#25513;&#30721;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#65292;&#36825;&#19968;&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#35821;&#35328;&#31867;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#30340;&#30740;&#31350;&#65292;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2310.13897</link><description>&lt;p&gt;
&#25513;&#30721;&#30828;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#21644;&#24067;&#23572;RASP&#20934;&#30830;&#35782;&#21035;&#26080;&#26143;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Hard-Attention Transformers and Boolean RASP Recognize Exactly the Star-Free Languages. (arXiv:2310.13897v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13897
&lt;/p&gt;
&lt;p&gt;
&#32473;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#21644;&#20005;&#26684;&#26410;&#26469;&#25513;&#30721;&#65292;&#24182;&#19988;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#36890;&#36807;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#65292;&#36825;&#19968;&#27169;&#22411;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#35821;&#35328;&#31867;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#30340;&#30740;&#31350;&#65292;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#30828;&#27880;&#24847;&#21147;&#65288;&#21363;&#25152;&#26377;&#27880;&#24847;&#21147;&#37117;&#38598;&#20013;&#22312;&#19968;&#20010;&#20301;&#32622;&#19978;&#65289;&#21644;&#20005;&#26684;&#30340;&#26410;&#26469;&#25513;&#30721;&#65288;&#21363;&#27599;&#20010;&#20301;&#32622;&#21482;&#19982;&#20005;&#26684;&#24038;&#20391;&#30340;&#20301;&#32622;&#36827;&#34892;&#27880;&#24847;&#21147;&#20132;&#20114;&#65289;&#30340;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#32593;&#32476;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#27491;&#26159;&#26080;&#26143;&#35821;&#35328;&#12290;&#28155;&#21152;&#20301;&#32622;&#23884;&#20837;&#23558;&#34987;&#35782;&#21035;&#30340;&#35821;&#35328;&#31867;&#21035;&#25193;&#23637;&#21040;&#20854;&#20182;&#30740;&#31350;&#20805;&#20998;&#30340;&#31867;&#21035;&#12290;&#36825;&#20123;&#35777;&#26126;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#26159;&#24067;&#23572;RASP&#65292;&#23427;&#26159;&#19968;&#31181;&#21463;&#38480;&#20110;&#24067;&#23572;&#20540;&#30340;RASP&#21464;&#31181;&#12290;&#36890;&#36807;&#26080;&#26143;&#35821;&#35328;&#65292;&#25105;&#20204;&#23558;&#21464;&#25442;&#22120;&#19982;&#19968;&#38454;&#36923;&#36753;&#12289;&#26102;&#24577;&#36923;&#36753;&#21644;&#20195;&#25968;&#33258;&#21160;&#26426;&#29702;&#35770;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider transformer encoders with hard attention (in which all attention is focused on exactly one position) and strict future masking (in which each position only attends to positions strictly to its left), and prove that the class of languages recognized by these networks is exactly the star-free languages. Adding position embeddings increases the class of recognized languages to other well-studied classes. A key technique in these proofs is Boolean RASP, a variant of RASP that is restricted to Boolean values. Via the star-free languages, we relate transformers to first-order logic, temporal logic, and algebraic automata theory.
&lt;/p&gt;</description></item><item><title>RealFM &#26159;&#19968;&#20010;&#30495;&#23454;&#30340;&#32852;&#37030;&#26426;&#21046;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#29615;&#22659;&#20013;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#39547;&#27850;&#32773;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#25311;&#35774;&#22791;&#25928;&#29992;&#12289;&#28608;&#21169;&#25968;&#25454;&#36129;&#29486;&#21644;&#35774;&#22791;&#21442;&#19982;&#65292;&#24182;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#25928;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26381;&#21153;&#22120;&#21644;&#35774;&#22791;&#30340;&#25928;&#29992;&#21644;&#25968;&#25454;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.13681</link><description>&lt;p&gt;
RealFM: &#19968;&#20010;&#30495;&#23454;&#26426;&#21046;&#65292;&#28608;&#21169;&#25968;&#25454;&#36129;&#29486;&#21644;&#35774;&#22791;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
RealFM: A Realistic Mechanism to Incentivize Data Contribution and Device Participation. (arXiv:2310.13681v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13681
&lt;/p&gt;
&lt;p&gt;
RealFM &#26159;&#19968;&#20010;&#30495;&#23454;&#30340;&#32852;&#37030;&#26426;&#21046;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#23454;&#29615;&#22659;&#20013;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#39547;&#27850;&#32773;&#38382;&#39064;&#65292;&#36890;&#36807;&#27169;&#25311;&#35774;&#22791;&#25928;&#29992;&#12289;&#28608;&#21169;&#25968;&#25454;&#36129;&#29486;&#21644;&#35774;&#22791;&#21442;&#19982;&#65292;&#24182;&#25552;&#20379;&#20102;&#38750;&#32447;&#24615;&#20851;&#31995;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#25928;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26381;&#21153;&#22120;&#21644;&#35774;&#22791;&#30340;&#25928;&#29992;&#21644;&#25968;&#25454;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35774;&#22791;&#21442;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#24120;&#22312;&#35774;&#22791;-&#26381;&#21153;&#22120;&#36890;&#20449;&#30340;&#35270;&#35282;&#19979;&#36827;&#34892;&#30740;&#31350;&#65288;&#20363;&#22914;&#35774;&#22791;&#25481;&#32447;&#65289;&#65292;&#24182;&#20551;&#35774;&#36793;&#32536;&#35774;&#22791;&#26377;&#25345;&#32493;&#21442;&#19982;FL&#30340;&#24895;&#26395;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#23454;&#26045;&#24403;&#21069;&#30340;FL&#26694;&#26550;&#23384;&#22312;&#32570;&#38519;&#65292;&#35768;&#22810;&#26694;&#26550;&#36935;&#21040;&#20102;&#39547;&#27850;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#23558;FL&#25512;&#21521;&#26356;&#30495;&#23454;&#30340;&#29615;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RealFM&#65306;&#31532;&#19968;&#20010;&#30495;&#27491;&#30340;&#32852;&#37030;&#26426;&#21046;&#65292;&#23427;&#65288;1&#65289;&#23454;&#38469;&#22320;&#27169;&#25311;&#35774;&#22791;&#25928;&#29992;&#65292;&#65288;2&#65289;&#28608;&#21169;&#25968;&#25454;&#36129;&#29486;&#21644;&#35774;&#22791;&#21442;&#19982;&#65292;&#65288;3&#65289;&#21487;&#35777;&#26126;&#22320;&#28040;&#38500;&#20102;&#39547;&#27850;&#32773;&#29616;&#35937;&#12290;RealFM&#19981;&#38656;&#35201;&#25968;&#25454;&#20849;&#20139;&#65292;&#24182;&#20801;&#35768;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#25928;&#29992;&#20043;&#38388;&#23384;&#22312;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#30456;&#27604;&#20110;&#19981;&#21442;&#19982;&#21644;&#20854;&#20182;FL&#26426;&#21046;&#30340;&#35774;&#22791;&#65292;&#25552;&#39640;&#20102;&#26381;&#21153;&#22120;&#21644;&#21442;&#19982;&#35774;&#22791;&#30340;&#25928;&#29992;&#21644;&#25968;&#25454;&#36129;&#29486;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#65292;RealFM&#25552;&#39640;&#20102;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#30340;&#25928;&#29992;&#20197;&#21450;&#25968;&#25454;&#36129;&#29486;&#65292;&#26368;&#22810;&#21487;&#36798;...
&lt;/p&gt;
&lt;p&gt;
Edge device participation in federating learning (FL) has been typically studied under the lens of device-server communication (e.g., device dropout) and assumes an undying desire from edge devices to participate in FL. As a result, current FL frameworks are flawed when implemented in real-world settings, with many encountering the free-rider problem. In a step to push FL towards realistic settings, we propose RealFM: the first truly federated mechanism which (1) realistically models device utility, (2) incentivizes data contribution and device participation, and (3) provably removes the free-rider phenomena. RealFM does not require data sharing and allows for a non-linear relationship between model accuracy and utility, which improves the utility gained by the server and participating devices compared to non-participating devices as well as devices participating in other FL mechanisms. On real-world data, RealFM improves device and server utility, as well as data contribution, by up t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11085</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#36890;&#36807;&#35780;&#20272;&#20351;&#29992;DocRED&#25968;&#25454;&#38598;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#20174;&#25991;&#26412;&#25991;&#26723;&#20013;&#25512;&#26029;&#32467;&#26500;&#21270;&#30340;&#20154;&#31867;&#30693;&#35782;&#12290;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#26377;&#20004;&#20010;&#38480;&#21046;&#65306;(1)&#23427;&#20204;&#35201;&#27714;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#36755;&#20837;&#25110;&#25512;&#26029;&#23427;&#20204;&#65292;&#20174;&#32780;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#22122;&#22768;&#65292;(2)&#23427;&#20204;&#38656;&#35201;&#20154;&#24037;&#23545;&#25991;&#26723;&#36827;&#34892;&#27880;&#37322;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#23450;&#21046;&#30340;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#23398;&#20064;&#33539;&#24335;&#30340;&#30740;&#31350;&#32773;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#22312;&#28040;&#38500;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26723;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20851;&#38190;&#24615;&#30340;&#20248;&#21183;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36731;&#26494;&#26356;&#26032;&#21040;&#26032;&#30340;&#20851;&#31995;&#38598;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;DocRED&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36825;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;&#22240;&#26524;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20123;&#26681;&#26412;&#24615;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#31561;&#26377;&#30410;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11011</link><description>&lt;p&gt;
&#20174;&#21487;&#35782;&#21035;&#30340;&#22240;&#26524;&#34920;&#31034;&#21040;&#21487;&#25511;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#65306;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling. (arXiv:2310.11011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#65292;&#20854;&#20013;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;&#22240;&#26524;&#29702;&#35770;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20123;&#26681;&#26412;&#24615;&#32570;&#28857;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#31561;&#26377;&#30410;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#23494;&#24230;&#20272;&#35745;&#21644;&#20174;&#26377;&#38480;&#26679;&#26412;&#20013;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#26681;&#26412;&#24615;&#30340;&#32570;&#28857;&#65292;&#22914;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#24341;&#20837;&#34394;&#20551;&#30456;&#20851;&#24615;&#21644;&#24046;&#21170;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21487;&#20197;&#23558;&#22240;&#26524;&#29702;&#35770;&#34701;&#20837;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#20013;&#12290;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#25551;&#36848;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#23545;&#31995;&#32479;&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#22240;&#26524;&#20851;&#31995;&#21644;&#26426;&#21046;&#36827;&#34892;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#21487;&#20197;&#19982;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#33258;&#28982;&#22320;&#32467;&#21512;&#12290;&#22240;&#26524;&#27169;&#22411;&#20026;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20960;&#20010;&#26377;&#30410;&#30340;&#23646;&#24615;&#65292;&#22914;&#20998;&#24067;&#20559;&#31227;&#40065;&#26834;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#22240;&#26524;&#29983;&#25104;&#24314;&#27169;&#30340;&#25216;&#26415;&#32508;&#36848;&#65292;&#20998;&#20026;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#21487;&#25511;&#21453;&#20107;&#23454;&#29983;&#25104;&#20004;&#20010;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models have shown tremendous success in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, the tendency to induce spurious correlations, and poor out-of-distribution extrapolation. In an effort to remedy such challenges, one can incorporate the theory of causality in deep generative modeling. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interoperability. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual ge
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2310.03272</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#20256;&#36755;&#30340;&#22270;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#33258;&#32534;&#30721;&#22120;&#30340;&#32593;&#32476;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;&#23545;&#40784;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#26159;&#22312;&#19981;&#21516;&#22270;&#20043;&#38388;&#24314;&#31435;&#19968;&#23545;&#19968;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#65292;&#22312;&#39640;&#24433;&#21709;&#39046;&#22495;&#20013;&#26377;&#22823;&#37327;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#34987;&#35748;&#20026;&#26159;NP&#38590;&#30340;&#65292;&#32780;&#19988;&#29616;&#26377;&#30340;&#31639;&#27861;&#22312;&#22270;&#30340;&#35268;&#27169;&#22686;&#22823;&#26102;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24191;&#20041;&#22270;&#33258;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#25552;&#21462;&#24378;&#22823;&#19988;&#40065;&#26834;&#30340;&#33410;&#28857;&#23884;&#20837;&#65292;&#36866;&#29992;&#20110;&#23545;&#40784;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#29983;&#25104;&#30340;&#23884;&#20837;&#19982;&#22270;&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#65292;&#24182;&#19988;&#19982;&#32463;&#20856;&#35889;&#26041;&#27861;&#30456;&#27604;&#21487;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#25928;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#23545;&#40784;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32593;&#32476;&#23545;&#40784;&#21644;&#23376;&#32593;&#32476;&#23545;&#40784;&#23454;&#39564;&#65292;&#25552;&#20379;&#20102;&#25903;&#25345;&#35813;&#26694;&#26550;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scala
&lt;/p&gt;</description></item><item><title>&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.17335</link><description>&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Graph Generators. (arXiv:2309.17335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17335
&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#65292;&#24182;&#38544;&#24335;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21462;&#24471;&#20102;state-of-the-art&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24322;&#27493;&#22270;&#29983;&#25104;&#22120;&#65288;AGG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#36890;&#36947;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;AGG&#23558;&#35266;&#27979;&#20540;&#24314;&#27169;&#20026;&#21160;&#24577;&#22270;&#19978;&#30340;&#33410;&#28857;&#65292;&#24182;&#36890;&#36807;&#36716;&#23548;&#24335;&#33410;&#28857;&#29983;&#25104;&#36827;&#34892;&#25968;&#25454;&#25554;&#34917;&#12290;AGG&#19981;&#20381;&#36182;&#20110;&#24490;&#29615;&#32452;&#20214;&#25110;&#23545;&#26102;&#38388;&#35268;&#24459;&#30340;&#20551;&#35774;&#65292;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#23558;&#27979;&#37327;&#20540;&#12289;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#30452;&#25509;&#34920;&#31034;&#22312;&#33410;&#28857;&#20013;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#23398;&#20064;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#26679;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#38544;&#24335;&#22320;&#23398;&#20064;&#20256;&#24863;&#22120;&#27979;&#37327;&#30340;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#21487;&#20197;&#22522;&#20110;&#26410;&#35265;&#26102;&#38388;&#25139;&#21644;&#20803;&#25968;&#25454;&#23545;&#26032;&#30340;&#27979;&#37327;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;AGG&#22312;&#27010;&#24565;&#21644;&#23454;&#35777;&#20004;&#26041;&#38754;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#31616;&#35201;&#35752;&#35770;&#20102;&#25968;&#25454;&#22686;&#24378;&#23545;AGG&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AGG&#22312;t
&lt;/p&gt;
&lt;p&gt;
We introduce the asynchronous graph generator (AGG), a novel graph neural network architecture for multi-channel time series which models observations as nodes on a dynamic graph and can thus perform data imputation by transductive node generation. Completely free from recurrent components or assumptions about temporal regularity, AGG represents measurements, timestamps and metadata directly in the nodes via learnable embeddings, to then leverage attention to learn expressive relationships across the variables of interest. This way, the proposed architecture implicitly learns a causal graph representation of sensor measurements which can be conditioned on unseen timestamps and metadata to predict new measurements by an expansion of the learnt graph. The proposed AGG is compared both conceptually and empirically to previous work, and the impact of data augmentation on the performance of AGG is also briefly discussed. Our experiments reveal that AGG achieved state-of-the-art results in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16391</link><description>&lt;p&gt;
&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;&#20108;&#32500;Copula&#36924;&#36817;&#21464;&#25442;&#65306;2-Cats&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Copula&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#32500;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#24212;&#29992;Copula&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#29420;&#31435;&#30340;&#36793;&#38469;&#20998;&#24067;&#65288;&#19968;&#20010;&#31616;&#21333;&#20219;&#21153;&#65289;&#65292;&#28982;&#21518;&#20272;&#35745;&#36830;&#25509;&#36793;&#38469;&#30340;&#21333;&#20010;Copula&#20989;&#25968;C&#65288;&#19968;&#20010;&#22256;&#38590;&#20219;&#21153;&#65289;&#26469;&#20272;&#35745;&#22810;&#20803;&#20998;&#24067;&#20989;&#25968;&#12290;&#23545;&#20110;&#20108;&#32500;&#25968;&#25454;&#65292;Copula&#26159;&#19968;&#20010;&#24418;&#22914;C&#65306;(u&#65292;v)&#8712;\mathbf{I}^2\rightarrow \mathbf{I}&#30340;&#20108;&#27425;&#22686;&#20989;&#25968;&#65292;&#20854;&#20013;\mathbf{I}=[0&#65292;1]&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22914;&#20309;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;2-Cats&#65292;&#21463;&#21040;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;Sobolev&#35757;&#32451;&#25991;&#29486;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#19981;&#20165;&#35777;&#26126;&#20102;&#25105;&#20204;&#33021;&#22815;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#22320;&#20272;&#35745;2D Copula&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#19988;&#31526;&#21512;Copula C&#30340;&#25968;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEL&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;20&#20010;&#35757;&#32451;&#36718;&#25968;&#20869;&#26377;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#24120;&#29992;&#24494;&#35843;&#26041;&#27861;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.10019</link><description>&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#38271;&#23614;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Long-Tailed Recognition. (arXiv:2309.10019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEL&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;20&#20010;&#35757;&#32451;&#36718;&#25968;&#20869;&#26377;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#24120;&#29992;&#24494;&#35843;&#26041;&#27861;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20986;&#29616;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;CLIP&#65289;&#65292;"&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;"&#33539;&#20363;&#22312;&#35299;&#20915;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#34429;&#28982;&#20808;&#21069;&#30740;&#31350;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24076;&#26395;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#36718;&#25968;&#25110;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20445;&#25345;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PEL&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;20&#20010;&#35757;&#32451;&#36718;&#25968;&#20869;&#26377;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#38271;&#23614;&#35782;&#21035;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#24120;&#29992;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;&#20363;&#22914;&#20840;&#38754;&#24494;&#35843;&#21644;&#20998;&#31867;&#22120;&#24494;&#35843;&#65289;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;PEL&#37319;&#29992;&#20102;&#29616;&#26377;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#65292;&#24341;&#20837;&#20102;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The "pre-training and fine-tuning" paradigm in addressing long-tailed recognition tasks has sparked significant interest since the emergence of large vision-language models like the contrastive language-image pre-training (CLIP). While previous studies have shown promise in adapting pre-trained models for these tasks, they often undesirably require extensive training epochs or additional training data to maintain good performance. In this paper, we propose PEL, a fine-tuning method that can effectively adapt pre-trained models to long-tailed recognition tasks in fewer than 20 epochs without the need for extra data. We first empirically find that commonly used fine-tuning methods, such as full fine-tuning and classifier fine-tuning, suffer from overfitting, resulting in performance deterioration on tail classes. To mitigate this issue, PEL introduces a small number of task-specific parameters by adopting the design of any existing parameter-efficient fine-tuning method. Additionally, to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05516</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;LLMs&#37327;&#21270;&#20013;&#30340;&#26435;&#37325;&#33293;&#20837;
&lt;/p&gt;
&lt;p&gt;
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25191;&#34892;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#38543;&#30528;&#20301;&#25968;&#30340;&#20943;&#23569;&#65292;&#37327;&#21270;&#32593;&#26684;&#21464;&#24471;&#26356;&#21152;&#23485;&#27867;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19978;&#19979;&#33293;&#20837;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#32454;&#35843;&#19978;&#19979;&#33293;&#20837;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21046;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#31934;&#30830;&#19988;&#26377;&#38480;&#30340;&#36793;&#30028;&#65292;&#21482;&#26377;&#25913;&#21464;&#33293;&#20837;&#20540;&#30340;&#38408;&#20540;&#25165;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#39640;&#25928;&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;SignRound&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#30340;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#20013;&#21033;&#29992;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#36827;&#34892;&#19981;&#24179;&#34913;&#31867;&#21035;&#20013;&#20266;&#36857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#26631;&#35760;&#21307;&#30103;&#25968;&#25454;&#38598;&#21644;&#20266;&#36857;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08480</link><description>&lt;p&gt;
&#21033;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#20013;&#30340;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#36827;&#34892;&#19981;&#24179;&#34913;&#31867;&#21035;&#20013;&#30340;&#20266;&#36857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Label Propagation Techniques for Artifact Detection in Imbalanced Classes using Photoplethysmogram Signals. (arXiv:2308.08480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08480
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#20013;&#21033;&#29992;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#36827;&#34892;&#19981;&#24179;&#34913;&#31867;&#21035;&#20013;&#20266;&#36857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#26631;&#35760;&#21307;&#30103;&#25968;&#25454;&#38598;&#21644;&#20266;&#36857;&#20998;&#31867;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20449;&#21495;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#34987;&#24191;&#27867;&#29992;&#20110;&#30417;&#27979;&#29983;&#21629;&#20307;&#24449;&#65292;&#20294;&#23427;&#20204;&#23481;&#26131;&#21463;&#21040;&#36816;&#21160;&#20266;&#36857;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#24179;&#34913;&#31867;&#21035;&#22330;&#26223;&#20013;&#20351;&#29992;&#26631;&#31614;&#20256;&#25773;&#25216;&#26415;&#22312;PPG&#26679;&#26412;&#20043;&#38388;&#20256;&#25773;&#26631;&#31614;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24178;&#20928;&#30340;PPG&#26679;&#26412;&#26126;&#26174;&#23569;&#20110;&#21463;&#20266;&#36857;&#27745;&#26579;&#30340;&#26679;&#26412;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#27809;&#26377;&#20266;&#36857;&#30340;&#31867;&#21035;&#20013;&#65292;&#31934;&#30830;&#24230;&#20026;91%&#65292;&#21484;&#22238;&#29575;&#20026;90%&#65292;F1&#24471;&#20998;&#20026;90%&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#26631;&#35760;&#21307;&#30103;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#24178;&#20928;&#26679;&#26412;&#24456;&#23569;&#12290;&#23545;&#20110;&#20266;&#36857;&#30340;&#20998;&#31867;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#27604;&#36739;&#20102;&#20256;&#32479;&#20998;&#31867;&#22120;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;MLP&#12289;Transformers&#12289;FCN&#65289;&#31561;&#26377;&#30417;&#30563;&#20998;&#31867;&#22120;&#19982;&#21322;&#30417;&#30563;&#26631;&#31614;&#20256;&#25773;&#31639;&#27861;&#12290;KNN&#26377;&#30417;&#30563;&#27169;&#22411;&#20855;&#26377;89%&#30340;&#31934;&#30830;&#24230;&#12289;95%&#30340;&#21484;&#22238;&#29575;&#21644;92%&#30340;F1&#24471;&#20998;&#65292;&#32467;&#26524;&#33391;&#22909;&#65292;&#20294;&#21322;&#30417;&#30563;&#31639;&#27861;&#22312;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Photoplethysmogram (PPG) signals are widely used in healthcare for monitoring vital signs, but they are susceptible to motion artifacts that can lead to inaccurate interpretations. In this study, the use of label propagation techniques to propagate labels among PPG samples is explored, particularly in imbalanced class scenarios where clean PPG samples are significantly outnumbered by artifact-contaminated samples. With a precision of 91%, a recall of 90% and an F1 score of 90% for the class without artifacts, the results demonstrate its effectiveness in labeling a medical dataset, even when clean samples are rare. For the classification of artifacts our study compares supervised classifiers such as conventional classifiers and neural networks (MLP, Transformers, FCN) with the semi-supervised label propagation algorithm. With a precision of 89%, a recall of 95% and an F1 score of 92%, the KNN supervised model gives good results, but the semi-supervised algorithm performs better in detec
&lt;/p&gt;</description></item><item><title>U-Turn&#25193;&#25955;&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;U-Turn Diffusion&#25216;&#26415;&#26469;&#25913;&#36827;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#25216;&#26415;&#32467;&#21512;&#20102;&#21069;&#21521;&#12289;U-Turn&#21644;&#21453;&#21521;&#36807;&#31243;&#65292;&#36890;&#36807;&#35299;&#26500;&#24555;&#36895;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#29983;&#25104;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07421</link><description>&lt;p&gt;
U-Turn&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
U-Turn Diffusion. (arXiv:2308.07421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07421
&lt;/p&gt;
&lt;p&gt;
U-Turn&#25193;&#25955;&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;U-Turn Diffusion&#25216;&#26415;&#26469;&#25913;&#36827;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#25216;&#26415;&#32467;&#21512;&#20102;&#21069;&#21521;&#12289;U-Turn&#21644;&#21453;&#21521;&#36807;&#31243;&#65292;&#36890;&#36807;&#35299;&#26500;&#24555;&#36895;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#29983;&#25104;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;AI&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#30001;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#39537;&#21160;&#30340;&#21160;&#24577;&#36741;&#21161;&#26102;&#38388;&#26426;&#21046;&#65292;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#33719;&#21462;&#20998;&#25968;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#35780;&#20272;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#25928;&#29575;&#30340;&#26631;&#20934;&#65306;&#29983;&#25104;&#36807;&#31243;&#30340;&#33021;&#21147;&#21462;&#20915;&#20110;&#22312;&#21453;&#21521;/&#21435;&#22122;&#38454;&#27573;&#35299;&#26500;&#24555;&#36895;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#36136;&#37327;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#8220;U-Turn Diffusion&#8221;&#30340;&#26041;&#27861;&#12290;U-Turn Diffusion&#25216;&#26415;&#20174;&#26631;&#20934;&#30340;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#24320;&#22987;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#20256;&#32479;&#35774;&#32622;&#65292;&#23427;&#30340;&#25345;&#32493;&#26102;&#38388;&#26356;&#30701;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25191;&#34892;&#26631;&#20934;&#30340;&#21453;&#21521;&#21160;&#21147;&#23398;&#65292;&#20197;&#21069;&#21521;&#36807;&#31243;&#30340;&#26368;&#32456;&#37197;&#32622;&#20026;&#21021;&#22987;&#20540;&#12290;&#36825;&#31181;&#32467;&#21512;&#20102;&#21069;&#21521;&#12289;U-Turn&#21644;&#21453;&#21521;&#36807;&#31243;&#30340;U-Turn Diffusion&#36807;&#31243;&#21019;&#24314;&#19968;&#20010;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive examination of score-based diffusion models of AI for generating synthetic images. These models hinge upon a dynamic auxiliary time mechanism driven by stochastic differential equations, wherein the score function is acquired from input images. Our investigation unveils a criterion for evaluating efficiency of the score-based diffusion models: the power of the generative process depends on the ability to de-construct fast correlations during the reverse/de-noising phase. To improve the quality of the produced synthetic images, we introduce an approach coined "U-Turn Diffusion". The U-Turn Diffusion technique starts with the standard forward diffusion process, albeit with a condensed duration compared to conventional settings. Subsequently, we execute the standard reverse dynamics, initialized with the concluding configuration from the forward process. This U-Turn Diffusion procedure, combining forward, U-turn, and reverse processes, creates a synthetic image 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#22240;&#23376;&#21246;&#32467;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37329;&#34701;&#25968;&#25454;&#20013;&#30340;&#19981;&#23545;&#31216;&#21644;&#26497;&#31471;&#23614;&#37096;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#32929;&#31080;&#23545;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#38750;&#23545;&#31216;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2308.05564</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#21246;&#32467;&#30340;&#39640;&#25928;&#21464;&#20998;&#25512;&#29702;&#21450;&#20854;&#22312;&#32929;&#31080;&#25910;&#30410;&#29575;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Variational Inference for Large Skew-t Copulas with Application to Intraday Equity Returns. (arXiv:2308.05564v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#22240;&#23376;&#21246;&#32467;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#37329;&#34701;&#25968;&#25454;&#20013;&#30340;&#19981;&#23545;&#31216;&#21644;&#26497;&#31471;&#23614;&#37096;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#32929;&#31080;&#23545;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#38750;&#23545;&#31216;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#20559;t&#20044;&#40486;&#22240;&#23376;&#21246;&#32467;&#27169;&#22411;&#23545;&#37329;&#34701;&#25968;&#25454;&#24314;&#27169;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20801;&#35768;&#19981;&#23545;&#31216;&#21644;&#26497;&#31471;&#30340;&#23614;&#37096;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Azzalini&#21644;Capitanio&#65288;2003&#65289;&#25152;&#38544;&#21547;&#30340;&#20044;&#40486;&#21246;&#32467;&#22312;&#25104;&#23545;&#38750;&#23545;&#31216;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#20004;&#31181;&#27969;&#34892;&#30340;&#20044;&#40486;&#21246;&#32467;&#26356;&#39640;&#12290;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#23545;&#35813;&#20044;&#40486;&#21246;&#32467;&#30340;&#20272;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#21464;&#20998;&#25512;&#29702;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#26465;&#20214;&#39640;&#26031;&#29983;&#25104;&#34920;&#31034;&#27861;&#23450;&#20041;&#20102;&#19968;&#20010;&#21487;&#20197;&#20934;&#30830;&#36817;&#20284;&#30340;&#38468;&#21152;&#21518;&#39564;&#12290;&#20351;&#29992;&#24555;&#36895;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#26469;&#35299;&#20915;&#21464;&#20998;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#30340;&#26041;&#27861;&#34987;&#29992;&#26469;&#20272;&#35745;2017&#24180;&#33267;2021&#24180;&#38388;93&#20010;&#32654;&#22269;&#32929;&#31080;&#30340;&#32929;&#31080;&#25910;&#30410;&#29575;&#30340;&#21246;&#32467;&#27169;&#22411;&#12290;&#38500;&#20102;&#25104;&#23545;&#30456;&#20851;&#24615;&#30340;&#21464;&#21270;&#22806;&#65292;&#35813;&#21246;&#32467;&#36824;&#25429;&#25417;&#21040;&#20102;&#32929;&#31080;&#23545;&#20043;&#38388;&#30340;&#38750;&#23545;&#31216;&#20381;&#36182;&#30340;&#22823;&#37327;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large skew-t factor copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a conditionally Gaussian generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A fast stochastic gradient ascent algorithm is used to solve the variational optimization. The new methodology is used to estimate copula models for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#26377;&#38480;&#26102;&#38388;&#19981;&#22343;&#21248;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#23454;&#29616;&#26497;&#23567;&#21518;&#24724;&#30340;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13586</link><description>&lt;p&gt;
&#35299;&#20915;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Settling the Sample Complexity of Online Reinforcement Learning. (arXiv:2307.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#26377;&#38480;&#26102;&#38388;&#19981;&#22343;&#21248;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#20013;&#23454;&#29616;&#26497;&#23567;&#21518;&#24724;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#25968;&#25454;&#25928;&#29575;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#28176;&#36817;&#26368;&#23567;&#30340;&#21518;&#24724;&#65292;&#20294;&#36825;&#20123;&#32467;&#26524;&#30340;&#26368;&#20248;&#24615;&#20165;&#22312;&#8220;&#22823;&#26679;&#26412;&#8221;&#24773;&#20917;&#19979;&#24471;&#21040;&#20445;&#35777;&#65292;&#20026;&#20102;&#20351;&#20854;&#31639;&#27861;&#36816;&#34892;&#26368;&#20339;&#65292;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#30340;&#39044;&#29123;&#25104;&#26412;&#12290;&#22914;&#20309;&#22312;&#19981;&#20135;&#29983;&#20219;&#20309;&#39044;&#29123;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26497;&#23567;&#21518;&#24724;&#30340;&#26368;&#20248;&#24615;&#19968;&#30452;&#26159;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#26377;&#38480;&#26102;&#38388;&#19981;&#22343;&#21248;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#38382;&#39064;&#30340;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31181;&#20462;&#25913;&#29256;&#30340;&#21333;&#35843;&#20540;&#20256;&#25773;(MVP)&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26159;&#30001;\cite{zhang2020reinforcement}&#25552;&#20986;&#30340;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#20351;&#24471;&#21518;&#24724;&#30340;&#37327;&#32423;&#20026;(&#27169;&#38500;&#23545;&#25968;&#22240;&#23376;)\begin{equation *} \min\biggr\{ \sqrt{SAH^3K}&#65292;\&#65292;HK \biggr\}&#65292;\end{equation *}&#20854;&#20013;$S$&#26159;&#29366;&#24577;&#25968;&#65292;$A$&#26159;&#21160;&#20316;&#25968;&#65292;$H$&#26159;&#35268;&#21010;&#26102;&#22495;&#65292;$K$&#26159;&#24635;&#30340;&#22238;&#21512;&#25968;&#12290;&#36825;&#20010;&#21518;&#24724;&#30340;&#37327;&#32423;&#19982;&#26497;&#23567;&#21270;&#21518;&#24724;&#37327;&#32423;&#26159;&#30456;&#21305;&#37197;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.  We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors) \begin{equation*}  \min\big\{ \sqrt{SAH^3K}, \,HK \big\}, \end{equation*} where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes. This regret matches the minimax 
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.13565</link><description>&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65306;&#22522;&#30784;&#12289;&#29616;&#29366;&#12289;&#22522;&#20934;&#21644;&#26410;&#26469;&#26426;&#20250;
&lt;/p&gt;
&lt;p&gt;
Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities. (arXiv:2307.13565v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13565
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#65292;&#26088;&#22312;&#20248;&#21270;&#20915;&#31574;&#12290;&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#30340;&#30456;&#20851;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#20998;&#31867;&#27861;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#23548;&#21521;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#35757;&#32451;&#27169;&#22411;&#20197;&#20248;&#21270;&#20915;&#31574;&#65292;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#20013;&#38598;&#25104;&#20102;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#36825;&#20010;&#33539;&#24335;&#26377;&#26395;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20915;&#31574;&#21046;&#23450;&#65292;&#36825;&#20123;&#24212;&#29992;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#36816;&#20316;&#65292;&#22312;&#36825;&#20123;&#20915;&#31574;&#27169;&#22411;&#20013;&#20272;&#35745;&#26410;&#30693;&#21442;&#25968;&#32463;&#24120;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#23545;DFL&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#23427;&#23545;&#21508;&#31181;&#25216;&#26415;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20197;&#25972;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26681;&#25454;&#20854;&#29420;&#29305;&#29305;&#24449;&#26469;&#21306;&#20998;DFL&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;DFL&#30340;&#21512;&#36866;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;DFL&#30740;&#31350;&#20013;&#24403;&#21069;&#21644;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused learning (DFL) is an emerging paradigm in machine learning which trains a model to optimize decisions, integrating prediction and optimization in an end-to-end system. This paradigm holds the promise to revolutionize decision-making in many real-world applications which operate under uncertainty, where the estimation of unknown parameters within these decision models often becomes a substantial roadblock. This paper presents a comprehensive review of DFL. It provides an in-depth analysis of the various techniques devised to integrate machine learning and optimization models introduces a taxonomy of DFL methods distinguished by their unique characteristics, and conducts an extensive empirical evaluation of these methods proposing suitable benchmark dataset and tasks for DFL. Finally, the study provides valuable insights into current and potential future avenues in DFL research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#30340;&#21442;&#25968;&#20272;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.04191</link><description>&lt;p&gt;
&#20851;&#20110;&#36923;&#36753;&#22238;&#24402;&#20013;&#21442;&#25968;&#20272;&#35745;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the sample complexity of estimation in logistic regression. (arXiv:2307.04191v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#30340;&#21442;&#25968;&#20272;&#35745;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#26159;&#22122;&#22768;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26631;&#20934;&#27491;&#24577;&#21327;&#21464;&#37327;&#19979;&#65292;&#20197;$\ell_2$&#35823;&#24046;&#20026;&#38480;&#65292;&#20272;&#35745;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#21442;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#32771;&#34385;&#20102;&#32500;&#24230;&#21644;&#36870;&#28201;&#24230;&#30340;&#24433;&#21709;&#12290;&#36870;&#28201;&#24230;&#25511;&#21046;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#20449;&#22122;&#27604;&#12290;&#34429;&#28982;&#36923;&#36753;&#22238;&#24402;&#30340;&#24191;&#20041;&#30028;&#38480;&#21644;&#28176;&#36817;&#24615;&#33021;&#24050;&#32463;&#26377;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#20851;&#20110;&#21442;&#25968;&#20272;&#35745;&#30340;&#38750;&#28176;&#36817;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#20043;&#21069;&#30340;&#20998;&#26512;&#20013;&#27809;&#26377;&#35752;&#35770;&#20854;&#19982;&#35823;&#24046;&#21644;&#36870;&#28201;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#26354;&#32447;&#22312;&#36870;&#28201;&#24230;&#26041;&#38754;&#20855;&#26377;&#20004;&#20010;&#36716;&#25240;&#28857;&#65288;&#25110;&#20020;&#30028;&#28857;&#65289;&#65292;&#26126;&#30830;&#21010;&#20998;&#20102;&#20302;&#12289;&#20013;&#21644;&#39640;&#28201;&#24230;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The logistic regression model is one of the most popular data generation model in noisy binary classification problems. In this work, we study the sample complexity of estimating the parameters of the logistic regression model up to a given $\ell_2$ error, in terms of the dimension and the inverse temperature, with standard normal covariates. The inverse temperature controls the signal-to-noise ratio of the data generation process. While both generalization bounds and asymptotic performance of the maximum-likelihood estimator for logistic regression are well-studied, the non-asymptotic sample complexity that shows the dependence on error and the inverse temperature for parameter estimation is absent from previous analyses. We show that the sample complexity curve has two change-points (or critical points) in terms of the inverse temperature, clearly separating the low, moderate, and high temperature regimes.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#37319;&#26679;&#36807;&#31243;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#21464;&#20998;&#24418;&#24335;&#30340;&#36335;&#24452;&#31354;&#38388;&#24230;&#37327;&#65292;&#25552;&#20986;&#20102;&#23545;&#25968;&#26041;&#24046;&#25439;&#22833;&#65292;&#20248;&#21270;&#20102;&#37319;&#26679;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01198</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#25913;&#36827;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Improved sampling via learned diffusions. (arXiv:2307.01198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01198
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#37319;&#26679;&#36807;&#31243;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#21464;&#20998;&#24418;&#24335;&#30340;&#36335;&#24452;&#31354;&#38388;&#24230;&#37327;&#65292;&#25552;&#20986;&#20102;&#23545;&#25968;&#26041;&#24046;&#25439;&#22833;&#65292;&#20248;&#21270;&#20102;&#37319;&#26679;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#25511;&#21046;&#25193;&#25955;&#36807;&#31243;&#20174;&#38750;&#26631;&#20934;&#21270;&#30446;&#26631;&#23494;&#24230;&#20013;&#37319;&#26679;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#35270;&#20026;Schr&#246;dinger&#26725;&#38382;&#39064;&#30340;&#29305;&#20363;&#65292;&#23547;&#27714;&#32473;&#23450;&#20808;&#39564;&#20998;&#24067;&#21644;&#25351;&#23450;&#30446;&#26631;&#20043;&#38388;&#26368;&#21487;&#33021;&#30340;&#38543;&#26426;&#28436;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26102;&#38388;&#21453;&#28436;&#25193;&#25955;&#36807;&#31243;&#30340;&#36335;&#24452;&#31354;&#38388;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#21464;&#20998;&#24418;&#24335;&#26469;&#25512;&#24191;&#36825;&#20010;&#26694;&#26550;&#12290;&#36825;&#20010;&#25277;&#35937;&#30340;&#35270;&#35282;&#23548;&#33268;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#30340;&#23454;&#38469;&#25439;&#22833;&#65292;&#24182;&#23558;&#20808;&#21069;&#30340;&#30446;&#26631;&#20316;&#20026;&#29305;&#20363;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#32771;&#34385;&#38500;&#20102;&#24050;&#30693;&#23384;&#22312;&#27169;&#24335;&#22349;&#32553;&#38382;&#39064;&#30340;&#21453;&#21521;Kullback-Leibler&#24046;&#21035;&#20043;&#22806;&#30340;&#20854;&#20182;&#24046;&#21035;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25152;&#35859;&#30340;&#23545;&#25968;&#26041;&#24046;&#25439;&#22833;&#65292;&#23427;&#20855;&#26377;&#33391;&#22909;&#30340;&#25968;&#20540;&#29305;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#25152;&#26377;&#32771;&#34385;&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a series of papers proposed deep learning-based approaches to sample from unnormalized target densities using controlled diffusion processes. In this work, we identify these approaches as special cases of the Schr\"odinger bridge problem, seeking the most likely stochastic evolution between a given prior distribution and the specified target. We further generalize this framework by introducing a variational formulation based on divergences between path space measures of time-reversed diffusion processes. This abstract perspective leads to practical losses that can be optimized by gradient-based algorithms and includes previous objectives as special cases. At the same time, it allows us to consider divergences other than the reverse Kullback-Leibler divergence that is known to suffer from mode collapse. In particular, we propose the so-called log-variance loss, which exhibits favorable numerical properties and leads to significantly improved performance across all considered a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#23398;&#20064;&#21512;&#36866;&#30340;&#25104;&#26412;&#32467;&#26500;&#26469;&#40723;&#21169;&#26144;&#23556;&#27839;&#30528;&#29305;&#23450;&#30340;&#24037;&#31243;&#29305;&#24449;&#26469;&#20256;&#36865;&#28857;&#65292;&#20854;&#22312;&#25193;&#23637; Monge-Bregman-Occam &#31649;&#36947;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#24182;&#22312;&#21305;&#37197;&#30452;&#26041;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11895</link><description>&lt;p&gt;
&#23398;&#20064;&#32467;&#26500;&#33945;&#26085;&#20301;&#31227;&#30340;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Learning Costs for Structured Monge Displacements. (arXiv:2306.11895v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#23398;&#20064;&#21512;&#36866;&#30340;&#25104;&#26412;&#32467;&#26500;&#26469;&#40723;&#21169;&#26144;&#23556;&#27839;&#30528;&#29305;&#23450;&#30340;&#24037;&#31243;&#29305;&#24449;&#26469;&#20256;&#36865;&#28857;&#65292;&#20854;&#22312;&#25193;&#23637; Monge-Bregman-Occam &#31649;&#36947;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#24182;&#22312;&#21305;&#37197;&#30452;&#26041;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#20026;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#22810;&#31181;&#25512;&#26029;&#26679;&#26412;&#38388;&#23494;&#24230;&#21069;&#21521;&#26144;&#23556;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#35813;&#29702;&#35770;&#24050;&#32463;&#35265;&#35777;&#20102;&#35768;&#22810;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#20294;&#20854;&#23454;&#38469;&#23454;&#29616;&#20173;&#28982;&#26497;&#20854;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#21516;&#26102;&#38754;&#20020;&#35745;&#31639;&#21644;&#32479;&#35745;&#19978;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#24456;&#23569;&#26377;&#19981;&#20351;&#29992;&#40664;&#35748;&#36873;&#25321;&#26469;&#20272;&#35745;&#36825;&#20123;&#26144;&#23556;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#31616;&#21333;&#30340;&#24179;&#26041;&#27431;&#27663;&#36317;&#31163;&#20316;&#20026;&#22320;&#38754;&#36153;&#29992;$c(x,y)=\|x-y\|^2_2$&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#37319;&#21462;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20197;\emph{&#23398;&#20064;}&#21512;&#36866;&#30340;&#25104;&#26412;&#32467;&#26500;&#65292;&#40723;&#21169;&#26144;&#23556;&#27839;&#30528;&#29305;&#23450;&#30340;&#24037;&#31243;&#29305;&#24449;&#26469;&#20256;&#36865;&#28857;&#12290;&#25105;&#20204;&#23558;&#26368;&#36817;&#25552;&#20986;&#30340; Monge-Bregman-Occam &#31649;&#36947;~\citep{cuturi2023monge} &#30340;&#33539;&#24335;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#35813;&#33539;&#24335;&#22522;&#20110;&#26367;&#20195;&#30340;&#25104;&#26412;&#20844;&#24335;$c(x,y)=h(x-y)$ &#65292;&#23427;&#20063;&#26159;&#25104;&#26412;&#19981;&#21464;&#30340;&#65292;&#20294;&#37319;&#29992;&#26356;&#19968;&#33324;&#30340;&#24418;&#24335;$h=\tfrac12 \ell_2^2+\tau$&#65292;&#20854;&#20013;$\tau$&#26159;&#36866;&#24403;&#30340;&#20984;&#35268;&#21017;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport theory has provided machine learning with several tools to infer a push-forward map between densities from samples. While this theory has recently seen tremendous methodological developments in machine learning, its practical implementation remains notoriously difficult, because it is plagued by both computational and statistical challenges. Because of such difficulties, existing approaches rarely depart from the default choice of estimating such maps with the simple squared-Euclidean distance as the ground cost, $c(x,y)=\|x-y\|^2_2$. We follow a different path in this work, with the motivation of \emph{learning} a suitable cost structure to encourage maps to transport points along engineered features. We extend the recently proposed Monge-Bregman-Occam pipeline~\citep{cuturi2023monge}, that rests on an alternative cost formulation that is also cost-invariant $c(x,y)=h(x-y)$, but which adopts a more general form as $h=\tfrac12 \ell_2^2+\tau$, where $\tau$ is an approp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27979;&#37327;&#28508;&#22312;&#32467;&#26524;&#22312;&#23384;&#22312;&#25110;&#32570;&#20047;&#27835;&#30103;&#30340;&#24773;&#20917;&#19979;&#30340;&#23614;&#37096;&#34928;&#20943;&#29575;&#21464;&#21270;&#65292;&#26469;&#20272;&#35745;&#26497;&#31471;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65288;ITE$_2$&#65289;&#12290;</title><link>http://arxiv.org/abs/2306.11697</link><description>&lt;p&gt;
&#26497;&#31471;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Individual Treatment Effects in Extreme Regimes. (arXiv:2306.11697v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#27979;&#37327;&#28508;&#22312;&#32467;&#26524;&#22312;&#23384;&#22312;&#25110;&#32570;&#20047;&#27835;&#30103;&#30340;&#24773;&#20917;&#19979;&#30340;&#23614;&#37096;&#34928;&#20943;&#29575;&#21464;&#21270;&#65292;&#26469;&#20272;&#35745;&#26497;&#31471;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65288;ITE$_2$&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#26497;&#31471;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#23545;&#20110;&#25551;&#36848;&#19981;&#21516;&#24178;&#39044;&#31574;&#30053;&#30340;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26497;&#31471;&#29615;&#22659;&#25968;&#25454;&#24456;&#38590;&#25910;&#38598;&#65292;&#22240;&#20026;&#23427;&#22312;&#23454;&#36341;&#20013;&#24456;&#23569;&#34987;&#35266;&#23519;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#20272;&#35745;&#26497;&#31471;&#29615;&#22659;&#19979;&#30340;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65288;ITE$_2$&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#28508;&#22312;&#32467;&#26524;&#22312;&#23384;&#22312;&#25110;&#32570;&#20047;&#27835;&#30103;&#30340;&#24773;&#20917;&#19979;&#30340;&#23614;&#37096;&#34928;&#20943;&#29575;&#21464;&#21270;&#26469;&#37327;&#21270;&#36825;&#31181;&#25928;&#26524;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102; ITE$_2$ &#30340;&#35745;&#31639;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#35745;&#31639;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding individual treatment effects in extreme regimes is important for characterizing risks associated with different interventions. This is hindered by the fact that extreme regime data may be hard to collect, as it is scarcely observed in practice. In addressing this issue, we propose a new framework for estimating the individual treatment effect in extreme regimes (ITE$_2$). Specifically, we quantify this effect by the changes in the tail decay rates of potential outcomes in the presence or absence of the treatment. Subsequently, we establish conditions under which ITE$_2$ may be calculated and develop algorithms for its computation. We demonstrate the efficacy of our proposed method on various synthetic and semi-synthetic datasets.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#20107;&#20808;&#30693;&#36947;&#30495;&#23454;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#25216;&#26415;&#21521;BO&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#25968;&#25454;&#28857;&#65292;&#37319;&#29992;&#26032;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#36229;&#21442;&#25968;&#20272;&#35745;&#65292;&#20197;&#36798;&#21040;&#27425;&#32447;&#24615;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.06844</link><description>&lt;p&gt;
&#20855;&#26377;&#26080;&#20559;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#20272;&#35745;&#30340;&#21487;&#35777;&#26126;&#39640;&#25928;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Bayesian Optimization with Unbiased Gaussian Process Hyperparameter Estimation. (arXiv:2306.06844v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06844
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#24120;&#35265;&#30340;&#25968;&#25454;&#20559;&#24046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#26080;&#38656;&#20107;&#20808;&#30693;&#36947;&#30495;&#23454;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#25216;&#26415;&#21521;BO&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#25968;&#25454;&#28857;&#65292;&#37319;&#29992;&#26032;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#36229;&#21442;&#25968;&#20272;&#35745;&#65292;&#20197;&#36798;&#21040;&#27425;&#32447;&#24615;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#26377;&#25928;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#21644;&#29702;&#35770;&#20445;&#35777;&#65292;&#21462;&#20915;&#20110;&#27491;&#30830;&#20272;&#35745;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#20540;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#24120;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#21487;&#33021;&#20250;&#24341;&#36215;&#25968;&#25454;&#20559;&#24046;&#65292;&#20174;&#32780;&#23548;&#33268;&#36229;&#21442;&#25968;&#20272;&#35745;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#20107;&#20808;&#19981;&#30693;&#36947;&#30495;&#23454;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#24182;&#38656;&#35201;&#20174;&#35266;&#23519;&#25968;&#25454;&#20013;&#36827;&#34892;&#20272;&#35745;&#26102;&#65292;&#35813;&#26041;&#27861;&#20063;&#33021;&#22815;&#27425;&#32447;&#24615;&#25910;&#25947;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#25216;&#26415;(EXP3)&#21521;BO&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#25968;&#25454;&#28857;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#20272;&#35745;&#36807;&#31243;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process (GP) based Bayesian optimization (BO) is a powerful method for optimizing black-box functions efficiently. The practical performance and theoretical guarantees associated with this approach depend on having the correct GP hyperparameter values, which are usually unknown in advance and need to be estimated from the observed data. However, in practice, these estimations could be incorrect due to biased data sampling strategies commonly used in BO. This can lead to degraded performance and break the sub-linear global convergence guarantee of BO. To address this issue, we propose a new BO method that can sub-linearly converge to the global optimum of the objective function even when the true GP hyperparameters are unknown in advance and need to be estimated from the observed data. Our method uses a multi-armed bandit technique (EXP3) to add random data points to the BO process, and employs a novel training loss function for the GP hyperparameter estimation process that ens
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#21442;&#25968;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06213</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#21442;&#25968;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#22810;&#31867;&#20998;&#31867;&#40065;&#26834;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust Twin Parametric Margin Support Vector Machine for Multiclass Classification. (arXiv:2306.06213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#21442;&#25968;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21452;&#21442;&#25968;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;(TPMSVM)&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#12290; &#23545;&#20110;&#27599;&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#23545;&#21106;&#24179;&#38754;&#30340;&#27169;&#24335;&#26500;&#24314;&#19968;&#20010;&#20998;&#31867;&#22120;&#12290;&#19968;&#26086;&#30830;&#23450;&#20102;&#25152;&#26377;&#20998;&#31867;&#22120;&#65292;&#21017;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#19968;&#20010;&#32508;&#21512;&#30340;&#20915;&#31574;&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20869;&#26680;&#24341;&#36215;&#30340;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#22686;&#24378;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290; &#21021;&#27493;&#30340;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a Twin Parametric-Margin Support Vector Machine (TPMSVM) model to tackle the problem of multiclass classification. In the spirit of one-versus-all paradigm, for each class we construct a classifier by solving a TPMSVM-type model. Once all classifiers have been determined, they are combined into an aggregate decision function. We consider the cases of both linear and nonlinear kernel-induced classifiers. In addition, we robustify the proposed approach through robust optimization techniques. Indeed, in real-world applications observations are subject to measurement errors and noise, affecting the quality of the solutions. Consequently, data uncertainties need to be included within the model in order to prevent low accuracies in the classification process. Preliminary computational experiments on real-world datasets show the good performance of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.06081</link><description>&lt;p&gt;
CARSO: &#23545;&#25239;&#24615;&#21512;&#25104;&#35266;&#27979;&#30340;&#21453;&#23545;&#25239;&#24615;&#21484;&#22238;
&lt;/p&gt;
&lt;p&gt;
CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#28789;&#24863;&#26469;&#33258;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#32447;&#32034;&#12290;&#35813;&#26041;&#27861;&#19982;&#23545;&#25239;&#35757;&#32451;&#20855;&#26377;&#21327;&#21516;&#20114;&#34917;&#24615;&#65292;&#24182;&#20381;&#36182;&#20110;&#34987;&#25915;&#20987;&#20998;&#31867;&#22120;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#35813;&#26041;&#27861;&#37319;&#26679;&#36755;&#20837;&#30340;&#37325;&#26500;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#12290;&#22312;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#20307;&#31995;&#32467;&#26500;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CARSO&#33021;&#22815;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#8212;&#8212;&#21516;&#26102;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#28165;&#27905;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#38450;&#24481;&#20307;&#31995;&#32467;&#26500;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DeepfakeArt Challenge&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#24110;&#21161;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#36827;&#34892;&#29983;&#25104;AI&#33402;&#26415;&#20266;&#36896;&#21644;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#25361;&#25112;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.01272</link><description>&lt;p&gt;
DeepfakeArt Challenge: &#29992;&#20110;&#29983;&#25104;AI&#33402;&#26415;&#20266;&#36896;&#21644;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery and Data Poisoning Detection. (arXiv:2306.01272v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DeepfakeArt Challenge&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#24110;&#21161;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#36827;&#34892;&#29983;&#25104;AI&#33402;&#26415;&#20266;&#36896;&#21644;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#22823;&#35268;&#27169;&#25361;&#25112;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24040;&#22823;&#36827;&#27493;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24102;&#26469;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#21644;&#21069;&#26223;&#65292;&#33539;&#22260;&#20174;&#20250;&#35805;&#20195;&#29702;&#21644;&#25991;&#26412;&#20869;&#23481;&#29983;&#25104;&#21040;&#35821;&#38899;&#21644;&#35270;&#35273;&#21512;&#25104;&#12290;&#22312;&#29983;&#25104;AI&#30340;&#23835;&#36215;&#21644;&#20854;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#37319;&#29992;&#20013;&#65292;&#23545;&#20110;&#29983;&#25104;AI&#30340;&#24694;&#24847;&#29992;&#36884;&#23384;&#22312;&#30528;&#26174;&#30528;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#20851;&#27880;&#12290;&#22312;&#20351;&#29992;&#29983;&#25104;AI&#36827;&#34892;&#35270;&#35273;&#20869;&#23481;&#21512;&#25104;&#30340;&#39046;&#22495;&#20013;&#65292;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#26159;&#22270;&#20687;&#20266;&#36896;&#65288;&#20363;&#22914;&#65292;&#29983;&#25104;&#21253;&#21547;&#25110;&#27966;&#29983;&#33258;&#29256;&#26435;&#20869;&#23481;&#30340;&#22270;&#20687;&#65289;&#21644;&#25968;&#25454;&#27745;&#26579;&#65288;&#21363;&#29983;&#25104;&#34987;&#25932;&#23545;&#27745;&#26579;&#30340;&#22270;&#20687;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20851;&#38190;&#38382;&#39064;&#65292;&#40723;&#21169;&#36127;&#36131;&#20219;&#30340;&#29983;&#25104;AI&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;DeepfakeArt Challenge&#65292;&#19968;&#20010;&#22823;&#22411;&#25361;&#25112;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24110;&#21161;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20197;&#36827;&#34892;&#29983;&#25104;AI&#33402;&#26415;&#20266;&#36896;&#21644;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tremendous recent advances in generative artificial intelligence techniques have led to significant successes and promise in a wide range of different applications ranging from conversational agents and textual content generation to voice and visual synthesis. Amid the rise in generative AI and its increasing widespread adoption, there has been significant growing concern over the use of generative AI for malicious purposes. In the realm of visual content synthesis using generative AI, key areas of significant concern has been image forgery (e.g., generation of images containing or derived from copyright content), and data poisoning (i.e., generation of adversarially contaminated images). Motivated to address these key concerns to encourage responsible generative AI, we introduce the DeepfakeArt Challenge, a large-scale challenge benchmark dataset designed specifically to aid in the building of machine learning algorithms for generative AI art forgery and data poisoning detection. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#20013;&#26159;&#21542;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#27880;&#24847;&#21147;&#26377;&#25152;&#19981;&#21516;&#65292;&#32467;&#26524;&#21457;&#29616;&#20182;&#20204;&#20043;&#38388;&#23384;&#22312;&#19968;&#33268;&#24615;&#20559;&#24046;&#12290;&#20316;&#32773;&#36890;&#36807;&#37327;&#21270;&#23454;&#39564;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#30830;&#35748;&#20102;&#25200;&#21160;&#26041;&#27861;&#35745;&#31639;&#30340;&#27880;&#24847;&#21147;&#26368;&#25509;&#36817;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#27880;&#24847;&#21147;&#65292;&#24182;&#19988;&#36825;&#31181;LLMs&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#33021;&#21147;&#21644;&#31243;&#24207;&#21592;&#20449;&#20219;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01220</link><description>&lt;p&gt;
&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;&#21542;&#19982;&#20154;&#31867;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#19968;&#33268;&#65311;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is Model Attention Aligned with Human Attention? An Empirical Study on Large Language Models for Code Generation. (arXiv:2306.01220v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#20013;&#26159;&#21542;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#27880;&#24847;&#21147;&#26377;&#25152;&#19981;&#21516;&#65292;&#32467;&#26524;&#21457;&#29616;&#20182;&#20204;&#20043;&#38388;&#23384;&#22312;&#19968;&#33268;&#24615;&#20559;&#24046;&#12290;&#20316;&#32773;&#36890;&#36807;&#37327;&#21270;&#23454;&#39564;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#30830;&#35748;&#20102;&#25200;&#21160;&#26041;&#27861;&#35745;&#31639;&#30340;&#27880;&#24847;&#21147;&#26368;&#25509;&#36817;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#27880;&#24847;&#21147;&#65292;&#24182;&#19988;&#36825;&#31181;LLMs&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#33021;&#21147;&#21644;&#31243;&#24207;&#21592;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#20195;&#30721;&#29983;&#25104;&#38750;&#24120;&#26377;&#25928;&#12290;&#30001;&#20110;LLMs&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#36879;&#26126;&#24615;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29983;&#25104;&#20195;&#30721;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#20013;&#26159;&#21542;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30456;&#21516;&#22320;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#30340;&#26576;&#20123;&#37096;&#20998;&#12290;&#36890;&#36807;&#23545;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;HumanEval&#19978;&#30340;&#20116;&#20010;LLMs&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;LLMs&#30340;&#27880;&#24847;&#21147;&#19982;&#31243;&#24207;&#21592;&#30340;&#27880;&#24847;&#21147;&#23384;&#22312;&#19968;&#33268;&#24615;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#20195;&#30721;&#29983;&#25104;&#20934;&#30830;&#24615;&#19982;&#23427;&#20204;&#19982;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#31243;&#24230;&#20043;&#38388;&#27809;&#26377;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#23454;&#39564;&#21644;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#30830;&#35748;&#65292;&#22312;12&#31181;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#35745;&#31639;&#30340;&#27880;&#24847;&#21147;&#26368;&#25509;&#36817;&#20154;&#31867;&#27880;&#24847;&#21147;&#65292;&#24182;&#19988;&#22987;&#32456;&#21463;&#21040;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#38738;&#30544;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#38656;&#35201;&#20154;&#31867;&#23545;&#40784;&#30340;LLMs&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31243;&#24207;&#21592;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been demonstrated effective for code generation. Due to the complexity and opacity of LLMs, little is known about how these models generate code. To deepen our understanding, we investigate whether LLMs attend to the same parts of a natural language description as human programmers during code generation. An analysis of five LLMs on a popular benchmark, HumanEval, revealed a consistent misalignment between LLMs' and programmers' attention. Furthermore, we found that there is no correlation between the code generation accuracy of LLMs and their alignment with human programmers. Through a quantitative experiment and a user study, we confirmed that, among twelve different attention computation methods, attention computed by the perturbation-based method is most aligned with human attention and is constantly favored by human programmers. Our findings highlight the need for human-aligned LLMs for better interpretability and programmer trust.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#32447;&#24615;Bandit&#30340;Pareto&#21069;&#27839;&#35782;&#21035;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25152;&#26377;&#21160;&#20316;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#21516;&#26102;&#20445;&#35777;&#20102;Pareto&#21069;&#27839;&#35782;&#21035;&#21644;Pareto&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.00096</link><description>&lt;p&gt;
&#36890;&#36807;&#36951;&#25022;&#26368;&#23567;&#21270;&#26041;&#27861;&#36827;&#34892;Pareto&#21069;&#27839;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Pareto Front Identification with Regret Minimization. (arXiv:2306.00096v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#32447;&#24615;Bandit&#30340;Pareto&#21069;&#27839;&#35782;&#21035;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25152;&#26377;&#21160;&#20316;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#21516;&#26102;&#20445;&#35777;&#20102;Pareto&#21069;&#27839;&#35782;&#21035;&#21644;Pareto&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#32447;&#24615;Bandit&#24773;&#20917;&#19979;&#30340;Pareto&#21069;&#27839;&#35782;&#21035;&#65288;PFILin&#65289;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#24179;&#22343;&#22870;&#21169;&#21521;&#37327;&#20316;&#20026;&#29615;&#22659;&#30340;&#32447;&#24615;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#19968;&#32452;&#22870;&#21169;&#21521;&#37327;&#19981;&#34987;&#20854;&#20182;&#20219;&#20309;&#21521;&#37327;&#25152;&#21344;&#20248;&#12290;PFILin&#21253;&#25324;&#26368;&#20339;&#21160;&#20316;&#35782;&#21035;&#21644;&#22810;&#30446;&#26631;&#20027;&#21160;&#23398;&#20064;&#31561;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\tilde{O}(d/\Delta^2)$&#65292;&#20854;&#20013;$d$&#26159;&#19978;&#19979;&#25991;&#30340;&#32500;&#25968;&#65292;$\Delta$&#26159;&#38382;&#39064;&#22797;&#26434;&#24615;&#30340;&#19968;&#31181;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#23545;&#25968;&#22240;&#23376;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;&#26412;&#31639;&#27861;&#30340;&#19968;&#20010;&#26032;&#29305;&#28857;&#26159;&#20351;&#29992;&#25152;&#26377;&#21160;&#20316;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#38500;&#20102;&#26377;&#25928;&#22320;&#35782;&#21035;Pareto&#21069;&#27839;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#20445;&#35777;&#65292;&#22312;&#26679;&#26412;&#25968;&#22823;&#20110;$\Omega(d\log dL)$&#26102;&#65292;&#23545;&#20110;$L$&#32500;&#30690;&#37327;&#22870;&#21169;&#65292;&#30636;&#26102;Pareto&#36951;&#25022;&#30340;$\tilde{O}(\sqrt{d/t})$&#30028;&#38480;&#12290;&#36890;&#36807;&#20351;&#29992;&#25152;&#26377;&#21160;&#20316;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21516;&#26102;&#20026;&#32447;&#24615;Bandit&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;Pareto&#21069;&#27839;&#35782;&#21035;&#21644;Pareto&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider Pareto front identification for linear bandits (PFILin) where the goal is to identify a set of arms whose reward vectors are not dominated by any of the others when the mean reward vector is a linear function of the context. PFILin includes the best arm identification problem and multi-objective active learning as special cases. The sample complexity of our proposed algorithm is $\tilde{O}(d/\Delta^2)$, where $d$ is the dimension of contexts and $\Delta$ is a measure of problem complexity. Our sample complexity is optimal up to a logarithmic factor. A novel feature of our algorithm is that it uses the contexts of all actions. In addition to efficiently identifying the Pareto front, our algorithm also guarantees $\tilde{O}(\sqrt{d/t})$ bound for instantaneous Pareto regret when the number of samples is larger than $\Omega(d\log dL)$ for $L$ dimensional vector rewards. By using the contexts of all arms, our proposed algorithm simultaneously provides efficient Pareto front ide
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312; mini-batch &#20013;&#26174;&#24335;&#22320;&#23398;&#20064;&#35823;&#24046;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.17028</link><description>&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26356;&#22909;Batch&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Better Batch for Deep Probabilistic Time Series Forecasting. (arXiv:2305.17028v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312; mini-batch &#20013;&#26174;&#24335;&#22320;&#23398;&#20064;&#35823;&#24046;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#65292;&#26469;&#25552;&#39640;&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22240;&#20854;&#33021;&#22815;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#27169;&#22411;&#36807;&#20110;&#31616;&#21333;&#21270;&#38382;&#39064;&#65292;&#20551;&#35774;&#35823;&#24046;&#36807;&#31243;&#26159;&#19982;&#26102;&#38388;&#26080;&#20851;&#30340;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#35823;&#24046;&#36807;&#31243;&#20013;&#30340;&#24207;&#21015;&#30456;&#20851;&#24615;&#12290;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#36825;&#20123;&#27169;&#22411;&#23545;&#20915;&#31574;&#24615;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#20943;&#24369;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23558;&#35823;&#24046;&#33258;&#30456;&#20851;&#24615;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#22686;&#24378;&#27010;&#29575;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#36896;&#19968;&#20010;mini-batch&#65292;&#20316;&#20026;$D$&#20010;&#36830;&#32493;&#26102;&#38388;&#24207;&#21015;&#27573;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#26174;&#24335;&#22320;&#23398;&#20064;&#19968;&#20010;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#35206;&#30422;&#20102;&#30456;&#37051;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#35823;&#24046;&#30456;&#20851;&#24615;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#21487;&#29992;&#20110;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep probabilistic time series forecasting has gained significant attention due to its ability to provide valuable uncertainty quantification for decision-making tasks. However, many existing models oversimplify the problem by assuming the error process is time-independent, thereby overlooking the serial correlation in the error process. This oversight can potentially diminish the accuracy of the forecasts, rendering these models less effective for decision-making purposes. To overcome this limitation, we propose an innovative training method that incorporates error autocorrelation to enhance the accuracy of probabilistic forecasting. Our method involves constructing a mini-batch as a collection of $D$ consecutive time series segments for model training and explicitly learning a covariance matrix over each mini-batch that encodes the error correlation among adjacent time steps. The resulting covariance matrix can be used to improve prediction accuracy and enhance uncertainty quantifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ReLU&#21333;&#20803;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15912</link><description>&lt;p&gt;
&#25913;&#36827;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#30340;&#31070;&#32463;&#29305;&#24449;&#28608;&#27963;&#20540;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning. (arXiv:2305.15912v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ReLU&#21333;&#20803;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#20960;&#20309;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#30340;&#35268;&#33539;&#21270;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;ReLU&#32593;&#32476;&#29305;&#24449;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20248;&#21270;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#21333;&#20010;ReLU&#21333;&#20803;&#30340;&#29305;&#24449;&#28608;&#27963;&#20540;&#12290;&#25105;&#20204;&#23558;ReLU&#21333;&#20803;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#23545;&#24212;&#30340;&#29305;&#24449;&#28608;&#27963;&#20540;&#38598;&#21512;&#31216;&#20026;ReLU&#21333;&#20803;&#30340;&#29305;&#24449;&#28608;&#27963;&#38598;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29305;&#24449;&#28608;&#27963;&#38598;&#19982;ReLU&#32593;&#32476;&#20013;&#23398;&#20064;&#29305;&#24449;&#20043;&#38388;&#30340;&#26126;&#30830;&#32852;&#31995;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#25216;&#26415;&#22914;&#20309;&#35268;&#33539;&#21270;&#21644;&#31283;&#23450;SGD&#20248;&#21270;&#12290;&#21033;&#29992;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#26041;&#27861;&#26469;&#21442;&#25968;&#21270;ReLU&#32593;&#32476;&#20197;&#25913;&#36827;&#29305;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#20854;&#26377;&#29992;&#24615;&#65292;&#20351;&#29992;&#20102;&#19981;&#37027;&#20040;&#31934;&#24515;&#36873;&#25321;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#21644;&#26356;&#22823;&#30340;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#26356;&#22909;&#30340;&#20248;&#21270;&#31283;&#23450;&#24615;&#65292;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21450;&#22522;&#20110;&#29983;&#25104;&#36807;&#31243;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31867;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#29305;&#24449;&#38598;&#21512;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#21050;&#28608;&#19979;&#65292;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14383</link><description>&lt;p&gt;
&#19968;&#20010;&#38477;&#32500;&#20154;&#31867;&#20998;&#31867;&#30340;&#29702;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Rational Model of Dimension-reduced Human Categorization. (arXiv:2305.14383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14383
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#20197;&#21450;&#22522;&#20110;&#29983;&#25104;&#36807;&#31243;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#30340;&#20998;&#31867;&#35299;&#37322;&#65292;&#35813;&#27169;&#22411;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#29305;&#24449;&#38598;&#21512;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#21050;&#28608;&#19979;&#65292;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#20013;&#29616;&#26377;&#30340;&#27169;&#22411;&#36890;&#24120;&#20551;&#35774;&#20154;&#31867;&#22312;&#24515;&#29702;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#32423;&#27010;&#25324;&#34892;&#20026;&#65292;&#20294;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#31867;&#21035;&#34920;&#31034;&#21487;&#33021;&#20250;&#21463;&#21040;&#32500;&#24230;&#35781;&#21650;&#30340;&#24433;&#21709;&#12290;&#20154;&#20204;&#19968;&#33324;&#20381;&#36182;&#20110;&#19968;&#32452;&#21487;&#34892;&#20294;&#36275;&#22815;&#30340;&#29305;&#24449;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#21270;&#27010;&#29575;&#20027;&#25104;&#20998;&#28151;&#21512;&#27169;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#31867;&#21035;&#34920;&#31034;&#21644;&#32463;&#27982;&#30340;&#29305;&#24449;&#38598;&#21512;&#12290;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#20154;&#31867;&#20998;&#31867;&#20013;&#30340;&#32500;&#24230;&#20559;&#24046;&#24182;&#25903;&#25345;&#38646;-shot&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20869;&#21033;&#29992;&#29983;&#25104;&#36807;&#31243;&#65292;&#25552;&#20379;&#39640;&#32500;&#21050;&#28608;&#19979;&#26356;&#22909;&#30340;&#20998;&#31867;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing models in cognitive science typically assume human categorization as graded generalization behavior in a multidimensional psychological space. However, category representations in these models may suffer from the curse of dimensionality in a natural setting. People generally rely on a tractable yet sufficient set of features to understand the complex environment. We propose a rational model of categorization based on a hierarchical mixture of probabilistic principal components, that simultaneously learn category representations and an economical collection of features. The model captures dimensional biases in human categorization and supports zero-shot learning. We further exploit a generative process within a low-dimensional latent space to provide a better account of categorization with high-dimensional stimuli. We validate the model with simulation and behavioral experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33609;&#22270;&#21644;&#25237;&#24433;&#30340;Newton&#26041;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#65292;&#36866;&#29992;&#20110;&#33258;&#20849;&#36717;&#20989;&#25968;&#65292;&#20855;&#26377;&#33609;&#22270;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20302;&#36845;&#20195;&#25104;&#26412;&#65292;&#20840;&#31209;Newton&#31867;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#20840;&#23616;&#25910;&#25947;&#29575;&#20197;&#21450;&#38459;&#23612;Newton&#26041;&#27861;&#30340;&#31639;&#27861;&#31616;&#21333;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13082</link><description>&lt;p&gt;
Sketch-and-Project Meets Newton Method: &#20855;&#26377;&#20302;&#31209;&#26356;&#26032;&#30340;&#20840;&#23616;$\mathcal O(k^{-2})$ &#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sketch-and-Project Meets Newton Method: Global $\mathcal O(k^{-2})$ Convergence with Low-Rank Updates. (arXiv:2305.13082v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33609;&#22270;&#21644;&#25237;&#24433;&#30340;Newton&#26041;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#65292;&#36866;&#29992;&#20110;&#33258;&#20849;&#36717;&#20989;&#25968;&#65292;&#20855;&#26377;&#33609;&#22270;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20302;&#36845;&#20195;&#25104;&#26412;&#65292;&#20840;&#31209;Newton&#31867;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#20840;&#23616;&#25910;&#25947;&#29575;&#20197;&#21450;&#38459;&#23612;Newton&#26041;&#27861;&#30340;&#31639;&#27861;&#31616;&#21333;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33609;&#22270;&#21644;&#25237;&#24433;&#30340;Newton&#26041;&#27861;&#65292;&#20855;&#26377;&#24555;&#36895;&#30340;$\mathcal O(k^{-2})$ &#20840;&#23616;&#25910;&#25947;&#29575;&#65292;&#36866;&#29992;&#20110;&#33258;&#20849;&#36717;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20174;&#19977;&#20010;&#26041;&#38754;&#26469;&#30475;&#24453;&#65306;i) &#20316;&#20026;&#19968;&#20010;&#33609;&#22270;&#21644;&#25237;&#24433;&#31639;&#27861;&#65292;&#23545;Newton&#26041;&#27861;&#30340;&#26356;&#26032;&#36827;&#34892;&#25237;&#24433;&#65292;ii) &#20316;&#20026;&#22312;&#34987;&#33609;&#22270;&#21270;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#31435;&#26041;&#27491;&#21017;&#21270;&#30340;Newton&#26041;&#27861;&#65292;&#21644; iii) &#20316;&#20026;&#22312;&#34987;&#33609;&#22270;&#21270;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#38459;&#23612;Newton&#26041;&#27861;&#12290;SGN&#32487;&#25215;&#20102;&#36825;&#19977;&#20010;&#26041;&#38754;&#30340;&#20248;&#28857;&#65306;&#33609;&#22270;&#21644;&#25237;&#24433;&#26041;&#27861;&#30340;&#20302;&#36845;&#20195;&#25104;&#26412;&#65292;&#20840;&#31209;Newton&#31867;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;$\mathcal O(k^{-2})$&#20840;&#23616;&#25910;&#25947;&#29575;&#20197;&#21450;&#38459;&#23612;Newton&#26041;&#27861;&#30340;&#31639;&#27861;&#31616;&#21333;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#19982;&#22522;&#20934;&#31639;&#27861;&#20855;&#26377;&#30456;&#24403;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the first sketch-and-project Newton method with fast $\mathcal O(k^{-2})$ global convergence rate for self-concordant functions. Our method, SGN, can be viewed in three ways: i) as a sketch-and-project algorithm projecting updates of Newton method, ii) as a cubically regularized Newton ethod in sketched subspaces, and iii) as a damped Newton method in sketched subspaces. SGN inherits best of all three worlds: cheap iteration costs of sketch-and-project methods, state-of-the-art $\mathcal O(k^{-2})$ global convergence rate of full-rank Newton-like methods and the algorithm simplicity of damped Newton methods. Finally, we demonstrate its comparable empirical performance to baseline algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#29992;&#36924;&#36817;&#30340;&#35789;&#27719;&#65292;&#35777;&#26126;&#20102;&#26377;&#38480;&#8220;&#35789;&#27719;&#8221;&#23384;&#22312;&#24182;&#21487;&#29992;&#20110;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#26144;&#23556;$f$&#21644;&#32039;&#33268;&#22495;$\Omega$&#20013;&#30340;&#27599;&#20010;&#28857;&#65292;&#35823;&#24046;&#23567;&#20110;$\varepsilon$&#12290;</title><link>http://arxiv.org/abs/2305.12205</link><description>&lt;p&gt;
&#36890;&#29992;&#36924;&#36817;&#30340;&#35789;&#27719;&#65306;&#19968;&#31181;&#23558;&#26144;&#23556;&#32452;&#21512;&#30475;&#20316;&#35821;&#35328;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Vocabulary for Universal Approximation: A Linguistic Perspective of Mapping Compositions. (arXiv:2305.12205v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#29992;&#36924;&#36817;&#30340;&#35789;&#27719;&#65292;&#35777;&#26126;&#20102;&#26377;&#38480;&#8220;&#35789;&#27719;&#8221;&#23384;&#22312;&#24182;&#21487;&#29992;&#20110;&#36924;&#36817;&#20219;&#20309;&#36830;&#32493;&#26144;&#23556;$f$&#21644;&#32039;&#33268;&#22495;$\Omega$&#20013;&#30340;&#27599;&#20010;&#28857;&#65292;&#35823;&#24046;&#23567;&#20110;$\varepsilon$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24207;&#21015;&#24314;&#27169;&#65292;&#22914;&#35821;&#35328;&#27169;&#22411;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#21644;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#36825;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#23558;&#38750;&#36830;&#32493;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#24418;&#24335;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#27839;&#30528;&#36825;&#20010;&#24605;&#36335;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#26144;&#23556;&#20989;&#25968;&#30340;&#32452;&#21512;&#65292;&#20854;&#20013;&#27599;&#20010;&#32452;&#21512;&#21487;&#35270;&#20026;&#19968;&#20010;&#8220;&#21333;&#35789;&#8221;&#12290;&#28982;&#32780;&#65292;&#32447;&#24615;&#26144;&#23556;&#30340;&#26435;&#37325;&#26159;&#26410;&#30830;&#23450;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#26080;&#38480;&#25968;&#37327;&#30340;&#21333;&#35789;&#12290;&#26412;&#25991;&#30740;&#31350;&#26377;&#38480;&#24773;&#20917;&#65292;&#26500;&#24314;&#24615;&#22320;&#35777;&#26126;&#20102;&#36890;&#29992;&#36924;&#36817;&#30340;&#26377;&#38480;&#8220;&#35789;&#27719;&#8221;$V=\{\phi_i: \mathbb{R}^d \to \mathbb{R}^d | i=1,...,n\}$&#23384;&#22312;&#65292;&#20854;&#20013;$n = O(d^2)$&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#23545;&#20110;&#20219;&#20309;&#36830;&#32493;&#26144;&#23556;$f: \mathbb{R}^d \to \mathbb{R}^d$&#12289;&#32039;&#33268;&#22495;$\Omega$&#21644;$\varepsilon&gt;0$&#65292;&#23384;&#22312;&#26144;&#23556;&#24207;&#21015;$\phi_{i_1}, ..., \phi_{i_m} \in V, m \in \mathbb{Z}_+$&#65292;&#20351;&#24471;&#32452;&#21512;$\phi_{i_m}$&#33021;&#22815;&#36924;&#36817;$f$&#21644;$\Omega$&#20013;&#30340;&#27599;&#20010;&#28857;&#65292;&#19988;&#35823;&#24046;&#23567;&#20110;$\varepsilon$&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning-based sequence modelings, such as language models, have received much attention and success, which pushes researchers to explore the possibility of transforming non-sequential problems into a sequential form. Following this thought, deep neural networks can be represented as composite functions of a sequence of mappings, linear or nonlinear, where each composition can be viewed as a \emph{word}. However, the weights of linear mappings are undetermined and hence require an infinite number of words. In this article, we investigate the finite case and constructively prove the existence of a finite \emph{vocabulary} $V=\{\phi_i: \mathbb{R}^d \to \mathbb{R}^d | i=1,...,n\}$ with $n=O(d^2)$ for the universal approximation. That is, for any continuous mapping $f: \mathbb{R}^d \to \mathbb{R}^d$, compact domain $\Omega$ and $\varepsilon&gt;0$, there is a sequence of mappings $\phi_{i_1}, ..., \phi_{i_m} \in V, m \in \mathbb{Z}_+$, such that the composition $\phi_{i_m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;YOLOv8&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#23454;&#26102;&#26816;&#27979;&#30340;&#39134;&#34892;&#29289;&#20307;&#65307;&#36890;&#36807;&#36827;&#19968;&#27493;&#35757;&#32451;&#65292;&#29983;&#25104;&#31934;&#32454;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#22312;&#30495;&#23454;&#29615;&#22659;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09972</link><description>&lt;p&gt;
&#22522;&#20110;YOLOv8&#30340;&#23454;&#26102;&#39134;&#34892;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Real-Time Flying Object Detection with YOLOv8. (arXiv:2305.09972v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;YOLOv8&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#23454;&#26102;&#26816;&#27979;&#30340;&#39134;&#34892;&#29289;&#20307;&#65307;&#36890;&#36807;&#36827;&#19968;&#27493;&#35757;&#32451;&#65292;&#29983;&#25104;&#31934;&#32454;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#22312;&#30495;&#23454;&#29615;&#22659;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#39134;&#34892;&#29289;&#20307;&#65292;&#21487;&#29992;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20379;&#23454;&#26045;&#30340;&#31934;&#32454;&#27169;&#22411;&#12290;&#25105;&#20204;&#20808;&#20351;&#29992;&#21253;&#21547;40&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#23545;&#36890;&#29992;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24378;&#21046;&#27169;&#22411;&#25552;&#21462;&#25277;&#35937;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#23398;&#20064;&#21040;&#30340;&#21442;&#25968;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#65292;&#20197;&#22312;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#30495;&#23454;&#29615;&#22659;&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#25105;&#20204;&#30340;&#31934;&#32454;&#27169;&#22411;&#12290;&#30001;&#20110;&#39134;&#34892;&#29289;&#20307;&#30340;&#29289;&#20307;&#31354;&#38388;&#22823;&#23567;/&#32437;&#27178;&#27604;&#12289;&#36895;&#24230;&#12289;&#36974;&#25377;&#21644;&#32972;&#26223;&#30340;&#24046;&#24322;&#24456;&#22823;&#65292;&#22240;&#27492;&#39134;&#34892;&#29289;&#20307;&#30340;&#26816;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#26032;&#30340;&#21333;&#27425;&#26816;&#27979;&#22120;YOLOv8&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#24615;&#33021;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a generalized model for real-time detection of flying objects that can be used for transfer learning and further research, as well as a refined model that is ready for implementation. We achieve this by training our first generalized model on a data set containing 40 different classes of flying objects, forcing the model to extract abstract feature representations. We then perform transfer learning with these learned parameters on a data set more representative of real world environments (i.e., higher frequency of occlusion, small spatial sizes, rotations, etc.) to generate our refined model. Object detection of flying objects remains challenging due to large variance object spatial sizes/aspect ratios, rate of speed, occlusion, and clustered backgrounds. To address some of the presented challenges while simultaneously maximizing performance, we utilize the current state of the art single-shot detector, YOLOv8, in an attempt to find the best tradeoff between inferen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65288;CEPIA&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#33719;&#24471;&#26377;&#25928;&#30340;&#34892;&#21160;&#24314;&#35758;&#24182;&#20102;&#35299;&#32570;&#22833;&#20540;&#23545;&#24314;&#35758;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.14606</link><description>&lt;p&gt;
&#32570;&#22833;&#20540;&#19979;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation with Missing Values. (arXiv:2304.14606v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65288;CEPIA&#65289;&#65292;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#33719;&#24471;&#26377;&#25928;&#30340;&#34892;&#21160;&#24314;&#35758;&#24182;&#20102;&#35299;&#32570;&#22833;&#20540;&#23545;&#24314;&#35758;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#19968;&#31181;&#25552;&#20379;&#25200;&#21160;&#20197;&#25913;&#21464;&#20998;&#31867;&#22120;&#39044;&#27979;&#32467;&#26524;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#12290;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#23436;&#25972;&#20449;&#24687;&#30340;&#36755;&#20837;&#65292;&#20294;&#23454;&#38469;&#24773;&#20917;&#20013;&#24448;&#24448;&#20250;&#26377;&#32570;&#22833;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;CE&#26694;&#26550;&#65288;&#31216;&#20026;CEPIA&#65289;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#22312;&#26377;&#32570;&#22833;&#20540;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26377;&#25928;&#30340;&#25805;&#20316;&#65292;&#24182;&#38416;&#26126;&#32570;&#22833;&#20540;&#23545;&#25805;&#20316;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation (CE) is a post-hoc explanation method that provides a perturbation for altering the prediction result of a classifier. Users can interpret the perturbation as an "action" to obtain their desired decision results. Existing CE methods require complete information on the features of an input instance. However, we often encounter missing values in a given instance, and the previous methods do not work in such a practical situation. In this paper, we first empirically and theoretically show the risk that missing value imputation methods affect the validity of an action, as well as the features that the action suggests changing. Then, we propose a new framework of CE, named Counterfactual Explanation by Pairs of Imputation and Action (CEPIA), that enables users to obtain valid actions even with missing values and clarifies how actions are affected by imputation of the missing values. Specifically, our CEPIA provides a representative set of pairs of an imputation ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31867;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#26080;&#26465;&#20214;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#24046;&#20998;&#38544;&#31169;&#12290;&#35813;&#26426;&#21046;&#23558;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#26080;&#38480;&#32500;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12681</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy via Distributionally Robust Optimization. (arXiv:2304.12681v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31867;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#26080;&#26465;&#20214;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#24046;&#20998;&#38544;&#31169;&#12290;&#35813;&#26426;&#21046;&#23558;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#26080;&#38480;&#32500;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#25104;&#20026;&#20849;&#20139;&#25968;&#25454;&#38598;&#32479;&#35745;&#20449;&#24687;&#24182;&#38480;&#21046;&#28041;&#21450;&#20010;&#20154;&#30340;&#31169;&#20154;&#20449;&#24687;&#25259;&#38706;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#36890;&#36807;&#23545;&#23558;&#35201;&#21457;&#24067;&#30340;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#25200;&#21160;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#36825;&#21453;&#36807;&#26469;&#23548;&#33268;&#20102;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#26356;&#22823;&#30340;&#25200;&#21160;&#25552;&#20379;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#20294;&#32467;&#26524;&#26159;&#25552;&#20379;&#36739;&#20302;&#23454;&#29992;&#24230;&#30340;&#32479;&#35745;&#25968;&#25454;&#21644;&#26356;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#26159;&#22312;&#39044;&#36873;&#38544;&#31169;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26368;&#39640;&#20934;&#30830;&#24615;&#30340;&#26368;&#20339;&#26426;&#21046;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#20107;&#20808;&#25351;&#23450;&#25200;&#21160;&#26063;&#24182;&#38543;&#21518;&#35777;&#26126;&#20854;&#28176;&#36817;&#21644;/&#25110;&#26368;&#20339;&#24615;&#19978;&#65292;&#26412;&#25991;&#21017;&#24320;&#21457;&#20102;&#19968;&#31867;&#26426;&#21046;&#65292;&#23427;&#20204;&#20855;&#26377;&#38750;&#28176;&#36817;&#21644;&#26080;&#26465;&#20214;&#30340;&#26368;&#20339;&#24615;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#26080;&#38480;&#32500;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, differential privacy has emerged as the de facto standard for sharing statistics of datasets while limiting the disclosure of private information about the involved individuals. This is achieved by randomly perturbing the statistics to be published, which in turn leads to a privacy-accuracy trade-off: larger perturbations provide stronger privacy guarantees, but they result in less accurate statistics that offer lower utility to the recipients. Of particular interest are therefore optimal mechanisms that provide the highest accuracy for a pre-selected level of privacy. To date, work in this area has focused on specifying families of perturbations a priori and subsequently proving their asymptotic and/or best-in-class optimality. In this paper, we develop a class of mechanisms that enjoy non-asymptotic and unconditional optimality guarantees. To this end, we formulate the mechanism design problem as an infinite-dimensional distributionally robust optimization problem. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#20851;&#22870;&#21169;&#25506;&#32034;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#20445;&#35777;&#26679;&#26412;&#25910;&#38598;&#25968;&#37327;&#28385;&#36275;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#21457;&#29616;&#25152;&#26377;&#32473;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#25506;&#32034;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.07278</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#26080;&#20851;&#22870;&#21169;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning. (arXiv:2304.07278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#20851;&#22870;&#21169;&#25506;&#32034;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#20445;&#35777;&#26679;&#26412;&#25910;&#38598;&#25968;&#37327;&#28385;&#36275;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#21457;&#29616;&#25152;&#26377;&#32473;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#25506;&#32034;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#20851;&#22870;&#21169;&#25506;&#32034;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#25913;&#36827;&#29616;&#26377;&#25216;&#26415;&#12290;&#30740;&#31350;&#20102;&#20855;&#26377;S&#20010;&#29366;&#24577;&#65292;A&#20010;&#21160;&#20316;&#21644;&#26377;&#38480;&#26102;&#38388;&#27700;&#24179;H&#30340;&#38750;&#24179;&#31283;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#23450;&#25968;&#37327;&#30340;&#26080;&#24341;&#23548;&#22870;&#21169;&#20449;&#24687;&#30340;&#26679;&#26412;&#38598;&#65292;&#22312;&#20445;&#35777;&#25910;&#38598;&#30340;&#25968;&#37327;&#28385;&#36275;&#22810;&#39033;&#24335;&#32423;&#21035;&#26102;&#65292;&#31639;&#27861;&#33021;&#22815;&#21457;&#29616;&#25152;&#26377;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#25506;&#32034;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies reward-agnostic exploration in reinforcement learning (RL) -- a scenario where the learner is unware of the reward functions during the exploration stage -- and designs an algorithm that improves over the state of the art. More precisely, consider a finite-horizon non-stationary Markov decision process with $S$ states, $A$ actions, and horizon length $H$, and suppose that there are no more than a polynomial number of given reward functions of interest. By collecting an order of \begin{align*}  \frac{SAH^3}{\varepsilon^2} \text{ sample episodes (up to log factor)} \end{align*} without guidance of the reward information, our algorithm is able to find $\varepsilon$-optimal policies for all these reward functions, provided that $\varepsilon$ is sufficiently small. This forms the first reward-agnostic exploration scheme in this context that achieves provable minimax optimality. Furthermore, once the sample size exceeds $\frac{S^2AH^3}{\varepsilon^2}$ episodes (up to log f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#36890;&#36807;&#20026;&#36739;&#24930;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#24809;&#32602;&#23427;&#20204;&#65292;&#20197;&#26356;&#21152;&#29616;&#23454;&#30340;&#26041;&#24335;&#35780;&#20272;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#12290;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32771;&#34385;&#25512;&#26029;&#36895;&#24230;&#26102;&#65292;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26356;&#22797;&#26434;&#20294;&#36739;&#24930;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04795</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#35780;&#20272;&#19979;&#37325;&#26032;&#23457;&#35270;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Revisiting Test Time Adaptation under Online Evaluation. (arXiv:2304.04795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#35780;&#20272;&#21327;&#35758;&#65292;&#35813;&#21327;&#35758;&#36890;&#36807;&#20026;&#36739;&#24930;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#24809;&#32602;&#23427;&#20204;&#65292;&#20197;&#26356;&#21152;&#29616;&#23454;&#30340;&#26041;&#24335;&#35780;&#20272;&#20102;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#12290;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#32771;&#34385;&#25512;&#26029;&#36895;&#24230;&#26102;&#65292;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26356;&#22797;&#26434;&#20294;&#36739;&#24930;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#65288;TTA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#36739;&#24930;&#30340;&#26041;&#27861;&#25552;&#20379;&#26356;&#23569;&#30340;&#26679;&#26412;&#26469;&#24809;&#32602;&#23427;&#20204;&#12290;TTA&#26041;&#27861;&#21033;&#29992;&#27979;&#35797;&#26102;&#38388;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#36866;&#24212;&#20998;&#24067;&#31227;&#20301;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#24778;&#20154;&#30340;&#24615;&#33021;&#36890;&#24120;&#35201;&#20197;&#26174;&#30528;&#22686;&#21152;&#30340;&#35745;&#31639;&#39044;&#31639;&#20026;&#20195;&#20215;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#21327;&#35758;&#24573;&#30053;&#20102;&#36825;&#31181;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#30340;&#24433;&#21709;&#65292;&#24433;&#21709;&#23427;&#20204;&#22312;&#23454;&#38469;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#29616;&#23454;&#30340;TTA&#26041;&#27861;&#35780;&#20272;&#21327;&#35758;&#65292;&#22312;&#36825;&#20010;&#21327;&#35758;&#20013;&#25968;&#25454;&#20197;&#24658;&#23450;&#36895;&#29575;&#20174;&#25968;&#25454;&#27969;&#20013;&#22312;&#32447;&#25509;&#25910;&#65292;&#20174;&#32780;&#32771;&#34385;&#21040;&#26041;&#27861;&#30340;&#36866;&#24212;&#36895;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#21327;&#35758;&#24212;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#22330;&#26223;&#20013;&#23545;&#22810;&#31181;TTA&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#32771;&#34385;&#25512;&#26029;&#36895;&#24230;&#26102;&#65292;&#31616;&#21333;&#24555;&#36895;&#30340;&#26041;&#27861;&#21487;&#20197;&#20248;&#20110;&#26356;&#22797;&#26434;&#20294;&#36739;&#24930;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel online evaluation protocol for Test Time Adaptation (TTA) methods, which penalizes slower methods by providing them with fewer samples for adaptation. TTA methods leverage unlabeled data at test time to adapt to distribution shifts. Though many effective methods have been proposed, their impressive performance usually comes at the cost of significantly increased computation budgets. Current evaluation protocols overlook the effect of this extra computation cost, affecting their real-world applicability. To address this issue, we propose a more realistic evaluation protocol for TTA methods, where data is received in an online fashion from a constant-speed data stream, thereby accounting for the method's adaptation speed. We apply our proposed protocol to benchmark several TTA methods on multiple datasets and scenarios. Extensive experiments shows that, when accounting for inference speed, simple and fast approaches can outperform more sophisticated but slower
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#27169;&#25311;&#33258;&#26059;&#31995;&#32479;&#65292;&#22312;&#20234;&#36763;&#27169;&#22411;&#12289;&#27874;&#33576;&#27169;&#22411;&#21644;&#32454;&#32990;&#27874;&#33576;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01772</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#26059;&#27169;&#22411;&#30340;&#21487;&#24494;&#32534;&#31243;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A differentiable programming framework for spin models. (arXiv:2304.01772v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#32534;&#31243;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#27169;&#25311;&#33258;&#26059;&#31995;&#32479;&#65292;&#22312;&#20234;&#36763;&#27169;&#22411;&#12289;&#27874;&#33576;&#27169;&#22411;&#21644;&#32454;&#32990;&#27874;&#33576;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#26059;&#31995;&#32479;&#26159;&#24314;&#27169;&#21508;&#31181;&#29289;&#29702;&#31995;&#32479;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#24494;&#32534;&#31243;&#24314;&#27169;&#33258;&#26059;&#31995;&#32479;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#27169;&#25311;&#33258;&#26059;&#31995;&#32479;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22797;&#26434;&#31995;&#32479;&#20013;&#36827;&#34892;&#24314;&#27169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#19977;&#31181;&#19981;&#21516;&#30340;&#33258;&#26059;&#31995;&#32479;&#8212;&#8212;&#20234;&#36763;&#27169;&#22411;&#12289;&#27874;&#33576;&#27169;&#22411;&#21644;&#32454;&#32990;&#27874;&#33576;&#27169;&#22411;&#8212;&#8212;&#26469;&#35777;&#26126;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#27169;&#25311;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#30828;&#20214;&#26550;&#26500;&#65288;&#21253;&#25324;&#22270;&#24418;&#22788;&#29702;&#22120;&#21644;&#24352;&#37327;&#22788;&#29702;&#22120;&#65289;&#19978;&#39640;&#25928;&#22320;&#25191;&#34892;&#20195;&#30721;&#65292;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spin systems are a powerful tool for modeling a wide range of physical systems. In this paper, we propose a novel framework for modeling spin systems using differentiable programming. Our approach enables us to efficiently simulate spin systems, making it possible to model complex systems at scale. Specifically, we demonstrate the effectiveness of our technique by applying it to three different spin systems: the Ising model, the Potts model, and the Cellular Potts model. Our simulations show that our framework offers significant speedup compared to traditional simulation methods, thanks to its ability to execute code efficiently across different hardware architectures, including Graphical Processing Units and Tensor Processing Units.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26500;&#25104;&#30005;&#24433;&#27785;&#28024;&#24863;&#30340;&#20855;&#20307;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;&#39537;&#21160;&#30340;&#25668;&#20687;&#26426;&#31227;&#21160;&#29983;&#25104;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#24773;&#24863;&#21644;&#31354;&#38388;&#30340;&#27785;&#28024;&#24863;&#12290;</title><link>http://arxiv.org/abs/2303.17041</link><description>&lt;p&gt;
&#27785;&#28024;&#24863;&#31192;&#35776;&#65306;&#22522;&#20110;&#28436;&#21592;&#30340;&#33258;&#21160;&#29983;&#25104;&#30005;&#24433;&#25668;&#24433;&#26426;&#31227;&#21160;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
The secret of immersion: actor driven camera movement generation for auto-cinematography. (arXiv:2303.17041v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26500;&#25104;&#30005;&#24433;&#27785;&#28024;&#24863;&#30340;&#20855;&#20307;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;&#39537;&#21160;&#30340;&#25668;&#20687;&#26426;&#31227;&#21160;&#29983;&#25104;&#31995;&#32479;&#65292;&#20197;&#23454;&#29616;&#24773;&#24863;&#21644;&#31354;&#38388;&#30340;&#27785;&#28024;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27785;&#28024;&#24863;&#22312;&#35774;&#35745;&#30005;&#24433;&#26102;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#28982;&#32780;&#65292;&#27785;&#28024;&#24335;&#25293;&#25668;&#30340;&#22256;&#38590;&#38459;&#30861;&#20102;&#35774;&#35745;&#24072;&#21019;&#36896;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#25104;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26500;&#25104;&#30005;&#24433;&#27785;&#28024;&#24863;&#30340;&#20855;&#20307;&#32452;&#25104;&#37096;&#20998;&#65292;&#32771;&#34385;&#20102;&#31354;&#38388;&#12289;&#24773;&#24863;&#21644;&#32654;&#23398;&#31561;&#26041;&#38754;&#65292;&#21516;&#26102;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#34987;&#32467;&#21512;&#21040;&#20102;&#19968;&#20010;&#39640;&#32423;&#35780;&#20272;&#26426;&#21046;&#20013;&#12290;&#22312;&#36825;&#26679;&#30340;&#27785;&#28024;&#26426;&#21046;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#25668;&#20687;&#26426;&#25511;&#21046;&#31995;&#32479;&#65292;&#33021;&#22815;&#22312;3D&#34394;&#25311;&#29615;&#22659;&#20013;&#29983;&#25104;&#22522;&#20110;&#28436;&#21592;&#39537;&#21160;&#30340;&#25668;&#20687;&#26426;&#31227;&#21160;&#65292;&#20197;&#33719;&#24471;&#27785;&#28024;&#24335;&#30005;&#24433;&#24207;&#21015;&#12290;&#29983;&#25104;&#36807;&#31243;&#20013;&#25552;&#20986;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#23558;&#28436;&#21592;&#36816;&#21160;&#36716;&#25442;&#20026;&#20197;&#24773;&#24863;&#22240;&#32032;&#20026;&#26465;&#20214;&#30340;&#25668;&#20687;&#26426;&#36712;&#36857;&#65292;&#30830;&#20445;&#20102;&#28436;&#21592;&#19982;&#25668;&#20687;&#26426;&#30340;&#29289;&#29702;&#21644;&#24515;&#29702;&#21516;&#27493;&#20197;&#23454;&#29616;&#31354;&#38388;&#21644;&#24773;&#24863;&#30340;&#27785;&#28024;&#24863;&#12290;&#36890;&#36807;&#21152;&#20837;&#25511;&#21046;&#25668;&#20687;&#26426;&#25238;&#21160;&#20197;&#34920;&#36798;&#19981;&#21516;&#24515;&#29702;&#29366;&#24577;&#30340;&#27491;&#21017;&#21270;&#65292;&#24773;&#24863;&#27785;&#28024;&#24863;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#21152;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Immersion plays a vital role when designing cinematic creations, yet the difficulty in immersive shooting prevents designers to create satisfactory outputs. In this work, we analyze the specific components that contribute to cinematographic immersion considering spatial, emotional, and aesthetic level, while these components are then combined into a high-level evaluation mechanism. Guided by such a immersion mechanism, we propose a GAN-based camera control system that is able to generate actor-driven camera movements in the 3D virtual environment to obtain immersive film sequences. The proposed encoder-decoder architecture in the generation flow transfers character motion into camera trajectory conditioned on an emotion factor. This ensures spatial and emotional immersion by performing actor-camera synchronization physically and psychologically. The emotional immersion is further strengthened by incorporating regularization that controls camera shakiness for expressing different mental
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20302;&#24310;&#36831;&#27969;&#24335;&#20998;&#31163;&#24212;&#29992;&#20013;&#30340;&#22522;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#21457;&#35328;&#32773;&#20998;&#31163;&#65288;SSGD&#65289;&#22312;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#65288;CTS&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20998;&#31163;&#35828;&#35805;&#20154;&#24182;&#22312;&#27599;&#20010;&#20998;&#31163;&#30340;&#27969;&#19978;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;VAD&#65289;&#26469;&#25191;&#34892;&#21457;&#35328;&#32773;&#20998;&#31163;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#12289;&#22240;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#27844;&#28431;&#21435;&#38500;&#31639;&#27861;&#12290;&#22312;CALLHOME&#21644;Fisher&#35821;&#26009;&#24211;&#65288;&#31532;1&#21644;2&#37096;&#20998;&#65289;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;SSGD&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#20998;&#31163;&#21644;&#21457;&#35328;&#32773;&#20998;&#31163;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12002</link><description>&lt;p&gt;
&#30005;&#35805;&#20250;&#35805;&#30340;&#20302;&#24310;&#36831;&#21457;&#35328;&#20998;&#31163;&#21644;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#30340;&#31471;&#21040;&#31471;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
End-to-End Integration of Speech Separation and Voice Activity Detection for Low-Latency Diarization of Telephone Conversations. (arXiv:2303.12002v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#20302;&#24310;&#36831;&#27969;&#24335;&#20998;&#31163;&#24212;&#29992;&#20013;&#30340;&#22522;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#21457;&#35328;&#32773;&#20998;&#31163;&#65288;SSGD&#65289;&#22312;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#65288;CTS&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20998;&#31163;&#35828;&#35805;&#20154;&#24182;&#22312;&#27599;&#20010;&#20998;&#31163;&#30340;&#27969;&#19978;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;VAD&#65289;&#26469;&#25191;&#34892;&#21457;&#35328;&#32773;&#20998;&#31163;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#12289;&#22240;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#27844;&#28431;&#21435;&#38500;&#31639;&#27861;&#12290;&#22312;CALLHOME&#21644;Fisher&#35821;&#26009;&#24211;&#65288;&#31532;1&#21644;2&#37096;&#20998;&#65289;&#19978;&#30340;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;SSGD&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#20998;&#31163;&#21644;&#21457;&#35328;&#32773;&#20998;&#31163;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#21457;&#35328;&#32773;&#20998;&#31163;&#65288;SSGD&#65289;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#36825;&#20027;&#35201;&#24471;&#30410;&#20110;&#35821;&#38899;&#20998;&#31163;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#23427;&#36890;&#36807;&#39318;&#20808;&#20998;&#31163;&#35828;&#35805;&#20154;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#20998;&#31163;&#30340;&#27969;&#19978;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;VAD&#65289;&#26469;&#25191;&#34892;&#21457;&#35328;&#32773;&#20998;&#31163;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20250;&#35805;&#30005;&#35805;&#35821;&#38899;&#65288;CTS&#65289;&#39046;&#22495;&#20013;&#30340;SSGD&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#37325;&#28857;&#26159;&#20302;&#24310;&#36831;&#27969;&#24335;&#20998;&#31163;&#24212;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#20998;&#31163;&#65288;SSep&#65289;&#31639;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#65292;&#32771;&#34385;&#20102;&#38750;&#22240;&#26524;&#21644;&#22240;&#26524;&#23454;&#29616;&#20197;&#21450;&#36830;&#32493;SSep&#65288;CSS&#65289;&#31383;&#21475;&#25512;&#29702;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;SSGD&#31639;&#27861;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;CTS&#25968;&#25454;&#38598;CALLHOME&#21644;Fisher&#35821;&#26009;&#24211;&#65288;&#31532;1&#21644;2&#37096;&#20998;&#65289;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#35780;&#20272;&#20102;&#20998;&#31163;&#21644;&#21457;&#35328;&#32773;&#20998;&#31163;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#12289;&#22240;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#27844;&#28431;&#21435;&#38500;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works show that speech separation guided diarization (SSGD) is an increasingly promising direction, mainly thanks to the recent progress in speech separation. It performs diarization by first separating the speakers and then applying voice activity detection (VAD) on each separated stream. In this work we conduct an in-depth study of SSGD in the conversational telephone speech (CTS) domain, focusing mainly on low-latency streaming diarization applications. We consider three state-of-the-art speech separation (SSep) algorithms and study their performance both in online and offline scenarios, considering non-causal and causal implementations as well as continuous SSep (CSS) windowed inference. We compare different SSGD algorithms on two widely used CTS datasets: CALLHOME and Fisher Corpus (Part 1 and 2) and evaluate both separation and diarization performance. To improve performance, a novel, causal and computationally efficient leakage removal algorithm is proposed, which signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#26469;&#35774;&#35745;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32452;&#20214;&#65292;&#20026;&#35299;&#20915;&#28041;&#21450;&#20840;&#23616;&#31354;&#38388;&#21644;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22270;&#24418;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.07482</link><description>&lt;p&gt;
&#29992;&#24352;&#37327;&#32593;&#32476;&#24418;&#24335;&#32479;&#19968;O(3)&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Unifying O(3) Equivariant Neural Networks Design with Tensor-Network Formalism. (arXiv:2211.07482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#26469;&#35774;&#35745;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32452;&#20214;&#65292;&#20026;&#35299;&#20915;&#28041;&#21450;&#20840;&#23616;&#31354;&#38388;&#21644;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22270;&#24418;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#20174;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#20013;&#23398;&#20064;&#21183;&#33021;&#38754;&#65292;&#28041;&#21450;&#21040;&#20840;&#23616;&#31354;&#38388;&#23545;&#31216;&#24615;&#21644;&#21407;&#23376;&#25110;&#19968;&#33324;&#31890;&#23376;&#20043;&#38388;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#12290;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#20043;&#19968;&#65292;&#20854;&#20013;&#26368;&#25104;&#21151;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#20351;&#29992;&#22312;&#31354;&#38388;&#32676;&#19979;&#21464;&#25442;&#30340;&#21508;&#31181;&#24352;&#37327;&#20043;&#38388;&#30340;&#24352;&#37327;&#31215;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#19981;&#21516;&#24352;&#37327;&#30340;&#25968;&#37327;&#21644;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20445;&#25345;&#31616;&#27905;&#21644;&#31561;&#21464;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#65292;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#27169;&#25311;SU(2)&#23545;&#31216;&#37327;&#23376;&#22810;&#20307;&#38382;&#39064;&#30340;&#25216;&#26415;&#65292;&#26469;&#20026;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#26032;&#30340;&#31561;&#21464;&#32452;&#20214;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#24403;&#24212;&#29992;&#20110;&#32473;&#23450;&#23616;&#37096;&#37051;&#22495;&#20013;&#30340;&#31890;&#23376;&#26102;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#34701;&#21512;&#22359;&#8221;&#30340;&#32467;&#26524;&#32452;&#20214;&#36215;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
Many learning tasks, including learning potential energy surfaces from ab initio calculations, involve global spatial symmetries and permutational symmetry between atoms or general particles. Equivariant graph neural networks are a standard approach to such problems, with one of the most successful methods employing tensor products between various tensors that transform under the spatial group. However, as the number of different tensors and the complexity of relationships between them increase, maintaining parsimony and equivariance becomes increasingly challenging. In this paper, we propose using fusion diagrams, a technique widely employed in simulating SU($2$)-symmetric quantum many-body problems, to design new equivariant components for equivariant neural networks. This results in a diagrammatic approach to constructing novel neural network architectures. When applied to particles within a given local neighborhood, the resulting components, which we term "fusion blocks," serve as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#27744;&#21270;&#31639;&#23376;&#23545;&#20110;Manifold-Mixup&#26041;&#27861;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27744;&#21270;&#26550;&#26500;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#36234;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2210.03123</link><description>&lt;p&gt;
&#28151;&#21512;&#27744;&#21270;&#22312;&#22522;&#20110;Mixup&#30340;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Hybrid Pooling in Mixup-Based Graph Learning for Language Processing. (arXiv:2210.03123v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#27744;&#21270;&#31639;&#23376;&#23545;&#20110;Manifold-Mixup&#26041;&#27861;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27744;&#21270;&#26550;&#26500;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#36234;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#22270;&#24418;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#21644;&#28304;&#20195;&#30721;&#20998;&#31867;&#26041;&#38754;&#12290;&#36890;&#24120;&#65292;GNN&#26159;&#30001;&#20132;&#26367;&#22270;&#23618;&#21644;&#22270;&#27744;&#21270;&#23618;&#26500;&#25104;&#30340;&#65292;&#20132;&#26367;&#22270;&#23618;&#21487;&#20197;&#23398;&#20064;&#22270;&#33410;&#28857;&#29305;&#24449;&#30340;&#36716;&#25442;&#65292;&#32780;&#22270;&#27744;&#21270;&#23618;&#21017;&#20351;&#29992;&#22270;&#27744;&#21270;&#31639;&#23376;&#65288;&#20363;&#22914;Max&#27744;&#21270;&#65289;&#26377;&#25928;&#22320;&#20943;&#23569;&#33410;&#28857;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#22270;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#22686;&#24378;GNN&#22312;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20154;&#20204;&#24191;&#27867;&#37319;&#29992;&#20102;Manifold-Mixup&#36825;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#32447;&#24615;&#28151;&#21512;&#19968;&#23545;&#22270;&#25968;&#25454;&#21644;&#23427;&#20204;&#30340;&#26631;&#31614;&#26469;&#29983;&#25104;&#21512;&#25104;&#22270;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;Manifold-Mixup&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#22270;&#27744;&#21270;&#31639;&#23376;&#30340;&#24433;&#21709;&#65292;&#32780;&#19988;&#24182;&#27809;&#26377;&#36827;&#34892;&#24456;&#22810;&#20851;&#20110;&#36825;&#31181;&#24433;&#21709;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#26089;&#26399;&#25506;&#32034;&#20102;&#22270;&#27744;&#21270;&#31639;&#23376;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;Mixup&#30340;&#22270;&#24418;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27744;&#21270;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;Max-pooling&#21644;Attention-pooling&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#26412;&#22320;&#21644;&#20840;&#23616;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#27744;&#21270;&#32467;&#26500;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN)-based graph learning has been popular in natural language and programming language processing, particularly in text and source code classification. Typically, GNNs are constructed by incorporating alternating layers which learn transformations of graph node features, along with graph pooling layers that use graph pooling operators (e.g., Max-pooling) to effectively reduce the number of nodes while preserving the semantic information of the graph. Recently, to enhance GNNs in graph learning tasks, Manifold-Mixup, a data augmentation technique that produces synthetic graph data by linearly mixing a pair of graph data and their labels, has been widely adopted. However, the performance of Manifold-Mixup can be highly affected by graph pooling operators, and there have not been many studies that are dedicated to uncovering such affection. To bridge this gap, we take an early step to explore how graph pooling operators affect the performance of Mixup-based graph le
&lt;/p&gt;</description></item><item><title>GeONet&#26159;&#19968;&#20010;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23398;&#20064;&#20102;&#20174;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#36890;&#36807;&#23398;&#20064;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#65292;GeONet&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#24182;&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;OT&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.14440</link><description>&lt;p&gt;
GeONet&#65306;&#19968;&#31181;&#23398;&#20064;Wasserstein&#27979;&#22320;&#30340;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
GeONet: a neural operator for learning the Wasserstein geodesic. (arXiv:2209.14440v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14440
&lt;/p&gt;
&lt;p&gt;
GeONet&#26159;&#19968;&#20010;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23398;&#20064;&#20102;&#20174;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#36890;&#36807;&#23398;&#20064;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#65292;GeONet&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#23454;&#26102;&#39044;&#27979;&#65292;&#24182;&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;&#27979;&#35797;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#19982;&#26631;&#20934;OT&#27714;&#35299;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;(OT)&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20960;&#20309;&#19978;&#26377;&#24847;&#20041;&#27604;&#36739;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#20256;&#32479;&#30340;&#35745;&#31639;&#27010;&#29575;&#27979;&#24230;&#30340;Wasserstein&#36317;&#31163;&#21644;&#27979;&#22320;&#30340;&#26041;&#27861;&#38656;&#35201;&#32593;&#26684;&#20381;&#36182;&#30340;&#22495;&#31163;&#25955;&#21270;&#65292;&#21516;&#26102;&#21463;&#21040;&#32500;&#24230;&#28798;&#38590;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GeONet&#65292;&#19968;&#31181;&#19981;&#21463;&#32593;&#26684;&#24433;&#21709;&#30340;&#28145;&#24230;&#31070;&#32463;&#31639;&#23376;&#32593;&#32476;&#65292;&#23427;&#23398;&#20064;&#20102;&#23558;&#36755;&#20837;&#30340;&#21021;&#22987;&#21644;&#32456;&#31471;&#20998;&#24067;&#26144;&#23556;&#21040;&#36830;&#25509;&#20004;&#20010;&#31471;&#28857;&#20998;&#24067;&#30340;Wasserstein&#27979;&#22320;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#12290;&#22312;&#33073;&#26426;&#35757;&#32451;&#38454;&#27573;&#65292;GeONet&#36890;&#36807;&#32806;&#21512;&#30340;PDE&#31995;&#32479;&#34920;&#24449;&#30340;&#21407;&#22987;&#21644;&#23545;&#20598;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#26368;&#20248;&#26465;&#20214;&#23398;&#20064;&#20102;OT&#38382;&#39064;&#30340;&#38797;&#28857;&#20248;&#21270;&#26465;&#20214;&#12290;&#21518;&#32493;&#30340;&#25512;&#29702;&#38454;&#27573;&#26159;&#30636;&#26102;&#23436;&#25104;&#30340;&#65292;&#24182;&#21487;&#20197;&#22312;&#22312;&#32447;&#23398;&#20064;&#35774;&#32622;&#20013;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GeONet&#22312;&#20223;&#30495;&#31034;&#20363;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) offers a versatile framework to compare complex data distributions in a geometrically meaningful way. Traditional methods for computing the Wasserstein distance and geodesic between probability measures require mesh-dependent domain discretization and suffer from the curse-of-dimensionality. We present GeONet, a mesh-invariant deep neural operator network that learns the non-linear mapping from the input pair of initial and terminal distributions to the Wasserstein geodesic connecting the two endpoint distributions. In the offline training stage, GeONet learns the saddle point optimality conditions for the dynamic formulation of the OT problem in the primal and dual spaces that are characterized by a coupled PDE system. The subsequent inference stage is instantaneous and can be deployed for real-time predictions in the online learning setting. We demonstrate that GeONet achieves comparable testing accuracy to the standard OT solvers on simulation examples and the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#34880;&#20957;&#32032;&#24207;&#21015;&#20026;&#22522;&#30784;&#65292;&#21033;&#29992;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;&#21644;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#35780;&#20272;&#25351;&#26631;&#22312;&#19981;&#21516;&#20998;&#31867;&#27700;&#24179;&#19978;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;5-grams-transformer&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#27969;&#24863;&#30149;&#27602;&#24207;&#21015;&#30340;&#36215;&#28304;&#12290;</title><link>http://arxiv.org/abs/2207.13842</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30740;&#31350;&#34880;&#20957;&#32032;&#24207;&#21015;&#22312;&#27969;&#24863;&#30149;&#27602;&#23487;&#20027;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dive into Machine Learning Algorithms for Influenza Virus Host Prediction with Hemagglutinin Sequences. (arXiv:2207.13842v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.13842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#34880;&#20957;&#32032;&#24207;&#21015;&#20026;&#22522;&#30784;&#65292;&#21033;&#29992;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;&#21644;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#35780;&#20272;&#25351;&#26631;&#22312;&#19981;&#21516;&#20998;&#31867;&#27700;&#24179;&#19978;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;5-grams-transformer&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#27969;&#24863;&#30149;&#27602;&#24207;&#21015;&#30340;&#36215;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24863;&#30149;&#27602;&#31361;&#21464;&#36805;&#36895;&#65292;&#23545;&#20844;&#20247;&#20581;&#24247;&#65292;&#29305;&#21035;&#26159;&#33030;&#24369;&#32676;&#20307;&#26500;&#25104;&#23041;&#32961;&#12290;&#22312;&#21382;&#21490;&#19978;&#65292;&#30002;&#22411;&#27969;&#24863;&#30149;&#27602;&#22312;&#19981;&#21516;&#29289;&#31181;&#20043;&#38388;&#24341;&#21457;&#36807;&#22823;&#27969;&#34892;&#12290;&#30830;&#23450;&#30149;&#27602;&#30340;&#36215;&#28304;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#38450;&#27490;&#30123;&#24773;&#30340;&#20256;&#25773;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#30149;&#27602;&#24207;&#21015;&#30340;&#24555;&#36895;&#20934;&#30830;&#39044;&#27979;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#30495;&#23454;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#20197;&#19981;&#21516;&#20998;&#31867;&#27700;&#24179;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#30001;&#20110;&#34880;&#20957;&#32032;&#26159;&#20813;&#30123;&#21453;&#24212;&#20013;&#30340;&#20027;&#35201;&#34507;&#30333;&#36136;&#65292;&#21482;&#20351;&#29992;&#34880;&#20957;&#32032;&#24207;&#21015;&#65292;&#24182;&#20197;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;&#21644;&#35789;&#23884;&#20837;&#34920;&#31034;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;5-grams-transformer&#31070;&#32463;&#32593;&#32476;&#26159;&#39044;&#27979;&#30149;&#27602;&#24207;&#21015;&#36215;&#28304;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#22312;&#36739;&#39640;&#20998;&#31867;&#27700;&#24179;&#19978;&#22823;&#32422;&#26377;99.54&#65285;&#30340;AUCPR&#65292;98.01&#65285;&#30340;F1&#24471;&#20998;&#21644;96.60&#65285;&#30340;MCC&#12290;
&lt;/p&gt;
&lt;p&gt;
Influenza viruses mutate rapidly and can pose a threat to public health, especially to those in vulnerable groups. Throughout history, influenza A viruses have caused pandemics between different species. It is important to identify the origin of a virus in order to prevent the spread of an outbreak. Recently, there has been increasing interest in using machine learning algorithms to provide fast and accurate predictions for viral sequences. In this study, real testing data sets and a variety of evaluation metrics were used to evaluate machine learning algorithms at different taxonomic levels. As hemagglutinin is the major protein in the immune response, only hemagglutinin sequences were used and represented by position-specific scoring matrix and word embedding. The results suggest that the 5-grams-transformer neural network is the most effective algorithm for predicting viral sequence origins, with approximately 99.54% AUCPR, 98.01% F1 score and 96.60% MCC at a higher classification l
&lt;/p&gt;</description></item></channel></rss>