<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#23545;&#20132;&#21449;&#29109;&#12289;&#24191;&#20041;&#20132;&#21449;&#29109;&#12289;&#22343;&#26041;&#35823;&#24046;&#31561;&#19968;&#22823;&#31867;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#20248;&#21183;&#30340;&#21452;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#25110;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.07288</link><description>&lt;p&gt;
&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65306;&#29702;&#35770;&#20998;&#26512;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Entropy Loss Functions: Theoretical Analysis and Applications. (arXiv:2304.07288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20132;&#21449;&#29109;&#12289;&#24191;&#20041;&#20132;&#21449;&#29109;&#12289;&#22343;&#26041;&#35823;&#24046;&#31561;&#19968;&#22823;&#31867;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;&#20248;&#21183;&#30340;&#21452;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#25110;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#21449;&#29109;&#26159;&#24191;&#27867;&#24212;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#24403;&#20351;&#29992;softmax&#20989;&#25968;&#26102;&#65292;&#23427;&#19982;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#24212;&#29992;&#20110;&#36923;&#36753;&#22238;&#24402;&#25439;&#22833;&#20989;&#25968;&#30456;&#31526;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#20132;&#21449;&#29109;&#20316;&#20026;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#26102;&#65292;&#25105;&#20204;&#33021;&#20381;&#38752;&#20160;&#20040;&#20445;&#35777;&#21602;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#23478;&#26063;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#21253;&#25324;&#20132;&#21449;&#29109;&#65288;&#25110;&#36923;&#36753;&#25439;&#22833;&#65289;&#12289;&#24191;&#20041;&#20132;&#21449;&#29109;&#12289;&#22343;&#26041;&#35823;&#24046;&#21644;&#20854;&#20182;&#20132;&#21449;&#29109;&#31867;&#20989;&#25968;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#36825;&#20123;&#25439;&#22833;&#20989;&#25968;&#30340;&#31532;&#19968;&#20010;$H$-&#36830;&#32493;&#24615;&#30028;&#38480;&#12290;&#36825;&#20123;&#37117;&#26159;&#38750;&#28176;&#36827;&#20445;&#35777;&#65292;&#20197;&#20272;&#35745;&#20195;&#29702;&#25439;&#22833;&#30340;&#20272;&#35745;&#35823;&#24046;&#20026;&#19978;&#38480;&#65292;&#29992;&#20110;&#29305;&#23450;&#30340;&#20551;&#35774;&#38598;$H$&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#36825;&#20123;&#36793;&#30028;&#30340;&#32039;&#23494;&#31243;&#24230;&#12290;&#36825;&#20123;&#36793;&#30028;&#21462;&#20915;&#20110;&#31216;&#20026;&#21487;&#26368;&#23567;&#21270;&#38388;&#38553;&#30340;&#37327;&#65292;&#36825;&#20123;&#38388;&#38553;&#21482;&#21462;&#20915;&#20110;&#25439;&#22833;&#20989;&#25968;&#21644;&#20551;&#35774;&#38598;&#12290;&#20026;&#20102;&#20351;&#23427;&#20204;&#26356;&#20855;&#20307;&#21270;&#65292;&#25105;&#20204;&#23545;&#22797;&#26434;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#36825;&#20123;&#38388;&#38553;&#36827;&#34892;&#20102;&#20855;&#20307;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#21452;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#23427;&#22522;&#20110;&#20004;&#20010;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#20248;&#20110;&#26631;&#20934;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#26631;&#31614;&#22122;&#22768;&#25110;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of losses, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other loss cross-entropy-like functions. We give the first $H$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set $H$ used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps, which only depend on the loss function and the hypothesis set. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#23569;&#37327;&#20154;&#31867;&#25968;&#25454;&#24320;&#22987;&#65292;&#36890;&#36807;&#22870;&#21169;&#22609;&#24418;&#21644;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#21512;&#25104;&#31867;&#20284;&#20154;&#31867;&#20915;&#31574;&#30340;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#21040;&#30005;&#33041;&#28216;&#25103;&#30340;&#36830;&#32493;&#20915;&#31574;&#20219;&#21153;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07280</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22870;&#21169;&#22609;&#24418;&#30340;&#27169;&#20223;&#23398;&#20064;&#21512;&#25104;&#31867;&#20284;&#20154;&#31867;&#25968;&#25454;&#65292;&#26469;&#35299;&#20915;&#36830;&#32493;&#20915;&#31574;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Synthetically Generating Human-like Data for Sequential Decision Making Tasks via Reward-Shaped Imitation Learning. (arXiv:2304.07280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#23569;&#37327;&#20154;&#31867;&#25968;&#25454;&#24320;&#22987;&#65292;&#36890;&#36807;&#22870;&#21169;&#22609;&#24418;&#21644;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#21512;&#25104;&#31867;&#20284;&#20154;&#31867;&#20915;&#31574;&#30340;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#21040;&#30005;&#33041;&#28216;&#25103;&#30340;&#36830;&#32493;&#20915;&#31574;&#20219;&#21153;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22312;&#19982;AI&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#20363;&#22914;&#19982;&#30005;&#33041;&#28216;&#25103;&#20132;&#20114;&#65292;&#22914;&#20309;&#21512;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#20915;&#31574;&#30340;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#20174;&#23569;&#37327;&#30340;&#20154;&#31867;&#20915;&#31574;&#25968;&#25454;&#20837;&#25163;&#65292;&#23558;&#22870;&#21169;&#22609;&#24418;&#30340;&#27010;&#24565;&#19982;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#21512;&#25104;&#30340;&#12289;&#31867;&#20284;&#20110;&#20154;&#31867;&#20915;&#31574;&#30340;&#25968;&#25454;&#12290;&#20316;&#32773;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#22312;&#19968;&#20010;&#23567;&#22411;&#30340;&#30005;&#33041;&#28216;&#25103;&#20013;&#23436;&#25104;&#20102;&#19977;&#20010;&#19981;&#21516;&#38590;&#24230;&#30340;&#36830;&#32493;&#20915;&#31574;&#20219;&#21153;&#65292;&#24182;&#23545;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#22810;&#26041;&#38754;&#30340;&#32463;&#39564;&#24615;&#21644;&#32479;&#35745;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#21512;&#25104;&#30340;&#25968;&#25454;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#25968;&#25454;&#65292;&#23454;&#29616;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20915;&#31574;&#21644;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of synthetically generating data that can closely resemble human decisions made in the context of an interactive human-AI system like a computer game. We propose a novel algorithm that can generate synthetic, human-like, decision making data while starting from a very small set of decision making data collected from humans. Our proposed algorithm integrates the concept of reward shaping with an imitation learning algorithm to generate the synthetic data. We have validated our synthetic data generation technique by using the synthetically generated data as a surrogate for human interaction data to solve three sequential decision making tasks of increasing complexity within a small computer game-like setup. Different empirical and statistical analyses of our results show that the synthetically generated data can substitute the human data and perform the game-playing tasks almost indistinguishably, with very low divergence, from a human performing the same tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#20851;&#22870;&#21169;&#25506;&#32034;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#20445;&#35777;&#26679;&#26412;&#25910;&#38598;&#25968;&#37327;&#28385;&#36275;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#21457;&#29616;&#25152;&#26377;&#32473;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#25506;&#32034;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.07278</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#26080;&#20851;&#22870;&#21169;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Minimax-Optimal Reward-Agnostic Exploration in Reinforcement Learning. (arXiv:2304.07278v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#20851;&#22870;&#21169;&#25506;&#32034;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#20445;&#35777;&#26679;&#26412;&#25910;&#38598;&#25968;&#37327;&#28385;&#36275;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#21457;&#29616;&#25152;&#26377;&#32473;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#25506;&#32034;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26080;&#20851;&#22870;&#21169;&#25506;&#32034;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#25913;&#36827;&#29616;&#26377;&#25216;&#26415;&#12290;&#30740;&#31350;&#20102;&#20855;&#26377;S&#20010;&#29366;&#24577;&#65292;A&#20010;&#21160;&#20316;&#21644;&#26377;&#38480;&#26102;&#38388;&#27700;&#24179;H&#30340;&#38750;&#24179;&#31283;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#23450;&#25968;&#37327;&#30340;&#26080;&#24341;&#23548;&#22870;&#21169;&#20449;&#24687;&#30340;&#26679;&#26412;&#38598;&#65292;&#22312;&#20445;&#35777;&#25910;&#38598;&#30340;&#25968;&#37327;&#28385;&#36275;&#22810;&#39033;&#24335;&#32423;&#21035;&#26102;&#65292;&#31639;&#27861;&#33021;&#22815;&#21457;&#29616;&#25152;&#26377;&#36825;&#20123;&#22870;&#21169;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#65292;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;&#25506;&#32034;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies reward-agnostic exploration in reinforcement learning (RL) -- a scenario where the learner is unware of the reward functions during the exploration stage -- and designs an algorithm that improves over the state of the art. More precisely, consider a finite-horizon non-stationary Markov decision process with $S$ states, $A$ actions, and horizon length $H$, and suppose that there are no more than a polynomial number of given reward functions of interest. By collecting an order of \begin{align*}  \frac{SAH^3}{\varepsilon^2} \text{ sample episodes (up to log factor)} \end{align*} without guidance of the reward information, our algorithm is able to find $\varepsilon$-optimal policies for all these reward functions, provided that $\varepsilon$ is sufficiently small. This forms the first reward-agnostic exploration scheme in this context that achieves provable minimax optimality. Furthermore, once the sample size exceeds $\frac{S^2AH^3}{\varepsilon^2}$ episodes (up to log f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#19988;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20316;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#31579;&#26597;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#12290;&#35813;&#27969;&#31243;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22810;&#36724;&#35270;&#35273;&#36716;&#25442;&#22120;&#26550;&#26500;&#23545;&#20896;&#29366;&#21160;&#33033;&#30340;&#22810;&#24179;&#38754;&#25237;&#24433;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#20998;&#31867;&#65292;&#21487;&#26681;&#25454;&#24120;&#29992;CAD-RADS&#38408;&#20540;&#20998;&#31867;&#24739;&#32773;&#21040;&#19981;&#21516;&#31243;&#24230;&#30340;&#29421;&#31364;&#20122;&#32452;&#20013;&#12290;</title><link>http://arxiv.org/abs/2304.07277</link><description>&lt;p&gt;
&#20896;&#29366;&#21160;&#33033;CT&#34880;&#31649;&#36896;&#24433;&#20013;&#30340;CAD-RADS&#35780;&#20998;&#19982;&#22810;&#36724;&#35270;&#35273;&#36716;&#25442;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
CAD-RADS scoring of coronary CT angiography with Multi-Axis Vision Transformer: a clinically-inspired deep learning pipeline. (arXiv:2304.07277v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#19988;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20316;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#31579;&#26597;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#12290;&#35813;&#27969;&#31243;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22810;&#36724;&#35270;&#35273;&#36716;&#25442;&#22120;&#26550;&#26500;&#23545;&#20896;&#29366;&#21160;&#33033;&#30340;&#22810;&#24179;&#38754;&#25237;&#24433;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#20998;&#31867;&#65292;&#21487;&#26681;&#25454;&#24120;&#29992;CAD-RADS&#38408;&#20540;&#20998;&#31867;&#24739;&#32773;&#21040;&#19981;&#21516;&#31243;&#24230;&#30340;&#29421;&#31364;&#20122;&#32452;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#21160;&#33033;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#34880;&#31649;&#36896;&#24433;&#65288;CCTA&#65289;&#26159;&#35780;&#20272;&#20896;&#29366;&#21160;&#33033;&#30142;&#30149;&#65288;CAD&#65289;&#20005;&#37325;&#31243;&#24230;&#21644;&#33539;&#22260;&#30340;&#26631;&#20934;&#38750;&#21019;&#20260;&#24615;&#25104;&#20687;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26681;&#25454;CAD-Reporting&#21644;&#25968;&#25454;&#31995;&#32479;&#65288;CAD-RADS&#65289;&#35780;&#20998;&#23545;&#27599;&#20010;&#24739;&#32773;&#30340;CCTA&#36827;&#34892;&#25163;&#21160;&#20998;&#32423;&#38750;&#24120;&#32791;&#26102;&#19988;&#25805;&#20316;&#32773;&#20381;&#36182;&#24615;&#24378;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36793;&#32536;&#30149;&#20363;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#19988;&#21487;&#35270;&#21270;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20316;CAD&#31579;&#26597;&#36807;&#31243;&#30340;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#12290;&#35813;&#27969;&#31243;&#25191;&#34892;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#65306;&#39318;&#20808;&#65292;&#35782;&#21035;&#38656;&#35201;&#36827;&#19968;&#27493;&#20020;&#24202;&#26816;&#26597;&#30340;&#24739;&#32773;&#65307;&#20854;&#27425;&#65292;&#26681;&#25454;&#24120;&#29992;&#30340;CAD-RADS&#38408;&#20540;&#23558;&#24739;&#32773;&#20998;&#31867;&#21040;&#19981;&#21516;&#31243;&#24230;&#30340;&#29421;&#31364;&#20122;&#32452;&#20013;&#12290;&#35813;&#27969;&#31243;&#23545;&#21407;&#22987;CCTA&#20013;&#30340;&#20896;&#29366;&#21160;&#33033;&#30340;&#22810;&#24179;&#38754;&#25237;&#24433;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#20998;&#31867;&#65292;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22810;&#36724;&#35270;&#35273;&#36716;&#25442;&#22120;&#26550;&#26500;&#36827;&#34892;&#20998;&#31867;&#12290;&#26088;&#22312;&#27169;&#25311;&#24403;&#21069;&#30340;&#21307;&#23398;&#23454;&#36341;&#27969;&#31243;&#65292;&#32553;&#30701;&#20020;&#24202;&#35780;&#20272;&#26102;&#38388;&#65292;&#38477;&#20302;&#25805;&#20316;&#32773;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard non-invasive imaging technique used to assess the severity and extent of Coronary Artery Disease (CAD) is Coronary Computed Tomography Angiography (CCTA). However, manual grading of each patient's CCTA according to the CAD-Reporting and Data System (CAD-RADS) scoring is time-consuming and operator-dependent, especially in borderline cases. This work proposes a fully automated, and visually explainable, deep learning pipeline to be used as a decision support system for the CAD screening procedure. The pipeline performs two classification tasks: firstly, identifying patients who require further clinical investigations and secondly, classifying patients into subgroups based on the degree of stenosis, according to commonly used CAD-RADS thresholds. The pipeline pre-processes multiplanar projections of the coronary arteries, extracted from the original CCTAs, and classifies them using a fine-tuned Multi-Axis Vision Transformer architecture. With the aim of emulating the current
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;&#24179;&#28369;IoU&#25439;&#22833;&#65292;&#21487;&#20197;&#30452;&#25509;&#26368;&#22823;&#21270;ground-truth bounding box&#21644;&#39044;&#27979;bounding box&#20043;&#38388;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07256</link><description>&lt;p&gt;
&#30452;&#25509;&#20248;&#21270;IoU&#29992;&#20110;&#36793;&#30028;&#26694;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Directly Optimizing IoU for Bounding Box Localization. (arXiv:2304.07256v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;&#24179;&#28369;IoU&#25439;&#22833;&#65292;&#21487;&#20197;&#30452;&#25509;&#26368;&#22823;&#21270;ground-truth bounding box&#21644;&#39044;&#27979;bounding box&#20043;&#38388;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#20174;&#32780;&#25552;&#39640;&#30446;&#26631;&#26816;&#27979;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#24341;&#36827;&#20351;&#24471;&#30446;&#26631;&#26816;&#27979;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#30446;&#26631;&#26816;&#27979;&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#38656;&#35201;&#27491;&#30830;&#22320;&#26631;&#35782;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#20301;&#32622;&#21644;&#31867;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;&#24179;&#28369;IoU&#25439;&#22833;&#65292;&#21487;&#20197;&#30452;&#25509;&#26368;&#22823;&#21270;ground-truth bounding box&#21644;&#39044;&#27979;bounding box&#20043;&#38388;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#36817;&#20284;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection has seen remarkable progress in recent years with the introduction of Convolutional Neural Networks (CNN). Object detection is a multi-task learning problem where both the position of the objects in the images as well as their classes needs to be correctly identified. The idea here is to maximize the overlap between the ground-truth bounding boxes and the predictions i.e. the Intersection over Union (IoU). In the scope of work seen currently in this domain, IoU is approximated by using the Huber loss as a proxy but this indirect method does not leverage the IoU information and treats the bounding box as four independent, unrelated terms of regression. This is not true for a bounding box where the four coordinates are highly correlated and hold a semantic meaning when taken together. The direct optimization of the IoU is not possible due to its non-convex and non-differentiable nature. In this paper, we have formulated a novel loss namely, the Smooth IoU, which directly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#29983;&#25104;&#37327;&#23376;&#21487;&#31215;&#33258;&#26059;&#38142; R-&#30697;&#38453;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#38480;&#21046;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25628;&#32034;&#21487;&#31215;&#21704;&#23494;&#39039;&#37327;&#21644;&#30456;&#24212;&#30340; R-&#30697;&#38453;&#65292;&#24182;&#25506;&#32034;&#21487;&#31215;&#33258;&#26059;&#38142;&#30340;&#23478;&#26063;&#12290;&#35813;&#32593;&#32476;&#24050;&#22312;&#20108;&#32500;&#24046;&#20998;&#22411;&#33258;&#26059;&#38142;&#19978;&#24471;&#21040;&#24212;&#29992;&#65292;&#24182;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.07247</link><description>&lt;p&gt;
R-mAtrIx Net&#65306;&#19968;&#31181;&#29983;&#25104;&#37327;&#23376;&#21487;&#31215;&#33258;&#26059;&#38142; R-&#30697;&#38453;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
The R-mAtrIx Net. (arXiv:2304.07247v1 [hep-th])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#29983;&#25104;&#37327;&#23376;&#21487;&#31215;&#33258;&#26059;&#38142; R-&#30697;&#38453;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#38480;&#21046;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25628;&#32034;&#21487;&#31215;&#21704;&#23494;&#39039;&#37327;&#21644;&#30456;&#24212;&#30340; R-&#30697;&#38453;&#65292;&#24182;&#25506;&#32034;&#21487;&#31215;&#33258;&#26059;&#38142;&#30340;&#23478;&#26063;&#12290;&#35813;&#32593;&#32476;&#24050;&#22312;&#20108;&#32500;&#24046;&#20998;&#22411;&#33258;&#26059;&#38142;&#19978;&#24471;&#21040;&#24212;&#29992;&#65292;&#24182;&#34987;&#35777;&#26126;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#65306;i&#65289;&#36755;&#20986;&#32473;&#23450;&#37327;&#23376;&#21487;&#31215;&#33258;&#26059;&#38142;&#30340;R&#30697;&#38453;&#65292;ii&#65289;&#22312;&#28385;&#36275;&#19968;&#23450;&#30340;&#23545;&#31216;&#24615;&#25110;&#20854;&#20182;&#38480;&#21046;&#30340;&#20551;&#35774;&#19979;&#25628;&#32034;&#21487;&#31215;&#21704;&#23494;&#39039;&#37327;&#21644;&#30456;&#24212;&#30340;R&#30697;&#38453;&#65292;iii&#65289;&#25506;&#32034;&#22260;&#32469;&#24050;&#23398;&#20250;&#30340;&#27169;&#22411;&#30340;&#21704;&#23494;&#39039;&#37327;&#31354;&#38388;&#65292;&#24182;&#37325;&#26500;&#23427;&#20204;&#25152;&#23646;&#30340;&#21487;&#31215;&#33258;&#26059;&#38142;&#23478;&#26063;&#12290;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36890;&#36807;&#26368;&#23567;&#21270;&#32534;&#30721;&#26472;-&#24052;&#20811;&#26031;&#29305;&#26041;&#31243;&#12289;&#27491;&#21017;&#24615;&#21644;&#20854;&#20182;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#38480;&#21046;&#20363;&#22914;&#21380;&#31859;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#23436;&#25104;&#12290;&#20840;&#32431;&#24615;&#26159;&#36890;&#36807;&#28608;&#27963;&#20989;&#25968;&#30340;&#36873;&#25321;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#32500;&#24046;&#20998;&#22411;&#33258;&#26059;&#38142;&#19978;&#30340;&#24037;&#20316;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#37325;&#26500;&#20102;&#25152;&#26377;14&#20010;&#31867;&#21035;&#30340;R&#30697;&#38453;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#20316;&#20026;&#8220;&#25506;&#32034;&#22120;&#8221;&#30340;&#26377;&#29992;&#24615;&#65292;&#25195;&#25551;&#29305;&#23450;&#30340;&#21704;&#23494;&#39039;&#37327;&#23376;&#31354;&#38388;&#24182;&#22312;&#32858;&#31867;&#21518;&#35782;&#21035;&#21487;&#31215;&#31867;&#12290;&#26368;&#21518;&#30340;&#31574;&#30053;&#21487;&#20197;&#22312;&#26410;&#26469;&#29992;&#20110;&#24314;&#31435;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a novel Neural Network architecture that can: i) output R-matrix for a given quantum integrable spin chain, ii) search for an integrable Hamiltonian and the corresponding R-matrix under assumptions of certain symmetries or other restrictions, iii) explore the space of Hamiltonians around already learned models and reconstruct the family of integrable spin chains which they belong to. The neural network training is done by minimizing loss functions encoding Yang-Baxter equation, regularity and other model-specific restrictions such as hermiticity. Holomorphy is implemented via the choice of activation functions. We demonstrate the work of our Neural Network on the two-dimensional spin chains of difference form. In particular, we reconstruct the R-matrices for all 14 classes. We also demonstrate its utility as an \textit{Explorer}, scanning a certain subspace of Hamiltonians and identifying integrable classes after clusterisation. The last strategy can be used in future to car
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#36951;&#20256;&#31639;&#27861;&#65292;&#23545;&#30424;&#24335;&#32852;&#36724;&#22120;&#20013;&#30340;&#26580;&#24615;&#30424;&#20803;&#20214;&#36827;&#34892;&#25913;&#36827;&#35774;&#35745;&#65292;&#38477;&#20302;&#20854;&#36136;&#37327;&#21644;&#24212;&#21147;&#65292;&#32780;&#19981;&#38477;&#20302;&#25197;&#30697;&#20256;&#36882;&#21644;&#19981;&#23545;&#40784;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.07245</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26580;&#24615;&#30424;&#20803;&#20214;&#30340;&#22810;&#30446;&#26631;&#35774;&#35745;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Based Multi-Objective Design Exploration Of Flexible Disc Elements. (arXiv:2304.07245v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#36951;&#20256;&#31639;&#27861;&#65292;&#23545;&#30424;&#24335;&#32852;&#36724;&#22120;&#20013;&#30340;&#26580;&#24615;&#30424;&#20803;&#20214;&#36827;&#34892;&#25913;&#36827;&#35774;&#35745;&#65292;&#38477;&#20302;&#20854;&#36136;&#37327;&#21644;&#24212;&#21147;&#65292;&#32780;&#19981;&#38477;&#20302;&#25197;&#30697;&#20256;&#36882;&#21644;&#19981;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#25506;&#32034;&#26159;&#24037;&#31243;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#12290;&#23427;&#28041;&#21450;&#23547;&#25214;&#28385;&#36275;&#25351;&#23450;&#35774;&#35745;&#26631;&#20934;&#21644;&#23436;&#25104;&#39044;&#23450;&#20041;&#30446;&#26631;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#23558;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#26550;&#26500;&#24212;&#29992;&#20110;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#20013;&#65292;&#20197;&#25506;&#32034;&#21644;&#35782;&#21035;&#25913;&#36827;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#30340;&#26696;&#20363;&#38382;&#39064;&#26159;&#26580;&#24615;&#30424;&#20803;&#20214;&#30340;&#35774;&#35745;&#65292;&#35813;&#20803;&#20214;&#29992;&#20110;&#30424;&#24335;&#32852;&#36724;&#22120;&#20013;&#12290;&#25105;&#20204;&#38656;&#35201;&#36890;&#36807;&#38477;&#20302;&#36136;&#37327;&#21644;&#24212;&#21147;&#32780;&#19981;&#38477;&#20302;&#25197;&#30697;&#20256;&#36882;&#21644;&#19981;&#23545;&#40784;&#33021;&#21147;&#26469;&#25913;&#36827;&#30424;&#20803;&#20214;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#35774;&#35745;&#25506;&#32034;&#27493;&#39588;&#20013;&#37319;&#29992;&#20102;ANN&#21644;&#36951;&#20256;&#31639;&#27861;&#65292;&#20197;&#35782;&#21035;&#31526;&#21512;&#25351;&#23450;&#26631;&#20934;&#65288;&#25197;&#30697;&#21644;&#19981;&#23545;&#40784;&#65289;&#19988;&#20855;&#26377;&#26368;&#23567;&#36136;&#37327;&#21644;&#24212;&#21147;&#30340;&#35774;&#35745;&#26041;&#26696;&#12290;&#32467;&#26524;&#19982;&#20248;&#21270;&#32467;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Design exploration is an important step in the engineering design process. This involves the search for design/s that meet the specified design criteria and accomplishes the predefined objective/s. In recent years, machine learning-based approaches have been widely used in engineering design problems. This paper showcases Artificial Neural Network (ANN) architecture applied to an engineering design problem to explore and identify improved design solutions. The case problem of this study is the design of flexible disc elements used in disc couplings. We are required to improve the design of the disc elements by lowering the mass and stress without lowering the torque transmission and misalignment capability. To accomplish this objective, we employ ANN coupled with genetic algorithm in the design exploration step to identify designs that meet the specified criteria (torque and misalignment) while having minimum mass and stress. The results are comparable to the optimized results obtained
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#30340;&#31232;&#30095;&#24615;&#33021;&#25552;&#39640;&#38544;&#31169;&#24615;&#24182;&#20445;&#25345;&#24615;&#33021;&#34920;&#29616;</title><link>http://arxiv.org/abs/2304.07234</link><description>&lt;p&gt;
&#31232;&#30095;&#24615;&#21487;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#24615;
&lt;/p&gt;
&lt;p&gt;
Sparsity in neural networks can increase their privacy. (arXiv:2304.07234v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07234
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#31232;&#30095;&#24615;&#33021;&#25552;&#39640;&#38544;&#31169;&#24615;&#24182;&#20445;&#25345;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#24615;&#22914;&#20309;&#20351;&#31070;&#32463;&#32593;&#32476;&#23545;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26356;&#21152;&#40065;&#26834;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#32593;&#32476;&#30340;&#38544;&#31169;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24212;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#23436;&#21892;&#20102;&#24182;&#25193;&#23637;&#20102;&#29616;&#26377;&#25991;&#29486;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article measures how sparsity can make neural networks more robust to membership inference attacks. The obtained empirical results show that sparsity improves the privacy of the network, while preserving comparable performances on the task at hand. This empirical study completes and extends existing literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#20026;&#30456;&#20284;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65288;BS-GAT&#65289;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#23398;&#20064;&#26500;&#24314;&#22270;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07226</link><description>&lt;p&gt;
&#22522;&#20110;&#34892;&#20026;&#30456;&#20284;&#24615;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#65288;arXiv&#65306;2304.07226v1 [cs.CR]&#65289;
&lt;/p&gt;
&lt;p&gt;
BS-GAT Behavior Similarity Based Graph Attention Network for Network Intrusion Detection. (arXiv:2304.07226v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#20026;&#30456;&#20284;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65288;BS-GAT&#65289;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#23398;&#20064;&#26500;&#24314;&#22270;&#30340;&#34920;&#31034;&#65292;&#23454;&#29616;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#30340;&#21457;&#23637;&#65292;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#24191;&#27867;&#12290;&#30740;&#31350;&#26234;&#33021;&#12289;&#33258;&#21160;&#21270;&#21644;&#31283;&#20581;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#24182;&#19981;&#23436;&#20840;&#36866;&#24212;&#23454;&#38469;&#32593;&#32476;&#20837;&#20405;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65292;&#22240;&#27492;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#20026;&#30456;&#20284;&#24615;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65288;BS-GAT&#65289;&#65292;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20998;&#26512;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#34892;&#20026;&#30456;&#20284;&#24615;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#12290;&#20197;&#25968;&#25454;&#27969;&#20026;&#33410;&#28857;&#65292;&#22312;&#22270;&#20013;&#20351;&#29992;&#33410;&#28857;&#30340;&#34892;&#20026;&#35268;&#21017;&#20316;&#20026;&#36793;&#32536;&#65292;&#26500;&#24314;&#19968;&#20010;&#37051;&#23621;&#33410;&#28857;&#25968;&#37327;&#30456;&#23545;&#24179;&#22343;&#30340;&#22270;&#12290;&#28982;&#21518;&#65292;&#25152;&#25552;&#20986;&#30340;BS-GAT&#31639;&#27861;&#21033;&#29992;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#23398;&#20064;&#26500;&#24314;&#22270;&#30340;&#34920;&#31034;&#65292;&#21033;&#29992;&#34892;&#20026;&#30456;&#20284;&#24615;&#25429;&#25417;&#32593;&#32476;&#27969;&#37327;&#30340;&#21028;&#21035;&#29305;&#24449;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of the Internet of Things (IoT), network intrusion detection is becoming more complex and extensive. It is essential to investigate an intelligent, automated, and robust network intrusion detection method. Graph neural networks based network intrusion detection methods have been proposed. However, it still needs further studies because the graph construction method of the existing methods does not fully adapt to the characteristics of the practical network intrusion datasets. To address the above issue, this paper proposes a graph neural network algorithm based on behavior similarity (BS-GAT) using graph attention network. First, a novel graph construction method is developed using the behavior similarity by analyzing the characteristics of the practical datasets. The data flows are treated as nodes in the graph, and the behavior rules of nodes are used as edges in the graph, constructing a graph with a relatively uniform number of neighbors for each node. Then, th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;TD-MPC&#26694;&#26550;&#20869;&#20351;&#29992;&#37325;&#26500;&#20989;&#25968;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.07219</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Control with Self-supervised Representation Learning. (arXiv:2304.07219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;TD-MPC&#26694;&#26550;&#20869;&#20351;&#29992;&#37325;&#26500;&#20989;&#25968;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#25105;&#20204;&#27809;&#26377;&#30475;&#21040;&#27169;&#22411;&#26080;&#20851;&#25110;&#27169;&#22411;&#22522;&#30784;&#23398;&#20064;&#26041;&#27861;&#26377;&#20219;&#20309;&#37325;&#22823;&#36827;&#23637;&#65292;&#20351;&#24471;&#20854;&#20013;&#19968;&#20010;&#30456;&#23545;&#20110;&#21478;&#19968;&#20010;&#36807;&#26102;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#25152;&#20351;&#29992;&#30340;&#25216;&#26415;&#20005;&#37325;&#20381;&#36182;&#20110;&#29992;&#20363;&#22330;&#26223;&#25110;&#20854;&#20182;&#23646;&#24615;&#65292;&#20363;&#22914;&#29615;&#22659;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#26377;&#33258;&#24049;&#30340;&#20248;&#28857;&#65292;&#20363;&#22914;&#26679;&#26412;&#25928;&#29575;&#25110;&#35745;&#31639;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#26102;&#65292;&#21487;&#20197;&#32467;&#21512;&#21508;&#33258;&#30340;&#20248;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;TD-MPC&#26694;&#26550;&#23601;&#26159;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20363;&#23376;&#12290;&#19968;&#26041;&#38754;&#65292;&#32467;&#21512;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#19990;&#30028;&#27169;&#22411;&#29992;&#20110;&#33719;&#24471;&#33391;&#22909;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#20272;&#35745;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Q&#20989;&#25968;&#29992;&#20110;&#25552;&#20379;&#33391;&#22909;&#30340;&#38271;&#26399;&#20272;&#35745;&#12290;&#19982;MuZero&#31561;&#31639;&#27861;&#31867;&#20284;&#65292;&#20351;&#29992;&#28508;&#22312;&#29366;&#24577;&#34920;&#31034;&#65292;&#20854;&#20013;&#20165;&#23545;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#20197;&#20943;&#23569;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;TD-MPC&#26694;&#26550;&#20869;&#20351;&#29992;&#37325;&#26500;&#20989;&#25968;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;&#36825;&#21487;&#20197;&#21019;&#24314;&#26356;&#20855;&#20449;&#24687;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#28508;&#22312;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19968;&#31995;&#21015;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last few years, we have not seen any major developments in model-free or model-based learning methods that would make one obsolete relative to the other. In most cases, the used technique is heavily dependent on the use case scenario or other attributes, e.g. the environment. Both approaches have their own advantages, for example, sample efficiency or computational efficiency. However, when combining the two, the advantages of each can be combined and hence achieve better performance. The TD-MPC framework is an example of this approach. On the one hand, a world model in combination with model predictive control is used to get a good initial estimate of the value function. On the other hand, a Q function is used to provide a good long-term estimate. Similar to algorithms like MuZero a latent state representation is used, where only task-relevant information is encoded to reduce the complexity. In this paper, we propose the use of a reconstruction function within the TD-MPC fram
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#27861;&#65292;&#21046;&#20316;&#20102;&#20122;&#31859;&#32423;&#20896;&#23618;&#39640;&#24230;&#22270;&#65292;&#21487;&#29992;&#20110;&#32454;&#31890;&#24230;&#30340;&#26893;&#34987;&#32467;&#26500;&#30417;&#27979;&#65292;&#20026;&#30899;&#36890;&#37327;&#35780;&#20272;&#21644;&#22303;&#22320;&#21033;&#29992;&#31649;&#29702;&#25552;&#20379;&#23453;&#36149;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.07213</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#21046;&#20316;&#20122;&#31859;&#32423;&#20896;&#23618;&#39640;&#24230;&#22270;
&lt;/p&gt;
&lt;p&gt;
Sub-meter resolution canopy height maps using self-supervised learning and a vision transformer trained on Aerial and GEDI Lidar. (arXiv:2304.07213v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#27861;&#65292;&#21046;&#20316;&#20102;&#20122;&#31859;&#32423;&#20896;&#23618;&#39640;&#24230;&#22270;&#65292;&#21487;&#29992;&#20110;&#32454;&#31890;&#24230;&#30340;&#26893;&#34987;&#32467;&#26500;&#30417;&#27979;&#65292;&#20026;&#30899;&#36890;&#37327;&#35780;&#20272;&#21644;&#22303;&#22320;&#21033;&#29992;&#31649;&#29702;&#25552;&#20379;&#23453;&#36149;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26893;&#34987;&#32467;&#26500;&#30340;&#26144;&#23556;&#23545;&#20110;&#29702;&#35299;&#20840;&#29699;&#30899;&#24490;&#29615;&#21644;&#30417;&#27979;&#22522;&#20110;&#33258;&#28982;&#30340;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;&#36716;&#25442;&#22120;&#26041;&#27861;&#65292;&#20351;&#29992;&#33322;&#31354;&#21644;GEDI&#28608;&#20809;&#36965;&#24863;&#25968;&#25454;&#21046;&#20316;&#20102;&#20122;&#31859;&#32423;&#20896;&#23618;&#39640;&#24230;&#22270;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#12290;&#21046;&#20316;&#30340;&#20896;&#23618;&#39640;&#24230;&#22270;&#21487;&#20197;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#26893;&#34987;&#32467;&#26500;&#30417;&#27979;&#65292;&#20026;&#30899;&#36890;&#37327;&#35780;&#20272;&#21644;&#22303;&#22320;&#21033;&#29992;&#31649;&#29702;&#25552;&#20379;&#23453;&#36149;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vegetation structure mapping is critical for understanding the global carbon cycle and monitoring nature-based approaches to climate adaptation and mitigation. Repeat measurements of these data allow for the observation of deforestation or degradation of existing forests, natural forest regeneration, and the implementation of sustainable agricultural practices like agroforestry. Assessments of tree canopy height and crown projected area at a high spatial resolution are also important for monitoring carbon fluxes and assessing tree-based land uses, since forest structures can be highly spatially heterogeneous, especially in agroforestry systems. Very high resolution satellite imagery (less than one meter (1m) ground sample distance) makes it possible to extract information at the tree level while allowing monitoring at a very large scale. This paper presents the first high-resolution canopy height map concurrently produced for multiple sub-national jurisdictions. Specifically, we produc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#37327;&#29992;&#25143;&#34920;&#31034;&#20013;&#20877;&#35782;&#21035;&#39118;&#38505;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#65292;&#21487;&#20197;&#38480;&#21046;&#25915;&#20987;&#32773;&#20174;&#29992;&#25143;&#30340;&#34920;&#31034;&#20013;&#33719;&#21462;&#20854;&#36523;&#20221;&#30340;&#27010;&#29575;&#12290;&#20316;&#32773;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#24212;&#23545;&#23454;&#38469;&#24212;&#29992;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#34917;&#20805;&#20102;&#25915;&#20987;&#31639;&#27861;&#26469;&#34913;&#37327;&#29616;&#23454;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.07210</link><description>&lt;p&gt;
&#35745;&#37327;&#20877;&#35782;&#21035;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Measuring Re-identification Risk. (arXiv:2304.07210v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#37327;&#29992;&#25143;&#34920;&#31034;&#20013;&#20877;&#35782;&#21035;&#39118;&#38505;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#65292;&#21487;&#20197;&#38480;&#21046;&#25915;&#20987;&#32773;&#20174;&#29992;&#25143;&#30340;&#34920;&#31034;&#20013;&#33719;&#21462;&#20854;&#36523;&#20221;&#30340;&#27010;&#29575;&#12290;&#20316;&#32773;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#24212;&#23545;&#23454;&#38469;&#24212;&#29992;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#34917;&#20805;&#20102;&#25915;&#20987;&#31639;&#27861;&#26469;&#34913;&#37327;&#29616;&#23454;&#24212;&#29992;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#20945;&#30340;&#29992;&#25143;&#34920;&#31034;&#65288;&#20363;&#22914;&#23884;&#20837;&#65289;&#26159;&#20010;&#24615;&#21270;&#26381;&#21153;&#30340;&#25903;&#26609;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#37327;&#36825;&#31181;&#29992;&#25143;&#34920;&#31034;&#20013;&#30340;&#20877;&#35782;&#21035;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#65292;&#27491;&#24335;&#38480;&#21046;&#20102;&#25915;&#20987;&#32773;&#33021;&#22815;&#20174;&#29992;&#25143;&#30340;&#34920;&#31034;&#20013;&#33719;&#21462;&#20854;&#36523;&#20221;&#30340;&#27010;&#29575;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#36275;&#22815;&#26222;&#36941;&#20197;&#27169;&#25311;&#37325;&#35201;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#65292;&#20363;&#22914; Chrome &#30340;&#22522;&#20110;&#20852;&#36259;&#30340;&#24191;&#21578;&#30340; Topics API&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20877;&#35782;&#21035;&#25915;&#20987;&#30340;&#21512;&#29702;&#22909;&#30340;&#31639;&#27861;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#25105;&#20204;&#29992;&#36825;&#20123;&#31639;&#27861;&#26469;&#20272;&#35745; Topics API &#20013;&#30340;&#20877;&#35782;&#21035;&#39118;&#38505;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#19988;&#21487;&#35299;&#37322;&#30340;&#20877;&#35782;&#21035;&#39118;&#38505;&#27010;&#24565;&#21644;&#27979;&#37327;&#23427;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#26469;&#25351;&#23548;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compact user representations (such as embeddings) form the backbone of personalization services. In this work, we present a new theoretical framework to measure re-identification risk in such user representations. Our framework, based on hypothesis testing, formally bounds the probability that an attacker may be able to obtain the identity of a user from their representation. As an application, we show how our framework is general enough to model important real-world applications such as the Chrome's Topics API for interest-based advertising. We complement our theoretical bounds by showing provably good attack algorithms for re-identification that we use to estimate the re-identification risk in the Topics API. We believe this work provides a rigorous and interpretable notion of re-identification risk and a framework to measure it that can be used to inform real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#22270;&#19978;&#20855;&#26377;&#19977;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31163;&#25955;&#26102;&#38388;&#38750;&#32447;&#24615;&#24179;&#22343;&#21160;&#21147;&#23398;&#65292;&#22312;&#21021;&#24577;&#21644;&#36229;&#22270;&#25299;&#25169;&#20197;&#21450;&#26356;&#26032;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#19979;&#65292;&#20135;&#29983;&#20102;&#39640;&#38454;&#21160;&#21147;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.07203</link><description>&lt;p&gt;
&#20851;&#20110;&#36229;&#22270;&#19978;&#19977;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#38750;&#32447;&#24615;&#24179;&#22343;&#21160;&#21147;&#23398;&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the convergence of nonlinear averaging dynamics with three-body interactions on hypergraphs. (arXiv:2304.07203v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#22270;&#19978;&#20855;&#26377;&#19977;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31163;&#25955;&#26102;&#38388;&#38750;&#32447;&#24615;&#24179;&#22343;&#21160;&#21147;&#23398;&#65292;&#22312;&#21021;&#24577;&#21644;&#36229;&#22270;&#25299;&#25169;&#20197;&#21450;&#26356;&#26032;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#19979;&#65292;&#20135;&#29983;&#20102;&#39640;&#38454;&#21160;&#21147;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#23398;&#12289;&#29983;&#29289;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#39046;&#22495;&#30340;&#22797;&#26434;&#32593;&#32476;&#31995;&#32479;&#36890;&#24120;&#28041;&#21450;&#36229;&#20986;&#31616;&#21333;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#30340;&#20132;&#20114;&#12290;&#36229;&#22270;&#20316;&#20026;&#25551;&#36848;&#21644;&#20998;&#26512;&#20855;&#26377;&#22810;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31995;&#32479;&#22797;&#26434;&#34892;&#20026;&#30340;&#24378;&#22823;&#24314;&#27169;&#24037;&#20855;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19977;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31163;&#25955;&#26102;&#38388;&#38750;&#32447;&#24615;&#24179;&#22343;&#21160;&#21147;&#23398;&#65306;&#24213;&#23618;&#36229;&#22270;&#30001;&#19977;&#20803;&#32452;&#20316;&#20026;&#36229;&#36793;&#30028;&#23450;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26500;&#65292;&#32780;&#39030;&#28857;&#36890;&#36807;&#21152;&#26435;&#30340;&#29366;&#24577;&#20381;&#36182;&#30340;&#37051;&#22495;&#23545;&#30340;&#29366;&#24577;&#24179;&#22343;&#26356;&#26032;&#20854;&#29366;&#24577;&#12290;&#30456;&#36739;&#20110;&#24102;&#26377;&#20108;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#22270;&#19978;&#30340;&#32447;&#24615;&#24179;&#22343;&#21160;&#21147;&#23398;&#65292;&#36825;&#20010;&#21160;&#21147;&#23398;&#19981;&#20250;&#25910;&#25947;&#21040;&#21021;&#22987;&#29366;&#24577;&#30340;&#24179;&#22343;&#20540;&#65292;&#32780;&#26159;&#20943;&#23569;&#20102;&#21021;&#24577;&#21644;&#36229;&#22270;&#25299;&#25169;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#21644;&#26356;&#26032;&#30340;&#38750;&#32447;&#24615;&#20135;&#29983;&#39640;&#38454;&#21160;&#21147;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex networked systems in fields such as physics, biology, and social sciences often involve interactions that extend beyond simple pairwise ones. Hypergraphs serve as powerful modeling tools for describing and analyzing the intricate behaviors of systems with multi-body interactions. Herein, we investigate a discrete-time nonlinear averaging dynamics with three-body interactions: an underlying hypergraph, comprising triples as hyperedges, delineates the structure of these interactions, while the vertices update their states through a weighted, state-dependent average of neighboring pairs' states. This dynamics captures reinforcing group effects, such as peer pressure, and exhibits higher-order dynamical effects resulting from a complex interplay between initial states, hypergraph topology, and nonlinearity of the update. Differently from linear averaging dynamics on graphs with two-body interactions, this model does not converge to the average of the initial states but rather induc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;BPM&#30740;&#31350;&#24102;&#26469;&#35832;&#22810;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.07183</link><description>&lt;p&gt;
Just Tell Me: &#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Just Tell Me: Prompt Engineering in Business Process Management. (arXiv:2304.07183v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;BPM&#30740;&#31350;&#24102;&#26469;&#35832;&#22810;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3&#21644;&#20854;&#20182;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#20063;&#22312;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#65288;BPM&#65289;&#39046;&#22495;&#25104;&#21151;&#24212;&#29992;&#65292;&#20363;&#22914;&#29992;&#20110;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#21644;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#23545;&#25152;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#20013;&#21253;&#25324;&#22823;&#37327;&#21512;&#36866;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#35748;&#35782;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35748;&#20026;&#25552;&#31034;&#24037;&#31243;&#21487;&#20197;&#24110;&#21161;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24341;&#20837;BPM&#30740;&#31350;&#12290;&#26412;&#31687;&#25991;&#31456;&#21033;&#29992;&#36825;&#19968;&#35266;&#28857;&#65292;&#36890;&#36807;&#30830;&#23450;&#30456;&#20851;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#20026;BPM&#30740;&#31350;&#30340;&#25552;&#31034;&#24037;&#31243;&#20351;&#29992;&#21046;&#23450;&#30740;&#31350;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3 and several other language models (LMs) can effectively address various natural language processing (NLP) tasks, including machine translation and text summarization. Recently, they have also been successfully employed in the business process management (BPM) domain, e.g., for predictive process monitoring and process extraction from text. This, however, typically requires fine-tuning the employed LM, which, among others, necessitates large amounts of suitable training data. A possible solution to this problem is the use of prompt engineering, which leverages pre-trained LMs without fine-tuning them. Recognizing this, we argue that prompt engineering can help bring the capabilities of LMs to BPM research. We use this position paper to develop a research agenda for the use of prompt engineering for BPM research by identifying the associated potentials and challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#22826;&#38451;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#32454;&#33410;&#26041;&#38754;&#34920;&#29616;&#27604;StyleGAN&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.07169</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#22826;&#38451;&#35266;&#27979;&#25104;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study on Generative Models for High Resolution Solar Observation Imaging. (arXiv:2304.07169v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#39640;&#20998;&#36776;&#29575;&#22826;&#38451;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#32454;&#33410;&#26041;&#38754;&#34920;&#29616;&#27604;StyleGAN&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#27963;&#21160;&#26159;&#25105;&#20204;&#22826;&#38451;&#31995;&#21464;&#21270;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#65292;&#20063;&#26159;&#24433;&#21709;&#22320;&#29699;&#21644;&#36817;&#22320;&#31354;&#38388;&#30340;&#31354;&#38388;&#22825;&#27668;&#29616;&#35937;&#30340;&#20851;&#38190;&#26469;&#28304;&#12290;&#26469;&#33258;&#22826;&#38451;&#21160;&#21147;&#23398;&#21488;&#31449;&#65288;SDO&#65289;&#30340;&#39640;&#20998;&#36776;&#29575;&#26497;&#32043;&#22806;&#32447;&#35266;&#27979;&#65288;EUV&#65289;&#35760;&#24405;&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#24222;&#22823;&#22826;&#38451;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#20840;&#38754;&#25968;&#25454;&#38598;&#26469;&#30740;&#31350;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20197;&#20934;&#30830;&#25429;&#25417;&#35266;&#23519;&#21040;&#30340;&#22826;&#38451;&#27963;&#21160;&#29366;&#24577;&#32972;&#21518;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#20174;&#22522;&#20110;StyleGAN&#30340;&#26041;&#27861;&#24320;&#22987;&#65292;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#26679;&#26412;&#26102;&#22788;&#29702;&#22826;&#38451;&#22270;&#20687;&#30340;&#32454;&#33410;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65292;&#32780;&#22788;&#29702;&#33258;&#28982;&#38754;&#37096;&#22270;&#20687;&#26102;&#21017;&#30456;&#21453;&#12290;&#24403;&#36716;&#25442;&#20026;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#26063;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23545;&#32454;&#33410;&#29983;&#25104;&#30340;&#26126;&#26174;&#25913;&#36827;&#12290;&#23545;&#20110;GAN&#26063;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#29306;&#29298;&#19968;&#20123;&#36739;&#22823;&#27604;&#20363;&#30340;&#32454;&#33410;&#24773;&#20917;&#19979;&#23454;&#29616;&#31867;&#20284;&#30340;&#32454;&#33410;&#29983;&#25104;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#35270;&#35273;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#26088;&#22312;&#35780;&#20272;&#20854;&#20316;&#20026;&#22826;&#38451;&#35266;&#27979;&#22270;&#20687;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#30340;&#19968;&#20123;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solar activity is one of the main drivers of variability in our solar system and the key source of space weather phenomena that affect Earth and near Earth space. The extensive record of high resolution extreme ultraviolet (EUV) observations from the Solar Dynamics Observatory (SDO) offers an unprecedented, very large dataset of solar images. In this work, we make use of this comprehensive dataset to investigate capabilities of current state-of-the-art generative models to accurately capture the data distribution behind the observed solar activity states. Starting from StyleGAN-based methods, we uncover severe deficits of this model family in handling fine-scale details of solar images when training on high resolution samples, contrary to training on natural face images. When switching to the diffusion based generative model family, we observe strong improvements of fine-scale detail generation. For the GAN family, we are able to achieve similar improvements in fine-scale generation wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22522;&#20110;Bandit&#26041;&#27861;&#23558;&#22806;&#37096;&#24314;&#35758;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07163</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;Bandit&#26041;&#27861;&#30340;&#26174;&#24335;&#22609;&#24418;&#22806;&#37096;&#24314;&#35758;&#31639;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning. (arXiv:2304.07163v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22522;&#20110;Bandit&#26041;&#27861;&#23558;&#22806;&#37096;&#24314;&#35758;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#22806;&#37096;&#25110;&#19987;&#23478;&#30340;&#24314;&#35758;&#34701;&#20837;&#21040;&#23398;&#20064;&#24403;&#20013;&#12290;&#26412;&#25991;&#23558;&#23558;&#23558;&#27492;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#31181;&#22810;&#33218;&#36172;&#21338;&#26426;&#31216;&#20026;&#22609;&#24418;&#36172;&#21338;&#26426;&#65288;shaping-bandits&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;LQR&#21644;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19977;&#31181;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38543;&#26426;&#35299;&#37322;&#22120;&#21644;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25361;&#36873;&#26377;&#24847;&#20041;&#30340;&#23376;&#22270;&#26469;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#32467;&#26524;&#12290;&#19982;&#26631;&#20934;&#23376;&#22270;&#25552;&#21462;&#31574;&#30053;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#20135;&#29983;&#30340;&#23376;&#22270;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#21487;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#35299;&#37322;&#30340;&#38468;&#21152;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.07152</link><description>&lt;p&gt;
&#32467;&#21512;&#38543;&#26426;&#35299;&#37322;&#22120;&#21644;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Combining Stochastic Explainers and Subgraph Neural Networks can Increase Expressivity and Interpretability. (arXiv:2304.07152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#38543;&#26426;&#35299;&#37322;&#22120;&#21644;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25361;&#36873;&#26377;&#24847;&#20041;&#30340;&#23376;&#22270;&#26469;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#32467;&#26524;&#12290;&#19982;&#26631;&#20934;&#23376;&#22270;&#25552;&#21462;&#31574;&#30053;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#20135;&#29983;&#30340;&#23376;&#22270;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#21487;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#35299;&#37322;&#30340;&#38468;&#21152;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#22270;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNN&#65289;&#21487;&#20197;&#22686;&#24378;&#26631;&#20934;&#30340;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#23478;&#26063;&#23558;&#27599;&#20010;&#22270;&#34920;&#31034;&#20026;&#19968;&#32452;&#23376;&#22270;&#65292;&#36890;&#24120;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#25110;&#25163;&#24037;&#21551;&#21457;&#24335;&#26041;&#27861;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#36890;&#36807;&#36873;&#25321;&#8220;&#26377;&#24847;&#20041;&#8221;&#30340;&#23376;&#22270;&#65292;&#38500;&#20102;&#25552;&#39640;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#22806;&#65292;&#36824;&#21487;&#20197;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#39044;&#27979;&#22270;&#30340;&#31867;&#21035;&#21644;&#19968;&#32452;&#35299;&#37322;&#24615;&#31232;&#30095;&#23376;&#22270;&#65292;&#21487;&#20197;&#20998;&#26512;&#36825;&#20123;&#23376;&#22270;&#26469;&#29702;&#35299;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#26631;&#20934;&#23376;&#22270;&#25552;&#21462;&#31574;&#30053;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#22914;&#38543;&#26426;&#33410;&#28857;/&#36793;&#32536;&#21024;&#38500;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20135;&#29983;&#30340;&#23376;&#22270;&#20801;&#35768;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#35299;&#37322;&#30340;&#38468;&#21152;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subgraph-enhanced graph neural networks (SGNN) can increase the expressive power of the standard message-passing framework. This model family represents each graph as a collection of subgraphs, generally extracted by random sampling or with hand-crafted heuristics. Our key observation is that by selecting "meaningful" subgraphs, besides improving the expressivity of a GNN, it is also possible to obtain interpretable results. For this purpose, we introduce a novel framework that jointly predicts the class of the graph and a set of explanatory sparse subgraphs, which can be analyzed to understand the decision process of the classifier. We compare the performance of our framework against standard subgraph extraction policies, like random node/edge deletion strategies. The subgraphs produced by our framework allow to achieve comparable performance in terms of accuracy, with the additional benefit of providing explanations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#21644;&#20840;&#31471;&#21040;&#31471;&#23398;&#20064;&#26469;&#23454;&#29616;&#31995;&#32479;&#20248;&#21270;&#21487;&#20877;&#29983;&#33021;&#28304;&#21363;&#26102;&#39044;&#25253;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#20840;&#22825;&#20505;&#22270;&#20687;&#21644;&#27668;&#35937;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#26377;&#25928;&#32452;&#21512;&#39044;&#27979;&#20540;&#65292;&#24182;&#29992;&#20110;&#23454;&#29616;&#20248;&#21270;&#30005;&#21147;&#27969;&#20844;&#24335;&#26469;&#27169;&#25311;&#33021;&#28304;&#31649;&#29702;&#12290;&#35813;&#26041;&#27861;&#39318;&#27425;&#32467;&#21512;&#22810;&#27169;&#24577;&#21644;&#20840;&#31471;&#21040;&#31471;&#23398;&#20064;&#36827;&#34892;&#22521;&#35757;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#21270;&#39044;&#26399;&#24635;&#20307;&#31995;&#32479;&#25104;&#26412;&#30340;&#30446;&#30340;&#12290;&#20351;&#29992;&#33655;&#20848;&#30495;&#23454;&#30340;&#22825;&#31354;&#21644;&#27668;&#35937;&#25968;&#25454;&#36827;&#34892;&#30340;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MM-E2E&#27169;&#22411;&#23558;&#31995;&#32479;&#25104;&#26412;&#19982;&#21333;&#27169;&#24335;&#22522;&#32447;&#30456;&#27604;&#38477;&#20302;&#20102;30&#65285;&#12290;</title><link>http://arxiv.org/abs/2304.07151</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#20840;&#31471;&#21040;&#31471;&#31995;&#32479;&#20248;&#21270;&#21487;&#20877;&#29983;&#33021;&#28304;&#21363;&#26102;&#39044;&#25253;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-End Learning with Multiple Modalities for System-Optimised Renewables Nowcasting. (arXiv:2304.07151v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#21644;&#20840;&#31471;&#21040;&#31471;&#23398;&#20064;&#26469;&#23454;&#29616;&#31995;&#32479;&#20248;&#21270;&#21487;&#20877;&#29983;&#33021;&#28304;&#21363;&#26102;&#39044;&#25253;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#20840;&#22825;&#20505;&#22270;&#20687;&#21644;&#27668;&#35937;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#26377;&#25928;&#32452;&#21512;&#39044;&#27979;&#20540;&#65292;&#24182;&#29992;&#20110;&#23454;&#29616;&#20248;&#21270;&#30005;&#21147;&#27969;&#20844;&#24335;&#26469;&#27169;&#25311;&#33021;&#28304;&#31649;&#29702;&#12290;&#35813;&#26041;&#27861;&#39318;&#27425;&#32467;&#21512;&#22810;&#27169;&#24577;&#21644;&#20840;&#31471;&#21040;&#31471;&#23398;&#20064;&#36827;&#34892;&#22521;&#35757;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#21270;&#39044;&#26399;&#24635;&#20307;&#31995;&#32479;&#25104;&#26412;&#30340;&#30446;&#30340;&#12290;&#20351;&#29992;&#33655;&#20848;&#30495;&#23454;&#30340;&#22825;&#31354;&#21644;&#27668;&#35937;&#25968;&#25454;&#36827;&#34892;&#30340;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MM-E2E&#27169;&#22411;&#23558;&#31995;&#32479;&#25104;&#26412;&#19982;&#21333;&#27169;&#24335;&#22522;&#32447;&#30456;&#27604;&#38477;&#20302;&#20102;30&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39118;&#33021;&#21644;&#22826;&#38451;&#33021;&#31561;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#26222;&#21450;&#65292;&#20934;&#30830;&#30340;&#30701;&#26399;&#21487;&#20877;&#29983;&#33021;&#28304;&#39044;&#27979;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#22810;&#27169;&#24335;&#65288;MM&#65289;&#23398;&#20064;&#21644;&#20840;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23454;&#29616;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#26102;&#20351;&#29992;&#20013;&#38388;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#26041;&#27861;&#12290;MM&#23558;&#20840;&#22825;&#20505;&#22270;&#20687;&#21644;&#27668;&#35937;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#20316;&#20026;&#20004;&#31181;&#27169;&#24577;&#26469;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#20135;&#29983;&#65292;&#20197;&#27492;&#23454;&#29616;&#39044;&#27979;&#20540;&#30340;&#26377;&#25928;&#32452;&#21512;&#12290;&#28982;&#21518;&#65292;&#23558;&#32452;&#21512;&#21518;&#30340;&#39044;&#27979;&#20540;&#20316;&#20026;&#19981;&#21516;iable&#20248;&#21270;&#30005;&#21147;&#27969;&#65288;OPF&#65289;&#20844;&#24335;&#30340;&#36755;&#20837;&#65292;&#27169;&#25311;&#33021;&#28304;&#31649;&#29702;&#12290;&#39318;&#27425;&#20351;&#29992;MM&#32467;&#21512;E2E&#27169;&#22411;&#36827;&#34892;&#22521;&#35757;&#65292;&#20197;&#36798;&#21040;&#26368;&#23567;&#21270;&#39044;&#26399;&#24635;&#20307;&#31995;&#32479;&#25104;&#26412;&#30340;&#30446;&#30340;&#12290;&#26696;&#20363;&#30740;&#31350;&#37319;&#29992;&#33655;&#20848;&#30495;&#23454;&#30340;&#22825;&#31354;&#21644;&#27668;&#35937;&#25968;&#25454;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;MM-E2E&#27169;&#22411;&#23558;&#31995;&#32479;&#25104;&#26412;&#19982;&#21333;&#27169;&#24335;&#22522;&#32447;&#30456;&#27604;&#38477;&#20302;&#20102;30&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing penetration of renewable power sources such as wind and solar, accurate short-term, nowcasting renewable power prediction is becoming increasingly important. This paper investigates the multi-modal (MM) learning and end-to-end (E2E) learning for nowcasting renewable power as an intermediate to energy management systems. MM combines features from all-sky imagery and meteorological sensor data as two modalities to predict renewable power generation that otherwise could not be combined effectively. The combined, predicted values are then input to a differentiable optimal power flow (OPF) formulation simulating the energy management. For the first time, MM is combined with E2E training of the model that minimises the expected total system cost. The case study tests the proposed methodology on the real sky and meteorological data from the Netherlands. In our study, the proposed MM-E2E model reduced system cost by 30% compared to uni-modal baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#30340;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#20840;&#36523;PET&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21464;&#25442;&#22120;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#23454;&#29616;&#20102;&#39640;&#25928;&#20934;&#30830;&#22320;&#23398;&#20064;&#22120;&#23448;&#38388;&#21644;&#20854;&#25104;&#20687;&#27169;&#24335;&#30340;&#38271;&#31243;&#20132;&#20114;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07147</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#20840;&#36523;PET&#24322;&#24120;&#26816;&#27979;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cross Attention Transformers for Multi-modal Unsupervised Whole-Body PET Anomaly Detection. (arXiv:2304.07147v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#30340;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#20840;&#36523;PET&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21464;&#25442;&#22120;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#23454;&#29616;&#20102;&#39640;&#25928;&#20934;&#30830;&#22320;&#23398;&#20064;&#22120;&#23448;&#38388;&#21644;&#20854;&#25104;&#20687;&#27169;&#24335;&#30340;&#38271;&#31243;&#20132;&#20114;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#19968;&#31181;&#39640;&#24230;&#24322;&#36136;&#24615;&#30340;&#30142;&#30149;&#65292;&#21487;&#20197;&#22312;&#20154;&#20307;&#30340;&#20960;&#20046;&#20219;&#20309;&#37096;&#20301;&#21457;&#29983;&#12290;18F-&#27679;&#33073;&#27687;&#33889;&#33796;&#31958;&#26159;&#19968;&#31181;&#24120;&#29992;&#20110;&#26816;&#27979;&#30284;&#30151;&#30340;&#25104;&#20687;&#27169;&#24335;&#65292;&#30001;&#20110;&#20854;&#39640;&#28789;&#25935;&#24230;&#21644;&#20195;&#35874;&#27963;&#24615;&#27169;&#24335;&#30340;&#26126;&#26174;&#21487;&#35270;&#21270;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30001;&#20110;&#30284;&#30151;&#39640;&#24230;&#24322;&#36136;&#24615;&#65292;&#35757;&#32451;&#36890;&#29992;&#30340;&#21306;&#20998;&#24615;&#30284;&#30151;&#26816;&#27979;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#30142;&#30149;&#22797;&#26434;&#24615;&#24120;&#24120;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#38480;&#21046;&#24615;&#22240;&#32032;&#12290;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#20581;&#24247;&#32452;&#32455;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#39044;&#27979;&#19982;&#20581;&#24247;&#27491;&#24120;&#30340;&#20559;&#24046;&#26469;&#26816;&#27979;&#30284;&#30151;&#65292;&#36825;&#38656;&#35201;&#20855;&#26377;&#39640;&#24230;&#34920;&#29616;&#21147;&#30340;&#22120;&#23448;&#20043;&#38388;&#21644;&#23427;&#20204;&#30340;&#25104;&#20687;&#27169;&#24335;&#20043;&#38388;&#20934;&#30830;&#23398;&#20064;&#38271;&#31243;&#20132;&#20114;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#24449;&#36890;&#36807;&#21464;&#25442;&#22120;&#24471;&#21040;&#20102;&#28385;&#36275;&#65292;&#21464;&#25442;&#22120;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#35757;&#32451;&#27491;&#24120;&#30340;PET&#25104;&#20687;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cancer is a highly heterogeneous condition that can occur almost anywhere in the human body. 18F-fluorodeoxyglucose is an imaging modality commonly used to detect cancer due to its high sensitivity and clear visualisation of the pattern of metabolic activity. Nonetheless, as cancer is highly heterogeneous, it is challenging to train general-purpose discriminative cancer detection models, with data availability and disease complexity often cited as a limiting factor. Unsupervised anomaly detection models have been suggested as a putative solution. These models learn a healthy representation of tissue and detect cancer by predicting deviations from the healthy norm, which requires models capable of accurately learning long-range interactions between organs and their imaging patterns with high levels of expressivity. Such characteristics are suitably satisfied by transformers, which have been shown to generate state-of-the-art results in unsupervised anomaly detection by training on norma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#20449;&#21495;&#38271;&#24230;&#20998;&#24067;&#65292;&#37319;&#29992;&#29305;&#23450;&#30340;&#35757;&#32451;&#20449;&#21495;&#38271;&#24230;&#38480;&#21046;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07142</link><description>&lt;p&gt;
&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Data Sampling Strategies for Training Neural Network Speech Separation Models. (arXiv:2304.07142v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#30340;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#20449;&#21495;&#38271;&#24230;&#20998;&#24067;&#65292;&#37319;&#29992;&#29305;&#23450;&#30340;&#35757;&#32451;&#20449;&#21495;&#38271;&#24230;&#38480;&#21046;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#20998;&#31163;&#20173;&#28982;&#26159;&#22810;&#35828;&#35805;&#20449;&#21495;&#22788;&#29702;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#22312;&#35768;&#22810;&#35821;&#38899;&#20998;&#31163;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#19968;&#20123;&#27169;&#22411;&#38656;&#35201;&#36739;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;&#36739;&#39640;&#30340;&#20869;&#23384;&#38656;&#27714;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#32553;&#30701;&#35757;&#32451;&#31034;&#20363;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#36825;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#24212;&#29992;&#36825;&#20123;&#35757;&#32451;&#20449;&#21495;&#38271;&#24230;&#65288;TSL&#65289;&#38480;&#21046;&#23545;&#20004;&#20010;&#35821;&#38899;&#20998;&#31163;&#27169;&#22411;&#65288;SepFormer&#65292;&#19968;&#20010;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21644;Conv-TasNet&#65292;&#19968;&#20010;&#21367;&#31215;&#27169;&#22411;&#65289;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;WJS0-2Mix&#65292;WHAMR&#21644;Libri2Mix&#25968;&#25454;&#38598;&#26469;&#20998;&#26512;&#20449;&#21495;&#38271;&#24230;&#20998;&#24067;&#21450;&#20854;&#23545;&#35757;&#32451;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#20998;&#24067;&#65292;&#24212;&#29992;&#29305;&#23450;&#30340;TSL&#38480;&#21046;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#23545;&#27874;&#24418;&#36215;&#22987;&#32034;&#24341;&#36827;&#34892;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#26356;&#22810;&#29420;&#29305;&#30340;&#31034;&#20363;&#29992;&#20110;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech separation remains an important area of multi-speaker signal processing. Deep neural network (DNN) models have attained the best performance on many speech separation benchmarks. Some of these models can take significant time to train and have high memory requirements. Previous work has proposed shortening training examples to address these issues but the impact of this on model performance is not yet well understood. In this work, the impact of applying these training signal length (TSL) limits is analysed for two speech separation models: SepFormer, a transformer model, and Conv-TasNet, a convolutional model. The WJS0-2Mix, WHAMR and Libri2Mix datasets are analysed in terms of signal length distribution and its impact on training efficiency. It is demonstrated that, for specific distributions, applying specific TSL limits results in better performance. This is shown to be mainly due to randomly sampling the start index of the waveforms resulting in more unique examples for tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#28857;&#20113;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#36827;&#34892;&#34920;&#38754;&#20998;&#21106;&#27979;&#35797;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#21019;&#24314;&#20102;TUM-FA\c{C}ADE&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#20419;&#36827;&#22522;&#20110;&#28857;&#20113;&#30340;&#34920;&#38754;&#20998;&#21106;&#20219;&#21153;&#30340;&#24320;&#21457;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;&#22522;&#20934;&#31867;&#22411;&#65292;&#21019;&#24314;&#26356;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.07140</link><description>&lt;p&gt;
TUM-FA\c{C}ADE&#65306;&#29992;&#20110;&#34920;&#38754;&#20998;&#21106;&#30340;&#28857;&#20113;&#22522;&#20934;&#30340;&#35780;&#20272;&#21644;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
TUM-FA\c{C}ADE: Reviewing and enriching point cloud benchmarks for fa\c{c}ade segmentation. (arXiv:2304.07140v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#29616;&#26377;&#28857;&#20113;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#36827;&#34892;&#34920;&#38754;&#20998;&#21106;&#27979;&#35797;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#21019;&#24314;&#20102;TUM-FA\c{C}ADE&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#20419;&#36827;&#22522;&#20110;&#28857;&#20113;&#30340;&#34920;&#38754;&#20998;&#21106;&#20219;&#21153;&#30340;&#24320;&#21457;&#65292;&#24182;&#19988;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#29992;&#20110;&#20854;&#20182;&#22522;&#20934;&#31867;&#22411;&#65292;&#21019;&#24314;&#26356;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20113;&#25968;&#25454;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#22478;&#24066;&#21046;&#22270;&#26368;&#20339;&#25968;&#25454;&#31867;&#22411;&#20043;&#19968;&#12290;&#22240;&#27492;&#65292;&#28857;&#20113;&#25968;&#25454;&#38598;&#36890;&#24120;&#34987;&#29992;&#20110;&#21508;&#31181;&#22478;&#24066;&#35299;&#37322;&#26041;&#27861;&#30340;&#22522;&#20934;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#32773;&#25506;&#31350;&#20351;&#29992;&#28857;&#20113;&#22522;&#20934;&#26469;&#36827;&#34892;&#34920;&#38754;&#20998;&#21106;&#12290;&#31283;&#20581;&#30340;&#34920;&#38754;&#20998;&#21106;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#33539;&#22260;&#20174;&#27169;&#25311;&#33258;&#21160;&#39550;&#39542;&#21151;&#33021;&#21040;&#25991;&#21270;&#36951;&#20135;&#20445;&#25252;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20016;&#23500;&#29616;&#26377;&#28857;&#20113;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#26032;&#22686;&#20102;&#38754;&#30456;&#20851;&#31867;&#21035;&#65292;&#20197;&#20415;&#20110;&#36827;&#34892;&#34920;&#38754;&#20998;&#21106;&#27979;&#35797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22914;&#20309;&#39640;&#25928;&#22320;&#25193;&#23637;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#23427;&#20204;&#22312;&#34920;&#38754;&#20998;&#21106;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#26041;&#27861;&#21019;&#24314;&#20102;TUM-FA\c{C}ADE&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25193;&#23637;TUM-MLS-2016&#30340;&#21151;&#33021;&#12290;TUM-FA\c{C}ADE&#19981;&#20165;&#21487;&#20197;&#20419;&#36827;&#22522;&#20110;&#28857;&#20113;&#30340;&#34920;&#38754;&#20998;&#21106;&#20219;&#21153;&#30340;&#24320;&#21457;&#65292;&#32780;&#19988;&#25105;&#20204;&#20016;&#23500;&#28857;&#20113;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#22522;&#20934;&#31867;&#22411;&#65292;&#20026;&#22478;&#24066;&#35299;&#37322;&#26041;&#27861;&#21019;&#24314;&#26356;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Point clouds are widely regarded as one of the best dataset types for urban mapping purposes. Hence, point cloud datasets are commonly investigated as benchmark types for various urban interpretation methods. Yet, few researchers have addressed the use of point cloud benchmarks for fa\c{c}ade segmentation. Robust fa\c{c}ade segmentation is becoming a key factor in various applications ranging from simulating autonomous driving functions to preserving cultural heritage. In this work, we present a method of enriching existing point cloud datasets with fa\c{c}ade-related classes that have been designed to facilitate fa\c{c}ade segmentation testing. We propose how to efficiently extend existing datasets and comprehensively assess their potential for fa\c{c}ade segmentation. We use the method to create the TUM-FA\c{C}ADE dataset, which extends the capabilities of TUM-MLS-2016. Not only can TUM-FA\c{C}ADE facilitate the development of point-cloud-based fa\c{c}ade segmentation tasks, but our 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65288;XIL&#65289;&#26469;&#20462;&#27491;&#27169;&#22411;&#65292;&#20294;&#21516;&#26102;&#21457;&#29616;"&#19968;&#31181;&#35299;&#37322;&#19981;&#33021;&#36866;&#29992;&#20110;XIL"&#65292;&#24314;&#35758;&#32771;&#34385;&#22810;&#31181;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.07136</link><description>&lt;p&gt;
&#19968;&#31181;&#35299;&#37322;&#19981;&#33021;&#36866;&#29992;&#20110;XIL
&lt;/p&gt;
&lt;p&gt;
One Explanation Does Not Fit XIL. (arXiv:2304.07136v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65288;XIL&#65289;&#26469;&#20462;&#27491;&#27169;&#22411;&#65292;&#20294;&#21516;&#26102;&#21457;&#29616;"&#19968;&#31181;&#35299;&#37322;&#19981;&#33021;&#36866;&#29992;&#20110;XIL"&#65292;&#24314;&#35758;&#32771;&#34385;&#22810;&#31181;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#20135;&#29983;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#20294;&#21516;&#26102;&#20063;&#23384;&#22312;&#30528;&#24555;&#25463;&#23398;&#20064;&#21644;&#38169;&#35823;&#30456;&#20851;&#30340;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;XIL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#21453;&#39304;&#26469;&#20462;&#27491;&#27169;&#22411;&#12290;&#26412;&#25991;&#37325;&#28857;&#25506;&#35752;&#20102;&#35813;&#26694;&#26550;&#20013;&#20351;&#29992;&#30340;&#35299;&#37322;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#22810;&#20010;&#35299;&#37322;&#26041;&#27861;&#36827;&#34892;&#21516;&#26102;&#27169;&#22411;&#20462;&#27491;&#65292;&#20174;&#32780;&#21457;&#29616;"&#19968;&#31181;&#35299;&#37322;&#19981;&#33021;&#36866;&#29992;&#20110;XIL" &#65292;&#24182;&#24314;&#35758;&#22312;&#36890;&#36807;XIL&#36827;&#34892;&#27169;&#22411;&#20462;&#27491;&#26102;&#32771;&#34385;&#22810;&#31181;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current machine learning models produce outstanding results in many areas but, at the same time, suffer from shortcut learning and spurious correlations. To address such flaws, the explanatory interactive machine learning (XIL) framework has been proposed to revise a model by employing user feedback on a model's explanation. This work sheds light on the explanations used within this framework. In particular, we investigate simultaneous model revision through multiple explanation methods. To this end, we identified that \textit{one explanation does not fit XIL} and propose considering multiple ones when revising models via XIL.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;(RGDM)&#65292;&#21487;&#20197;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;(RL)&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#22312;&#19981;&#38656;&#35201;&#21487;&#24494;&#20998;&#30340;&#24341;&#23548;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#23545;&#29983;&#25104;&#30340;&#26679;&#26412;&#30340;&#26377;&#25928;&#25511;&#21046;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.07132</link><description>&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#24341;&#23548;&#25506;&#32034;&#23454;&#29616;&#21487;&#25511;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Controllable Diffusion Models via Reward-Guided Exploration. (arXiv:2304.07132v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;(RGDM)&#65292;&#21487;&#20197;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;(RL)&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#22312;&#19981;&#38656;&#35201;&#21487;&#24494;&#20998;&#30340;&#24341;&#23548;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#23545;&#29983;&#25104;&#30340;&#26679;&#26412;&#30340;&#26377;&#25928;&#25511;&#21046;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#26679;&#26412;&#30340;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#21435;&#22122;&#36807;&#31243;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#25193;&#25955;&#27169;&#22411;&#30340;&#21464;&#20307;&#24050;&#34987;&#25552;&#20986;&#65292;&#20197;&#20351;&#29983;&#25104;&#30340;&#26679;&#26412;&#33021;&#22815;&#34987;&#26377;&#25928;&#25511;&#21046;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#23558;&#25511;&#21046;&#20449;&#24687;&#20316;&#20026;&#22122;&#22768;&#36924;&#36817;&#22120;&#30340;&#36755;&#20837;&#65288;&#21363;&#26465;&#20214;&#34920;&#31034;&#65289;&#65292;&#35201;&#20040;&#22312;&#27979;&#35797;&#38454;&#27573;&#24341;&#20837;&#39044;&#20808;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#26469;&#25351;&#23548;Langevin&#21160;&#21147;&#23398;&#26397;&#21521;&#26465;&#20214;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#21069;&#19968;&#31181;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#25511;&#21046;&#20449;&#24687;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#26465;&#20214;&#34920;&#31034;&#30340;&#24773;&#20917;&#65292;&#32780;&#21518;&#19968;&#31181;&#26041;&#27861;&#21017;&#38656;&#35201;&#21487;&#24494;&#20998;&#30340;&#39044;&#35757;&#32451;&#36741;&#23548;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RGDM&#65288;&#22870;&#21169;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;&#25152;&#25552;&#20986;&#30340;&#35757;&#32451;&#26694;&#26550;&#23558;&#21152;&#26435;&#23545;&#25968;-&#20284;&#28982;&#30340;&#30446;&#26631;&#21644;&#26368;&#22823;&#29109;RL&#30446;&#26631;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#20165;&#20351;&#29992;&#38750;&#21487;&#24494;&#20998;&#30340;&#24341;&#23548;&#20449;&#21495;&#21363;&#21487;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RGDM&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
By formulating data samples' formation as a Markov denoising process, diffusion models achieve state-of-the-art performances in a collection of tasks. Recently, many variants of diffusion models have been proposed to enable controlled sample generation. Most of these existing methods either formulate the controlling information as an input (i.e.,: conditional representation) for the noise approximator, or introduce a pre-trained classifier in the test-phase to guide the Langevin dynamic towards the conditional goal. However, the former line of methods only work when the controlling information can be formulated as conditional representations, while the latter requires the pre-trained guidance classifier to be differentiable. In this paper, we propose a novel framework named RGDM (Reward-Guided Diffusion Model) that guides the training-phase of diffusion models via reinforcement learning (RL). The proposed training framework bridges the objective of weighted log-likelihood and maximum e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#32452;&#29305;&#24449;&#23545;&#20135;&#37327;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07111</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#30340;&#20998;&#32452;Shapley&#20540;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#21487;&#29992;&#20110;&#20135;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Grouping Shapley Value Feature Importances of Random Forests for explainable Yield Prediction. (arXiv:2304.07111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#20998;&#32452;&#29305;&#24449;&#23545;&#20135;&#37327;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20135;&#37327;&#39044;&#27979;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26377;&#21161;&#20110;&#20805;&#20998;&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#33021;&#22815;&#22312;&#21508;&#31181;&#20135;&#37327;&#39044;&#27979;&#26041;&#26696;&#20013;&#23454;&#29616;&#39640;&#31934;&#24230;&#12290;&#29992;&#20110;&#20135;&#37327;&#39044;&#27979;&#30340;&#25968;&#25454;&#26159;&#22797;&#26434;&#30340;&#65292;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#20998;&#32452;&#30340;&#36755;&#20837;&#29305;&#24449;&#21487;&#20197;&#31616;&#21270;&#29702;&#35299;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#21487;&#20197;&#36890;&#36807;&#37319;&#38598;&#29305;&#24449;&#30340;&#26102;&#38388;&#25110;&#20256;&#24863;&#22120;&#26469;&#23454;&#29616;&#20998;&#32452;&#12290;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#26041;&#27861;&#26159;&#36890;&#36807;Shapley values&#30340;&#21338;&#24328;&#35770;&#26041;&#27861;&#23454;&#29616;&#30340;&#12290;&#20026;&#20102;&#22788;&#29702;&#29305;&#24449;&#32452;&#65292;&#36890;&#24120;&#23558;&#35745;&#31639;&#20986;&#30340;Shapley values&#30456;&#21152;&#65292;&#24573;&#30053;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#29702;&#35770;&#38480;&#21046;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;Shapley values&#30340;&#27010;&#24565;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#26641;&#32467;&#26500;&#19978;&#39640;&#25928;&#35745;&#31639;&#23427;&#20204;&#30340;&#31639;&#27861;&#65292;&#20197;&#30452;&#25509;&#35745;&#31639;&#39044;&#23450;&#29305;&#24449;&#32452;&#30340;Shapley values&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability in yield prediction helps us fully explore the potential of machine learning models that are already able to achieve high accuracy for a variety of yield prediction scenarios. The data included for the prediction of yields are intricate and the models are often difficult to understand. However, understanding the models can be simplified by using natural groupings of the input features. Grouping can be achieved, for example, by the time the features are captured or by the sensor used to do so. The state-of-the-art for interpreting machine learning models is currently defined by the game-theoretic approach of Shapley values. To handle groups of features, the calculated Shapley values are typically added together, ignoring the theoretical limitations of this approach. We explain the concept of Shapley values directly computed for predefined groups of features and introduce an algorithm to compute them efficiently on tree structures. We provide a blueprint for designing swar
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;HLTPR@RWTH&#22242;&#38431;&#22312;DSTC9&#21644;DSTC10&#20013;&#20026;&#20219;&#21153;&#23548;&#21521;&#22411;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#25152;&#20570;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#20351;&#36873;&#25321;&#20219;&#21153;&#26356;&#26377;&#25928;&#29575;&#65292;&#22312;DSTC10&#20013;&#25552;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#36866;&#24212;&#29983;&#25104;&#22238;&#31572;&#30340;&#39118;&#26684;&#65292;&#20197;&#21450;&#25552;&#20986;&#20102;&#19968;&#20010;&#22024;&#26434;&#30340;&#36890;&#36947;&#27169;&#22411;&#26469;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22242;&#38431;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.07101</link><description>&lt;p&gt;
HLTPR@RWTH&#22312;DSTC9&#21644;DSTC10&#20013;&#30340;&#20219;&#21153;&#23548;&#21521;&#22411;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Task-oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9 and DSTC10. (arXiv:2304.07101v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;HLTPR@RWTH&#22242;&#38431;&#22312;DSTC9&#21644;DSTC10&#20013;&#20026;&#20219;&#21153;&#23548;&#21521;&#22411;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#25152;&#20570;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#20351;&#36873;&#25321;&#20219;&#21153;&#26356;&#26377;&#25928;&#29575;&#65292;&#22312;DSTC10&#20013;&#25552;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#36866;&#24212;&#29983;&#25104;&#22238;&#31572;&#30340;&#39118;&#26684;&#65292;&#20197;&#21450;&#25552;&#20986;&#20102;&#19968;&#20010;&#22024;&#26434;&#30340;&#36890;&#36947;&#27169;&#22411;&#26469;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22242;&#38431;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#25105;&#20204;&#22312;&#31532;9&#21644;&#31532;10&#27425;Dialog System Technology Challenges&#65288;DSTC9&#21644;DSTC10&#65289;&#20013;&#20026;&#23545;&#35805;&#22522;&#20110;&#25991;&#26723;&#30340;&#20219;&#21153;&#20316;&#20986;&#30340;&#36129;&#29486;&#12290;&#22312;&#20004;&#27425;&#36845;&#20195;&#20013;&#65292;&#20219;&#21153;&#30001;&#19977;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#65306;&#39318;&#20808;&#26816;&#27979;&#24403;&#21069;&#22238;&#21512;&#26159;&#21542;&#38656;&#35201;&#30693;&#35782;&#65292;&#20854;&#27425;&#36873;&#25321;&#30456;&#20851;&#30340;&#30693;&#35782;&#25991;&#26723;&#65292;&#31532;&#19977;&#29983;&#25104;&#22522;&#20110;&#25152;&#36873;&#25991;&#26723;&#30340;&#22238;&#31572;&#12290;&#23545;&#20110;DSTC9&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#20351;&#36873;&#25321;&#20219;&#21153;&#26356;&#26377;&#25928;&#29575;&#12290;&#20854;&#20013;&#26368;&#22909;&#30340;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#36873;&#25321;&#65292;&#23454;&#38469;&#19978;&#27604;&#21407;&#22987;&#22522;&#32447;&#25913;&#36827;&#20102;&#32467;&#26524;&#65292;&#24182;&#25552;&#39640;&#20102;24&#20493;&#36895;&#24230;&#12290;&#22312;DSTC10&#36845;&#20195;&#20013;&#65292;&#25361;&#25112;&#26159;&#35201;&#20351;&#32463;&#36807;&#20070;&#38754;&#23545;&#35805;&#35757;&#32451;&#30340;&#31995;&#32479;&#33021;&#22815;&#22312;&#22024;&#26434;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#36716;&#24405;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#36866;&#24212;&#29983;&#25104;&#22238;&#31572;&#30340;&#39118;&#26684;&#65292;&#20351;&#20854;&#19982;&#21069;&#26399;&#23545;&#35805;&#30456;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22024;&#26434;&#30340;&#36890;&#36947;&#27169;&#22411;&#26469;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper summarizes our contributions to the document-grounded dialog tasks at the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). In both iterations the task consists of three subtasks: first detect whether the current turn is knowledge seeking, second select a relevant knowledge document, and third generate a response grounded on the selected document. For DSTC9 we proposed different approaches to make the selection task more efficient. The best method, Hierarchical Selection, actually improves the results compared to the original baseline and gives a speedup of 24x. In the DSTC10 iteration of the task, the challenge was to adapt systems trained on written dialogs to perform well on noisy automatic speech recognition transcripts. Therefore, we proposed data augmentation techniques to increase the robustness of the models as well as methods to adapt the style of generated responses to fit well into the proceeding dialog. Additionally, we proposed a noisy channel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21152;&#26435;&#36830;&#20307;&#32593;&#32476;&#23545;&#36827;&#23637;&#24615;MCI&#24739;&#32773;&#36827;&#34892;&#24207;&#25968;&#20998;&#31867;&#65292;&#20197;&#39044;&#27979;&#20182;&#20204;&#36317;&#31163;&#20005;&#37325;AD&#38454;&#27573;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.07097</link><description>&lt;p&gt;
&#26435;&#37325;&#36830;&#20307;&#32593;&#32476;&#29992;&#20110;&#20174;MRI&#22270;&#20687;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21457;&#30149;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Weighted Siamese Network to Predict the Time to Onset of Alzheimer's Disease from MRI Images. (arXiv:2304.07097v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21152;&#26435;&#36830;&#20307;&#32593;&#32476;&#23545;&#36827;&#23637;&#24615;MCI&#24739;&#32773;&#36827;&#34892;&#24207;&#25968;&#20998;&#31867;&#65292;&#20197;&#39044;&#27979;&#20182;&#20204;&#36317;&#31163;&#20005;&#37325;AD&#38454;&#27573;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#30196;&#21574;&#30151;&#26368;&#24120;&#35265;&#30340;&#21407;&#22240;&#65292;&#26159;&#19968;&#31181;&#36827;&#34892;&#24615;&#30142;&#30149;&#65292;&#20250;&#20808;&#20986;&#29616;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#12290;&#26089;&#26399;&#26816;&#27979;&#23545;&#20110;&#20570;&#20986;&#27835;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#35745;&#31639;&#26426;&#36741;&#21161;&#26816;&#27979;AD&#30340;&#25991;&#29486;&#32858;&#28966;&#20110;&#23558;&#33041;&#22270;&#20687;&#20998;&#31867;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#20043;&#19968;&#65306;&#20581;&#24247;&#12289;MCI&#21644;AD&#65307;&#25110;&#23558;MCI&#24739;&#32773;&#20998;&#31867;&#20026;(1)&#36827;&#23637;&#65306;&#22312;&#32473;&#23450;&#30740;&#31350;&#26399;&#20869;&#30340;&#26410;&#26469;&#26816;&#26597;&#26102;&#38388;&#36827;&#23637;&#21040;AD&#30340;&#24739;&#32773;&#65292;&#20197;&#21450;(2)&#31283;&#23450;&#65306;&#25345;&#32493;&#20316;&#20026;MCI&#24739;&#32773;&#32780;&#20174;&#26410;&#36827;&#23637;&#20026;AD&#12290;&#36825;&#31181;&#26041;&#27861;&#38169;&#36807;&#20102;&#20934;&#30830;&#35782;&#21035;&#36827;&#23637;&#24615;MCI&#24739;&#32773;&#36712;&#36857;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;AD&#35782;&#21035;&#30340;&#33041;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#23558;&#20854;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#24207;&#25968;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#39044;&#27979;&#24739;&#32773;&#36317;&#31163;&#20005;&#37325;AD&#38454;&#27573;&#26377;&#22810;&#36817;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#35745;&#21010;&#65288;ADNI&#65289;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#36827;&#23637;&#24615;MCI&#24739;&#32773;&#65292;&#24182;&#20351;&#29992;&#21152;&#26435;&#36830;&#20307;&#32593;&#32476;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's Disease (AD), which is the most common cause of dementia, is a progressive disease preceded by Mild Cognitive Impairment (MCI). Early detection of the disease is crucial for making treatment decisions. However, most of the literature on computer-assisted detection of AD focuses on classifying brain images into one of three major categories: healthy, MCI, and AD; or categorising MCI patients into one of (1) progressive: those who progress from MCI to AD at a future examination time during a given study period, and (2) stable: those who stay as MCI and never progress to AD. This misses the opportunity to accurately identify the trajectory of progressive MCI patients. In this paper, we revisit the brain image classification task for AD identification and re-frame it as an ordinal classification task to predict how close a patient is to the severe AD stage. To this end, we select progressive MCI patients from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Delta&#21435;&#22122;&#20998;&#25968;&#65288;DDS&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20316;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#32534;&#36753;&#12289;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25439;&#22833;&#39033;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#26426;&#21046;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#24182;&#36890;&#36807;&#21305;&#37197;&#25552;&#31034;&#21644;&#22270;&#20687;&#26469;&#21435;&#38500;&#19981;&#24076;&#26395;&#30340;SDS&#38169;&#35823;&#26041;&#21521;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#32534;&#36753;&#30340;&#31934;&#24230;&#21644;&#28165;&#26224;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07090</link><description>&lt;p&gt;
Delta&#21435;&#22122;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Delta Denoising Score. (arXiv:2304.07090v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Delta&#21435;&#22122;&#20998;&#25968;&#65288;DDS&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20316;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#32534;&#36753;&#12289;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25439;&#22833;&#39033;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#26426;&#21046;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#65292;&#24182;&#36890;&#36807;&#21305;&#37197;&#25552;&#31034;&#21644;&#22270;&#20687;&#26469;&#21435;&#38500;&#19981;&#24076;&#26395;&#30340;SDS&#38169;&#35823;&#26041;&#21521;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#32534;&#36753;&#30340;&#31934;&#24230;&#21644;&#28165;&#26224;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Delta&#21435;&#22122;&#20998;&#25968;&#65288;DDS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#21487;&#20197;&#24341;&#23548;&#26368;&#23567;&#20462;&#25913;&#36755;&#20837;&#22270;&#20687;&#65292;&#20351;&#20854;&#26397;&#21521;&#30446;&#26631;&#25552;&#31034;&#25152;&#25551;&#36848;&#30340;&#20869;&#23481;&#12290;DDS&#21033;&#29992;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#20016;&#23500;&#29983;&#25104;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#21487;&#29992;&#20316;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25439;&#22833;&#39033;&#65292;&#20197;&#20415;&#23558;&#22270;&#20687;&#26397;&#30528;&#30001;&#25991;&#26412;&#25351;&#23450;&#30340;&#25152;&#38656;&#26041;&#21521;&#24341;&#23548;&#12290;DDS&#21033;&#29992;&#20102;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#26426;&#21046;&#26469;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#20351;&#29992;SDS&#36890;&#24120;&#20250;&#20135;&#29983;&#38750;&#35814;&#32454;&#21644;&#27169;&#31946;&#30340;&#36755;&#20986;&#65292;&#22240;&#20026;&#23384;&#22312;&#22122;&#22768;&#26799;&#24230;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;DDS&#20351;&#29992;&#19968;&#20010;&#19982;&#36755;&#20837;&#22270;&#20687;&#21305;&#37197;&#30340;&#25552;&#31034;&#26469;&#35782;&#21035;&#21644;&#21435;&#38500;&#19981;&#24076;&#26395;&#30340;SDS&#38169;&#35823;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21069;&#25552;&#26159;&#65292;&#24403;&#35745;&#31639;&#21305;&#37197;&#25552;&#31034;&#21644;&#22270;&#20687;&#30340;&#23545;&#26102;&#65292;SDS&#24212;&#20026;&#38646;&#65292;&#36825;&#24847;&#21619;&#30528;&#22914;&#26524;&#20998;&#25968;&#38750;&#38646;&#65292;&#21017;&#20854;&#26799;&#24230;&#21487;&#20197;&#24402;&#22240;&#20110;SDS&#30340;&#38169;&#35823;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#26126;&#20102;DDS&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32534;&#36753;&#19978;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Delta Denoising Score (DDS), a novel scoring function for text-based image editing that guides minimal modifications of an input image towards the content described in a target prompt. DDS leverages the rich generative prior of text-to-image diffusion models and can be used as a loss term in an optimization problem to steer an image towards a desired direction dictated by a text. DDS utilizes the Score Distillation Sampling (SDS) mechanism for the purpose of image editing. We show that using only SDS often produces non-detailed and blurry outputs due to noisy gradients. To address this issue, DDS uses a prompt that matches the input image to identify and remove undesired erroneous directions of SDS. Our key premise is that SDS should be zero when calculated on pairs of matched prompts and images, meaning that if the score is non-zero, its gradients can be attributed to the erroneous component of SDS. Our analysis demonstrates the competence of DDS for text based image-to-i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#29983;&#25104;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#31181;&#26465;&#20214;&#26041;&#27861;&#26469;&#20445;&#35777;&#29983;&#25104;&#30340;&#22270;&#20687;&#22312;&#36866;&#24403;&#30340;&#20301;&#32622;&#65292;&#24182;&#20855;&#26377;&#19968;&#33268;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#33410;&#30465;&#20869;&#23384;&#65292;&#26356;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/2304.07087</link><description>&lt;p&gt;
&#22522;&#20110;&#34917;&#19969;&#29983;&#25104;&#30340;&#20869;&#23384;&#26377;&#25928;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Memory Efficient Diffusion Probabilistic Models via Patch-based Generation. (arXiv:2304.07087v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#29983;&#25104;&#30340;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#31181;&#26465;&#20214;&#26041;&#27861;&#26469;&#20445;&#35777;&#29983;&#25104;&#30340;&#22270;&#20687;&#22312;&#36866;&#24403;&#30340;&#20301;&#32622;&#65292;&#24182;&#20855;&#26377;&#19968;&#33268;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#33410;&#30465;&#20869;&#23384;&#65292;&#26356;&#36866;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#37117;&#26159;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#38656;&#35201;&#36807;&#22810;&#30340;&#20869;&#23384;&#65292;&#22240;&#27492;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#19981;&#22826;&#23454;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34917;&#19969;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#34917;&#19969;&#29983;&#25104;&#30340;&#26465;&#20214;&#26041;&#27861;&#65306;&#31532;&#19968;&#31181;&#26159;&#20351;&#29992;&#21333;&#28909;&#34920;&#31034;&#30340;&#20301;&#32622;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#34917;&#19969;&#22312;&#36866;&#24403;&#30340;&#20301;&#32622;&#65307;&#31532;&#20108;&#31181;&#26159;&#20840;&#23616;&#20869;&#23481;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#25340;&#25509;&#22312;&#19968;&#36215;&#30340;&#34917;&#19969;&#20855;&#26377;&#19968;&#33268;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#22312;CelebA&#21644;LSUN&#24202;&#19978;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have been successful in generating high-quality and diverse images. However, traditional models, whose input and output are high-resolution images, suffer from excessive memory requirements, making them less practical for edge devices. Previous approaches for generative adversarial networks proposed a patch-based method that uses positional encoding and global content information. Nevertheless, designing a patch-based approach for diffusion probabilistic models is non-trivial. In this paper, we resent a diffusion probabilistic model that generates images on a patch-by-patch basis. We propose two conditioning methods for a patch-based generation. First, we propose position-wise conditioning using one-hot representation to ensure patches are in proper positions. Second, we propose Global Content Conditioning (GCC) to ensure patches have coherent content when concatenated together. We evaluate our model qualitatively and quantitatively on CelebA and LSUN bed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23569;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24182;&#36755;&#20986;&#34913;&#37327;&#20540;&#30340;&#36710;&#36742;&#33021;&#25928;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#38477;&#20302;&#30899;&#36275;&#36857;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07073</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#36710;&#36742;&#33021;&#25928;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Vehicle Energy Efficiency Prediction using an Ensemble of Neural Networks. (arXiv:2304.07073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20943;&#23569;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24182;&#36755;&#20986;&#34913;&#37327;&#20540;&#30340;&#36710;&#36742;&#33021;&#25928;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#38477;&#20302;&#30899;&#36275;&#36857;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#36755;&#37096;&#38376;&#32422;&#21344;&#20840;&#29699;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;25&#65285;&#12290;&#22240;&#27492;&#65292;&#22312;&#20132;&#36890;&#37096;&#38376;&#25552;&#39640;&#33021;&#25928;&#26159;&#20943;&#23569;&#30899;&#36275;&#36857;&#30340;&#20851;&#38190;&#12290;&#33021;&#25928;&#36890;&#24120;&#20197;&#27599;&#34892;&#39542;&#36317;&#31163;&#30340;&#33021;&#28304;&#28040;&#32791;&#26469;&#34913;&#37327;&#65292;&#20363;&#22914;&#27599;&#20844;&#37324;&#30340;&#29123;&#27833;&#21319;&#25968;&#12290;&#24433;&#21709;&#33021;&#25928;&#30340;&#20027;&#35201;&#22240;&#32032;&#21253;&#25324;&#36710;&#36742;&#31867;&#22411;&#65292;&#29615;&#22659;&#65292;&#39550;&#39542;&#21592;&#34892;&#20026;&#21644;&#22825;&#27668;&#26465;&#20214;&#12290;&#36825;&#20123;&#19981;&#21516;&#30340;&#22240;&#32032;&#24341;&#20837;&#20102;&#20272;&#35745;&#36710;&#36742;&#33021;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24182;&#36755;&#20986;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#34913;&#37327;&#20540;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20844;&#24320;&#21487;&#24471;&#30340;&#36710;&#36742;&#33021;&#28304;&#25968;&#25454;&#38598;&#65288;VED&#65289;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#19982;&#27599;&#36742;&#36710;&#21644;&#33021;&#28304;&#31867;&#22411;&#30340;&#20960;&#20010;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#36755;&#20986;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#34913;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transportation sector accounts for about 25% of global greenhouse gas emissions. Therefore, an improvement of energy efficiency in the traffic sector is crucial to reducing the carbon footprint. Efficiency is typically measured in terms of energy use per traveled distance, e.g. liters of fuel per kilometer. Leading factors that impact the energy efficiency are the type of vehicle, environment, driver behavior, and weather conditions. These varying factors introduce uncertainty in estimating the vehicles' energy efficiency. We propose in this paper an ensemble learning approach based on deep neural networks (ENN) that is designed to reduce the predictive uncertainty and to output measures of such uncertainty. We evaluated it using the publicly available Vehicle Energy Dataset (VED) and compared it with several baselines per vehicle and energy type. The results showed a high predictive performance and they allowed to output a measure of predictive uncertainty.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30446;&#26631;&#23548;&#21521;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36798;&#21040;&#39044;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#20943;&#23569;&#26469;&#23454;&#29616;&#20174;&#21208;&#25506;&#21040;&#21033;&#29992;&#30340;&#36716;&#25442;&#65292;&#19982;&#26631;&#20934;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07070</link><description>&lt;p&gt;
&#22522;&#20110;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30446;&#26631;&#23548;&#21521;&#35757;&#32451;: &#8220;&#35841;&#26089;&#36864;&#65292;&#35841;&#36755;&#8221;
&lt;/p&gt;
&lt;p&gt;
Who breaks early, looses: goal oriented training of deep neural networks based on port Hamiltonian dynamics. (arXiv:2304.07070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30446;&#26631;&#23548;&#21521;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36798;&#21040;&#39044;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#20943;&#23569;&#26469;&#23454;&#29616;&#20174;&#21208;&#25506;&#21040;&#21033;&#29992;&#30340;&#36716;&#25442;&#65292;&#19982;&#26631;&#20934;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20316;&#20026;&#21442;&#25968;&#20989;&#25968;&#30340;&#39640;&#24230;&#32467;&#26500;&#21270;&#33021;&#28304;&#26223;&#35266;&#65292;&#22240;&#27492;&#38656;&#35201;&#20351;&#29992;&#31934;&#23494;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#21457;&#29616;&#20445;&#35777;&#21512;&#29702;&#24615;&#33021;&#30340;&#65288;&#23616;&#37096;&#65289;&#26368;&#23567;&#20540;&#12290;&#26368;&#23567;&#21270;&#27425;&#20248;&#35299;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#21069;&#25552;&#65292;&#36890;&#24120;&#20351;&#29992;&#21160;&#37327;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#20687;&#20854;&#20182;&#38750;&#23616;&#37096;&#20248;&#21270;&#36807;&#31243;&#19968;&#26679;&#65292;&#36825;&#21019;&#24314;&#20102;&#22312;&#21208;&#25506;&#21644;&#21033;&#29992;&#20043;&#38388;&#24179;&#34913;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36798;&#21040;&#39044;&#23450;&#25439;&#22833;&#20989;&#25968;&#20943;&#23569;&#30340;&#20107;&#20214;&#25511;&#21046;&#26426;&#21046;&#65292;&#29992;&#20110;&#20174;&#21208;&#25506;&#36716;&#21521;&#21033;&#29992;&#12290;&#30001;&#20110;&#25105;&#20204;&#32473;&#20986;&#20102;&#21160;&#37327;&#27861;&#30340;&#21704;&#23494;&#39039;&#35299;&#37322;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#8220;&#24102;&#26377;&#25705;&#25830;&#30340;&#37325;&#29699;&#8221;&#35299;&#37322;&#65292;&#24182;&#22312;&#23454;&#29616;&#26576;&#20123;&#30446;&#26631;&#26102;&#35302;&#21457;&#8220;&#25705;&#25830;&#8221;&#25110;&#8220;&#26029;&#35010;&#8221;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26631;&#20934;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#24615;&#33021;&#30340;&#23454;&#39564;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The highly structured energy landscape of the loss as a function of parameters for deep neural networks makes it necessary to use sophisticated optimization strategies in order to discover (local) minima that guarantee reasonable performance. Overcoming less suitable local minima is an important prerequisite and often momentum methods are employed to achieve this. As in other non local optimization procedures, this however creates the necessity to balance between exploration and exploitation. In this work, we suggest an event based control mechanism for switching from exploration to exploitation based on reaching a predefined reduction of the loss function. As we give the momentum method a port Hamiltonian interpretation, we apply the 'heavy ball with friction' interpretation and trigger breaking (or friction) when achieving certain goals. We benchmark our method against standard stochastic gradient descent and provide experimental evidence for improved performance of deep neural netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07063</link><description>&lt;p&gt;
&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Existential First Order Queries Inference on Knowledge Graphs. (arXiv:2304.07063v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#24615;&#19968;&#38454;&#26597;&#35810;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#35266;&#23519;&#21040;&#30340;&#20449;&#24687;&#26469;&#39044;&#27979;&#32570;&#22833;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#22320;&#65292;&#22238;&#31572;&#19968;&#38454;&#36923;&#36753;&#20844;&#24335;&#26159;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#28165;&#26224;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#26597;&#35810;&#23884;&#20837;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#32452;&#23454;&#20307;&#30340;&#23884;&#20837;&#65292;&#24182;&#23558;&#36923;&#36753;&#36816;&#31639;&#35270;&#20026;&#38598;&#21512;&#36816;&#31639;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#30740;&#31350;&#36981;&#24490;&#30456;&#21516;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#32570;&#20047;&#20174;&#36923;&#36753;&#35282;&#24230;&#36827;&#34892;&#31995;&#32479;&#26816;&#26597;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20808;&#21069;&#30740;&#31350;&#35843;&#26597;&#30340;&#26597;&#35810;&#33539;&#22260;&#65292;&#24182;&#20934;&#30830;&#22320;&#30830;&#23450;&#20102;&#23427;&#19982;&#25972;&#20010;&#23384;&#22312;&#24615;&#20844;&#24335;&#23478;&#26063;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#21313;&#20010;&#26032;&#20844;&#24335;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#24182;&#35752;&#35770;&#20102;&#21516;&#26102;&#20986;&#29616;&#30340;&#26032;&#25361;&#25112;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26469;&#33258;&#27169;&#31946;&#36923;&#36753;&#29702;&#35770;&#30340;&#26032;&#25628;&#32034;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#35299;&#20915;&#26032;&#20844;&#24335;&#65292;&#24182;&#22312;&#29616;&#26377;&#20844;&#24335;&#20013;&#36229;&#36807;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Specifically, answering first-order logic formulas is of particular interest because of its clear syntax and semantics. Recently, the query embedding method has been proposed which learns the embedding of a set of entities and treats logic operations as set operations. Though there has been much research following the same methodology, it lacks a systematic inspection from the standpoint of logic. In this paper, we characterize the scope of queries investigated previously and precisely identify the gap between it and the whole family of existential formulas. Moreover, we develop a new dataset containing ten new formulas and discuss the new challenges coming simultaneously. Finally, we propose a new search algorithm from fuzzy logic theory which is capable of solving new formulas and outperforming the previous methods in existing formulas.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.07056</link><description>&lt;p&gt;
&#38754;&#37096;&#35270;&#39057;&#21387;&#32553;&#30340;&#24863;&#30693;&#36136;&#37327;&#35780;&#20272;&#65306;&#22522;&#20934;&#21644;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method. (arXiv:2304.07056v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#38754;&#37096;&#35270;&#39057;&#21387;&#32553;&#30340;&#38656;&#27714;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#25104;&#21151;&#20351;&#24471;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#28151;&#21512;&#35270;&#39057;&#32534;&#30721;&#33539;&#22260;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#31354;&#38388;&#21644;&#26102;&#38388;&#22495;&#20013;&#25197;&#26354;&#31867;&#22411;&#30340;&#26497;&#22823;&#22810;&#26679;&#24615;&#65292;&#20174;&#20256;&#32479;&#30340;&#28151;&#21512;&#32534;&#30721;&#26694;&#26550;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#32473;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;VQA&#65289;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#36825;&#26159;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#12290;&#35813;&#25968;&#25454;&#24211;&#21253;&#21547; 3,240 &#20010;&#21387;&#32553;&#30340;&#38754;&#37096;&#35270;&#39057;&#29255;&#27573;&#65292;&#28085;&#30422;&#22810;&#20010;&#21387;&#32553;&#32423;&#21035;&#65292;&#36825;&#20123;&#29255;&#27573;&#26469;&#33258; 135 &#20010;&#28304;&#35270;&#39057;&#65292;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an exponential increase in the demand for face video compression, and the success of artificial intelligence has expanded the boundaries beyond traditional hybrid video coding. Generative coding approaches have been identified as promising alternatives with reasonable perceptual rate-distortion trade-offs, leveraging the statistical priors of face videos. However, the great diversity of distortion types in spatial and temporal domains, ranging from the traditional hybrid coding frameworks to generative models, present grand challenges in compressed face video quality assessment (VQA). In this paper, we introduce the large-scale Compressed Face Video Quality Assessment (CFVQA) database, which is the first attempt to systematically understand the perceptual quality and diversified compression distortions in face videos. The database contains 3,240 compressed face video clips in multiple compression levels, which are derived from 135 source videos with diversif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#32447;&#30456;&#20851;&#30340;&#26377;&#25439;&#21387;&#32553;&#25216;&#26415;&#26469;&#38477;&#20302;&#23556;&#30005;&#24178;&#28041;&#21487;&#35265;&#24230;&#25968;&#25454;&#20307;&#31215;&#30340;&#26041;&#27861;&#65292;&#23558;&#25972;&#20010;&#21487;&#35265;&#24230;&#25968;&#25454;&#34920;&#31034;&#20026;&#22522;&#32447;&#30340;&#25968;&#25454;&#30697;&#38453;&#38598;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#30340;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#30041;&#35270;&#22330;&#36793;&#32536;&#30340;&#27169;&#31946;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.07050</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#23556;&#30005;&#24178;&#28041;&#25968;&#25454;&#30340;&#26377;&#25439;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Lossy Compression of Large-Scale Radio Interferometric Data. (arXiv:2304.07050v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22522;&#32447;&#30456;&#20851;&#30340;&#26377;&#25439;&#21387;&#32553;&#25216;&#26415;&#26469;&#38477;&#20302;&#23556;&#30005;&#24178;&#28041;&#21487;&#35265;&#24230;&#25968;&#25454;&#20307;&#31215;&#30340;&#26041;&#27861;&#65292;&#23558;&#25972;&#20010;&#21487;&#35265;&#24230;&#25968;&#25454;&#34920;&#31034;&#20026;&#22522;&#32447;&#30340;&#25968;&#25454;&#30697;&#38453;&#38598;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#25968;&#25454;&#30340;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#30041;&#35270;&#22330;&#36793;&#32536;&#30340;&#27169;&#31946;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#22522;&#32447;&#30456;&#20851;&#30340;&#26377;&#25439;&#21387;&#32553;&#25216;&#26415;&#26469;&#20943;&#23569;&#21487;&#35265;&#24230;&#25968;&#25454;&#30340;&#20307;&#31215;&#65292;&#21516;&#26102;&#20445;&#30041;&#35270;&#22330;&#36793;&#32536;&#30340;&#27169;&#31946;&#25928;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#30697;&#38453;&#31209;&#21644;&#20302;&#31209;&#36817;&#20284;&#25551;&#36848;&#21407;&#22987;&#21487;&#35265;&#24230;&#25968;&#25454;&#30340;&#22522;&#26412;&#20998;&#37327;&#21644;&#20855;&#20307;&#30340;&#22825;&#31354;&#20998;&#24067;&#30697;&#38453;&#12290;&#22240;&#27492;&#65292;&#25972;&#20010;&#21487;&#35265;&#24230;&#25968;&#25454;&#34987;&#34920;&#31034;&#20026;&#22522;&#32447;&#30340;&#25968;&#25454;&#30697;&#38453;&#38598;&#21512;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#24352;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21387;&#32553;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#31616;&#21333;&#30340;$SVD$&#21644;&#21629;&#21517;&#20026;$BDSVD$&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes to reduce visibility data volume using a baseline-dependent lossy compression technique that preserves smearing at the edges of the field-of-view. We exploit the relation of the rank of a matrix and the fact that a low-rank approximation can describe the raw visibility data as a sum of basic components where each basic component corresponds to a specific Fourier component of the sky distribution. As such, the entire visibility data is represented as a collection of data matrices from baselines, instead of a single tensor. The proposed methods are formulated as follows: provided a large dataset of the entire visibility data; the first algorithm, named $simple~SVD$ projects the data into a regular sampling space of rank$-r$ data matrices. In this space, the data for all the baselines has the same rank, which makes the compression factor equal across all baselines. The second algorithm, named $BDSVD$ projects the data into an irregular sampling space of rank$-r_{pq}$ da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25193;&#23637;&#30340; Wasserstein PAC-Bayes &#26694;&#26550;&#65292;&#21033;&#29992;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#20960;&#20309;&#20551;&#35774;&#25552;&#20379;&#20102;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102; \cite{lambert2022variational} &#20013;&#31639;&#27861;&#30340;&#36755;&#20986;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#24314;&#31435;&#20102; PAC-Bayes &#21644;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;</title><link>http://arxiv.org/abs/2304.07048</link><description>&lt;p&gt;
Wasserstein PAC-Bayes &#23398;&#20064;&#65306;&#27867;&#21270;&#19982;&#20248;&#21270;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and Optimisation. (arXiv:2304.07048v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25193;&#23637;&#30340; Wasserstein PAC-Bayes &#26694;&#26550;&#65292;&#21033;&#29992;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#20960;&#20309;&#20551;&#35774;&#25552;&#20379;&#20102;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102; \cite{lambert2022variational} &#20013;&#31639;&#27861;&#30340;&#36755;&#20986;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#24314;&#31435;&#20102; PAC-Bayes &#21644;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PAC-Bayes &#23398;&#20064;&#26159;&#19968;&#31181;&#24050;&#24314;&#31435;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#35757;&#32451;&#38454;&#27573;&#35780;&#20272;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#24324;&#28165;&#26970;&#20026;&#20160;&#20040;&#30693;&#21517;&#31639;&#27861;&#30340;&#36755;&#20986;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#29305;&#24615;&#32780; PAC-Bayes &#26159;&#21542;&#26377;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;&#31616;&#35201;&#20171;&#32461;&#22312;&#25991;&#29486; \cite{amit2022ipm} &#20013;&#25552;&#20986;&#30340; \emph{Wasserstein PAC-Bayes} &#26694;&#26550;&#26469;&#31215;&#26497;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21033;&#29992;&#25439;&#22833;&#20989;&#25968;&#19978;&#30340;&#20960;&#20309;&#20551;&#35774;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#20219;&#20309;&#35757;&#32451;&#20043;&#21069;&#23601;&#35777;&#26126;&#20102; \cite{lambert2022variational} &#20013;&#31639;&#27861;&#30340;&#36755;&#20986;&#20855;&#26377;&#24378;&#22823;&#30340;&#28176;&#36817;&#27867;&#21270;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27867;&#21270;&#26694;&#26550;&#20013;&#23558;&#20248;&#21270;&#32467;&#26524;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102; PAC-Bayes &#21644;&#20248;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayes learning is an established framework to assess the generalisation ability of learning algorithm during the training phase. However, it remains challenging to know whether PAC-Bayes is useful to understand, before training, why the output of well-known algorithms generalise well. We positively answer this question by expanding the \emph{Wasserstein PAC-Bayes} framework, briefly introduced in \cite{amit2022ipm}. We provide new generalisation bounds exploiting geometric assumptions on the loss function. Using our framework, we prove, before any training, that the output of an algorithm from \citet{lambert2022variational} has a strong asymptotic generalisation ability. More precisely, we show that it is possible to incorporate optimisation results within a generalisation framework, building a bridge between PAC-Bayes and optimisation algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#35780;&#20272;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#36136;&#37327;&#30340;&#23618;&#27425;&#20195;&#29702;&#26694;&#26550;&#65292;&#20855;&#26377;&#20248;&#21270;&#26631;&#27880;&#37327;&#12289;&#25972;&#20307;&#36136;&#37327;&#35780;&#20272;&#21644;&#20154;&#31867;&#19987;&#23478;&#32467;&#26524;&#19968;&#33268;&#24615;&#31561;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.07036</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#20195;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#36136;&#37327;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Agent-based Reinforcement Learning Framework for Automated Quality Assessment of Fetal Ultrasound Video. (arXiv:2304.07036v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#35780;&#20272;&#32974;&#20799;&#36229;&#22768;&#35270;&#39057;&#36136;&#37327;&#30340;&#23618;&#27425;&#20195;&#29702;&#26694;&#26550;&#65292;&#20855;&#26377;&#20248;&#21270;&#26631;&#27880;&#37327;&#12289;&#25972;&#20307;&#36136;&#37327;&#35780;&#20272;&#21644;&#20154;&#31867;&#19987;&#23478;&#32467;&#26524;&#19968;&#33268;&#24615;&#31561;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22768;&#26159;&#23381;&#26399;&#26816;&#26597;&#32974;&#20799;&#29983;&#38271;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20294;&#22270;&#20687;&#36136;&#37327;&#21487;&#33021;&#20250;&#21463;&#21040;&#21508;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#36136;&#37327;&#35780;&#20272;&#23545;&#20110;&#25511;&#21046;&#36229;&#22768;&#22270;&#20687;&#30340;&#36136;&#37327;&#20197;&#20445;&#35777;&#20854;&#24863;&#30693;&#21644;&#35786;&#26029;&#20215;&#20540;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#32467;&#26500;&#24615;&#26631;&#27880;&#65292;&#32780;&#19988;&#20854;&#39044;&#27979;&#32467;&#26524;&#21487;&#33021;&#19981;&#19968;&#23450;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#35780;&#20272;&#32467;&#26524;&#20445;&#25345;&#19968;&#33268;&#12290;&#27492;&#22806;&#65292;&#19981;&#24212;&#24573;&#35270;&#25195;&#25551;&#30340;&#25972;&#20307;&#36136;&#37327;&#21644;&#24103;&#38388;&#36136;&#37327;&#30340;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#30001;&#20004;&#20010;&#23618;&#27425;&#20195;&#29702;&#21327;&#21516;&#23398;&#20064;&#25191;&#34892;&#24103;&#32423;&#21644;&#35270;&#39057;&#32423;&#36136;&#37327;&#35780;&#20272;&#12290;&#23427;&#37197;&#22791;&#20102;&#19968;&#31181;&#29305;&#27530;&#35774;&#35745;&#30340;&#22870;&#21169;&#26426;&#21046;&#65292;&#32771;&#34385;&#24103;&#36136;&#37327;&#20043;&#38388;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#21482;&#38656;&#35201;&#31232;&#30095;&#30340;&#20108;&#36827;&#21046;&#27880;&#37322;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#25361;&#25112;&#24615;&#30340;&#32974;&#20799;&#33041;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#36229;&#36807;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#21516;&#26102;&#65292;&#25928;&#26524;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;&#22312;&#19968;&#33268;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ultrasound is the primary modality to examine fetal growth during pregnancy, while the image quality could be affected by various factors. Quality assessment is essential for controlling the quality of ultrasound images to guarantee both the perceptual and diagnostic values. Existing automated approaches often require heavy structural annotations and the predictions may not necessarily be consistent with the assessment results by human experts. Furthermore, the overall quality of a scan and the correlation between the quality of frames should not be overlooked. In this work, we propose a reinforcement learning framework powered by two hierarchical agents that collaboratively learn to perform both frame-level and video-level quality assessments. It is equipped with a specially-designed reward mechanism that considers temporal dependency among frame quality and only requires sparse binary annotations to train. Experimental results on a challenging fetal brain dataset verify that the prop
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#28909;&#25104;&#20687;&#30340;&#20027;&#21160;&#36866;&#24212;&#26041;&#27861;&#65292;&#23558;&#21487;&#35265;&#20809;&#35889;&#21644;&#28909;&#25104;&#20687;&#32467;&#21512;&#36215;&#26469;&#24182;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#30446;&#26631;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#26469;&#32531;&#35299;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#22495;&#36866;&#24212;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2304.07031</link><description>&lt;p&gt;
&#38754;&#21521;&#28909;&#25104;&#20687;&#30340;&#20809;&#35889;&#36716;&#31227;&#24341;&#23548;&#30340;&#20027;&#21160;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Spectral Transfer Guided Active Domain Adaptation For Thermal Imagery. (arXiv:2304.07031v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#28909;&#25104;&#20687;&#30340;&#20027;&#21160;&#36866;&#24212;&#26041;&#27861;&#65292;&#23558;&#21487;&#35265;&#20809;&#35889;&#21644;&#28909;&#25104;&#20687;&#32467;&#21512;&#36215;&#26469;&#24182;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#30446;&#26631;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#26469;&#32531;&#35299;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#22495;&#36866;&#24212;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35265;&#20809;&#35889;&#25968;&#25454;&#38598;&#30340;&#21033;&#29992;&#24050;&#32463;&#20351;&#24471;&#28145;&#24230;&#32593;&#32476;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24212;&#29992;&#20219;&#21153;&#21253;&#25324;&#20302;&#20809;&#29031;&#26465;&#20214;&#65292;&#36825;&#20351;&#24471;&#22312;&#22823;&#35268;&#27169;RGB&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#24615;&#33021;&#29942;&#39048;&#12290;&#28909;&#32418;&#22806;&#30456;&#26426;&#23545;&#36825;&#31181;&#24773;&#20917;&#26356;&#20026;&#31283;&#20581;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#28909;&#25104;&#20687;&#21487;&#20197;&#26159;&#26377;&#29992;&#30340;&#12290;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#65288;UDA&#65289;&#20801;&#35768;&#23558;&#20449;&#24687;&#20174;&#28304;&#22495;&#36716;&#31227;&#33267;&#23436;&#20840;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#12290;&#23613;&#31649;UDA&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#65292;&#20294;&#20854;&#19982;&#20854;&#30417;&#30563;&#23398;&#20064;&#23545;&#24212;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#20173;&#28982;&#26174;&#33879;&#12290;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#30446;&#26631;&#26679;&#26412;&#36827;&#34892;&#27880;&#37322;&#24182;&#23558;&#20854;&#29992;&#20110;&#35757;&#32451;&#65292;&#20027;&#21160;&#22495;&#36866;&#24212;&#35797;&#22270;&#20197;&#26368;&#23567;&#30340;&#27880;&#37322;&#36153;&#29992;&#26469;&#32531;&#35299;&#36825;&#31181;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#20197;&#26816;&#39564;&#23558;&#21487;&#35265;&#20809;&#35889;&#19982;&#28909;&#25104;&#20687;&#27169;&#24577;&#32467;&#21512;&#30340;&#25928;&#29575;&#12290;&#24403;&#22495;&#24046;&#36317;&#26126;&#26174;&#36739;&#22823;&#26102;&#65292;
&lt;/p&gt;
&lt;p&gt;
The exploitation of visible spectrum datasets has led deep networks to show remarkable success. However, real-world tasks include low-lighting conditions which arise performance bottlenecks for models trained on large-scale RGB image datasets. Thermal IR cameras are more robust against such conditions. Therefore, the usage of thermal imagery in real-world applications can be useful. Unsupervised domain adaptation (UDA) allows transferring information from a source domain to a fully unlabeled target domain. Despite substantial improvements in UDA, the performance gap between UDA and its supervised learning counterpart remains significant. By picking a small number of target samples to annotate and using them in training, active domain adaptation tries to mitigate this gap with minimum annotation expense. We propose an active domain adaptation method in order to examine the efficiency of combining the visible spectrum and thermal imagery modalities. When the domain gap is considerably la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#26102;&#38388;&#33258;&#22238;&#24402;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(CTRNNs)&#30340;&#24212;&#29992;, &#36890;&#36807;&#36830;&#32493;&#28436;&#21270;&#26469;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;, &#20197;&#27010;&#29575;&#39044;&#27979;&#20020;&#24202;&#30417;&#25252;&#35774;&#32622;&#20013;&#30340;&#34880;&#31958;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.07025</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65306;&#27010;&#36848;&#21450;&#22312;&#30417;&#25252;&#30149;&#25151;&#34880;&#31958;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Continuous time recurrent neural networks: overview and application to forecasting blood glucose in the intensive care unit. (arXiv:2304.07025v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36830;&#32493;&#26102;&#38388;&#33258;&#22238;&#24402;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(CTRNNs)&#30340;&#24212;&#29992;, &#36890;&#36807;&#36830;&#32493;&#28436;&#21270;&#26469;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;, &#20197;&#27010;&#29575;&#39044;&#27979;&#20020;&#24202;&#30417;&#25252;&#35774;&#32622;&#20013;&#30340;&#34880;&#31958;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24456;&#22810;&#24212;&#29992;&#20013;&#65292;&#38750;&#35268;&#21017;&#37319;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#26159;&#24120;&#35265;&#30340;&#65292;&#21253;&#25324;&#21307;&#23398;&#39046;&#22495;&#12290;&#36825;&#25552;&#20379;&#20102;&#27169;&#22411;&#36873;&#25321;&#30340;&#25361;&#25112;&#65292;&#36890;&#24120;&#38656;&#35201;&#25554;&#34917;&#25110;&#31867;&#20284;&#31574;&#30053;&#12290;&#36830;&#32493;&#26102;&#38388;&#33258;&#22238;&#24402;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;(CTRNNs) &#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35266;&#27979;&#20540;&#20043;&#38388;&#34701;&#21512;&#38544;&#34255;&#29366;&#24577;&#30340;&#36830;&#32493;&#28436;&#21270;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#26159;&#36890;&#36807;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#25110;&#31070;&#32463;&#27969;&#23618;&#26469;&#23454;&#29616;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#65292;&#21253;&#25324;&#20026;&#24212;&#23545; ongoing medical interventions &#31561;&#38382;&#39064;&#32780;&#25552;&#20986;&#30340;&#21508;&#31181;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20351;&#29992;&#30005;&#23376;&#30149;&#21382;&#21644;&#27169;&#25311;&#25968;&#25454;&#36827;&#34892;&#34880;&#31958;&#27010;&#29575;&#39044;&#27979;&#30340;&#20020;&#24202;&#30417;&#25252;&#35774;&#32622;&#20013;&#30340;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#28155;&#21152;&#31070;&#32463;ODE&#25110;&#31070;&#32463;&#27969;&#23618;&#30340;&#19968;&#33324;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Irregularly measured time series are common in many of the applied settings in which time series modelling is a key statistical tool, including medicine. This provides challenges in model choice, often necessitating imputation or similar strategies. Continuous time autoregressive recurrent neural networks (CTRNNs) are a deep learning model that account for irregular observations through incorporating continuous evolution of the hidden states between observations. This is achieved using a neural ordinary differential equation (ODE) or neural flow layer. In this manuscript, we give an overview of these models, including the varying architectures that have been proposed to account for issues such as ongoing medical interventions. Further, we demonstrate the application of these models to probabilistic forecasting of blood glucose in a critical care setting using electronic medical record and simulated data. The experiments confirm that addition of a neural ODE or neural flow layer general
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIPNet&#30340;&#26032;&#22411;&#22810;&#38454;&#27573;&#36731;&#37327;&#32423;&#32593;&#32476;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#39640;&#20998;&#36776;&#29575;&#36755;&#20986;&#20316;&#20026;&#39069;&#22806;&#30417;&#30563;&#65292;&#25552;&#39640;&#36731;&#37327;&#32423;&#23398;&#29983;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#37319;&#29992;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#21644;&#36845;&#20195;&#32593;&#32476;&#20462;&#21098;&#31561;&#31616;&#21270;&#32593;&#32476;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#36229;&#20998;&#36776;&#29575;&#30340;&#26377;&#25928;&#32593;&#32476;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2304.07018</link><description>&lt;p&gt;
DIPNet&#65306;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#30340;&#25928;&#29575;&#33976;&#39311;&#19982;&#36845;&#20195;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
DIPNet: Efficiency Distillation and Iterative Pruning for Image Super-Resolution. (arXiv:2304.07018v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIPNet&#30340;&#26032;&#22411;&#22810;&#38454;&#27573;&#36731;&#37327;&#32423;&#32593;&#32476;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#30340;&#39640;&#20998;&#36776;&#29575;&#36755;&#20986;&#20316;&#20026;&#39069;&#22806;&#30417;&#30563;&#65292;&#25552;&#39640;&#36731;&#37327;&#32423;&#23398;&#29983;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#37319;&#29992;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#21644;&#36845;&#20195;&#32593;&#32476;&#20462;&#21098;&#31561;&#31616;&#21270;&#32593;&#32476;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#36229;&#20998;&#36776;&#29575;&#30340;&#26377;&#25928;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#25928;&#21333;&#24133;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#36817;&#26399;&#20851;&#20110;&#39640;&#25928;&#36229;&#20998;&#36776;&#29575;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#21508;&#31181;&#32593;&#32476;&#35774;&#35745;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#21644;&#28014;&#28857;&#36816;&#31639;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#21442;&#25968;&#25968;&#37327;&#21644;&#28014;&#28857;&#36816;&#31639;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#32553;&#30701;&#23454;&#38469;&#36816;&#34892;&#26102;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#38454;&#27573;&#36731;&#37327;&#32423;&#32593;&#32476;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#36731;&#37327;&#32423;&#32593;&#32476;&#23454;&#29616;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#22686;&#24378;&#30340;&#39640;&#20998;&#36776;&#29575;&#36755;&#20986;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#65292;&#25552;&#39640;&#36731;&#37327;&#32423;&#23398;&#29983;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#23398;&#29983;&#32593;&#32476;&#25910;&#25947;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#21644;&#36845;&#20195;&#32593;&#32476;&#20462;&#21098;&#31616;&#21270;&#32593;&#32476;&#32467;&#26500;&#21040;&#26356;&#36731;&#37327;&#32423;&#30340;&#27700;&#24179;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37319;&#29992;&#26377;&#25928;&#30340;&#36731;&#37327;&#32423;&#32593;&#32476;&#35757;&#32451;&#31574;&#30053;&#65292;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#20445;&#35777;&#32593;&#32476;&#25928;&#29575;&#65292;&#21516;&#26102;&#20063;&#33021;&#22312;&#23454;&#29616;&#39640;&#36136;&#37327;&#36229;&#20998;&#36776;&#29575;&#26041;&#38754;&#21462;&#24471;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient deep learning-based approaches have achieved remarkable performance in single image super-resolution. However, recent studies on efficient super-resolution have mainly focused on reducing the number of parameters and floating-point operations through various network designs. Although these methods can decrease the number of parameters and floating-point operations, they may not necessarily reduce actual running time. To address this issue, we propose a novel multi-stage lightweight network boosting method, which can enable lightweight networks to achieve outstanding performance. Specifically, we leverage enhanced high-resolution output as additional supervision to improve the learning ability of lightweight student networks. Upon convergence of the student network, we further simplify our network structure to a more lightweight level using reparameterization techniques and iterative network pruning. Meanwhile, we adopt an effective lightweight network training strategy that c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22522;&#20110;ANN&#30340;&#22343;&#34913;&#22120;&#21450;&#20854;&#21487;&#35757;&#32451;&#30340;FPGA&#23454;&#29616;&#65292;&#36890;&#36807;&#33258;&#23450;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2304.06987</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;ANN&#30340;&#22343;&#34913;&#22120;&#21450;&#20854;&#35757;&#32451;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Unsupervised ANN-Based Equalizer and Its Trainable FPGA Implementation. (arXiv:2304.06987v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#22522;&#20110;ANN&#30340;&#22343;&#34913;&#22120;&#21450;&#20854;&#21487;&#35757;&#32451;&#30340;FPGA&#23454;&#29616;&#65292;&#36890;&#36807;&#33258;&#23450;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#25928;&#29575;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#20449;&#24037;&#31243;&#24072;&#24378;&#35843;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#21450;&#20854;&#32452;&#20214;&#30340;&#28789;&#27963;&#24615;&#21644;&#33258;&#20027;&#24615;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#26080;&#30417;&#30563;&#35757;&#32451;&#20855;&#26377;&#29305;&#27530;&#30340;&#24847;&#20041;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#19981;&#20256;&#36755;&#23548;&#39057;&#31526;&#21495;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ANN&#30340;&#26080;&#30417;&#30563;&#22343;&#34913;&#22120;&#21450;&#20854;&#21487;&#35757;&#32451;&#30340;&#29616;&#22330;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#23454;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#23450;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#35753;ANN&#36866;&#24212;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#65292;&#25509;&#36817;&#30417;&#30563;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#23454;&#38469;&#30340;&#36890;&#20449;&#31995;&#32479;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;FPGA&#23454;&#29616;&#65292;&#20854;&#21534;&#21520;&#37327;&#36798;&#21040;Gbit/s&#32423;&#21035;&#65292;&#36828;&#36828;&#36229;&#20986;&#39640;&#24615;&#33021;GPU&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, communication engineers put strong emphasis on artificial neural network (ANN)-based algorithms with the aim of increasing the flexibility and autonomy of the system and its components. In this context, unsupervised training is of special interest as it enables adaptation without the overhead of transmitting pilot symbols. In this work, we present a novel ANN-based, unsupervised equalizer and its trainable field programmable gate array (FPGA) implementation. We demonstrate that our custom loss function allows the ANN to adapt for varying channel conditions, approaching the performance of a supervised baseline. Furthermore, as a first step towards a practical communication system, we design an efficient FPGA implementation of our proposed algorithm, which achieves a throughput in the order of Gbit/s, outperforming a high-performance GPU by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#21644;&#20256;&#36882;&#23398;&#20064;&#30340;&#22810;&#20445;&#30495;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#39044;&#27979;&#27969;&#20307;&#27969;&#21160;&#21644;&#28201;&#24230;&#22330;&#65292;&#19988;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.06972</link><description>&lt;p&gt;
&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20256;&#36882;&#23398;&#20064;&#22810;&#20445;&#30495;&#24230;&#27969;&#22330;&#21644;&#28201;&#24230;&#22330;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-fidelity prediction of fluid flow and temperature field based on transfer learning using Fourier Neural Operator. (arXiv:2304.06972v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#21644;&#20256;&#36882;&#23398;&#20064;&#30340;&#22810;&#20445;&#30495;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#36739;&#23569;&#30340;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#39044;&#27979;&#27969;&#20307;&#27969;&#21160;&#21644;&#28201;&#24230;&#22330;&#65292;&#19988;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#27969;&#20307;&#27969;&#21160;&#21644;&#28201;&#24230;&#20998;&#24067;&#39044;&#27979;&#22312;&#28023;&#27915;&#21644;&#33322;&#31354;&#33322;&#22825;&#24037;&#31243;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#24182;&#26368;&#36817;&#23637;&#31034;&#20102;&#20854;&#23454;&#26102;&#39044;&#27979;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#26469;&#25551;&#36848;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#30340;&#29289;&#29702;&#20449;&#24687;&#65292;&#32780;&#22312;&#29616;&#23454;&#20013;&#65292;&#30001;&#20110;&#39640;&#25104;&#26412;&#23454;&#39564;/&#35745;&#31639;&#65292;&#21482;&#26377;&#26377;&#38480;&#30340;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#21487;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#22810;&#20445;&#30495;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20256;&#36882;&#23398;&#20064;&#33539;&#24335;&#19979;&#65292;&#32852;&#21512;&#20016;&#23500;&#30340;&#20302;&#20445;&#30495;&#24230;&#25968;&#25454;&#21644;&#26377;&#38480;&#30340;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#20316;&#20026;&#19968;&#20010;&#20998;&#36776;&#29575;&#19981;&#21464;&#30340;&#31639;&#23376;&#65292;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#39318;&#20808;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#30452;&#25509;&#38598;&#25104;&#22810;&#20445;&#30495;&#24230;&#25968;&#25454;&#65292;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#31232;&#32570;&#30340;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#21644;&#20016;&#23500;&#30340;&#20302;&#20445;&#30495;&#24230;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25552;&#21462;&#20016;&#23500;&#30340;&#20302;&#20445;&#30495;&#24230;&#25968;&#25454;&#65292;&#20026;&#24403;&#21069;&#20219;&#21153;&#24320;&#21457;&#20256;&#36882;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#22686;&#24378;&#26377;&#38480;&#30340;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#26356;&#23569;&#30340;&#39640;&#20445;&#30495;&#24230;&#25968;&#25454;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#27969;&#20307;&#27969;&#21160;&#21644;&#28201;&#24230;&#22330;&#39044;&#27979;&#65292;&#24182;&#19988;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven prediction of fluid flow and temperature distribution in marine and aerospace engineering has received extensive research and demonstrated its potential in real-time prediction recently. However, usually large amounts of high-fidelity data are required to describe and accurately predict the complex physical information, while in reality, only limited high-fidelity data is available due to the high experiment/computational cost. Therefore, this work proposes a novel multi-fidelity learning method based on the Fourier Neural Operator by jointing abundant low-fidelity data and limited high-fidelity data under transfer learning paradigm. First, as a resolution-invariant operator, the Fourier Neural Operator is first and gainfully applied to integrate multi-fidelity data directly, which can utilize the scarce high-fidelity data and abundant low-fidelity data simultaneously. Then, the transfer learning framework is developed for the current task by extracting the rich low-fidelit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Locality-Preserved Attention&#65288;LPA&#65289;&#23618;&#65292;&#20197;&#20445;&#30041;ViT&#20013;&#30340;&#23616;&#37096;&#29305;&#24449;&#20197;&#22686;&#24378;&#20854;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06971</link><description>&lt;p&gt;
&#20445;&#30041;&#35270;&#35273;Transformer&#20013;&#30340;&#23616;&#37096;&#29305;&#24449;&#20197;&#24212;&#29992;&#20110;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Preserving Locality in Vision Transformers for Class Incremental Learning. (arXiv:2304.06971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Locality-Preserved Attention&#65288;LPA&#65289;&#23618;&#65292;&#20197;&#20445;&#30041;ViT&#20013;&#30340;&#23616;&#37096;&#29305;&#24449;&#20197;&#22686;&#24378;&#20854;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20998;&#31867;&#27169;&#22411;&#32780;&#35328;&#65292;&#33021;&#22815;&#22312;&#19981;&#24536;&#35760;&#20197;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#30340;&#31867;&#21035;&#23545;&#20110;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#38754;&#65292;Vision Transformer&#65288;ViT&#65289;&#36817;&#26469;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;ViT&#30340;&#22359;&#35774;&#35745;&#21644;&#27169;&#22411;&#25193;&#23637;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#21457;&#29616;&#22312;ViT&#36827;&#34892;&#22686;&#37327;&#35757;&#32451;&#26102;&#65292;&#27880;&#24847;&#21147;&#23618;&#36880;&#28176;&#22833;&#21435;&#23545;&#23616;&#37096;&#29305;&#24449;&#30340;&#20851;&#27880;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;ViT&#22312;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;\emph{&#23616;&#37096;&#20449;&#24687;&#38477;&#35299;}&#12290;&#30001;&#20110;&#20302;&#32423;&#21035;&#30340;&#23616;&#37096;&#20449;&#24687;&#23545;&#20110;&#29305;&#24449;&#20256;&#36882;&#30340;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#20445;&#30041;&#23616;&#37096;&#29305;&#24449;&#23545;&#20110;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#20855;&#26377;&#31215;&#26497;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#40723;&#21169;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;Locality-Preserved Attention&#65288;LPA&#65289;&#23618;&#26469;&#24378;&#35843;&#23616;&#37096;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23558;&#23616;&#37096;&#20449;&#24687;&#30452;&#25509;&#25972;&#21512;&#21040;&#21407;&#22987;&#27880;&#24847;&#21147;&#20013;&#20197;&#25511;&#21046;&#21021;&#22987;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning new classes without forgetting is crucial for real-world applications for a classification model. Vision Transformers (ViT) recently achieve remarkable performance in Class Incremental Learning (CIL). Previous works mainly focus on block design and model expansion for ViTs. However, in this paper, we find that when the ViT is incrementally trained, the attention layers gradually lose concentration on local features. We call this interesting phenomenon as \emph{Locality Degradation} in ViTs for CIL. Since the low-level local information is crucial to the transferability of the representation, it is beneficial to preserve the locality in attention layers. In this paper, we encourage the model to preserve more local information as the training procedure goes on and devise a Locality-Preserved Attention (LPA) layer to emphasize the importance of local features. Specifically, we incorporate the local information directly into the vanilla attention and control the initial gradients 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21333;&#30446;&#22270;&#20687;&#28145;&#24230;&#20272;&#35745;&#65292;&#25506;&#32034;&#20102;&#22810;&#31181;&#25193;&#23637;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#35745;&#21010;&#23454;&#29616;&#23039;&#24577;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#25216;&#26415;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#28145;&#24230;&#22270;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06966</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21333;&#30446;&#22270;&#20687;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning based Depth Estimation from Monocular Images. (arXiv:2304.06966v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#21333;&#30446;&#22270;&#20687;&#28145;&#24230;&#20272;&#35745;&#65292;&#25506;&#32034;&#20102;&#22810;&#31181;&#25193;&#23637;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#35745;&#21010;&#23454;&#29616;&#23039;&#24577;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#25216;&#26415;&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#28145;&#24230;&#22270;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20272;&#35745;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#22914;&#30446;&#26631;&#36319;&#36394;&#12289;&#22686;&#24378;&#29616;&#23454;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#30446;&#26631;&#26159;&#39044;&#27979;&#28145;&#24230;&#22270;&#65292;&#32473;&#23450;&#19968;&#20010;&#20108;&#32500;&#21333;&#30446; RGB &#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#12290;&#20256;&#32479;&#30340;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#32447;&#32034;&#65292;&#20351;&#29992;&#20102;&#35832;&#22914;&#26497;&#32447;&#20960;&#20309;&#31561;&#27010;&#24565;&#12290;&#38543;&#30528;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#65292;&#28145;&#24230;&#20272;&#35745;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#21487;&#33021;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#20197;&#21450;&#26159;&#21542;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#24230;&#37327;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#24847;&#20041;&#19978;&#65292;&#25105;&#20204;&#27491;&#22312;&#32771;&#34385;&#23454;&#29616;&#23039;&#24577;&#20272;&#35745;&#12289;&#39640;&#25928;&#20122;&#20687;&#32032;&#21367;&#31215;&#25554;&#20540;&#12289;&#35821;&#20041;&#20998;&#21106;&#20272;&#35745;&#25216;&#26415;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26550;&#26500;&#65292;&#24182;&#25552;&#20379;&#32454;&#31890;&#24230;&#21644;&#26356;&#20840;&#23616;&#19968;&#33268;&#30340;&#28145;&#24230;&#22270;&#39044;&#27979;&#12290;&#25105;&#20204;&#36824;&#35745;&#21010;&#25918;&#24323;&#30456;&#26426;&#20869;&#21442;&#25968;&#65292;&#24182;&#35843;&#26597;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28145;&#24230;&#20272;&#35745;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#26159;&#20174;&#21333;&#30446; RGB &#22270;&#20687;&#33258;&#21160;&#29983;&#25104;&#30340;&#65292;&#26080;&#38656;&#22320;&#38754;&#30495;&#23454;&#28145;&#24230;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depth Estimation has wide reaching applications in the field of Computer vision such as target tracking, augmented reality, and self-driving cars. The goal of Monocular Depth Estimation is to predict the depth map, given a 2D monocular RGB image as input. The traditional depth estimation methods are based on depth cues and used concepts like epipolar geometry. With the evolution of Convolutional Neural Networks, depth estimation has undergone tremendous strides. In this project, our aim is to explore possible extensions to existing SoTA Deep Learning based Depth Estimation Models and to see whether performance metrics could be further improved. In a broader sense, we are looking at the possibility of implementing Pose Estimation, Efficient Sub-Pixel Convolution Interpolation, Semantic Segmentation Estimation techniques to further enhance our proposed architecture and to provide fine-grained and more globally coherent depth map predictions. We also plan to do away with camera intrinsic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20984;&#23545;&#20598;&#32593;&#32476;&#65292;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#21452;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#36719;&#38408;&#20540;&#30340;&#20984;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.06959</link><description>&lt;p&gt;
&#20351;&#29992;&#36719;&#38408;&#20540;&#30340;&#21452;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20984;&#23545;&#20598;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convex Dual Theory Analysis of Two-Layer Convolutional Neural Networks with Soft-Thresholding. (arXiv:2304.06959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#20984;&#23545;&#20598;&#32593;&#32476;&#65292;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#21452;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#36719;&#38408;&#20540;&#30340;&#20984;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#38408;&#20540;&#34987;&#24191;&#27867;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20854;&#22522;&#26412;&#32593;&#32476;&#32467;&#26500;&#20026;&#20855;&#26377;&#36719;&#38408;&#20540;&#30340;&#21452;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#30001;&#20110;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#21644;&#38750;&#20984;&#24615;&#65292;&#35757;&#32451;&#36807;&#31243;&#20005;&#37325;&#20381;&#36182;&#20110;&#32593;&#32476;&#21442;&#25968;&#30340;&#36866;&#24403;&#21021;&#22987;&#21270;&#65292;&#23548;&#33268;&#38590;&#20197;&#33719;&#24471;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#20984;&#23545;&#20598;&#32593;&#32476;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#32593;&#32476;&#30340;&#20984;&#24615;&#65292;&#24182;&#22312;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#24378;&#23545;&#20598;&#24615;&#12290;&#36825;&#20010;&#32467;&#35770;&#22312;&#32447;&#24615;&#25311;&#21512;&#21644;&#21435;&#22122;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#20984;&#21270;&#36719;&#38408;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft-thresholding has been widely used in neural networks. Its basic network structure is a two-layer convolution neural network with soft-thresholding. Due to the network's nature of nonlinearity and nonconvexity, the training process heavily depends on an appropriate initialization of network parameters, resulting in the difficulty of obtaining a globally optimal solution. To address this issue, a convex dual network is designed here. We theoretically analyze the network convexity and numerically confirm that the strong duality holds. This conclusion is further verified in the linear fitting and denoising experiments. This work provides a new way to convexify soft-thresholding neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25991;&#21270;&#24847;&#35782;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;COVID-19&#30123;&#33495;&#25509;&#31181;&#24847;&#24895;&#65292;&#24182;&#25581;&#31034;&#20102;&#35199;&#29677;&#29273;&#35028;&#21644;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#26368;&#26377;&#21487;&#33021;&#21463;&#21040;&#25991;&#21270;&#24046;&#24322;&#30340;&#24433;&#21709;&#65292;&#36825;&#26377;&#21161;&#20110;&#26410;&#26469;&#21046;&#23450;&#26356;&#25104;&#21151;&#30340;&#30123;&#33495;&#25509;&#31181;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.06953</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#21270;&#24847;&#35782;&#30340; COVID-19 &#30123;&#33495;&#29369;&#35947;&#29616;&#35937;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cultural-aware Machine Learning based Analysis of COVID-19 Vaccine Hesitancy. (arXiv:2304.06953v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25991;&#21270;&#24847;&#35782;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;COVID-19&#30123;&#33495;&#25509;&#31181;&#24847;&#24895;&#65292;&#24182;&#25581;&#31034;&#20102;&#35199;&#29677;&#29273;&#35028;&#21644;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#26368;&#26377;&#21487;&#33021;&#21463;&#21040;&#25991;&#21270;&#24046;&#24322;&#30340;&#24433;&#21709;&#65292;&#36825;&#26377;&#21161;&#20110;&#26410;&#26469;&#21046;&#23450;&#26356;&#25104;&#21151;&#30340;&#30123;&#33495;&#25509;&#31181;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299; COVID-19 &#30123;&#33495;&#29369;&#35947;&#29616;&#35937;&#65288;&#22914;&#20309;&#20197;&#21450;&#20026;&#20160;&#20040;&#29369;&#35947;&#65289;&#23545;&#20110;&#25511;&#21046;&#27969;&#34892;&#30149;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#21363;&#22823;&#35268;&#27169;&#25509;&#31181;&#30123;&#33495;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#31181;&#29702;&#35299;&#36824;&#20026;&#26410;&#26469;&#30340;&#27969;&#34892;&#30149;&#35774;&#35745;&#25104;&#21151;&#30340;&#25509;&#31181;&#36816;&#21160;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20174;&#25991;&#21270;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#26377;&#24456;&#22810;&#22240;&#32032;&#28041;&#21450;&#20915;&#23450;&#26159;&#21542;&#25509;&#31181;&#30123;&#33495;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#21270;&#24847;&#35782;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#25910;&#38598;&#65292;&#29992;&#20110;&#39044;&#27979;&#25509;&#31181;&#24847;&#24895;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#20808;&#36827;&#30340; AI &#35299;&#37322;&#22120;&#20998;&#26512;&#20102;&#23545; ML &#27169;&#22411;&#39044;&#27979;&#26377;&#36129;&#29486;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#65292;&#22914;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#21644; Shapley Additive Explanations&#65288;SHAP&#65289;&#12290;&#36825;&#20123;&#20998;&#26512;&#25581;&#31034;&#20102;&#26368;&#26377;&#21487;&#33021;&#24433;&#21709;&#30123;&#33495;&#25509;&#31181;&#20915;&#31574;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#35199;&#29677;&#29273;&#35028;&#21644;&#38750;&#27954;&#35028;&#32654;&#22269;&#20154;&#26368;&#26377;&#21487;&#33021;&#21463;&#21040;&#25991;&#21270;&#24046;&#24322;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the COVID-19 vaccine hesitancy, such as who and why, is very crucial since a large-scale vaccine adoption remains as one of the most efficient methods of controlling the pandemic. Such an understanding also provides insights into designing successful vaccination campaigns for future pandemics. Unfortunately, there are many factors involving in deciding whether to take the vaccine, especially from the cultural point of view. To obtain these goals, we design a novel culture-aware machine learning (ML) model, based on our new data collection, for predicting vaccination willingness. We further analyze the most important features which contribute to the ML model's predictions using advanced AI explainers such as the Probabilistic Graphical Model (PGM) and Shapley Additive Explanations (SHAP). These analyses reveal the key factors that most likely impact the vaccine adoption decisions. Our findings show that Hispanic and African American are most likely impacted by cultural cha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;PPG&#20449;&#21495;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#34880;&#21387;&#20998;&#32423;&#26032;&#26041;&#27861;&#65292;&#32463;&#23454;&#39564;&#39564;&#35777;&#20854;&#22312;&#39640;&#34880;&#21387;&#35786;&#26029;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2304.06952</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;PPG&#20449;&#21495;&#29992;&#20110;&#39640;&#34880;&#21387;&#35786;&#26029;&#65306;&#19968;&#31181;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PPG Signals for Hypertension Diagnosis: A Novel Method using Deep Learning Models. (arXiv:2304.06952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;PPG&#20449;&#21495;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#34880;&#21387;&#20998;&#32423;&#26032;&#26041;&#27861;&#65292;&#32463;&#23454;&#39564;&#39564;&#35777;&#20854;&#22312;&#39640;&#34880;&#21387;&#35786;&#26029;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#34880;&#21387;&#26159;&#30001;&#20110;&#34880;&#21387;&#39640;&#24341;&#36215;&#30340;&#19968;&#31181;&#24120;&#35265;&#30149;&#65292;&#23558;&#20854;&#20998;&#20026;&#19981;&#21516;&#30340;&#38454;&#27573;&#23545;&#20110;&#31649;&#29702;&#35813;&#30142;&#30149;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20809;&#23481;&#31215;&#33033;&#25615;&#22270;&#65288;PPG&#65289;&#20449;&#21495;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#39640;&#34880;&#21387;&#20998;&#32423;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;AvgPool_VGG-16&#12290; PPG&#20449;&#21495;&#26159;&#36890;&#36807;&#20351;&#29992;&#27979;&#37327;&#32452;&#32455;&#24494;&#34880;&#31649;&#20013;&#34880;&#23481;&#37327;&#21464;&#21270;&#30340;&#20809;&#20256;&#24863;&#22120;&#26469;&#27979;&#37327;&#34880;&#21387;&#30340;&#38750;&#20405;&#20837;&#24615;&#26041;&#27861;&#12290;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#34880;&#21387;&#20998;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;PPG&#22270;&#20687;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#36827;&#34892;&#20102;&#21508;&#31181;PPG&#38454;&#27573;&#30340;&#22810;&#31867;&#20998;&#31867;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39640;&#34880;&#21387;&#20998;&#32423;&#20013;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#35777;&#26126;&#20102;PPG&#20449;&#21495;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#34880;&#21387;&#35786;&#26029;&#21644;&#31649;&#29702;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypertension is a medical condition characterized by high blood pressure, and classifying it into its various stages is crucial to managing the disease. In this project, a novel method is proposed for classifying stages of hypertension using Photoplethysmography (PPG) signals and deep learning models, namely AvgPool_VGG-16. The PPG signal is a non-invasive method of measuring blood pressure through the use of light sensors that measure the changes in blood volume in the microvasculature of tissues. PPG images from the publicly available blood pressure classification dataset were used to train the model. Multiclass classification for various PPG stages were done. The results show the proposed method achieves high accuracy in classifying hypertension stages, demonstrating the potential of PPG signals and deep learning models in hypertension diagnosis and management.
&lt;/p&gt;</description></item><item><title>TimelyFL&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#37096;&#20998;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#35774;&#22791;&#30340;&#24322;&#26500;&#24615;&#21644;&#36890;&#20449;&#27169;&#24335;&#35843;&#25972;&#27719;&#24635;&#35745;&#21010;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#35201;&#26356;&#26032;&#27169;&#22411;&#30340;&#35774;&#22791;&#23376;&#38598;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#38754;&#23545;&#24322;&#26500;&#32593;&#32476;&#26102;&#21487;&#33021;&#23548;&#33268;&#30340;&#35757;&#32451;&#31934;&#24230;&#19979;&#38477;&#21644;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06947</link><description>&lt;p&gt;
TimelyFL&#65306;&#38754;&#21521;&#24322;&#26500;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#37096;&#20998;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TimelyFL: Heterogeneity-aware Asynchronous Federated Learning with Adaptive Partial Training. (arXiv:2304.06947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06947
&lt;/p&gt;
&lt;p&gt;
TimelyFL&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#37096;&#20998;&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26681;&#25454;&#35774;&#22791;&#30340;&#24322;&#26500;&#24615;&#21644;&#36890;&#20449;&#27169;&#24335;&#35843;&#25972;&#27719;&#24635;&#35745;&#21010;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#35201;&#26356;&#26032;&#27169;&#22411;&#30340;&#35774;&#22791;&#23376;&#38598;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#38754;&#23545;&#24322;&#26500;&#32593;&#32476;&#26102;&#21487;&#33021;&#23548;&#33268;&#30340;&#35757;&#32451;&#31934;&#24230;&#19979;&#38477;&#21644;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#21516;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#35268;&#27169;&#21270;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#28382;&#21518;&#32773;&#20250;&#38459;&#30861;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31995;&#32479;&#24322;&#26500;&#24615;&#21644;&#38388;&#27463;&#36830;&#25509;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#21152;&#20837;&#35757;&#32451;&#30340;&#21487;&#29992;&#24615;&#38543;&#26102;&#38388;&#39640;&#24230;&#21464;&#21270;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;&#20363;&#22914;FedBuff&#65289;&#20197;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#20801;&#35768;&#36739;&#24930;&#30340;&#29992;&#25143;&#22522;&#20110;&#26087;&#27169;&#22411;&#32487;&#32493;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#65292;&#24182;&#22312;&#20934;&#22791;&#22909;&#26102;&#36827;&#34892;&#36129;&#29486;&#27719;&#24635;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#35757;&#32451;&#31934;&#24230;&#22823;&#24133;&#19979;&#38477;&#65292;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#24555;&#36895;&#35774;&#22791;&#23545;&#35768;&#22810;&#27719;&#24635;&#36718;&#27425;&#20570;&#20986;&#36129;&#29486;&#65292;&#32780;&#20854;&#20182;&#35774;&#22791;&#19981;&#23450;&#26399;&#21152;&#20837;&#65292;&#27169;&#22411;&#26356;&#26032;&#20063;&#36807;&#26102;&#20102;&#12290;&#20026;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TimelyFL&#65292;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#37096;&#20998;&#35757;&#32451;&#26694;&#26550;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;TimelyFL&#26681;&#25454;&#27599;&#20010;&#35774;&#22791;&#30340;&#31995;&#32479;&#24322;&#26500;&#24615;&#21644;&#36890;&#20449;&#27169;&#24335;&#26469;&#35843;&#25972;&#27719;&#24635;&#35745;&#21010;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#27599;&#36718;&#35201;&#26356;&#26032;&#27169;&#22411;&#30340;&#35774;&#22791;&#23376;&#38598;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#24322;&#26500;&#32593;&#32476;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In cross-device Federated Learning (FL) environments, scaling synchronous FL methods is challenging as stragglers hinder the training process. Moreover, the availability of each client to join the training is highly variable over time due to system heterogeneities and intermittent connectivity. Recent asynchronous FL methods (e.g., FedBuff) have been proposed to overcome these issues by allowing slower users to continue their work on local training based on stale models and to contribute to aggregation when ready. However, we show empirically that this method can lead to a substantial drop in training accuracy as well as a slower convergence rate. The primary reason is that fast-speed devices contribute to many more rounds of aggregation while others join more intermittently or not at all, and with stale model updates. To overcome this barrier, we propose TimelyFL, a heterogeneity-aware asynchronous FL framework with adaptive partial training. During the training, TimelyFL adjusts the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;AutoSparse&#30340;&#33258;&#21160;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#26799;&#24230;&#36864;&#28779;&#27861;&#26469;&#26435;&#34913;&#31232;&#30095;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;ResNet50&#21644;MobileNetV1&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.06941</link><description>&lt;p&gt;
AUTOSPARSE:&#23454;&#29616;&#31070;&#32463;&#32593;&#32476;&#33258;&#21160;&#31232;&#30095;&#35757;&#32451;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AUTOSPARSE: Towards Automated Sparse Training of Deep Neural Networks. (arXiv:2304.06941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;AutoSparse&#30340;&#33258;&#21160;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#26799;&#24230;&#36864;&#28779;&#27861;&#26469;&#26435;&#34913;&#31232;&#30095;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;ResNet50&#21644;MobileNetV1&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#35757;&#32451;&#26159;&#20943;&#23569;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#25104;&#26412;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#21487;&#23398;&#20064;&#38408;&#20540;&#30340;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#27169;&#22411;&#20013;&#20869;&#22312;&#31232;&#30095;&#24615;&#30340;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26799;&#24230;&#36864;&#28779;&#27861;&#65288;GA&#65289;&#65292;&#20854;&#20013;&#25513;&#30721;&#26435;&#37325;&#30340;&#26799;&#24230;&#25353;&#38750;&#32447;&#24615;&#26041;&#24335;&#32553;&#23567;&#12290; GA&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#31232;&#30095;&#35825;&#23548;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#32654;&#30340;&#26435;&#34913;&#31232;&#30095;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;GA&#19982;&#26368;&#26032;&#30340;&#21487;&#23398;&#20064;&#20462;&#21098;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#31216;&#20026;AutoSparse&#30340;&#33258;&#21160;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;ImageNet-1K&#19978;&#30340;&#31232;&#30095;ResNet50&#21644;MobileNetV1&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;/&#25110;&#35757;&#32451;/&#25512;&#29702;FLOPS&#20943;&#23569;&#65292;&#20363;&#22914;AutoSparse&#22312;80&#65285;&#30340;&#31232;&#30095;&#19979;&#65292;ResNet50&#22312;ImageNet&#19978;&#23454;&#29616;&#20102;&#65288;2&#20493;&#65292;7&#20493;&#65289;&#30340;&#65288;&#35757;&#32451;&#65292;&#25512;&#29702;&#65289;FLOPS&#20943;&#23569;&#12290;&#26368;&#21518;&#65292;AutoSparse&#22312;&#21019;&#26032;&#24615;&#31232;&#30095;&#39046;&#22495;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse training is emerging as a promising avenue for reducing the computational cost of training neural networks. Several recent studies have proposed pruning methods using learnable thresholds to efficiently explore the non-uniform distribution of sparsity inherent within the models. In this paper, we propose Gradient Annealing (GA), where gradients of masked weights are scaled down in a non-linear manner. GA provides an elegant trade-off between sparsity and accuracy without the need for additional sparsity-inducing regularization. We integrated GA with the latest learnable pruning methods to create an automated sparse training algorithm called AutoSparse, which achieves better accuracy and/or training/inference FLOPS reduction than existing learnable pruning methods for sparse ResNet50 and MobileNetV1 on ImageNet-1K: AutoSparse achieves (2x, 7x) reduction in (training,inference) FLOPS for ResNet50 on ImageNet at 80% sparsity. Finally, AutoSparse outperforms sparse-to-sparse SotA me
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;Lstm-cnn&#27169;&#22411;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21306;&#20998;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#26377;&#23475;&#21644;&#38750;&#26377;&#23475;&#35780;&#35770;&#65292;&#20197;&#35299;&#20915;&#32593;&#32476;&#27450;&#20940;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06934</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#31038;&#20132;&#23186;&#20307;&#26377;&#23475;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Classification of social media Toxic comments using Machine learning models. (arXiv:2304.06934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;Lstm-cnn&#27169;&#22411;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21306;&#20998;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#26377;&#23475;&#21644;&#38750;&#26377;&#23475;&#35780;&#35770;&#65292;&#20197;&#35299;&#20915;&#32593;&#32476;&#27450;&#20940;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#27010;&#36848;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#26377;&#23475;&#35780;&#35770;&#30340;&#38382;&#39064;&#12290;&#20010;&#21035;&#20154;&#20351;&#29992;&#19981;&#23562;&#37325;&#12289;&#28389;&#29992;&#21644;&#19981;&#21512;&#29702;&#30340;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#39537;&#20351;&#29992;&#25143;&#31163;&#24320;&#35752;&#35770;&#12290;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#21453;&#31038;&#20250;&#34892;&#20026;&#65292;&#32463;&#24120;&#20986;&#29616;&#22312;&#22312;&#32447;&#36777;&#35770;&#12289;&#35780;&#35770;&#21644;&#20105;&#26007;&#20013;&#12290;&#21547;&#26377;&#26126;&#30830;&#19981;&#33391;&#35821;&#35328;&#30340;&#35780;&#35770;&#21487;&#20197;&#34987;&#24402;&#31867;&#20026;&#22810;&#31181;&#31867;&#22411;&#65292;&#22914;&#26377;&#23475;&#12289;&#20005;&#37325;&#26377;&#23475;&#12289;&#28139;&#31229;&#12289;&#23041;&#32961;&#12289;&#20398;&#36785;&#21644;&#36523;&#20221;&#20167;&#24680;&#12290;&#36825;&#31181;&#34892;&#20026;&#23548;&#33268;&#32593;&#32476;&#39578;&#25200;&#21644;&#32593;&#32476;&#27450;&#20940;&#65292;&#36843;&#20351;&#20010;&#20154;&#20572;&#27490;&#34920;&#36798;&#33258;&#24049;&#30340;&#35266;&#28857;&#21644;&#24819;&#27861;&#12290;&#20026;&#20102;&#20445;&#25252;&#29992;&#25143;&#20813;&#21463;&#20882;&#29359;&#24615;&#35328;&#35770;&#65292;&#20844;&#21496;&#24050;&#32463;&#24320;&#22987;&#26631;&#35760;&#35780;&#35770;&#21644;&#38459;&#27490;&#29992;&#25143;&#12290;&#25688;&#35201;&#25552;&#20986;&#20102;&#20351;&#29992;Lstm-cnn&#27169;&#22411;&#21019;&#24314;&#20998;&#31867;&#22120;&#65292;&#20197;&#39640;&#20934;&#30830;&#29575;&#21306;&#20998;&#26377;&#23475;&#21644;&#38750;&#26377;&#23475;&#35780;&#35770;&#12290;&#20998;&#31867;&#22120;&#21487;&#20197;&#24110;&#21161;&#32452;&#32455;&#26356;&#22909;&#22320;&#26816;&#26597;&#35780;&#35770;&#21306;&#30340;&#26377;&#23475;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The abstract outlines the problem of toxic comments on social media platforms, where individuals use disrespectful, abusive, and unreasonable language that can drive users away from discussions. This behavior is referred to as anti-social behavior, which occurs during online debates, comments, and fights. The comments containing explicit language can be classified into various categories, such as toxic, severe toxic, obscene, threat, insult, and identity hate. This behavior leads to online harassment and cyberbullying, which forces individuals to stop expressing their opinions and ideas. To protect users from offensive language, companies have started flagging comments and blocking users. The abstract proposes to create a classifier using an Lstm-cnn model that can differentiate between toxic and non-toxic comments with high accuracy. The classifier can help organizations examine the toxicity of the comment section better.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedLSM&#26694;&#26550;&#20197;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#26631;&#31614;&#38598;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#20197;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25110;&#37096;&#20998;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20998;&#31867;&#23618;&#37319;&#29992;&#36880;&#31867;&#21035;&#33258;&#36866;&#24212;&#32858;&#21512;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26631;&#31614;&#38598;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.06931</link><description>&lt;p&gt;
&#38754;&#21521;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#26631;&#31614;&#38598;&#19981;&#21305;&#37197;&#30340;&#35268;&#27169;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scale Federated Learning for Label Set Mismatch in Medical Image Classification. (arXiv:2304.06931v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedLSM&#26694;&#26550;&#20197;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#26631;&#31614;&#38598;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#20197;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25110;&#37096;&#20998;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20998;&#31867;&#23618;&#37319;&#29992;&#36880;&#31867;&#21035;&#33258;&#36866;&#24212;&#32858;&#21512;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26631;&#31614;&#38598;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#19981;&#27844;&#38706;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#22810;&#20551;&#23450;&#27599;&#20010;&#23458;&#25143;&#31471;&#25317;&#26377;&#30456;&#21516;&#30340;&#26631;&#31614;&#38598;&#12290;&#20107;&#23454;&#19978;&#65292;&#21307;&#23398;&#19987;&#23478;&#24448;&#24448;&#21482;&#27880;&#37322;&#20854;&#30693;&#35782;&#39046;&#22495;&#25110;&#20852;&#36259;&#33539;&#22260;&#20869;&#30340;&#30142;&#30149;&#12290;&#36825;&#24847;&#21619;&#30528;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#38598;&#21487;&#33021;&#26159;&#19981;&#21516;&#29978;&#33267;&#26159;&#19981;&#30456;&#20132;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedLSM&#26694;&#26550;&#20197;&#35299;&#20915;&#26631;&#31614;&#38598;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;FedLSM&#37319;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#22788;&#29702;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#31243;&#24230;&#30340;&#25968;&#25454;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#26410;&#26631;&#35760;&#25110;&#37096;&#20998;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20998;&#31867;&#23618;&#37319;&#29992;&#36880;&#31867;&#21035;&#33258;&#36866;&#24212;&#32858;&#21512;&#20197;&#36991;&#20813;&#23458;&#25143;&#31471;&#32570;&#22833;&#26631;&#31614;&#26102;&#30340;&#19981;&#20934;&#30830;&#32858;&#21512;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#20849;&#23454;&#38469;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;FedLSM&#65292;&#21253;&#25324;&#25317;&#26377;112,120&#24352;&#33016;&#36879;&#22270;&#20687;&#30340;&#33016;&#37096;X&#23556;&#32447;&#35786;&#26029;&#21644;&#30382;&#32932;&#30149;&#21464;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has been introduced to the healthcare domain as a decentralized learning paradigm that allows multiple parties to train a model collaboratively without privacy leakage. However, most previous studies have assumed that every client holds an identical label set. In reality, medical specialists tend to annotate only diseases within their knowledge domain or interest. This implies that label sets in each client can be different and even disjoint. In this paper, we propose the framework FedLSM to solve the problem Label Set Mismatch. FedLSM adopts different training strategies on data with different uncertainty levels to efficiently utilize unlabeled or partially labeled data as well as class-wise adaptive aggregation in the classification layer to avoid inaccurate aggregation when clients have missing labels. We evaluate FedLSM on two public real-world medical image datasets, including chest x-ray (CXR) diagnosis with 112,120 CXR images and skin lesion diagnosis wit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#35299;&#37322;&#22120;&#19982;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#22120;&#30340;&#38598;&#25104;&#26694;&#26550;X-Ensemble&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#26816;&#27979;-&#30699;&#27491;&#36807;&#31243;&#65292;&#33021;&#22815;&#36827;&#34892;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2304.06919</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#26159;&#19968;&#31181;&#23433;&#20840;&#65306;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#22120;&#30340;&#38598;&#25104;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpretability is a Kind of Safety: An Interpreter-based Ensemble for Adversary Defense. (arXiv:2304.06919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20102;&#35299;&#37322;&#22120;&#19982;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#37322;&#22120;&#30340;&#38598;&#25104;&#26694;&#26550;X-Ensemble&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#26816;&#27979;-&#30699;&#27491;&#36807;&#31243;&#65292;&#33021;&#22815;&#36827;&#34892;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#28982;&#32780;&#23427;&#20204;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#19968;&#30452;&#21463;&#21040;&#25209;&#35780;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#20570;&#20986;&#20102;&#24040;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#23545;&#25239;&#26679;&#26412;&#30340;&#26412;&#36136;&#29305;&#24449;&#23578;&#19981;&#28165;&#26970;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#28151;&#21512;&#25915;&#20987;&#21644;&#21453;&#21046;&#25915;&#20987;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#25935;&#24863;&#24615;&#20998;&#26512;&#22411;DNN&#35299;&#37322;&#22120;&#19982;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;&#26799;&#24230;&#30456;&#20851;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#24369;&#28857;&#65292;&#24182;&#20026;&#23558;DNN&#30340;&#20004;&#20010;&#38271;&#26399;&#25361;&#25112;&#8212;&#8212;&#33030;&#24369;&#24615;&#21644;&#19981;&#21487;&#35299;&#37322;&#24615;&#32852;&#31995;&#22312;&#19968;&#36215;&#25552;&#20379;&#20102;&#24605;&#36335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;X-Ensemble&#30340;&#22522;&#20110;&#35299;&#37322;&#22120;&#30340;&#38598;&#25104;&#26694;&#26550;&#26469;&#36827;&#34892;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;X-Ensemble&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;-&#30699;&#27491;&#36807;&#31243;&#65292;&#24182;&#22312;&#26500;&#24314;&#22810;&#20010;&#23376;&#26816;&#27979;&#22120;&#21644;&#19968;&#20010;&#26816;&#27979;&#22120;&#38598;&#21512;&#19978;&#20855;&#26377;&#29305;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
While having achieved great success in rich real-life applications, deep neural network (DNN) models have long been criticized for their vulnerability to adversarial attacks. Tremendous research efforts have been dedicated to mitigating the threats of adversarial attacks, but the essential trait of adversarial examples is not yet clear, and most existing methods are yet vulnerable to hybrid attacks and suffer from counterattacks. In light of this, in this paper, we first reveal a gradient-based correlation between sensitivity analysis-based DNN interpreters and the generation process of adversarial examples, which indicates the Achilles's heel of adversarial attacks and sheds light on linking together the two long-standing challenges of DNN: fragility and unexplainability. We then propose an interpreter-based ensemble framework called X-Ensemble for robust adversary defense. X-Ensemble adopts a novel detection-rectification process and features in building multiple sub-detectors and a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;MUP&#26041;&#27861;&#36890;&#36807;&#25513;&#30422;&#26080;&#20851;&#21442;&#25968;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#20855;&#21487;&#36716;&#31227;&#24615;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26679;&#26412;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.06908</link><description>&lt;p&gt;
&#36890;&#36807;&#25513;&#30422;&#26080;&#20851;&#21442;&#25968;&#65292;&#29983;&#25104;&#26356;&#20855;&#21487;&#36716;&#31227;&#24615;&#30340;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Generating Adversarial Examples with Better Transferability via Masking Unimportant Parameters of Surrogate Model. (arXiv:2304.06908v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;MUP&#26041;&#27861;&#36890;&#36807;&#25513;&#30422;&#26080;&#20851;&#21442;&#25968;&#30340;&#26041;&#24335;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#20855;&#21487;&#36716;&#31227;&#24615;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#25552;&#39640;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26679;&#26412;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25915;&#20987;&#12290;&#22312;&#36817;&#24180;&#26469;&#65292;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36716;&#31227;&#24615;&#20063;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#36825;&#24847;&#21619;&#30528;&#30001;&#20195;&#29702;&#27169;&#22411;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#20063;&#21487;&#20197;&#25915;&#20987;&#26410;&#30693;&#27169;&#22411;&#12290;&#36825;&#19968;&#29616;&#35937;&#20135;&#29983;&#20102;&#22522;&#20110;&#36716;&#31227;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25513;&#30422;&#26080;&#20851;&#21442;&#25968;&#65288;MUP&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#36716;&#31227;&#25915;&#20987;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#36716;&#31227;&#24615;&#12290;MUP&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#25913;&#36827;&#39044;&#35757;&#32451;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#25552;&#39640;&#22522;&#20110;&#36716;&#31227;&#25915;&#20987;&#30340;&#25915;&#20987;&#12290;&#22522;&#20110;&#35813;&#24605;&#24819;&#65292;&#20351;&#29992;&#22522;&#20110;&#27888;&#21202;&#23637;&#24320;&#30340;&#24230;&#37327;&#26041;&#27861;&#35780;&#20272;&#21442;&#25968;&#37325;&#35201;&#24615;&#24471;&#20998;&#65292;&#24182;&#22312;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#26102;&#25513;&#30422;&#26080;&#20851;&#37325;&#35201;&#30340;&#21442;&#25968;&#12290;&#36825;&#20010;&#36807;&#31243;&#31616;&#21333;&#26131;&#34892;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#19982;&#21508;&#31181;&#29616;&#26377;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#20197;&#29983;&#25104;&#23545;&#25239;&#24615;&#30340;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;MUP&#26041;&#27861;&#22312;&#21508;&#31181;&#30446;&#26631;&#27169;&#22411;&#19978;&#33719;&#24471;&#20102;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#22686;&#21152;&#20102;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have been shown to be vulnerable to adversarial examples. Moreover, the transferability of the adversarial examples has received broad attention in recent years, which means that adversarial examples crafted by a surrogate model can also attack unknown models. This phenomenon gave birth to the transfer-based adversarial attacks, which aim to improve the transferability of the generated adversarial examples. In this paper, we propose to improve the transferability of adversarial examples in the transfer-based attack via masking unimportant parameters (MUP). The key idea in MUP is to refine the pretrained surrogate models to boost the transfer-based attack. Based on this idea, a Taylor expansion-based metric is used to evaluate the parameter importance score and the unimportant parameters are masked during the generation of adversarial examples. This process is simple, yet can be naturally combined with various existing gradient-based optimizers for generating
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32806;&#21512;&#23383;&#20856;&#23398;&#20064;&#21644;&#36793;&#32536;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#23454;&#26102;&#22270;&#20687;&#27880;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26377;&#38480;&#25968;&#37327;&#30340;&#35270;&#35273;&#21407;&#22411;&#21644;&#30456;&#24212;&#30340;&#35821;&#20041;&#65292;&#24182;&#20445;&#25345;&#26631;&#31614;&#30340;&#31232;&#30095;&#19981;&#24179;&#34913;&#24615;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.06907</link><description>&lt;p&gt;
&#22522;&#20110;&#36793;&#32536;&#21270;&#32806;&#21512;&#23383;&#20856;&#23398;&#20064;&#30340;&#23454;&#26102;&#22270;&#20687;&#27880;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Toward Real-Time Image Annotation Using Marginalized Coupled Dictionary Learning. (arXiv:2304.06907v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32806;&#21512;&#23383;&#20856;&#23398;&#20064;&#21644;&#36793;&#32536;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#23454;&#26102;&#22270;&#20687;&#27880;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#26377;&#38480;&#25968;&#37327;&#30340;&#35270;&#35273;&#21407;&#22411;&#21644;&#30456;&#24212;&#30340;&#35821;&#20041;&#65292;&#24182;&#20445;&#25345;&#26631;&#31614;&#30340;&#31232;&#30095;&#19981;&#24179;&#34913;&#24615;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#22270;&#20687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#22270;&#20687;&#21253;&#21547;&#21508;&#31181;&#39640;&#23618;&#35821;&#20041;&#65292;&#34987;&#31216;&#20026;&#26631;&#31614;&#25110;&#27880;&#37322;&#12290;&#20960;&#20046;&#25152;&#26377;&#22788;&#29702;&#38750;&#22343;&#34913;&#26631;&#35760;&#30340;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#27880;&#37322;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#25628;&#32034;&#30340;&#25216;&#26415;&#65292;&#36825;&#20123;&#25216;&#26415;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32806;&#21512;&#23383;&#20856;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#23398;&#20064;&#26377;&#38480;&#25968;&#37327;&#30340;&#35270;&#35273;&#21407;&#22411;&#21644;&#30456;&#24212;&#30340;&#35821;&#20041;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23454;&#26102;&#22270;&#20687;&#27880;&#37322;&#36807;&#31243;&#12290;&#26412;&#25991;&#30340;&#21478;&#19968;&#20010;&#36129;&#29486;&#26159;&#20351;&#29992;&#36793;&#32536;&#25439;&#22833;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#23545;&#20110;&#19981;&#24179;&#34913;&#26631;&#31614;&#30340;&#22270;&#20687;&#27880;&#37322;&#19981;&#21512;&#36866;&#30340;&#24179;&#26041;&#25439;&#22833;&#20989;&#25968;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#36793;&#32536;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#21033;&#29992;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#21407;&#22411;&#26356;&#26032;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#22312;&#35821;&#20041;&#21407;&#22411;&#19978;&#24341;&#20837;&#20102;${\ell}_1$&#27491;&#21017;&#21270;&#65292;&#20197;&#20445;&#25345;&#23398;&#20064;&#35821;&#20041;&#21407;&#22411;&#20013;&#26631;&#31614;&#30340;&#31232;&#30095;&#19981;&#24179;&#34913;&#24615;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most image retrieval systems, images include various high-level semantics, called tags or annotations. Virtually all the state-of-the-art image annotation methods that handle imbalanced labeling are search-based techniques which are time-consuming. In this paper, a novel coupled dictionary learning approach is proposed to learn a limited number of visual prototypes and their corresponding semantics simultaneously. This approach leads to a real-time image annotation procedure. Another contribution of this paper is that utilizes a marginalized loss function instead of the squared loss function that is inappropriate for image annotation with imbalanced labels. We have employed a marginalized loss function in our method to leverage a simple and effective method of prototype updating. Meanwhile, we have introduced ${\ell}_1$ regularization on semantic prototypes to preserve the sparse and imbalanced nature of labels in learned semantic prototypes. Finally, comprehensive experimental resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#25991;&#29486;&#20013;&#26356;&#24191;&#27867;&#30340;&#31995;&#32479;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#20026;&#20854;&#24320;&#21457;&#20102;&#24418;&#24335;&#20027;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.06901</link><description>&lt;p&gt;
&#31995;&#32479;&#24615;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Systemic Fairness. (arXiv:2304.06901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#31639;&#27861;&#20844;&#24179;&#24615;&#25991;&#29486;&#20013;&#26356;&#24191;&#27867;&#30340;&#31995;&#32479;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#20026;&#20854;&#24320;&#21457;&#20102;&#24418;&#24335;&#20027;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#22330;&#26223;&#20013;&#29992;&#20110;&#20570;&#20986;&#25110;&#25903;&#25345;&#20915;&#31574;&#12290;&#38543;&#30528;&#24191;&#27867;&#20351;&#29992;&#65292;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#20063;&#26085;&#30410;&#20851;&#27880;&#12290;&#20197;&#21069;&#20851;&#20110;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#25991;&#29486;&#24191;&#27867;&#25506;&#35752;&#20102;&#39118;&#38505;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#25552;&#20986;&#20102;&#31649;&#29702;&#26576;&#20123;&#39118;&#38505;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#30001;&#21333;&#20010;&#20915;&#31574;&#21046;&#23450;&#32773;&#25110;&#20195;&#29702;&#20154;&#24341;&#36215;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#19978;&#12290;&#30456;&#21453;&#65292;&#22823;&#22810;&#25968;&#29616;&#23454;&#31995;&#32479;&#37117;&#26377;&#35768;&#22810;&#20195;&#29702;&#20154;&#65292;&#20182;&#20204;&#20849;&#21516;&#20316;&#20026;&#26356;&#22823;&#29983;&#24577;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#24037;&#20316;&#12290;&#20363;&#22914;&#65292;&#22312;&#36151;&#27454;&#22330;&#26223;&#20013;&#65292;&#26377;&#22810;&#20010;&#25918;&#36151;&#20154;&#35780;&#20272;&#30003;&#35831;&#32773;&#30340;&#36151;&#27454;&#65292;&#21516;&#26102;&#36824;&#26377;&#20915;&#31574;&#21046;&#23450;&#32773;&#21644;&#20854;&#20182;&#26426;&#26500;&#20854;&#20915;&#31574;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#21333;&#20010;&#20915;&#31574;&#21046;&#23450;&#32773;&#20570;&#20986;&#30340;&#20219;&#20309;&#25918;&#36151;&#20915;&#31574;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#21487;&#33021;&#21462;&#20915;&#20110;&#29983;&#24577;&#31995;&#32479;&#20013;&#22810;&#20010;&#19981;&#21516;&#20195;&#29702;&#20154;&#30340;&#34892;&#21160;&#12290;&#26412;&#25991;&#20026;&#20225;&#19994;&#21644;&#31995;&#32479;&#20844;&#24179;&#24615;&#24320;&#21457;&#20102;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#21628;&#21505;&#31639;&#27861;&#20844;&#24179;&#24615;&#25991;&#29486;&#26356;&#21152;&#20851;&#27880;&#21518;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are increasingly used to make or support decisions in a wide range of settings. With such expansive use there is also growing concern about the fairness of such methods. Prior literature on algorithmic fairness has extensively addressed risks and in many cases presented approaches to manage some of them. However, most studies have focused on fairness issues that arise from actions taken by a (single) focal decision-maker or agent. In contrast, most real-world systems have many agents that work collectively as part of a larger ecosystem. For example, in a lending scenario, there are multiple lenders who evaluate loans for applicants, along with policymakers and other institutions whose decisions also affect outcomes. Thus, the broader impact of any lending decision of a single decision maker will likely depend on the actions of multiple different agents in the ecosystem. This paper develops formalisms for firm versus systemic fairness, and calls for a greater
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25191;&#34892;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25214;&#21040;&#20855;&#26377;&#25191;&#34892;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#22120;&#26469;&#36866;&#29992;&#20110;&#25968;&#25454;&#20998;&#24067;&#12290;&#36890;&#36807;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#30456;&#23545;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#20540;&#21487;Lipschitz&#36830;&#32493;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25918;&#23485;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20551;&#35774;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.06879</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#19979;&#30340;&#25191;&#34892;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performative Prediction with Neural Networks. (arXiv:2304.06879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25191;&#34892;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25214;&#21040;&#20855;&#26377;&#25191;&#34892;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#22120;&#26469;&#36866;&#29992;&#20110;&#25968;&#25454;&#20998;&#24067;&#12290;&#36890;&#36807;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#30456;&#23545;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#20540;&#21487;Lipschitz&#36830;&#32493;&#65292;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#25918;&#23485;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20551;&#35774;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25191;&#34892;&#39044;&#27979;&#26159;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#24182;&#24433;&#21709;&#20854;&#39044;&#27979;&#25968;&#25454;&#30340;&#26694;&#26550;&#12290;&#26412;&#25991;&#26088;&#22312;&#25214;&#21040;&#20998;&#31867;&#22120;&#65292;&#20351;&#20854;&#20855;&#26377;&#25191;&#34892;&#31283;&#23450;&#24615;&#65292;&#21363;&#36866;&#29992;&#20110;&#20854;&#20135;&#29983;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#26368;&#20339;&#20998;&#31867;&#22120;&#12290;&#22312;&#20351;&#29992;&#37325;&#22797;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#25214;&#21040;&#20855;&#26377;&#25191;&#34892;&#31283;&#23450;&#24615;&#30340;&#20998;&#31867;&#22120;&#30340;&#26631;&#20934;&#25910;&#25947;&#32467;&#26524;&#20013;&#65292;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#23545;&#20110;&#27169;&#22411;&#21442;&#25968;&#26159;&#21487;Lipschitz&#36830;&#32493;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25439;&#22833;&#24517;&#39035;&#23545;&#36825;&#20123;&#21442;&#25968;&#24378;&#20984;&#21644;&#24179;&#28369;&#65307;&#21542;&#21017;&#65292;&#35813;&#26041;&#27861;&#23558;&#22312;&#26576;&#20123;&#38382;&#39064;&#19978;&#21457;&#25955;&#12290;&#28982;&#32780;&#26412;&#25991;&#21017;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#26159;&#30456;&#23545;&#20110;&#27169;&#22411;&#30340;&#39044;&#27979;&#20540;&#21487;Lipschitz&#36830;&#32493;&#30340;&#65292;&#36825;&#26159;&#25191;&#34892;&#31995;&#32479;&#30340;&#26356;&#21152;&#33258;&#28982;&#30340;&#20551;&#35774;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25918;&#23485;&#23545;&#25439;&#22833;&#20989;&#25968;&#30340;&#20551;&#35774;&#35201;&#27714;&#12290;&#20316;&#20026;&#19968;&#20010;&#35828;&#26126;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24314;&#27169;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#37325;&#37319;&#26679;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#20854;&#26469;&#23454;&#35777;&#25191;&#34892;&#31283;&#23450;&#24615;&#30456;&#23545;&#20110;&#20854;&#20182;&#30446;&#26631;&#30340;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performative prediction is a framework for learning models that influence the data they intend to predict. We focus on finding classifiers that are performatively stable, i.e. optimal for the data distribution they induce. Standard convergence results for finding a performatively stable classifier with the method of repeated risk minimization assume that the data distribution is Lipschitz continuous to the model's parameters. Under this assumption, the loss must be strongly convex and smooth in these parameters; otherwise, the method will diverge for some problems. In this work, we instead assume that the data distribution is Lipschitz continuous with respect to the model's predictions, a more natural assumption for performative systems. As a result, we are able to significantly relax the assumptions on the loss function. In particular, we do not need to assume convexity with respect to the model's parameters. As an illustration, we introduce a resampling procedure that models realisti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06875</link><description>&lt;p&gt;
&#19981;&#38656;&#37325;&#26032;&#25628;&#32034;&#30340;&#30740;&#31350;&#65306;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#21487;&#31934;&#30830;&#39044;&#27979;&#36328;&#23610;&#24230;&#30340;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#39564;&#35777;&#30740;&#31350;&#24819;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#65292;&#22240;&#20026;&#23567;&#27169;&#22411;&#30340;&#32467;&#35770;&#19981;&#33021;&#31616;&#21333;&#22320;&#36716;&#31227;&#21040;&#22823;&#27169;&#22411;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#31995;&#32479;&#65292;&#20165;&#22522;&#20110;&#23567;&#27169;&#22411;&#30340;&#32467;&#26524;&#21644;&#36229;&#21442;&#25968;&#30452;&#25509;&#39044;&#27979;&#22823;&#27169;&#22411;&#30340;&#19968;&#20123;&#25351;&#26631;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#32553;&#25918;&#24459;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#65288;muP&#65289;&#20351;&#24471;&#21487;&#20197;&#22312;&#38752;&#36817;&#24120;&#35265;&#25439;&#22833;&#27969;&#22495;&#30340;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25311;&#21512;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#25628;&#32034;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#22823;&#23610;&#24230;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#65292;&#22312;&#35757;&#32451;&#24320;&#22987;&#20043;&#21069;&#23601;&#21487;&#20197;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20316;&#20026;&#21487;&#38752;&#30340;&#23398;&#26415;&#30740;&#31350;&#30340;&#31532;&#19968;&#27493;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#35268;&#27169;&#65292;&#32780;&#19981;&#38656;&#22823;&#37327;&#30340;&#35745;&#31639;&#12290;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
&lt;/p&gt;</description></item><item><title>&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20013;&#65292;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#23454;&#39564;&#21078;&#26512;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#33410;&#22863;&#20272;&#35745;&#30340;&#33258;&#30417;&#30563;&#38899;&#39640;&#20272;&#35745;&#27169;&#22411;&#65292;&#30528;&#37325;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#33410;&#22863;&#20272;&#35745;&#20013;&#36755;&#20837;&#34920;&#36848;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.06868</link><description>&lt;p&gt;
&#36895;&#24230;&#19982;&#38899;&#35843;&#65306;&#29702;&#35299;&#33258;&#30417;&#30563;&#33410;&#22863;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Tempo vs. Pitch: understanding self-supervised tempo estimation. (arXiv:2304.06868v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06868
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20013;&#65292;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#23454;&#39564;&#21078;&#26512;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#33410;&#22863;&#20272;&#35745;&#30340;&#33258;&#30417;&#30563;&#38899;&#39640;&#20272;&#35745;&#27169;&#22411;&#65292;&#30528;&#37325;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#33410;&#22863;&#20272;&#35745;&#20013;&#36755;&#20837;&#34920;&#36848;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#26041;&#27861;&#36890;&#36807;&#35299;&#20915;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#20808;&#20915;&#20219;&#21153;&#26469;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;&#65292;&#20943;&#36731;&#20102;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#30340;&#26631;&#27880;&#24037;&#20316;&#12290;&#36825;&#20123;&#26041;&#27861;&#24050;&#34987;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#29615;&#22659;&#22768;&#38899;&#20998;&#26512;&#65292;&#26368;&#36817;&#36824;&#29992;&#20110;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65292;&#20363;&#22914;&#38899;&#39640;&#20272;&#35745;&#12290;&#23588;&#20854;&#26159;&#22312;&#38899;&#20048;&#39046;&#22495;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#33030;&#24369;&#24615;&#20197;&#21450;&#22914;&#20309;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#30340;&#35265;&#35299;&#36824;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#21078;&#26512;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#38899;&#39640;&#20272;&#35745;&#27169;&#22411;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33410;&#22863;&#20272;&#35745;&#65292;&#25506;&#35752;&#20102;&#36755;&#20837;&#34920;&#31034;&#21644;&#25968;&#25454;&#20998;&#24067;&#19982;&#33258;&#30417;&#30563;&#33410;&#22863;&#20272;&#35745;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervision methods learn representations by solving pretext tasks that do not require human-generated labels, alleviating the need for time-consuming annotations. These methods have been applied in computer vision, natural language processing, environmental sound analysis, and recently in music information retrieval, e.g. for pitch estimation. Particularly in the context of music, there are few insights about the fragility of these models regarding different distributions of data, and how they could be mitigated. In this paper, we explore these questions by dissecting a self-supervised model for pitch estimation adapted for tempo estimation via rigorous experimentation with synthetic data. Specifically, we study the relationship between the input representation and data distribution for self-supervised tempo estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;&#19977;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20004;&#20010;&#20559;&#35265;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#38543;&#30528;&#25216;&#26415;&#36827;&#27493;&#65292;&#26368;&#26032;&#30340;&#12289;&#26356;&#24555;&#12289;&#26356;&#36731;&#30340;&#27169;&#22411;&#22312;&#24320;&#21457;&#26102;&#36127;&#36131;&#20219;&#22320;&#38477;&#20302;&#20102;&#19982;&#26087;&#27169;&#22411;&#30456;&#27604;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2304.06861</link><description>&lt;p&gt;
&#23545;&#26368;&#26032;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Social Biases in Recent Large Pre-Trained Models. (arXiv:2304.06861v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;&#19977;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20004;&#20010;&#20559;&#35265;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#38543;&#30528;&#25216;&#26415;&#36827;&#27493;&#65292;&#26368;&#26032;&#30340;&#12289;&#26356;&#24555;&#12289;&#26356;&#36731;&#30340;&#27169;&#22411;&#22312;&#24320;&#21457;&#26102;&#36127;&#36131;&#20219;&#22320;&#38477;&#20302;&#20102;&#19982;&#26087;&#27169;&#22411;&#30456;&#27604;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#22312;&#31038;&#21306;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#26469;&#33258;&#20110;&#20114;&#32852;&#32593;&#31561;&#24320;&#25918;&#26469;&#28304;&#30340;&#26410;&#23457;&#26680;&#25110;&#26410;&#31579;&#36873;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#30001;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#30475;&#21040;&#30340;&#20559;&#35265;&#21453;&#26144;&#20102;&#31038;&#20250;&#19978;&#30340;&#20559;&#35265;&#65292;&#24182;&#34987;&#36825;&#20123;&#27169;&#22411;&#25152;&#25429;&#25417;&#21644;&#23398;&#20064;&#12290;&#36825;&#20123;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#24433;&#21709;&#25968;&#30334;&#19975;&#20154;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#23427;&#20204;&#20869;&#22312;&#30340;&#20559;&#35265;&#23545;&#20110;&#23450;&#21521;&#30340;&#31038;&#20250;&#32676;&#20307;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#26032;&#39044;&#35757;&#32451;&#27169;&#22411;&#21457;&#24067;&#21518;&#30340;&#20559;&#35265;&#32553;&#20943;&#36235;&#21183;&#12290;&#36873;&#25321;&#20102;&#19977;&#20010;&#26368;&#26032;&#27169;&#22411;(ELECTRA&#12289;DeBERTa&#21644;DistilBERT)&#65292;&#24182;&#23545;&#20004;&#20010;&#20559;&#35265;&#22522;&#20934;&#65288;StereoSet&#21644;CrowS-Pairs&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#20204;&#20351;&#29992;&#30456;&#20851;&#24230;&#37327;&#26631;&#20934;&#19982;BERT&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25506;&#32034;&#26159;&#21542;&#38543;&#30528;&#25216;&#26415;&#30340;&#36827;&#27493;&#21644;&#26032;&#30340;&#12289;&#26356;&#24555;&#12289;&#26356;&#36731;&#30340;&#27169;&#22411;&#21457;&#24067;&#65292;&#23427;&#20204;&#26159;&#21542;&#36127;&#36131;&#20219;&#22320;&#21457;&#23637;&#65292;&#20351;&#20854;&#20869;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#19982;&#26087;&#27169;&#22411;&#30456;&#27604;&#26377;&#25152;&#38477;&#20302;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. In this work, we study the general trend in bias reduction as newer pre-trained models are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT) are chosen and evaluated against two bias benchmarks, StereoSet and CrowS-Pairs. They are compared to the baseline of BERT using the associated metrics. We explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? The re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25512;&#29305;&#30123;&#33495;&#25968;&#25454;&#38598;Vax-Culture&#65292;&#23427;&#26088;&#22312;&#25214;&#20986;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#24110;&#21161;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.06858</link><description>&lt;p&gt;
Vax-Culture: &#29992;&#20110;&#30740;&#31350;&#25512;&#29305;&#19978;&#30123;&#33495;&#35752;&#35770;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25512;&#29305;&#30123;&#33495;&#25968;&#25454;&#38598;Vax-Culture&#65292;&#23427;&#26088;&#22312;&#25214;&#20986;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#24110;&#21161;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#26399;&#38388;&#65292;&#30123;&#33495;&#29369;&#35947;&#32487;&#32493;&#26159;&#20844;&#20849;&#21355;&#29983;&#23448;&#21592;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#35813;&#29369;&#35947;&#30772;&#22351;&#20102;&#30123;&#33495;&#36816;&#21160;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#30830;&#23450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#21453;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#19981;&#26029;&#22686;&#38271;&#26159;&#35813;&#38382;&#39064;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#23558;&#25512;&#29305;&#20316;&#20026;&#35823;&#23548;&#20869;&#23481;&#30340;&#26469;&#28304;&#65292;&#24182;&#26088;&#22312;&#25552;&#21462;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#19982;&#30123;&#33495;&#26377;&#20851;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#24182;&#20511;&#21161;&#19987;&#19994;&#27807;&#36890;&#21644;&#26032;&#38395;&#32972;&#26223;&#30340;&#27880;&#37322;&#20154;&#21592;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#26368;&#32456;&#24076;&#26395;&#36825;&#21487;&#20197;&#24102;&#26469;&#26377;&#25928;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#20844;&#20849;&#21355;&#29983;&#36890;&#20449;&#31574;&#30053;&#65292;&#20197;&#25509;&#35302;&#37027;&#20123;&#25345;&#21453;&#30123;&#33495;&#20449;&#20208;&#32773;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20449;&#24687;&#26377;&#21161;&#20110;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine hesitancy continues to be a main challenge for public health officials during the COVID-19 pandemic. As this hesitancy undermines vaccine campaigns, many researchers have sought to identify its root causes, finding that the increasing volume of anti-vaccine misinformation on social media platforms is a key element of this problem. We explored Twitter as a source of misleading content with the goal of extracting overlapping cultural and political beliefs that motivate the spread of vaccine misinformation. To do this, we have collected a data set of vaccine-related Tweets and annotated them with the help of a team of annotators with a background in communications and journalism. Ultimately we hope this can lead to effective and targeted public health communication strategies for reaching individuals with anti-vaccine beliefs. Moreover, this information helps with developing Machine Learning models to automatically detect vaccine misinformation posts and combat their negative impa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;CAR-DESPOT&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.06848</link><description>&lt;p&gt;
CAR-DESPOT: &#38024;&#23545;&#28151;&#26434;&#29615;&#22659;&#19979;&#30340;&#26426;&#22120;&#20154;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;CAR-DESPOT&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#65292;&#24182;&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#24037;&#20316;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#32771;&#34385;&#38543;&#26426;&#34892;&#20026;&#30340;&#21487;&#33021;&#32467;&#26524;&#65292;&#24182;&#26681;&#25454;&#30495;&#23454;&#30340;&#19990;&#30028;&#29366;&#24577;&#30340;&#37096;&#20998;&#35266;&#23519;&#36827;&#34892;&#20915;&#31574;&#12290;&#22240;&#26524;&#28151;&#28102;&#30340;&#38382;&#39064;&#26159;&#36827;&#34892;&#20934;&#30830;&#21644;&#24378;&#20581;&#30340;&#34892;&#20026;&#39044;&#27979;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(POMDP)&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#27169;&#25311;&#36825;&#20123;&#38543;&#26426;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26126;&#30830;&#30340;&#22240;&#26524;&#35821;&#20041;&#65292;POMDP&#35268;&#21010;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#28151;&#28102;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#22312;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#34920;&#29616;&#19981;&#20339;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#22312;&#32447;POMDP&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#22240;&#26524;&#24314;&#27169;&#21644;&#25512;&#29702;&#26469;&#28040;&#38500;&#26410;&#27979;&#37327;&#28151;&#28102;&#21464;&#37327;&#24341;&#36215;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#20351;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;CAR-DESPOT&#22312;&#28151;&#26434;&#29615;&#22659;&#20013;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;POMDP&#35268;&#21010;&#31243;&#24207;&#34920;&#29616;&#26174;&#33879;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots operating in real-world environments must reason about possible outcomes of stochastic actions and make decisions based on partial observations of the true world state. A major challenge for making accurate and robust action predictions is the problem of confounding, which if left untreated can lead to prediction errors. The partially observable Markov decision process (POMDP) is a widely-used framework to model these stochastic and partially-observable decision-making problems. However, due to a lack of explicit causal semantics, POMDP planning methods are prone to confounding bias and thus in the presence of unobserved confounders may produce underperforming policies. This paper presents a novel causally-informed extension of "anytime regularized determinized sparse partially observable tree" (AR-DESPOT), a modern anytime online POMDP planner, using causal modelling and inference to eliminate errors caused by unmeasured confounder variables. We further propose a method to lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#25506;&#32034;&#26694;&#26550;&#65292;&#21253;&#25324;&#29992;&#25143;-&#21019;&#24314;&#32773;&#25506;&#32034;&#12289;&#22312;&#32447;&#25506;&#32034;&#26694;&#26550;&#21644;&#21453;&#39304;&#32452;&#25104;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#27969;&#34892;&#20869;&#23481;&#30340;&#38480;&#21046;&#21644;&#26080;&#27861;&#31995;&#32479;&#25506;&#32034;&#29992;&#25143;&#20852;&#36259;&#30340;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#30340;&#25972;&#20307;&#36136;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06844</link><description>&lt;p&gt;
PIE: &#38024;&#23545;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
PIE: Personalized Interest Exploration for Large-Scale Recommender Systems. (arXiv:2304.06844v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#30340;&#25506;&#32034;&#26694;&#26550;&#65292;&#21253;&#25324;&#29992;&#25143;-&#21019;&#24314;&#32773;&#25506;&#32034;&#12289;&#22312;&#32447;&#25506;&#32034;&#26694;&#26550;&#21644;&#21453;&#39304;&#32452;&#25104;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#27969;&#34892;&#20869;&#23481;&#30340;&#38480;&#21046;&#21644;&#26080;&#27861;&#31995;&#32479;&#25506;&#32034;&#29992;&#25143;&#20852;&#36259;&#30340;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#30340;&#25972;&#20307;&#36136;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36234;&#26469;&#36234;&#25104;&#21151;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#20010;&#24615;&#21270;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#24120;&#24120;&#21033;&#29992;&#28909;&#38376;&#20869;&#23481;&#65292;&#19988;&#29992;&#25143;&#20852;&#36259;&#30340;&#25345;&#32493;&#28436;&#36827;&#38656;&#35201;&#34987;&#25429;&#25417;&#65292;&#20294;&#27809;&#26377;&#30452;&#25509;&#30340;&#26041;&#24335;&#26469;&#31995;&#32479;&#24615;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#20852;&#36259;&#12290;&#36825;&#20063;&#32463;&#24120;&#24433;&#21709;&#21040;&#25512;&#33616;&#31995;&#32479;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#26159;&#20174;&#25512;&#33616;&#32473;&#29992;&#25143;&#30340;&#20505;&#36873;&#39033;&#20013;&#29983;&#25104;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25506;&#32034;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#23427;&#30001;&#19977;&#20010;&#37096;&#20998;&#32452;&#25104;&#65306;&#31532;&#19968;&#37096;&#20998;&#26159;&#29992;&#25143;&#21019;&#24314;&#32773;&#25506;&#32034;&#65292;&#19987;&#27880;&#20110;&#35782;&#21035;&#29992;&#25143;&#24863;&#20852;&#36259;&#30340;&#26368;&#20339;&#21019;&#24314;&#32773;&#65292;&#31532;&#20108;&#37096;&#20998;&#26159;&#22312;&#32447;&#25506;&#32034;&#26694;&#26550;&#65292;&#31532;&#19977;&#37096;&#20998;&#26159;&#19968;&#20010;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#21453;&#39304;&#32452;&#25104;&#26426;&#21046;&#65292;&#20197;&#30830;&#20445;&#25506;&#32034;&#24615;&#35270;&#39057;&#30340;&#26368;&#20339;&#26222;&#21450;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#34987;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#21482;&#38656;&#36827;&#34892;&#26368;&#23567;&#38480;&#24230;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are increasingly successful in recommending personalized content to users. However, these systems often capitalize on popular content. There is also a continuous evolution of user interests that need to be captured, but there is no direct way to systematically explore users' interests. This also tends to affect the overall quality of the recommendation pipeline as training data is generated from the candidates presented to the user. In this paper, we present a framework for exploration in large-scale recommender systems to address these challenges. It consists of three parts, first the user-creator exploration which focuses on identifying the best creators that users are interested in, second the online exploration framework and third a feed composition mechanism that balances explore and exploit to ensure optimal prevalence of exploratory videos. Our methodology can be easily integrated into an existing large-scale recommender system with minimal modifications. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#23558;&#24103;&#36716;&#21270;&#20026;&#26102;&#38388;&#24207;&#21015;&#24182;&#20351;&#29992;&#23545;&#35282;&#21270;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#31639;&#27861;&#36827;&#34892;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2304.06841</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#29992;&#20110;&#35270;&#39057;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Video alignment using unsupervised learning of local and global features. (arXiv:2304.06841v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#23545;&#40784;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#23558;&#24103;&#36716;&#21270;&#20026;&#26102;&#38388;&#24207;&#21015;&#24182;&#20351;&#29992;&#23545;&#35282;&#21270;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#31639;&#27861;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#35270;&#39057;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#21363;&#21305;&#37197;&#21253;&#21547;&#30456;&#20284;&#27963;&#21160;&#30340;&#19968;&#23545;&#35270;&#39057;&#30340;&#24103;&#12290;&#35270;&#39057;&#23545;&#40784;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#65292;&#23613;&#31649;&#20004;&#20010;&#35270;&#39057;&#20043;&#38388;&#30340;&#25191;&#34892;&#36807;&#31243;&#21644;&#22806;&#35266;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#20173;&#38656;&#35201;&#24314;&#31435;&#31934;&#30830;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24103;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#36827;&#34892;&#23545;&#40784;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20154;&#29289;&#26816;&#27979;&#12289;&#23039;&#24577;&#20272;&#35745;&#21644;VGG&#32593;&#32476;&#19977;&#31181;&#26426;&#22120;&#35270;&#35273;&#24037;&#20855;&#20026;&#27599;&#20010;&#35270;&#39057;&#24103;&#24341;&#20837;&#26377;&#25928;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#23545;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#22788;&#29702;&#21644;&#32452;&#21512;&#20197;&#26500;&#24314;&#20195;&#34920;&#35270;&#39057;&#30340;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#12290;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;&#23545;&#35282;&#21270;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#30340;&#26032;&#29256;&#26412;&#65288;Diagonalized Dynamic Time Warping, DDTW&#65289;&#23545;&#29983;&#25104;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#28857;&#22312;&#20110;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#20219;&#20309;&#26032;&#31867;&#22411;&#30340;&#27963;&#21160;&#32780;&#26080;&#38656;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we tackle the problem of video alignment, the process of matching the frames of a pair of videos containing similar actions. The main challenge in video alignment is that accurate correspondence should be established despite the differences in the execution processes and appearances between the two videos. We introduce an unsupervised method for alignment that uses global and local features of the frames. In particular, we introduce effective features for each video frame by means of three machine vision tools: person detection, pose estimation, and VGG network. Then the features are processed and combined to construct a multidimensional time series that represent the video. The resulting time series are used to align videos of the same actions using a novel version of dynamic time warping named Diagonalized Dynamic Time Warping(DDTW). The main advantage of our approach is that no training is required, which makes it applicable for any new type of action without any need
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22810;&#20219;&#21153;&#27169;&#22411;&#19978;&#24212;&#29992;&#32467;&#26500;&#21270;&#21098;&#26525;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#30456;&#20284;&#30340;&#24773;&#20917;&#19979;&#65292;&#26469;&#33258;&#19981;&#21516;&#21098;&#26525;&#26041;&#27861;&#30340;&#26550;&#26500;&#22312;&#20219;&#21153;&#24615;&#33021;&#19978;&#27809;&#26377;&#26174;&#30528;&#24046;&#24322;&#65292;&#36845;&#20195;&#32467;&#26500;&#21098;&#26525;&#21487;&#33021;&#19981;&#26159;&#23454;&#29616;&#26368;&#20248;&#32467;&#26500;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.06840</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Structured Pruning for Multi-Task Deep Neural Networks. (arXiv:2304.06840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22810;&#20219;&#21153;&#27169;&#22411;&#19978;&#24212;&#29992;&#32467;&#26500;&#21270;&#21098;&#26525;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#21442;&#25968;&#25968;&#37327;&#30456;&#20284;&#30340;&#24773;&#20917;&#19979;&#65292;&#26469;&#33258;&#19981;&#21516;&#21098;&#26525;&#26041;&#27861;&#30340;&#26550;&#26500;&#22312;&#20219;&#21153;&#24615;&#33021;&#19978;&#27809;&#26377;&#26174;&#30528;&#24046;&#24322;&#65292;&#36845;&#20195;&#32467;&#26500;&#21098;&#26525;&#21487;&#33021;&#19981;&#26159;&#23454;&#29616;&#26368;&#20248;&#32467;&#26500;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#30456;&#23545;&#20110;&#21333;&#20010;&#21333;&#20219;&#21153;DNN&#27169;&#22411;&#65292;&#22810;&#20219;&#21153;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#35745;&#31639;&#21644;&#23384;&#20648;&#20248;&#21183;&#65292;&#20294;&#26159;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#21387;&#32553;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;&#35768;&#22810;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#23454;&#29616;&#21333;&#20219;&#21153;&#27169;&#22411;&#30340;&#21152;&#36895;&#65292;&#20294;&#26159;&#23545;&#20110;&#22810;&#20219;&#21153;&#32593;&#32476;&#30340;&#21098;&#26525;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#22312;&#22810;&#20219;&#21153;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#21333;&#20219;&#21153;&#28388;&#27874;&#22120;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;MTL&#30340;&#28388;&#27874;&#22120;&#21098;&#26525;&#20934;&#21017;&#26469;&#20272;&#35745;&#28388;&#27874;&#22120;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#36845;&#20195;&#21098;&#26525;&#31574;&#30053;&#20351;&#29992;&#20004;&#31181;&#21098;&#26525;&#26041;&#27861;&#26469;&#21098;&#26525;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#20180;&#32454;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#19979;&#65292;&#24403;&#21442;&#25968;&#25968;&#37327;&#30456;&#20284;&#26102;&#65292;&#26469;&#33258;&#19981;&#21516;&#21098;&#26525;&#26041;&#27861;&#30340;&#26550;&#26500;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#24615;&#33021;&#19978;&#27809;&#26377;&#26174;&#30528;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36845;&#20195;&#32467;&#26500;&#21098;&#26525;&#21487;&#33021;&#19981;&#26159;&#23454;&#29616;&#22810;&#20219;&#21153;&#32593;&#32476;&#26368;&#20248;&#32467;&#26500;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although multi-task deep neural network (DNN) models have computation and storage benefits over individual single-task DNN models, they can be further optimized via model compression. Numerous structured pruning methods are already developed that can readily achieve speedups in single-task models, but the pruning of multi-task networks has not yet been extensively studied. In this work, we investigate the effectiveness of structured pruning on multi-task models. We use an existing single-task filter pruning criterion and also introduce an MTL-based filter pruning criterion for estimating the filter importance scores. We prune the model using an iterative pruning strategy with both pruning methods. We show that, with careful hyper-parameter tuning, architectures obtained from different pruning methods do not have significant differences in their performances across tasks when the number of parameters is similar. We also show that iterative structure pruning may not be the best way to ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#21508;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#32467;&#26500;&#22312;&#25968;&#23383;&#23376;&#36733;&#27874;&#22797;&#29992;&#65288;DSCM&#65289;&#20809;&#20256;&#36755;&#31995;&#32479;&#20013;&#27169;&#25311;&#21644;&#34917;&#20607;&#20809;&#32420;&#38750;&#32447;&#24615;&#24178;&#25200;&#65292;&#36890;&#36807;&#24212;&#29992;&#20840;&#36830;&#25509;&#32593;&#32476;&#21644;&#27169;&#22359;&#21270;&#32467;&#26500;&#26469;&#23436;&#25104;&#38750;&#32447;&#24615;&#20449;&#36947;&#22343;&#34913;&#65292;&#20026;&#26410;&#26469;&#20809;&#23398;&#36890;&#20449;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#29616;&#23454;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.06836</link><description>&lt;p&gt;
&#25968;&#23383;&#23376;&#36733;&#27874;&#22797;&#29992;&#31995;&#32479;&#20013;&#20809;&#20449;&#36947;&#38750;&#32447;&#24615;&#34917;&#20607;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Neural Network Architectures for Optical Channel Nonlinear Compensation in Digital Subcarrier Multiplexing Systems. (arXiv:2304.06836v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#21508;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#32467;&#26500;&#22312;&#25968;&#23383;&#23376;&#36733;&#27874;&#22797;&#29992;&#65288;DSCM&#65289;&#20809;&#20256;&#36755;&#31995;&#32479;&#20013;&#27169;&#25311;&#21644;&#34917;&#20607;&#20809;&#32420;&#38750;&#32447;&#24615;&#24178;&#25200;&#65292;&#36890;&#36807;&#24212;&#29992;&#20840;&#36830;&#25509;&#32593;&#32476;&#21644;&#27169;&#22359;&#21270;&#32467;&#26500;&#26469;&#23436;&#25104;&#38750;&#32447;&#24615;&#20449;&#36947;&#22343;&#34913;&#65292;&#20026;&#26410;&#26469;&#20809;&#23398;&#36890;&#20449;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#29616;&#23454;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#32467;&#26500;&#26469;&#23545;&#25968;&#23383;&#23376;&#36733;&#27874;&#22797;&#29992;&#65288;DSCM&#65289;&#20809;&#20256;&#36755;&#31995;&#32479;&#20013;&#30340;&#23376;&#36733;&#27874;&#20869;&#37096;&#21644;&#20114;&#38388;&#20809;&#32420;&#38750;&#32447;&#24615;&#24178;&#25200;&#24314;&#27169;&#21644;&#34917;&#20607;&#12290;&#25105;&#20204;&#37319;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#23618;&#31561;&#19981;&#21516;&#30340;ANN&#26680;&#24515;&#36827;&#34892;&#38750;&#32447;&#24615;&#20449;&#36947;&#22343;&#34913;&#12290;&#36890;&#36807;&#22312;&#25152;&#26377;&#23376;&#36733;&#27874;&#19978;&#37319;&#29992;&#20840;&#36830;&#25509;&#32593;&#32476;&#26469;&#24320;&#22987;&#23545;DSCM&#31995;&#32479;&#20013;&#20809;&#32420;&#38750;&#32447;&#24615;&#22833;&#30495;&#36827;&#34892;&#34917;&#20607;&#12290;&#22312;&#38543;&#21518;&#30340;&#27493;&#39588;&#20013;&#65292;&#20511;&#37492;&#20809;&#32420;&#38750;&#32447;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#36880;&#27493;&#23558;&#35774;&#35745;&#21319;&#32423;&#20026;&#20855;&#26377;&#26356;&#22909;&#24615;&#33021;-&#22797;&#26434;&#24615;&#20248;&#21183;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;DSCM&#31995;&#32479;&#30340;ANN&#38750;&#32447;&#24615;&#22343;&#34913;&#22120;&#35774;&#35745;&#20013;&#25918;&#32622;&#36866;&#24403;&#30340;&#23439;&#35266;&#32467;&#26500;&#23545;&#26410;&#26469;&#19990;&#20195;&#30456;&#24178;&#20809;&#25910;&#21457;&#26426;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose to use various artificial neural network (ANN) structures for modeling and compensation of intra- and inter-subcarrier fiber nonlinear interference in digital subcarrier multiplexing (DSCM) optical transmission systems. We perform nonlinear channel equalization by employing different ANN cores including convolutional neural networks (CNN) and long short-term memory (LSTM) layers. We start to compensate the fiber nonlinearity distortion in DSCM systems by a fully connected network across all subcarriers. In subsequent steps, and borrowing from fiber nonlinearity analysis, we gradually upgrade the designs towards modular structures with better performance-complexity advantages. Our study shows that putting proper macro structures in design of ANN nonlinear equalizers in DSCM systems can be crucial for practical solutions in future generations of coherent optical transceivers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#65292;&#24403;&#27169;&#22411;&#31867;&#36275;&#22815;&#20016;&#23500;&#20197;&#28085;&#30422;&#30495;&#23454;&#24773;&#20917;&#26102;&#65292;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#8220;&#20808;&#20272;&#35745;&#20877;&#20248;&#21270;&#8221;&#26041;&#27861;&#20248;&#20110;&#38598;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20248;&#21270;&#38388;&#38553;&#30340;&#28176;&#36827;&#20248;&#21183;&#30340;&#22343;&#20540;&#65292;&#25152;&#26377;&#20854;&#20182;&#26102;&#21051;&#21644;&#25972;&#20010;&#28176;&#36827;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2304.06833</link><description>&lt;p&gt;
&#35780;&#20272;-&#20248;&#21270;&#26041;&#27861;&#19982;&#38598;&#25104;&#35780;&#20272;&#20248;&#21270;&#27861;&#65306;&#22522;&#20110;&#38543;&#26426;&#20248;&#21183;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective. (arXiv:2304.06833v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#65292;&#24403;&#27169;&#22411;&#31867;&#36275;&#22815;&#20016;&#23500;&#20197;&#28085;&#30422;&#30495;&#23454;&#24773;&#20917;&#26102;&#65292;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#8220;&#20808;&#20272;&#35745;&#20877;&#20248;&#21270;&#8221;&#26041;&#27861;&#20248;&#20110;&#38598;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20248;&#21270;&#38388;&#38553;&#30340;&#28176;&#36827;&#20248;&#21183;&#30340;&#22343;&#20540;&#65292;&#25152;&#26377;&#20854;&#20182;&#26102;&#21051;&#21644;&#25972;&#20010;&#28176;&#36827;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#38543;&#26426;&#20248;&#21270;&#20013;&#65292;&#38500;&#20102;&#38656;&#35201;&#20248;&#21270;&#20219;&#21153;&#65292;&#36824;&#38656;&#35201;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#28508;&#22312;&#20998;&#24067;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#36890;&#36807;&#36873;&#25321;&#23548;&#33268;&#26368;&#20339;&#32463;&#39564;&#30446;&#26631;&#24615;&#33021;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#21487;&#20197;&#38598;&#25104;&#20272;&#35745;&#21644;&#20248;&#21270;&#36807;&#31243;&#12290;&#24403;&#27169;&#22411;&#34987;&#38169;&#35823;&#22320;&#25351;&#23450;&#26102;&#65292;&#36825;&#31181;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#26174;&#31034;&#20986;&#20248;&#20110;&#31616;&#21333;&#30340;&#8220;&#20808;&#20272;&#35745;&#20877;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#27169;&#22411;&#31867;&#36275;&#22815;&#20016;&#23500;&#20197;&#28085;&#30422;&#30495;&#23454;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#38382;&#39064;&#65292;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#25490;&#24207;&#22312;&#24378;&#28872;&#30340;&#24847;&#20041;&#19979;&#34987;&#39072;&#20498;&#12290;&#22312;&#21463;&#38480;&#26465;&#20214;&#21644;&#24403;&#19978;&#19979;&#25991;&#29305;&#24449;&#21487;&#29992;&#26102;&#65292;&#31867;&#20284;&#30340;&#32467;&#26524;&#20063;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-driven stochastic optimization, model parameters of the underlying distribution need to be estimated from data in addition to the optimization task. Recent literature suggests the integration of the estimation and optimization processes, by selecting model parameters that lead to the best empirical objective performance. Such an integrated approach can be readily shown to outperform simple ``estimate then optimize" when the model is misspecified. In this paper, we argue that when the model class is rich enough to cover the ground truth, the performance ordering between the two approaches is reversed for nonlinear problems in a strong sense. Simple ``estimate then optimize" outperforms the integrated approach in terms of stochastic dominance of the asymptotic optimality gap, i,e, the mean, all other moments, and the entire asymptotic distribution of the optimality gap is always better. Analogous results also hold under constrained settings and when contextual features are availa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19968;&#27425;&#24615;&#23398;&#20064;&#30340;&#36866;&#24212;&#20219;&#21153;&#30340;&#29305;&#24449;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#22312;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#25913;&#21892;&#25512;&#29702;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#20010;&#19968;&#27425;&#24615;&#27979;&#35797;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.06832</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#24212;&#20219;&#21153;&#30340;&#29305;&#24449;&#36716;&#25442;&#26041;&#27861;&#29992;&#20110;&#19968;&#27425;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task Adaptive Feature Transformation for One-Shot Learning. (arXiv:2304.06832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06832
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#19968;&#27425;&#24615;&#23398;&#20064;&#30340;&#36866;&#24212;&#20219;&#21153;&#30340;&#29305;&#24449;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#22312;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#25913;&#21892;&#25512;&#29702;&#25928;&#26524;&#65292;&#24182;&#22312;&#22810;&#20010;&#19968;&#27425;&#24615;&#27979;&#35797;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38750;&#32447;&#24615;&#23884;&#20837;&#35843;&#25972;&#23618;&#65292;&#23427;&#22312;&#39044;&#35757;&#32451;&#30340;&#22266;&#23450;&#29305;&#24449;&#20043;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25913;&#21892;&#20302;&#26679;&#26412;&#24773;&#20917;&#19979;&#36716;&#23548;&#29109;&#25512;&#29702;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#33539;&#25968;&#24341;&#23548;&#36716;&#25442;&#21487;&#20197;&#29702;&#35299;&#20026;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#37325;&#26032;&#21442;&#25968;&#21270;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#35299;&#24320;&#19981;&#21516;&#31867;&#21035;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#23427;&#19987;&#27880;&#20110;&#30456;&#20851;&#30340;&#29305;&#24449;&#32500;&#24230;&#65292;&#21516;&#26102;&#38459;&#27490;&#21487;&#33021;&#20250;&#22312;&#19968;&#27425;&#24615;&#23398;&#20064;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#30340;&#38750;&#30456;&#20851;&#32500;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#22312;K&#22343;&#20540;&#32858;&#31867;&#30340;&#22522;&#26412;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#29305;&#24449;&#36716;&#25442;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;K&#22343;&#20540;&#21644;&#29109;&#26368;&#23567;&#21270;&#20043;&#38388;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#30028;&#38480;&#20248;&#21270;&#38142;&#25509;&#12290;&#36825;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#29305;&#24449;&#36716;&#25442;&#22312;&#29109;&#26368;&#23567;&#21270;&#30340;&#24773;&#20917;&#19979;&#20026;&#20160;&#20040;&#26377;&#29992;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#19968;&#27425;&#24615;&#27979;&#35797;&#20013;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a simple non-linear embedding adaptation layer, which is fine-tuned on top of fixed pre-trained features for one-shot tasks, improving significantly transductive entropy-based inference for low-shot regimes. Our norm-induced transformation could be understood as a re-parametrization of the feature space to disentangle the representations of different classes in a task specific manner. It focuses on the relevant feature dimensions while hindering the effects of non-relevant dimensions that may cause overfitting in a one-shot setting. We also provide an interpretation of our proposed feature transformation in the basic case of few-shot inference with K-means clustering. Furthermore, we give an interesting bound-optimization link between K-means and entropy minimization. This emphasizes why our feature transformation is useful in the context of entropy minimization. We report comprehensive experiments, which show consistent improvements over a variety of one-shot benchmarks, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#29992;&#22270;&#21644;&#20855;&#26377;&#23616;&#37096;&#24615;&#36136;&#30340;&#22270;&#20013;&#30340;&#25104;&#23545;&#27604;&#36739;&#25490;&#24207;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#21487;&#20197;&#23454;&#29616;&#31526;&#21512;Cram\'er-Rao&#19979;&#30028;&#30340;&#36880;&#20803;&#20272;&#35745;&#35823;&#24046;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#30830;&#23450;&#20102;&#23616;&#37096;&#24615;&#19981;&#20250;&#24433;&#21709;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#27835;&#31639;&#27861;&#20197;&#23454;&#29616;&#31867;&#20284;&#20445;&#38556;&#12290;</title><link>http://arxiv.org/abs/2304.06821</link><description>&lt;p&gt;
&#36890;&#29992;&#22270;&#21644;&#20855;&#26377;&#23616;&#37096;&#24615;&#36136;&#30340;&#22270;&#20013;&#30340;&#25104;&#23545;&#27604;&#36739;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Ranking from Pairwise Comparisons in General Graphs and Graphs with Locality. (arXiv:2304.06821v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#29992;&#22270;&#21644;&#20855;&#26377;&#23616;&#37096;&#24615;&#36136;&#30340;&#22270;&#20013;&#30340;&#25104;&#23545;&#27604;&#36739;&#25490;&#24207;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#21487;&#20197;&#23454;&#29616;&#31526;&#21512;Cram\'er-Rao&#19979;&#30028;&#30340;&#36880;&#20803;&#20272;&#35745;&#35823;&#24046;&#12290;&#21516;&#26102;&#65292;&#25991;&#31456;&#36824;&#30830;&#23450;&#20102;&#23616;&#37096;&#24615;&#19981;&#20250;&#24433;&#21709;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#20998;&#27835;&#31639;&#27861;&#20197;&#23454;&#29616;&#31867;&#20284;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#30740;&#31350;&#20102;&#32463;&#20856;&#30340;Bradley-Terry-Luce&#65288;BTL&#65289;&#27169;&#22411;&#20013;&#30340;&#25104;&#23545;&#27604;&#36739;&#25490;&#24207;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#24471;&#20998;&#20272;&#35745;&#12290;&#23545;&#20110;&#36890;&#29992;&#22270;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#36275;&#22815;&#22810;&#30340;&#26679;&#26412;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#21487;&#20197;&#23454;&#29616;&#19968;&#20010;&#31526;&#21512;Cram\'er-Rao&#19979;&#30028;&#30340;&#36880;&#20803;&#20272;&#35745;&#35823;&#24046;&#65292;&#36825;&#21487;&#20197;&#29992;&#26377;&#25928;&#30005;&#38459;&#26469;&#35828;&#26126;&#65307;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#26159;&#32479;&#35745;&#20272;&#35745;&#21644;&#36890;&#36807;&#39044;&#22788;&#29702;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#29305;&#21035;&#20851;&#27880;&#20855;&#26377;&#23616;&#37096;&#24615;&#36136;&#30340;&#22270;&#65292;&#20854;&#20013;&#20165;&#30456;&#37051;&#39033;&#20043;&#38388;&#21487;&#20197;&#36830;&#25509;&#36793;&#32536;&#65307;&#25105;&#20204;&#30340;&#20998;&#26512;&#30830;&#23450;&#20102;&#23616;&#37096;&#24615;&#19981;&#20250;&#24433;&#21709;&#30340;&#26465;&#20214;&#65292;&#21363;&#22312;&#22270;&#20013;&#36317;&#31163;&#36739;&#36828;&#30340;&#19968;&#23545;&#39033;&#30446;&#20043;&#38388;&#36827;&#34892;&#24471;&#20998;&#27604;&#36739;&#20960;&#20046;&#19982;&#27604;&#36739;&#30456;&#37051;&#39033;&#30446;&#23545;&#19968;&#23545;&#39033;&#30446;&#30456;&#20284;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#20998;&#27835;&#31639;&#27861;&#65292;&#21363;&#20351;&#22312;&#26368;&#31232;&#30095;&#30340;&#26679;&#26412;&#21306;&#22495;&#20869;&#65292;&#20063;&#21487;&#20197;&#35777;&#26126;&#33021;&#22815;&#23454;&#29616;&#31867;&#20284;&#30340;&#20445;&#35777;&#65292;&#24182;&#20139;&#21463;&#26412;&#22320;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report studies the problem of ranking from pairwise comparisons in the classical Bradley-Terry-Luce (BTL) model, with a focus on score estimation. For general graphs, we show that, with sufficiently many samples, maximum likelihood estimation (MLE) achieves an entrywise estimation error matching the Cram\'er-Rao lower bound, which can be stated in terms of effective resistances; the key to our analysis is a connection between statistical estimation and iterative optimization by preconditioned gradient descent. We are also particularly interested in graphs with locality, where only nearby items can be connected by edges; our analysis identifies conditions under which locality does not hurt, i.e. comparing the scores between a pair of items that are far apart in the graph is nearly as easy as comparing a pair of nearby items. We further explore divide-and-conquer algorithms that can provably achieve similar guarantees even in the regime with the sparsest samples, while enj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#30456;&#20851;&#38745;&#24577;&#20998;&#26512;&#20135;&#21697;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#20013;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#26174;&#31034;&#20449;&#24687;&#26469;&#25552;&#21462;&#20195;&#30721;&#20013;&#30340;&#35821;&#20041;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2304.06815</link><description>&lt;p&gt;
&#29992;&#30456;&#20851;&#38745;&#24577;&#20998;&#26512;&#20135;&#21697;&#25913;&#21892;&#23569;&#26679;&#26412;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improving Few-Shot Prompts with Relevant Static Analysis Products. (arXiv:2304.06815v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#30456;&#20851;&#38745;&#24577;&#20998;&#26512;&#20135;&#21697;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#20013;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#26174;&#31034;&#20449;&#24687;&#26469;&#25552;&#21462;&#20195;&#30721;&#20013;&#30340;&#35821;&#20041;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#31867;&#26032;&#22411;&#35745;&#31639;&#24341;&#25806;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23454;&#29616;"&#32534;&#31243;"&#12290;&#25105;&#20204;&#20173;&#22312;&#23398;&#20064;&#22914;&#20309;&#26368;&#22909;&#22320;"&#32534;&#31243;"&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#36825;&#26679;&#19968;&#31181;&#30452;&#35273;&#20986;&#21457;&#65292;&#21363;&#24320;&#21457;&#20154;&#21592;&#22312;&#22788;&#29702;&#32534;&#30721;&#20219;&#21153;&#26102;&#20250;&#26377;&#19968;&#31995;&#21015;&#24847;&#35782;&#21644;&#26080;&#24847;&#35782;&#30340;&#35821;&#20041;&#20107;&#23454;&#12290;&#23545;&#20110;&#19968;&#20010;&#20989;&#25968;&#32780;&#35328;&#65292;&#36825;&#20123;&#35821;&#20041;&#20107;&#23454;&#21487;&#33021;&#21253;&#25324;&#21442;&#25968;&#21644;&#23616;&#37096;&#21464;&#37327;&#21517;&#31216;&#12289;&#36820;&#22238;&#34920;&#36798;&#24335;&#12289;&#31616;&#21333;&#30340;&#21069;&#32622;&#21644;&#21518;&#32622;&#26465;&#20214;&#20197;&#21450;&#22522;&#26412;&#30340;&#25511;&#21046;&#21644;&#25968;&#25454;&#27969;&#65292;&#31561;&#31561;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#20195;&#30721;&#25688;&#35201;&#20219;&#21153;&#24182;&#35780;&#20272;&#26159;&#21542;&#20351;&#29992;&#26174;&#24335;&#28155;&#21152;&#20449;&#24687;&#33021;&#22815;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#36825;&#20123;&#35821;&#20041;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLM) are a new class of computation engines, "programmed" via prompt engineering. We are still learning how to best "program" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks. Mostly these are shallow, simple facts arising from a quick read. For a function, examples of facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.  One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of "code analysis" and extracting such information, implicitly, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether auto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26816;&#27979;&#27169;&#22411;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#27979;&#35797;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#22806;&#37096;&#20998;&#24067;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2304.06813</link><description>&lt;p&gt;
&#27169;&#22411;&#29305;&#23450;&#35270;&#35282;&#19979;&#30340;&#32479;&#19968;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unified Out-Of-Distribution Detection: A Model-Specific Perspective. (arXiv:2304.06813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#25193;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#26816;&#27979;&#27169;&#22411;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#27979;&#35797;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#29305;&#23450;&#30340;&#22806;&#37096;&#20998;&#24067;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#20998;&#24067;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#19981;&#23646;&#20110;&#35757;&#32451;&#20998;&#24067;&#24182;&#19981;&#21487;&#38752;&#39044;&#27979;&#30340;&#27979;&#35797;&#26679;&#20363;&#12290;&#34429;&#28982;&#24050;&#26377;&#22823;&#37327;&#30456;&#20851;&#24037;&#20316;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#21482;&#20851;&#27880;&#26469;&#33258;&#35821;&#20041;&#36716;&#25442;&#65288;&#22914;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#65289;&#30340;OOD&#20363;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#21487;&#33021;&#30340;&#21407;&#22240;&#65288;&#22914;&#21327;&#21464;&#37327;&#36716;&#25442;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#26356;&#24191;&#27867;&#30340;&#33539;&#22260;&#30740;&#31350;OOD&#26816;&#27979;&#12290;&#25105;&#20204;&#24314;&#35758;&#19981;&#26159;&#26816;&#27979;&#29305;&#23450;&#21407;&#22240;&#23548;&#33268;&#30340;OOD&#20363;&#23376;&#65292;&#32780;&#26159;&#26816;&#27979;&#24050;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#22120;&#65289;&#26080;&#27861;&#27491;&#30830;&#39044;&#27979;&#30340;&#20363;&#23376;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26159;&#21542;&#24212;&#35813;&#26816;&#27979;&#21644;&#25298;&#32477;&#27979;&#35797;&#20363;&#23376;&#26159;&#8220;&#27169;&#22411;&#29305;&#23450;&#8221;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#32479;&#19968;&#20102;&#30001;&#35821;&#20041;&#21464;&#21270;&#21644;&#21327;&#21464;&#37327;&#21464;&#21270;&#24341;&#36215;&#30340;OOD&#20363;&#23376;&#30340;&#26816;&#27979;&#65292;&#24182;&#23494;&#20999;&#20851;&#27880;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#19981;&#21463;&#25511;&#21046;&#30340;&#29615;&#22659;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#35813;&#26694;&#26550;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection aims to identify test examples that do not belong to the training distribution and are thus unlikely to be predicted reliably. Despite a plethora of existing works, most of them focused only on the scenario where OOD examples come from semantic shift (e.g., unseen categories), ignoring other possible causes (e.g., covariate shift). In this paper, we present a novel, unifying framework to study OOD detection in a broader scope. Instead of detecting OOD examples from a particular cause, we propose to detect examples that a deployed machine learning model (e.g., an image classifier) is unable to predict correctly. That is, whether a test example should be detected and rejected or not is ``model-specific''. We show that this framework unifies the detection of OOD examples caused by semantic shift and covariate shift, and closely addresses the concern of applying a machine learning model to uncontrolled environments. We provide an extensive analysis that 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35774;&#35745;&#38750;&#32447;&#24615;&#20809;&#23376;&#26230;&#20307;&#21644;&#27893;&#28006;&#20809;&#26463;&#29983;&#25104;&#39640;&#32500;&#37327;&#23376;&#24577;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#29289;&#29702;&#32422;&#26463;&#21644;&#21487;&#24494;&#20998;&#30340;&#26041;&#27861;&#65292;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#28436;&#31034;&#20102;&#22914;&#20309;&#29983;&#25104;&#26368;&#22823;&#32416;&#32544;&#24577;&#12290;&#36825;&#20026;&#25511;&#21046;&#20219;&#24847;&#37327;&#23376;&#24577;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#65292;&#24182;&#22312;&#20840;&#20809;&#23398;&#30456;&#24178;&#25511;&#21046;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06810</link><description>&lt;p&gt;
&#39640;&#32500;&#37327;&#23376;&#24577;&#24037;&#31243;&#30340;&#38750;&#32447;&#24615;&#20809;&#23376;&#26230;&#20307;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Designing Nonlinear Photonic Crystals for High-Dimensional Quantum State Engineering. (arXiv:2304.06810v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06810
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35774;&#35745;&#38750;&#32447;&#24615;&#20809;&#23376;&#26230;&#20307;&#21644;&#27893;&#28006;&#20809;&#26463;&#29983;&#25104;&#39640;&#32500;&#37327;&#23376;&#24577;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#29289;&#29702;&#32422;&#26463;&#21644;&#21487;&#24494;&#20998;&#30340;&#26041;&#27861;&#65292;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#28436;&#31034;&#20102;&#22914;&#20309;&#29983;&#25104;&#26368;&#22823;&#32416;&#32544;&#24577;&#12290;&#36825;&#20026;&#25511;&#21046;&#20219;&#24847;&#37327;&#23376;&#24577;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#65292;&#24182;&#22312;&#20840;&#20809;&#23398;&#30456;&#24178;&#25511;&#21046;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#21463;&#29289;&#29702;&#32422;&#26463;&#21644;&#21487;&#24494;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37327;&#23376;&#20809;&#23398;&#20013;&#30340;&#33258;&#21457;&#21442;&#37327;&#19979;&#36716;&#25442; (SPDC)&#65292;&#29983;&#25104; D &#32500; qudit &#29366;&#24577;&#12290;&#25105;&#20204;&#35268;&#36991;&#20102;&#29289;&#29702;&#36807;&#31243;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#25152;&#24102;&#26469;&#30340;&#20219;&#20309;&#38480;&#21046;&#65292;&#24182;&#24182;&#20837;&#20102;&#19968;&#32452;&#38543;&#26426;&#21160;&#21147;&#23398;&#26041;&#31243;&#65292;&#25511;&#21046;&#20854;&#22312; SPDC &#21704;&#23494;&#39039;&#19979;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#32467;&#26500;&#21270;&#30340;&#38750;&#32447;&#24615;&#20809;&#23376;&#26230;&#20307; (NLPCs) &#21644;&#24418;&#29366;&#21270;&#30340;&#27893;&#28006;&#20809;&#26463;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65307;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#31354;&#38388;&#33258;&#30001;&#24230;&#20013;&#29983;&#25104;&#26368;&#22823;&#32416;&#32544;&#24577;&#12290;&#23398;&#20064; NLPC &#32467;&#26500;&#20026;&#22609;&#36896;&#21644;&#25511;&#21046;&#20219;&#24847;&#37327;&#23376;&#24577;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#26032;&#36884;&#24452;&#65292;&#24182;&#19988;&#20351;&#24471;&#29983;&#25104;&#24577;&#30340;&#20840;&#20809;&#23398;&#30456;&#24178;&#25511;&#21046;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#20174;&#24222;&#22823;&#30340;&#26230;&#20307;&#25193;&#23637;&#21040;&#34180;&#22411;&#20803;&#34920;&#38754;&#65292;&#24182;&#19988;&#21487;&#33021;&#24212;&#29992;&#20110;&#20854;&#20182;&#20849;&#20139;&#31867;&#20284;&#21704;&#23494;&#39039;&#37327;&#30340;&#37327;&#23376;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel, physically-constrained and differentiable approach for the generation of D-dimensional qudit states via spontaneous parametric down-conversion (SPDC) in quantum optics. We circumvent any limitations imposed by the inherently stochastic nature of the physical process and incorporate a set of stochastic dynamical equations governing its evolution under the SPDC Hamiltonian. We demonstrate the effectiveness of our model through the design of structured nonlinear photonic crystals (NLPCs) and shaped pump beams; and show, theoretically and experimentally, how to generate maximally entangled states in the spatial degree of freedom. The learning of NLPC structures offers a promising new avenue for shaping and controlling arbitrary quantum states and enables all-optical coherent control of the generated states. We believe that this approach can readily be extended from bulky crystals to thin Metasurfaces and potentially applied to other quantum systems sharing a similar Ham
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24335;&#25968;&#25454;&#20013;&#30340;&#20027;&#21160;&#35745;&#36153;&#26631;&#27880;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26631;&#35760;&#28857;&#24182;&#32500;&#25252;&#26102;&#38388;&#21644;&#25104;&#26412;&#30456;&#20851;&#38408;&#20540;&#65292;&#22312;$T$&#36718;&#20043;&#21518;&#23454;&#29616;&#20102;$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$&#30340;&#26368;&#22351;&#24773;&#20917;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2304.06808</link><description>&lt;p&gt;
&#27969;&#24335;&#25968;&#25454;&#20027;&#21160;&#35745;&#36153;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Active Cost-aware Labeling of Streaming Data. (arXiv:2304.06808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27969;&#24335;&#25968;&#25454;&#20013;&#30340;&#20027;&#21160;&#35745;&#36153;&#26631;&#27880;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26631;&#35760;&#28857;&#24182;&#32500;&#25252;&#26102;&#38388;&#21644;&#25104;&#26412;&#30456;&#20851;&#38408;&#20540;&#65292;&#22312;$T$&#36718;&#20043;&#21518;&#23454;&#29616;&#20102;$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$&#30340;&#26368;&#22351;&#24773;&#20917;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20027;&#21160;&#26631;&#35760;&#27969;&#25968;&#25454;&#38382;&#39064;&#65292;&#20854;&#20013;&#20027;&#21160;&#23398;&#20064;&#32773;&#38754;&#20020;&#19968;&#31995;&#21015;&#25968;&#25454;&#28857;&#65292;&#24182;&#24517;&#39035;&#36890;&#36807;&#26114;&#36149;&#30340;&#23454;&#39564;&#31934;&#24515;&#36873;&#25321;&#21738;&#20123;&#28857;&#36827;&#34892;&#26631;&#35760;&#65292;&#27492;&#31867;&#38382;&#39064;&#24120;&#24120;&#20986;&#29616;&#22312;&#21307;&#30103;&#21644;&#22825;&#25991;&#23398;&#31561;&#39046;&#22495;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#30340;&#26159;&#25968;&#25454;&#36755;&#20837;&#23646;&#20110;$K$&#20010;&#31163;&#25955;&#20998;&#24067;&#20043;&#19968;&#30340;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#24418;&#24335;&#21270;&#25551;&#36848;&#27492;&#38382;&#39064;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#25429;&#25417;&#20102;&#26631;&#35760;&#25104;&#26412;&#21644;&#39044;&#27979;&#35823;&#24046;&#12290;&#24403;&#26631;&#35760;&#25104;&#26412;&#20026;$B$&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#36873;&#25321;&#26631;&#35760;&#28857;&#65292;&#20165;&#22312;&#19981;&#30830;&#23450;&#24615;&#22823;&#20110;&#26102;&#38388;&#21644;&#25104;&#26412;&#30456;&#20851;&#38408;&#20540;&#26102;&#36827;&#34892;&#65292;&#21487;&#20197;&#22312;$T$&#36718;&#20043;&#21518;&#23454;&#29616;$O(B^{\frac { 1 }{ 3 }}K^{\frac { 1 }{ 3 }}T^{\frac { 2 }{ 3 }})$&#30340;&#26368;&#22351;&#24773;&#20917;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26356;&#32454;&#33268;&#30340;&#19978;&#30028;&#65292;&#35777;&#26126;&#20102;&#22312;&#21040;&#36798;&#27169;&#24335;&#26356;&#26377;&#21033;&#26102;&#65292;&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#21040;&#36798;&#27169;&#24335;&#65292;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#34917;&#20805;&#20102;&#20004;&#20010;&#19978;&#30028;&#30340;&#21305;&#37197;&#19979;&#30028;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27969;&#25968;&#25454;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#26631;&#35760;&#27969;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#19982;&#21069;&#38754;&#24773;&#20917;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study actively labeling streaming data, where an active learner is faced with a stream of data points and must carefully choose which of these points to label via an expensive experiment. Such problems frequently arise in applications such as healthcare and astronomy. We first study a setting when the data's inputs belong to one of $K$ discrete distributions and formalize this problem via a loss that captures the labeling cost and the prediction error. When the labeling cost is $B$, our algorithm, which chooses to label a point if the uncertainty is larger than a time and cost dependent threshold, achieves a worst-case upper bound of $O(B^{\frac{1}{3}} K^{\frac{1}{3}} T^{\frac{2}{3}})$ on the loss after $T$ rounds. We also provide a more nuanced upper bound which demonstrates that the algorithm can adapt to the arrival pattern, and achieves better performance when the arrival pattern is more favorable. We complement both upper bounds with matching lower bounds. We next study this pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#20248;&#20110;&#20856;&#22411;&#25351;&#26631;&#65292;&#25913;&#21892;&#27963;&#24615;&#29289;&#36136;&#27169;&#22411;&#30340;&#21442;&#25968;&#25512;&#26029;&#31934;&#24230;&#65292;&#21482;&#38656;&#20351;&#29992;&#23569;&#37327;&#30340;&#31995;&#32479;&#24555;&#29031;&#12290;</title><link>http://arxiv.org/abs/2304.06806</link><description>&lt;p&gt;
&#27963;&#24615;&#29289;&#36136;&#27169;&#22411;&#30340;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#27169;&#25311;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph-informed simulation-based inference for models of active matter. (arXiv:2304.06806v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#20248;&#20110;&#20856;&#22411;&#25351;&#26631;&#65292;&#25913;&#21892;&#27963;&#24615;&#29289;&#36136;&#27169;&#22411;&#30340;&#21442;&#25968;&#25512;&#26029;&#31934;&#24230;&#65292;&#21482;&#38656;&#20351;&#29992;&#23569;&#37327;&#30340;&#31995;&#32479;&#24555;&#29031;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32454;&#32990;&#34180;&#29255;&#21040;&#40479;&#32676;&#31561;&#35768;&#22810;&#38598;&#20307;&#31995;&#32479;&#37117;&#23384;&#22312;&#20110;&#36828;&#31163;&#24179;&#34913;&#29366;&#24577;&#30340;&#33258;&#28982;&#29615;&#22659;&#20013;&#12290;&#36825;&#20123;&#31995;&#32479;&#21453;&#26144;&#20102;&#19968;&#31181;&#27963;&#24615;&#29289;&#36136;&#24418;&#24335;&#65292;&#20854;&#20013;&#21333;&#20010;&#29289;&#36136;&#32452;&#20214;&#20855;&#26377;&#20869;&#37096;&#33021;&#37327;&#12290;&#22312;&#29305;&#23450;&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#65292;&#36825;&#20123;&#27963;&#24615;&#31995;&#32479;&#20250;&#32463;&#21382;&#30456;&#21464;&#65292;&#34920;&#29616;&#20026;&#21333;&#20010;&#32452;&#20214;&#30340;&#23567;&#27874;&#21160;&#21487;&#20197;&#23548;&#33268;&#31995;&#32479;&#27969;&#21464;&#24615;&#36136;&#30340;&#20840;&#23616;&#21464;&#21270;&#12290;&#36890;&#24120;&#20351;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#30340;&#27169;&#25311;&#21644;&#26041;&#27861;&#26469;&#29702;&#35299;&#21644;&#39044;&#27979;&#36825;&#20123;&#30456;&#21464;&#30340;&#30495;&#23454;&#35266;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#21487;&#29992;&#20110;&#20174;&#31995;&#32479;&#35266;&#23519;&#20013;&#24378;&#20581;&#22320;&#25512;&#26029;&#20986;&#27963;&#24615;&#29289;&#36136;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#37327;&#65288;&#20174;&#19968;&#20010;&#21040;&#19977;&#20010;&#65289;&#31995;&#32479;&#24555;&#29031;&#26469;&#36827;&#34892;&#21442;&#25968;&#25512;&#26029;&#65292;&#24182;&#19988;&#36825;&#31181;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#26041;&#27861;&#20248;&#20110;&#31995;&#32479;&#30340;&#24179;&#22343;&#36895;&#24230;&#25110;&#24179;&#22343;&#24179;&#26041;&#20301;&#31227;&#31561;&#20856;&#22411;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#65292;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#24555;&#29031;&#26469;&#20934;&#30830;&#39044;&#27979;&#39640;&#32423;&#31995;&#32479;&#34892;&#20026;&#65292;&#24182;&#19988;&#22522;&#20110;&#22270;&#24418;&#20449;&#24687;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27963;&#24615;&#29289;&#36136;&#27169;&#22411;&#30340;&#21442;&#25968;&#25512;&#26029;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many collective systems exist in nature far from equilibrium, ranging from cellular sheets up to flocks of birds. These systems reflect a form of active matter, whereby individual material components have internal energy. Under specific parameter regimes, these active systems undergo phase transitions whereby small fluctuations of single components can lead to global changes to the rheology of the system. Simulations and methods from statistical physics are typically used to understand and predict these phase transitions for real-world observations. In this work, we demonstrate that simulation-based inference can be used to robustly infer active matter parameters from system observations. Moreover, we demonstrate that a small number (from one to three) snapshots of the system can be used for parameter inference and that this graph-informed approach outperforms typical metrics such as the average velocity or mean square displacement of the system. Our work highlights that high-level sys
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#26679;&#26412;&#24179;&#22343;&#20272;&#35745;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#31561;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#19988;&#24615;&#33021;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2304.06803</link><description>&lt;p&gt;
&#29992;&#20110;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#26679;&#26412;&#24179;&#22343;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample Average Approximation for Black-Box VI. (arXiv:2304.06803v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06803
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#26679;&#26412;&#24179;&#22343;&#20272;&#35745;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#31561;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#19988;&#24615;&#33021;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#26799;&#24230;&#19978;&#21319;&#30340;&#22256;&#38590;&#65292;&#21253;&#25324;&#36873;&#25321;&#27493;&#38271;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#19968;&#31995;&#21015;&#26679;&#26412;&#24179;&#22343;&#20272;&#35745;&#38382;&#39064;&#65288;SAA&#65289;&#12290;&#36890;&#36807;&#23558;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;SAA&#36924;&#36817;&#20102;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#25311;&#29275;&#39039;&#26041;&#27861;&#21644;&#32447;&#24615;&#25628;&#32034;&#26469;&#35299;&#20915;&#27599;&#20010;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#31574;&#30053;&#26469;&#33258;&#21160;&#36873;&#25321;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#21464;&#20998;&#25512;&#26029;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#24555;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for black-box VI that bypasses the difficulties of stochastic gradient ascent, including the task of selecting step-sizes. Our approach involves using a sequence of sample average approximation (SAA) problems. SAA approximates the solution of stochastic optimization problems by transforming them into deterministic ones. We use quasi-Newton methods and line search to solve each deterministic optimization problem and present a heuristic policy to automate hyperparameter selection. Our experiments show that our method simplifies the VI problem and achieves faster performance than existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24207;&#21015;&#36716;&#23548;&#26550;&#26500;TDT&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#27604;&#20256;&#32479;Transducers&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#30528;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06795</link><description>&lt;p&gt;
Token-and-Duration Transducer&#26550;&#26500;&#65306;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#19982;&#26102;&#38271;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;
&lt;/p&gt;
&lt;p&gt;
Efficient Sequence Transduction by Jointly Predicting Tokens and Durations. (arXiv:2304.06795v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24207;&#21015;&#36716;&#23548;&#26550;&#26500;TDT&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#27604;&#20256;&#32479;Transducers&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#30528;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#30340;&#26032;&#22411;Token-and-Duration Transducer(TDT)&#26550;&#26500;&#12290;TDT&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#21363;&#21457;&#23556;&#30340;&#26631;&#35760;&#35206;&#30422;&#30340;&#36755;&#20837;&#24103;&#30340;&#25968;&#37327;&#65292;&#26469;&#25193;&#23637;&#20256;&#32479;&#30340;RNN-Transducer&#26550;&#26500;&#12290;&#23427;&#20351;&#29992;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#26631;&#20934;&#21270;&#36755;&#20986;&#30340;&#32852;&#21512;&#32593;&#32476;&#26469;&#29983;&#25104;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#20998;&#24067;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;TDT&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#30340;&#25345;&#32493;&#26102;&#38388;&#36755;&#20986;&#36339;&#36807;&#36755;&#20837;&#24103;&#65292;&#20351;&#20854;&#27604;&#36880;&#24103;&#22788;&#29702;&#32534;&#30721;&#22120;&#36755;&#20986;&#30340;&#20256;&#32479;Transducers&#26174;&#30528;&#26356;&#24555;&#12290;&#22312;&#19981;&#21516;&#30340;&#24207;&#21015;&#36716;&#23548;&#20219;&#21153;&#19978;&#65292;TDT&#27169;&#22411;&#22343;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#30528;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#35821;&#38899;&#35782;&#21035;&#30340;TDT&#27169;&#22411;&#27604;RNN-Transducers&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#39640;&#36798;2.82&#20493;&#12290;&#35821;&#38899;&#32763;&#35793;&#30340;TDT&#27169;&#22411;&#19982;MUST-C&#27979;&#35797;&#30456;&#27604;&#25552;&#39640;&#20102;1&#20010;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than RNN-Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared wi
&lt;/p&gt;</description></item><item><title>Speck&#26159;&#19968;&#27454;&#26234;&#33021;&#35270;&#35273;&#20256;&#24863;&#22120;SoC&#65292;&#35813;&#33455;&#29255;&#38598;&#25104;&#20102;&#22522;&#20110;&#20107;&#20214;&#30340;&#30456;&#26426;&#21644;&#20302;&#21151;&#32791;&#30340;sCNN&#35745;&#31639;&#26550;&#26500;&#65292;&#21487;&#26174;&#33879;&#38477;&#20302;&#21333;&#20803;&#29983;&#20135;&#25104;&#26412;&#12290;&#36890;&#36807;&#20248;&#21270;&#39640;&#24230;&#31232;&#30095;&#30340;&#35745;&#31639;&#21644;&#26368;&#23567;&#21270;&#24310;&#36831;&#65292;Speck&#21487;&#20197;&#22788;&#29702;&#39640;&#36895;&#30340;&#31232;&#30095;&#25968;&#25454;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.06793</link><description>&lt;p&gt;
Speck: &#19968;&#27454;&#25317;&#26377;&#20302;&#24310;&#36831;327K&#31070;&#32463;&#20803;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#31649;&#36947;&#30340;&#26234;&#33021;&#20107;&#20214;&#22411;&#35270;&#35273;&#20256;&#24863;&#22120;
&lt;/p&gt;
&lt;p&gt;
Speck: A Smart event-based Vision Sensor with a low latency 327K Neuron Convolutional Neuronal Network Processing Pipeline. (arXiv:2304.06793v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06793
&lt;/p&gt;
&lt;p&gt;
Speck&#26159;&#19968;&#27454;&#26234;&#33021;&#35270;&#35273;&#20256;&#24863;&#22120;SoC&#65292;&#35813;&#33455;&#29255;&#38598;&#25104;&#20102;&#22522;&#20110;&#20107;&#20214;&#30340;&#30456;&#26426;&#21644;&#20302;&#21151;&#32791;&#30340;sCNN&#35745;&#31639;&#26550;&#26500;&#65292;&#21487;&#26174;&#33879;&#38477;&#20302;&#21333;&#20803;&#29983;&#20135;&#25104;&#26412;&#12290;&#36890;&#36807;&#20248;&#21270;&#39640;&#24230;&#31232;&#30095;&#30340;&#35745;&#31639;&#21644;&#26368;&#23567;&#21270;&#24310;&#36831;&#65292;Speck&#21487;&#20197;&#22788;&#29702;&#39640;&#36895;&#30340;&#31232;&#30095;&#25968;&#25454;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38656;&#35201;&#20174;&#21508;&#31181;&#20256;&#24863;&#22120;&#20013;&#25552;&#21462;&#39640;&#32423;&#20449;&#24687;&#30340;&#36793;&#32536;&#35745;&#31639;&#26041;&#26696;&#27491;&#22312;&#26085;&#30410;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#35270;&#35273;&#20256;&#24863;&#22120;SoC&#65292;&#20855;&#26377;&#22522;&#20110;&#20107;&#20214;&#30340;&#30456;&#26426;&#21644;&#20302;&#21151;&#32791;&#30340;sCNN&#35745;&#31639;&#26550;&#26500;&#12290;&#35813;SoC&#30340;&#31471;&#21040;&#31471;&#35774;&#35745;&#31616;&#21333;&#65292;&#21487;&#20316;&#20026;&#29420;&#31435;&#24212;&#29992;&#25110;&#20316;&#20026;&#36739;&#22823;&#31995;&#32479;&#20013;&#30340;&#36793;&#32536;&#33410;&#28857;&#12290;&#32508;&#21512;&#20256;&#24863;&#22120;&#21644;&#22788;&#29702;&#20110;&#21333;&#20010;&#33455;&#29255;&#20013;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21333;&#20803;&#29983;&#20135;&#25104;&#26412;&#12290;&#22312;&#22788;&#29702;&#31649;&#36947;&#20013;&#65292;&#23545;&#39640;&#24230;&#31232;&#30095;&#30340;&#35745;&#31639;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#23558;&#24310;&#36831;&#26368;&#23567;&#21270;&#20026;$3.36\mu s$&#65292;&#20197;&#24212;&#23545;&#35270;&#35273;&#20256;&#24863;&#22120;&#30340;&#20107;&#20214;&#39537;&#21160;&#20449;&#21495;&#31561;&#39640;&#36895;&#20449;&#21495;&#30340;&#31232;&#30095;&#25968;&#25454;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge computing solutions that enable the extraction of high level information from a variety of sensors is in increasingly high demand. This is due to the increasing number of smart devices that require sensory processing for their application on the edge. To tackle this problem, we present a smart vision sensor System on Chip (Soc), featuring an event-based camera and a low power asynchronous spiking Convolutional Neuronal Network (sCNN) computing architecture embedded on a single chip. By combining both sensor and processing on a single die, we can lower unit production costs significantly. Moreover, the simple end-to-end nature of the SoC facilitates small stand-alone applications as well as functioning as an edge node in a larger systems. The event-driven nature of the vision sensor delivers high-speed signals in a sparse data stream. This is reflected in the processing pipeline, focuses on optimising highly sparse computation and minimising latency for 9 sCNN layers to $3.36\mu s$
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#12289;&#32431;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;$\{0,1\}^d$&#19978;&#20934;&#30830;&#20272;&#35745;&#20108;&#20803;&#31215;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06787</link><description>&lt;p&gt;
&#20108;&#20803;&#31215;&#20998;&#24067;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#32431;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions. (arXiv:2304.06787v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#12289;&#32431;&#24046;&#20998;&#38544;&#31169;&#20272;&#35745;&#22120;&#65292;&#21487;&#20197;&#22312;$\{0,1\}^d$&#19978;&#20934;&#30830;&#20272;&#35745;&#20108;&#20803;&#31215;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#949;-&#24046;&#20998;&#38544;&#31169;&#12289;&#35745;&#31639;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#24635;&#21464;&#21270;&#36317;&#31163;&#19979;&#20934;&#30830;&#22320;&#20272;&#35745;$\{0,1\}^d$&#19978;&#30340;&#20056;&#31215;&#20998;&#24067;&#30340;&#22343;&#20540;&#65292;&#21516;&#26102;&#22312;&#22810;&#39033;&#24335;&#23545;&#25968;&#22240;&#23376;&#20869;&#33719;&#24471;&#20102;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#22312;&#26356;&#24369;&#30340;&#38544;&#31169;&#27010;&#24565;&#19979;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35201;&#20040;&#22312;&#25351;&#25968;&#32423;&#36816;&#34892;&#26102;&#38388;&#20869;&#26368;&#20248;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first $\varepsilon$-differentially private, computationally efficient algorithm that estimates the means of product distributions over $\{0,1\}^d$ accurately in total-variation distance, whilst attaining the optimal sample complexity to within polylogarithmic factors. The prior work had either solved this problem efficiently and optimally under weaker notions of privacy, or had solved it optimally while having exponential running times.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#40065;&#26834;&#26041;&#27861;&#23454;&#29616;&#21518;&#24724;&#26368;&#20248;&#25511;&#21046;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.06783</link><description>&lt;p&gt;
&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#40065;&#26834;&#26041;&#27861;&#23454;&#29616;&#21518;&#24724;&#26368;&#20248;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Distributionally Robust Approach to Regret Optimal Control using the Wasserstein Distance. (arXiv:2304.06783v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06783
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wasserstein&#36317;&#31163;&#30340;&#20998;&#24067;&#40065;&#26834;&#26041;&#27861;&#23454;&#29616;&#21518;&#24724;&#26368;&#20248;&#25511;&#21046;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#40065;&#26834;&#26041;&#27861;&#30340;&#31163;&#25955;&#26102;&#38388;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#21518;&#24724;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#35813;&#31995;&#32479;&#21463;&#21040;&#38543;&#26426;&#21152;&#24615;&#25200;&#21160;&#24433;&#21709;&#65292;&#25104;&#26412;&#20026;&#20108;&#27425;&#22411;&#12290;&#25200;&#21160;&#36807;&#31243;&#30340;&#27010;&#29575;&#20998;&#24067;&#26410;&#30693;&#65292;&#20294;&#20551;&#23450;&#20301;&#20110;&#32473;&#23450;&#30340;&#20998;&#24067;&#29699;&#20869;&#65292;&#23450;&#20041;&#20026;&#20108;&#38454;Wasserstein&#36317;&#31163;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#35774;&#35745;&#20855;&#26377;&#20005;&#26684;&#22240;&#26524;&#32447;&#24615;&#25200;&#21160;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#20197;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#39044;&#26399;&#21518;&#24724;&#12290;&#25511;&#21046;&#22120;&#25152;&#20135;&#29983;&#30340;&#21518;&#24724;&#26159;&#25351;&#23427;&#23545;&#25239;&#25200;&#21160;&#26102;&#20135;&#29983;&#30340;&#25104;&#26412;&#19982;&#26368;&#20248;&#38750;&#22240;&#26524;&#25511;&#21046;&#22120;&#22312;&#19968;&#24320;&#22987;&#23601;&#26377;&#23436;&#20840;&#20102;&#35299;&#25200;&#21160;&#36807;&#31243;&#21518;&#20135;&#29983;&#30340;&#25104;&#26412;&#20043;&#24046;&#12290;&#24314;&#31435;&#22312;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#23545;&#20598;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#31561;&#20215;&#22320;&#25913;&#20889;&#36825;&#20010;&#26497;&#23567;&#26497;&#22823;&#30340;&#21518;&#24724;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#20026;&#19968;&#20010;&#21487;&#34892;&#30340;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a distributionally robust approach to regret optimal control of discrete-time linear dynamical systems with quadratic costs subject to stochastic additive disturbance on the state process. The underlying probability distribution of the disturbance process is unknown, but assumed to lie in a given ball of distributions defined in terms of the type-2 Wasserstein distance. In this framework, strictly causal linear disturbance feedback controllers are designed to minimize the worst-case expected regret. The regret incurred by a controller is defined as the difference between the cost it incurs in response to a realization of the disturbance process and the cost incurred by the optimal noncausal controller which has perfect knowledge of the disturbance process realization at the outset. Building on a well-established duality theory for optimal transport problems, we show how to equivalently reformulate this minimax regret optimal control problem as a tractable semidefini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#20445;&#25345;&#21018;&#24615;&#36816;&#21160;&#26465;&#20214;&#19981;&#21464;&#24615;&#30340;&#36830;&#32493;&#27491;&#21017;&#21270;&#27969;&#65292;&#24182;&#22312;&#20998;&#23376;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06779</link><description>&lt;p&gt;
&#21322;&#31561;&#21464;&#26465;&#20214;&#27491;&#21017;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Semi-Equivariant Conditional Normalizing Flows. (arXiv:2304.06779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#20445;&#25345;&#21018;&#24615;&#36816;&#21160;&#26465;&#20214;&#19981;&#21464;&#24615;&#30340;&#36830;&#32493;&#27491;&#21017;&#21270;&#27969;&#65292;&#24182;&#22312;&#20998;&#23376;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#36830;&#32493;&#27491;&#21017;&#21270;&#27969;&#23398;&#20064;&#24418;&#22914; $p(G | \hat G)$ &#30340;&#26465;&#20214;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013; $G$ &#21644; $\hat G$ &#26159;&#20004;&#20010;&#19977;&#32500;&#22270;&#24418;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#27969;&#30340;&#21322;&#31561;&#21464;&#26465;&#20214;&#65292;&#30830;&#20445;&#23545;&#21018;&#24615;&#36816;&#21160;&#30340;&#26465;&#20214;&#19981;&#21464;&#24615;&#20445;&#25345;&#12290;&#25105;&#20204;&#22312;&#24863;&#21463;&#21463;&#20307;&#24863;&#30693;&#37197;&#20307;&#29983;&#25104;&#30340;&#20998;&#23376;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#36825;&#31181;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning conditional distributions of the form $p(G | \hat G)$, where $G$ and $\hat G$ are two 3D graphs, using continuous normalizing flows. We derive a semi-equivariance condition on the flow which ensures that conditional invariance to rigid motions holds. We demonstrate the effectiveness of the technique in the molecular setting of receptor-aware ligand generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20154;&#26426;&#20132;&#20114;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25429;&#25417;&#20132;&#32455;&#30340;&#38745;&#24577;&#25163;&#21183;&#21644;&#21160;&#24577;&#25163;&#21183;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#38477;&#32500;&#25216;&#26415;&#20197;&#33719;&#24471;DG&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#38543;&#26426;&#26862;&#26519;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20998;&#21035;&#21487;&#20197;&#23545;24&#20010;SG&#21644;10&#20010;DG&#36827;&#34892;&#20934;&#30830;&#30340;&#20998;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#30340;&#25163;&#21183;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.06777</link><description>&lt;p&gt;
&#22312;&#32447;&#35782;&#21035;&#19981;&#23436;&#25972;&#30340;&#25163;&#21183;&#25968;&#25454;&#20197;&#25509;&#21475;&#21327;&#20316;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Online Recognition of Incomplete Gesture Data to Interface Collaborative Robots. (arXiv:2304.06777v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20154;&#26426;&#20132;&#20114;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25429;&#25417;&#20132;&#32455;&#30340;&#38745;&#24577;&#25163;&#21183;&#21644;&#21160;&#24577;&#25163;&#21183;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#38477;&#32500;&#25216;&#26415;&#20197;&#33719;&#24471;DG&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#38543;&#26426;&#26862;&#26519;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20998;&#21035;&#21487;&#20197;&#23545;24&#20010;SG&#21644;10&#20010;DG&#36827;&#34892;&#20934;&#30830;&#30340;&#20998;&#31867;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#30340;&#25163;&#21183;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#29616;&#20154;&#26426;&#20132;&#20114;&#21644;&#36827;&#19968;&#27493;&#25512;&#21160;&#21327;&#20316;&#26426;&#22120;&#20154;&#36827;&#20837;&#24066;&#22330;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#34987;&#20154;&#20204;&#25509;&#21463;&#26102;&#65292;&#25163;&#21183;&#30340;&#22312;&#32447;&#35782;&#21035;&#23545;&#20110;&#23454;&#29616;&#30452;&#35266;&#30340;&#20154;&#26426;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#22833;&#30495;&#21644;&#19981;&#23436;&#25972;&#30340;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#23454;&#29616;&#20934;&#30830;&#30340;&#25163;&#21183;&#35782;&#21035;&#21364;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20154;&#26426;&#20132;&#20114;&#26694;&#26550;&#65292;&#20351;&#29992;&#21487;&#31359;&#25140;&#30340;&#20256;&#24863;&#22120;&#25429;&#25417;&#20132;&#32455;&#30340;&#38745;&#24577;&#25163;&#21183;&#65288;SG&#65289;&#21644;&#21160;&#24577;&#25163;&#21183;&#65288;DG&#65289;&#65292;&#36890;&#36807;&#23545;&#21407;&#22987;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#38477;&#32500;&#65288;&#20351;&#29992;&#19977;&#27425;&#25554;&#20540;&#21644;&#20027;&#25104;&#20998;&#20998;&#26512;&#37325;&#37319;&#26679;&#65289;&#26469;&#33719;&#21462;DG&#29305;&#24449;&#12290;&#23454;&#39564;&#27979;&#35797;&#20351;&#29992;&#20102;UC2017&#25163;&#21183;&#25968;&#25454;&#38598;&#20174;8&#20010;&#19981;&#21516;&#30340;&#21463;&#35797;&#32773;&#20013;&#37319;&#38598;&#30340;&#26679;&#26412;&#12290;&#20998;&#31867;&#27169;&#22411;&#23545;&#20110;&#21253;&#21547;24&#20010;SG&#30340;&#24211;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#26174;&#31034;&#20986;95.6&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#23545;10&#20010;DG&#26174;&#31034;&#20986;99.3&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#19981;&#21516;&#30340;&#20808;&#21069;&#30740;&#31350;&#30456;&#27604;&#34920;&#29616;&#19968;&#33268;&#25110;&#26356;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online recognition of gestures is critical for intuitive human-robot interaction (HRI) and further push collaborative robotics into the market, making robots accessible to more people. The problem is that it is difficult to achieve accurate gesture recognition in real unstructured environments, often using distorted and incomplete multisensory data. This paper introduces an HRI framework to classify large vocabularies of interwoven static gestures (SGs) and dynamic gestures (DGs) captured with wearable sensors. DG features are obtained by applying data dimensionality reduction to raw data from sensors (resampling with cubic interpolation and principal component analysis). Experimental tests were conducted using the UC2017 hand gesture dataset with samples from eight different subjects. The classification models show an accuracy of 95.6% for a library of 24 SGs with a random forest and 99.3% for 10 DGs using artificial neural networks. These results compare equally or favorably with dif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22352;&#26631;&#21464;&#25442;&#26469;&#21152;&#36895;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#12289;&#25913;&#21892;&#33618;&#21407;&#39640;&#21407;&#21644;&#23616;&#37096;&#26368;&#23567;&#20540;&#24433;&#21709;&#30340;&#36890;&#29992;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22810;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06768</link><description>&lt;p&gt;
&#36890;&#36807;&#22352;&#26631;&#21464;&#25442;&#25913;&#36827;&#26799;&#24230;&#26041;&#27861;&#65306;&#24212;&#29992;&#20110;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Gradient Methods via Coordinate Transformations: Applications to Quantum Machine Learning. (arXiv:2304.06768v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#22352;&#26631;&#21464;&#25442;&#26469;&#21152;&#36895;&#26799;&#24230;&#20248;&#21270;&#31639;&#27861;&#12289;&#25913;&#21892;&#33618;&#21407;&#39640;&#21407;&#21644;&#23616;&#37096;&#26368;&#23567;&#20540;&#24433;&#21709;&#30340;&#36890;&#29992;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#22810;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#26159;&#32463;&#20856;&#30340;&#36824;&#26159;&#37327;&#23376;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#37117;&#22823;&#37327;&#20381;&#36182;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#22914;&#26799;&#24230;&#19979;&#38477;&#31561;&#12290;&#24635;&#20307;&#24615;&#33021;&#21462;&#20915;&#20110;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#33618;&#21407;&#39640;&#21407;&#30340;&#20986;&#29616;&#65292;&#36825;&#20250;&#20943;&#32531;&#35745;&#31639;&#36895;&#24230;&#24182;&#23548;&#33268;&#38750;&#26368;&#20248;&#35299;&#12290;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#20250;&#23548;&#33268;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#25104;&#26412;&#28608;&#22686;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#31574;&#30053;&#65292;&#20197;&#21152;&#36895;&#21644;&#25913;&#21892;&#36825;&#20123;&#26041;&#27861;&#30340;&#24635;&#20307;&#24615;&#33021;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#33618;&#21407;&#39640;&#21407;&#21644;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#22352;&#26631;&#21464;&#25442;&#65292;&#26377;&#28857;&#31867;&#20284;&#20110;&#21464;&#20998;&#26059;&#36716;&#65292;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#28155;&#21152;&#20102;&#39069;&#22806;&#30340;&#26041;&#21521;&#65292;&#36825;&#20123;&#26041;&#21521;&#21462;&#20915;&#20110;&#25104;&#26412;&#20989;&#25968;&#26412;&#36523;&#65292;&#24182;&#19988;&#20801;&#35768;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#37197;&#32622;&#26223;&#35266;&#12290;&#25105;&#20204;&#24050;&#36890;&#36807;&#22686;&#24378;&#22810;&#31181;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24471;&#21040;&#20102;&#38750;&#24120;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms, both in their classical and quantum versions, heavily rely on optimization algorithms based on gradients, such as gradient descent and alike. The overall performance is dependent on the appearance of local minima and barren plateaus, which slow-down calculations and lead to non-optimal solutions. In practice, this results in dramatic computational and energy costs for AI applications. In this paper we introduce a generic strategy to accelerate and improve the overall performance of such methods, allowing to alleviate the effect of barren plateaus and local minima. Our method is based on coordinate transformations, somehow similar to variational rotations, adding extra directions in parameter space that depend on the cost function itself, and which allow to explore the configuration landscape more efficiently. The validity of our method is benchmarked by boosting a number of quantum machine learning algorithms, getting a very significant improvement in their
&lt;/p&gt;</description></item><item><title>RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06767</link><description>&lt;p&gt;
RAFT: &#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06767
&lt;/p&gt;
&lt;p&gt;
RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24191;&#27867;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#24102;&#26469;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23376;&#20248;&#26679;&#26412;&#12289;&#25197;&#26354;&#30340;&#32467;&#26524;&#21644;&#19981;&#20844;&#24179;&#65292;&#21487;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#30340;&#20262;&#29702;&#21644;&#20559;&#22909;&#23545;&#40784;&#26159;&#30830;&#20445;&#23427;&#20204;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#30340;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288; RLHF&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312; RL &#31639;&#27861;&#30340;&#25351;&#23548;&#19979;&#65292;&#29992;&#20154;&#31867;&#21453;&#39304;&#25351;&#23548;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292; RL &#31639;&#27861;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#24120;&#24120;&#20250;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#23545;&#40784;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#21644;&#31616;&#21270;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#65288; RAFT &#65289;&#65292;&#26088;&#22312;&#23545;&#40784;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#23454;&#29616;&#21327;&#21516;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#20197;&#39640;&#25928;&#22320;&#22312;FPGAs&#21644;ASICs&#19978;&#23454;&#29616;NNs&#65292;&#24182;&#22312;&#31890;&#23376;&#29289;&#29702;&#24212;&#29992;&#31243;&#24207;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.06745</link><description>&lt;p&gt;
&#38754;&#21521; FPGAs &#21644; ASICs &#30340; Hessian-aware &#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#21327;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
End-to-end codesign of Hessian-aware quantized neural networks for FPGAs and ASICs. (arXiv:2304.06745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06745
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#23454;&#29616;&#21327;&#21516;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#20197;&#39640;&#25928;&#22320;&#22312;FPGAs&#21644;ASICs&#19978;&#23454;&#29616;NNs&#65292;&#24182;&#22312;&#31890;&#23376;&#29289;&#29702;&#24212;&#29992;&#31243;&#24207;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#23454;&#29616;&#21327;&#21516;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#29616;&#22330;&#21487;&#32534;&#31243;&#38376;&#38453;&#21015;&#65288;FPGA&#65289;&#21644;&#19987;&#29992;&#38598;&#25104;&#30005;&#36335;&#65288;ASIC&#65289;&#30828;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110; Hessian &#30340;&#37327;&#21270;&#65288;HAWQ&#65289;NNs&#65292;&#37327;&#21270;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#20132;&#25442;&#65288;QONNX&#65289;&#20013;&#38388;&#34920;&#31034;&#21644;hls4ml&#24037;&#20855;&#27969;&#65292;&#23558;NNs&#36716;&#25442;&#20026;FPGA&#21644;ASIC&#22266;&#20214;&#12290;&#36825;&#20351;&#24471;&#38750;&#19987;&#23478;&#21487;&#20197;&#22312;&#21333;&#20010;&#24320;&#28304;&#24037;&#20316;&#27969;&#20013;&#23454;&#29616;&#39640;&#25928;&#30340;NN&#30828;&#20214;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#31185;&#23398;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#37096;&#32626;&#29992;&#20110;&#23454;&#26102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#31890;&#23376;&#29289;&#29702;&#24212;&#29992;&#31243;&#24207;&#20013;&#28436;&#31034;&#20102;&#24037;&#20316;&#27969;&#31243;&#65292;&#28041;&#21450;&#24517;&#39035;&#22312;CERN&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#65288;LHC&#65289;&#30340;40 MHz&#30896;&#25758;&#29575;&#19979;&#25805;&#20316;&#30340;&#35302;&#21457;&#20915;&#31574;&#12290;&#30001;&#20110;&#39640;&#30896;&#25758;&#29575;&#65292;&#25152;&#26377;&#25968;&#25454;&#22788;&#29702;&#24517;&#39035;&#22312;&#20005;&#26684;&#30340;&#38754;&#31215;&#21644;&#24310;&#36831;&#20869;&#22312;&#23450;&#21046;&#30340;ASIC&#21644;FPGA&#30828;&#20214;&#19978;&#23454;&#29616;&#12290;&#22522;&#20110;&#36825;&#20123;&#35201;&#27714;&#25105;&#20204;&#23637;&#31034;&#20102;&#26412;&#25991;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an end-to-end workflow for the training and implementation of co-designed neural networks (NNs) for efficient field-programmable gate array (FPGA) and application-specific integrated circuit (ASIC) hardware. Our approach leverages Hessian-aware quantization (HAWQ) of NNs, the Quantized Open Neural Network Exchange (QONNX) intermediate representation, and the hls4ml tool flow for transpiling NNs into FPGA and ASIC firmware. This makes efficient NN implementations in hardware accessible to nonexperts, in a single open-sourced workflow that can be deployed for real-time machine learning applications in a wide range of scientific and industrial settings. We demonstrate the workflow in a particle physics application involving trigger decisions that must operate at the 40 MHz collision rate of the CERN Large Hadron Collider (LHC). Given the high collision rate, all data processing must be implemented on custom ASIC and FPGA hardware within a strict area and latency. Based on these
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#21551;&#21457;&#26426;&#21046;&#30340;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20013;&#21253;&#25324;&#31232;&#30095;&#19981;&#37325;&#21472;&#34920;&#31034;&#12289;&#36203;&#24067;&#23398;&#20064;&#12289;&#31361;&#35302;&#24041;&#22266;&#21644;&#37325;&#25773;&#31561;&#26426;&#21046;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#24207;&#21015;&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.06738</link><description>&lt;p&gt;
&#19968;&#39033;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#65306;&#33041;&#21551;&#21457;&#26426;&#21046;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#21644;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Study of Biologically Plausible Neural Network: The Role and Interactions of Brain-Inspired Mechanisms in Continual Learning. (arXiv:2304.06738v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33041;&#21551;&#21457;&#26426;&#21046;&#30340;&#29983;&#29289;&#21487;&#34892;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#20013;&#21253;&#25324;&#31232;&#30095;&#19981;&#37325;&#21472;&#34920;&#31034;&#12289;&#36203;&#24067;&#23398;&#20064;&#12289;&#31361;&#35302;&#24041;&#22266;&#21644;&#37325;&#25773;&#31561;&#26426;&#21046;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#24207;&#21015;&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25797;&#38271;&#19981;&#26029;&#20174;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#33719;&#21462;&#12289;&#24041;&#22266;&#21644;&#20445;&#30041;&#20449;&#24687;&#65292;&#32780;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21017;&#34920;&#29616;&#20986;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#21644;&#23427;&#20204;&#30340;&#20154;&#24037;&#23545;&#24212;&#29289;&#22312;&#31361;&#35302;&#22797;&#26434;&#24615;&#12289;&#20449;&#24687;&#22788;&#29702;&#21644;&#23398;&#20064;&#26426;&#21046;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#35299;&#37322;&#20102;&#24615;&#33021;&#19978;&#30340;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#29983;&#29289;&#21487;&#34892;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#23436;&#20840;&#20852;&#22859;&#21644;&#25233;&#21046;&#31070;&#32463;&#20803;&#30340;&#21333;&#29420;&#31181;&#32676;&#65292;&#36981;&#24490;&#25140;&#23572;&#21407;&#21017;&#65292;&#24182;&#20026;&#20852;&#22859;&#30340;&#38181;&#20307;&#31070;&#32463;&#20803;&#22686;&#21152;&#20102;&#31867;&#20284;&#26641;&#31361;&#30340;&#32467;&#26500;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#21050;&#28608;&#22788;&#29702;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#21463;&#33041;&#21551;&#21457;&#30340;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#31232;&#30095;&#19981;&#37325;&#21472;&#34920;&#31034;&#12289;&#36203;&#24067;&#23398;&#20064;&#12289;&#31361;&#35302;&#24041;&#22266;&#21644;&#37325;&#25773;&#20276;&#38543;&#23398;&#20064;&#20107;&#20214;&#30340;&#36807;&#21435;&#28608;&#27963;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22522;&#20110;&#24207;&#21015;&#30340;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#32771;&#34385;&#36825;&#20123;&#26426;&#21046;&#22312;&#29983;&#29289;&#21487;&#34892;&#30340;&#26694;&#26550;&#20013;&#30456;&#20114;&#20316;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans excel at continually acquiring, consolidating, and retaining information from an ever-changing environment, whereas artificial neural networks (ANNs) exhibit catastrophic forgetting. There are considerable differences in the complexity of synapses, the processing of information, and the learning mechanisms in biological neural networks and their artificial counterparts, which may explain the mismatch in performance. We consider a biologically plausible framework that constitutes separate populations of exclusively excitatory and inhibitory neurons that adhere to Dale's principle, and the excitatory pyramidal neurons are augmented with dendritic-like structures for context-dependent processing of stimuli. We then conduct a comprehensive study on the role and interactions of different mechanisms inspired by the brain, including sparse non-overlapping representations, Hebbian learning, synaptic consolidation, and replay of past activations that accompanied the learning event. Our s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#20837;&#24230;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#8220;&#36817;&#20284;&#27491;&#30830;&#24615;&#8221;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#21644;&#22312; $\chi^2$ &#25955;&#24230;&#19979;&#30340;&#39640;&#27010;&#29575;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.06733</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#32593;&#32476;&#36817;&#20284;&#26368;&#20248;&#20837;&#24230;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Near-Optimal Degree Testing for Bayes Nets. (arXiv:2304.06733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#20837;&#24230;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#65292;&#21516;&#26102;&#24320;&#21457;&#20102;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#8220;&#36817;&#20284;&#27491;&#30830;&#24615;&#8221;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#21644;&#22312; $\chi^2$ &#25955;&#24230;&#19979;&#30340;&#39640;&#27010;&#29575;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#23545; $P$ &#30340;&#26679;&#26412;&#35775;&#38382;&#30340;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#28508;&#22312;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26368;&#22823;&#20837;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#38382;&#39064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026; $\tilde{\Theta}(2^{n/2}/\varepsilon^2)$&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#27979;&#35797;&#21644;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20808;&#21069;&#29992;&#20110;&#33719;&#24471;&#26679;&#26412;&#26368;&#20248;&#27979;&#35797;&#22120;&#12290;&#20026;&#20102;&#24212;&#29992;&#27492;&#26694;&#26550;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#8220;&#36817;&#20284;&#27491;&#30830;&#24615;&#8221;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#23398;&#20064;&#65292;&#24182;&#22312; $\chi^2$ &#25955;&#24230;&#19979;&#30340;&#39640;&#27010;&#29575;&#23398;&#20064;&#65292;&#36825;&#20123;&#31639;&#27861;&#37117;&#20855;&#26377;&#29420;&#31435;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of testing the maximum in-degree of the Bayes net underlying an unknown probability distribution $P$ over $\{0,1\}^n$, given sample access to $P$. We show that the sample complexity of the problem is $\tilde{\Theta}(2^{n/2}/\varepsilon^2)$. Our algorithm relies on a testing-by-learning framework, previously used to obtain sample-optimal testers; in order to apply this framework, we develop new algorithms for ``near-proper'' learning of Bayes nets, and high-probability learning under $\chi^2$ divergence, which are of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30149;&#27602;&#23487;&#20027;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21050;&#31361;&#34507;&#30333;&#24207;&#21015;&#39044;&#27979;&#20896;&#29366;&#30149;&#27602;&#30340;&#23487;&#20027;&#29305;&#24322;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#27850;&#26494;&#26657;&#27491;&#36317;&#31163;&#29983;&#25104;&#36317;&#31163;&#30697;&#38453;&#65292;&#28982;&#21518;&#23545;&#21050;&#31361;&#34507;&#30333;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.06731</link><description>&lt;p&gt;
PCD2Vec&#65306;&#19968;&#31181;&#22522;&#20110;&#27850;&#26494;&#26657;&#27491;&#36317;&#31163;&#30340;&#30149;&#27602;&#23487;&#20027;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PCD2Vec: A Poisson Correction Distance-Based Approach for Viral Host Classification. (arXiv:2304.06731v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30149;&#27602;&#23487;&#20027;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#21050;&#31361;&#34507;&#30333;&#24207;&#21015;&#39044;&#27979;&#20896;&#29366;&#30149;&#27602;&#30340;&#23487;&#20027;&#29305;&#24322;&#24615;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#27850;&#26494;&#26657;&#27491;&#36317;&#31163;&#29983;&#25104;&#36317;&#31163;&#30697;&#38453;&#65292;&#28982;&#21518;&#23545;&#21050;&#31361;&#34507;&#30333;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20896;&#29366;&#30149;&#27602;&#26159;&#19968;&#31181;&#24102;&#33180;&#30340;&#12289;&#38750;&#20998;&#27573;&#30340;&#27491;&#38142;RNA&#30149;&#27602;&#65292;&#23646;&#20110;&#20896;&#29366;&#30149;&#27602;&#31185;&#12290;&#23427;&#20005;&#37325;&#24863;&#26579;&#21508;&#31181;&#21160;&#29289;&#65292;&#20027;&#35201;&#26159;&#21754;&#20083;&#21160;&#29289;&#21644;&#31165;&#40479;&#65292;&#24341;&#36215;&#20005;&#37325;&#25285;&#24551;&#65292;&#22914;&#26368;&#36817;&#30340;&#22823;&#27969;&#34892;&#30149;&#65288;COVID-19&#65289;&#12290;&#22240;&#27492;&#65292;&#28145;&#20837;&#29702;&#35299;&#36825;&#20123;&#30149;&#27602;&#23545;&#21046;&#23450;&#39044;&#38450;&#21644;&#32531;&#35299;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290; &#22312;&#20896;&#29366;&#30149;&#27602;&#22522;&#22240;&#32452;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#32467;&#26500;&#21306;&#22495;&#26159;&#21050;&#31361;&#21306;&#22495;&#65292;&#23427;&#36127;&#36131;&#23558;&#30149;&#27602;&#38468;&#30528;&#22312;&#23487;&#20027;&#32454;&#32990;&#33180;&#19978;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#20165;&#21050;&#31361;&#34507;&#30333;&#32780;&#19981;&#26159;&#23436;&#25972;&#22522;&#22240;&#32452;&#65292;&#25552;&#20379;&#20102;&#25191;&#34892;&#35832;&#22914;&#23487;&#20027;&#20998;&#31867;&#20043;&#31867;&#30340;&#20998;&#26512;&#25152;&#38656;&#30340;&#22823;&#37096;&#20998;&#22522;&#26412;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#26469;&#33258;&#19981;&#21516;&#30149;&#27602;&#20122;&#23646;&#21644;&#29289;&#31181;&#30340;&#21050;&#31361;&#34507;&#30333;&#24207;&#21015;&#65292;&#39044;&#27979;&#20896;&#29366;&#30149;&#27602;&#30340;&#23487;&#20027;&#29305;&#24322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#27850;&#26494;&#26657;&#27491;&#36317;&#31163;&#29983;&#25104;&#36317;&#31163;&#30697;&#38453;&#65292;&#28982;&#21518;&#22522;&#20110;&#35813;&#30697;&#38453;&#23545;&#21050;&#31361;&#34507;&#30333;&#24207;&#21015;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coronaviruses are membrane-enveloped, non-segmented positive-strand RNA viruses belonging to the Coronaviridae family. Various animal species, mainly mammalian and avian, are severely infected by various coronaviruses, causing serious concerns like the recent pandemic (COVID-19). Therefore, building a deeper understanding of these viruses is essential to devise prevention and mitigation mechanisms. In the Coronavirus genome, an essential structural region is the spike region, and it's responsible for attaching the virus to the host cell membrane. Therefore, the usage of only the spike protein, instead of the full genome, provides most of the essential information for performing analyses such as host classification. In this paper, we propose a novel method for predicting the host specificity of coronaviruses by analyzing spike protein sequences from different viral subgenera and species. Our method involves using the Poisson correction distance to generate a distance matrix, followed by
&lt;/p&gt;</description></item><item><title>&#20803;&#23398;&#20064;&#27169;&#22411;&#26159;&#26500;&#24314;&#20154;&#31867;&#35748;&#30693;&#27169;&#22411;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#36125;&#21494;&#26031;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.06729</link><description>&lt;p&gt;
&#35748;&#30693;&#30340;&#20803;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Meta-Learned Models of Cognition. (arXiv:2304.06729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06729
&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#27169;&#22411;&#26159;&#26500;&#24314;&#20154;&#31867;&#35748;&#30693;&#27169;&#22411;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#36125;&#21494;&#26031;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#26159;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#21453;&#22797;&#20132;&#20114;&#32780;&#23398;&#20064;&#23398;&#20064;&#31639;&#27861;&#30340;&#26694;&#26550;&#65292;&#32780;&#19981;&#26159;&#25163;&#21160;&#35774;&#35745;&#23427;&#20204;&#12290;&#36817;&#24180;&#26469;&#65292;&#36825;&#20010;&#26694;&#26550;&#24050;&#32463;&#25104;&#20026;&#26500;&#24314;&#20154;&#31867;&#35748;&#30693;&#27169;&#22411;&#30340;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36824;&#32570;&#20047;&#19968;&#20010;&#20851;&#20110;&#20803;&#23398;&#20064;&#35748;&#30693;&#27169;&#22411;&#30340;&#36830;&#36143;&#30340;&#30740;&#31350;&#35745;&#21010;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#32508;&#21512;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#24314;&#31435;&#36825;&#26679;&#30340;&#30740;&#31350;&#35745;&#21010;&#12290;&#25105;&#20204;&#20381;&#38752;&#19977;&#20010;&#20851;&#38190;&#25903;&#26609;&#26469;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25351;&#20986;&#20803;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#36125;&#21494;&#26031;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#12290;&#36825;&#20010;&#32467;&#26524;&#19981;&#20165;&#24847;&#21619;&#30528;&#20219;&#20309;&#21487;&#20197;&#36890;&#36807;&#36125;&#21494;&#26031;&#27169;&#22411;&#35299;&#37322;&#30340;&#34892;&#20026;&#29616;&#35937;&#20063;&#21487;&#20197;&#36890;&#36807;&#20803;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#65292;&#32780;&#19988;&#36824;&#20801;&#35768;&#25105;&#20204;&#19982;&#35748;&#30693;&#30340;&#29702;&#24615;&#20998;&#26512;&#24314;&#31435;&#24378;&#36830;&#25509;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20803;&#23398;&#20064;&#26694;&#26550;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#20960;&#20010;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35748;&#20026;&#20803;&#23398;&#20064;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Meta-learning is a framework for learning learning algorithms through repeated interactions with an environment as opposed to designing them by hand. In recent years, this framework has established itself as a promising tool for building models of human cognition. Yet, a coherent research program around meta-learned models of cognition is still missing. The purpose of this article is to synthesize previous work in this field and establish such a research program. We rely on three key pillars to accomplish this goal. We first point out that meta-learning can be used to construct Bayes-optimal learning algorithms. This result not only implies that any behavioral phenomenon that can be explained by a Bayesian model can also be explained by a meta-learned model but also allows us to draw strong connections to the rational analysis of cognition. We then discuss several advantages of the meta-learning framework over traditional Bayesian methods. In particular, we argue that meta-learning can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#26032;&#22411;&#33021;&#37327;&#23548;&#21521;&#25915;&#20987;&#31639;&#27861;GradMDM&#65292;&#20854;&#21487;&#20197;&#26377;&#25928;&#22320;&#22686;&#21152;&#35745;&#31639;&#22797;&#26434;&#24230;&#21516;&#26102;&#20943;&#23569;&#25200;&#21160;&#30340;&#24863;&#30693;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06724</link><description>&lt;p&gt;
GradMDM&#65306;&#21160;&#24577;&#32593;&#32476;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
GradMDM: Adversarial Attack on Dynamic Networks. (arXiv:2304.06724v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#26032;&#22411;&#33021;&#37327;&#23548;&#21521;&#25915;&#20987;&#31639;&#27861;GradMDM&#65292;&#20854;&#21487;&#20197;&#26377;&#25928;&#22320;&#22686;&#21152;&#35745;&#31639;&#22797;&#26434;&#24230;&#21516;&#26102;&#20943;&#23569;&#25200;&#21160;&#30340;&#24863;&#30693;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#26681;&#25454;&#36755;&#20837;&#35843;&#25972;&#20854;&#32467;&#26500;&#26469;&#26497;&#22823;&#22320;&#20943;&#23569;&#35745;&#31639;&#20887;&#20313;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#31934;&#24230;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38024;&#23545;&#26088;&#22312;&#38477;&#20302;&#20854;&#25928;&#29575;&#30340;&#33021;&#37327;&#23548;&#21521;&#25915;&#20987;&#30340;&#21160;&#24577;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#31639;&#27861;GradMDM&#25915;&#20987;&#21160;&#24577;&#27169;&#22411;&#12290; GradMDM&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#23427;&#35843;&#25972;&#26799;&#24230;&#30340;&#26041;&#21521;&#21644;&#22823;&#23567;&#65292;&#26377;&#25928;&#22320;&#20026;&#27599;&#20010;&#36755;&#20837;&#25214;&#21040;&#19968;&#20010;&#23567;&#25200;&#21160;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#28608;&#27963;&#21160;&#24577;&#27169;&#22411;&#30340;&#26356;&#22810;&#35745;&#31639;&#21333;&#20803;&#12290; &#25105;&#20204;&#35780;&#20272;&#20102;GradMDM&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#21160;&#24577;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#23427;&#20248;&#20110;&#20808;&#21069;&#30340;&#33021;&#37327;&#23548;&#21521;&#25915;&#20987;&#25216;&#26415;&#65292;&#26174;&#30528;&#22686;&#21152;&#20102;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25200;&#21160;&#30340;&#24863;&#30693;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic neural networks can greatly reduce computation redundancy without compromising accuracy by adapting their structures based on the input. In this paper, we explore the robustness of dynamic neural networks against energy-oriented attacks targeted at reducing their efficiency. Specifically, we attack dynamic models with our novel algorithm GradMDM. GradMDM is a technique that adjusts the direction and the magnitude of the gradients to effectively find a small perturbation for each input, that will activate more computational units of dynamic models during inference. We evaluate GradMDM on multiple datasets and dynamic models, where it outperforms previous energy-oriented attack techniques, significantly increasing computation complexity while reducing the perceptibility of the perturbations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06671</link><description>&lt;p&gt;
&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;&#21644;&#36845;&#20195;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#25511;&#21046;&#26159;&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#22312;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20855;&#26377;&#31867;&#20284;&#31354;&#38388;&#37197;&#32622;&#30340;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#38598;&#19978;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#20219;&#24847;&#19981;&#30830;&#23450;&#30340;&#24067;&#23616;&#30340;&#31163;&#32447;&#20998;&#24067;&#26679;&#26412;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#29616;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayoutBench&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35786;&#26029;&#30340;&#22522;&#20934;&#65292;&#23427;&#26816;&#26597;&#20102;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#65306;&#25968;&#37327;&#65292;&#20301;&#32622;&#65292;&#22823;&#23567;&#21644;&#24418;&#29366;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#26368;&#36817;&#20195;&#34920;&#24615;&#30340;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#33391;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#36793;&#30028;&#19978;&#30340;&#23545;&#35937;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#65292;&#23427;&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#23637;&#31034;&#20986;&#22312;LayoutBench&#30340;OOD&#24067;&#23616;&#19978;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#34920;&#26126;IterInpaint&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#22810;&#26679;&#21644;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#22270;&#20687;&#21644;&#21487;&#25511;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
&lt;/p&gt;</description></item><item><title>G2T&#26159;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;G2T&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.06653</link><description>&lt;p&gt;
G2T: &#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection. (arXiv:2304.06653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06653
&lt;/p&gt;
&lt;p&gt;
G2T&#26159;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;G2T&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#32858;&#31867;&#30340;&#20027;&#39064;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36866;&#24403;&#30340;&#35789;&#35821;&#31579;&#36873;&#26041;&#27861;&#32858;&#31867;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#29983;&#25104;&#27604;&#29983;&#25104;&#24335;&#27010;&#29575;&#20027;&#39064;&#27169;&#22411;&#26356;&#22909;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#36873;&#25321;&#21512;&#36866;&#21442;&#25968;&#30340;&#22256;&#38590;&#20197;&#21450;&#19981;&#23436;&#25972;&#30340;&#27169;&#22411;&#24573;&#30053;&#21333;&#35789;&#19982;&#20027;&#39064;&#21450;&#20027;&#39064;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#23450;&#37327;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#20294;&#26377;&#25928;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#22270;&#20027;&#39064;&#65288;G2T&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been reported that clustering-based topic models, which cluster high-quality sentence embeddings with an appropriate word selection method, can generate better topics than generative probabilistic topic models. However, these approaches suffer from the inability to select appropriate parameters and incomplete models that overlook the quantitative relation between words with topics and topics with text. To solve these issues, we propose graph to topic (G2T), a simple but effective framework for topic modelling. The framework is composed of four modules. First, document representation is acquired using pretrained language models. Second, a semantic graph is constructed according to the similarity between document representations. Third, communities in document semantic graphs are identified, and the relationship between topics and documents is quantified accordingly. Fourth, the word--topic distribution is computed based on a variant of TFIDF. Automatic evaluation suggests that G2
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#30340;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;MProtoNet&#65292;&#29992;&#20110;&#24102;&#26377;3D&#22810;&#21442;&#25968;&#30913;&#20849;&#25391;&#25104;&#20687;&#30340;&#33041;&#32959;&#30244;&#20998;&#31867;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#22411;&#30340;&#27880;&#24847;&#27169;&#22359;&#65292;&#27604;&#22235;&#20010;&#29366;&#24577;-of-the-art&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35299;&#37322;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#19978;&#26377;&#25152;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2304.06258</link><description>&lt;p&gt;
MProtoNet&#65306;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#30340;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;&#65292;&#29992;&#20110;&#24102;&#26377;3D&#22810;&#21442;&#25968;&#30913;&#20849;&#25391;&#25104;&#20687;&#30340;&#33041;&#32959;&#30244;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MProtoNet: A Case-Based Interpretable Model for Brain Tumor Classification with 3D Multi-parametric Magnetic Resonance Imaging. (arXiv:2304.06258v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26696;&#20363;&#30340;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;MProtoNet&#65292;&#29992;&#20110;&#24102;&#26377;3D&#22810;&#21442;&#25968;&#30913;&#20849;&#25391;&#25104;&#20687;&#30340;&#33041;&#32959;&#30244;&#20998;&#31867;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#22411;&#30340;&#27880;&#24847;&#27169;&#22359;&#65292;&#27604;&#22235;&#20010;&#29366;&#24577;-of-the-art&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35299;&#37322;&#24615;&#21644;&#20998;&#31867;&#24615;&#33021;&#19978;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#24212;&#29992;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#21487;&#35299;&#37322;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21307;&#30103;&#21407;&#22411;&#32593;&#32476;&#65288;MProtoNet&#65289;&#65292;&#29992;&#20110;&#23558;ProtoPNet&#25193;&#23637;&#21040;&#20351;&#29992;3D&#22810;&#21442;&#25968;&#30913;&#20849;&#25391;&#25104;&#20687;&#25968;&#25454;&#36827;&#34892;&#33041;&#32959;&#30244;&#20998;&#31867;&#12290;&#20026;&#20102;&#35299;&#20915;2D&#33258;&#28982;&#22270;&#20687;&#21644;3D mpMRI&#20043;&#38388;&#22312;&#26412;&#22320;&#21270;&#27880;&#24847;&#21306;&#22495;&#26041;&#38754;&#30340;&#19981;&#21516;&#35201;&#27714;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25513;&#33180;&#21644;&#22312;&#32447;CAM&#25439;&#22833;&#30340;&#26032;&#22411;&#27880;&#24847;&#27169;&#22359;&#12290;MProtoNet&#36890;&#36807;&#27880;&#24847;&#21147;&#26144;&#23556;&#21644;&#21407;&#22411;&#21487;&#35270;&#21270;&#32500;&#25345;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#19968;&#20010;&#21253;&#21547;&#33041;&#33014;&#36136;&#30244;&#21644;&#33041;&#33180;&#30244;&#30340;mpMRI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20998;&#31867;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent applications of deep convolutional neural networks in medical imaging raise concerns about their interpretability. While most explainable deep learning applications use post hoc methods (such as GradCAM) to generate feature attribution maps, there is a new type of case-based reasoning models, namely ProtoPNet and its variants, which identify prototypes during training and compare input image patches with those prototypes. We propose the first medical prototype network (MProtoNet) to extend ProtoPNet to brain tumor classification with 3D multi-parametric magnetic resonance imaging (mpMRI) data. To address different requirements between 2D natural images and 3D mpMRIs especially in terms of localizing attention regions, a new attention module with soft masking and online-CAM loss is introduced. Soft masking helps sharpen attention maps, while online-CAM loss directly utilizes image-level labels when training the attention module. MProtoNet achieves statistically significant improv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;PINNs&#20013;&#20351;&#29992;MLE&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#36890;&#36807;ODE&#32806;&#21512;&#30697;&#38453;&#30340;SVD&#20998;&#35299;&#38477;&#32500;&#65292;&#22686;&#21152;&#20102;PINNs&#39044;&#27979;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05991</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#29992;&#20110;&#39640;&#32500;&#21453;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Maximum-likelihood Estimators in Physics-Informed Neural Networks for High-dimensional Inverse Problems. (arXiv:2304.05991v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;PINNs&#20013;&#20351;&#29992;MLE&#30340;&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#36890;&#36807;ODE&#32806;&#21512;&#30697;&#38453;&#30340;SVD&#20998;&#35299;&#38477;&#32500;&#65292;&#22686;&#21152;&#20102;PINNs&#39044;&#27979;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#24050;&#34987;&#35777;&#26126;&#26159;&#35299;&#20915;&#21453;&#24120;(ODE)&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#30340;&#21512;&#36866;&#25968;&#23398;&#26694;&#26550;&#12290;&#20856;&#22411;&#30340;&#21453;&#21521;PINNs&#34987;&#21046;&#23450;&#20026;&#24102;&#26377;&#20960;&#20010;&#36229;&#21442;&#25968;&#30340;&#36719;&#32422;&#26463;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21453;&#21521;PINNs&#21487;&#20197;&#29992;&#26497;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;(MLE)&#30340;&#24418;&#24335;&#26469;&#34920;&#36798;&#65292;&#36890;&#36807;Taylor&#23637;&#24320;&#65292;&#23558;&#25554;&#20540;&#35823;&#24046;&#26126;&#30830;&#22320;&#20256;&#25773;&#21040;&#29289;&#29702;&#27169;&#22411;&#31354;&#38388;&#20013;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20854;&#24212;&#29992;&#20110;&#39640;&#32500;&#32806;&#21512;ODEs&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;ODEs&#21463;&#21040;&#22312;&#30636;&#24577;&#21270;&#23398;&#21644;&#29983;&#29289;&#21160;&#21147;&#23398;&#20013;&#24120;&#35265;&#30340;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ODE&#32806;&#21512;&#30697;&#38453;(&#21453;&#24212;&#21270;&#23398;&#35745;&#37327;&#30697;&#38453;)&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;(SVD)&#25552;&#20379;&#20102;&#20943;&#23569;&#30340;&#19981;&#30456;&#20851;&#23376;&#31354;&#38388;&#65292;&#22312;&#20854;&#20013;&#21487;&#20197;&#34920;&#31034;PINNs&#35299;&#65292;&#24182;&#21487;&#20197;&#23545;&#27531;&#24046;&#36827;&#34892;&#25237;&#24433;&#12290;&#26368;&#21518;&#65292;SVD&#22522;&#20989;&#25968;&#20316;&#20026;&#20808;&#39564;&#32422;&#26463;&#22686;&#24378;&#20102;&#39044;&#27979;&#30340;&#31283;&#23450;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) have proven a suitable mathematical scaffold for solving inverse ordinary (ODE) and partial differential equations (PDE). Typical inverse PINNs are formulated as soft-constrained multi-objective optimization problems with several hyperparameters. In this work, we demonstrate that inverse PINNs can be framed in terms of maximum-likelihood estimators (MLE) to allow explicit error propagation from interpolation to the physical model space through Taylor expansion, without the need of hyperparameter tuning. We explore its application to high-dimensional coupled ODEs constrained by differential algebraic equations that are common in transient chemical and biological kinetics. Furthermore, we show that singular-value decomposition (SVD) of the ODE coupling matrices (reaction stoichiometry matrix) provides reduced uncorrelated subspaces in which PINNs solutions can be represented and over which residuals can be projected. Finally, SVD bases serve as pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26368;&#23567;&#20540;&#19978;&#30028;&#25552;&#20986;&#20102;&#25240;&#25187;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#30340;&#20272;&#35745;&#35823;&#24046;&#19982;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#28151;&#21512;&#29305;&#24615;&#21644;&#25240;&#25187;&#22240;&#23376;&#20043;&#38388;&#30340;&#26126;&#30830;&#32852;&#31995;&#65292;&#24182;&#23545;&#19968;&#32452;&#26174;&#33879;&#20272;&#35745;&#22120;&#21450;&#20854;&#23545;&#24212;&#30340;&#37319;&#26679;&#31243;&#24207;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.05073</link><description>&lt;p&gt;
&#25240;&#25187;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#37319;&#26679;&#21644;&#20272;&#35745;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
A Tale of Sampling and Estimation in Discounted Reinforcement Learning. (arXiv:2304.05073v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26368;&#23567;&#20540;&#19978;&#30028;&#25552;&#20986;&#20102;&#25240;&#25187;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#30340;&#20272;&#35745;&#35823;&#24046;&#19982;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#28151;&#21512;&#29305;&#24615;&#21644;&#25240;&#25187;&#22240;&#23376;&#20043;&#38388;&#30340;&#26126;&#30830;&#32852;&#31995;&#65292;&#24182;&#23545;&#19968;&#32452;&#26174;&#33879;&#20272;&#35745;&#22120;&#21450;&#20854;&#23545;&#24212;&#30340;&#37319;&#26679;&#31243;&#24207;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25240;&#25187;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#30456;&#20851;&#30340;&#38382;&#39064;&#21253;&#25324;&#22312;&#39532;&#23572;&#21487;&#22827;&#22870;&#21169;&#36807;&#31243;&#30340;&#31283;&#24577;&#20998;&#24067;&#19979;&#23545;&#20989;&#25968;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#65292;&#20363;&#22914;&#31574;&#30053;&#35780;&#20272;&#20013;&#30340;&#39044;&#26399;&#22238;&#25253;&#25110;&#31574;&#30053;&#20248;&#21270;&#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#20272;&#35745;&#36890;&#36807;&#26377;&#38480;&#22320;&#36827;&#34892;&#21608;&#26399;&#37319;&#26679;&#26469;&#20135;&#29983;&#65292;&#36825;&#31181;&#37319;&#26679;&#26041;&#24335;&#24573;&#30053;&#20102;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#28151;&#21512;&#29305;&#24615;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#36825;&#31181;&#23454;&#38469;&#21644;&#29702;&#35770;&#35774;&#32622;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#65292;&#25991;&#29486;&#20013;&#20063;&#32570;&#20047;&#23545;&#21608;&#26399;&#37319;&#26679;&#30340;&#32570;&#38519;&#20197;&#21450;&#22914;&#20309;&#26368;&#20248;&#22320;&#36827;&#34892;&#21608;&#26399;&#37319;&#26679;&#30340;&#27491;&#24335;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25240;&#25187;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#30340;&#26368;&#23567;&#20540;&#19978;&#30028;&#65292;&#26126;&#30830;&#22320;&#23558;&#20272;&#35745;&#35823;&#24046;&#19982;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#28151;&#21512;&#29305;&#24615;&#21644;&#25240;&#25187;&#22240;&#23376;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#26174;&#33879;&#20272;&#35745;&#22120;&#21450;&#20854;&#23545;&#24212;&#30340;&#37319;&#26679;&#31243;&#24207;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#65292;&#21253;&#25324;&#36890;&#24120;&#20351;&#29992;&#30340;&#26377;&#38480;&#26102;&#38388;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The most relevant problems in discounted reinforcement learning involve estimating the mean of a function under the stationary distribution of a Markov reward process, such as the expected return in policy evaluation, or the policy gradient in policy optimization. In practice, these estimates are produced through a finite-horizon episodic sampling, which neglects the mixing properties of the Markov process. It is mostly unclear how this mismatch between the practical and the ideal setting affects the estimation, and the literature lacks a formal study on the pitfalls of episodic sampling, and how to do it optimally. In this paper, we present a minimax lower bound on the discounted mean estimation problem that explicitly connects the estimation error with the mixing properties of the Markov process and the discount factor. Then, we provide a statistical analysis on a set of notable estimators and the corresponding sampling procedures, which includes the finite-horizon estimators often u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;MicroTVM&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#35299;&#26512;&#20026;&#21518;&#31471;&#30340;C&#28304;&#20195;&#30721;&#24211;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;Ahead-of-Time C&#36816;&#34892;&#26102;&#22312;ARM Cortex M4F&#26680;&#24515;&#19978;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2304.04842</link><description>&lt;p&gt;
&#20351;&#29992;MicroTVM&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#21040;&#36793;&#32536;Ahead-of-Time&#36816;&#34892;
&lt;/p&gt;
&lt;p&gt;
Deploying Machine Learning Models to Ahead-of-Time Runtime on Edge Using MicroTVM. (arXiv:2304.04842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;MicroTVM&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#35299;&#26512;&#20026;&#21518;&#31471;&#30340;C&#28304;&#20195;&#30721;&#24211;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;Ahead-of-Time C&#36816;&#34892;&#26102;&#22312;ARM Cortex M4F&#26680;&#24515;&#19978;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;AI&#24212;&#29992;&#31243;&#24207;&#24050;&#32463;&#24212;&#29992;&#21040;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#25968;&#25454;&#31185;&#23398;&#23478;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65288;&#22914;PyTorch&#25110;TensorFlow&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#26080;&#27861;&#26080;&#32541;&#22320;&#22312;&#36793;&#32536;&#19978;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;MicroTVM&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#35299;&#26512;&#20026;&#21518;&#31471;&#30340;C&#28304;&#20195;&#30721;&#24211;&#65292;MicroTVM&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#32534;&#35793;&#22120;&#26694;&#26550;&#25193;&#23637;&#65292;&#29992;&#20110;&#22788;&#29702;&#35064;&#26426;&#35774;&#22791;&#19978;&#30340;&#25512;&#26029;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#29305;&#23450;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#36816;&#31639;&#31526;&#21487;&#20197;&#36731;&#26494;&#22320;&#36890;&#36807;&#36890;&#29992;&#27169;&#22359;&#21152;&#36895;&#22120;&#65288;UMA&#65289;&#25509;&#21475;&#21368;&#36733;&#21040;&#19987;&#29992;&#21152;&#36895;&#22120;&#19978;&#65292;&#32780;&#20854;&#20182;&#36816;&#31639;&#31526;&#21017;&#22312;CPU&#26680;&#24515;&#20013;&#22788;&#29702;&#12290;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;Ahead-of-Time C&#36816;&#34892;&#26102;&#65292;&#22312;ARM Cortex M4F&#26680;&#24515;&#19978;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, more and more AI applications have been applied to edge devices. However, models trained by data scientists with machine learning frameworks, such as PyTorch or TensorFlow, can not be seamlessly executed on edge. In this paper, we develop an end-to-end code generator parsing a pre-trained model to C source libraries for the backend using MicroTVM, a machine learning compiler framework extension addressing inference on bare metal devices. An analysis shows that specific compute-intensive operators can be easily offloaded to the dedicated accelerator with a Universal Modular Accelerator (UMA) interface, while others are processed in the CPU cores. By using the automatically generated ahead-of-time C runtime, we conduct a hand gesture recognition experiment on an ARM Cortex M4F core.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.03997</link><description>&lt;p&gt;
REDf&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
REDf: A Renewable Energy Demand Forecasting Model for Smart Grids using Long Short Term Memory Network. (arXiv:2304.03997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#30340;&#26234;&#33021;&#30005;&#32593;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#27169;&#22411;REDf&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#65292;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20934;&#30830;&#24230;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19990;&#30028;&#21521;&#26356;&#21487;&#25345;&#32493;&#30340;&#33021;&#28304;&#26410;&#26469;&#21457;&#23637;&#65292;&#23558;&#21487;&#20877;&#29983;&#33021;&#28304;&#28304;&#32435;&#20837;&#30005;&#32593;&#30340;&#38598;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38388;&#27463;&#24615;&#20351;&#30005;&#32593;&#31649;&#29702;&#21644;&#30830;&#20445;&#31283;&#23450;&#30340;&#30005;&#21147;&#20379;&#24212;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#33021;&#37327;&#38656;&#27714;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;&#33021;&#37327;&#38656;&#27714;&#39044;&#27979;&#26469;&#25913;&#21892;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#33021;&#47071;&#38656;&#27714;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20123;&#32593;&#32476;&#29305;&#21035;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#22235;&#20010;&#21382;&#21490;&#33021;&#37327;&#38656;&#27714;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#33258;&#19981;&#21516;&#30340;&#33021;&#28304;&#20998;&#37197;&#20844;&#21496;&#65292;&#21253;&#25324;&#32654;&#22269;&#30005;&#21147;&#12289;Commonwealth Edison&#12289;Dayton Power and Light&#20197;&#21450;&#23486;&#22805;&#27861;&#23612;&#20122;-&#26032;&#27901;&#35199;-&#39532;&#37324;&#20848;&#20114;&#32852;&#32593;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;REDf&#27169;&#22411;&#19982;&#20854;&#20182;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;REDf&#27169;&#22411;&#22312;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#12289;&#22343;&#26041;&#26681;&#35823;&#24046;&#21644;&#20915;&#23450;&#31995;&#25968;&#31561;&#20934;&#30830;&#24230;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;REDf&#21487;&#20197;&#20316;&#20026;&#21487;&#20877;&#29983;&#33021;&#28304;&#38656;&#27714;&#39044;&#27979;&#30340;&#21487;&#38752;&#24037;&#20855;&#65292;&#24182;&#25552;&#39640;&#21487;&#20877;&#29983;&#33021;&#28304;&#32435;&#20837;&#26234;&#33021;&#30005;&#32593;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future. However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity. In this paper, we propose a deep learning-based approach for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. We use long short-term memory networks, which are well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four datasets of historical energy demand data from different energy distribution companies including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is also compared with two 
&lt;/p&gt;</description></item><item><title>InstructBio&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21270;&#23398;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#25945;&#32451;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#30340;&#32622;&#20449;&#24230;&#27604;&#29575;&#26469;&#25351;&#23548;&#30446;&#26631;&#27169;&#22411;&#23545;&#19981;&#21516;&#25968;&#25454;&#28857;&#32473;&#20104;&#26126;&#26174;&#20851;&#27880;&#65292;&#36991;&#20813;&#20381;&#36182;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#19981;&#27491;&#30830;&#30340;&#20266;&#27880;&#37322;&#65292;&#25552;&#39640;&#20102;&#20998;&#23376;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.03906</link><description>&lt;p&gt;
InstructBio&#65306;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21270;&#23398;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
InstructBio: A Large-scale Semi-supervised Learning Paradigm for Biochemical Problems. (arXiv:2304.03906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03906
&lt;/p&gt;
&lt;p&gt;
InstructBio&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#29289;&#21270;&#23398;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#24341;&#20837;&#25945;&#32451;&#27169;&#22411;&#25552;&#20379;&#26377;&#25928;&#30340;&#32622;&#20449;&#24230;&#27604;&#29575;&#26469;&#25351;&#23548;&#30446;&#26631;&#27169;&#22411;&#23545;&#19981;&#21516;&#25968;&#25454;&#28857;&#32473;&#20104;&#26126;&#26174;&#20851;&#27880;&#65292;&#36991;&#20813;&#20381;&#36182;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#19981;&#27491;&#30830;&#30340;&#20266;&#27880;&#37322;&#65292;&#25552;&#39640;&#20102;&#20998;&#23376;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#30340;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#22987;&#32456;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#22312;&#22823;&#22411;&#26410;&#26631;&#35760;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#24378;&#21147;&#30340;&#20219;&#21153;&#26080;&#20851;&#27169;&#22411;&#65292;&#20294;&#22312;&#21521;&#19979;&#28216;&#20219;&#21153;&#36716;&#31227;&#30693;&#35782;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructBio&#65292;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#26679;&#20363;&#12290;&#23427;&#24341;&#20837;&#25945;&#32451;&#27169;&#22411;&#26469;&#25552;&#20379;&#20266;&#26631;&#31614;&#21487;&#38752;&#24615;&#30340;&#32622;&#20449;&#24230;&#27604;&#29575;&#12290;&#36825;&#20123;&#32622;&#20449;&#24230;&#20998;&#25968;&#28982;&#21518;&#25351;&#23548;&#30446;&#26631;&#27169;&#22411;&#23545;&#19981;&#21516;&#30340;&#25968;&#25454;&#28857;&#32473;&#20104;&#26126;&#26174;&#30340;&#20851;&#27880;&#65292;&#36991;&#20813;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#20197;&#21450;&#19981;&#27491;&#30830;&#30340;&#20266;&#27880;&#37322;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;InstructBio&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#23376;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26041;&#38754;&#65292;&#22312;&#27963;&#24615;&#24748;&#23830;&#20272;&#35745;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of artificial intelligence for science, it is consistently an essential challenge to face a limited amount of labeled data for real-world problems. The prevailing approach is to pretrain a powerful task-agnostic model on a large unlabeled corpus but may struggle to transfer knowledge to downstream tasks. In this study, we propose InstructMol, a semi-supervised learning algorithm, to take better advantage of unlabeled examples. It introduces an instructor model to provide the confidence ratios as the measurement of pseudo-labels' reliability. These confidence scores then guide the target model to pay distinct attention to different data points, avoiding the over-reliance on labeled data and the negative influence of incorrect pseudo-annotations. Comprehensive experiments show that InstructBio substantially improves the generalization ability of molecular models, in not only molecular property predictions but also activity cliff estimations, demonstrating the superiority of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedLA&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20837;&#20010;&#24615;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#32531;&#35299;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.01783</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#19982;&#26412;&#22320;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Personalized Federated Learning with Local Attention. (arXiv:2304.01783v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;pFedLA&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20837;&#20010;&#24615;&#21270;&#27169;&#22411;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#32531;&#35299;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#21333;&#19968;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#20351;&#24471;&#20013;&#22830;&#26381;&#21153;&#22120;&#21487;&#20197;&#24110;&#21161;&#22312;&#26412;&#22320;&#23458;&#25143;&#31471;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#19981;&#24517;&#35775;&#38382;&#20854;&#26412;&#22320;&#25968;&#25454;&#12290;&#32852;&#37030;&#23398;&#20064;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#19981;&#21516;&#23458;&#25143;&#31471;&#20013;&#26412;&#22320;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#20363;&#22914;&#24322;&#36136;&#26631;&#31614;&#20998;&#24067;&#21644;&#29305;&#24449;&#20559;&#31227;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#27169;&#22411;&#30340;&#26174;&#30528;&#24615;&#33021;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21363;&#20855;&#26377;&#26412;&#22320;&#27880;&#24847;&#21147;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFedLA&#65289;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#26426;&#21046;&#24182;&#20837;&#23458;&#25143;&#31471;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#27880;&#24847;&#22359;&#29305;&#23450;&#20110;&#23458;&#25143;&#31471;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;pFedLA&#25552;&#20986;&#20102;&#20004;&#20010;&#27169;&#22359;&#65292;&#21363;&#20010;&#24615;&#21270;&#21333;&#27880;&#24847;&#27169;&#22359;&#21644;&#20010;&#24615;&#21270;&#28151;&#21512;&#27880;&#24847;&#27169;&#22359;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;FL&#25968;&#25454;&#38598;SplitMNIST-C&#65292;&#36890;&#36807;&#24341;&#20837;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#20559;&#31227;&#12290;&#22312;SplitMNIST-C&#21644;EMNIST&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;pFedLA&#22312;&#20934;&#30830;&#24615;&#21644;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;FL&#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#32531;&#35299;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aims to learn a single global model that enables the central server to help the model training in local clients without accessing their local data. The key challenge of FL is the heterogeneity of local data in different clients, such as heterogeneous label distribution and feature shift, which could lead to significant performance degradation of the learned models. Although many studies have been proposed to address the heterogeneous label distribution problem, few studies attempt to explore the feature shift issue. To address this issue, we propose a simple yet effective algorithm, namely \textbf{p}ersonalized \textbf{Fed}erated learning with \textbf{L}ocal \textbf{A}ttention (pFedLA), by incorporating the attention mechanism into personalized models of clients while keeping the attention blocks client-specific. Specifically, two modules are proposed in pFedLA, i.e., the personalized single attention module and the personalized hybrid attention module. In addit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#39592;&#24178;&#32593;&#32476;&#30340;&#28176;&#36827;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20854;&#20013;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;MAE&#27169;&#22411;&#36827;&#34892;&#27491;&#24120;&#22270;&#20687;&#30340;&#35757;&#32451;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#20687;&#32032;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#29983;&#25104;&#25439;&#22351;&#30340;&#27491;&#24120;&#22270;&#20687;&#65292;&#26368;&#32456;&#36890;&#36807;&#20687;&#32032;&#37325;&#24314;&#35823;&#24046;&#30697;&#38453;&#21644;&#20687;&#32032;&#24322;&#24120;&#27010;&#29575;&#30697;&#38453;&#32508;&#21512;&#24471;&#21040;&#19968;&#20010;&#24322;&#24120;&#24471;&#20998;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2303.17354</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#28176;&#36827;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization. (arXiv:2303.17354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#39592;&#24178;&#32593;&#32476;&#30340;&#28176;&#36827;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20854;&#20013;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;MAE&#27169;&#22411;&#36827;&#34892;&#27491;&#24120;&#22270;&#20687;&#30340;&#35757;&#32451;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#20687;&#32032;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#29983;&#25104;&#25439;&#22351;&#30340;&#27491;&#24120;&#22270;&#20687;&#65292;&#26368;&#32456;&#36890;&#36807;&#20687;&#32032;&#37325;&#24314;&#35823;&#24046;&#30697;&#38453;&#21644;&#20687;&#32032;&#24322;&#24120;&#27010;&#29575;&#30697;&#38453;&#32508;&#21512;&#24471;&#21040;&#19968;&#20010;&#24322;&#24120;&#24471;&#20998;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#23545;&#20110;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#22312;&#24037;&#19994;&#32570;&#38519;&#26816;&#27979;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#39592;&#24178;&#32593;&#32476;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#31574;&#30053;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#27491;&#24120;&#22270;&#20687;&#23545;Masked Autoencoder &#65288;MAE&#65289;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20687;&#32032;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20197;&#29983;&#25104;&#24050;&#25439;&#22351;&#30340;&#27491;&#24120;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#20687;&#32032;&#26631;&#31614;&#12290;&#36825;&#20010;&#36807;&#31243;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20462;&#22797;&#25439;&#22351;&#30340;&#21306;&#22495;&#21644;&#20998;&#31867;&#27599;&#20010;&#20687;&#32032;&#30340;&#29366;&#24577;&#12290;&#26368;&#32456;&#65292;&#35813;&#27169;&#22411;&#20135;&#29983;&#19968;&#20010;&#20687;&#32032;&#37325;&#24314;&#35823;&#24046;&#30697;&#38453;&#21644;&#19968;&#20010;&#20687;&#32032;&#24322;&#24120;&#27010;&#29575;&#30697;&#38453;&#65292;&#36825;&#20004;&#20010;&#30697;&#38453;&#32508;&#21512;&#25104;&#19968;&#20010;&#24322;&#24120;&#24471;&#20998;&#30697;&#38453;&#65292;&#26377;&#25928;&#22320;&#29992;&#20110;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the machine learning domain, research on anomaly detection and localization within image data has garnered significant attention, particularly in practical applications such as industrial defect detection. While existing approaches predominantly rely on Convolutional Neural Networks (CNN) as their backbone network, we propose an innovative method based on the Transformer backbone network. Our approach employs a two-stage incremental learning strategy. In the first stage, we train a Masked Autoencoder (MAE) model exclusively on normal images. Subsequently, in the second stage, we implement pixel-level data augmentation techniques to generate corrupted normal images and their corresponding pixel labels. This process enables the model to learn how to repair corrupted regions and classify the state of each pixel. Ultimately, the model produces a pixel reconstruction error matrix and a pixel anomaly probability matrix, which are combined to create an anomaly scoring matrix that effective
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#24212;&#29992;&#28145;&#24230;&#38598;&#21512;&#37327;&#21270;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#26694;&#26550;&#65292;&#20854;&#22312;&#22238;&#24402;&#20934;&#30830;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#38752;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16210</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#21512;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#37327;&#21270;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Quantifying Calibrated Uncertainty via Deep Ensembles in Multi-output Regression Task. (arXiv:2303.16210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#24212;&#29992;&#28145;&#24230;&#38598;&#21512;&#37327;&#21270;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#26694;&#26550;&#65292;&#20854;&#22312;&#22238;&#24402;&#20934;&#30830;&#24615;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21487;&#38752;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#21512;&#26159;&#36924;&#36817;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#19968;&#31181;&#31616;&#21333;&#30452;&#25509;&#30340;&#26041;&#27861;&#65292;&#24050;&#34987;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#20998;&#31867;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20840;&#38754;&#25506;&#31350;&#35813;&#26041;&#27861;&#22312;&#22810;&#36755;&#20986;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#39044;&#27979;&#23548;&#24377;&#32467;&#26500;&#30340;&#31354;&#27668;&#21160;&#21147;&#24615;&#33021;&#12290;&#36890;&#36807;&#20180;&#32454;&#30740;&#31350;&#38598;&#21512;&#20013;&#31070;&#32463;&#32593;&#32476;&#25968;&#37327;&#30340;&#24433;&#21709;&#65292;&#35266;&#23519;&#21040;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#26222;&#36941;&#23384;&#22312;&#20302;&#20272;&#30340;&#36235;&#21183;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20107;&#21518;&#26657;&#20934;&#30340;&#28145;&#24230;&#38598;&#21512;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20854;&#25913;&#36827;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#24615;&#33021;&#12290;&#30452;&#35266;&#22320;&#23558;&#20854;&#19982;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#26159;&#24037;&#31243;&#20013;&#26368;&#24120;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22238;&#24402;&#20934;&#30830;&#24615;&#12289;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#38752;&#24615;&#21644;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#20063;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensemble is a simple and straightforward approach for approximating Bayesian inference and has been successfully applied to many classification tasks. This study aims to comprehensively investigate this approach in the multi-output regression task to predict the aerodynamic performance of a missile configuration. By scrutinizing the effect of the number of neural networks used in the ensemble, an obvious trend toward underconfidence in estimated uncertainty is observed. In this context, we propose the deep ensemble framework that applies the post-hoc calibration method, and its improved uncertainty quantification performance is demonstrated. It is compared with Gaussian process regression, the most prevalent model for uncertainty quantification in engineering, and is proven to have superior performance in terms of regression accuracy, reliability of estimated uncertainty, and training efficiency. Finally, the impact of the suggested framework on the results of Bayesian optimizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#22312;&#20840;&#29699;21&#23478;&#21307;&#38498;&#30340;10,000&#22810;&#20301;COVID&#24739;&#32773;&#30340;&#33016;&#37096;CT&#25195;&#25551;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#21512;&#25104;&#29983;&#25104;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65292;&#20026;&#21307;&#23398;AI&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.13567</link><description>&lt;p&gt;
&#20351;&#29992;&#33016;&#37096;CT&#36827;&#34892;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning on Heterogenous Data using Chest CT. (arXiv:2303.13567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#22312;&#20840;&#29699;21&#23478;&#21307;&#38498;&#30340;10,000&#22810;&#20301;COVID&#24739;&#32773;&#30340;&#33016;&#37096;CT&#25195;&#25551;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#32467;&#21512;&#21512;&#25104;&#29983;&#25104;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#65292;&#20026;&#21307;&#23398;AI&#22312;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#12290;&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#65292;&#26469;&#33258;&#36951;&#20256;&#12289;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#39278;&#39135;&#21644;&#21508;&#31181;&#29615;&#22659;&#22240;&#32032;&#30340;&#20154;&#32676;&#24046;&#24322;&#23545;&#30142;&#30149;&#36129;&#29486;&#26174;&#33879;&#65292;&#20294;&#26159;&#65292;&#22312;&#21307;&#23398;&#20013;&#36827;&#34892;&#30340;AI&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20855;&#26377;&#36739;&#23569;&#25968;&#25454;&#26469;&#28304;&#21644;&#36739;&#23569;&#26679;&#26412;&#24046;&#24322;&#30340;&#21306;&#22495;&#24739;&#32773;&#38431;&#21015;&#20013;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110;&#22312;&#21307;&#23398;&#20013;&#23454;&#29616;&#22823;&#35268;&#27169;&#25968;&#25454;&#20849;&#20139;&#30340;&#38556;&#30861;&#20197;&#21450;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#36947;&#24503;&#25285;&#24551;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#28508;&#22312;&#30340;AI&#21457;&#23637;&#36884;&#24452;&#65292;&#21487;&#20197;&#22312;&#21307;&#38498;&#20043;&#38388;&#36827;&#34892;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#25968;&#25454;&#20849;&#20139;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#19977;&#31181;&#25216;&#26415;&#65288;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#12289;&#22686;&#37327;&#21046;&#24230;&#23398;&#20064;&#65288;IIL&#65289;&#21644;&#24490;&#29615;&#22686;&#37327;&#21046;&#24230;&#23398;&#20064;&#65288;CIIL&#65289;&#65289;&#22312;&#28085;&#30422;5&#20010;&#22823;&#38470;&#30340;21&#23478;&#21442;&#19982;&#21307;&#38498;&#20013;&#36827;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#22312;COVID-19&#33016;&#37096;CT&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#26368;&#22823;&#21644;&#26368;&#22810;&#26679;&#21270;&#30340;&#65292;&#21253;&#25324;&gt;10,000&#20301;&#24739;&#32773;&#21644;&gt;1,000,000&#24352;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#29983;&#25104;&#25968;&#25454;&#21644;FL&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large data have accelerated advances in AI. While it is well known that population differences from genetics, sex, race, diet, and various environmental factors contribute significantly to disease, AI studies in medicine have largely focused on locoregional patient cohorts with less diverse data sources. Such limitation stems from barriers to large-scale data share in medicine and ethical concerns over data privacy. Federated learning (FL) is one potential pathway for AI development that enables learning across hospitals without data share. In this study, we show the results of various FL strategies on one of the largest and most diverse COVID-19 chest CT datasets: 21 participating hospitals across five continents that comprise &gt;10,000 patients with &gt;1 million images. We present three techniques: Fed Averaging (FedAvg), Incremental Institutional Learning (IIL), and Cyclical Incremental Institutional Learning (CIIL). We also propose an FL strategy that leverages synthetically generated 
&lt;/p&gt;</description></item><item><title>CoLT5&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#26469;&#21152;&#36895;&#38271;&#36317;&#31163;&#36755;&#20837;&#30340;&#22788;&#29702;&#12290;CoLT5&#22312;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09752</link><description>&lt;p&gt;
CoLT5: &#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;&#24555;&#36895;&#38271;&#36317;&#31163;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09752
&lt;/p&gt;
&lt;p&gt;
CoLT5&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#26469;&#21152;&#36895;&#38271;&#36317;&#31163;&#36755;&#20837;&#30340;&#22788;&#29702;&#12290;CoLT5&#22312;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#22788;&#29702;&#38271;&#36755;&#20837;&#65292;&#20294;&#20351;&#29992;Transformer&#22788;&#29702;&#38271;&#25991;&#26723;&#24456;&#26114;&#36149;&#8212;&#8212;&#36825;&#19981;&#20165;&#26159;&#22240;&#20026;&#20108;&#27425;&#27880;&#24847;&#22797;&#26434;&#24615;&#65292;&#36824;&#22240;&#20026;&#23545;&#27599;&#20010;&#26631;&#35760;&#24212;&#29992;&#21069;&#39304;&#21644;&#25237;&#24433;&#23618;&#12290;&#28982;&#32780;&#65292;&#19981;&#26159;&#25152;&#26377;&#26631;&#35760;&#37117;&#21516;&#26679;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CoLT5&#65292;&#19968;&#31181;&#38271;&#36755;&#20837;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#35745;&#31639;&#26469;&#21033;&#29992;&#27492;&#30452;&#35273;&#65292;&#22312;&#21069;&#39304;&#21644;&#27880;&#24847;&#23618;&#20013;&#20026;&#37325;&#35201;&#26631;&#35760;&#25552;&#20379;&#26356;&#22810;&#36164;&#28304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoLT5&#27604;LongT5&#34920;&#29616;&#26356;&#24378;&#65292;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26356;&#24555;&#65292;&#22312;&#38271;&#36755;&#20837;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;SOTA&#12290;&#27492;&#22806;&#65292;CoLT5&#33021;&#22815;&#26377;&#25928;&#19988;&#21487;&#25511;&#22320;&#21033;&#29992;&#26497;&#38271;&#30340;&#36755;&#20837;&#65292;&#23637;&#31034;&#20102;&#39640;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#30340;&#24378;&#22823;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;SPSA&#31639;&#27861;&#27714;&#35299;n&#27493;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;n&#20540;&#30340;&#31639;&#27861;SDPSA&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07068</link><description>&lt;p&gt;
&#12298;&#20855;&#26377;&#26368;&#20248;n&#30340;n&#27493;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
n-Step Temporal Difference Learning with Optimal n. (arXiv:2303.07068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;SPSA&#31639;&#27861;&#27714;&#35299;n&#27493;&#26102;&#24207;&#24046;&#20998;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;n&#20540;&#30340;&#31639;&#27861;SDPSA&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;n&#27493;&#26102;&#24207;&#24046;&#20998;&#31639;&#27861;&#20013;&#25214;&#21040;&#26368;&#20248;n&#20540;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#27169;&#22411;&#33258;&#30001;&#20248;&#21270;&#25216;&#26415;&#65292;&#21363;&#21516;&#26102;&#25200;&#21160;&#38543;&#26426;&#36924;&#36817;&#65288;SPSA&#65289;&#26041;&#27861;&#26469;&#23547;&#25214;&#26368;&#20248;n&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#27169;&#25311;SPSA&#31243;&#24207;&#65292;&#23558;&#20854;&#21407;&#22987;&#36830;&#32493;&#20248;&#21270;&#26694;&#26550;&#24341;&#20837;&#31163;&#25955;&#20248;&#21270;&#26694;&#26550;&#65292;&#20294;&#24182;&#32467;&#21512;&#20102;&#24490;&#29615;&#25200;&#21160;&#24207;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;SDPSA&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#20219;&#24847;&#21021;&#22987;&#20540;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;n&#27493;TD&#20013;&#30340;&#26368;&#20248;n&#20540;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SDPSA&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;n&#20540;&#30340;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of finding the optimal value of n in the n-step temporal difference (TD) algorithm. We find the optimal n by resorting to the model-free optimization technique of simultaneous perturbation stochastic approximation (SPSA). We adopt a one-simulation SPSA procedure that is originally for continuous optimization to the discrete optimization framework but incorporates a cyclic perturbation sequence. We prove the convergence of our proposed algorithm, SDPSA, and show that it finds the optimal value of n in n-step TD. Through experiments, we show that the optimal value of n is achieved with SDPSA for any arbitrary initial value of the same.
&lt;/p&gt;</description></item><item><title>MCTS-GEB&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#37325;&#20889;&#31995;&#32479;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#26500;&#24314;&#26368;&#20248;&#30340;E&#22270;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;E&#22270;&#26500;&#24314;&#20013;&#30340;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04651</link><description>&lt;p&gt;
MCTS-GEB&#65306;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26159;&#19968;&#20010;&#22909;&#30340;E&#22270;&#26500;&#24314;&#22120;
&lt;/p&gt;
&lt;p&gt;
MCTS-GEB: Monte Carlo Tree Search is a Good E-graph Builder. (arXiv:2303.04651v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04651
&lt;/p&gt;
&lt;p&gt;
MCTS-GEB&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#37325;&#20889;&#31995;&#32479;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#26469;&#26500;&#24314;&#26368;&#20248;&#30340;E&#22270;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;E&#22270;&#26500;&#24314;&#20013;&#30340;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#20889;&#31995;&#32479;&#24191;&#27867;&#20351;&#29992;&#31561;&#24335;&#39281;&#21644;&#25216;&#26415;&#26469;&#20248;&#21270;&#37325;&#20889;&#39034;&#24207;&#65292;&#20294;&#26159;&#24403;E&#22270;&#27809;&#26377;&#39281;&#21644;&#26102;&#65292;&#26080;&#27861;&#20195;&#34920;&#25152;&#26377;&#21487;&#33021;&#30340;&#37325;&#20889;&#26426;&#20250;&#65292;&#20250;&#37325;&#26032;&#24341;&#20837;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MCTS-GEB&#65292;&#19968;&#20010;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#20110;E&#22270;&#26500;&#24314;&#30340;&#36890;&#29992;&#37325;&#20889;&#31995;&#32479;&#12290;MCTS-GEB&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#39640;&#25928;&#35268;&#21010;&#26368;&#20248;&#30340;E&#22270;&#26500;&#24314;&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;E&#22270;&#26500;&#24314;&#38454;&#27573;&#30340;&#39034;&#24207;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#21512;&#29702;&#26102;&#38388;&#20869;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#65292;MCTS-GEB&#37117;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rewrite systems [6, 10, 12] have been widely employing equality saturation [9], which is an optimisation methodology that uses a saturated e-graph to represent all possible sequences of rewrite simultaneously, and then extracts the optimal one. As such, optimal results can be achieved by avoiding the phase-ordering problem. However, we observe that when the e-graph is not saturated, it cannot represent all possible rewrite opportunities and therefore the phase-ordering problem is re-introduced during the construction phase of the e-graph. To address this problem, we propose MCTS-GEB, a domain-general rewrite system that applies reinforcement learning (RL) to e-graph construction. At its core, MCTS-GEB uses a Monte Carlo Tree Search (MCTS) [3] to efficiently plan for the optimal e-graph construction, and therefore it can effectively eliminate the phase-ordering problem at the construction phase and achieve better performance within a reasonable time. Evaluation in two different domains 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25104;&#21151;&#35774;&#32622;&#21518;&#38376;&#65292;&#22312;&#29616;&#26377;&#38450;&#24481;&#25514;&#26045;&#19979;&#20063;&#20855;&#26377;&#24378;&#22823;&#30340;&#25915;&#20987;&#24615;&#33021;&#21644;&#32784;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.03320</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35774;&#32622;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Learning to Backdoor Federated Learning. (arXiv:2303.03320v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25104;&#21151;&#35774;&#32622;&#21518;&#38376;&#65292;&#22312;&#29616;&#26377;&#38450;&#24481;&#25514;&#26045;&#19979;&#20063;&#20855;&#26377;&#24378;&#22823;&#30340;&#25915;&#20987;&#24615;&#33021;&#21644;&#32784;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31995;&#32479;&#20013;&#65292;&#24694;&#24847;&#21442;&#19982;&#32773;&#21487;&#20197;&#36731;&#26131;&#22320;&#23558;&#21518;&#38376;&#23884;&#20837;&#21040;&#32858;&#21512;&#27169;&#22411;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#22312;&#20027;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#36817;&#26399;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#25514;&#26045;&#65292;&#21253;&#25324;&#35757;&#32451;&#38454;&#27573;&#30340;&#22522;&#20110;&#32858;&#21512;&#30340;&#38450;&#24481;&#21644;&#21518;&#26399;&#38450;&#24481;&#25514;&#26045;&#12290;&#34429;&#28982;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#22312;&#29616;&#26377;&#30340;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;&#21518;&#38376;&#25915;&#20987;&#20013;&#21487;&#20197;&#33719;&#24471;&#21512;&#29702;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#38754;&#23545;&#26356;&#39640;&#32423;&#30340;&#25915;&#20987;&#26102;&#19981;&#36275;&#20197;&#24212;&#23545;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#21518;&#38376;&#25915;&#20987;&#26694;&#26550;&#65292;&#25915;&#20987;&#32773;&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;&#20854;&#26412;&#22320;&#25968;&#25454;&#21644;FL&#31995;&#32479;&#30340;&#20849;&#21516;&#30693;&#35782;&#24314;&#31435;&#30340;&#20223;&#30495;&#22120;&#35757;&#32451;&#19968;&#20010;&#65288;&#38750;&#36817;&#35270;&#65289;&#25915;&#20987;&#31574;&#30053;&#65292;&#28982;&#21518;&#22312;&#23454;&#38469;&#30340;FL&#35757;&#32451;&#20013;&#24212;&#29992;&#23427;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#26694;&#26550;&#26082;&#26159;&#36866;&#24212;&#24615;&#21448;&#26159;&#28789;&#27963;&#30340;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25514;&#26045;&#19979;&#20063;&#33021;&#23454;&#29616;&#24378;&#22823;&#30340;&#25915;&#20987;&#24615;&#33021;&#21644;&#32784;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a federated learning (FL) system, malicious participants can easily embed backdoors into the aggregated model while maintaining the model's performance on the main task. To this end, various defenses, including training stage aggregation-based defenses and post-training mitigation defenses, have been proposed recently. While these defenses obtain reasonable performance against existing backdoor attacks, which are mainly heuristics based, we show that they are insufficient in the face of more advanced attacks. In particular, we propose a general reinforcement learning-based backdoor attack framework where the attacker first trains a (non-myopic) attack policy using a simulator built upon its local data and common knowledge on the FL system, which is then applied during actual FL training. Our attack framework is both adaptive and flexible and achieves strong attack performance and durability even under state-of-the-art defenses.
&lt;/p&gt;</description></item><item><title>WISK&#26159;&#19968;&#31181;&#29992;&#20110;&#31354;&#38388;&#20851;&#38190;&#23383;&#26597;&#35810;&#30340;&#23398;&#20064;&#32034;&#24341;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#24050;&#30693;&#30340;&#26597;&#35810;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#26597;&#35810;&#25104;&#26412;&#65292;&#24182;&#22312;&#32771;&#34385;&#20102;&#31354;&#38388;&#23646;&#24615;&#21644;&#25991;&#26412;&#20449;&#24687;&#21518;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;WISK&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#32034;&#24341;&#26356;&#22909;&#30340;&#26597;&#35810;&#24310;&#36831;&#21644;&#31454;&#20105;&#24615;&#30340;&#32034;&#24341;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2302.14287</link><description>&lt;p&gt;
WISK&#65306;&#19968;&#31181;&#29992;&#20110;&#31354;&#38388;&#20851;&#38190;&#23383;&#26597;&#35810;&#30340;&#24037;&#20316;&#36127;&#36733;&#24863;&#30693;&#23398;&#20064;&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
WISK: A Workload-aware Learned Index for Spatial Keyword Queries. (arXiv:2302.14287v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14287
&lt;/p&gt;
&lt;p&gt;
WISK&#26159;&#19968;&#31181;&#29992;&#20110;&#31354;&#38388;&#20851;&#38190;&#23383;&#26597;&#35810;&#30340;&#23398;&#20064;&#32034;&#24341;&#65292;&#23427;&#21487;&#20197;&#21033;&#29992;&#24050;&#30693;&#30340;&#26597;&#35810;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#26597;&#35810;&#25104;&#26412;&#65292;&#24182;&#22312;&#32771;&#34385;&#20102;&#31354;&#38388;&#23646;&#24615;&#21644;&#25991;&#26412;&#20449;&#24687;&#21518;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;WISK&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#30340;&#32034;&#24341;&#26356;&#22909;&#30340;&#26597;&#35810;&#24310;&#36831;&#21644;&#31454;&#20105;&#24615;&#30340;&#32034;&#24341;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#23545;&#35937;&#24120;&#24120;&#20276;&#38543;&#30528;&#25991;&#26412;&#20449;&#24687;&#65292;&#20363;&#22914;&#21253;&#21547;&#23427;&#20204;&#25551;&#36848;&#30340;&#20852;&#36259;&#28857; (POIs)&#65292;&#36825;&#34987;&#31216;&#20026;&#22320;&#29702;-&#25991;&#26412;&#25968;&#25454;&#12290;&#20026;&#20102;&#26816;&#32034;&#36825;&#26679;&#30340;&#25968;&#25454;&#65292;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#21516;&#26102;&#32771;&#34385;&#31354;&#38388;&#25509;&#36817;&#24615;&#21644;&#25991;&#26412;&#20851;&#32852;&#24615;&#30340;&#31354;&#38388;&#20851;&#38190;&#23383;&#26597;&#35810;&#12290;&#20026;&#31354;&#38388;&#20851;&#38190;&#23383;&#26597;&#35810;&#35774;&#35745;&#30340;&#29616;&#26377;&#32034;&#24341;&#22823;&#22810;&#22522;&#20110;&#22320;&#29702;-&#25991;&#26412;&#25968;&#25454;&#26500;&#24314;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#24050;&#32463;&#25910;&#21040;&#30340;&#26597;&#35810;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#24050;&#30693;&#30340;&#26597;&#35810;&#20998;&#24067;&#21487;&#20197;&#25913;&#36827;&#26410;&#26469;&#26597;&#35810;&#22788;&#29702;&#30340;&#32034;&#24341;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; WISK&#65292;&#19968;&#31181;&#29992;&#20110;&#31354;&#38388;&#20851;&#38190;&#23383;&#26597;&#35810;&#30340;&#23398;&#20064;&#32034;&#24341;&#65292;&#23427;&#33258;&#36866;&#24212;&#20110;&#20248;&#21270;&#26597;&#35810;&#25104;&#26412;&#65292;&#32473;&#23450;&#19968;&#20010;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#22312;&#23398;&#20064;&#32034;&#24341;&#26102;&#21033;&#29992;&#32467;&#26500;&#21270;&#30340;&#31354;&#38388;&#23646;&#24615;&#21644;&#38750;&#32467;&#26500;&#21270;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#25968;&#25454;&#23545;&#35937;&#21010;&#20998;&#20026;&#20998;&#21306;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#32473;&#23450;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#30340;&#22788;&#29702;&#25104;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#24050;&#32463;&#25509;&#25910;&#21040;&#30340;&#26597;&#35810;&#30340;&#20998;&#24067;&#26469;&#23398;&#20064;&#31354;&#38388;&#20851;&#38190;&#23383;&#26597;&#35810;&#30340;&#26368;&#20248;&#21010;&#20998;&#26041;&#26696;&#21644;&#32034;&#24341;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#30340;&#22320;&#29702;-&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WISK&#22312;&#26597;&#35810;&#24310;&#36831;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#32034;&#24341;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#32034;&#24341;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial objects often come with textual information, such as Points of Interest (POIs) with their descriptions, which are referred to as geo-textual data. To retrieve such data, spatial keyword queries that take into account both spatial proximity and textual relevance have been extensively studied. Existing indexes designed for spatial keyword queries are mostly built based on the geo-textual data without considering the distribution of queries already received. However, previous studies have shown that utilizing the known query distribution can improve the index structure for future query processing. In this paper, we propose WISK, a learned index for spatial keyword queries, which self-adapts for optimizing querying costs given a query workload. One key challenge is how to utilize both structured spatial attributes and unstructured textual information during learning the index. We first divide the data objects into partitions, aiming to minimize the processing costs of the given que
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Inseq&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#25512;&#24191;&#21487;&#35299;&#37322;&#24615;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#23427;&#20026;&#24120;&#35265;&#30340;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#25552;&#20379;&#20102;&#25552;&#21462;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#30452;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;GPT-2&#20013;&#23637;&#31034;&#20102;Inseq&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.13942</link><description>&lt;p&gt;
Inseq&#65306;&#19968;&#20010;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Inseq: An Interpretability Toolkit for Sequence Generation Models. (arXiv:2302.13942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Inseq&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#25512;&#24191;&#21487;&#35299;&#37322;&#24615;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#23427;&#20026;&#24120;&#35265;&#30340;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#25552;&#20379;&#20102;&#25552;&#21462;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#30452;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;GPT-2&#20013;&#23637;&#31034;&#20102;Inseq&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36807;&#21435;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27969;&#34892;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#32780;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#19987;&#38376;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Inseq&#65292;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#20351;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#26222;&#21450;&#21270;&#12290;Inseq&#33021;&#22815;&#30452;&#35266;&#19988;&#20248;&#21270;&#22320;&#25552;&#21462;&#27969;&#34892;&#30340;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#30340;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#23427;&#26469;&#31361;&#20986;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24182;&#22312;GPT-2&#20013;&#23450;&#20301;&#20107;&#23454;&#30693;&#35782;&#12290;&#30001;&#20110;&#20854;&#25903;&#25345;&#23545;&#27604;&#29305;&#24449;&#24402;&#22240;&#31561;&#21069;&#27839;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#25509;&#21475;&#65292;&#22240;&#27492;Inseq&#21487;&#20197;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#65292;&#38598;&#20013;&#20248;&#33391;&#23454;&#36341;&#65292;&#24182;&#23454;&#29616;&#20844;&#27491;&#21644;&#21487;&#37325;&#22797;&#30340;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models' internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;TwERC&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Twitter&#24191;&#21578;&#25512;&#33616;&#30340;&#20505;&#36873;&#29983;&#25104;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#23454;&#26102;&#36731;&#22411;&#25490;&#21517;&#22120;&#21644;&#25968;&#25454;&#26469;&#28304;&#31574;&#30053;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#25910;&#20837;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#30456;&#20114;&#20316;&#29992;&#22270;&#21644;&#25490;&#21517;&#20998;&#25968;&#32531;&#23384;&#30340;&#20004;&#31181;&#31574;&#30053;&#20114;&#34917;&#24212;&#29992;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;4.08%&#21644;1.38%&#30340;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2302.13915</link><description>&lt;p&gt;
TwERC: Twitter&#24191;&#21578;&#25512;&#33616;&#39640;&#24615;&#33021;&#38598;&#25104;&#24335;&#20505;&#36873;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TwERC: High Performance Ensembled Candidate Generation for Ads Recommendation at Twitter. (arXiv:2302.13915v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TwERC&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Twitter&#24191;&#21578;&#25512;&#33616;&#30340;&#20505;&#36873;&#29983;&#25104;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#23454;&#26102;&#36731;&#22411;&#25490;&#21517;&#22120;&#21644;&#25968;&#25454;&#26469;&#28304;&#31574;&#30053;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#25910;&#20837;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#30456;&#20114;&#20316;&#29992;&#22270;&#21644;&#25490;&#21517;&#20998;&#25968;&#32531;&#23384;&#30340;&#20004;&#31181;&#31574;&#30053;&#20114;&#34917;&#24212;&#29992;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;4.08%&#21644;1.38%&#30340;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26159;&#31038;&#20132;&#23186;&#20307;&#20844;&#21496;&#30340;&#26680;&#24515;&#21151;&#33021;&#65292;&#21253;&#25324;&#26377;&#26426;&#21644;&#25512;&#24191;&#20869;&#23481;&#25512;&#33616;&#12290;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#36890;&#24120;&#20998;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#21363;&#20505;&#36873;&#29983;&#25104;&#21644;&#37325;&#25490;&#24207;&#65292;&#20197;&#24179;&#34913;&#35745;&#31639;&#25104;&#26412;&#21644;&#25512;&#33616;&#36136;&#37327;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#22823;&#35268;&#27169;&#24191;&#21578;&#25512;&#33616;&#38382;&#39064;&#20013;&#30340;&#20505;&#36873;&#29983;&#25104;&#38454;&#27573;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#20026;&#20808;&#30340;&#24322;&#26500;&#37325;&#26500;&#26041;&#27861;&#65292;&#31216;&#20026;TwERC&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#23558;&#23454;&#26102;&#36731;&#22411;&#25490;&#21517;&#22120;&#19982;&#33021;&#22815;&#25429;&#33719;&#39069;&#22806;&#20449;&#24687;&#30340;&#25968;&#25454;&#26469;&#28304;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#25552;&#20379;&#39564;&#35777;&#22686;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#12290;&#31532;&#19968;&#20010;&#31574;&#30053;&#20351;&#29992;&#30456;&#20114;&#20316;&#29992;&#22270;&#20013;&#30340;&#30456;&#20284;&#24615;&#27010;&#24565;&#65292;&#32780;&#31532;&#20108;&#20010;&#31574;&#30053;&#32531;&#23384;&#25490;&#21517;&#38454;&#27573;&#30340;&#20808;&#21069;&#24471;&#20998;&#12290;&#22522;&#20110;&#22270;&#24418;&#30340;&#31574;&#30053;&#23454;&#29616;&#20102;4.08%&#30340;&#25910;&#20837;&#22686;&#30410;&#65292;&#32780;&#22522;&#20110;&#25490;&#21517;&#20998;&#25968;&#30340;&#31574;&#30053;&#23454;&#29616;&#20102;1.38%&#30340;&#22686;&#30410;&#12290;&#36825;&#20004;&#31181;&#31574;&#30053;&#20855;&#26377;&#20114;&#34917;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems are a core feature of social media companies with their uses including recommending organic and promoted contents. Many modern recommendation systems are split into multiple stages - candidate generation and heavy ranking - to balance computational cost against recommendation quality. We focus on the candidate generation phase of a large-scale ads recommendation problem in this paper, and present a machine learning first heterogeneous re-architecture of this stage which we term TwERC. We show that a system that combines a real-time light ranker with sourcing strategies capable of capturing additional information provides validated gains. We present two strategies. The first strategy uses a notion of similarity in the interaction graph, while the second strategy caches previous scores from the ranking stage. The graph based strategy achieves a 4.08% revenue gain and the rankscore based strategy achieves a 1.38% gain. These two strategies have biases that complemen
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38463;&#27931;&#26222;&#22827;&#27861;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#20102;&#26469;&#33258;10,368&#21517;&#23398;&#29983;&#30340;29,349&#20010;&#38382;&#39064;&#21644;&#35299;&#37322;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.07738</link><description>&lt;p&gt;
Alloprof&#65306;&#19968;&#20010;&#26032;&#30340;&#27861;&#35821;&#38382;&#31572;&#25945;&#32946;&#25968;&#25454;&#38598;&#21450;&#20854;&#22312;&#20449;&#24687;&#26816;&#32034;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Alloprof: a new French question-answer education dataset and its use in an information retrieval case study. (arXiv:2302.07738v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07738
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38463;&#27931;&#26222;&#22827;&#27861;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#20102;&#26469;&#33258;10,368&#21517;&#23398;&#29983;&#30340;29,349&#20010;&#38382;&#39064;&#21644;&#35299;&#37322;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#24072;&#21644;&#23398;&#29983;&#36234;&#26469;&#36234;&#20381;&#36182;&#22312;&#32447;&#23398;&#20064;&#36164;&#28304;&#26469;&#34917;&#20805;&#23398;&#26657;&#25552;&#20379;&#30340;&#36164;&#28304;&#12290;&#21487;&#29992;&#36164;&#28304;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#30340;&#22686;&#21152;&#23545;&#23398;&#29983;&#26469;&#35828;&#26159;&#19968;&#20214;&#22909;&#20107;&#65292;&#20294;&#21069;&#25552;&#26159;&#20182;&#20204;&#33021;&#22815;&#25214;&#21040;&#31572;&#26696;&#12290;&#38382;&#31572;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#24050;&#21463;&#30410;&#20110;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#20197;&#35757;&#32451;&#21644;&#35780;&#20272;&#20854;&#31639;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#36825;&#20123;&#25968;&#25454;&#38598;&#37117;&#26159;&#33521;&#25991;&#25991;&#26412;&#65292;&#30001;&#25104;&#24180;&#20154;&#32534;&#20889;&#21644;&#38405;&#35835;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#20849;&#27861;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20174;&#24635;&#37096;&#20301;&#20110;&#39745;&#21271;&#20811;&#30340;&#23567;&#23398;&#21644;&#20013;&#23398;&#24110;&#21161;&#32593;&#31449;Alloprof&#25910;&#38598;&#65292;&#21253;&#21547;29,349&#20010;&#38382;&#39064;&#21450;&#20854;&#35299;&#37322;&#65292;&#28085;&#30422;&#21508;&#31181;&#23398;&#31185;&#30340;10,368&#21517;&#23398;&#29983;&#65292;&#36229;&#36807;&#19968;&#21322;&#30340;&#35299;&#37322;&#21253;&#21547;&#38142;&#25509;&#21040;&#20854;&#20182;&#38382;&#39064;&#25110;&#32593;&#31449;&#19978;&#30340;2,596&#20010;&#21442;&#32771;&#39029;&#38754;&#20043;&#19968;&#12290;&#25105;&#20204;&#36824;&#21521;&#24744;&#23637;&#31034;&#20102;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#20351;&#29992;&#26412;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teachers and students are increasingly relying on online learning resources to supplement the ones provided in school. This increase in the breadth and depth of available resources is a great thing for students, but only provided they are able to find answers to their queries. Question-answering and information retrieval systems have benefited from public datasets to train and evaluate their algorithms, but most of these datasets have been in English text written by and for adults. We introduce a new public French question-answering dataset collected from Alloprof, a Quebec-based primary and high-school help website, containing 29 349 questions and their explanations in a variety of school subjects from 10 368 students, with more than half of the explanations containing links to other questions or some of the 2 596 reference pages on the website. We also present a case study of this dataset in an information retrieval task. This dataset was collected on the Alloprof public forum, with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.05743</link><description>&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20165;&#20381;&#38752;&#36317;&#31163;&#30697;&#38453;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24120;&#29992;&#20110;&#28041;&#21450;&#22270;&#24418;&#20960;&#20309;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#34429;&#28982;&#20960;&#20309;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#21253;&#21547;&#23436;&#25972;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#26080;&#27861;&#23398;&#20064;&#36825;&#31181;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#26032;&#39062;&#30340;&#23545;&#31216;&#20960;&#20309;&#22270;&#30340;&#23478;&#26063;&#65292;&#25193;&#23637;&#20102;MPNN&#26080;&#27861;&#21306;&#20998;&#20854;&#36317;&#31163;&#30697;&#38453;&#30340;&#21453;&#20363;&#23478;&#26063;&#65292;&#24182;&#25552;&#20986;$k$-DisGNNs&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#20960;&#20309;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;$k$-DisGNNs&#30340;&#29305;&#27530;&#24773;&#20917;&#32479;&#19968;&#36215;&#26469;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23637;&#31034;&#20102;&#37027;&#20123;&#26368;&#21021;&#20026;&#20302;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;GNN&#27169;&#22411;&#35774;&#35745;&#30340;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#24314;&#27169;&#65292;&#24320;&#21457;&#19968;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#33258;&#20027;&#36866;&#24212;&#25915;&#20987;&#32773;&#65292;&#22312;&#38408;&#20540;&#29305;&#24615;&#35777;&#26126;&#30340;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992; Threshold Fictitious Self-Play (T-FP) &#31639;&#27861;&#23398;&#20064;&#32435;&#20160;&#22343;&#34913;&#65292;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2301.06085</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#24577;&#25915;&#20987;&#19979;&#30340;&#36817;&#20284;&#26368;&#20248;&#20837;&#20405;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Learning Near-Optimal Intrusion Responses Against Dynamic Attackers. (arXiv:2301.06085v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#24314;&#27169;&#65292;&#24320;&#21457;&#19968;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#33258;&#20027;&#36866;&#24212;&#25915;&#20987;&#32773;&#65292;&#22312;&#38408;&#20540;&#29305;&#24615;&#35777;&#26126;&#30340;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992; Threshold Fictitious Self-Play (T-FP) &#31639;&#27861;&#23398;&#20064;&#32435;&#20160;&#22343;&#34913;&#65292;&#23454;&#29616;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33258;&#21160;&#21270;&#20837;&#20405;&#21709;&#24212;&#65292;&#24182;&#23558;&#25915;&#20987;&#32773;&#19982;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#24418;&#24335;&#24314;&#27169;&#20026;&#19968;&#31181;&#26368;&#20248;&#20572;&#27490;&#21338;&#24328;&#65292;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#33258;&#25105;&#21338;&#24328;&#21457;&#23637;&#12290;&#21338;&#24328;&#35770;&#24314;&#27169;&#20351;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#38024;&#23545;&#21160;&#24577;&#25915;&#20987;&#32773;&#26377;&#25928;&#30340;&#38450;&#24481;&#32773;&#31574;&#30053;&#65292;&#21363;&#25915;&#20987;&#32773;&#26681;&#25454;&#38450;&#24481;&#32773;&#31574;&#30053;&#36827;&#34892;&#21453;&#24212;&#26469;&#33258;&#36866;&#24212;&#30340;&#25915;&#20987;&#32773;&#12290;&#27492;&#22806;&#65292;&#26368;&#20248;&#20572;&#27490;&#20844;&#24335;&#20351;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;&#26368;&#20248;&#31574;&#30053;&#20855;&#26377;&#38408;&#20540;&#29305;&#24615;&#12290;&#20026;&#20102;&#33719;&#24471;&#36817;&#20284;&#26368;&#20248;&#30340;&#38450;&#24481;&#32773;&#31574;&#30053;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102; Threshold Fictitious Self-Play (T-FP)&#65292;&#36825;&#26159;&#19968;&#31181;&#34394;&#26500;&#33258;&#25105;&#21338;&#24328;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#36924;&#36817;&#23398;&#20064;&#32435;&#20160;&#22343;&#34913; &#12290;&#25105;&#20204;&#35777;&#26126;&#20102; T-FP &#22312;&#25105;&#20204;&#30340;&#29992;&#20363;&#20013;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;&#35813;&#30740;&#31350;&#30340;&#23454;&#39564;&#37096;&#20998;&#21253;&#25324;&#20004;&#20010;&#31995;&#32479;&#65306;&#19968;&#20010;&#27169;&#25311;&#31995;&#32479;&#65292;&#20854;&#20013;&#38450;&#24481;&#31574;&#30053;&#26159;&#36880;&#27493;&#23398;&#20064;&#30340;&#65292;&#20197;&#21450;&#19968;&#20010;&#20223;&#30495;&#31995;&#32479;&#65292;&#25910;&#38598;&#20102;&#39537;&#21160;&#27169;&#25311;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study automated intrusion response and formulate the interaction between an attacker and a defender as an optimal stopping game where attack and defense strategies evolve through reinforcement learning and self-play. The game-theoretic modeling enables us to find defender strategies that are effective against a dynamic attacker, i.e. an attacker that adapts its strategy in response to the defender strategy. Further, the optimal stopping formulation allows us to prove that optimal strategies have threshold properties. To obtain near-optimal defender strategies, we develop Threshold Fictitious Self-Play (T-FP), a fictitious self-play algorithm that learns Nash equilibria through stochastic approximation. We show that T-FP outperforms a state-of-the-art algorithm for our use case. The experimental part of this investigation includes two systems: a simulation system where defender strategies are incrementally learned and an emulation system where statistics are collected that drive simu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;TSViT&#65292;&#29992;&#20110;&#22788;&#29702;&#36890;&#29992;&#30340;&#21355;&#26143;&#22270;&#20687;&#26102;&#24207;(SITS)&#65292;&#36890;&#36807;&#20998;&#35299;&#30340;&#26102;&#31354;&#32534;&#30721;&#22120;&#22788;&#29702;&#38750;&#37325;&#21472;&#30340;&#22359;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26426;&#21046;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#26368;&#32456;&#22312;SITS&#35821;&#20041;&#20998;&#21106;&#21644;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2301.04944</link><description>&lt;p&gt;
ViTs for SITS&#65306;&#22522;&#20110;Vision Transformer&#30340;&#21355;&#26143;&#22270;&#20687;&#26102;&#24207;&#22788;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ViTs for SITS: Vision Transformers for Satellite Image Time Series. (arXiv:2301.04944v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;TSViT&#65292;&#29992;&#20110;&#22788;&#29702;&#36890;&#29992;&#30340;&#21355;&#26143;&#22270;&#20687;&#26102;&#24207;(SITS)&#65292;&#36890;&#36807;&#20998;&#35299;&#30340;&#26102;&#31354;&#32534;&#30721;&#22120;&#22788;&#29702;&#38750;&#37325;&#21472;&#30340;&#22359;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#26426;&#21046;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#26368;&#32456;&#22312;SITS&#35821;&#20041;&#20998;&#21106;&#21644;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#8212;&#8212;Temporo-Spatial Vision Transformer&#65288;TSViT&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#36890;&#29992;&#30340;&#21355;&#26143;&#22270;&#20687;&#26102;&#24207;(SITS)&#65292;&#24182;&#23558;&#20854;&#22522;&#20110;Vision Transformer&#65288;ViT&#65289;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;TSViT&#23558;SITS&#35760;&#24405;&#22312;&#26102;&#31354;&#19978;&#21010;&#20998;&#25104;&#38750;&#37325;&#21472;&#30340;&#22359;&#65292;&#36825;&#20123;&#22359;&#34987;&#35760;&#21495;&#21270;&#21518;&#30001;&#20998;&#35299;&#30340;&#26102;&#31354;&#32534;&#30721;&#22120;&#22788;&#29702;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#30456;&#27604;&#33258;&#28982;&#22270;&#20687;&#65292;&#26102;&#22495;&#20248;&#20808;&#31354;&#38388;&#27425;&#20043;&#30340;&#20998;&#35299;&#23545;&#20110;SITS&#22788;&#29702;&#26356;&#21152;&#30452;&#35266;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#21363;&#33719;&#21462;&#26102;&#38388;&#29305;&#23450;&#30340;&#32534;&#30721;&#21644;&#22810;&#20010;&#21487;&#23398;&#20064;&#31867;&#26631;&#35760;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;&#25152;&#26377;&#26032;&#35774;&#35745;&#30340;&#25928;&#26524;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#22312;&#19977;&#20010;&#20844;&#24320;&#30340;SITS&#35821;&#20041;&#20998;&#21106;&#21644;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we introduce the Temporo-Spatial Vision Transformer (TSViT), a fully-attentional model for general Satellite Image Time Series (SITS) processing based on the Vision Transformer (ViT). TSViT splits a SITS record into non-overlapping patches in space and time which are tokenized and subsequently processed by a factorized temporo-spatial encoder. We argue, that in contrast to natural images, a temporal-then-spatial factorization is more intuitive for SITS processing and present experimental evidence for this claim. Additionally, we enhance the model's discriminative power by introducing two novel mechanisms for acquisition-time-specific temporal positional encodings and multiple learnable class tokens. The effect of all novel design choices is evaluated through an extensive ablation study. Our proposed architecture achieves state-of-the-art performance, surpassing previous approaches by a significant margin in three publicly available SITS semantic segmentation and classific
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;Cox-Weibull&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#27494;&#22120;&#31995;&#32479;&#21487;&#38752;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#27494;&#22120;&#31995;&#32479;&#29305;&#24449;&#26469;&#25913;&#21892;&#39044;&#27979;&#24615;&#32500;&#20462;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#27169;&#22411;&#30340;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2301.01850</link><description>&lt;p&gt;
&#24102;&#26377;Cox-Weibull&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#27494;&#22120;&#31995;&#32479;&#21487;&#38752;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Bayesian Weapon System Reliability Modeling with Cox-Weibull Neural Network. (arXiv:2301.01850v5 [stat.AP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;Cox-Weibull&#31070;&#32463;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#27494;&#22120;&#31995;&#32479;&#21487;&#38752;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25972;&#21512;&#27494;&#22120;&#31995;&#32479;&#29305;&#24449;&#26469;&#25913;&#21892;&#39044;&#27979;&#24615;&#32500;&#20462;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#27169;&#22411;&#30340;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#65288;&#20363;&#22914;DeepSurv&#65289;&#23558;&#27494;&#22120;&#31995;&#32479;&#29305;&#24449;&#65288;&#22914;&#27494;&#22120;&#31995;&#32479;&#21046;&#36896;&#21830;&#12289;&#37096;&#32626;&#26102;&#38388;&#21644;&#22320;&#28857;&#12289;&#23384;&#20648;&#26102;&#38388;&#21644;&#22320;&#28857;&#31561;&#65289;&#25972;&#21512;&#21040;&#21442;&#25968;&#21270;&#30340;Cox-Weibull&#21487;&#38752;&#24615;&#27169;&#22411;&#20013;&#65292;&#20197;&#25913;&#21892;&#39044;&#27979;&#24615;&#32500;&#20462;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;MC-dropout&#31561;&#36864;&#28779;&#26041;&#27861;&#26469;&#23558;Weibull&#21442;&#25968;&#21442;&#25968;&#21270;&#24182;&#24320;&#21457;&#20102;&#21478;&#19968;&#31181;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#20197;&#36827;&#34892;&#27604;&#36739;&#30446;&#30340;&#12290;&#22240;&#20026;&#27494;&#22120;&#31995;&#32479;&#27979;&#35797;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#31243;&#24207;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21306;&#38388;&#34987;&#23457;&#26597;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#65292;&#23427;&#22312;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#26399;&#38388;&#21512;&#24182;&#20102;Weibull&#21442;&#25968;&#30340;MCMC&#21462;&#26679;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20998;&#31867;&#25351;&#26631;&#65292;&#22914;&#25509;&#25910;&#22120;&#24037;&#20316;&#29305;&#24615;&#65288;ROC&#65289;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#12289;&#31934;&#24230;-&#21484;&#22238;&#65288;PR&#65289;AUC&#21644;F&#20998;&#25968;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#24120;&#20248;&#20110;&#20256;&#32479;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#22914;XGBoost&#21644;&#24403;&#21069;&#26631;&#20934;&#30340;&#26465;&#20214;Weibull&#27010;&#29575;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose to integrate weapon system features (such as weapon system manufacturer, deployment time and location, storage time and location, etc.) into a parameterized Cox-Weibull [1] reliability model via a neural network, like DeepSurv [2], to improve predictive maintenance. In parallel, we develop an alternative Bayesian model by parameterizing the Weibull parameters with a neural network and employing dropout methods such as Monte-Carlo (MC)-dropout for comparative purposes. Due to data collection procedures in weapon system testing we employ a novel interval-censored log-likelihood which incorporates Monte-Carlo Markov Chain (MCMC) [3] sampling of the Weibull parameters during gradient descent optimization. We compare classification metrics such as receiver operator curve (ROC) area under the curve (AUC), precision-recall (PR) AUC, and F scores to show our model generally outperforms traditional powerful models such as XGBoost and the current standard conditional Weibull probabili
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#24341;&#23548;&#30340;&#26102;&#31354;&#25968;&#25454;&#24314;&#27169;&#65292;&#21487;&#20197;&#36827;&#34892;&#22240;&#26524;&#26410;&#26469;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.00736</link><description>&lt;p&gt;
&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#24341;&#23548;&#30340;&#26102;&#31354;&#25968;&#25454;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixed moving average field guided learning for spatio-temporal data. (arXiv:2301.00736v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#24341;&#23548;&#30340;&#26102;&#31354;&#25968;&#25454;&#24314;&#27169;&#65292;&#21487;&#20197;&#36827;&#34892;&#22240;&#26524;&#26410;&#26469;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#28151;&#21512;&#31227;&#21160;&#24179;&#22343;&#22330;&#30340;&#24433;&#21709;&#65292;&#26102;&#31354;&#25968;&#25454;&#30340;&#24314;&#27169;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#25216;&#24039;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#30340;&#39044;&#27979;&#20998;&#24067;&#36890;&#24120;&#19981;&#21487;&#35775;&#38382;&#12290;&#22312;&#36825;&#20010;&#24314;&#27169;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#24341;&#23548;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#24191;&#20041;&#36125;&#21494;&#26031;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#37319;&#29992;Lipschitz&#39044;&#27979;&#22120;&#65288;&#20363;&#22914;&#32447;&#24615;&#27169;&#22411;&#25110;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#27839;&#31354;&#38388;&#21644;&#26102;&#38388;&#32500;&#24230;&#20018;&#34892;&#30456;&#20851;&#30340;&#25968;&#25454;&#30340;&#26032;&#22411;PAC&#36125;&#21494;&#26031;&#30028;&#38480;&#26469;&#30830;&#23450;&#19968;&#20010;&#38543;&#26426;&#20272;&#35745;&#20540;&#12290;&#36827;&#34892;&#22240;&#26524;&#26410;&#26469;&#39044;&#27979;&#26159;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20142;&#28857;&#65292;&#22240;&#20026;&#23427;&#36866;&#29992;&#20110;&#20855;&#26377;&#30701;&#26399;&#21644;&#38271;&#26399;&#30456;&#20851;&#24615;&#30340;&#25968;&#25454;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#32447;&#24615;&#39044;&#27979;&#22120;&#21644;&#27169;&#25311;STOU&#36807;&#31243;&#30340;&#26102;&#31354;&#25968;&#25454;&#30340;&#31034;&#20363;&#26469;&#23637;&#31034;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Influenced mixed moving average fields are a versatile modeling class for spatio-temporal data. However, their predictive distribution is not generally accessible. Under this modeling assumption, we define a novel theory-guided machine learning approach that employs a generalized Bayesian algorithm to make predictions. We employ a Lipschitz predictor, for example, a linear model or a feed-forward neural network, and determine a randomized estimator by minimizing a novel PAC Bayesian bound for data serially correlated along a spatial and temporal dimension. Performing causal future predictions is a highlight of our methodology as its potential application to data with short and long-range dependence. We conclude by showing the performance of the learning methodology in an example with linear predictors and simulated spatio-temporal data from an STOU process.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#27979;&#37327;&#27963;&#21160;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#36710;&#36742;&#21644;&#26080;&#32447;&#30005;&#35775;&#38382;&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#21508;&#31181;V2X&#30740;&#31350;&#20026;&#36710;&#36742;&#21644;&#24037;&#19994;&#36890;&#20449;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.10343</link><description>&lt;p&gt;
Berlin V2X&#65306;&#22810;&#36710;&#36742;&#21644;&#26080;&#32447;&#30005;&#35775;&#38382;&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Berlin V2X: A Machine Learning Dataset from Multiple Vehicles and Radio Access Technologies. (arXiv:2212.10343v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10343
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#27979;&#37327;&#27963;&#21160;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#36710;&#36742;&#21644;&#26080;&#32447;&#30005;&#35775;&#38382;&#25216;&#26415;&#30340;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#21508;&#31181;V2X&#30740;&#31350;&#20026;&#36710;&#36742;&#21644;&#24037;&#19994;&#36890;&#20449;&#39046;&#22495;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#36890;&#20449;&#30340;&#21457;&#23637;&#24050;&#34987;&#39044;&#26399;&#20381;&#38752;&#26032;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#20197;&#20351;&#26080;&#32447;&#32593;&#32476;&#32452;&#20214;&#37319;&#21462;&#20027;&#21160;&#20915;&#31574;&#21644;&#34892;&#21160;&#65292;&#20197;&#32500;&#25345;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;&#27492;&#22806;&#65292;&#23558;&#20986;&#29616;&#22312;&#36710;&#36742;&#21644;&#24037;&#19994;&#36890;&#20449;&#39046;&#22495;&#30340;&#26032;&#29992;&#20363;&#12290;&#20855;&#20307;&#22312;&#36710;&#36742;&#36890;&#20449;&#39046;&#22495;&#65292;&#36710;&#36742;&#23545;&#19968;&#20999;&#65288;V2X&#65289;&#26041;&#26696;&#23558;&#20250;&#20174;&#36825;&#20123;&#36827;&#23637;&#20013;&#21463;&#30410;&#21290;&#27973;&#12290;&#32771;&#34385;&#21040;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#27979;&#37327;&#27963;&#21160;&#65292;&#20026;&#21508;&#31181;&#22810;&#26679;&#30340;&#22522;&#20110;ML&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25152;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#22312;&#19981;&#21516;&#22478;&#24066;&#29615;&#22659;&#19979;&#22522;&#31449;&#65288;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#36816;&#33829;&#21830;&#65289;&#21644;&#37051;&#36817;&#38142;&#36335;&#26080;&#32447;&#30005;&#35775;&#38382;&#25216;&#26415;&#30340;GPS&#23450;&#20301;&#26080;&#32447;&#27979;&#37327;&#65292;&#22240;&#27492;&#21487;&#20197;&#36827;&#34892;&#21508;&#31181;&#19981;&#21516;&#30340;V2X&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#26631;&#26377;&#26631;&#31614;&#65292;&#24182;&#20197;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#38468;&#26377;&#25152;&#38656;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of wireless communications into 6G and beyond is expected to rely on new machine learning (ML)-based capabilities. These can enable proactive decisions and actions from wireless-network components to sustain quality-of-service (QoS) and user experience. Moreover, new use cases in the area of vehicular and industrial communications will emerge. Specifically in the area of vehicle communication, vehicle-to-everything (V2X) schemes will benefit strongly from such advances. With this in mind, we have conducted a detailed measurement campaign that paves the way to a plethora of diverse ML-based studies. The resulting datasets offer GPS-located wireless measurements across diverse urban environments for both cellular (with two different operators) and sidelink radio access technologies, thus enabling a variety of different studies towards V2X. The datasets are labeled and sampled with a high time resolution. Furthermore, we make the data publicly available with all the necessar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#20351;&#29992;AUC&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#26368;&#22823;&#21270;AUC&#20998;&#25968;&#30340;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#36164;&#28304;NER&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2212.04800</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;AUC&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
AUC Maximization for Low-Resource Named Entity Recognition. (arXiv:2212.04800v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#20351;&#29992;AUC&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#26368;&#22823;&#21270;AUC&#20998;&#25968;&#30340;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#36164;&#28304;NER&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#30340;&#24037;&#20316;&#20351;&#29992;&#20132;&#21449;&#29109;&#65288;CE&#65289;&#25110;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRF&#65289;&#20316;&#20026;&#20248;&#21270;NER&#27169;&#22411;&#30340;&#30446;&#26631;/&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#20256;&#32479;&#30340;NER&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#36890;&#24120;&#22312;&#25968;&#25454;&#20998;&#24067;&#24179;&#34913;&#65292;&#24182;&#19988;&#26377;&#36275;&#22815;&#30340;&#27880;&#37322;&#35757;&#32451;&#26679;&#20363;&#26102;&#21487;&#20197;&#20135;&#29983;&#36275;&#22815;&#30340;&#24615;&#33021;&#12290;&#20294;&#30001;&#20110;NER&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#19981;&#24179;&#34913;&#30340;&#26631;&#35760;&#38382;&#39064;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#36825;&#20123;&#26631;&#20934;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#22522;&#20110;&#26368;&#22823;&#21270;ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26368;&#22823;&#21270;AUC&#20998;&#25968;&#26469;&#20248;&#21270;NER&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#32467;&#21512;&#20004;&#20010;&#26368;&#22823;&#21270;AUC&#20998;&#25968;&#30340;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#36164;&#28304;NER&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current work in named entity recognition (NER) uses either cross entropy (CE) or conditional random fields (CRF) as the objective/loss functions to optimize the underlying NER model. Both of these traditional objective functions for the NER problem generally produce adequate performance when the data distribution is balanced and there are sufficient annotated training examples. But since NER is inherently an imbalanced tagging problem, the model performance under the low-resource settings could suffer using these standard objective functions. Based on recent advances in area under the ROC curve (AUC) maximization, we propose to optimize the NER model by maximizing the AUC score. We give evidence that by simply combining two binary-classifiers that maximize the AUC score, significant performance improvement over traditional loss functions is achieved under low-resource NER settings. We also conduct extensive experiments to demonstrate the advantages of our method under the low-resource 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2212.03932</link><description>&lt;p&gt;
&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#26041;&#27861;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#34892;&#20026;&#31574;&#30053;&#31163;&#32447;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Low Variance Off-policy Evaluation with State-based Importance Sampling. (arXiv:2212.03932v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#32447;&#35780;&#20272;&#20013;&#65292;&#38656;&#35201;&#35780;&#20272;&#30446;&#26631;&#31574;&#30053;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#38656;&#35201;&#20351;&#29992;&#30001;&#34892;&#20026;&#31574;&#30053;&#37319;&#38598;&#30340;&#26679;&#26412;&#25968;&#25454;&#12290;&#20256;&#32479;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#26041;&#27861;&#30001;&#20110;&#35745;&#31639;&#21160;&#20316;&#27010;&#29575;&#27604;&#20540;&#30340;&#20056;&#31215;&#32780;&#23548;&#33268;&#26041;&#24046;&#22686;&#21152;&#65292;&#20174;&#32780;&#22312;&#28041;&#21450;&#38271;&#26399;&#35268;&#21010;&#30340;&#20219;&#21153;&#20013;&#20986;&#29616;&#20272;&#35745;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#25277;&#26679;&#65288;SIS&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#27979;&#8220;&#24573;&#30053;&#29366;&#24577;&#8221;&#30340;&#23376;&#36712;&#36857;&#26469;&#23454;&#29616;&#20302;&#26041;&#24046;&#30340;&#31163;&#32447;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In off-policy reinforcement learning, a behaviour policy performs exploratory interactions with the environment to obtain state-action-reward samples which are then used to learn a target policy that optimises the expected return. This leads to a problem of off-policy evaluation, where one needs to evaluate the target policy from samples collected by the often unrelated behaviour policy. Importance sampling is a traditional statistical technique that is often applied to off-policy evaluation. While importance sampling estimators are unbiased, their variance increases exponentially with the horizon of the decision process due to computing the importance weight as a product of action probability ratios, yielding estimates with low accuracy for domains involving long-term planning. This paper proposes state-based importance sampling (SIS), which drops the action probability ratios of sub-trajectories with "negligible states" -- roughly speaking, those for which the chosen actions have no 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FaiREE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2211.15072</link><description>&lt;p&gt;
FaiREE&#65306;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#20445;&#35777;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FaiREE: Fair Classification with Finite-Sample and Distribution-Free Guarantee. (arXiv:2211.15072v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FaiREE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20844;&#24179;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#21644;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#30340;&#20844;&#24179;&#20445;&#35777;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#26679;&#26412;&#37327;&#65292;&#24182;&#19988;&#22312;&#26679;&#26412;&#37327;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#36829;&#21453;&#20844;&#24179;&#24615;&#65292;&#32780;&#36825;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FaiREE&#31639;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#20844;&#24179;&#20998;&#31867;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#26679;&#26412;&#21644;&#26080;&#20998;&#24067;&#29702;&#35770;&#20445;&#35777;&#19979;&#28385;&#36275;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;FaiREE&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#32676;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#26426;&#20250;&#24179;&#31561;&#65292;&#24179;&#34913;&#20960;&#29575;&#65292;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#34913;&#31561;&#65289;&#24182;&#23454;&#29616;&#26368;&#20339;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#29702;&#35770;&#20445;&#35777;&#36827;&#19968;&#27493;&#24471;&#21040;&#20102;&#23545;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#23454;&#39564;&#25903;&#25345;&#12290;FaiREE&#34920;&#29616;&#20986;&#27604;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic fairness plays an increasingly critical role in machine learning research. Several group fairness notions and algorithms have been proposed. However, the fairness guarantee of existing fair classification methods mainly depends on specific data distributional assumptions, often requiring large sample sizes, and fairness could be violated when there is a modest number of samples, which is often the case in practice. In this paper, we propose FaiREE, a fair classification algorithm that can satisfy group fairness constraints with finite-sample and distribution-free theoretical guarantees. FaiREE can be adapted to satisfy various group fairness notions (e.g., Equality of Opportunity, Equalized Odds, Demographic Parity, etc.) and achieve the optimal accuracy. These theoretical guarantees are further supported by experiments on both synthetic and real data. FaiREE is shown to have favorable performance over state-of-the-art algorithms.
&lt;/p&gt;</description></item><item><title>&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#36890;&#36807;&#30417;&#27979;&#26465;&#20214;&#24615;&#34920;&#29616;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#30417;&#27979;&#32771;&#34385;&#28151;&#28102;&#21307;&#30103;&#24178;&#39044;&#26102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39118;&#38505;&#39044;&#27979;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.09781</link><description>&lt;p&gt;
&#30417;&#27979;&#32771;&#34385;&#28151;&#28102;&#21307;&#30103;&#24178;&#39044;&#26102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39118;&#38505;&#39044;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Monitoring machine learning (ML)-based risk prediction algorithms in the presence of confounding medical interventions. (arXiv:2211.09781v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09781
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#36890;&#36807;&#30417;&#27979;&#26465;&#20214;&#24615;&#34920;&#29616;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#30417;&#27979;&#32771;&#34385;&#28151;&#28102;&#21307;&#30103;&#24178;&#39044;&#26102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39118;&#38505;&#39044;&#27979;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#30417;&#27979;&#21463;&#28151;&#28102;&#21307;&#30103;&#24178;&#39044;&#65288;CMI&#65289;&#30340;&#24433;&#21709;&#65292;&#22240;&#20026;&#20020;&#24202;&#21307;&#29983;&#26356;&#21487;&#33021;&#20026;&#39640;&#39118;&#38505;&#24739;&#32773;&#25552;&#20379;&#39044;&#38450;&#24615;&#27835;&#30103;&#24182;&#25913;&#21464;&#31639;&#27861;&#25152;&#39044;&#27979;&#30340;&#30446;&#26631;&#12290;&#24573;&#30053;CMI&#24182;&#21482;&#30417;&#27979;&#26410;&#25509;&#21463;&#27835;&#30103;&#30340;&#24739;&#32773;&#21487;&#33021;&#20250;&#23548;&#33268;&#31867;&#22411;I&#38169;&#35823;&#30340;&#33192;&#32960;&#65292;&#19981;&#36807;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#30417;&#25511;&#26465;&#20214;&#24615;&#34920;&#29616;&#65292;&#24182;&#19988;&#31526;&#21512;&#26465;&#20214;&#20132;&#25442;&#24615;&#25110;&#26102;&#38388;&#19981;&#21464;&#36873;&#25321;&#20559;&#24046;&#65292;&#21017;&#20173;&#28982;&#21487;&#20197;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance monitoring of machine learning (ML)-based risk prediction models in healthcare is complicated by the issue of confounding medical interventions (CMI): when an algorithm predicts a patient to be at high risk for an adverse event, clinicians are more likely to administer prophylactic treatment and alter the very target that the algorithm aims to predict. A simple approach is to ignore CMI and monitor only the untreated patients, whose outcomes remain unaltered. In general, ignoring CMI may inflate Type I error because (i) untreated patients disproportionally represent those with low predicted risk and (ii) evolution in both the model and clinician trust in the model can induce complex dependencies that violate standard assumptions. Nevertheless, we show that valid inference is still possible if one monitors conditional performance and if either conditional exchangeability or time-constant selection bias hold. Specifically, we develop a new score-based cumulative sum (CUSUM) m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#26657;&#20934;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#32467;&#26524;&#21457;&#29616;&#26368;&#20339;&#27491;&#21017;&#21270;&#20272;&#35745;&#37327;&#30340;&#26657;&#20934;&#26354;&#32447;&#20855;&#26377;&#21452;&#37325;&#19979;&#38477;&#34892;&#20026;&#65292;&#19982;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2210.12760</link><description>&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#39640;&#32500;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A study of uncertainty quantification in overparametrized high-dimensional models. (arXiv:2210.12760v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#26657;&#20934;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#32467;&#26524;&#21457;&#29616;&#26368;&#20339;&#27491;&#21017;&#21270;&#20272;&#35745;&#37327;&#30340;&#26657;&#20934;&#26354;&#32447;&#20855;&#26377;&#21452;&#37325;&#19979;&#38477;&#34892;&#20026;&#65292;&#19982;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#21487;&#38752;&#21644;&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#20013;&#24515;&#25361;&#25112;&#12290;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#32972;&#26223;&#19979;&#65292;&#26420;&#32032;&#30340;&#24230;&#37327;&#26041;&#27861;(&#22914;&#26368;&#21518;&#19968;&#23618;&#20998;&#25968;)&#24050;&#32463;&#34987;&#24191;&#20026;&#20154;&#30693;&#22320;&#20135;&#29983;&#36807;&#24230;&#33258;&#20449;&#30340;&#20272;&#35745;&#12290;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20174;&#28201;&#24230;&#32553;&#25918;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21516;&#36125;&#21494;&#26031;&#22788;&#29702;&#65292;&#20197;&#32531;&#35299;&#36807;&#24230;&#33258;&#20449;&#65292;&#36890;&#24120;&#36890;&#36807;&#25968;&#20540;&#35266;&#23519;&#25903;&#25345;&#23427;&#20204;&#20135;&#29983;&#26356;&#22909;&#30340;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#25968;&#23398;&#21487;&#22788;&#29702;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#65292;&#23545;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#25552;&#20379;&#20102;&#24120;&#35265;&#19981;&#30830;&#23450;&#24230;&#37327;&#20043;&#38388;&#30340;&#23574;&#38160;&#27604;&#36739;&#65306;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25259;&#38706;&#26368;&#20339;&#27491;&#21017;&#21270;&#20272;&#35745;&#37327;&#30340;&#26657;&#20934;&#26354;&#32447;&#19982;&#36807;&#21442;&#25968;&#21270;&#30340;&#20989;&#25968;&#30340;&#21452;&#37325;&#19979;&#38477;&#34892;&#20026;&#12290;&#36825;&#19982;&#32463;&#39564;&#36125;&#21494;&#26031;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#25105;&#20204;&#23637;&#31034;&#23427;&#30340;&#26657;&#20934;&#26159;&#33391;&#22909;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is a central challenge in reliable and trustworthy machine learning. Naive measures such as last-layer scores are well-known to yield overconfident estimates in the context of overparametrized neural networks. Several methods, ranging from temperature scaling to different Bayesian treatments of neural networks, have been proposed to mitigate overconfidence, most often supported by the numerical observation that they yield better calibrated uncertainty measures. In this work, we provide a sharp comparison between popular uncertainty measures for binary classification in a mathematically tractable model for overparametrized neural networks: the random features model. We discuss a trade-off between classification accuracy and calibration, unveiling a double descent like behavior in the calibration curve of optimally regularized estimators as a function of overparametrization. This is in contrast with the empirical Bayes method, which we show to be well calibrate
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22810;&#32500;&#22825;&#27668;&#21644;&#27668;&#20505;&#25968;&#25454;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21387;&#32553;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#30041;&#37325;&#35201;&#27668;&#35937;&#32467;&#26500;&#21644;&#19981;&#24341;&#20837;&#20266;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#23558;&#25968;&#25454;&#21387;&#32553;&#19977;&#20010;&#25968;&#37327;&#32423;&#65292;&#20026;&#39640;&#20998;&#36776;&#29575;&#27668;&#20505;&#25968;&#25454;&#27665;&#20027;&#21270;&#35775;&#38382;&#21644;&#22810;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12538</link><description>&lt;p&gt;
&#21387;&#32553;&#22810;&#32500;&#22825;&#27668;&#21644;&#27668;&#20505;&#25968;&#25454;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Compressing multidimensional weather and climate data into neural networks. (arXiv:2210.12538v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12538
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#22810;&#32500;&#22825;&#27668;&#21644;&#27668;&#20505;&#25968;&#25454;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21387;&#32553;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#30041;&#37325;&#35201;&#27668;&#35937;&#32467;&#26500;&#21644;&#19981;&#24341;&#20837;&#20266;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#23558;&#25968;&#25454;&#21387;&#32553;&#19977;&#20010;&#25968;&#37327;&#32423;&#65292;&#20026;&#39640;&#20998;&#36776;&#29575;&#27668;&#20505;&#25968;&#25454;&#27665;&#20027;&#21270;&#35775;&#38382;&#21644;&#22810;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#21644;&#27668;&#20505;&#27169;&#25311;&#20135;&#29983;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#36798;&#21040;&#20102;Petabyte&#32423;&#21035;&#65292;&#30740;&#31350;&#32773;&#38656;&#35201;&#23545;&#20854;&#36827;&#34892;&#20998;&#26512;&#20197;&#20102;&#35299;&#27668;&#20505;&#21464;&#21270;&#25110;&#24694;&#21155;&#22825;&#27668;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#21387;&#32553;&#36825;&#31181;&#22810;&#32500;&#22825;&#27668;&#21644;&#27668;&#20505;&#25968;&#25454;&#65306;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;&#22352;&#26631;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36807;&#24230;&#25311;&#21512;&#25968;&#25454;&#65292;&#28982;&#21518;&#21462;&#24471;&#21040;&#30340;&#21442;&#25968;&#20316;&#20026;&#21407;&#22987;&#22522;&#20110;&#32593;&#26684;&#25968;&#25454;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#34429;&#28982;&#21387;&#32553;&#27604;&#33539;&#22260;&#20174;300&#20493;&#21040;&#36229;&#36807;3,000&#20493;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21152;&#26435;RMSE&#65292;MAE&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;SZ3&#21387;&#32553;&#22120;&#12290;&#23427;&#21487;&#20197;&#24544;&#23454;&#22320;&#20445;&#30041;&#37325;&#35201;&#30340;&#22823;&#27668;&#32467;&#26500;&#24182;&#19981;&#20250;&#24341;&#20837;&#20266;&#20687;&#12290;&#20351;&#29992;&#24471;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;790&#20493;&#21387;&#32553;&#25968;&#25454;&#21152;&#36733;&#22120;&#26469;&#35757;&#32451;WeatherBench&#39044;&#27979;&#27169;&#22411;&#65292;&#20854;RMSE&#21482;&#22686;&#21152;&#19981;&#21040;2%&#12290;&#19977;&#20010;&#25968;&#37327;&#32423;&#30340;&#21387;&#32553;&#20351;&#24471;&#39640;&#20998;&#36776;&#29575;&#27668;&#20505;&#25968;&#25454;&#24471;&#20197;&#27665;&#20027;&#21270;&#22320;&#35775;&#38382;&#65292;&#24182;&#20351;&#22810;&#20010;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather and climate simulations produce petabytes of high-resolution data that are later analyzed by researchers in order to understand climate change or severe weather. We propose a new method of compressing this multidimensional weather and climate data: a coordinate-based neural network is trained to overfit the data, and the resulting parameters are taken as a compact representation of the original grid-based data. While compression ratios range from 300x to more than 3,000x, our method outperforms the state-of-the-art compressor SZ3 in terms of weighted RMSE, MAE. It can faithfully preserve important large scale atmosphere structures and does not introduce artifacts. When using the resulting neural network as a 790x compressed dataloader to train the WeatherBench forecasting model, its RMSE increases by less than 2%. The three orders of magnitude compression democratizes access to high-resolution climate data and enables numerous new research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;8&#31181;&#19981;&#21516;&#29305;&#24449;&#30340;DGNN&#22312;CPU&#21644;GPU&#19978;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#20248;&#21270;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2210.03900</link><description>&lt;p&gt;
CPU&#21644;GPU&#19978;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#29942;&#39048;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Bottleneck Analysis of Dynamic Graph Neural Network Inference on CPU and GPU. (arXiv:2210.03900v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;8&#31181;&#19981;&#21516;&#29305;&#24449;&#30340;DGNN&#22312;CPU&#21644;GPU&#19978;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#20248;&#21270;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;(DGNN)&#22240;&#20854;&#22312;&#25429;&#25417;&#29616;&#23454;&#19990;&#30028;&#21160;&#24577;&#29305;&#24449;&#26041;&#38754;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#20174;&#31639;&#27861;&#30340;&#35282;&#24230;&#35774;&#35745;&#20102;&#21508;&#31181;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25104;&#21151;&#22320;&#23558;&#26102;&#38388;&#20449;&#24687;&#32435;&#20837;&#20102;&#22270;&#24418;&#22788;&#29702;&#20013;&#12290;&#23613;&#31649;&#31639;&#27861;&#24615;&#33021;&#34920;&#29616;&#26377;&#21069;&#36884;&#65292;&#20294;&#23558;DGNN&#37096;&#32626;&#21040;&#30828;&#20214;&#19978;&#20173;&#38754;&#20020;&#30528;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#27169;&#22411;&#22797;&#26434;&#24615;&#12289;&#22810;&#26679;&#24615;&#20197;&#21450;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#29305;&#24615;&#12290;&#21516;&#26102;&#65292;DGNN&#19982;&#38745;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#20351;&#24471;&#38024;&#23545;&#38745;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30828;&#20214;&#30456;&#20851;&#20248;&#21270;&#19981;&#36866;&#29992;&#20110;DGNN&#12290;&#26412;&#25991;&#36873;&#21462;&#20102;&#20843;&#31181;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#27969;&#34892;DGNN&#65292;&#24182;&#22312;CPU&#21644;GPU&#19978;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#24635;&#32467;&#24182;&#20998;&#26512;&#20102;&#20998;&#26512;&#32467;&#26524;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;DGNN&#22312;&#30828;&#20214;&#19978;&#30340;&#29942;&#39048;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;DGNN&#21152;&#36895;&#30340;&#28508;&#22312;&#20248;&#21270;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic graph neural network (DGNN) is becoming increasingly popular because of its widespread use in capturing dynamic features in the real world. A variety of dynamic graph neural networks designed from algorithmic perspectives have succeeded in incorporating temporal information into graph processing. Despite the promising algorithmic performance, deploying DGNNs on hardware presents additional challenges due to the model complexity, diversity, and the nature of the time dependency. Meanwhile, the differences between DGNNs and static graph neural networks make hardware-related optimizations for static graph neural networks unsuitable for DGNNs. In this paper, we select eight prevailing DGNNs with different characteristics and profile them on both CPU and GPU. The profiling results are summarized and analyzed, providing in-depth insights into the bottlenecks of DGNNs on hardware and identifying potential optimization opportunities for future DGNN acceleration. Followed by a comprehen
&lt;/p&gt;</description></item><item><title>TPGNN &#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20256;&#25773;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23398;&#20064;&#21160;&#24577;&#22270;&#20013;&#30340;&#39640;&#38454;&#20449;&#24687;&#12290;&#23545;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TPGNN&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.01171</link><description>&lt;p&gt;
TPGNN: &#36890;&#36807;&#26102;&#38388;&#20256;&#25773;&#23398;&#20064;&#21160;&#24577;&#22270;&#20013;&#30340;&#39640;&#38454;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
TPGNN: Learning High-order Information in Dynamic Graphs via Temporal Propagation. (arXiv:2210.01171v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01171
&lt;/p&gt;
&lt;p&gt;
TPGNN &#26159;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20256;&#25773;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23398;&#20064;&#21160;&#24577;&#22270;&#20013;&#30340;&#39640;&#38454;&#20449;&#24687;&#12290;&#23545;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TPGNN&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#26159;&#29992;&#20110;&#24314;&#27169;&#30001;&#28436;&#21270;&#20114;&#21160;&#35201;&#32032;&#32452;&#25104;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#25277;&#35937;&#27010;&#24565;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#38382;&#39064;&#8212;&#8212;&#22914;&#20309;&#20174;&#26102;&#38388;&#22270;&#20013;&#23398;&#20064;&#39640;&#38454;&#37051;&#23621;&#30340;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#25152;&#23398;&#33410;&#28857;&#34920;&#31034;&#30340;&#20449;&#24687;&#37327;&#21644;&#21306;&#20998;&#24230;&#65311;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#23398;&#20064;&#21160;&#24577;&#22270;&#20013;&#30340;&#39640;&#38454;&#20449;&#24687;&#26102;&#65292;&#25105;&#20204;&#20250;&#36935;&#21040;&#20004;&#20010;&#25361;&#25112;&#65306;&#35745;&#31639;&#25928;&#29575;&#20302;&#21644;&#36807;&#24230;&#24179;&#28369;&#65292;&#36825;&#20123;&#38382;&#39064;&#26080;&#27861;&#36890;&#36807;&#24212;&#29992;&#38745;&#24577;&#22270;&#19978;&#30340;&#20256;&#32479;&#25216;&#26415;&#26469;&#35299;&#20915;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20256;&#25773;&#30340;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21363;TPGNN&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#27169;&#22411;&#30001;&#20004;&#20010;&#19981;&#21516;&#30340;&#37096;&#20998;&#32452;&#25104;&#65292;&#21363;&#20256;&#25773;&#22120;&#21644;&#33410;&#28857;&#32534;&#30721;&#22120;&#12290;&#20256;&#25773;&#22120;&#26159;&#29992;&#20110;&#23558;&#28040;&#24687;&#20174;&#38170;&#33410;&#28857;&#20256;&#25773;&#21040;&#20854;$k$&#27493;&#26102;&#38388;&#37051;&#23621;&#30340;&#24037;&#20855;&#65292;&#28982;&#21518;&#21516;&#26102;&#26356;&#26032;&#37051;&#22495;&#29366;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#65292;&#29305;&#21035;&#26159;&#24403;$k$&#24456;&#22823;&#26102;&#12290;&#33410;&#28857;&#32534;&#30721;&#22120;&#26088;&#22312;&#32534;&#30721;&#38170;&#33410;&#28857;&#21450;&#20854;$k$&#27493;&#37051;&#23621;&#30340;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#26368;&#32456;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;&#25105;&#20204;&#23558;TPGNN&#24212;&#29992;&#20110;&#19968;&#32452;&#30495;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#32593;&#32476;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TPGNN&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#19968;&#20123;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal graph is an abstraction for modeling dynamic systems that consist of evolving interaction elements. In this paper, we aim to solve an important yet neglected problem -- how to learn information from high-order neighbors in temporal graphs? -- to enhance the informativeness and discriminativeness for the learned node representations. We argue that when learning high-order information from temporal graphs, we encounter two challenges, i.e., computational inefficiency and over-smoothing, that cannot be solved by conventional techniques applied on static graphs. To remedy these deficiencies, we propose a temporal propagation-based graph neural network, namely TPGNN. To be specific, the model consists of two distinct components, i.e., propagator and node-wise encoder. The propagator is leveraged to propagate messages from the anchor node to its temporal neighbors within $k$-hop, and then simultaneously update the state of neighborhoods, which enables efficient computation, especial
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#36755;&#36816;&#35745;&#21010;&#26174;&#24335;&#22522;&#25968;&#32422;&#26463;&#30340; OT &#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#27599;&#20010;&#36755;&#20837;&#20196;&#29260;&#37117;&#19982;&#23569;&#37327;&#19987;&#23478;&#21305;&#37197;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.15466</link><description>&lt;p&gt;
&#31232;&#30095;&#21463;&#38480;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Sparsity-Constrained Optimal Transport. (arXiv:2209.15466v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15466
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#36755;&#36816;&#35745;&#21010;&#26174;&#24335;&#22522;&#25968;&#32422;&#26463;&#30340; OT &#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#27599;&#20010;&#36755;&#20837;&#20196;&#29260;&#37117;&#19982;&#23569;&#37327;&#19987;&#23478;&#21305;&#37197;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#30340;&#26368;&#20248;&#36755;&#36816; (OT) &#29616;&#22312;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20316;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25439;&#22833;&#25110;&#21305;&#37197;&#23618;&#12290;&#20351;&#29992;&#29109;&#27491;&#21017;&#21270; OT &#21487;&#20197;&#20351;&#29992; Sinkhorn &#31639;&#27861;&#35745;&#31639;&#65292;&#20294;&#23427;&#20250;&#20135;&#29983;&#23436;&#20840;&#23494;&#38598;&#30340;&#36816;&#36755;&#35745;&#21010;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#26377;&#28304;&#37117;&#19982;&#25152;&#26377;&#30446;&#26631;&#65288;&#20998;&#25968;&#65289;&#21305;&#37197;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20960;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;&#27491;&#21017;&#21270;&#12290;&#36825;&#31181;&#27491;&#21017;&#21270;&#20445;&#30041;&#20102;&#31232;&#30095;&#24615;&#65292;&#24182;&#23548;&#33268;&#20102;&#26080;&#32422;&#26463;&#21644;&#24179;&#28369;&#65288;&#21322;&#65289;&#23545;&#20598;&#30446;&#26631;&#65292;&#21487;&#20197;&#20351;&#29992;&#29616;&#26377;&#30340;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#27714;&#35299;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20108;&#27425;&#27491;&#21017;&#21270;&#19981;&#33021;&#30452;&#25509;&#25511;&#21046;&#36816;&#36755;&#35745;&#21010;&#30340;&#22522;&#25968;&#65288;&#38750;&#38646;&#25968;&#30340;&#25968;&#37327;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24102;&#26377;&#36755;&#36816;&#35745;&#21010;&#26174;&#24335;&#22522;&#25968;&#32422;&#26463;&#30340; OT &#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#30001;&#23545;&#31232;&#30095;&#19987;&#23478;&#28151;&#21512;&#29289;&#30340;&#24212;&#29992;&#25152;&#39537;&#21160;&#30340;&#65292;&#20854;&#20013; OT &#21487;&#20197;&#29992;&#20110;&#23558;&#36755;&#20837;&#20196;&#29260;&#65288;&#20363;&#22914;&#22270;&#20687;&#34917;&#19969;&#65289;&#19982;&#19987;&#23478;&#27169;&#22411;&#65288;&#20363;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#36827;&#34892;&#21305;&#37197;&#12290;&#22522;&#25968;&#38480;&#21046;&#21487;&#20197;&#30830;&#20445;&#27599;&#20010;&#36755;&#20837;&#20196;&#29260;&#19982;&#23569;&#37327;&#19987;&#23478;&#21305;&#37197;&#65292;&#36825;&#26377;&#21161;&#20110;&#35299;&#37322;&#28151;&#21512;&#26435;&#37325;&#24182;&#26500;&#24314;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#31181;&#22522;&#20110; Proximal &#26799;&#24230;&#26041;&#27861;&#30340;&#31232;&#30095;&#21463;&#38480; OT &#38382;&#39064;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularized optimal transport (OT) is now increasingly used as a loss or as a matching layer in neural networks. Entropy-regularized OT can be computed using the Sinkhorn algorithm but it leads to fully-dense transportation plans, meaning that all sources are (fractionally) matched with all targets. To address this issue, several works have investigated quadratic regularization instead. This regularization preserves sparsity and leads to unconstrained and smooth (semi) dual objectives, that can be solved with off-the-shelf gradient methods. Unfortunately, quadratic regularization does not give direct control over the cardinality (number of nonzeros) of the transportation plan. We propose in this paper a new approach for OT with explicit cardinality constraints on the transportation plan. Our work is motivated by an application to sparse mixture of experts, where OT can be used to match input tokens such as image patches with expert models such as neural networks. Cardinality constraint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#27491;&#35268;&#21270;&#27969;&#36716;&#25442;&#20026; OT &#25928;&#29575;&#26356;&#39640;&#30340;&#33945;&#28909;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#20998;&#24067;&#30340;&#37325;&#26032;&#25490;&#21015;&#23454;&#29616;&#65292;&#21516;&#26102;&#36890;&#36807;&#27431;&#25289;&#26041;&#31243;&#32422;&#26463;&#23548;&#33268;&#20272;&#35745;&#30340;&#33945;&#28909;&#26144;&#23556;&#36335;&#24452;&#20301;&#20110;&#20445;&#25345;&#20307;&#31215;&#30340;&#24494;&#20998;&#21516;&#32986;&#31354;&#38388;&#20013;&#30340;&#27979;&#22320;&#32447;&#19978;&#12290;</title><link>http://arxiv.org/abs/2209.10873</link><description>&lt;p&gt;
&#23558;&#27491;&#35268;&#21270;&#27969;&#36716;&#25442;&#20026;&#20445;&#25345;&#27979;&#22320;&#32447;&#39640;&#26031;&#27969;&#30340;&#33945;&#28909;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Turning Normalizing Flows into Monge Maps with Geodesic Gaussian Preserving Flows. (arXiv:2209.10873v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#27491;&#35268;&#21270;&#27969;&#36716;&#25442;&#20026; OT &#25928;&#29575;&#26356;&#39640;&#30340;&#33945;&#28909;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#20998;&#24067;&#30340;&#37325;&#26032;&#25490;&#21015;&#23454;&#29616;&#65292;&#21516;&#26102;&#36890;&#36807;&#27431;&#25289;&#26041;&#31243;&#32422;&#26463;&#23548;&#33268;&#20272;&#35745;&#30340;&#33945;&#28909;&#26144;&#23556;&#36335;&#24452;&#20301;&#20110;&#20445;&#25345;&#20307;&#31215;&#30340;&#24494;&#20998;&#21516;&#32986;&#31354;&#38388;&#20013;&#30340;&#27979;&#22320;&#32447;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#35268;&#21270;&#27969;&#65288;NF&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#34920;&#29616;&#21147;&#21644;&#21487;&#24314;&#27169;&#22797;&#26434;&#23494;&#24230;&#30340;&#21487;&#22788;&#29702;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#29616;&#22312;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#25104;&#29087;&#30340;&#30740;&#31350;&#26041;&#21521;&#26159;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#23547;&#25214;&#33945;&#28909;&#26144;&#23556;&#65292;&#21363;&#28304;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20855;&#26377;&#26368;&#23567;&#25104;&#26412;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Brenier&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#20219;&#20309;&#35757;&#32451;&#22909;&#30340;NF&#36716;&#25442;&#20026;&#26356;&#20855;OT&#25928;&#29575;&#29256;&#26412;&#65292;&#32780;&#19981;&#25913;&#21464;&#26368;&#32456;&#23494;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#28304;&#65288;&#39640;&#26031;&#65289;&#20998;&#24067;&#30340;&#37325;&#26032;&#25490;&#21015;&#65292;&#26368;&#23567;&#21270;&#28304;&#19982;&#26368;&#32456;&#23494;&#24230;&#20043;&#38388;&#30340;OT&#25104;&#26412;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#27431;&#25289;&#26041;&#31243;&#32422;&#26463;&#23548;&#33268;&#20272;&#35745;&#30340;&#33945;&#28909;&#26144;&#23556;&#36335;&#24452;&#20301;&#20110;&#20445;&#25345;&#20307;&#31215;&#30340;&#24494;&#20998;&#21516;&#32986;&#31354;&#38388;&#20013;&#30340;&#27979;&#22320;&#32447;&#19978;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23548;&#33268;&#22810;&#20010;&#29616;&#26377;&#27169;&#22411;&#30340;&#24179;&#28369;&#27969;&#65292;&#24182;&#38477;&#20302;&#20102;OT&#25104;&#26412;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flows (NF) are powerful likelihood-based generative models that are able to trade off between expressivity and tractability to model complex densities. A now well established research avenue leverages optimal transport (OT) and looks for Monge maps, i.e. models with minimal effort between the source and target distributions. This paper introduces a method based on Brenier's polar factorization theorem to transform any trained NF into a more OT-efficient version without changing the final density. We do so by learning a rearrangement of the source (Gaussian) distribution that minimizes the OT cost between the source and the final density. We further constrain the path leading to the estimated Monge map to lie on a geodesic in the space of volume-preserving diffeomorphisms thanks to Euler's equations. The proposed method leads to smooth flows with reduced OT cost for several existing models without affecting the model performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#21644;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#21457;&#29616;&#31232;&#30095;&#20462;&#25913;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#30340;&#35266;&#27979;&#23545;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.09572</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#23545;&#25239;&#25915;&#20987;&#19982;&#38450;&#24481;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Robust Multivariate Time-Series Forecasting: Adversarial Attacks and Defense Mechanisms. (arXiv:2207.09572v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#21644;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#21457;&#29616;&#31232;&#30095;&#20462;&#25913;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#30340;&#35266;&#27979;&#23545;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#24320;&#21457;&#20102;&#20004;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#23041;&#32961;&#21644;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#36890;&#36807;&#23545;&#23569;&#25968;&#20854;&#20182;&#26102;&#38388;&#24207;&#21015;&#30340;&#36807;&#21435;&#35266;&#27979;&#36827;&#34892;&#25112;&#30053;&#24615;&#30340;&#31232;&#30095;&#65288;&#19981;&#21487;&#23519;&#35273;&#30340;&#65289;&#20462;&#25913;&#65292;&#23545;&#30446;&#26631;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#38450;&#24481;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#20808;&#21069;&#24320;&#21457;&#30340;&#20998;&#31867;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#25193;&#23637;&#21040;&#22810;&#20803;&#39044;&#27979;&#22330;&#26223;&#20013;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#23398;&#20064;&#21019;&#24314;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#21516;&#26102;&#20248;&#21270;&#39044;&#27979;&#27169;&#22411;&#20197;&#25552;&#39640;&#20854;&#23545;&#27492;&#31867;&#23545;&#25239;&#27169;&#25311;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#26041;&#26696;&#24378;&#22823;&#65292;&#25105;&#20204;&#30340;&#38450;&#24481;&#31639;&#27861;&#19982;&#22522;&#32447;&#38450;&#24481;&#26426;&#21046;&#30456;&#27604;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies the threats of adversarial attack on multivariate probabilistic forecasting models and viable defense mechanisms. Our studies discover a new attack pattern that negatively impact the forecasting of a target time series via making strategic, sparse (imperceptible) modifications to the past observations of a small number of other time series. To mitigate the impact of such attack, we have developed two defense strategies. First, we extend a previously developed randomized smoothing technique in classification to multivariate forecasting scenarios. Second, we develop an adversarial training algorithm that learns to create adversarial examples and at the same time optimizes the forecasting model to improve its robustness against such adversarial simulation. Extensive experiments on real-world datasets confirm that our attack schemes are powerful and our defense algorithms are more effective compared with baseline defense mechanisms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;BDM&#65288;&#30450;&#28857;&#21457;&#29616;&#26041;&#27861;&#65289;&#35780;&#20272;&#26694;&#26550;SpotCheck&#21644;&#19968;&#20010;&#20351;&#29992;2D&#22270;&#20687;&#34920;&#31034;&#30340;BDMPlaneSpot&#12290;&#23454;&#39564;&#32467;&#26524;&#32473;&#20986;&#24433;&#21709;BDM&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#35777;&#26126;PlaneSpot&#19982;&#29616;&#26377;&#30340;BDM&#30456;&#31454;&#20105;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2207.04104</link><description>&lt;p&gt;
&#22312;&#22270;&#20687;&#27169;&#22411;&#30340;&#30450;&#28857;&#21457;&#29616;&#20013;&#26397;&#30528;&#26356;&#21152;&#20005;&#35880;&#30340;&#31185;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards a More Rigorous Science of Blindspot Discovery in Image Models. (arXiv:2207.04104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;BDM&#65288;&#30450;&#28857;&#21457;&#29616;&#26041;&#27861;&#65289;&#35780;&#20272;&#26694;&#26550;SpotCheck&#21644;&#19968;&#20010;&#20351;&#29992;2D&#22270;&#20687;&#34920;&#31034;&#30340;BDMPlaneSpot&#12290;&#23454;&#39564;&#32467;&#26524;&#32473;&#20986;&#24433;&#21709;BDM&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#35777;&#26126;PlaneSpot&#19982;&#29616;&#26377;&#30340;BDM&#30456;&#31454;&#20105;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#30450;&#28857;&#21457;&#29616;&#26041;&#27861;&#65288;&#8220;BDM&#8221;&#65289;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#22270;&#20687;&#23884;&#20837;&#26469;&#26597;&#25214;&#25968;&#25454;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#23376;&#38598;&#65292;&#22312;&#36825;&#20123;&#23376;&#38598;&#20013;&#22270;&#20687;&#20998;&#31867;&#22120;&#34920;&#29616;&#26174;&#33879;&#26356;&#24046;&#65288;&#21363;&#23384;&#22312;&#30450;&#28857;&#65289;&#12290;&#21463;&#21040;&#20043;&#21069;&#24037;&#20316;&#20013;&#35266;&#23519;&#21040;&#30340;&#24046;&#36317;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;BDM&#35780;&#20272;&#26694;&#26550;&#65288;SpotCheck&#65289;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20855;&#26377;&#24050;&#30693;&#30450;&#28857;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20351;&#29992;&#26032;&#30340;BDM&#65288;PlaneSpot&#65289;&#26469;&#20351;&#29992;2D&#22270;&#20687;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;SpotCheck&#36827;&#34892;&#21463;&#25511;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#24433;&#21709;BDM&#24615;&#33021;&#30340;&#22240;&#32032;&#65288;&#20363;&#22914;&#27169;&#22411;&#20013;&#30340;&#30450;&#28857;&#25968;&#37327;&#25110;&#23450;&#20041;&#30450;&#28857;&#30340;&#29305;&#24449;&#65289;&#65292;&#24182;&#34920;&#26126;PlaneSpot&#19982;&#29616;&#26377;&#30340;BDM&#30456;&#31454;&#20105;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#39069;&#22806;&#30340;&#23454;&#39564;&#26469;&#39564;&#35777;&#36825;&#20123;&#21457;&#29616;&#65292;&#20351;&#29992;MS-COCO&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#26410;&#26469;&#22312;BDM&#35774;&#35745;&#26041;&#38754;&#25552;&#20986;&#20102;&#20960;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of work studies Blindspot Discovery Methods ("BDM"s): methods that use an image embedding to find semantically meaningful (i.e., united by a human-understandable concept) subsets of the data where an image classifier performs significantly worse. Motivated by observed gaps in prior work, we introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic image datasets to train models with known blindspots and a new BDM, PlaneSpot, that uses a 2D image representation. We use SpotCheck to run controlled experiments that identify factors that influence BDM performance (e.g., the number of blindspots in a model, or features used to define the blindspot) and show that PlaneSpot is competitive with and in many cases outperforms existing BDMs. Importantly, we validate these findings by designing additional experiments that use real image data from MS-COCO, a large image benchmark dataset. Our findings suggest several promising directions for future work on BDM des
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#31574;&#30053;&#25552;&#21319;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#32447;&#26041;&#27861;&#30340;&#31574;&#30053;&#25552;&#21319;&#20445;&#35777;&#21644;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#36890;&#36807;&#26679;&#26412;&#37325;&#29992;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.13714</link><description>&lt;p&gt;
&#24102;&#29702;&#35770;&#25903;&#25345;&#30340;&#26679;&#26412;&#37325;&#29992;&#30340;&#24191;&#20041;&#31574;&#30053;&#25552;&#21319;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generalized Policy Improvement Algorithms with Theoretically Supported Sample Reuse. (arXiv:2206.13714v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13714
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#31574;&#30053;&#25552;&#21319;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22312;&#32447;&#26041;&#27861;&#30340;&#31574;&#30053;&#25552;&#21319;&#20445;&#35777;&#21644;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#36890;&#36807;&#26679;&#26412;&#37325;&#29992;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#25511;&#21046;&#26041;&#27861;&#20855;&#26377;&#25913;&#21892;&#22797;&#26434;&#31995;&#32479;&#36816;&#34892;&#30340;&#28508;&#21147;&#65292;&#32780;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#34920;&#20102;&#19968;&#31181;&#27969;&#34892;&#30340;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#31867;&#21035;&#22312;&#23454;&#38469;&#25511;&#21046;&#37096;&#32626;&#30340;&#20004;&#20010;&#37325;&#35201;&#35201;&#27714;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65306;&#65288;i&#65289;&#23454;&#38469;&#24615;&#33021;&#20445;&#35777;&#21644;&#65288;ii&#65289;&#25968;&#25454;&#25928;&#29575;&#12290;&#31163;&#32447;&#31574;&#30053;&#31639;&#27861;&#36890;&#36807;&#26679;&#26412;&#37325;&#29992;&#26377;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#20294;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#65292;&#32780;&#22312;&#32447;&#31574;&#30053;&#31639;&#27861;&#20445;&#35777;&#20102;&#35757;&#32451;&#26399;&#38388;&#30340;&#36817;&#20284;&#31574;&#30053;&#25913;&#36827;&#65292;&#20294;&#21463;&#21040;&#39640;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24179;&#34913;&#36825;&#20123;&#31454;&#20105;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31867;&#24191;&#20041;&#31574;&#30053;&#25552;&#21319;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22312;&#32447;&#26041;&#27861;&#30340;&#31574;&#30053;&#25552;&#21319;&#20445;&#35777;&#21644;&#26679;&#26412;&#37325;&#29992;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#23545;&#26469;&#33258;DeepMind C&#30340;&#22810;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#36827;&#34892; extensive &#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26032;&#31867;&#31639;&#27861;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven, learning-based control methods offer the potential to improve operations in complex systems, and model-free deep reinforcement learning represents a popular approach to data-driven control. However, existing classes of algorithms present a trade-off between two important deployment requirements for real-world control: (i) practical performance guarantees and (ii) data efficiency. Off-policy algorithms make efficient use of data through sample reuse but lack theoretical guarantees, while on-policy algorithms guarantee approximate policy improvement throughout training but suffer from high sample complexity. In order to balance these competing goals, we develop a class of Generalized Policy Improvement algorithms that combines the policy improvement guarantees of on-policy methods with the efficiency of sample reuse. We demonstrate the benefits of this new class of algorithms through extensive experimental analysis on a variety of continuous control tasks from the DeepMind C
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#21387;&#32553;&#30340;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#20809;&#28369;&#24378;&#20984;-&#24378;&#20985;&#38797;&#28857;&#38382;&#39064;&#30340;&#20998;&#25955;&#27714;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#21253;&#25324;&#26799;&#24230;&#35745;&#31639;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2205.14452</link><description>&lt;p&gt;
&#21387;&#32553;&#36890;&#20449;&#30340;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#20998;&#25955;&#38797;&#28857;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Methods with Compressed Communication for Decentralized Saddle Point Problems. (arXiv:2205.14452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14452
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#21387;&#32553;&#30340;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#20809;&#28369;&#24378;&#20984;-&#24378;&#20985;&#38797;&#28857;&#38382;&#39064;&#30340;&#20998;&#25955;&#27714;&#35299;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#21253;&#25324;&#26799;&#24230;&#35745;&#31639;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#21387;&#32553;&#30340;&#38543;&#26426;&#26799;&#24230;&#31639;&#27861;&#26469;&#35299;&#20915;&#19968;&#31867;&#38750;&#20809;&#28369;&#24378;&#20984;-&#24378;&#20985;&#38797;&#28857;&#38382;&#39064;&#30340;&#20998;&#25955;&#24335;&#65288;&#26080;&#20013;&#22830;&#26381;&#21153;&#22120;&#65289;&#27714;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#19968;&#33324;&#38543;&#26426;&#24773;&#24418;&#30340;&#22522;&#20110;&#37325;&#21551;&#30340;&#21387;&#32553;&#20998;&#25955;&#24335;&#36817;&#31471;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#65288;C-RDPSG&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20854;&#20013;C-RDPSG&#30340;&#26799;&#24230;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#36890;&#20449;&#22797;&#26434;&#24230;&#20998;&#21035;&#20026;$\mathcal{O}( (1+\delta)^4 \frac{1}{L^2}{\kappa_f^2}\kappa_g^2 \frac{1}{\epsilon} )$&#65292;&#20197;&#23454;&#29616;$\epsilon$&#31934;&#24230;&#30340;&#38797;&#28857;&#35299;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#32553;&#30340;&#20998;&#25955;&#24335;&#36817;&#31471;&#38543;&#26426;&#26041;&#24046;&#32553;&#20943;&#26799;&#24230;&#31639;&#27861;(C-DPSVRG)&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop two compression based stochastic gradient algorithms to solve a class of non-smooth strongly convex-strongly concave saddle-point problems in a decentralized setting (without a central server). Our first algorithm is a Restart-based Decentralized Proximal Stochastic Gradient method with Compression (C-RDPSG) for general stochastic settings. We provide rigorous theoretical guarantees of C-RDPSG with gradient computation complexity and communication complexity of order $\mathcal{O}( (1+\delta)^4 \frac{1}{L^2}{\kappa_f^2}\kappa_g^2 \frac{1}{\epsilon} )$, to achieve an $\epsilon$-accurate saddle-point solution, where $\delta$ denotes the compression factor, $\kappa_f$ and $\kappa_g$ denote respectively the condition numbers of objective function and communication graph, and $L$ denotes the smoothness parameter of the smooth part of the objective function. Next, we present a Decentralized Proximal Stochastic Variance Reduced Gradient algorithm with Compression (C-DPSVRG) for fini
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#23545;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26131;&#21463;&#25915;&#20987;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#21487;&#20197;&#22312;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#38544;&#31169;&#21644;&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#38544;&#31169;&#19982;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.14055</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#23545;&#25104;&#21592;&#25512;&#29702;&#20013;&#32500;&#24230;&#30340;&#31069;&#31119;
&lt;/p&gt;
&lt;p&gt;
A Blessing of Dimensionality in Membership Inference through Regularization. (arXiv:2205.14055v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14055
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#23545;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26131;&#21463;&#25915;&#20987;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#21487;&#20197;&#22312;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#38544;&#31169;&#21644;&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#38544;&#31169;&#19982;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#23545;&#20998;&#31867;&#22120;&#22312;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#22914;&#20309;&#24341;&#21457;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#38382;&#39064;&#65306;&#22686;&#21152;&#21442;&#25968;&#25968;&#37327;&#36890;&#24120;&#20250;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#20250;&#38477;&#20302;&#38544;&#31169;&#20445;&#38556;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#38543;&#21518;&#35777;&#26126;&#65292;&#22914;&#26524;&#19982;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#30456;&#32467;&#21512;&#65292;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23454;&#38469;&#19978;&#21487;&#20197;&#21516;&#26102;&#22686;&#21152;&#20854;&#38544;&#31169;&#21644;&#24615;&#33021;&#65292;&#20174;&#32780;&#28040;&#38500;&#38544;&#31169;&#19982;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21452;&#37325;&#29305;&#24449;&#38598;&#21512;&#35774;&#32622;&#30340;&#23725;&#22238;&#24402;&#36923;&#36753;&#22238;&#24402;&#36827;&#34892;&#30340;&#23454;&#39564;&#26469;&#35777;&#26126;&#20102;&#36825;&#19968;&#31070;&#22855;&#29616;&#35937;&#12290;&#22312;&#25105;&#20204;&#30340;&#29702;&#35770;&#25506;&#32034;&#20043;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;leave-one-out&#20998;&#26512;&#24037;&#20855;&#65292;&#20197;&#31934;&#30830;&#21051;&#30011;&#32447;&#24615;&#20998;&#31867;&#22120;&#23545;&#26368;&#20339;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#26368;&#21518;&#25105;&#20204;&#36824;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is overparameterization a privacy liability? In this work, we study the effect that the number of parameters has on a classifier's vulnerability to membership inference attacks. We first demonstrate how the number of parameters of a model can induce a privacy--utility trade-off: increasing the number of parameters generally improves generalization performance at the expense of lower privacy. However, remarkably, we then show that if coupled with proper regularization, increasing the number of parameters of a model can actually simultaneously increase both its privacy and performance, thereby eliminating the privacy--utility trade-off. Theoretically, we demonstrate this curious phenomenon for logistic regression with ridge regularization in a bi-level feature ensemble setting. Pursuant to our theoretical exploration, we develop a novel leave-one-out analysis tool to precisely characterize the vulnerability of a linear classifier to the optimal membership inference attack. We empirically
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#65292;&#33021;&#22815;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36866;&#24212;&#26410;&#30693;&#26799;&#24230;&#33539;&#25968;&#12289;&#24179;&#28369;&#24615;&#21644;&#24378;&#20984;&#24615;&#65292;&#24182;&#22312;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#39640;&#27010;&#29575;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2205.02160</link><description>&lt;p&gt;
&#20351;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#26080;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making SGD Parameter-Free. (arXiv:2205.02160v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02160
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#21442;&#25968;&#21270;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#65292;&#33021;&#22815;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36866;&#24212;&#26410;&#30693;&#26799;&#24230;&#33539;&#25968;&#12289;&#24179;&#28369;&#24615;&#21644;&#24378;&#20984;&#24615;&#65292;&#24182;&#22312;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#39640;&#27010;&#29575;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#21442;&#25968;&#38543;&#26426;&#20984;&#20248;&#21270;&#65288;SCO&#65289;&#31639;&#27861;&#65292;&#20854;&#25910;&#25947;&#36895;&#24230;&#20165;&#27604;&#23545;&#24212;&#30340;&#24050;&#30693;&#21442;&#25968;&#35774;&#32622;&#30340;&#26368;&#20248;&#36895;&#24230;&#22810;&#19968;&#20010;&#21452;&#23545;&#25968;&#22240;&#23376;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20808;&#21069;&#24050;&#30693;&#30340;&#26080;&#21442;&#25968;SCO&#30340;&#26368;&#20339;&#36895;&#24230;&#26159;&#22522;&#20110;&#22312;&#32447;&#26080;&#21442;&#25968;&#21518;&#24724;&#30028;&#30340;&#65292;&#19982;&#24050;&#30693;&#21442;&#25968;&#30340;&#23545;&#24212;&#26041;&#27861;&#30456;&#27604;&#21253;&#21547;&#19981;&#21487;&#36991;&#20813;&#30340;&#39069;&#22806;&#23545;&#25968;&#39033;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#27010;&#24565;&#19978;&#30340;&#31616;&#21333;&#24615;&#65292;&#20855;&#26377;&#39640;&#27010;&#29575;&#20445;&#35777;&#65292;&#24182;&#19988;&#37096;&#20998;&#36866;&#24212;&#26410;&#30693;&#26799;&#24230;&#33539;&#25968;&#12289;&#24179;&#28369;&#24615;&#21644;&#24378;&#20984;&#24615;&#12290;&#25105;&#20204;&#30340;&#25104;&#26524;&#30340;&#26680;&#24515;&#26159;SGD&#27493;&#38271;&#36873;&#25321;&#30340;&#26032;&#22411;&#26080;&#21442;&#25968;&#35777;&#20070;&#65292;&#20197;&#21450;&#20551;&#35774;&#22312;SGD&#36845;&#20195;&#19978;&#27809;&#26377;&#20808;&#39564;&#30028;&#38480;&#30340;&#26102;&#38388;&#19968;&#33268;&#38598;&#20013;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an algorithm for parameter-free stochastic convex optimization (SCO) whose rate of convergence is only a double-logarithmic factor larger than the optimal rate for the corresponding known-parameter setting. In contrast, the best previously known rates for parameter-free SCO are based on online parameter-free regret bounds, which contain unavoidable excess logarithmic terms compared to their known-parameter counterparts. Our algorithm is conceptually simple, has high-probability guarantees, and is also partially adaptive to unknown gradient norms, smoothness, and strong convexity. At the heart of our results is a novel parameter-free certificate for SGD step size choice, and a time-uniform concentration result that assumes no a-priori bounds on SGD iterates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#30340;&#26694;&#26550;&#65292;&#24182;&#25351;&#20986;&#23454;&#39564;&#35774;&#32622;&#38382;&#39064;&#21644;&#26410;&#33021;&#27491;&#30830;&#32771;&#34385;&#25968;&#25454;&#21464;&#24322;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2204.07610</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Sources of Irreproducibility in Machine Learning: A Review. (arXiv:2204.07610v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#30340;&#26694;&#26550;&#65292;&#24182;&#25351;&#20986;&#23454;&#39564;&#35774;&#32622;&#38382;&#39064;&#21644;&#26410;&#33021;&#27491;&#30830;&#32771;&#34385;&#25968;&#25454;&#21464;&#24322;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#35768;&#22810;&#21457;&#24067;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#26159;&#19981;&#21487;&#37325;&#22797;&#30340;&#12290;&#26041;&#27861;&#35770;&#38382;&#39064;&#20197;&#21450;&#26410;&#33021;&#27491;&#30830;&#32771;&#34385;&#31639;&#27861;&#26412;&#36523;&#25110;&#20854;&#23454;&#29616;&#24341;&#20837;&#30340;&#21464;&#24322;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#37325;&#22797;&#24615;&#30340;&#20027;&#35201;&#36129;&#29486;&#22240;&#32032;&#12290;&#38382;&#39064;&#65306;&#19981;&#23384;&#22312;&#23558;&#23454;&#39564;&#35774;&#35745;&#36873;&#25321;&#19982;&#20854;&#23545;&#32467;&#35770;&#30340;&#28508;&#22312;&#24433;&#21709;&#32852;&#31995;&#36215;&#26469;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#32570;&#20047;&#36825;&#26679;&#30340;&#26694;&#26550;&#65292;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#23454;&#39564;&#32467;&#26524;&#21644;&#25551;&#36848;&#23454;&#39564;&#30340;&#38480;&#21046;&#20250;&#26356;&#21152;&#22256;&#38590;&#12290;&#32570;&#20047;&#36825;&#26679;&#30340;&#26694;&#26550;&#20063;&#20351;&#24471;&#29420;&#31435;&#30740;&#31350;&#20154;&#21592;&#38590;&#20197;&#31995;&#32479;&#22320;&#24402;&#22240;&#20110;&#22833;&#36133;&#30340;&#21487;&#37325;&#22797;&#24615;&#23454;&#39564;&#30340;&#21407;&#22240;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#29702;&#35299;&#21738;&#20123;&#23454;&#39564;&#35774;&#35745;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#21457;&#29616;&#65292;&#24182;&#36890;&#36807;&#27492;&#29702;&#35299;&#22914;&#20309;&#20998;&#26512;&#21487;&#37325;&#22797;&#24615;&#23454;&#39564;&#30340;&#32467;&#35770;&#65292;&#20174;&#32780;&#24110;&#21161;&#20998;&#26512;&#32467;&#35770;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#31639;&#27861;&#36873;&#25321;&#12289;&#25968;&#25454;&#21464;&#24322;&#12289;&#23454;&#39564;&#35774;&#32622;&#21644;&#23454;&#29616;&#32454;&#33410;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#20851;&#20110;&#21487;&#37325;&#22797;&#24615;&#30340;&#26368;&#36817;&#25991;&#29486;&#65292;&#30830;&#23450;&#20102;&#24120;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#23545;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21487;&#37325;&#22797;&#24615;&#30340;&#24433;&#21709;&#30340;&#20363;&#23376;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;&#23454;&#39564;&#35774;&#32622;&#38382;&#39064;&#21644;&#26410;&#33021;&#27491;&#30830;&#32771;&#34385;&#25968;&#25454;&#21464;&#24322;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#12290;&#32467;&#35770;&#65306;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#19981;&#21487;&#37325;&#22797;&#24615;&#26469;&#28304;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#24182;&#22686;&#21152;&#32467;&#26524;&#30340;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Many published machine learning studies are irreproducible. Issues with methodology and not properly accounting for variation introduced by the algorithm themselves or their implementations are attributed as the main contributors to the irreproducibility.Problem: There exist no theoretical framework that relates experiment design choices to potential effects on the conclusions. Without such a framework, it is much harder for practitioners and researchers to evaluate experiment results and describe the limitations of experiments. The lack of such a framework also makes it harder for independent researchers to systematically attribute the causes of failed reproducibility experiments. Objective: The objective of this paper is to develop a framework that enable applied data science practitioners and researchers to understand which experiment design choices can lead to false findings and how and by this help in analyzing the conclusions of reproducibility experiments. Method: We
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#26102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31995;&#32479;&#65292;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#27969;&#31243;&#20013;&#39640;&#25928;&#38598;&#25104;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20316;&#20026;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22235;&#26059;&#32764;&#31561;&#28789;&#27963;&#26426;&#22120;&#20154;&#24179;&#21488;&#39640;&#24615;&#33021;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2203.07747</link><description>&lt;p&gt;
&#23454;&#26102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65306;&#38754;&#21521;&#22235;&#26059;&#32764;&#21644;&#28789;&#27963;&#26426;&#22120;&#20154;&#24179;&#21488;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Real-time Neural-MPC: Deep Learning Model Predictive Control for Quadrotors and Agile Robotic Platforms. (arXiv:2203.07747v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23454;&#26102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31995;&#32479;&#65292;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#27969;&#31243;&#20013;&#39640;&#25928;&#38598;&#25104;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20316;&#20026;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22235;&#26059;&#32764;&#31561;&#28789;&#27963;&#26426;&#22120;&#20154;&#24179;&#21488;&#39640;&#24615;&#33021;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#24050;&#25104;&#20026;&#39640;&#24615;&#33021;&#33258;&#20027;&#31995;&#32479;&#23884;&#20837;&#24335;&#25511;&#21046;&#30340;&#27969;&#34892;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#35201;&#20351;&#29992;MPC&#23454;&#29616;&#33391;&#22909;&#30340;&#25511;&#21046;&#24615;&#33021;&#65292;&#31934;&#30830;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#26159;&#20851;&#38190;&#12290;&#20026;&#20102;&#20445;&#25345;&#23454;&#26102;&#25805;&#20316;&#65292;&#23884;&#20837;&#24335;&#31995;&#32479;&#19978;&#20351;&#29992;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#19968;&#33324;&#38480;&#20110;&#31616;&#21333;&#30340;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#27169;&#22411;&#65292;&#36825;&#26174;&#33879;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#19982;&#36825;&#31181;&#31616;&#21333;&#27169;&#22411;&#30456;&#21453;&#30340;&#26159;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#20934;&#30830;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#21160;&#24577;&#25928;&#24212;&#65292;&#20294;&#23427;&#20204;&#30340;&#24040;&#22823;&#35745;&#31639;&#22797;&#26434;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#19982;&#24555;&#36895;&#23454;&#26102;&#36845;&#20195;&#24490;&#29615;&#30340;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23454;&#26102;&#31070;&#32463;MPC&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#27969;&#31243;&#20013;&#39640;&#25928;&#22320;&#38598;&#25104;&#22823;&#22411;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20316;&#20026;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#39640;&#24230;&#28789;&#27963;&#30340;&#22235;&#26059;&#32764;&#24179;&#21488;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25551;&#36848;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#33021;&#22815;&#23454;&#29616;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#22312;&#23454;&#26102;&#25511;&#21046;&#20013;&#39640;&#24615;&#33021;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model Predictive Control (MPC) has become a popular framework in embedded control for high-performance autonomous systems. However, to achieve good control performance using MPC, an accurate dynamics model is key. To maintain real-time operation, the dynamics models used on embedded systems have been limited to simple first-principle models, which substantially limits their representative power. In contrast to such simple models, machine learning approaches, specifically neural networks, have been shown to accurately model even complex dynamic effects, but their large computational complexity hindered combination with fast real-time iteration loops. With this work, we present Real-time Neural MPC, a framework to efficiently integrate large, complex neural network architectures as dynamics models within a model-predictive control pipeline. Our experiments, performed in simulation and the real world onboard a highly agile quadrotor platform, demonstrate the capabilities of the described 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#37325;&#22797;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#36825;&#31181;&#31639;&#27861;&#33021;&#22815;&#25269;&#24481;&#26679;&#26412;&#21464;&#24322;&#65292;&#22312;&#20445;&#35777;&#39640;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#19982;&#23398;&#20064;&#25928;&#26524;&#30456;&#24726;&#65292;&#35774;&#35745;&#21487;&#37325;&#22797;&#24615;&#31639;&#27861;&#21487;&#20197;&#25512;&#21160;&#25105;&#20204;&#24320;&#21457;&#26356;&#39640;&#25928;&#12289;&#26356;&#31283;&#20581;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#24314;&#27169;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.08430</link><description>&lt;p&gt;
&#23398;&#20064;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reproducibility in Learning. (arXiv:2201.08430v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#37325;&#22797;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#27010;&#24565;&#65292;&#36825;&#31181;&#31639;&#27861;&#33021;&#22815;&#25269;&#24481;&#26679;&#26412;&#21464;&#24322;&#65292;&#22312;&#20445;&#35777;&#39640;&#20934;&#30830;&#24230;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#19982;&#23398;&#20064;&#25928;&#26524;&#30456;&#24726;&#65292;&#35774;&#35745;&#21487;&#37325;&#22797;&#24615;&#31639;&#27861;&#21487;&#20197;&#25512;&#21160;&#25105;&#20204;&#24320;&#21457;&#26356;&#39640;&#25928;&#12289;&#26356;&#31283;&#20581;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#24314;&#27169;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#37325;&#22797;&#24615;&#31639;&#27861;&#30340;&#27010;&#24565;&#12290;&#21487;&#37325;&#22797;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#25269;&#24481;&#26679;&#26412;&#21464;&#24322;&#8212;&#8212;&#22312;&#30456;&#21516;&#30340;&#22522;&#30784;&#20998;&#24067;&#19979;&#65292;&#36816;&#34892;&#22312;&#20004;&#20010;&#26679;&#26412;&#19978;&#26102;&#33021;&#22815;&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#30456;&#21516;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#24320;&#22987;&#35299;&#24320;&#35813;&#23450;&#20041;&#65292;&#28548;&#28165;&#38543;&#26426;&#24615;&#22914;&#20309;&#22312;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#26041;&#38754;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;&#25105;&#20204;&#21551;&#21160;&#20102;&#19968;&#20010;&#21487;&#37325;&#22797;&#31639;&#27861;&#30340;&#29702;&#35770;&#65292;&#23637;&#31034;&#20102;&#21487;&#37325;&#22797;&#24615;&#22914;&#20309;&#26263;&#31034;&#30528;&#20855;&#26377;&#33391;&#22909;&#24615;&#36136;&#65292;&#20363;&#22914;&#25968;&#25454;&#37325;&#29992;&#21644;&#39640;&#25928;&#24615;&#21487;&#27979;&#35797;&#24615;&#65292;&#23613;&#31649;&#35201;&#27714;&#37325;&#22797;&#24615;&#38750;&#24120;&#24378;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#32479;&#35745;&#23398;&#21644;&#23398;&#20064;&#20013;&#30340;&#33509;&#24178;&#22522;&#30784;&#38382;&#39064;&#65292;&#26377;&#19968;&#20123;&#39640;&#25928;&#29575;&#30340;&#21487;&#37325;&#22797;&#24615;&#31639;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#32479;&#35745;&#26597;&#35810;&#31639;&#27861;&#37117;&#21487;&#20197;&#36890;&#36807;&#36866;&#24230;&#22686;&#21152;&#26679;&#26412;&#22797;&#26434;&#24230;&#32780;&#21464;&#24471;&#21487;&#37325;&#22797;&#65292;&#24182;&#19988;&#25105;&#20204;&#20351;&#29992;&#36825;&#19968;&#28857;&#26500;&#24314;&#20102;&#23547;&#25214;&#36817;&#20284;&#20851;&#38190;&#20540;&#21644;&#20013;&#20301;&#25968;&#30340;&#21487;&#37325;&#22797;&#24615;&#31639;&#27861;&#12290;&#21033;&#29992;&#36825;&#20123;&#24819;&#27861;&#65292;&#25105;&#20204;&#20026;&#23547;&#25214;&#23545;&#25968;&#20985;&#23494;&#24230;&#20272;&#35745;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#21487;&#37325;&#22797;&#24615;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the notion of a reproducible algorithm in the context of learning. A reproducible learning algorithm is resilient to variations in its samples -- with high probability, it returns the exact same output when run on two samples from the same underlying distribution. We begin by unpacking the definition, clarifying how randomness is instrumental in balancing accuracy and reproducibility. We initiate a theory of reproducible algorithms, showing how reproducibility implies desirable properties such as data reuse and efficient testability. Despite the exceedingly strong demand of reproducibility, there are efficient reproducible algorithms for several fundamental problems in statistics and learning. First, we show that any statistical query algorithm can be made reproducible with a modest increase in sample complexity, and we use this to construct reproducible algorithms for finding approximate heavy-hitters and medians. Using these ideas, we give the first reproducible algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25968;&#22343;&#26041;&#35823;&#24046;&#65288;LMSE&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20989;&#25968;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#26356;&#31283;&#23450;&#12289;&#20855;&#26377;&#26356;&#24378;&#30340;&#25910;&#25947;&#24615;&#21644;&#26356;&#22909;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.05748</link><description>&lt;p&gt;
&#31283;&#20581;&#35757;&#32451;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#31616;&#26126;&#23545;&#25968;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Concise Logarithmic Loss Function for Robust Training of Anomaly Detection Model. (arXiv:2201.05748v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25968;&#22343;&#26041;&#35823;&#24046;&#65288;LMSE&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20989;&#25968;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#26356;&#31283;&#23450;&#12289;&#20855;&#26377;&#26356;&#24378;&#30340;&#25910;&#25947;&#24615;&#21644;&#26356;&#22909;&#30340;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#33021;&#22815;&#22312;&#27809;&#26377;&#25110;&#26368;&#23569;&#39046;&#22495;&#30693;&#35782;&#24773;&#20917;&#19979;&#24314;&#31435;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31639;&#27861;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26356;&#31283;&#23450;&#22320;&#35757;&#32451;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#35813;&#23450;&#20041;&#36866;&#24403;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25110;&#25439;&#22833;&#20989;&#25968;&#12290;&#23545;&#20110;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20989;&#25968;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#25968;&#22343;&#26041;&#35823;&#24046;&#65288;LMSE&#65289;&#65292;&#20197;&#26356;&#31283;&#23450;&#22320;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#30740;&#31350;&#28085;&#30422;&#20102;&#25968;&#23398;&#27604;&#36739;&#65292;&#21453;&#21521;&#20256;&#25773;&#30340;&#24046;&#20998;&#22495;&#21487;&#35270;&#21270;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25439;&#22833;&#25910;&#25947;&#21644;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#31561;&#21508;&#20010;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#22312;&#24635;&#20307;&#19978;&#65292;LMSE&#22312;&#25439;&#22833;&#25910;&#25947;&#24378;&#24230;&#12289;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;MSE&#20989;&#25968;&#12290;LMSE&#20989;&#25968;&#39044;&#35745;&#21487;&#36866;&#29992;&#20110;&#35757;&#32451;&#21508;&#31181;&#31867;&#22411;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning-based algorithms are widely adopted due to the advantage of being able to establish anomaly detection models without or with minimal domain knowledge of the task. Instead, to train the artificial neural network more stable, it should be better to define the appropriate neural network structure or the loss function. For the training anomaly detection model, the mean squared error (MSE) function is adopted widely. On the other hand, the novel loss function, logarithmic mean squared error (LMSE), is proposed in this paper to train the neural network more stable. This study covers a variety of comparisons from mathematical comparisons, visualization in the differential domain for backpropagation, loss convergence in the training process, and anomaly detection performance. In an overall view, LMSE is superior to the existing MSE function in terms of strongness of loss convergence, anomaly detection performance. The LMSE function is expected to be applicable for train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#20197;&#20943;&#23569;&#25968;&#25454;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#36890;&#36807;&#38477;&#20302;&#25968;&#25454;&#20351;&#29992;&#37327;&#26469;&#20943;&#23569;&#25104;&#26412;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36739;&#31616;&#21333;&#30340;&#27169;&#22411;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#32771;&#34385;&#30340;&#26368;&#20339;&#20998;&#31867;&#22120;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#31934;&#24230;&#19981;&#38477;&#20302;&#36229;&#36807;5%&#12290;</title><link>http://arxiv.org/abs/2201.01415</link><description>&lt;p&gt;
&#38382;&#39064;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#20851;&#27880;&#21147;&#21644;&#21162;&#21147;&#22312;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Problem-dependent attention and effort in neural networks with applications to image resolution and model selection. (arXiv:2201.01415v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.01415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#20197;&#20943;&#23569;&#25968;&#25454;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#36890;&#36807;&#38477;&#20302;&#25968;&#25454;&#20351;&#29992;&#37327;&#26469;&#20943;&#23569;&#25104;&#26412;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#36739;&#31616;&#21333;&#30340;&#27169;&#22411;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#32771;&#34385;&#30340;&#26368;&#20339;&#20998;&#31867;&#22120;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#31934;&#24230;&#19981;&#38477;&#20302;&#36229;&#36807;5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22270;&#20687;&#20998;&#31867;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#23427;&#20204;&#21487;&#20197;&#19982;&#20219;&#20309;&#20998;&#31867;&#22120;&#38598;&#21512;&#19968;&#36215;&#20351;&#29992;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#22312;&#31532;&#19968;&#31181;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#21482;&#22312;&#27169;&#22411;&#23545;&#20302;&#20998;&#36776;&#29575;&#20687;&#32032;&#21270;&#29256;&#26412;&#30340;&#20998;&#31867;&#34920;&#29616;&#20986;&#20302;&#20449;&#24515;&#26102;&#25165;&#20998;&#26512;&#23436;&#25972;&#23610;&#23544;&#30340;&#22270;&#20687;&#65292;&#20174;&#32780;&#20943;&#23569;&#25968;&#25454;&#20351;&#29992;&#37327;&#12290;&#24403;&#24212;&#29992;&#20110;&#26412;&#25991;&#32771;&#34385;&#30340;&#26368;&#20339;&#20998;&#31867;&#22120;&#26102;&#65292;MNIST&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#20351;&#29992;&#37327;&#20943;&#23569;&#20102;61.2%&#65292;KMNIST&#20943;&#23569;&#20102;69.6%&#65292;FashionMNIST&#20943;&#23569;&#20102;56.3%&#65292;SVHN&#20943;&#23569;&#20102;84.6%&#65292;ImageNet&#20943;&#23569;&#20102;40.6%&#65292;ImageNet-V2&#20943;&#23569;&#20102;27.6%&#65292;&#20294;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#31934;&#24230;&#22343;&#19981;&#38477;&#20302;&#36229;&#36807;5%&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;CIFAR-10&#65292;&#20687;&#32032;&#21270;&#30340;&#25968;&#25454;&#24182;&#19981;&#29305;&#21035;&#26377;&#20449;&#24687;&#37327;&#65292;&#32780;&#38598;&#25104;&#26041;&#27861;&#20250;&#22686;&#21152;&#25968;&#25454;&#20351;&#29992;&#37327;&#21516;&#26102;&#38477;&#20302;&#31934;&#24230;&#12290;&#22312;&#31532;&#20108;&#31181;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#22312;&#19968;&#20010;&#31616;&#21333;&#27169;&#22411;&#30340;&#20998;&#31867;&#32622;&#20449;&#24230;&#36739;&#20302;&#26102;&#65292;&#25165;&#20351;&#29992;&#19968;&#20010;&#22797;&#26434;&#27169;&#22411;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;MNIST&#19978;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#20102;82.1%&#65292;KMNIST&#38477;&#20302;&#20102;47.6%&#65292;FashionMNIST&#38477;&#20302;&#20102;72.3%&#65292;&#21516;&#26102;&#20998;&#31867;&#31934;&#24230;&#19981;&#38477;&#20302;&#36229;&#36807;0.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces two new ensemble-based methods to reduce the data and computation costs of image classification. They can be used with any set of classifiers and do not require additional training. In the first approach, data usage is reduced by only analyzing a full-sized image if the model has low confidence in classifying a low-resolution pixelated version. When applied on the best performing classifiers considered here, data usage is reduced by 61.2% on MNIST, 69.6% on KMNIST, 56.3% on FashionMNIST, 84.6% on SVHN, 40.6% on ImageNet, and 27.6% on ImageNet-V2, all with a less than 5% reduction in accuracy. However, for CIFAR-10, the pixelated data are not particularly informative, and the ensemble approach increases data usage while reducing accuracy. In the second approach, compute costs are reduced by only using a complex model if a simpler model has low confidence in its classification. Computation cost is reduced by 82.1% on MNIST, 47.6% on KMNIST, 72.3% on FashionMNIST, 86
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PINO&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#21644;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#26469;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#65292;&#20811;&#26381;&#20102;&#32431;&#25968;&#25454;&#39537;&#21160;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#19978;&#21512;&#24182;&#25968;&#25454;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2111.03794</link><description>&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#31639;&#23376;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Operator for Learning Partial Differential Equations. (arXiv:2111.03794v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PINO&#30340;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#21644;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#26469;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#65292;&#20811;&#26381;&#20102;&#32431;&#25968;&#25454;&#39537;&#21160;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#19978;&#21512;&#24182;&#25968;&#25454;&#21644;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#31639;&#23376;&#8221;&#65288;PINO&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#29616;&#26377;&#25968;&#25454;&#21644;/&#25110;&#29289;&#29702;&#32422;&#26463;&#26465;&#20214;&#26469;&#23398;&#20064;&#19968;&#31867;&#21442;&#25968;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#12290;&#35813;&#28151;&#21512;&#26041;&#27861;&#20801;&#35768;PINO&#20811;&#26381;&#32431;&#25968;&#25454;&#39537;&#21160;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#23616;&#38480;&#24615;&#12290;&#23558;&#25968;&#25454;&#21644;PDE&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;PINO&#33021;&#22815;&#21516;&#26102;&#36866;&#24212;&#25968;&#25454;&#37327;&#26377;&#38480;&#21644;/&#25110;&#36136;&#37327;&#36739;&#24046;&#30340;&#24773;&#20917;&#20197;&#21450;&#22256;&#38590;PDE&#32422;&#26463;&#30340;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;PINO&#20855;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#29305;&#24615;&#65292;&#21363;&#33021;&#22815;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#19978;&#21512;&#24182;&#25968;&#25454;&#21644;PDE&#32422;&#26463;&#12290;&#36825;&#20801;&#35768;&#25105;&#20204;&#23558;&#26469;&#33258;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#19982;&#39640;&#20998;&#36776;&#29575;PDE&#32422;&#26463;&#30456;&#32467;&#21512;&#65292;&#32780;&#25152;&#24471;&#21040;&#30340;PINO&#21363;&#20351;&#22312;&#39640;&#20998;&#36776;&#29575;&#30340;&#27979;&#35797;&#23454;&#20363;&#19978;&#20063;&#27809;&#26377;&#31934;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose physics-informed neural operators (PINO) that uses available data and/or physics constraints to learn the solution operator of a family of parametric Partial Differential Equation (PDE). This hybrid approach allows PINO to overcome the limitations of purely data-driven and physics-based methods. For instance, data-driven methods fail to learn when data is of limited quantity and/or quality, and physics-based approaches fail to optimize on challenging PDE constraints. By combining both data and PDE constraints, PINO overcomes all these challenges. Additionally, a unique property that PINO enjoys over other hybrid learning methods is its ability to incorporate data and PDE constraints at different resolutions. This allows us to combine coarse-resolution data, which is inexpensive to obtain from numerical solvers, with higher resolution PDE constraints, and the resulting PINO has no degradation in accuracy even on high-resolution test instances. This discretizati
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;PAC-Bayesian&#26694;&#26550;&#20026;&#20855;&#26377;&#27491;&#24577;&#20998;&#24067;&#26435;&#37325;&#30340;&#20108;&#20540;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#30340;&#32858;&#21512;&#25552;&#20379;&#20102;&#32039;&#20945;&#30340;&#27867;&#21270;&#30028;&#38480;&#21644;&#23398;&#20064;&#36807;&#31243;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22855;&#29305;&#30340;&#30028;&#38480;&#26368;&#23567;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2110.15137</link><description>&lt;p&gt;
&#20197;&#34920;&#31034;&#27010;&#29575;&#20026;&#22522;&#30784;&#30340;&#32858;&#21512;&#20108;&#20540;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#30340;PAC-Bayesian&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Learning of Aggregated Binary Activated Neural Networks with Probabilities over Representations. (arXiv:2110.15137v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.15137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;PAC-Bayesian&#26694;&#26550;&#20026;&#20855;&#26377;&#27491;&#24577;&#20998;&#24067;&#26435;&#37325;&#30340;&#20108;&#20540;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#30340;&#32858;&#21512;&#25552;&#20379;&#20102;&#32039;&#20945;&#30340;&#27867;&#21270;&#30028;&#38480;&#21644;&#23398;&#20064;&#36807;&#31243;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22855;&#29305;&#30340;&#30028;&#38480;&#26368;&#23567;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#23545;&#21442;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#23398;&#20064;&#24102;&#26377;&#19981;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26399;&#26395;&#20026;&#33258;&#36523;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39044;&#27979;&#22120;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#27491;&#24577;&#20998;&#24067;&#26435;&#37325;&#30340;&#20108;&#20540;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#30340;&#32858;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#20174;PAC-Bayesian&#26694;&#26550;&#20013;&#27966;&#29983;&#20986;&#30340;&#20998;&#26512;&#65292;&#20026;&#36825;&#31181;&#32858;&#21512;&#30340;&#39044;&#26399;&#36755;&#20986;&#20540;&#25552;&#20379;&#20102;&#32039;&#20945;&#30340;&#27867;&#21270;&#30028;&#38480;&#21644;&#23398;&#20064;&#36807;&#31243;&#65292;&#36825;&#30001;&#35299;&#26512;&#34920;&#36798;&#24335;&#32473;&#20986;&#12290;&#34429;&#28982;&#21518;&#32773;&#30340;&#32452;&#21512;&#24615;&#36136;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#34987;&#36817;&#20284;&#26367;&#20195;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#23545;&#20110;&#28145;&#32780;&#31364;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#30830;&#20999;&#30340;&#35745;&#31639;&#20173;&#28982;&#26159;&#21487;&#34892;&#30340;&#65292;&#36825;&#24471;&#30410;&#20110;&#19968;&#31181;&#21160;&#24577;&#32534;&#31243;&#26041;&#27861;&#12290;&#36825;&#23548;&#33268;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22855;&#29305;&#30340;&#29992;&#20110;&#20108;&#20803;&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#30340;&#30028;&#38480;&#26368;&#23567;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20854;&#20013;&#21069;&#21521;&#20256;&#36882;&#20256;&#25773;&#34920;&#31034;&#27010;&#29575;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Considering a probability distribution over parameters is known as an efficient strategy to learn a neural network with non-differentiable activation functions. We study the expectation of a probabilistic neural network as a predictor by itself, focusing on the aggregation of binary activated neural networks with normal distributions over real-valued weights. Our work leverages a recent analysis derived from the PAC-Bayesian framework that derives tight generalization bounds and learning procedures for the expected output value of such an aggregation, which is given by an analytical expression. While the combinatorial nature of the latter has been circumvented by approximations in previous works, we show that the exact computation remains tractable for deep but narrow neural networks, thanks to a dynamic programming approach. This leads us to a peculiar bound minimization learning algorithm for binary activated neural networks, where the forward pass propagates probabilities over repre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Rowhammer&#26041;&#27861;&#30340;&#31471;&#21040;&#31471;&#21518;&#38376;&#27880;&#20837;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#25915;&#20987;&#20998;&#31867;&#22120;&#27169;&#22411;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#20248;&#21270;&#35282;&#24230;&#35299;&#20915;&#20102;&#30828;&#20214;&#23454;&#29616;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2110.07683</link><description>&lt;p&gt;
&#21035;&#25970;&#38376;&#65281;Rowhammer&#25915;&#20987;&#36827;&#20837;DNN&#27169;&#22411;&#30340;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Don't Knock! Rowhammer at the Backdoor of DNN Models. (arXiv:2110.07683v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.07683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Rowhammer&#26041;&#27861;&#30340;&#31471;&#21040;&#31471;&#21518;&#38376;&#27880;&#20837;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#25915;&#20987;&#20998;&#31867;&#22120;&#27169;&#22411;&#12290;&#36890;&#36807;&#26032;&#39062;&#30340;&#20248;&#21270;&#35282;&#24230;&#35299;&#20915;&#20102;&#30828;&#20214;&#23454;&#29616;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25805;&#32437;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#21518;&#38376;&#27169;&#22411;&#22312;&#39044;&#20808;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#20250;&#20559;&#31163;&#39044;&#26399;&#34892;&#20026;&#65292;&#32780;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#20445;&#25345;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#22312;&#25512;&#26029;&#38454;&#27573;&#36890;&#36807;&#20462;&#25913;&#32593;&#32476;&#26435;&#37325;&#23545;&#21518;&#38376;&#27880;&#20837;&#36827;&#34892;&#36719;&#20214;&#27169;&#25311;&#65292;&#28982;&#32780;&#30001;&#20110;&#30828;&#20214;&#38480;&#21046;&#65292;&#36825;&#31181;&#26041;&#24335;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#19981;&#22826;&#29616;&#23454;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Rowhammer&#20316;&#20026;&#25925;&#38556;&#27880;&#20837;&#26041;&#27861;&#30340;&#23454;&#38469;&#30828;&#20214;&#19978;&#30340;&#31471;&#21040;&#31471;&#21518;&#38376;&#27880;&#20837;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#20998;&#31867;&#22120;&#27169;&#22411;&#36827;&#34892;&#25915;&#20987;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#22312;&#30828;&#20214;&#19978;&#23454;&#38469;&#37096;&#32626;DNNs&#20013;&#21518;&#38376;&#27880;&#20837;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20174;&#26032;&#39062;&#30340;&#20248;&#21270;&#35282;&#24230;&#35299;&#20915;&#20102;&#36825;&#20123;&#23454;&#38469;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#22312;&#20110;&#26131;&#21463;&#25915;&#20987;&#30340;&#20869;&#23384;&#20301;&#32622;&#38750;&#24120;&#32597;&#35265;&#12289;&#29305;&#23450;&#20110;&#35774;&#22791;&#24182;&#19988;&#31232;&#30095;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art deep neural networks (DNNs) have been proven to be vulnerable to adversarial manipulation and backdoor attacks. Backdoored models deviate from expected behavior on inputs with predefined triggers while retaining performance on clean data. Recent works focus on software simulation of backdoor injection during the inference phase by modifying network weights, which we find often unrealistic in practice due to restrictions in hardware.  In contrast, in this work for the first time, we present an end-to-end backdoor injection attack realized on actual hardware on a classifier model using Rowhammer as the fault injection method. To this end, we first investigate the viability of backdoor injection attacks in real-life deployments of DNNs on hardware and address such practical issues in hardware implementation from a novel optimization perspective. We are motivated by the fact that vulnerable memory locations are very rare, device-specific, and sparsely distributed. Conseque
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#24320;&#21457;&#20154;&#21592;&#21644;&#28304;&#25991;&#20214;&#32452;&#25104;&#30340;&#36129;&#29486;&#22270;&#65292;&#21033;&#29992;&#36825;&#20010;&#22270;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102; JIT &#32570;&#38519;&#39044;&#27979;&#65292;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#26131;&#20986;&#29616;&#32570;&#38519;&#30340;&#26356;&#25913;&#12290;</title><link>http://arxiv.org/abs/2110.05371</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25913;&#36827;&#20102; JIT &#32570;&#38519;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Machine Learning Improves Just-in-Time Defect Prediction. (arXiv:2110.05371v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.05371
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#30001;&#24320;&#21457;&#20154;&#21592;&#21644;&#28304;&#25991;&#20214;&#32452;&#25104;&#30340;&#36129;&#29486;&#22270;&#65292;&#21033;&#29992;&#36825;&#20010;&#22270;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102; JIT &#32570;&#38519;&#39044;&#27979;&#65292;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26356;&#22909;&#22320;&#39044;&#27979;&#20102;&#26131;&#20986;&#29616;&#32570;&#38519;&#30340;&#26356;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#36719;&#20214;&#19981;&#26029;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#25968;&#21315;&#21517;&#24320;&#21457;&#20154;&#21592;&#30340;&#36129;&#29486;&#12290;&#30001;&#20110;&#22797;&#26434;&#30340;&#21512;&#20316;&#32467;&#26500;&#65292;&#24320;&#21457;&#20154;&#21592;&#26356;&#21487;&#33021;&#24341;&#20837;&#26131;&#20986;&#29616;&#32570;&#38519;&#30340;&#26356;&#25913;&#65292;&#23548;&#33268;&#36719;&#20214;&#25925;&#38556;&#12290;&#30830;&#23450;&#36825;&#20123;&#26131;&#20986;&#29616;&#32570;&#38519;&#30340;&#26356;&#25913;&#34987;&#24341;&#20837;&#30340;&#26102;&#38388;&#24050;&#32463;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#32780;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36825;&#20123;&#20915;&#31574;&#20284;&#20046;&#24050;&#32463;&#36798;&#21040;&#20102;&#29942;&#39048;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#30001;&#24320;&#21457;&#20154;&#21592;&#21644;&#28304;&#25991;&#20214;&#32452;&#25104;&#30340;&#36129;&#29486;&#22270;&#26469;&#25429;&#25417;&#26500;&#24314;&#36719;&#20214;&#25152;&#38656;&#30340;&#24494;&#22937;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#36129;&#29486;&#22270;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#25913;&#36827; JIT &#32570;&#38519;&#39044;&#27979;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20551;&#35774;&#20174;&#36129;&#29486;&#22270;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#21487;&#33021;&#27604;&#20174;&#36719;&#20214;&#29305;&#24449;&#27966;&#29983;&#30340;&#26412;&#36136;&#29305;&#24449;&#26356;&#22909;&#22320;&#39044;&#27979;&#26131;&#20986;&#29616;&#32570;&#38519;&#30340;&#26356;&#25913;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26469;&#20998;&#31867;&#34920;&#31034;&#24320;&#21457;&#20154;&#21592;&#20043;&#38388;&#20132;&#20114;&#30340;&#36793;&#65292;&#20197;&#35777;&#23454;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing complexity of today's software requires the contribution of thousands of developers. This complex collaboration structure makes developers more likely to introduce defect-prone changes that lead to software faults. Determining when these defect-prone changes are introduced has proven challenging, and using traditional machine learning (ML) methods to make these determinations seems to have reached a plateau. In this work, we build contribution graphs consisting of developers and source files to capture the nuanced complexity of changes required to build software. By leveraging these contribution graphs, our research shows the potential of using graph-based ML to improve Just-In-Time (JIT) defect prediction. We hypothesize that features extracted from the contribution graphs may be better predictors of defect-prone changes than intrinsic features derived from software characteristics. We corroborate our hypothesis using graph-based ML for classifying edges that represent 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26063;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#22122;&#22768;&#26102;&#38388;&#34920;&#30340;&#26377;&#25928;&#20248;&#21270;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22270;&#20687;&#23494;&#24230;&#20272;&#35745;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#33021;&#24615;&#21644;&#35270;&#35273;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2107.00630</link><description>&lt;p&gt;
&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Variational Diffusion Models. (arXiv:2107.00630v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.00630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#26063;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#22122;&#22768;&#26102;&#38388;&#34920;&#30340;&#26377;&#25928;&#20248;&#21270;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22270;&#20687;&#23494;&#24230;&#20272;&#35745;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#33021;&#24615;&#21644;&#35270;&#35273;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#21512;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20063;&#33021;&#25104;&#20026;&#24456;&#22909;&#30340;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#27169;&#22411;&#21527;&#65311;&#25105;&#20204;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#26063;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#26631;&#20934;&#22270;&#20687;&#23494;&#24230;&#20272;&#35745;&#22522;&#20934;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;&#19982;&#20854;&#20182;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#19982;&#27169;&#22411;&#30340;&#20854;&#20313;&#37096;&#20998;&#20849;&#21516;&#26377;&#25928;&#22320;&#20248;&#21270;&#22122;&#22768;&#26102;&#38388;&#34920;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#25193;&#25955;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#26041;&#38754;&#65292;&#21464;&#20998;&#19979;&#38480;&#65288;VLB&#65289;&#31616;&#21270;&#20026;&#19968;&#20010;&#38750;&#24120;&#31616;&#30701;&#30340;&#34920;&#36798;&#24335;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25105;&#20204;&#23545;&#35813;&#27169;&#22411;&#31867;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20960;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36830;&#32493;&#26102;&#38388;VLB&#23545;&#20110;&#22122;&#22768;&#26102;&#38388;&#34920;&#26159;&#19981;&#21464;&#30340;&#65292;&#38500;&#20102;&#20854;&#31471;&#28857;&#22788;&#30340;&#20449;&#22122;&#27604;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#19968;&#31181;&#22122;&#22768;&#26102;&#38388;&#34920;&#65292;&#35813;&#34920;&#38024;&#23545;&#20449;&#22122;&#27604;&#26368;&#23567;&#21270;&#21464;&#20998;&#19979;&#38480;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;CIFAR-10&#65292; CelebA-HQ&#21644;FFHQ&#22312;&#20869;&#30340;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#21487;&#33021;&#24615;&#36824;&#26159;&#35270;&#35273;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the vari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#22909;&#30340; ConvNets &#30340;&#36229;&#24179;&#38754;&#37197;&#32622;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#32479;&#35745;&#24615;&#20559;&#24046;&#65292;&#32780;&#19988;&#36825;&#31181;&#20559;&#24046;&#19982;&#39564;&#35777;&#24615;&#33021;&#24687;&#24687;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2003.07797</link><description>&lt;p&gt;
&#35757;&#32451;&#22909;&#30340; ConvNets &#30340;&#36229;&#24179;&#38754;&#23433;&#25490;&#23384;&#22312;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Hyperplane Arrangements of Trained ConvNets Are Biased. (arXiv:2003.07797v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.07797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#22909;&#30340; ConvNets &#30340;&#36229;&#24179;&#38754;&#37197;&#32622;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#32479;&#35745;&#24615;&#20559;&#24046;&#65292;&#32780;&#19988;&#36825;&#31181;&#20559;&#24046;&#19982;&#39564;&#35777;&#24615;&#33021;&#24687;&#24687;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#21367;&#31215;&#23618;&#39044;&#28608;&#27963;&#31354;&#38388;&#20013;&#30340;&#20989;&#25968;&#36827;&#34892;&#20960;&#20309;&#23646;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340; ConvNets &#25152;&#35782;&#21035;&#30340;&#36229;&#24179;&#38754;&#37197;&#32622;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#35745;&#23398;&#26041;&#27861;&#26469;&#30740;&#31350;&#23616;&#37096;&#37197;&#32622;&#65292;&#24182;&#23558;&#20854;&#19982;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21160;&#24577;&#20851;&#32852;&#36215;&#26469;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35757;&#32451;&#22909;&#30340; ConvNets &#38754;&#21521;&#35268;&#21017;&#36229;&#24179;&#38754;&#37197;&#32622;&#20986;&#29616;&#20102;&#26174;&#33879;&#30340;&#32479;&#35745;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#22312; CIFAR10&#12289;CIFAR100 &#21644; ImageNet &#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#32467;&#26500;&#65292;&#20559;&#32622;&#23618;&#30340;&#37197;&#32622;&#23545;&#20110;&#39564;&#35777;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the geometric properties of the functions learned by trained ConvNets in the preactivation space of their convolutional layers, by performing an empirical study of hyperplane arrangements induced by a convolutional layer. We introduce statistics over the weights of a trained network to study local arrangements and relate them to the training dynamics. We observe that trained ConvNets show a significant statistical bias towards regular hyperplane configurations. Furthermore, we find that layers showing biased configurations are critical to validation performance for the architectures considered, trained on CIFAR10, CIFAR100 and ImageNet.
&lt;/p&gt;</description></item></channel></rss>