<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;DiffAug&#65292;&#36890;&#36807;&#25193;&#25955;&#27493;&#39588;&#23454;&#29616;&#22686;&#24378;&#25968;&#25454;&#21644;&#21407;&#22987;&#25968;&#25454;&#20849;&#20139;&#24179;&#28369;&#30340;&#28508;&#31354;&#38388;&#65292;&#26088;&#22312;&#25552;&#39640;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07909</link><description>&lt;p&gt;
&#20174;&#22836;&#24320;&#22987;&#21033;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#25552;&#21319;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch. (arXiv:2309.07909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;DiffAug&#65292;&#36890;&#36807;&#25193;&#25955;&#27493;&#39588;&#23454;&#29616;&#22686;&#24378;&#25968;&#25454;&#21644;&#21407;&#22987;&#25968;&#25454;&#20849;&#20139;&#24179;&#28369;&#30340;&#28508;&#31354;&#38388;&#65292;&#26088;&#22312;&#25552;&#39640;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20135;&#29983;&#31283;&#20581;&#19988;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24448;&#24448;&#36807;&#20110;&#20381;&#36182;&#20808;&#39564;&#30693;&#35782;&#25110;&#22806;&#37096;&#25968;&#25454;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#22312;&#36716;&#25442;&#21040;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#26102;&#24212;&#29992;&#21463;&#38480;&#65292;&#29305;&#21035;&#26159;&#22312;&#31185;&#23398;&#30456;&#20851;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#38480;&#21046;&#26469;&#28304;&#20110;&#36825;&#20123;&#39046;&#22495;&#20013;&#20808;&#39564;&#30693;&#35782;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#21294;&#20047;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#39640;&#25928;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#8212;&#8212;DiffAug&#12290;DiffAug&#36890;&#36807;&#25193;&#25955;&#27493;&#39588;&#30830;&#20445;&#22686;&#24378;&#25968;&#25454;&#21644;&#21407;&#22987;&#25968;&#25454;&#20849;&#20139;&#19968;&#20010;&#24179;&#28369;&#30340;&#28508;&#31354;&#38388;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;DiffAug&#39318;&#20808;&#23545;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#65292;&#20351;&#22686;&#24378;&#25968;&#25454;&#21644;&#21407;&#22987;&#25968;&#25454;&#22312;&#28508;&#31354;&#38388;&#19978;&#24179;&#28369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised contrastive learning methods have recently seen significant improvements, particularly through data augmentation strategies that aim to produce robust and generalizable representations. However, prevailing data augmentation methods, whether hand designed or based on foundation models, tend to rely heavily on prior knowledge or external data. This dependence often compromises their effectiveness and efficiency. Furthermore, the applicability of most existing data augmentation strategies is limited when transitioning to other research domains, especially science-related data. This limitation stems from the paucity of prior knowledge and labeled data available in these domains. To address these challenges, we introduce DiffAug-a novel and efficient Diffusion-based data Augmentation technique. DiffAug aims to ensure that the augmented and original data share a smoothed latent space, which is achieved through diffusion steps. Uniquely, unlike traditional methods, DiffAug first 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#20840;&#36523;&#29615;&#22659;&#20013;&#30340;&#28789;&#24039;&#25163;&#29289;&#20132;&#20114;&#12290;&#26041;&#27861;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#29289;&#29702;&#20223;&#30495;&#65292;&#36890;&#36807;&#23398;&#20064;&#20808;&#39564;&#25216;&#33021;&#26469;&#25511;&#21046;&#25163;&#29289;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07907</link><description>&lt;p&gt;
&#21487;&#34892;&#30340;&#20840;&#36523;&#25163;&#29289;&#20132;&#20114;&#21512;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Physically Plausible Full-Body Hand-Object Interaction Synthesis. (arXiv:2309.07907v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07907
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#25104;&#20840;&#36523;&#29615;&#22659;&#20013;&#30340;&#28789;&#24039;&#25163;&#29289;&#20132;&#20114;&#12290;&#26041;&#27861;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#29289;&#29702;&#20223;&#30495;&#65292;&#36890;&#36807;&#23398;&#20064;&#20808;&#39564;&#25216;&#33021;&#26469;&#25511;&#21046;&#25163;&#29289;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20840;&#36523;&#29615;&#22659;&#20013;&#21512;&#25104;&#28789;&#24039;&#30340;&#25163;&#29289;&#20132;&#20114;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#36827;&#23637;&#22312;&#22788;&#29702;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#30340;&#20855;&#20307;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#26524;&#65292;&#20294;&#32508;&#21512;&#24615;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#19987;&#27880;&#20110;&#20132;&#20114;&#36807;&#31243;&#30340;&#23396;&#31435;&#37096;&#20998;&#65292;&#24182;&#20381;&#36182;&#20110;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#20266;&#24433;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#29289;&#29702;&#20223;&#30495;&#26469;&#32531;&#35299;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#20998;&#23618;&#26694;&#26550;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#35299;&#32806;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#36523;&#20307;&#21644;&#25163;&#37096;&#36816;&#21160;&#30340;&#25216;&#33021;&#20808;&#39564;&#12290;&#36890;&#29992;&#30340;&#25216;&#33021;&#20808;&#39564;&#23558;&#28508;&#22312;&#30340;&#25216;&#33021;&#23884;&#20837;&#35299;&#30721;&#20026;&#24213;&#23618;&#37096;&#20998;&#30340;&#36816;&#21160;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#39640;&#23618;&#31574;&#30053;&#22312;&#36825;&#20123;&#39044;&#35757;&#32451;&#30340;&#28508;&#31354;&#38388;&#20013;&#25511;&#21046;&#25163;&#29289;&#20132;&#20114;&#65292;&#21463;&#21040;&#25569;&#25345;&#21644;3D&#30446;&#26631;&#36712;&#36857;&#36319;&#36394;&#30340;&#20219;&#21153;&#30446;&#26631;&#30340;&#25351;&#23548;&#12290;&#23427;&#26159;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#35813;&#20989;&#25968;&#32467;&#21512;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
We propose a physics-based method for synthesizing dexterous hand-object interactions in a full-body setting. While recent advancements have addressed specific facets of human-object interactions, a comprehensive physics-based approach remains a challenge. Existing methods often focus on isolated segments of the interaction process and rely on data-driven techniques that may result in artifacts. In contrast, our proposed method embraces reinforcement learning (RL) and physics simulation to mitigate the limitations of data-driven approaches. Through a hierarchical framework, we first learn skill priors for both body and hand movements in a decoupled setting. The generic skill priors learn to decode a latent skill embedding into the motion of the underlying part. A high-level policy then controls hand-object interactions in these pretrained latent spaces, guided by task objectives of grasping and 3D target trajectory following. It is trained using a novel reward function that combines an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#23398;&#20064;&#38656;&#27714;&#65292;&#24182;&#30830;&#20445;&#22312;&#22810;&#27425;&#24212;&#29992;&#26102;&#24471;&#21040;&#30340;&#20989;&#25968;&#26159;&#36830;&#32493;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.07899</link><description>&lt;p&gt;
&#25913;&#36827;&#20855;&#26377;&#30828;&#32422;&#26463;&#30340;&#29289;&#29702;&#20449;&#24687;DeepONets
&lt;/p&gt;
&lt;p&gt;
Improving physics-informed DeepONets with hard constraints. (arXiv:2309.07899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#23545;&#21021;&#22987;&#26465;&#20214;&#30340;&#23398;&#20064;&#38656;&#27714;&#65292;&#24182;&#30830;&#20445;&#22312;&#22810;&#27425;&#24212;&#29992;&#26102;&#24471;&#21040;&#30340;&#20989;&#25968;&#26159;&#36830;&#32493;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;&#26631;&#20934;&#25110;&#25805;&#20316;&#31526;&#65289;&#20173;&#28982;&#20381;&#36182;&#20110;&#20934;&#30830;&#22320;&#23398;&#20064;&#25152;&#35299;&#20915;&#31995;&#32479;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#30340;&#25968;&#20540;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#23398;&#20064;&#36825;&#20123;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#28436;&#21270;&#36825;&#20123;&#21021;&#22987;&#26465;&#20214;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#25913;&#36827;&#24403;&#21069;&#30340;&#29289;&#29702;&#20449;&#24687;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#20351;&#24471;&#19981;&#38656;&#35201;&#23398;&#20064;&#21021;&#22987;&#26465;&#20214;&#65292;&#24182;&#19988;&#23558;&#20854;&#20934;&#30830;&#22320;&#34920;&#31034;&#22312;&#39044;&#27979;&#30340;&#35299;&#20013;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#20445;&#35777;&#24403;&#23558;DeepONet&#22810;&#27425;&#24212;&#29992;&#20110;&#26102;&#38388;&#27493;&#38271;&#35299;&#19978;&#26102;&#65292;&#24471;&#21040;&#30340;&#20989;&#25968;&#26159;&#36830;&#32493;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current physics-informed (standard or operator) neural networks still rely on accurately learning the initial conditions of the system they are solving. In contrast, standard numerical methods evolve such initial conditions without needing to learn these. In this study, we propose to improve current physics-informed deep learning strategies such that initial conditions do not need to be learned and are represented exactly in the predicted solution. Moreover, this method guarantees that when a DeepONet is applied multiple times to time step a solution, the resulting function is continuous.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21516;&#36136;&#31181;&#32676;&#30340;&#38543;&#26426;&#23454;&#39564;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#20195;&#29702;&#25351;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23558;&#26368;&#20248;&#20195;&#29702;&#25351;&#26631;&#30340;&#26500;&#24314;&#36827;&#34892;&#20102;&#24402;&#32422;&#65292;&#24182;&#23545;&#35266;&#23519;&#21040;&#30340;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#20102;&#38477;&#22122;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.07893</link><description>&lt;p&gt;
&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#20013;&#36873;&#25321;&#20195;&#29702;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Choosing a Proxy Metric from Past Experiments. (arXiv:2309.07893v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21516;&#36136;&#31181;&#32676;&#30340;&#38543;&#26426;&#23454;&#39564;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#20195;&#29702;&#25351;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23558;&#26368;&#20248;&#20195;&#29702;&#25351;&#26631;&#30340;&#26500;&#24314;&#36827;&#34892;&#20102;&#24402;&#32422;&#65292;&#24182;&#23545;&#35266;&#23519;&#21040;&#30340;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#20102;&#38477;&#22122;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#38543;&#26426;&#23454;&#39564;&#20013;&#65292;&#24448;&#24448;&#24456;&#38590;&#25110;&#19981;&#21487;&#34892;&#22320;&#27979;&#37327;&#38271;&#26399;&#25351;&#26631;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#20027;&#35201;&#32467;&#26524;&#65289;&#12290;&#36825;&#20123;&#38271;&#26399;&#25351;&#26631;&#24448;&#24448;&#21453;&#24212;&#21464;&#21270;&#36739;&#24930;&#65292;&#19988;&#22122;&#22768;&#36739;&#22823;&#65292;&#20351;&#24471;&#22312;&#30701;&#26399;&#23454;&#39564;&#20013;&#38590;&#20197;&#20934;&#30830;&#20272;&#35745;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26367;&#20195;&#26041;&#27861;&#26159;&#27979;&#37327;&#20960;&#20010;&#30701;&#26399;&#20195;&#29702;&#25351;&#26631;&#65292;&#24076;&#26395;&#23427;&#20204;&#33021;&#22815;&#32039;&#23494;&#36861;&#36394;&#38271;&#26399;&#25351;&#26631;&#65292;&#20174;&#32780;&#22312;&#36817;&#26399;&#26377;&#25928;&#22320;&#25351;&#23548;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#21644;&#26500;&#24314;&#19968;&#20010;&#36866;&#29992;&#20110;&#21516;&#36136;&#31181;&#32676;&#38543;&#26426;&#23454;&#39564;&#30340;&#26368;&#20248;&#20195;&#29702;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23558;&#32473;&#23450;&#23454;&#39564;&#20013;&#26368;&#20248;&#20195;&#29702;&#25351;&#26631;&#30340;&#26500;&#24314;&#24402;&#32422;&#20026;&#19968;&#20010;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21462;&#20915;&#20110;&#32771;&#34385;&#20013;&#23454;&#39564;&#30340;&#30495;&#23454;&#28508;&#22312;&#27835;&#30103;&#25928;&#26524;&#21644;&#22122;&#22768;&#27700;&#24179;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#38271;&#26399;&#25351;&#26631;&#21644;&#19968;&#32452;&#20195;&#29702;&#30340;&#35266;&#23519;&#21040;&#30340;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#38477;&#22122;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many randomized experiments, the treatment effect of the long-term metric (i.e. the primary outcome of interest) is often difficult or infeasible to measure. Such long-term metrics are often slow to react to changes and sufficiently noisy they are challenging to faithfully estimate in short-horizon experiments. A common alternative is to measure several short-term proxy metrics in the hope they closely track the long-term metric -- so they can be used to effectively guide decision-making in the near-term. We introduce a new statistical framework to both define and construct an optimal proxy metric for use in a homogeneous population of randomized experiments. Our procedure first reduces the construction of an optimal proxy metric in a given experiment to a portfolio optimization problem which depends on the true latent treatment effects and noise level of experiment under consideration. We then denoise the observed treatment effects of the long-term metric and a set of proxies in a 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23616;&#37096;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#26694;&#26550;&#30340;&#20307;&#37325;&#36816;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#21387;&#21147;&#20256;&#24863;&#22120;&#21644;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#32467;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#22312;&#32771;&#34385;&#29289;&#29702;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;11%&#30340;&#25552;&#21319;&#25913;&#21892;&#20102;&#36816;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07888</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23616;&#37096;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#26694;&#26550;&#30340;&#20307;&#37325;&#36816;&#21160;&#35782;&#21035;&#26041;&#27861;&#19982;&#21387;&#21147;&#20256;&#24863;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Novel Local-Global Feature Fusion Framework for Body-weight Exercise Recognition with Pressure Mapping Sensors. (arXiv:2309.07888v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23616;&#37096;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#26694;&#26550;&#30340;&#20307;&#37325;&#36816;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#21387;&#21147;&#20256;&#24863;&#22120;&#21644;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#32467;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#22312;&#32771;&#34385;&#29289;&#29702;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;11%&#30340;&#25552;&#21319;&#25913;&#21892;&#20102;&#36816;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23616;&#37096;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#26694;&#26550;&#30340;&#20307;&#37325;&#36816;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#22320;&#38754;&#30340;&#21160;&#24577;&#21387;&#21147;&#22270;&#12290;&#30456;&#27604;&#20110;&#29616;&#26377;&#20027;&#35201;&#38598;&#20013;&#20110;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#21033;&#29992;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#21644;YOLO&#30446;&#26631;&#26816;&#27979;&#26469;&#32467;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#23450;&#20301;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#30340;&#21387;&#21147;&#20998;&#24067;&#24182;&#32771;&#34385;&#29289;&#29702;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#29983;&#25104;&#20102;&#20004;&#32452;&#39640;&#32423;&#23616;&#37096;&#29305;&#24449;&#65292;&#21253;&#25324;&#35009;&#21098;&#21518;&#30340;&#21387;&#21147;&#20998;&#24067;&#22270;&#19982;&#25968;&#20540;&#29305;&#24449;&#65292;&#22914;&#35282;&#24230;&#26041;&#21521;&#12289;&#22443;&#23376;&#19978;&#30340;&#20301;&#32622;&#21644;&#21387;&#21147;&#38754;&#31215;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#30693;&#35782;&#33976;&#39311;&#26469;&#35268;&#33539;&#21270;&#20445;&#30041;&#20840;&#23616;&#29305;&#24449;&#25552;&#21462;&#30340;&#30693;&#35782;&#65292;&#24182;&#25552;&#39640;&#36816;&#21160;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36816;&#21160;&#35782;&#21035;&#30340;F1&#24471;&#20998;&#19978;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;11%&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel local-global feature fusion framework for body-weight exercise recognition with floor-based dynamic pressure maps. One step further from the existing studies using deep neural networks mainly focusing on global feature extraction, the proposed framework aims to combine local and global features using image processing techniques and the YOLO object detection to localize pressure profiles from different body parts and consider physical constraints. The proposed local feature extraction method generates two sets of high-level local features consisting of cropped pressure mapping and numerical features such as angular orientation, location on the mat, and pressure area. In addition, we adopt a knowledge distillation for regularization to preserve the knowledge of the global feature extraction and improve the performance of the exercise recognition. Our experimental results demonstrate a notable 11 percent improvement in F1 score for exercise recognition while preserving 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#24191;&#20041;KMM&#31867;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#26041;&#27861;&#20197;&#36866;&#24212;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#65292;&#24182;&#21253;&#25324;&#23545;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#23376;&#38598;&#30456;&#23545;&#20110;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.07887</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#31181;&#24191;&#20041;KMM&#31867;&#22411;&#23494;&#24230;&#27604;&#20272;&#35745;&#26041;&#27861;&#30340;&#19968;&#20123;&#27880;&#35760;
&lt;/p&gt;
&lt;p&gt;
Some notes concerning a generalized KMM-type optimization method for density ratio estimation. (arXiv:2309.07887v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#24191;&#20041;KMM&#31867;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#36866;&#24403;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#26041;&#27861;&#20197;&#36866;&#24212;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#65292;&#24182;&#21253;&#25324;&#23545;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#23376;&#38598;&#30456;&#23545;&#20110;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#20219;&#21153;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#26500;&#24314;&#21512;&#36866;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25193;&#23637;&#33879;&#21517;&#30340;KMM&#26041;&#27861;&#65292;&#20197;&#28085;&#30422;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#65292;&#21253;&#25324;&#23545;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#23376;&#38598;&#30456;&#23545;&#20110;&#23494;&#24230;&#27604;&#36827;&#34892;&#20272;&#35745;&#12290;&#30456;&#20851;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/CDAlecsa/Generalized-KMM&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present paper we introduce new optimization algorithms for the task of density ratio estimation. More precisely, we consider extending the well-known KMM method using the construction of a suitable loss function, in order to encompass more general situations involving the estimation of density ratio with respect to subsets of the training data and test data, respectively. The associated codes can be found at https://github.com/CDAlecsa/Generalized-KMM.
&lt;/p&gt;</description></item><item><title>beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.07867</link><description>&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07867
&lt;/p&gt;
&lt;p&gt;
beta&#25193;&#25955;&#26159;&#19968;&#31181;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#36827;&#34892;&#20056;&#27861;&#36716;&#25442;&#65292;&#23454;&#29616;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;KL&#25955;&#24230;&#19978;&#30028;&#36827;&#34892;&#20248;&#21270;&#65292;&#35777;&#26126;&#20102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;beta&#25193;&#25955;&#65292;&#19968;&#31181;&#23558;&#21435;&#25513;&#30422;&#21644;&#21435;&#22122;&#38598;&#25104;&#21040;&#19968;&#36215;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26377;&#30028;&#33539;&#22260;&#20869;&#29983;&#25104;&#25968;&#25454;&#12290;&#20351;&#29992;&#20102;&#32553;&#25918;&#21644;&#20559;&#31227;&#30340;beta&#20998;&#24067;&#65292;beta&#25193;&#25955;&#21033;&#29992;&#20102;&#38543;&#26102;&#38388;&#30340;&#20056;&#27861;&#36716;&#25442;&#26469;&#21019;&#24314;&#27491;&#21521;&#21644;&#21453;&#21521;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21516;&#26102;&#32500;&#25345;&#30528;&#27491;&#21521;&#36793;&#32536;&#20998;&#24067;&#21644;&#21453;&#21521;&#26465;&#20214;&#20998;&#24067;&#65292;&#32473;&#23450;&#20219;&#24847;&#26102;&#38388;&#28857;&#30340;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;&#20256;&#32479;&#27169;&#22411;&#20381;&#36182;&#20110;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#21644;&#37325;&#26032;&#21152;&#26435;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#65292;beta&#25193;&#25955;&#26159;&#20056;&#27861;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#20174;KL&#25955;&#24230;&#30340;&#20984;&#24615;&#25512;&#23548;&#20986;&#26469;&#30340;KL&#25955;&#24230;&#19978;&#30028;&#65288;KLUB&#65289;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;KLUB&#30456;&#23545;&#20110;&#36127;ELBO&#26469;&#35828;&#23545;&#20110;&#20248;&#21270;beta&#25193;&#25955;&#26356;&#21152;&#26377;&#25928;&#65292;&#36127;ELBO&#20063;&#21487;&#20197;&#20316;&#20026;&#30456;&#21516;KL&#25955;&#24230;&#30340;KLUB&#65292;&#21482;&#26159;&#20854;&#20004;&#20010;&#21442;&#25968;&#20132;&#25442;&#20102;&#20301;&#32622;&#12290;beta&#25193;&#25955;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;Bregman&#25955;&#24230;&#20026;&#25351;&#26631;&#26469;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce beta diffusion, a novel generative modeling method that integrates demasking and denoising to generate data within bounded ranges. Using scaled and shifted beta distributions, beta diffusion utilizes multiplicative transitions over time to create both forward and reverse diffusion processes, maintaining beta distributions in both the forward marginals and the reverse conditionals, given the data at any point in time. Unlike traditional diffusion-based generative models relying on additive Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived from the convexity of the KL divergence. We demonstrate that the proposed KLUBs are more effective for optimizing beta diffusion compared to negative ELBOs, which can also be derived as the KLUBs of the same KL divergence with its two arguments swapped. The loss function of beta diffusion, expressed in terms of Bregman divergence, furt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#26597;&#21644;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#23545;&#31216;&#24615;&#32676;&#35770;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#23545;&#31216;&#24615;&#21457;&#29616;&#30340;&#28145;&#24230;&#23398;&#20064;&#38454;&#27573;&#25110;&#21518;&#22788;&#29702;&#38454;&#27573;&#25506;&#27979;&#23376;&#20195;&#25968;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#30340;SU(3)&#21644;SU(5)&#38750;&#38463;&#36125;&#23572;&#35268;&#33539;&#23545;&#31216;&#24615;&#30340;&#33258;&#21457;&#30772;&#32570;&#12290;</title><link>http://arxiv.org/abs/2309.07860</link><description>&lt;p&gt;
&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#23545;&#31216;&#24615;&#30340;&#32676;&#35770;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Identifying the Group-Theoretic Structure of Machine-Learned Symmetries. (arXiv:2309.07860v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#26597;&#21644;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#23545;&#31216;&#24615;&#32676;&#35770;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#23545;&#31216;&#24615;&#21457;&#29616;&#30340;&#28145;&#24230;&#23398;&#20064;&#38454;&#27573;&#25110;&#21518;&#22788;&#29702;&#38454;&#27573;&#25506;&#27979;&#23376;&#20195;&#25968;&#32467;&#26500;&#65292;&#24182;&#24212;&#29992;&#20110;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#30340;SU(3)&#21644;SU(5)&#38750;&#38463;&#36125;&#23572;&#35268;&#33539;&#23545;&#31216;&#24615;&#30340;&#33258;&#21457;&#30772;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#25104;&#21151;&#22320;&#29992;&#20110;&#25512;&#23548;&#20445;&#25345;&#37325;&#35201;&#29289;&#29702;&#37327;&#19981;&#21464;&#30340;&#23545;&#31216;&#24615;&#21464;&#25442;&#12290;&#36825;&#20123;&#25216;&#26415;&#23436;&#20840;&#19981;&#32771;&#34385;&#23545;&#24050;&#21457;&#29616;&#30340;&#23545;&#31216;&#24615;&#36827;&#34892;&#35782;&#21035;&#65292;&#23558;&#20854;&#25512;&#36831;&#21040;&#21518;&#32493;&#38454;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#26597;&#21644;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#23545;&#31216;&#24615;&#30340;&#32676;&#35770;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#23545;&#31216;&#24615;&#21457;&#29616;&#30340;&#28145;&#24230;&#23398;&#20064;&#38454;&#27573;&#25110;&#21518;&#32493;&#21518;&#22788;&#29702;&#38454;&#27573;&#25506;&#27979;&#23376;&#20195;&#25968;&#32467;&#26500;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;U(n)&#26446;&#32676;&#23478;&#26063;&#30340;&#20363;&#23376;&#35828;&#26126;&#20102;&#36825;&#20123;&#26032;&#26041;&#27861;&#65292;&#33719;&#24471;&#30456;&#24212;&#30340;&#23376;&#20195;&#25968;&#20998;&#35299;&#12290;&#20316;&#20026;&#31890;&#23376;&#29289;&#29702;&#23398;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#38750;&#38463;&#36125;&#23572;&#35268;&#33539;&#23545;&#31216;&#24615;&#22914;SU(3)&#21644;SU(5)&#33258;&#21457;&#30772;&#32570;&#21518;&#35782;&#21035;&#27531;&#20313;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning was recently successfully used in deriving symmetry transformations that preserve important physics quantities. Being completely agnostic, these techniques postpone the identification of the discovered symmetries to a later stage. In this letter we propose methods for examining and identifying the group-theoretic structure of such machine-learned symmetries. We design loss functions which probe the subalgebra structure either during the deep learning stage of symmetry discovery or in a subsequent post-processing stage. We illustrate the new methods with examples from the U(n) Lie group family, obtaining the respective subalgebra decompositions. As an application to particle physics, we demonstrate the identification of the residual symmetries after the spontaneous breaking of non-Abelian gauge symmetries like SU(3) and SU(5) which are commonly used in model building.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#22266;&#23450;&#28857;&#20248;&#21270;&#31639;&#27861;&#25552;&#20379;&#39044;&#28909;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#38382;&#39064;&#21442;&#25968;&#30340;&#39044;&#28909;&#36215;&#22987;&#28857;&#65292;&#24182;&#36890;&#36807;&#22266;&#23450;&#28857;&#36845;&#20195;&#26469;&#20248;&#21270;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36845;&#20195;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.07835</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#20026;&#22266;&#23450;&#28857;&#20248;&#21270;&#31639;&#27861;&#39044;&#28909;
&lt;/p&gt;
&lt;p&gt;
Learning to Warm-Start Fixed-Point Optimization Algorithms. (arXiv:2309.07835v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#22266;&#23450;&#28857;&#20248;&#21270;&#31639;&#27861;&#25552;&#20379;&#39044;&#28909;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#38382;&#39064;&#21442;&#25968;&#30340;&#39044;&#28909;&#36215;&#22987;&#28857;&#65292;&#24182;&#36890;&#36807;&#22266;&#23450;&#28857;&#36845;&#20195;&#26469;&#20248;&#21270;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36845;&#20195;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#20026;&#22266;&#23450;&#28857;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#39044;&#28909;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#30001;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#23558;&#38382;&#39064;&#21442;&#25968;&#26144;&#23556;&#21040;&#39044;&#28909;&#36215;&#22987;&#28857;&#65292;&#28982;&#21518;&#36827;&#34892;&#19968;&#23450;&#25968;&#37327;&#30340;&#22266;&#23450;&#28857;&#36845;&#20195;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#22266;&#23450;&#28857;&#27531;&#24046;&#25110;&#19982;&#22522;&#20934;&#35299;&#30340;&#36317;&#31163;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20102;&#39044;&#28909;&#36215;&#22987;&#28857;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#19979;&#28216;&#25439;&#22833;&#12290;&#25105;&#20204;&#26550;&#26500;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#28857;&#26159;&#20854;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#20026;&#36816;&#34892;&#20219;&#24847;&#27493;&#25968;&#30340;&#22266;&#23450;&#28857;&#31639;&#27861;&#39044;&#27979;&#39044;&#28909;&#36215;&#22987;&#28857;&#65292;&#32780;&#19981;&#20165;&#38480;&#20110;&#23427;&#35757;&#32451;&#30340;&#27493;&#25968;&#12290;&#25105;&#20204;&#20026;&#24120;&#35265;&#30340;&#22266;&#23450;&#28857;&#31639;&#23376;&#31867;&#65288;&#25910;&#32553;&#12289;&#32447;&#24615;&#25910;&#25947;&#21644;&#24179;&#22343;&#20540;&#65289;&#25552;&#20379;&#20102;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#12290;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#25511;&#21046;&#12289;&#32479;&#35745;&#21644;&#20449;&#21495;&#22788;&#29702;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#25968;&#37327;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a machine-learning framework to warm-start fixed-point optimization algorithms. Our architecture consists of a neural network mapping problem parameters to warm starts, followed by a predefined number of fixed-point iterations. We propose two loss functions designed to either minimize the fixed-point residual or the distance to a ground truth solution. In this way, the neural network predicts warm starts with the end-to-end goal of minimizing the downstream loss. An important feature of our architecture is its flexibility, in that it can predict a warm start for fixed-point algorithms run for any number of steps, without being limited to the number of steps it has been trained on. We provide PAC-Bayes generalization bounds on unseen data for common classes of fixed-point operators: contractive, linearly convergent, and averaged. Applying this framework to well-known applications in control, statistics, and signal processing, we observe a significant reduction in the number
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26377;&#21521;&#25955;&#23556;&#33258;&#32534;&#30721;&#22120;&#65288;DSAE&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#32454;&#32990;&#20449;&#21495;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#26377;&#21521;&#29256;&#26412;&#30340;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#12289;&#33258;&#32534;&#30721;&#22120;&#30340;&#38750;&#32447;&#24615;&#38477;&#32500;&#29305;&#24615;&#21644;&#21452;&#26354;&#31354;&#38388;&#30340;&#20960;&#20309;&#23646;&#24615;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#32454;&#32990;&#20449;&#21495;&#32593;&#32476;&#20013;&#30340;&#28508;&#22312;&#20998;&#23618;&#20449;&#24687;&#65292;&#24182;&#22312;&#32454;&#32990;&#20449;&#21495;&#32593;&#32476;&#25512;&#26029;&#31561;&#31185;&#23398;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07813</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#32454;&#32990;&#20449;&#21495;&#20998;&#26512;&#30340;&#26377;&#21521;&#25955;&#23556;
&lt;/p&gt;
&lt;p&gt;
Directed Scattering for Knowledge Graph-based Cellular Signaling Analysis. (arXiv:2309.07813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26377;&#21521;&#25955;&#23556;&#33258;&#32534;&#30721;&#22120;&#65288;DSAE&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#32454;&#32990;&#20449;&#21495;&#20998;&#26512;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#26377;&#21521;&#29256;&#26412;&#30340;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#12289;&#33258;&#32534;&#30721;&#22120;&#30340;&#38750;&#32447;&#24615;&#38477;&#32500;&#29305;&#24615;&#21644;&#21452;&#26354;&#31354;&#38388;&#30340;&#20960;&#20309;&#23646;&#24615;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#32454;&#32990;&#20449;&#21495;&#32593;&#32476;&#20013;&#30340;&#28508;&#22312;&#20998;&#23618;&#20449;&#24687;&#65292;&#24182;&#22312;&#32454;&#32990;&#20449;&#21495;&#32593;&#32476;&#25512;&#26029;&#31561;&#31185;&#23398;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#21521;&#22270;&#26159;&#35768;&#22810;&#29616;&#35937;&#30340;&#33258;&#28982;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#31185;&#23398;&#30693;&#35782;&#22270;&#35889;&#65292;&#22914;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#25110;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#65292;&#29992;&#20110;&#23450;&#20041;&#32454;&#32990;&#20449;&#21495;&#20851;&#31995;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#28304;&#33410;&#28857;&#36890;&#24120;&#20855;&#26377;&#19982;&#27719;&#33410;&#28857;&#19981;&#21516;&#30340;&#29983;&#29289;&#29289;&#29702;&#24615;&#36136;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#26377;&#24207;&#21644;&#21333;&#21521;&#20851;&#31995;&#65292;&#35768;&#22810;&#27492;&#31867;&#32593;&#32476;&#36824;&#20855;&#26377;&#20998;&#23618;&#21644;&#22810;&#23610;&#24230;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#25191;&#34892;&#33410;&#28857;&#32423;&#21644;&#36793;&#32423;&#20219;&#21153;&#26102;&#24182;&#19981;&#32771;&#34385;&#36825;&#20123;&#23646;&#24615;&#65292;&#22240;&#27492;&#23545;&#20110;&#32454;&#32990;&#20449;&#21495;&#32593;&#32476;&#25512;&#26029;&#31561;&#31185;&#23398;&#20219;&#21153;&#30340;&#21033;&#29992;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26377;&#21521;&#25955;&#23556;&#33258;&#32534;&#30721;&#22120;&#65288;DSAE&#65289;&#65292;&#23427;&#20351;&#29992;&#26377;&#21521;&#29256;&#26412;&#30340;&#20960;&#20309;&#25955;&#23556;&#21464;&#25442;&#65292;&#32467;&#21512;&#33258;&#32534;&#30721;&#22120;&#30340;&#38750;&#32447;&#24615;&#38477;&#32500;&#29305;&#24615;&#21644;&#21452;&#26354;&#31354;&#38388;&#30340;&#20960;&#20309;&#23646;&#24615;&#65292;&#26469;&#23398;&#20064;&#28508;&#22312;&#30340;&#20998;&#23618;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Directed graphs are a natural model for many phenomena, in particular scientific knowledge graphs such as molecular interaction or chemical reaction networks that define cellular signaling relationships. In these situations, source nodes typically have distinct biophysical properties from sinks. Due to their ordered and unidirectional relationships, many such networks also have hierarchical and multiscale structure. However, the majority of methods performing node- and edge-level tasks in machine learning do not take these properties into account, and thus have not been leveraged effectively for scientific tasks such as cellular signaling network inference. We propose a new framework called Directed Scattering Autoencoder (DSAE) which uses a directed version of a geometric scattering transform, combined with the non-linear dimensionality reduction properties of an autoencoder and the geometric properties of the hyperbolic space to learn latent hierarchies. We show this method outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#20013;&#24120;&#35265;&#30340;&#25490;&#38500;&#26631;&#20934;&#65292;&#36890;&#36807;&#24212;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#20998;&#31867;&#25490;&#38500;&#26631;&#20934;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19987;&#38376;&#20026;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.07812</link><description>&lt;p&gt;
&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text Classification of Cancer Clinical Trial Eligibility Criteria. (arXiv:2309.07812v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#20013;&#24120;&#35265;&#30340;&#25490;&#38500;&#26631;&#20934;&#65292;&#36890;&#36807;&#24212;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#20998;&#31867;&#25490;&#38500;&#26631;&#20934;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19987;&#38376;&#20026;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#38472;&#36848;&#65292;&#22240;&#27492;&#33258;&#21160;&#30830;&#23450;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#35797;&#39564;&#36164;&#26684;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#20010;&#28508;&#22312;&#26041;&#27861;&#26159;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#23545;&#24120;&#35265;&#31867;&#22411;&#30340;&#36164;&#26684;&#26631;&#20934;&#36827;&#34892;&#22788;&#29702;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#30284;&#30151;&#35797;&#39564;&#20013;&#30340;&#19971;&#20010;&#24120;&#35265;&#25490;&#38500;&#26631;&#20934;&#65306;&#20808;&#21069;&#24694;&#24615;&#32959;&#30244;&#12289;&#20154;&#31867;&#20813;&#30123;&#32570;&#38519;&#30149;&#27602;&#12289;&#20057;&#32925;&#30149;&#27602;&#12289;&#19993;&#32925;&#30149;&#27602;&#12289;&#31934;&#31070;&#30142;&#30149;&#12289;&#33647;&#29289;/&#29289;&#36136;&#28389;&#29992;&#21644;&#33258;&#36523;&#20813;&#30123;&#30142;&#30149;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;764&#20010;&#24102;&#26377;&#36825;&#20123;&#25490;&#38500;&#26631;&#20934;&#27880;&#37322;&#30340;&#19977;&#26399;&#30284;&#30151;&#35797;&#39564;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#24120;&#35265;&#30340;transformer&#27169;&#22411;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#30340;&#20020;&#24202;&#35797;&#39564;BERT&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#21160;&#20998;&#31867;&#24120;&#35265;&#30340;&#25490;&#38500;&#26631;&#20934;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#20020;&#24202;&#35797;&#39564;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#26631;&#20934;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic identification of clinical trials for which a patient is eligible is complicated by the fact that trial eligibility is stated in natural language. A potential solution to this problem is to employ text classification methods for common types of eligibility criteria. In this study, we focus on seven common exclusion criteria in cancer trials: prior malignancy, human immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness, drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase III cancer trials with these exclusions annotated at the trial level. We experiment with common transformer models as well as a new pre-trained clinical trial BERT model. Our results demonstrate the feasibility of automatically classifying common exclusion criteria. Additionally, we demonstrate the value of a pre-trained language model specifically for clinical trials, which yields the highest average performance across all criteria.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25238;&#21160;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#30340;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#20351;&#29992;&#20943;&#27861;&#25238;&#21160;&#30340;&#37327;&#21270;&#26041;&#26696;&#65292;&#21487;&#20197;&#22797;&#21046;&#27491;&#24120;&#22122;&#22768;&#28155;&#21152;&#36807;&#31243;&#65292;&#23454;&#29616;&#19982;&#23436;&#25972;&#31934;&#24230;&#26799;&#24230;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#25152;&#38656;&#36890;&#20449;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.07809</link><description>&lt;p&gt;
&#20351;&#29992;&#25238;&#21160;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#30340;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication Efficient Private Federated Learning Using Dithering. (arXiv:2309.07809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25238;&#21160;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#30340;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#20351;&#29992;&#20943;&#27861;&#25238;&#21160;&#30340;&#37327;&#21270;&#26041;&#26696;&#65292;&#21487;&#20197;&#22797;&#21046;&#27491;&#24120;&#22122;&#22768;&#28155;&#21152;&#36807;&#31243;&#65292;&#23454;&#29616;&#19982;&#23436;&#25972;&#31934;&#24230;&#26799;&#24230;&#26041;&#27861;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22823;&#24133;&#20943;&#23569;&#25152;&#38656;&#36890;&#20449;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#20445;&#25252;&#38544;&#31169;&#21644;&#30830;&#20445;&#39640;&#25928;&#36890;&#20449;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#26412;&#25991;&#22312;&#21487;&#20449;&#30340;&#32858;&#21512;&#22120;&#27169;&#22411;&#20013;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#21516;&#26102;&#36798;&#21040;&#20004;&#20010;&#30446;&#26631;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23458;&#25143;&#31471;&#20351;&#29992;&#22522;&#20110;&#20943;&#27861;&#25238;&#21160;&#30340;&#37327;&#21270;&#26041;&#26696;&#21487;&#20197;&#26377;&#25928;&#22320;&#22797;&#21046;&#32858;&#21512;&#22120;&#20013;&#30340;&#27491;&#24120;&#22122;&#22768;&#28155;&#21152;&#36807;&#31243;&#12290;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#22312;&#19982;&#20854;&#20182;&#23458;&#25143;&#31471;&#30456;&#27604;&#20445;&#35777;&#30456;&#21516;&#27700;&#24179;&#30340;&#24046;&#20998;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#22823;&#22823;&#20943;&#23569;&#25152;&#38656;&#30340;&#36890;&#20449;&#37327;&#65292;&#32780;&#19981;&#26159;&#20256;&#36755;&#23436;&#25972;&#31934;&#24230;&#30340;&#26799;&#24230;&#24182;&#20351;&#29992;&#20013;&#24515;&#22122;&#22768;&#28155;&#21152;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#19982;&#23436;&#25972;&#31934;&#24230;&#26799;&#24230;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of preserving privacy while ensuring efficient communication is a fundamental challenge in federated learning. In this work, we tackle this challenge in the trusted aggregator model, and propose a solution that achieves both objectives simultaneously. We show that employing a quantization scheme based on subtractive dithering at the clients can effectively replicate the normal noise addition process at the aggregator. This implies that we can guarantee the same level of differential privacy against other clients while substantially reducing the amount of communication required, as opposed to transmitting full precision gradients and using central noise addition. We also experimentally demonstrate that the accuracy of our proposed approach matches that of the full precision gradient method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07808</link><description>&lt;p&gt;
&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#30340;&#20851;&#38190;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20840;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#65292;&#22312;&#36825;&#31181;&#25216;&#26415;&#20013;&#65292;&#25972;&#20010;&#39550;&#39542;&#27969;&#31243;&#34987;&#26367;&#25442;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#30001;&#20110;&#20854;&#32467;&#26500;&#31616;&#21333;&#21644;&#25512;&#29702;&#26102;&#38388;&#24555;&#65292;&#22240;&#27492;&#21464;&#24471;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#39550;&#39542;&#27969;&#31243;&#20013;&#30340;&#32452;&#20214;&#65292;&#20294;&#20854;&#31616;&#21333;&#24615;&#20063;&#23548;&#33268;&#35299;&#37322;&#24615;&#38382;&#39064;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#35757;&#32451;&#24471;&#21040;&#30340;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#20132;&#36890;&#35268;&#21017;&#65292;&#21516;&#26102;&#20063;&#24456;&#38590;&#21457;&#29616;&#20854;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#22240;&#20026;&#32570;&#20047;&#20013;&#38388;&#36755;&#20986;&#12290;&#21516;&#26102;&#65292;&#20256;&#24863;&#22120;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#34892;&#24615;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#24863;&#30693;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
More research attention has recently been given to end-to-end autonomous driving technologies where the entire driving pipeline is replaced with a single neural network because of its simpler structure and faster inference time. Despite this appealing approach largely reducing the components in driving pipeline, its simplicity also leads to interpretability problems and safety issues arXiv:2003.06404. The trained policy is not always compliant with the traffic rules and it is also hard to discover the reason for the misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are also critical to autonomous driving's security and feasibility to perceive the surrounding environment under complex driving scenarios. In this paper, we proposed P-CSG, a novel penalty-based imitation learning approach with cross semantics generation sensor fusion technologies to increase the overall performance of End-to-End Autonomous Driving. We conducted an assessment of our model's perform
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;-&#25991;&#26412;&#36741;&#21161;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#65292;&#36890;&#36807;&#20004;&#20010;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#23545;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#35843;&#25972;&#65292;&#25429;&#25417;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#21644;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.07794</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#20687;-&#25991;&#26412;&#36741;&#21161;&#20219;&#21153;&#25552;&#39640;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Improving Multimodal Classification of Social Media Posts by Leveraging Image-Text Auxiliary tasks. (arXiv:2309.07794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22270;&#20687;-&#25991;&#26412;&#36741;&#21161;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#65292;&#36890;&#36807;&#20004;&#20010;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#23545;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#35843;&#25972;&#65292;&#25429;&#25417;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#21644;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#23545;&#24773;&#24863;&#20998;&#26512;&#12289;&#35773;&#21050;&#26816;&#27979;&#21644;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#31561;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21305;&#37197;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#23384;&#22312;&#38544;&#34255;&#25110;&#20114;&#34917;&#20449;&#24687;&#30340;&#29420;&#29305;&#36328;&#27169;&#24577;&#35821;&#20041;&#65292;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22312;&#24494;&#35843;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26102;&#32852;&#21512;&#20351;&#29992;&#20004;&#20010;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#26469;&#30452;&#25509;&#24314;&#27169;&#36825;&#19968;&#38382;&#39064;&#12290;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#65288;ITC&#65289;&#23558;&#19968;&#31687;&#24086;&#23376;&#30340;&#22270;&#20687;-&#25991;&#26412;&#34920;&#31034;&#26356;&#21152;&#38752;&#36817;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#24086;&#23376;&#20998;&#31163;&#24320;&#26469;&#65292;&#25429;&#25417;&#24213;&#23618;&#20381;&#36182;&#20851;&#31995;&#12290;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#65288;ITM&#65289;&#36890;&#36807;&#24809;&#32602;&#19981;&#30456;&#20851;&#30340;&#23545;&#26469;&#20419;&#36827;&#29702;&#35299;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#30446;&#26631;&#19982;&#20116;&#20010;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#22312;&#22235;&#20010;&#28909;&#38376;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#19978;&#30340;&#19968;&#33268;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively leveraging multimodal information from social media posts is essential to various downstream tasks such as sentiment analysis, sarcasm detection and hate speech classification. However, combining text and image information is challenging because of the idiosyncratic cross-modal semantics with hidden or complementary information present in matching image-text pairs. In this work, we aim to directly model this by proposing the use of two auxiliary losses jointly with the main task when fine-tuning any pre-trained multimodal model. Image-Text Contrastive (ITC) brings image-text representations of a post closer together and separates them from different posts, capturing underlying dependencies. Image-Text Matching (ITM) facilitates the understanding of semantic correspondence between images and text by penalizing unrelated pairs. We combine these objectives with five multimodal models, demonstrating consistent improvements across four popular social media datasets. Furthermore,
&lt;/p&gt;</description></item><item><title>Virchow&#26159;&#19968;&#20010;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.07778</link><description>&lt;p&gt;
Virchow: &#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Virchow: A Million-Slide Digital Pathology Foundation Model. (arXiv:2309.07778v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07778
&lt;/p&gt;
&lt;p&gt;
Virchow&#26159;&#19968;&#20010;&#25968;&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25968;&#30334;&#19975;&#24352;&#20840;&#25968;&#23383;&#30149;&#29702;&#23398;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#35757;&#32451;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#20013;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#30149;&#29702;&#23398;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20998;&#26512;&#20840;&#25968;&#23383;&#20999;&#29255;&#22270;&#20687;&#23454;&#29616;&#31934;&#20934;&#21307;&#23398;&#21644;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#26377;&#28508;&#21147;&#24443;&#24213;&#25913;&#21464;&#30284;&#30151;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#23545;&#20110;&#35768;&#22810;&#29305;&#23450;&#30340;&#35745;&#31639;&#30149;&#29702;&#23398;&#20219;&#21153;&#65292;&#25968;&#25454;&#37327;&#19981;&#36275;&#20197;&#36827;&#34892;&#24320;&#21457;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Virchow&#65292;&#19968;&#20010;632&#30334;&#19975;&#21442;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#30149;&#29702;&#23398;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;Virchow&#22312;1.5&#30334;&#19975;&#20010;&#19981;&#21516;&#32452;&#32455;&#26679;&#26412;&#30340;&#33487;&#26408;&#31934;&#21644;&#20234;&#32418;&#26579;&#33394;&#20840;&#25968;&#23383;&#20999;&#29255;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#25968;&#25454;&#37327;&#22823;&#24471;&#22810;&#12290;&#22312;&#21253;&#25324;&#29926;&#29255;&#32423;&#20840;&#30284;&#26816;&#27979;&#21644;&#20122;&#22411;&#20197;&#21450;&#24187;&#28783;&#29255;&#32423;&#29983;&#29289;&#26631;&#24535;&#29289;&#39044;&#27979;&#22312;&#20869;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;Virchow&#22312;&#26469;&#33258;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20154;&#32676;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#21644;&#22806;&#37096;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#22343;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational pathology uses artificial intelligence to enable precision medicine and decision support systems through the analysis of whole slide images. It has the potential to revolutionize the diagnosis and treatment of cancer. However, a major challenge to this objective is that for many specific computational pathology tasks the amount of data is inadequate for development. To address this challenge, we created Virchow, a 632 million parameter deep neural network foundation model for computational pathology. Using self-supervised learning, Virchow is trained on 1.5 million hematoxylin and eosin stained whole slide images from diverse tissue groups, which is orders of magnitude more data than previous works. When evaluated on downstream tasks including tile-level pan-cancer detection and subtyping and slide-level biomarker prediction, Virchow outperforms state-of-the-art systems both on internal datasets drawn from the same population as the pretraining data as well as external pu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#22686;&#24378;&#30340;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22024;&#26434;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#19978;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#31639;&#27861;&#21033;&#29992;&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#35299;&#20915;&#20102;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#32447;&#24615;&#26041;&#31243;&#32452;&#65292;&#24182;&#22312;Iris&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#36890;&#36807;&#32463;&#20856;&#35745;&#31639;&#21644;&#37327;&#23376;&#35745;&#31639;&#30340;&#31574;&#30053;&#24615;&#32452;&#21512;&#65292;&#31639;&#27861;&#22312;&#19981;&#21516;&#32500;&#24230;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#20102;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07770</link><description>&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#22686;&#24378;&#30340;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Variational Quantum Linear Solver enhanced Quantum Support Vector Machine. (arXiv:2309.07770v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07770
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#22686;&#24378;&#30340;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22024;&#26434;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#19978;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#31639;&#27861;&#21033;&#29992;&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#35299;&#20915;&#20102;&#26368;&#23567;&#20108;&#20056;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#32447;&#24615;&#26041;&#31243;&#32452;&#65292;&#24182;&#22312;Iris&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#36890;&#36807;&#32463;&#20856;&#35745;&#31639;&#21644;&#37327;&#23376;&#35745;&#31639;&#30340;&#31574;&#30053;&#24615;&#32452;&#21512;&#65292;&#31639;&#27861;&#22312;&#19981;&#21516;&#32500;&#24230;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#29616;&#20986;&#20102;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#20351;&#29992;&#37327;&#23376;&#36164;&#28304;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65288;&#22914;&#20998;&#31867;&#65289;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#22024;&#26434;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#30340;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#65288;VQLS&#65289;&#22686;&#24378;&#30340;&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#12290;&#36825;&#26159;&#22522;&#20110;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#37327;&#23376;&#32447;&#24615;&#27714;&#35299;&#22120;&#22312;NISQ&#35774;&#22791;&#19978;&#35299;&#20915;&#26368;&#23567;&#20108;&#20056;SVM&#30340;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#24605;&#24819;&#26500;&#24314;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;Iris&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#25968;&#20540;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29616;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19977;&#20010;&#19981;&#21516;&#30340;&#40482;&#23614;&#26893;&#29289;&#29289;&#31181;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#22312;&#20174;&#19968;&#32500;&#21040;&#19971;&#32500;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#31867;&#30340;&#20998;&#31867;&#22120;&#65292;&#25506;&#32034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#21033;&#29992;&#32463;&#20856;&#35745;&#31639;&#21644;&#37327;&#23376;&#35745;&#31639;&#23545;&#21508;&#31181;&#23376;&#31243;&#24207;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;NISQ&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;QSVM&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum Support Vector Machines (QSVM) play a vital role in using quantum resources for supervised machine learning tasks, such as classification. However, current methods are strongly limited in terms of scalability on Noisy Intermediate Scale Quantum (NISQ) devices. In this work, we propose a novel approach called the Variational Quantum Linear Solver (VQLS) enhanced QSVM. This is built upon our idea of utilizing the variational quantum linear solver to solve system of linear equations of a least squares-SVM on a NISQ device. The implementation of our approach is evaluated by an extensive series of numerical experiments with the Iris dataset, which consists of three distinct iris plant species. Based on this, we explore the practicality and effectiveness of our algorithm by constructing a classifier capable of classification in a feature space ranging from one to seven dimensions. Furthermore, by strategically exploiting both classical and quantum computing for various subroutines of
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07760</link><description>&lt;p&gt;
PRE: &#35270;&#35273;-&#35821;&#35328;&#25552;&#31034;&#23398;&#20064;&#19982;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07760
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PRE&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32534;&#30721;&#22120;&#26469;&#22686;&#24378;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#38646;&#26679;&#26412;&#36801;&#31227;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#38656;&#35201;&#25163;&#21160;&#36873;&#25321;&#25552;&#31034;&#20197;&#25913;&#36827;&#19979;&#28216;&#22270;&#20687;&#20998;&#24067;&#21644;&#25991;&#26412;&#31867;&#25551;&#36848;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#36825;&#31181;&#25163;&#21160;&#25552;&#31034;&#24037;&#31243;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#21040;&#23454;&#36341;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#24182;&#19988;&#38750;&#24120;&#32791;&#26102;&#12290;&#20026;&#20102;&#36991;&#20813;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#26368;&#36817;&#30340;CoOp&#24037;&#20316;&#24341;&#20837;&#20102;&#22312;&#35270;&#35273;&#39046;&#22495;&#20351;&#29992;&#21487;&#25511;&#25991;&#26412;&#26631;&#35760;&#30340;&#25552;&#31034;&#23398;&#20064;&#27010;&#24565;&#12290;&#34429;&#28982;CoOp&#21487;&#20197;&#22312;&#25163;&#21160;&#25552;&#31034;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#20294;&#20854;&#23398;&#21040;&#30340;&#19978;&#19979;&#25991;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#20013;&#26356;&#24191;&#27867;&#30340;&#26410;&#35265;&#31867;&#21035;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Learning with Reparameterization Encoder (PRE) &#30340;&#31616;&#21333;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#21487;&#23398;&#20064;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained vision-language models such as CLIP have demonstrated great potential in zero-shot transferability to downstream tasks. However, to attain optimal performance, the manual selection of prompts is necessary to improve alignment between the downstream image distribution and the textual class descriptions. This manual prompt engineering is the major challenge for deploying such models in practice since it requires domain expertise and is extremely time-consuming. To avoid non-trivial prompt engineering, recent work Context Optimization (CoOp) introduced the concept of prompt learning to the vision domain using learnable textual tokens. While CoOp can achieve substantial improvements over manual prompts, its learned context is worse generalizable to wider unseen classes within the same dataset. In this work, we present Prompt Learning with Reparameterization Encoder (PRE) - a simple and efficient method that enhances the generalization ability of the learnable prompt to un
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#24615;&#34920;&#31034;&#23398;&#20064;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#22312;&#20154;&#31867;&#22240;&#32032;&#20013;&#36827;&#34892;&#24314;&#27169;&#21644;&#25805;&#20316;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#33719;&#21462;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#36866;&#29992;&#20110;&#20107;&#21518;&#35299;&#37322;&#32773;&#21644;&#22522;&#20110;&#27010;&#24565;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.07742</link><description>&lt;p&gt;
&#35299;&#35835;&#23384;&#22312;&#20110;&#35266;&#23519;&#32773;&#30340;&#24515;&#20013;&#65306;&#29992;&#20110;&#21487;&#35299;&#37322;&#24615;&#34920;&#31034;&#23398;&#20064;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Interpretability is in the Mind of the Beholder: A Causal Framework for Human-interpretable Representation Learning. (arXiv:2309.07742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07742
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#34920;&#31034;&#23398;&#20064;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#22312;&#20154;&#31867;&#22240;&#32032;&#20013;&#36827;&#34892;&#24314;&#27169;&#21644;&#25805;&#20316;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#33719;&#21462;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#65292;&#36866;&#29992;&#20110;&#20107;&#21518;&#35299;&#37322;&#32773;&#21644;&#22522;&#20110;&#27010;&#24565;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#28857;&#27491;&#22312;&#20174;&#20197;&#36755;&#20837;&#29305;&#24449;&#20026;&#22522;&#30784;&#23450;&#20041;&#30340;&#35299;&#37322;&#65292;&#36716;&#21521;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#27010;&#24565;&#20026;&#22522;&#30784;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21487;&#38752;&#22320;&#33719;&#24471;&#36825;&#20123;&#27010;&#24565;&#20173;&#28982;&#22522;&#26412;&#19981;&#28165;&#26970;&#12290;&#32570;&#23569;&#23545;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#33268;&#24615;&#27010;&#24565;&#65292;&#23548;&#33268;&#20107;&#21518;&#35299;&#37322;&#32773;&#21644;&#22522;&#20110;&#27010;&#24565;&#30340;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#30340;&#27010;&#24565;&#36890;&#36807;&#22810;&#31181;&#30456;&#20114;&#19981;&#20860;&#23481;&#30340;&#31574;&#30053;&#33719;&#24471;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#20123;&#31574;&#30053;&#20013;&#22823;&#22810;&#25968;&#24573;&#35270;&#20102;&#20154;&#31867;&#38382;&#39064;&#38754;&#65306;&#19968;&#20010;&#34920;&#31034;&#21482;&#26377;&#22312;&#34987;&#25509;&#25910;&#30340;&#20154;&#31867;&#33021;&#22815;&#29702;&#35299;&#30340;&#31243;&#24230;&#19978;&#25165;&#33021;&#29702;&#35299;&#12290;&#22312;&#20154;&#31867;&#21487;&#35299;&#37322;&#34920;&#31034;&#23398;&#20064;&#65288;HRL&#65289;&#20013;&#65292;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#23545;&#36825;&#20010;&#20154;&#31867;&#22240;&#32032;&#36827;&#34892;&#24314;&#27169;&#21644;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#33719;&#21462;&#36866;&#29992;&#20110;&#20107;&#21518;&#35299;&#37322;&#32773;&#21644;&#22522;&#20110;&#27010;&#24565;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#12290;&#25105;&#20204;&#23545;HRL&#30340;&#24418;&#24335;&#21270;&#24314;&#27169;...
&lt;/p&gt;
&lt;p&gt;
Focus in Explainable AI is shifting from explanations defined in terms of low-level elements, such as input features, to explanations encoded in terms of interpretable concepts learned from data. How to reliably acquire such concepts is, however, still fundamentally unclear. An agreed-upon notion of concept interpretability is missing, with the result that concepts used by both post-hoc explainers and concept-based neural networks are acquired through a variety of mutually incompatible strategies. Critically, most of these neglect the human side of the problem: a representation is understandable only insofar as it can be understood by the human at the receiving end. The key challenge in Human-interpretable Representation Learning (HRL) is how to model and operationalize this human element. In this work, we propose a mathematical framework for acquiring interpretable representations suitable for both post-hoc explainers and concept-based neural networks. Our formalization of HRL builds 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;V-nets&#65289;&#30340;&#24191;&#27867;&#26694;&#26550;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#19982;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.07716</link><description>&lt;p&gt;
&#29702;&#35299;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#19982;&#23454;&#25968;&#21644;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Understanding Vector-Valued Neural Networks and Their Relationship with Real and Hypercomplex-Valued Neural Networks. (arXiv:2309.07716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;V-nets&#65289;&#30340;&#24191;&#27867;&#26694;&#26550;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#19982;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22810;&#32500;&#20449;&#21495;&#21644;&#22270;&#20687;&#22788;&#29702;&#26041;&#38754;&#26377;&#35768;&#22810;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#20294;&#22823;&#22810;&#25968;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#30001;&#65288;&#22810;&#32500;&#65289;&#23454;&#25968;&#25968;&#32452;&#34920;&#31034;&#30340;&#25968;&#25454;&#12290;&#29305;&#24449;&#36890;&#36947;&#20043;&#38388;&#30340;&#20114;&#30456;&#20851;&#36890;&#24120;&#34987;&#26399;&#26395;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#21644;&#20180;&#32454;&#30340;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#34987;&#35774;&#35745;&#25104;&#22788;&#29702;&#21521;&#37327;&#25968;&#32452;&#65292;&#24182;&#33258;&#28982;&#22320;&#32771;&#34385;&#29305;&#24449;&#36890;&#36947;&#20043;&#38388;&#30340;&#20114;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#36890;&#24120;&#27604;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#24378;&#30340;&#35757;&#32451;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#20010;&#24191;&#27867;&#30340;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#31216;&#20026;V-nets&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#36229;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#34987;&#35270;&#20026;&#20855;&#26377;&#39069;&#22806;&#20195;&#25968;&#23646;&#24615;&#30340;&#21521;&#37327;&#20540;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#35299;&#37322;&#20102;&#21521;&#37327;&#20540;&#31070;&#32463;&#32593;&#32476;&#19982;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the many successful applications of deep learning models for multidimensional signal and image processing, most traditional neural networks process data represented by (multidimensional) arrays of real numbers. The intercorrelation between feature channels is usually expected to be learned from the training data, requiring numerous parameters and careful training. In contrast, vector-valued neural networks are conceived to process arrays of vectors and naturally consider the intercorrelation between feature channels. Consequently, they usually have fewer parameters and often undergo more robust training than traditional neural networks. This paper aims to present a broad framework for vector-valued neural networks, referred to as V-nets. In this context, hypercomplex-valued neural networks are regarded as vector-valued models with additional algebraic properties. Furthermore, this paper explains the relationship between vector-valued and traditional neural networks. Precisely, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#20855;&#26377;&#24066;&#22330;&#21160;&#24577;&#12289;&#32929;&#31080;&#20195;&#30721;&#21644;&#21382;&#21490;&#29366;&#24577;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#19978;&#19979;&#25991;&#24066;&#22330;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#23454;&#29616;&#23545;&#37329;&#34701;&#25968;&#25454;&#29983;&#25104;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.07708</link><description>&lt;p&gt;
&#28155;&#21152;&#35821;&#20041;&#19978;&#19979;&#25991;&#30340;&#25511;&#21046;&#21147;&#37327;&#65292;&#20026;&#37329;&#34701;&#24066;&#22330;&#25968;&#25454;&#29983;&#25104;&#24341;&#20837;Market-GAN
&lt;/p&gt;
&lt;p&gt;
Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context. (arXiv:2309.07708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#20855;&#26377;&#24066;&#22330;&#21160;&#24577;&#12289;&#32929;&#31080;&#20195;&#30721;&#21644;&#21382;&#21490;&#29366;&#24577;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#19978;&#19979;&#25991;&#24066;&#22330;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#23454;&#29616;&#23545;&#37329;&#34701;&#25968;&#25454;&#29983;&#25104;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#27169;&#25311;&#22120;&#22312;&#25552;&#21319;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#31649;&#29702;&#39118;&#38505;&#21644;&#20419;&#36827;&#25112;&#30053;&#37329;&#34701;&#20915;&#31574;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#37329;&#34701;&#24066;&#22330;&#27169;&#25311;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#26694;&#26550;&#24120;&#24120;&#38590;&#20197;&#36866;&#24212;&#19987;&#38376;&#30340;&#27169;&#25311;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#23558;&#25361;&#25112;&#24402;&#32467;&#20026;&#65306;i&#65289;&#24403;&#21069;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19981;&#21253;&#21547;&#19978;&#19979;&#25991;&#26631;&#31614;&#65307;ii&#65289;&#24403;&#21069;&#30340;&#25216;&#26415;&#27809;&#26377;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#25511;&#21046;&#30340;&#37329;&#34701;&#25968;&#25454;&#65292;&#19982;&#20854;&#20182;&#27169;&#24577;&#30456;&#27604;&#65292;&#36825;&#35201;&#27714;&#26356;&#39640;&#30340;&#31934;&#24230;&#65307;iii&#65289;&#30001;&#20110;&#37329;&#34701;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#12289;&#22122;&#22768;&#24615;&#36136;&#65292;&#29983;&#25104;&#19982;&#19978;&#19979;&#25991;&#23545;&#40784;&#12289;&#39640;&#20445;&#30495;&#24230;&#30340;&#25968;&#25454;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#65306;i&#65289;&#25552;&#20986;&#20102;&#20855;&#26377;&#24066;&#22330;&#21160;&#24577;&#12289;&#32929;&#31080;&#20195;&#30721;&#21644;&#21382;&#21490;&#29366;&#24577;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#19978;&#19979;&#25991;&#24066;&#22330;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#32447;&#24615;&#22238;&#24402;&#21644;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#32858;&#31867;&#32467;&#21512;&#30340;&#24066;&#22330;&#21160;&#24577;&#24314;&#27169;&#26041;&#27861;&#25552;&#21462;&#24066;&#22330;&#21160;&#24577;&#65307;ii&#65289;&#25105;&#20204;&#39044;&#20808;&#20934;&#22791;&#20102;Market-GAN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#20197;&#21450;&#32534;&#30721;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#23545;&#37329;&#34701;&#25968;&#25454;&#29983;&#25104;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial simulators play an important role in enhancing forecasting accuracy, managing risks, and fostering strategic financial decision-making. Despite the development of financial market simulation methodologies, existing frameworks often struggle with adapting to specialized simulation context. We pinpoint the challenges as i) current financial datasets do not contain context labels; ii) current techniques are not designed to generate financial data with context as control, which demands greater precision compared to other modalities; iii) the inherent difficulties in generating context-aligned, high-fidelity data given the non-stationary, noisy nature of financial data. To address these challenges, our contributions are: i) we proposed the Contextual Market Dataset with market dynamics, stock ticker, and history state as context, leveraging a market dynamics modeling method that combines linear regression and Dynamic Time Warping clustering to extract market dynamics; ii) we prese
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22240;&#26524;&#32467;&#26500;&#30340;&#20449;&#24687;&#35770;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#26576;&#20010;&#29305;&#23450;&#32467;&#26524;&#21464;&#37327;&#30340;&#22240;&#26524;&#37325;&#35201;&#24615;&#65292;&#35299;&#20915;&#20102;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07703</link><description>&lt;p&gt;
&#27979;&#37327;&#22240;&#26524;&#25511;&#21046;&#30340;&#22240;&#26524;&#29109;&#21644;&#20449;&#24687;&#22686;&#30410;
&lt;/p&gt;
&lt;p&gt;
Causal Entropy and Information Gain for Measuring Causal Control. (arXiv:2309.07703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22240;&#26524;&#32467;&#26500;&#30340;&#20449;&#24687;&#35770;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#26576;&#20010;&#29305;&#23450;&#32467;&#26524;&#21464;&#37327;&#30340;&#22240;&#26524;&#37325;&#35201;&#24615;&#65292;&#35299;&#20915;&#20102;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21644;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#22240;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;&#23613;&#31649;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#65288;IML&#65289;&#26041;&#27861;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#23558;&#37325;&#35201;&#24615;&#36171;&#20104;&#37027;&#20123;&#23545;&#32467;&#26524;&#21464;&#37327;&#27809;&#26377;&#22240;&#26524;&#24433;&#21709;&#30340;&#29305;&#24449;&#12290;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#25110;&#20043;&#21518;&#65292;&#36873;&#25321;&#22240;&#26524;&#30456;&#20851;&#30340;&#29305;&#24449;&#23558;&#25552;&#20379;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#21033;&#29992;&#20449;&#24687;&#35770;&#37327;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#32479;&#35745;&#30456;&#20851;&#29305;&#24449;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#25152;&#22522;&#20110;&#30340;&#20449;&#24687;&#35770;&#37327;&#19981;&#21253;&#21547;&#22240;&#26524;&#20851;&#31995;&#65292;&#22240;&#27492;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#33021;&#22815;&#32771;&#34385;&#31995;&#32479;&#22240;&#26524;&#32467;&#26500;&#30340;&#20449;&#24687;&#35770;&#37327;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#26576;&#20010;&#32473;&#23450;&#32467;&#26524;&#21464;&#37327;&#30340;&#22240;&#26524;&#37325;&#35201;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22240;&#26524;&#29109;&#21644;&#22240;&#26524;&#20114;&#20449;&#24687;&#30340;&#22240;&#26524;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence models and methods commonly lack causal interpretability. Despite the advancements in interpretable machine learning (IML) methods, they frequently assign importance to features which lack causal influence on the outcome variable. Selecting causally relevant features among those identified as relevant by these methods, or even before model training, would offer a solution. Feature selection methods utilizing information theoretical quantities have been successful in identifying statistically relevant features. However, the information theoretical quantities they are based on do not incorporate causality, rendering them unsuitable for such scenarios. To address this challenge, this article proposes information theoretical quantities that incorporate the causal structure of the system, which can be used to evaluate causal importance of features for some given outcome variable. Specifically, we introduce causal versions of entropy and mutual information, termed cau
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65288;TouT&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#20002;&#24323;&#26469;&#37327;&#21270;&#20013;&#38388;&#27493;&#39588;&#19978;&#30340;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#21709;&#24212;&#30340;&#31934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07694</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tree of Uncertain Thoughts Reasoning for Large Language Models. (arXiv:2309.07694v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65288;TouT&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#20002;&#24323;&#26469;&#37327;&#21270;&#20013;&#38388;&#27493;&#39588;&#19978;&#30340;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#21709;&#24212;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#24341;&#20837;&#30340;&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65288;Tree of Thoughts, ToT&#65289;&#22312;&#20801;&#35768;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#39044;&#35265;&#21644;&#22238;&#28335;&#36827;&#34892;&#20840;&#23616;&#20915;&#31574;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#24573;&#35270;&#20102;&#20013;&#38388;&#20915;&#31574;&#28857;&#25110;&#8220;&#24605;&#32500;&#8221;&#20013;&#30340;&#22266;&#26377;&#23616;&#37096;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20123;&#22266;&#26377;&#30340;&#23616;&#37096;&#19981;&#30830;&#23450;&#24615;&#65292;&#30001;&#20110;LLMs&#28508;&#22312;&#30340;&#22810;&#26679;&#24615;&#21709;&#24212;&#33021;&#21147;&#65292;&#25104;&#20026;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24605;&#32500;&#26641;&#65288;Tree of Uncertain Thoughts, TouT&#65289;-&#19968;&#31181;&#38024;&#23545;LLMs&#35774;&#35745;&#30340;&#25512;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;TouT&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#33945;&#29305;&#21345;&#27931;&#20002;&#24323;(Monte Carlo Dropout)&#26469;&#37327;&#21270;&#19982;LLMs&#22312;&#36825;&#20123;&#20013;&#38388;&#27493;&#39588;&#19978;&#30340;&#19981;&#21516;&#26412;&#22320;&#21709;&#24212;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#24471;&#20998;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#26412;&#22320;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#19982;&#20840;&#23616;&#25628;&#32034;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;TouT&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#21709;&#24212;&#30340;&#31934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20004;&#20010;&#33499;&#21051;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#36827;&#34892;&#20005;&#26684;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;24&#28857;&#28216;&#25103;&#21644;&#36855;&#20320;&#22635;&#23383;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or "thoughts". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model's precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DenseNet&#30340;EEG&#36741;&#21161;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;EEG&#30005;&#26497;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#20102;&#26102;&#31354;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#35299;&#30721;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07690</link><description>&lt;p&gt;
&#22522;&#20110;DenseNet&#30340;EEG&#36741;&#21161;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A DenseNet-based method for decoding auditory spatial attention with EEG. (arXiv:2309.07690v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DenseNet&#30340;EEG&#36741;&#21161;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;EEG&#30005;&#26497;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#20102;&#26102;&#31354;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#35299;&#30721;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#26816;&#27979;&#65288;ASAD&#65289;&#26088;&#22312;&#21033;&#29992;EEG&#22312;&#22810;&#20154;&#28436;&#35762;&#32773;&#29615;&#22659;&#20013;&#35299;&#30721;&#34987;&#27880;&#24847;&#30340;&#31354;&#38388;&#20301;&#32622;&#12290;ASAD&#26041;&#27861;&#21463;&#21040;&#22823;&#33041;&#30382;&#23618;&#31070;&#32463;&#20803;&#22312;&#22788;&#29702;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#26102;&#30340;&#20391;&#21270;&#21551;&#21457;&#65292;&#24182;&#22312;&#31070;&#32463;&#35760;&#24405;&#30340;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#65288;AAD&#65289;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#22312;&#20197;&#24448;&#30340;ASAD&#26041;&#27861;&#20013;&#65292;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;EEG&#30005;&#26497;&#30340;&#31354;&#38388;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;EEG&#20449;&#36947;&#36716;&#25442;&#20026;&#20108;&#32500;&#31354;&#38388;&#25299;&#25169;&#22270;&#65292;&#23558;EEG&#25968;&#25454;&#36716;&#25442;&#20026;&#21253;&#21547;&#26102;&#31354;&#20449;&#24687;&#30340;&#19977;&#32500;&#25490;&#21015;&#12290;&#28982;&#21518;&#20351;&#29992;&#19977;&#32500;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DenseNet-3D&#65289;&#26469;&#25552;&#21462;&#25152;&#20851;&#27880;&#20301;&#32622;&#30340;&#31070;&#32463;&#34920;&#24449;&#30340;&#26102;&#31354;&#29305;&#24449;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#20256;&#32479;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#35299;&#30721;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auditory spatial attention detection (ASAD) aims to decode the attended spatial location with EEG in a multiple-speaker setting. ASAD methods are inspired by the brain lateralization of cortical neural responses during the processing of auditory spatial attention, and show promising performance for the task of auditory attention decoding (AAD) with neural recordings. In the previous ASAD methods, the spatial distribution of EEG electrodes is not fully exploited, which may limit the performance of these methods. In the present work, by transforming the original EEG channels into a two-dimensional (2D) spatial topological map, the EEG data is transformed into a three-dimensional (3D) arrangement containing spatial-temporal information. And then a 3D deep convolutional neural network (DenseNet-3D) is used to extract temporal and spatial features of the neural representation for the attended locations. The results show that the proposed method achieves higher decoding accuracy than the sta
&lt;/p&gt;</description></item><item><title>deepFDEnet&#26159;&#19968;&#31181;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#27714;&#35299;&#21508;&#31181;&#24418;&#24335;&#30340;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.07684</link><description>&lt;p&gt;
deepFDEnet: &#19968;&#20010;&#29992;&#20110;&#27714;&#35299;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
deepFDEnet: A Novel Neural Network Architecture for Solving Fractional Differential Equations. (arXiv:2309.07684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07684
&lt;/p&gt;
&lt;p&gt;
deepFDEnet&#26159;&#19968;&#31181;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#27714;&#35299;&#21508;&#31181;&#24418;&#24335;&#30340;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#27714;&#35299;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#35774;&#35745;&#20351;&#29992;&#20102;&#39640;&#26031;&#31215;&#20998;&#35268;&#21017;&#21644;$L_1$&#31163;&#25955;&#21270;&#25216;&#26415;&#12290;&#22312;&#27599;&#20010;&#26041;&#31243;&#20013;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36817;&#20284;&#26410;&#30693;&#20989;&#25968;&#12290;&#30740;&#31350;&#36824;&#23545;&#19977;&#20010;&#24418;&#24335;&#30340;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20102;&#26816;&#39564;&#65292;&#20197;&#31361;&#20986;&#35813;&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#65306;&#20998;&#25968;&#38454;&#24120;&#24494;&#20998;&#26041;&#31243;&#12289;&#20998;&#25968;&#38454;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#21644;&#20998;&#25968;&#38454;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#33021;&#22815;&#20197;&#26497;&#39640;&#30340;&#31934;&#24230;&#35299;&#20915;&#19981;&#21516;&#24418;&#24335;&#30340;&#20998;&#25968;&#38454;&#24494;&#20998;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary goal of this research is to propose a novel architecture for a deep neural network that can solve fractional differential equations accurately. A Gaussian integration rule and a $L_1$ discretization technique are used in the proposed design. In each equation, a deep neural network is used to approximate the unknown function. Three forms of fractional differential equations have been examined to highlight the method's versatility: a fractional ordinary differential equation, a fractional order integrodifferential equation, and a fractional order partial differential equation. The results show that the proposed architecture solves different forms of fractional differential equations with excellent precision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24212;&#29992;&#20110;&#30495;&#23454;&#37327;&#23376;&#35774;&#22791;&#30340;&#22810;&#31181;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#26469;&#21306;&#20998;&#22522;&#24577;&#21644;&#28608;&#21457;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.07679</link><description>&lt;p&gt;
&#29992;&#20110;&#37327;&#23376;&#24577;&#20998;&#31867;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking machine learning models for quantum state classification. (arXiv:2309.07679v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24212;&#29992;&#20110;&#30495;&#23454;&#37327;&#23376;&#35774;&#22791;&#30340;&#22810;&#31181;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#26469;&#21306;&#20998;&#22522;&#24577;&#21644;&#28608;&#21457;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;&#20449;&#24687;&#36890;&#36807;&#31216;&#20026;&#37327;&#23376;&#27604;&#29305;&#30340;&#20108;&#32423;&#37327;&#23376;&#24577;&#36827;&#34892;&#22788;&#29702;&#12290;&#30001;&#20110;&#22122;&#22768;&#21644;&#36864;&#30456;&#24178;&#29616;&#35937;&#65292;&#24403;&#21069;&#30340;&#37327;&#23376;&#27604;&#29305;&#29289;&#29702;&#23454;&#29616;&#38656;&#35201;&#32463;&#36807;&#31934;&#32454;&#30340;&#26657;&#20934;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#23454;&#39564;&#12290;&#22312;&#19981;&#21516;&#30340;&#34920;&#24449;&#23454;&#39564;&#20013;&#65292;&#20851;&#38190;&#27493;&#39588;&#20043;&#19968;&#26159;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#21306;&#20998;&#22522;&#24577;&#21644;&#28608;&#21457;&#24577;&#26469;&#23545;&#27979;&#37327;&#30340;&#24577;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24212;&#29992;&#20110;&#30495;&#23454;&#37327;&#23376;&#35774;&#22791;&#30340;&#22810;&#31181;&#20998;&#31867;&#25216;&#26415;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing is a growing field where the information is processed by two-levels quantum states known as qubits. Current physical realizations of qubits require a careful calibration, composed by different experiments, due to noise and decoherence phenomena. Among the different characterization experiments, a crucial step is to develop a model to classify the measured state by discriminating the ground state from the excited state. In this proceedings we benchmark multiple classification techniques applied to real quantum devices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807; emergent representation &#23454;&#29616;&#30446;&#26631;&#31354;&#38388;&#25277;&#35937;&#65292;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#20027;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#23553;&#24314;HRL&#31639;&#27861;&#26469;&#21516;&#26102;&#23398;&#20064;&#30446;&#26631;&#34920;&#31034;&#21644;&#20998;&#23618;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.07675</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#38598;&#21512;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#65292;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#30446;&#26631;&#31354;&#38388;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Goal Space Abstraction in Hierarchical Reinforcement Learning via Set-Based Reachability Analysis. (arXiv:2309.07675v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38598;&#21512;&#30340;&#21487;&#36798;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807; emergent representation &#23454;&#29616;&#30446;&#26631;&#31354;&#38388;&#25277;&#35937;&#65292;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#20027;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#23553;&#24314;HRL&#31639;&#27861;&#26469;&#21516;&#26102;&#23398;&#20064;&#30446;&#26631;&#34920;&#31034;&#21644;&#20998;&#23618;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#23398;&#20064;&#36890;&#36807;&#20351;&#29992;&#31526;&#21495;&#26041;&#27861;&#36827;&#34892;&#30446;&#26631;&#34920;&#31034;&#32780;&#33719;&#30410;&#33391;&#22810;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30693;&#35782;&#20197;&#36827;&#34892;&#39640;&#25928;&#21644;&#21487;&#20256;&#36882;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20381;&#36182;&#31526;&#21495;&#25512;&#29702;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;(HRL)&#26041;&#27861;&#36890;&#24120;&#21463;&#38480;&#20110;&#38656;&#35201;&#25163;&#21160;&#35774;&#32622;&#30446;&#26631;&#34920;&#31034;&#12290;&#33258;&#20027;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#30340;&#25361;&#25112;&#22312;&#20110;&#23427;&#24517;&#39035;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#65292;&#20363;&#22914;&#29615;&#22659;&#21160;&#21147;&#23398;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26032;&#20986;&#29616;&#30340;&#34920;&#31034;&#26469;&#23454;&#29616;&#30446;&#26631;&#21457;&#29616;&#30340;&#21457;&#23637;&#26426;&#21046;&#65292;&#35813;&#34920;&#31034;&#23558;&#20855;&#26377;&#31867;&#20284;&#20219;&#21153;&#20013;&#30340;&#35282;&#33394;&#30340;&#29615;&#22659;&#29366;&#24577;&#38598;&#21512;&#36827;&#34892;&#25277;&#35937;&#65288;&#21363;&#20998;&#32452;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21516;&#26102;&#23398;&#20064;&#30446;&#26631;&#34920;&#31034;&#21644;&#20998;&#23618;&#31574;&#30053;&#30340;&#23553;&#24314;HRL&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#31526;&#21495;&#21487;&#36798;&#24615;&#20998;&#26512;&#26469;&#36817;&#20284;&#29366;&#24577;&#38598;&#21512;&#20043;&#38388;&#30340;&#36807;&#28193;&#20851;&#31995;&#65292;&#24182;&#25913;&#36827;&#30446;&#26631;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this paper, we propose a developmental mechanism for goal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We introduce a Feudal HRL algorithm that concurrently learns both the goal representation and a hierarchical policy. The algorithm uses symbolic reachability analysis for neural networks to approximate the transition relation among sets of states and to refine the goal representation. We eval
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;R-DISCOVER&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26377;&#38480;&#19988;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#25581;&#31034;&#24320;&#25918;&#24335;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#31526;&#21495;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#24341;&#23548;&#19979;&#30340;&#28151;&#21512;PDE&#29983;&#25104;&#22120;&#65292;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#21457;&#29616;&#21644;&#23884;&#20837;PDE&#65292;&#24182;&#36873;&#25321;&#34920;&#29616;&#26368;&#20339;&#30340;PDE&#36827;&#34892;&#31995;&#32479;&#21709;&#24212;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.07672</link><description>&lt;p&gt;
&#20174;&#26377;&#38480;&#19988;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#20013;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#30340;&#31283;&#20581;&#23398;&#20064;&#24320;&#25918;&#24335;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained robust learning of open-form PDEs from limited and noisy data. (arXiv:2309.07672v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;R-DISCOVER&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#26377;&#38480;&#19988;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#25581;&#31034;&#24320;&#25918;&#24335;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#31526;&#21495;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#24341;&#23548;&#19979;&#30340;&#28151;&#21512;PDE&#29983;&#25104;&#22120;&#65292;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#21457;&#29616;&#21644;&#23884;&#20837;PDE&#65292;&#24182;&#36873;&#25321;&#34920;&#29616;&#26368;&#20339;&#30340;PDE&#36827;&#34892;&#31995;&#32479;&#21709;&#24212;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#26412;&#25511;&#21046;&#26041;&#31243;&#22312;&#36935;&#21040;&#22122;&#22768;&#35266;&#27979;&#21644;&#27809;&#26377;&#29616;&#25104;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;R-DISCOVER&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20174;&#26377;&#38480;&#21644;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#20013;&#31283;&#20581;&#22320;&#25581;&#31034;&#24320;&#25918;&#24335;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20004;&#20010;&#20132;&#26367;&#26356;&#26032;&#36807;&#31243;&#36827;&#34892;&#25805;&#20316;&#65306;&#21457;&#29616;&#21644;&#23884;&#20837;&#12290;&#21457;&#29616;&#38454;&#27573;&#21033;&#29992;&#31526;&#21495;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24341;&#23548;&#19979;&#30340;&#28151;&#21512;PDE&#29983;&#25104;&#22120;&#65292;&#39640;&#25928;&#22320;&#20135;&#29983;&#20855;&#26377;&#26641;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#27169;&#22411;&#36866;&#24212;&#31995;&#32479;&#21709;&#24212;&#24182;&#20316;&#20026;&#29983;&#25104;&#30340;PDE&#30340;&#22870;&#21169;&#35780;&#20272;&#22120;&#12290;&#21033;&#29992;&#25311;&#21512;&#25928;&#26524;&#36739;&#22909;&#30340;PDE&#36890;&#36807;RL&#26041;&#27861;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#29983;&#25104;&#22120;&#65292;&#24182;&#36890;&#36807;&#26080;&#21442;&#25968;&#31283;&#23450;&#24230;&#25351;&#26631;&#36873;&#25321;&#34920;&#29616;&#26368;&#20339;&#30340;PDE&#12290;&#23884;&#20837;&#38454;&#27573;&#23558;&#26368;&#21021;&#20174;&#21457;&#29616;&#36807;&#31243;&#20013;&#30830;&#23450;&#30340;PDE&#19982;&#36924;&#36817;&#30495;&#23454;&#31995;&#32479;&#36827;&#34892;&#39057;&#35889;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unveiling the underlying governing equations of nonlinear dynamic systems remains a significant challenge, especially when encountering noisy observations and no prior knowledge available. This study proposes R-DISCOVER, a framework designed to robustly uncover open-form partial differential equations (PDEs) from limited and noisy data. The framework operates through two alternating update processes: discovering and embedding. The discovering phase employs symbolic representation and a reinforcement learning (RL)-guided hybrid PDE generator to efficiently produce diverse open-form PDEs with tree structures. A neural network-based predictive model fits the system response and serves as the reward evaluator for the generated PDEs. PDEs with superior fits are utilized to iteratively optimize the generator via the RL method and the best-performing PDE is selected by a parameter-free stability metric. The embedding phase integrates the initially identified PDE from the discovering process a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#32463;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#23458;&#25143;&#31471;&#38388;&#20998;&#24067;&#20559;&#31227;&#21644;&#37096;&#20998;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#21327;&#20316;&#36890;&#20449;&#21327;&#35758;&#21644;&#32858;&#21512;&#25805;&#20316;&#65292;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#65292;&#24182;&#25104;&#21151;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20102;&#26631;&#35760;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.07670</link><description>&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#32852;&#37030;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation. (arXiv:2309.07670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#32463;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#23458;&#25143;&#31471;&#38388;&#20998;&#24067;&#20559;&#31227;&#21644;&#37096;&#20998;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35774;&#35745;&#21327;&#20316;&#36890;&#20449;&#21327;&#35758;&#21644;&#32858;&#21512;&#25805;&#20316;&#65292;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#65292;&#24182;&#25104;&#21151;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20102;&#26631;&#35760;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#25143;&#31471;&#20013;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#19988;&#37096;&#20998;&#23458;&#25143;&#31471;&#20855;&#26377;&#26080;&#26631;&#31614;&#30340;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;FedDaDiL&#36890;&#36807;&#23383;&#20856;&#23398;&#20064;&#32463;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#65292;&#23458;&#25143;&#31471;&#30340;&#20998;&#24067;&#20195;&#34920;&#30528;&#29305;&#23450;&#30340;&#39046;&#22495;&#65292;&#32780;FedDaDiL&#21017;&#20849;&#21516;&#35757;&#32451;&#20102;&#19968;&#20010;&#32852;&#37030;&#32463;&#39564;&#20998;&#24067;&#23383;&#20856;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#19978;&#35774;&#35745;&#20102;&#21327;&#20316;&#36890;&#20449;&#21327;&#35758;&#21644;&#32858;&#21512;&#25805;&#20316;&#12290;&#25152;&#36873;&#25321;&#30340;&#21327;&#35758;&#20445;&#25252;&#20102;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;&#30456;&#27604;&#20110;&#38598;&#20013;&#24335;&#26041;&#27861;&#25552;&#39640;&#20102;&#25972;&#20307;&#38544;&#31169;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;Caltech-Office&#12289;TEP&#21644;CWRU&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#22312;&#30446;&#26631;&#39046;&#22495;&#29983;&#25104;&#20102;&#26631;&#35760;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#38598;&#20013;&#24335;&#26041;&#27861;&#21644;&#20854;&#20182;&#32852;&#37030;&#39046;&#22495;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this article, we propose an approach for federated domain adaptation, a setting where distributional shift exists among clients and some have unlabeled data. The proposed framework, FedDaDiL, tackles the resulting challenge through dictionary learning of empirical distributions. In our setting, clients' distributions represent particular domains, and FedDaDiL collectively trains a federated dictionary of empirical distributions. In particular, we build upon the Dataset Dictionary Learning framework by designing collaborative communication protocols and aggregation operations. The chosen protocols keep clients' data private, thus enhancing overall privacy compared to its centralized counterpart. We empirically demonstrate that our approach successfully generates labeled data on the target domain with extensive experiments on (i) Caltech-Office, (ii) TEP, and (iii) CWRU benchmarks. Furthermore, we compare our method to its centralized counterpart and other benchmarks in federated doma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#31216;&#20026;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36935;&#35265;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;MSDA-DD&#65289;&#65292;&#36890;&#36807;&#36866;&#24212;&#20808;&#21069;&#30340;&#26041;&#27861;&#20197;&#21450;&#20998;&#37197;&#21305;&#37197;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20165;&#20351;&#29992;&#27599;&#31867;1&#20010;&#26679;&#26412;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#36866;&#24212;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07666</link><description>&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36935;&#35265;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning. (arXiv:2309.07666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#31216;&#20026;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#36890;&#36807;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#36935;&#35265;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;MSDA-DD&#65289;&#65292;&#36890;&#36807;&#36866;&#24212;&#20808;&#21069;&#30340;&#26041;&#27861;&#20197;&#21450;&#20998;&#37197;&#21305;&#37197;&#26041;&#27861;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20165;&#20351;&#29992;&#27599;&#31867;1&#20010;&#26679;&#26412;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#36866;&#24212;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20004;&#20010;&#38382;&#39064;&#30340;&#20132;&#38598;&#65306;&#22810;&#28304;&#39046;&#22495;&#36866;&#24212;&#65288;MSDA&#65289;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;DD&#65289;&#12290;&#19968;&#26041;&#38754;&#65292;&#21069;&#32773;&#32771;&#34385;&#20102;&#23558;&#22810;&#20010;&#24322;&#36136;&#30340;&#26631;&#27880;&#28304;&#39046;&#22495;&#36866;&#24212;&#21040;&#19968;&#20010;&#26410;&#26631;&#27880;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21518;&#32773;&#25915;&#20987;&#20102;&#20851;&#20110;&#21512;&#25104;&#19968;&#20010;&#21253;&#21547;&#26377;&#20851;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#20449;&#24687;&#30340;&#23567;&#25688;&#35201;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#31216;&#20026;MSDA-DD&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36866;&#24212;&#20102;MSDA&#25991;&#29486;&#20013;&#30340;&#20808;&#21069;&#20316;&#21697;&#65292;&#22914;Wasserstein Barycenter Transport&#21644;&#25968;&#25454;&#38598;&#23383;&#20856;&#23398;&#20064;&#65292;&#20197;&#21450;DD&#26041;&#27861;Distribution Matching&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#23545;&#36825;&#20010;&#26032;&#38382;&#39064;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65288;Caltech-Office 10&#65292; Tennessee-Eastman Process&#65292; Continuous Stirred Tank Reactor&#21644;Case Western Reserve University&#65289;&#65292;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#27599;&#31867;&#21482;&#26377;1&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#20063;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#36866;&#24212;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the intersection of two problems in machine learning: Multi-Source Domain Adaptation (MSDA) and Dataset Distillation (DD). On the one hand, the first considers adapting multiple heterogeneous labeled source domains to an unlabeled target domain. On the other hand, the second attacks the problem of synthesizing a small summary containing all the information about the datasets. We thus consider a new problem called MSDA-DD. To solve it, we adapt previous works in the MSDA literature, such as Wasserstein Barycenter Transport and Dataset Dictionary Learning, as well as DD method Distribution Matching. We thoroughly experiment with this novel problem on four benchmarks (Caltech-Office 10, Tennessee-Eastman Process, Continuous Stirred Tank Reactor, and Case Western Reserve University), where we show that, even with as little as 1 sample per class, one achieves state-of-the-art adaptation performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22312;&#39640;&#32500;&#38480;&#21046;&#19979;&#30340;&#26368;&#31616;&#21270;&#30340;VAE&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#35780;&#20272;&#20102;beta&#19982;VAE&#20013;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#21518;&#39564;&#22349;&#32553;&#21644;&#29575;&#22833;&#30495;&#26354;&#32447;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#30528;beta&#30340;&#22686;&#21152;&#65292;&#20135;&#29983;&#36739;&#22823;&#30340;&#24191;&#20041;&#35823;&#24046;&#24179;&#21488;&#65292;&#24182;&#19988;&#36873;&#25321;&#19968;&#20010;&#23567;&#20110;&#29305;&#23450;&#38408;&#20540;&#30340;beta&#20540;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07663</link><description>&lt;p&gt;
&#32447;&#24615;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#29575;&#22833;&#30495;&#26354;&#32447;&#21644;&#21518;&#39564;&#22349;&#32553;&#38408;&#20540;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Dataset Size Dependence of Rate-Distortion Curve and Threshold of Posterior Collapse in Linear VAE. (arXiv:2309.07663v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22312;&#39640;&#32500;&#38480;&#21046;&#19979;&#30340;&#26368;&#31616;&#21270;&#30340;VAE&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#35780;&#20272;&#20102;beta&#19982;VAE&#20013;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#21518;&#39564;&#22349;&#32553;&#21644;&#29575;&#22833;&#30495;&#26354;&#32447;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#30528;beta&#30340;&#22686;&#21152;&#65292;&#20135;&#29983;&#36739;&#22823;&#30340;&#24191;&#20041;&#35823;&#24046;&#24179;&#21488;&#65292;&#24182;&#19988;&#36873;&#25321;&#19968;&#20010;&#23567;&#20110;&#29305;&#23450;&#38408;&#20540;&#30340;beta&#20540;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#20013;&#65292;&#21464;&#20998;&#21518;&#39564;&#32463;&#24120;&#19982;&#20808;&#39564;&#23494;&#20999;&#21563;&#21512;&#65292;&#36825;&#34987;&#31216;&#20026;&#21518;&#39564;&#22349;&#32553;&#65292;&#24433;&#21709;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;VAE&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35843;&#33410;&#30340;&#36229;&#21442;&#25968;beta&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#39640;&#32500;&#38480;&#21046;&#19979;&#20998;&#26512;&#26368;&#31616;&#21270;&#30340;VAE&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#35780;&#20272;&#20102;beta&#19982;VAE&#20013;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#21518;&#39564;&#22349;&#32553;&#21644;&#29575;&#22833;&#30495;&#26354;&#32447;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#36739;&#22823;&#30340;beta&#20250;&#20135;&#29983;&#19968;&#20010;&#38271;&#30340;&#24191;&#20041;&#35823;&#24046;&#24179;&#21488;&#12290;&#38543;&#30528;beta&#30340;&#22686;&#21152;&#65292;&#24179;&#21488;&#30340;&#38271;&#24230;&#24310;&#38271;&#65292;&#36229;&#36807;&#19968;&#23450;&#30340;&#38408;&#20540;&#21518;&#21464;&#20026;&#26080;&#31351;&#12290;&#36825;&#24847;&#21619;&#30528;&#19982;&#36890;&#24120;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#19981;&#21516;&#65292;beta&#30340;&#36873;&#25321;&#21487;&#33021;&#20250;&#23548;&#33268;&#21518;&#39564;&#22349;&#32553;&#65292;&#32780;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#26080;&#20851;&#12290;&#22240;&#27492;&#65292;beta&#26159;&#19968;&#20010;&#38656;&#35201;&#35880;&#24910;&#35843;&#25972;&#30340;&#39118;&#38505;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#29575;&#22833;&#30495;&#26354;&#32447;&#30340;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#23384;&#22312;&#19968;&#20010;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#30456;&#20851;&#30340;&#38408;&#20540;&#65292;&#36873;&#25321;&#23567;&#20110;&#36825;&#20010;&#38408;&#20540;&#30340;beta&#20540;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the Variational Autoencoder (VAE), the variational posterior often aligns closely with the prior, which is known as posterior collapse and hinders the quality of representation learning. To mitigate this problem, an adjustable hyperparameter beta has been introduced in the VAE. This paper presents a closed-form expression to assess the relationship between the beta in VAE, the dataset size, the posterior collapse, and the rate-distortion curve by analyzing a minimal VAE in a high-dimensional limit. These results clarify that a long plateau in the generalization error emerges with a relatively larger beta. As the beta increases, the length of the plateau extends and then becomes infinite beyond a certain beta threshold. This implies that the choice of beta, unlike the usual regularization parameters, can induce posterior collapse regardless of the dataset size. Thus, beta is a risky parameter that requires careful tuning. Furthermore, considering the dataset-size dependence on the ra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31038;&#21306;&#38382;&#31572;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#25490;&#21517;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#30340;&#20960;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;BERT&#30340;&#29305;&#24449;&#65292;&#25429;&#25417;&#35821;&#20041;&#30456;&#20284;&#24615;&#65307;&#20854;&#27425;&#65292;&#32467;&#21512;&#38382;&#39064;&#21644;&#31572;&#26696;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#65307;&#31532;&#19977;&#65292;&#36890;&#36807;&#32463;&#39564;&#24615;&#30740;&#31350;&#25506;&#32034;&#20102;&#19981;&#21516;&#25490;&#21517;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07610</link><description>&lt;p&gt;
&#23398;&#20064;&#21040;&#25490;&#21517;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#22312;&#31038;&#21306;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Feature Engineering in Learning-to-Rank for Community Question Answering Task. (arXiv:2309.07610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31038;&#21306;&#38382;&#31572;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#25490;&#21517;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#30340;&#20960;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;BERT&#30340;&#29305;&#24449;&#65292;&#25429;&#25417;&#35821;&#20041;&#30456;&#20284;&#24615;&#65307;&#20854;&#27425;&#65292;&#32467;&#21512;&#38382;&#39064;&#21644;&#31572;&#26696;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#65307;&#31532;&#19977;&#65292;&#36890;&#36807;&#32463;&#39564;&#24615;&#30740;&#31350;&#25506;&#32034;&#20102;&#19981;&#21516;&#25490;&#21517;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;&#65288;CQA&#65289;&#35770;&#22363;&#26159;&#22522;&#20110;&#20114;&#32852;&#32593;&#30340;&#24179;&#21488;&#65292;&#29992;&#25143;&#22312;&#36825;&#37324;&#25552;&#20986;&#38382;&#39064;&#65292;&#20854;&#20182;&#19987;&#23478;&#29992;&#25143;&#35797;&#22270;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#35768;&#22810;CQA&#35770;&#22363;&#65292;&#22914;Quora&#65292;Stackoverflow&#65292;Yahoo&#65281;Answer&#65292;StackExchange&#31561;&#37117;&#26377;&#22823;&#37327;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#36825;&#20123;&#25968;&#25454;&#22312;&#33258;&#21160;&#21270;&#30340;CQA&#25490;&#21517;&#31995;&#32479;&#20013;&#24471;&#21040;&#21033;&#29992;&#65292;&#20197;&#22238;&#24212;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#21576;&#29616;&#31867;&#20284;&#30340;&#38382;&#39064;&#65288;&#21644;&#31572;&#26696;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35843;&#26597;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#38500;&#20102;&#20256;&#32479;&#30340;&#29305;&#24449;&#22914;TF-IDF&#12289;BM25&#31561;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;BERT&#30340;&#29305;&#24449;&#65292;&#25429;&#25417;&#38382;&#39064;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#20854;&#27425;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30740;&#31350;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#20165;&#20174;&#38382;&#39064;&#37096;&#20998;&#25552;&#21462;&#30340;&#29305;&#24449;&#19978;&#65292;&#23578;&#26410;&#24191;&#27867;&#25506;&#32034;&#20174;&#31572;&#26696;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#20197;&#32447;&#24615;&#26041;&#24335;&#32467;&#21512;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#29305;&#24449;&#12290;&#31532;&#19977;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#25490;&#21517;&#31639;&#27861;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community question answering (CQA) forums are Internet-based platforms where users ask questions about a topic and other expert users try to provide solutions. Many CQA forums such as Quora, Stackoverflow, Yahoo!Answer, StackExchange exist with a lot of user-generated data. These data are leveraged in automated CQA ranking systems where similar questions (and answers) are presented in response to the query of the user. In this work, we empirically investigate a few aspects of this domain. Firstly, in addition to traditional features like TF-IDF, BM25 etc., we introduce a BERT-based feature that captures the semantic similarity between the question and answer. Secondly, most of the existing research works have focused on features extracted only from the question part; features extracted from answers have not been explored extensively. We combine both types of features in a linear fashion. Thirdly, using our proposed concepts, we conduct an empirical investigation with different rank-lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#26032;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#32553;&#25918;&#26041;&#27861;&#23454;&#29616;&#20102;&#19981;&#21516;&#38271;&#24230;&#30340;&#21487;&#21464;&#24418;&#19968;&#32500;&#29289;&#20307;&#30340;&#26356;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22823;&#22823;&#25552;&#39640;&#20102;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#29978;&#33267;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#20063;&#33021;&#36798;&#21040;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2309.07609</link><description>&lt;p&gt;
&#23398;&#20064;&#38750;&#26631;&#35760;&#21487;&#21464;&#24418;&#19968;&#32500;&#29289;&#20307;&#30340;&#20934;&#38745;&#24577;3D&#27169;&#22411;&#65292;&#29992;&#20110;&#21452;&#25163;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Learning Quasi-Static 3D Models of Markerless Deformable Linear Objects for Bimanual Robotic Manipulation. (arXiv:2309.07609v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#26032;&#30340;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#32553;&#25918;&#26041;&#27861;&#23454;&#29616;&#20102;&#19981;&#21516;&#38271;&#24230;&#30340;&#21487;&#21464;&#24418;&#19968;&#32500;&#29289;&#20307;&#30340;&#26356;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22823;&#22823;&#25552;&#39640;&#20102;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#29978;&#33267;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#20063;&#33021;&#36798;&#21040;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#21464;&#24418;&#19968;&#32500;&#29289;&#20307;&#65288;DLOs&#65289;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#24456;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#20934;&#30830;&#30340;&#27169;&#22411;&#26469;&#25429;&#25417;&#26426;&#22120;&#20154;&#36816;&#21160;&#23545;DLO&#21464;&#24418;&#30340;&#24433;&#21709;&#12290;&#22914;&#20170;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22411;&#22312;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#20043;&#38388;&#25552;&#20379;&#20102;&#26368;&#22909;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20960;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;DLO&#19977;&#32500;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#32553;&#25918;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#38271;&#24230;&#30340;DLO&#19978;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#20960;&#20046;&#25152;&#26377;&#32771;&#34385;&#30340;DLO&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20511;&#21161;&#36825;&#31181;&#25216;&#26415;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#20063;&#33021;&#36798;&#21040;&#25509;&#36817;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#27700;&#24179;&#65292;&#32780;&#19988;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20123;&#23398;&#20064;&#30340;3D&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robotic manipulation of Deformable Linear Objects (DLOs) is a vital and challenging task that is important in many practical applications. Classical model-based approaches to this problem require an accurate model to capture how robot motions affect the deformation of the DLO. Nowadays, data-driven models offer the best tradeoff between quality and computation time. This paper analyzes several learning-based 3D models of the DLO and proposes a new one based on the Transformer architecture that achieves superior accuracy, even on the DLOs of different lengths, thanks to the proposed scaling method. Moreover, we introduce a data augmentation technique, which improves the prediction performance of almost all considered DLO data-driven models. Thanks to this technique, even a simple Multilayer Perceptron (MLP) achieves close to state-of-the-art performance while being significantly faster to evaluate. In the experiments, we compare the performance of the learning-based 3D models of the
&lt;/p&gt;</description></item><item><title>&#22312;&#27604;&#36739;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20004;&#31181;&#27169;&#22411;SASRec&#21644;BERT4Rec&#26102;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#20004;&#20010;&#27169;&#22411;&#37117;&#20351;&#29992;&#30456;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;SASRec&#22312;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;BERT4Rec&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#36127;&#37319;&#26679;&#65292;SASRec&#20173;&#28982;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#24182;&#20248;&#20110;BERT4Rec&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#36127;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.07602</link><description>&lt;p&gt;
&#23558;&#24223;&#26009;&#21464;&#20026;&#40644;&#37329;&#30340;&#25439;&#22833;&#65306;BERT4Rec&#30495;&#30340;&#27604;SASRec&#26356;&#22909;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?. (arXiv:2309.07602v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07602
&lt;/p&gt;
&lt;p&gt;
&#22312;&#27604;&#36739;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20004;&#31181;&#27169;&#22411;SASRec&#21644;BERT4Rec&#26102;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22914;&#26524;&#20004;&#20010;&#27169;&#22411;&#37117;&#20351;&#29992;&#30456;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;SASRec&#22312;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;BERT4Rec&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#21363;&#20351;&#20351;&#29992;&#36127;&#37319;&#26679;&#65292;SASRec&#20173;&#28982;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#24182;&#20248;&#20110;BERT4Rec&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#65292;&#39034;&#24207;&#25512;&#33616;&#21644;&#19979;&#19968;&#20010;&#39033;&#30446;&#39044;&#27979;&#20219;&#21153;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;SASRec&#21644;BERT4Rec&#26159;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#26377;&#24456;&#22810;&#21457;&#34920;&#30340;&#35770;&#25991;&#27604;&#36739;&#20102;&#36825;&#20004;&#20010;&#31639;&#27861;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#22312;&#22823;&#22810;&#25968;&#35770;&#25991;&#20013;&#65292;BERT4Rec&#30340;&#24615;&#33021;&#20248;&#20110;SASRec&#12290;&#20294;&#26159;&#65292;BERT4Rec&#23545;&#25152;&#26377;&#39033;&#30446;&#20351;&#29992;&#20132;&#21449;&#29109;&#65292;&#32780;SASRec&#20351;&#29992;&#36127;&#37319;&#26679;&#23545;&#19968;&#20010;&#27491;&#26679;&#26412;&#21644;&#19968;&#20010;&#36127;&#26679;&#26412;&#35745;&#31639;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#20004;&#20010;&#27169;&#22411;&#37117;&#20351;&#29992;BERT4Rec&#25152;&#29992;&#30340;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#65292;&#37027;&#20040;SASRec&#22312;&#36136;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#23558;&#26126;&#26174;&#20248;&#20110;BERT4Rec&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21363;&#20351;&#20351;&#29992;&#36127;&#37319;&#26679;&#65292;SASRec&#20173;&#28982;&#33021;&#22815;&#26377;&#25928;&#35757;&#32451;&#24182;&#20248;&#20110;BERT4Rec&#65292;&#20294;&#36127;&#26679;&#26412;&#30340;&#25968;&#37327;&#24212;&#35813;&#27604;BERT4Rec&#35201;&#22823;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently sequential recommendations and next-item prediction task has become increasingly popular in the field of recommender systems. Currently, two state-of-the-art baselines are Transformer-based models SASRec and BERT4Rec. Over the past few years, there have been quite a few publications comparing these two algorithms and proposing new state-of-the-art models. In most of the publications, BERT4Rec achieves better performance than SASRec. But BERT4Rec uses cross-entropy over softmax for all items, while SASRec uses negative sampling and calculates binary cross-entropy loss for one positive and one negative item. In our work, we show that if both models are trained with the same loss, which is used by BERT4Rec, then SASRec will significantly outperform BERT4Rec both in terms of quality and training speed. In addition, we show that SASRec could be effectively trained with negative sampling and still outperform BERT4Rec, but the number of negative examples should be much larger than on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2309.07601</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#20449;&#21495;&#21644;&#24369;&#30417;&#30563;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#24230;&#20449;&#21495;&#20195;&#34920;&#20102;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#21592;&#36890;&#24120;&#29992;&#26469;&#35780;&#20272;&#22312;&#32447;&#20869;&#23481;&#30495;&#23454;&#24615;&#30340;&#19968;&#31995;&#21015;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#21487;&#20449;&#24230;&#20449;&#21495;&#25552;&#21462;&#30340;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#35757;&#32451;&#39640;&#20934;&#30830;&#29575;&#30340;&#29305;&#23450;&#20449;&#21495;&#25552;&#21462;&#22120;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#38598;&#23545;&#25152;&#26377;&#21487;&#20449;&#24230;&#20449;&#21495;&#36827;&#34892;&#27880;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#19968;&#32452;18&#20010;&#21487;&#20449;&#24230;&#20449;&#21495;&#26469;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20197;&#20135;&#29983;&#27599;&#20010;&#20449;&#21495;&#30340;&#24369;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#36825;&#20123;&#28508;&#22312;&#30340;&#22122;&#22768;&#26631;&#31614;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#39044;&#27979;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#32467;&#21512;&#20102;&#38646;-shot LLM&#21487;&#20449;&#24230;&#20449;&#21495;&#26631;&#27880;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#65292;&#32780;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#35757;&#32451;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credibility signals represent a wide range of heuristics that are typically used by journalists and fact-checkers to assess the veracity of online content. Automating the task of credibility signal extraction, however, is very challenging as it requires high-accuracy signal-specific extractors to be trained, while there are currently no sufficiently large datasets annotated with all credibility signals. This paper investigates whether large language models (LLMs) can be prompted effectively with a set of 18 credibility signals to produce weak labels for each signal. We then aggregate these potentially noisy labels using weak supervision in order to predict content veracity. We demonstrate that our approach, which combines zero-shot LLM credibility signal labeling and weak supervision, outperforms state-of-the-art classifiers on two misinformation datasets without using any ground-truth labels for training. We also analyse the contribution of the individual credibility signals towards p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#32622;&#25442;&#36827;&#34892;&#32479;&#35745;&#26377;&#25928;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#20934;&#32622;&#25442;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07593</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#32622;&#25442;&#36827;&#34892;&#32479;&#35745;&#26377;&#25928;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26465;&#20214;&#32622;&#25442;&#36827;&#34892;&#32479;&#35745;&#26377;&#25928;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#26041;&#27861;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#20934;&#32622;&#25442;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22797;&#26434;&#23398;&#20064;&#22120;&#65288;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#26102;&#65292;&#21464;&#37327;&#37325;&#35201;&#24615;&#35780;&#20272;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#30446;&#21069;&#65292;&#22522;&#20110;&#31227;&#38500;&#30340;&#37325;&#35201;&#24615;&#35780;&#20272;&#26159;&#21442;&#32771;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#32479;&#35745;&#20445;&#35777;&#26469;&#39564;&#35777;&#21464;&#37327;&#21253;&#21547;&#24615;&#26102;&#12290;&#36890;&#24120;&#65292;&#23427;&#20204;&#20351;&#29992;&#21464;&#37327;&#32622;&#25442;&#26041;&#26696;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23384;&#22312;&#21327;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26102;&#23481;&#26131;&#23558;&#19981;&#37325;&#35201;&#30340;&#21464;&#37327;&#35823;&#35782;&#21035;&#20026;&#37325;&#35201;&#21464;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#30740;&#31350;&#26465;&#20214;&#32622;&#25442;&#37325;&#35201;&#24615;&#65288;Conditional Permutation Importance&#65292;CPI&#65289;&#65292;&#23427;&#26159;&#27169;&#22411;&#26080;&#20851;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#22120;&#12290;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;CPI&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#30340;I&#22411;&#38169;&#35823;&#25511;&#21046;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#32622;&#25442;&#37325;&#35201;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#24403;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;CPI&#22987;&#32456;&#26174;&#31034;&#20986;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variable importance assessment has become a crucial step in machine-learning applications when using complex learners, such as deep neural networks, on large-scale data. Removal-based importance assessment is currently the reference approach, particularly when statistical guarantees are sought to justify variable inclusion. It is often implemented with variable permutation schemes. On the flip side, these approaches risk misidentifying unimportant variables as important in the presence of correlations among covariates. Here we develop a systematic approach for studying Conditional Permutation Importance (CPI) that is model agnostic and computationally lean, as well as reusable benchmarks of state-of-the-art variable importance estimators. We show theoretically and empirically that $\textit{CPI}$ overcomes the limitations of standard permutation importance by providing accurate type-I error control. When used with a deep neural network, $\textit{CPI}$ consistently showed top accuracy ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#30340;&#32467;&#26500;&#20445;&#25345;&#21464;&#21387;&#22120;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07579</link><description>&lt;p&gt;
&#20445;&#25345;&#32467;&#26500;&#30340;&#21464;&#21387;&#22120;&#29992;&#20110;&#24207;&#21015;&#30340;SPD&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Structure-Preserving Transformers for Sequences of SPD Matrices. (arXiv:2309.07579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20445;&#25345;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#30340;&#32467;&#26500;&#20445;&#25345;&#21464;&#21387;&#22120;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25968;&#25454;&#31867;&#22411;&#30340;&#20998;&#26512;&#65292;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#31561;&#65292;&#21253;&#25324;&#38750;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36825;&#26679;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#20998;&#31867;&#24207;&#21015;&#30340;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#65292;&#24182;&#22312;&#25972;&#20010;&#20998;&#26512;&#36807;&#31243;&#20013;&#20445;&#25345;&#23427;&#20204;&#30340;&#40654;&#26364;&#20960;&#20309;&#29305;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26469;&#33258;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#33041;&#30005;&#22270;&#21327;&#26041;&#24046;&#30697;&#38453;&#24207;&#21015;&#30340;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#65292;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#30340;&#38454;&#27573;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Transformer-based auto-attention mechanisms have been successfully applied to the analysis of a variety of context-reliant data types, from texts to images and beyond, including data from non-Euclidean geometries. In this paper, we present such a mechanism, designed to classify sequences of Symmetric Positive Definite matrices while preserving their Riemannian geometry throughout the analysis. We apply our method to automatic sleep staging on timeseries of EEG-derived covariance matrices from a standard dataset, obtaining high levels of stage-wise performance.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#30340;&#31561;&#21464;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#31561;&#21464;&#38598;&#21512;&#24182;&#22686;&#24378;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07578</link><description>&lt;p&gt;
Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning.&#65288;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#27867;&#21270;&#30340;&#31561;&#21464;&#25968;&#25454;&#22686;&#24378;&#65289;
&lt;/p&gt;
&lt;p&gt;
Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning. (arXiv:2309.07578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07578
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27867;&#21270;&#30340;&#31561;&#21464;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25193;&#23637;&#31561;&#21464;&#38598;&#21512;&#24182;&#22686;&#24378;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#31574;&#30053;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#25361;&#25112;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#20174;&#22266;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#32780;&#26080;&#38656;&#19982;&#29615;&#22659;&#36827;&#34892;&#39069;&#22806;&#20132;&#20114;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#39640;&#26234;&#33021;&#20307;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#30446;&#26631;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#24182;&#26816;&#26597;&#20854;&#26159;&#21542;&#31561;&#21464;&#20110;&#22266;&#23450;&#31867;&#22411;&#30340;&#21464;&#25442;&#65292;&#21363;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#24179;&#31227;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29109;&#27491;&#21017;&#21270;&#22120;&#26469;&#25193;&#23637;&#31561;&#21464;&#38598;&#24182;&#36890;&#36807;&#25152;&#24471;&#21040;&#30340;&#36716;&#25442;&#26679;&#26412;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#22686;&#24378;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#20351;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#31163;&#32447;&#23398;&#20064;&#19968;&#20010;&#26032;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26497;&#22823;&#22320;&#25552;&#39640;&#31574;&#30053;&#22312;&#32771;&#34385;&#30340;&#29615;&#22659;&#20013;&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to address the challenge of generalization in offline reinforcement learning (RL), where the agent learns from a fixed dataset without any additional interaction with the environment. Specifically, we aim to improve the agent's ability to generalize to out-of-distribution goals. To achieve this, we propose to learn a dynamics model and check if it is equivariant with respect to a fixed type of transformation, namely translations in the state space. We then use an entropy regularizer to increase the equivariant set and augment the dataset with the resulting transformed samples. Finally, we learn a new policy offline based on the augmented dataset, with an off-the-shelf offline RL algorithm. Our experimental results demonstrate that our approach can greatly improve the test performance of the policy on the considered environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#33258;&#22238;&#24402;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#36816;&#21160;&#36712;&#36857;&#26469;&#29983;&#25104;&#33258;&#28982;&#30340;&#26426;&#22120;&#20154;&#33218;&#36816;&#21160;&#36712;&#36857;&#65292;&#20026;&#36718;&#26885;&#22352;&#24335;&#36741;&#21161;&#26426;&#22120;&#20154;&#25552;&#20379;&#26356;&#21487;&#39044;&#27979;&#21644;&#26356;&#31867;&#20284;&#20154;&#31867;&#30340;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2309.07550</link><description>&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#33258;&#28982;&#30340;&#26426;&#22120;&#20154;&#33218;&#36816;&#21160;&#36712;&#36857;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Naturalistic Robot Arm Trajectory Generation via Representation Learning. (arXiv:2309.07550v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#33258;&#22238;&#24402;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#31034;&#33539;&#32773;&#30340;&#36816;&#21160;&#36712;&#36857;&#26469;&#29983;&#25104;&#33258;&#28982;&#30340;&#26426;&#22120;&#20154;&#33218;&#36816;&#21160;&#36712;&#36857;&#65292;&#20026;&#36718;&#26885;&#22352;&#24335;&#36741;&#21161;&#26426;&#22120;&#20154;&#25552;&#20379;&#26356;&#21487;&#39044;&#27979;&#21644;&#26356;&#31867;&#20284;&#20154;&#31867;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#38598;&#25104;&#26426;&#26800;&#33218;&#26426;&#22120;&#20154;&#38656;&#35201;&#26356;&#21487;&#39044;&#27979;&#21644;&#26356;&#31867;&#20284;&#20154;&#31867;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#65292;&#23588;&#20854;&#23545;&#20110;&#33021;&#22815;&#25903;&#25345;&#30251;&#30186;&#20154;&#32676;&#29420;&#31435;&#24615;&#30340;&#36718;&#26885;&#22352;&#24335;&#36741;&#21161;&#26426;&#22120;&#20154;&#32780;&#35328;&#26356;&#20026;&#37325;&#35201;&#12290;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#31034;&#33539;&#32773;&#26469;&#29983;&#25104;&#33258;&#28982;&#30340;&#36816;&#21160;&#36712;&#36857;&#26159;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#25191;&#34892;&#36741;&#21161;&#39278;&#27700;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#31359;&#25140;IMU&#20256;&#24863;&#22120;&#25429;&#25417;&#21040;&#30340;&#20154;&#33218;&#36816;&#21160;&#25968;&#25454;&#20316;&#20026;&#26080;&#21160;&#20316;&#20219;&#21153;&#28436;&#31034;&#26469;&#23398;&#20064;&#22810;&#26679;&#30340;&#20154;&#20307;&#36816;&#21160;&#36712;&#36857;&#25968;&#25454;&#12290;&#21033;&#29992;&#22810;&#20010;&#21442;&#19982;&#32773;&#30340;&#35266;&#27979;&#21040;&#30340;&#33218;&#37096;&#36816;&#21160;&#25968;&#25454;&#26469;&#20026;UR5e&#26426;&#22120;&#20154;&#33218;&#29983;&#25104;&#33258;&#28982;&#19988;&#21151;&#33021;&#24615;&#30340;&#39278;&#27700;&#36816;&#21160;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of manipulator robots in household environments suggests a need for more predictable and human-like robot motion. This holds especially true for wheelchair-mounted assistive robots that can support the independence of people with paralysis. One method of generating naturalistic motion trajectories is via the imitation of human demonstrators. This paper explores a self-supervised imitation learning method using an autoregressive spatio-temporal graph neural network for an assistive drinking task. We address learning from diverse human motion trajectory data that were captured via wearable IMU sensors on a human arm as the action-free task demonstrations. Observed arm motion data from several participants is used to generate natural and functional drinking motion trajectories for a UR5e robot arm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#36125;&#23572;&#26364;&#26144;&#23556;&#31867;&#65292;&#24182;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#40065;&#26834;&#33258;&#36866;&#24212;&#28388;&#27874;&#20013;&#24212;&#29992;&#12290;&#36825;&#20123;&#26144;&#23556;&#20855;&#26377;&#20016;&#23500;&#30340;&#35774;&#35745;&#33258;&#30001;&#24230;&#65292;&#24182;&#21487;&#20197;&#35299;&#20915;&#32447;&#24615;&#33258;&#36866;&#24212;&#28388;&#27874;&#20013;&#36873;&#25321;&#25351;&#25968;&#30340;&#38382;&#39064;&#12290;&#25968;&#20540;&#27979;&#35797;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07548</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36817;&#20284;&#36125;&#23572;&#26364;&#26144;&#23556;&#21450;&#20854;&#22312;&#40065;&#26834;&#33258;&#36866;&#24212;&#28388;&#27874;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Proximal Bellman mappings for reinforcement learning and their application to robust adaptive filtering. (arXiv:2309.07548v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36817;&#20284;&#36125;&#23572;&#26364;&#26144;&#23556;&#31867;&#65292;&#24182;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#40065;&#26834;&#33258;&#36866;&#24212;&#28388;&#27874;&#20013;&#24212;&#29992;&#12290;&#36825;&#20123;&#26144;&#23556;&#20855;&#26377;&#20016;&#23500;&#30340;&#35774;&#35745;&#33258;&#30001;&#24230;&#65292;&#24182;&#21487;&#20197;&#35299;&#20915;&#32447;&#24615;&#33258;&#36866;&#24212;&#28388;&#27874;&#20013;&#36873;&#25321;&#25351;&#25968;&#30340;&#38382;&#39064;&#12290;&#25968;&#20540;&#27979;&#35797;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31867;&#26032;&#39062;&#30340;&#36817;&#20284;&#36125;&#23572;&#26364;&#26144;&#23556;&#65292;&#25506;&#35752;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31639;&#27861;&#29702;&#35770;&#26680;&#24515;&#12290;&#36825;&#20123;&#26144;&#23556;&#23450;&#20041;&#22312;&#37325;&#22797;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#20013;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;RKHS&#30340;&#36817;&#20284;&#24615;&#36136;&#21644;&#20869;&#31215;&#65292;&#19981;&#35770;&#25240;&#25187;&#22240;&#23376;&#30340;&#20540;&#22914;&#20309;&#65292;&#37117;&#23646;&#20110;&#24378;&#22823;&#30340;&#24076;&#23572;&#20271;&#29305;&#65288;Firmly&#65289;&#38750;&#25193;&#24352;&#26144;&#23556;&#23478;&#26063;&#65292;&#24182;&#20855;&#26377;&#20016;&#23500;&#30340;&#35774;&#35745;&#33258;&#30001;&#24230;&#65292;&#29978;&#33267;&#21487;&#20197;&#27169;&#25311;&#32463;&#20856;&#36125;&#23572;&#26364;&#26144;&#23556;&#30340;&#23646;&#24615;&#65292;&#20026;&#26032;&#30340;RL&#35774;&#35745;&#38138;&#24179;&#36947;&#36335;&#12290;&#22312;&#25552;&#20986;&#30340;&#26144;&#23556;&#31867;&#19978;&#26500;&#24314;&#20102;&#19968;&#31181;&#36817;&#20284;&#31574;&#30053;&#36845;&#20195;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#22312;&#32447;&#36873;&#25321;"&#26368;&#20339;"&#25351;&#25968;$p$&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#32447;&#24615;&#33258;&#36866;&#24212;&#28388;&#27874;&#20013;&#25269;&#25239;&#24322;&#24120;&#20540;&#32780;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#21644;&#24322;&#24120;&#20540;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#25968;&#20540;&#27979;&#35797;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims at the algorithmic/theoretical core of reinforcement learning (RL) by introducing the novel class of proximal Bellman mappings. These mappings are defined in reproducing kernel Hilbert spaces (RKHSs), to benefit from the rich approximation properties and inner product of RKHSs, they are shown to belong to the powerful Hilbertian family of (firmly) nonexpansive mappings, regardless of the values of their discount factors, and possess ample degrees of design freedom to even reproduce attributes of the classical Bellman mappings and to pave the way for novel RL designs. An approximate policy-iteration scheme is built on the proposed class of mappings to solve the problem of selecting online, at every time instance, the "optimal" exponent $p$ in a $p$-norm loss to combat outliers in linear adaptive filtering, without training data and any knowledge on the statistical properties of the outliers. Numerical tests on synthetic data showcase the superior performance of the propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;Verilog&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24615;&#33021;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;156&#20010;&#38382;&#39064;&#30340;&#32508;&#21512;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#19982;&#40644;&#37329;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#33258;&#21160;&#27979;&#35797;Verilog&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#38382;&#39064;-&#20195;&#30721;&#23545;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;Verilog&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.07544</link><description>&lt;p&gt;
VerilogEval&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;Verilog&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
VerilogEval: Evaluating Large Language Models for Verilog Code Generation. (arXiv:2309.07544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;Verilog&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#24615;&#33021;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;156&#20010;&#38382;&#39064;&#30340;&#32508;&#21512;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#19982;&#40644;&#37329;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#21487;&#20197;&#33258;&#21160;&#27979;&#35797;Verilog&#20195;&#30721;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#38382;&#39064;-&#20195;&#30721;&#23545;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;Verilog&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26085;&#30410;&#26222;&#21450;&#20026;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;Verilog&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#35780;&#20272;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#30828;&#20214;&#35774;&#35745;&#21644;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;Verilog&#25945;&#23398;&#32593;&#31449;HDLBits&#30340;156&#20010;&#38382;&#39064;&#30340;&#32508;&#21512;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#35813;&#35780;&#20272;&#38598;&#21253;&#21547;&#20102;&#21508;&#31181;Verilog&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#20174;&#31616;&#21333;&#30340;&#32452;&#21512;&#30005;&#36335;&#21040;&#22797;&#26434;&#30340;&#26377;&#38480;&#29366;&#24577;&#26426;&#12290;&#21487;&#20197;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#35774;&#35745;&#30340;&#30636;&#24577;&#20223;&#30495;&#36755;&#20986;&#19982;&#40644;&#37329;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27604;&#36739;&#65292;&#33258;&#21160;&#27979;&#35797;Verilog&#20195;&#30721;&#23436;&#25104;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;Verilog&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#21512;&#25104;&#38382;&#39064;-&#20195;&#30721;&#23545;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#26469;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of large language models (LLMs) has paved the way for their application in diverse domains. This paper proposes a benchmarking framework tailored specifically for evaluating LLM performance in the context of Verilog code generation for hardware design and verification. We present a comprehensive evaluation dataset consisting of 156 problems from the Verilog instructional website HDLBits. The evaluation set consists of a diverse set of Verilog code generation tasks, ranging from simple combinational circuits to complex finite state machines. The Verilog code completions can be automatically tested for functional correctness by comparing the transient simulation outputs of the generated design with a golden solution. We also demonstrate that the Verilog code generation capability of pretrained language models could be improved with supervised fine-tuning by bootstrapping with LLM generated synthetic problem-code pairs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#36890;&#36807;&#39034;&#24207;&#26597;&#35810;&#20989;&#25968;&#20540;&#26469;&#36924;&#36817;&#38750;&#36882;&#20943;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#31639;&#27861;GreedyBox&#65292;&#35813;&#31639;&#27861;&#23545;&#20110;&#20219;&#24847;&#20989;&#25968;&#37117;&#33021;&#23454;&#29616;&#26368;&#20339;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07530</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36924;&#36817;&#21333;&#35843;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Adaptive approximation of monotone functions. (arXiv:2309.07530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07530
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#36890;&#36807;&#39034;&#24207;&#26597;&#35810;&#20989;&#25968;&#20540;&#26469;&#36924;&#36817;&#38750;&#36882;&#20943;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#31639;&#27861;GreedyBox&#65292;&#35813;&#31639;&#27861;&#23545;&#20110;&#20219;&#24847;&#20989;&#25968;&#37117;&#33021;&#23454;&#29616;&#26368;&#20339;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24050;&#30693;&#32039;&#23454;&#25968;&#21306;&#38388;X&#12289;Y&#21644;&#24050;&#30693;&#27010;&#29575;&#27979;&#24230;&#956;&#19979;&#65292;&#36890;&#36807;&#39034;&#24207;&#26597;&#35810;&#20989;&#25968;&#20540;&#26469;&#36924;&#36817;&#38750;&#36882;&#20943;&#20989;&#25968;$f:\mathcal{X} \to \mathcal{Y}$&#22312;$L^p(\mu)$&#33539;&#25968;&#19979;&#30340;&#32463;&#20856;&#38382;&#39064;&#12290;&#23545;&#20110;&#20219;&#24847;&#20989;&#25968;$f$&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#31639;&#27861;&#38656;&#35201;&#20445;&#35777;&#22312;&#20572;&#27490;&#21518;&#35823;&#24046;&#23567;&#20110;&#949;&#30340;&#36924;&#36817;$\hat{f}$&#25152;&#38656;&#30340;&#26368;&#23567;&#35780;&#20272;&#27425;&#25968;&#30340;&#29305;&#24449;&#12290;&#19982;&#25152;&#26377;$f$&#19978;&#22343;&#19968;&#33268;&#36866;&#29992;&#30340;&#26368;&#22351;&#24773;&#20917;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26159;&#20381;&#36182;&#20110;&#27599;&#20010;&#29305;&#23450;&#20989;&#25968;$f$&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GreedyBox&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#25968;&#20540;&#31215;&#20998;&#30340;&#21407;&#22987;&#31639;&#27861;Novak(1992)&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26080;&#35770;&#23545;&#20110;&#20219;&#20309;&#20989;&#25968;$f$&#65292;GreedyBox&#37117;&#33021;&#23454;&#29616;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#20165;&#24046;&#23545;&#25968;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#20851;&#20110;&#20998;&#27573;&#20809;&#28369;&#20989;&#25968;&#30340;&#32467;&#26524;&#12290;&#20063;&#35768;&#39044;&#26009;&#20043;&#20013;&#30340;&#26159;&#65292;GreedyBox&#30340;$L^p(\mu)$&#35823;&#24046;&#20943;&#23567;&#24471;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the classical problem of approximating a non-decreasing function $f: \mathcal{X} \to \mathcal{Y}$ in $L^p(\mu)$ norm by sequentially querying its values, for known compact real intervals $\mathcal{X}$, $\mathcal{Y}$ and a known probability measure $\mu$ on $\cX$. For any function~$f$ we characterize the minimum number of evaluations of $f$ that algorithms need to guarantee an approximation $\hat{f}$ with an $L^p(\mu)$ error below $\epsilon$ after stopping. Unlike worst-case results that hold uniformly over all $f$, our complexity measure is dependent on each specific function $f$. To address this problem, we introduce GreedyBox, a generalization of an algorithm originally proposed by Novak (1992) for numerical integration. We prove that GreedyBox achieves an optimal sample complexity for any function $f$, up to logarithmic factors. Additionally, we uncover results regarding piecewise-smooth functions. Perhaps as expected, the $L^p(\mu)$ error of GreedyBox decreases much faster
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;DEBS&#65289;&#65292;&#36890;&#36807;&#34701;&#21512;&#27491;&#20363;&#23545;&#20043;&#38388;&#30340;&#19981;&#30456;&#20284;&#24615;&#65292;&#36229;&#36234;&#20102;&#31616;&#21333;&#30340;&#30456;&#20284;&#24615;&#65292;&#29992;&#20110;&#24515;&#30005;&#22270;(ECG)&#20449;&#21495;&#30340;&#24515;&#25151;&#39076;&#21160;(AFib)&#26816;&#27979;&#12290;&#20351;&#29992;DEBS&#21487;&#20197;&#25552;&#39640;10%&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#65292;&#24378;&#35843;&#20102;&#21033;&#29992;&#19981;&#30456;&#20284;&#24615;&#26469;&#32534;&#30721;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2309.07526</link><description>&lt;p&gt;
&#36229;&#36234;&#30456;&#20284;&#24615;&#30340;&#23398;&#20064;&#65306;&#22312;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#20013;&#34701;&#21512;&#27491;&#20363;&#23545;&#30340;&#19981;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning Beyond Similarities: Incorporating Dissimilarities between Positive Pairs in Self-Supervised Time Series Learning. (arXiv:2309.07526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65288;DEBS&#65289;&#65292;&#36890;&#36807;&#34701;&#21512;&#27491;&#20363;&#23545;&#20043;&#38388;&#30340;&#19981;&#30456;&#20284;&#24615;&#65292;&#36229;&#36234;&#20102;&#31616;&#21333;&#30340;&#30456;&#20284;&#24615;&#65292;&#29992;&#20110;&#24515;&#30005;&#22270;(ECG)&#20449;&#21495;&#30340;&#24515;&#25151;&#39076;&#21160;(AFib)&#26816;&#27979;&#12290;&#20351;&#29992;DEBS&#21487;&#20197;&#25552;&#39640;10%&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#65292;&#24378;&#35843;&#20102;&#21033;&#29992;&#19981;&#30456;&#20284;&#24615;&#26469;&#32534;&#30721;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21160;&#24577;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;&#36830;&#32493;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#23637;&#31034;&#20102;&#20854;&#32534;&#30721;&#26102;&#38388;&#25968;&#25454;&#22266;&#26377;&#38745;&#24577;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#30456;&#20284;&#24615;&#30340;&#29420;&#23478;&#20851;&#27880;&#21487;&#33021;&#23548;&#33268;&#24573;&#35270;&#23545;&#27169;&#25311;&#24515;&#34880;&#31649;&#30142;&#30149;&#24314;&#27169;&#33267;&#20851;&#37325;&#35201;&#30340;&#21160;&#24577;&#23646;&#24615;&#30340;&#34920;&#31034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Distilled Encoding Beyond Similarities (DEBS)&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#27491;&#20363;&#23545;&#20043;&#38388;&#30340;&#19981;&#30456;&#20284;&#24615;&#65292;&#36229;&#36234;&#20102;&#31616;&#21333;&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26694;&#26550;&#34987;&#24212;&#29992;&#20110;&#24515;&#30005;&#22270;(ECG)&#20449;&#21495;&#65292;&#25552;&#39640;&#20102;&#19981;&#21516;&#34987;&#35797;&#20013;&#25151;&#39076;(AFib)&#26816;&#27979;&#20934;&#30830;&#24230;&#30340;10%&#12290;DEBS&#24378;&#35843;&#36890;&#36807;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#21033;&#29992;&#19981;&#30456;&#20284;&#24615;&#26469;&#32534;&#30721;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#21160;&#24577;&#29305;&#24449;&#65292;&#23454;&#29616;&#26356;&#31934;&#32454;&#30340;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#30740;&#31350;&#24320;&#25299;&#20102;&#19968;&#31181;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21033;&#29992;&#27491;&#20363;&#23545;&#19981;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#32534;&#30721;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
By identifying similarities between successive inputs, Self-Supervised Learning (SSL) methods for time series analysis have demonstrated their effectiveness in encoding the inherent static characteristics of temporal data. However, an exclusive emphasis on similarities might result in representations that overlook the dynamic attributes critical for modeling cardiovascular diseases within a confined subject cohort. Introducing Distilled Encoding Beyond Similarities (DEBS), this paper pioneers an SSL approach that transcends mere similarities by integrating dissimilarities among positive pairs. The framework is applied to electrocardiogram (ECG) signals, leading to a notable enhancement of +10\% in the detection accuracy of Atrial Fibrillation (AFib) across diverse subjects. DEBS underscores the potential of attaining a more refined representation by encoding the dynamic characteristics of time series data, tapping into dissimilarities during the optimization process. Broadly, the strat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#28909;&#22270;&#25490;&#24207;&#38382;&#39064;&#22312;&#20445;&#30041;&#31751;&#30340;&#21516;&#26102;&#37325;&#26032;&#25490;&#21015;&#21644;&#21512;&#24182;&#28857;&#21644;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#24230;&#24182;&#34892;&#30340;&#22266;&#23450;&#21442;&#25968;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#31639;&#27861;&#22312;&#24212;&#29992;&#20110;&#30005;&#23376;&#37038;&#20214;&#21644;&#35745;&#31639;&#26426;&#32593;&#32476;&#31561;&#39046;&#22495;&#26102;&#65292;&#19982;k-means&#21644;DBSCAN&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2309.07486</link><description>&lt;p&gt;
&#39640;&#24230;&#24182;&#34892;&#30340;&#28909;&#22270;&#25490;&#24207;&#21450;&#20854;&#22312;&#21487;&#35299;&#37322;&#32858;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Massively-Parallel Heat Map Sorting and Applications To Explainable Clustering. (arXiv:2309.07486v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#28909;&#22270;&#25490;&#24207;&#38382;&#39064;&#22312;&#20445;&#30041;&#31751;&#30340;&#21516;&#26102;&#37325;&#26032;&#25490;&#21015;&#21644;&#21512;&#24182;&#28857;&#21644;&#32500;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#24230;&#24182;&#34892;&#30340;&#22266;&#23450;&#21442;&#25968;&#31639;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#31639;&#27861;&#22312;&#24212;&#29992;&#20110;&#30005;&#23376;&#37038;&#20214;&#21644;&#35745;&#31639;&#26426;&#32593;&#32476;&#31561;&#39046;&#22495;&#26102;&#65292;&#19982;k-means&#21644;DBSCAN&#30340;&#25928;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#20855;&#26377;$k$&#20010;&#26631;&#31614;&#30340;&#28857;&#38598;&#65292;&#25105;&#20204;&#23558;&#28909;&#22270;&#25490;&#24207;&#38382;&#39064;&#23450;&#20041;&#20026;&#22312;&#20445;&#30041;&#31751;&#65288;&#26631;&#31614;&#65289;&#30340;&#21516;&#26102;&#37325;&#26032;&#25490;&#21015;&#21644;&#21512;&#24182;&#28857;&#21644;&#32500;&#24230;&#12290;&#22914;&#26524;&#19968;&#20010;&#31751;&#20445;&#25345;&#36830;&#36890;&#65292;&#21363;&#27809;&#26377;&#34987;&#20998;&#35010;&#25104;&#22810;&#20010;&#31751;&#19988;&#27809;&#26377;&#20004;&#20010;&#31751;&#34987;&#21512;&#24182;&#65292;&#21017;&#31216;&#35813;&#31751;&#34987;&#20445;&#30041;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#26159;NP-hard&#30340;&#65292;&#24182;&#22312;&#39640;&#24230;&#24182;&#34892;&#35745;&#31639;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#22266;&#23450;&#27425;&#25968;&#36718;&#30340;&#22266;&#23450;&#21442;&#25968;&#31639;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#26426;&#22120;&#20855;&#26377;&#20122;&#32447;&#24615;&#20869;&#23384;&#65292;&#24635;&#20869;&#23384;&#20026;&#32447;&#24615;&#12290;&#23545;&#20110;&#38382;&#39064;&#30340;NP-hard&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#22522;&#20110;&#30005;&#23376;&#37038;&#20214;&#21644;&#35745;&#31639;&#26426;&#32593;&#32476;&#30340;&#26377;&#21521;&#21644;&#26080;&#21521;&#22270;&#65292;&#32463;&#39564;&#24615;&#22320;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;k-means&#21644;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#65288;DBSCAN&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a set of points labeled with $k$ labels, we introduce the heat map sorting problem as reordering and merging the points and dimensions while preserving the clusters (labels). A cluster is preserved if it remains connected, i.e., if it is not split into several clusters and no two clusters are merged.  We prove the problem is NP-hard and we give a fixed-parameter algorithm with a constant number of rounds in the massively parallel computation model, where each machine has a sublinear memory and the total memory of the machines is linear. We give an approximation algorithm for a NP-hard special case of the problem. We empirically compare our algorithm with k-means and density-based clustering (DBSCAN) using a dimensionality reduction via locality-sensitive hashing on several directed and undirected graphs of email and computer networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#30830;&#23450;&#24615;&#25237;&#24433;&#32622;&#20449;&#32593;&#32476;&#65288;D-PBN&#65289;&#32467;&#21512;&#21487;&#35757;&#32451;&#30340;&#22797;&#21512;&#28608;&#27963;&#20989;&#25968;&#65288;TCAs&#65289;&#65292;&#23545;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20855;&#26377;TCAs&#30340;D-PBN&#33258;&#21160;&#32534;&#30721;&#22120;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.07481</link><description>&lt;p&gt;
&#20351;&#29992;&#30830;&#23450;&#24615;&#25237;&#24433;&#32622;&#20449;&#32593;&#32476;&#25552;&#21319;&#33258;&#21160;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Improved Auto-Encoding using Deterministic Projected Belief Networks. (arXiv:2309.07481v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#30830;&#23450;&#24615;&#25237;&#24433;&#32622;&#20449;&#32593;&#32476;&#65288;D-PBN&#65289;&#32467;&#21512;&#21487;&#35757;&#32451;&#30340;&#22797;&#21512;&#28608;&#27963;&#20989;&#25968;&#65288;TCAs&#65289;&#65292;&#23545;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20855;&#26377;TCAs&#30340;D-PBN&#33258;&#21160;&#32534;&#30721;&#22120;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#30830;&#23450;&#24615;&#25237;&#24433;&#32622;&#20449;&#32593;&#32476;&#65288;D-PBN&#65289;&#30340;&#29420;&#29305;&#24615;&#36136;&#65292;&#20805;&#20998;&#21033;&#29992;&#21487;&#35757;&#32451;&#30340;&#22797;&#21512;&#28608;&#27963;&#20989;&#25968;&#65288;TCAs&#65289;&#12290;D-PBN&#26159;&#19968;&#31181;&#36890;&#36807;&#21521;&#21518;&#20256;&#25773;&#31070;&#32463;&#32593;&#32476;&#26469;&#36816;&#34892;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;TCAs&#26159;&#20855;&#26377;&#22797;&#26434;&#21333;&#35843;&#36882;&#22686;&#24418;&#29366;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#21487;&#20197;&#25913;&#21464;&#25968;&#25454;&#30340;&#20998;&#24067;&#65292;&#20351;&#21518;&#32493;&#30340;&#32447;&#24615;&#21464;&#25442;&#26356;&#21152;&#26377;&#25928;&#12290;&#22240;&#20026;D-PBN&#26159;&#36890;&#36807;&#21521;&#21518;&#20256;&#25773;&#36816;&#34892;&#30340;&#65292;&#25152;&#20197;&#22312;&#37325;&#24314;&#36807;&#31243;&#20013;&#65292;TCAs&#34987;&#21453;&#36716;&#65292;&#24674;&#22797;&#25968;&#25454;&#30340;&#21407;&#22987;&#20998;&#24067;&#65292;&#20174;&#32780;&#20805;&#20998;&#21033;&#29992;&#32473;&#23450;&#30340;TCAs&#36827;&#34892;&#20998;&#26512;&#21644;&#37325;&#24314;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20855;&#26377;TCAs&#30340;D-PBN&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#26631;&#20934;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we exploit the unique properties of a deterministic projected belief network (D-PBN) to take full advantage of trainable compound activation functions (TCAs). A D-PBN is a type of auto-encoder that operates by "backing up" through a feed-forward neural network. TCAs are activation functions with complex monotonic-increasing shapes that change the distribution of the data so that the linear transformation that follows is more effective. Because a D-PBN operates by "backing up", the TCAs are inverted in the reconstruction process, restoring the original distribution of the data, thus taking advantage of a given TCA in both analysis and reconstruction. In this paper, we show that a D-PBN auto-encoder with TCAs can significantly out-perform standard auto-encoders including variational auto-encoders.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#22768;&#23398;&#21333;&#20803;&#30340;&#30452;&#25509;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#38899;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#31639;&#27861;&#25552;&#21462;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#39044;&#27979;&#21644;&#29983;&#25104;&#35821;&#38899;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#22810;&#25968;&#35780;&#20272;&#30340;&#35821;&#35328;&#23545;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#21021;&#22987;&#21270;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.07478</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#23398;&#21333;&#20803;&#30340;&#30452;&#25509;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Direct Text to Speech Translation System using Acoustic Units. (arXiv:2309.07478v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#22768;&#23398;&#21333;&#20803;&#30340;&#30452;&#25509;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#38899;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#31639;&#27861;&#25552;&#21462;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#39044;&#27979;&#21644;&#29983;&#25104;&#35821;&#38899;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31995;&#32479;&#22312;&#22810;&#25968;&#35780;&#20272;&#30340;&#35821;&#35328;&#23545;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#21021;&#22987;&#21270;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#22768;&#23398;&#21333;&#20803;&#30340;&#30452;&#25509;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#19981;&#21516;&#28304;&#35821;&#35328;&#30340;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#22312;&#19981;&#38656;&#35201;&#35813;&#35821;&#35328;&#30340;&#25991;&#26412;&#36716;&#24405;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#30446;&#26631;&#35821;&#35328;&#30340;&#35821;&#38899;&#12290;&#21463;&#21040;&#20197;&#21069;&#30452;&#25509;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#20013;&#22768;&#23398;&#21333;&#20803;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#27969;&#27700;&#32447;&#36890;&#36807;&#32467;&#21512;&#35821;&#38899;&#32534;&#30721;&#22120;&#21644;&#32858;&#31867;&#31639;&#27861;&#25552;&#21462;&#22768;&#23398;&#21333;&#20803;&#12290;&#19968;&#26086;&#33719;&#24471;&#21333;&#20803;&#65292;&#23601;&#20250;&#35757;&#32451;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#26469;&#39044;&#27979;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#22768;&#30721;&#22120;&#20174;&#21333;&#20803;&#29983;&#25104;&#35821;&#38899;&#12290;&#25105;&#20204;&#30340;&#30452;&#25509;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#22312;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;mBART&#27169;&#22411;&#20316;&#20026;&#21021;&#22987;&#21270;&#30340;&#26032;CVSS&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#22823;&#22810;&#25968;&#35780;&#20272;&#30340;&#35821;&#35328;&#23545;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#26356;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21021;&#22987;&#21270;&#25105;&#20204;&#25552;&#20986;&#30340;&#26550;&#26500;&#26102;&#65292;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a direct text to speech translation system using discrete acoustic units. This framework employs text in different source languages as input to generate speech in the target language without the need for text transcriptions in this language. Motivated by the success of acoustic units in previous works for direct speech to speech translation systems, we use the same pipeline to extract the acoustic units using a speech encoder combined with a clustering algorithm. Once units are obtained, an encoder-decoder architecture is trained to predict them. Then a vocoder generates speech from units. Our approach for direct text to speech translation was tested on the new CVSS corpus with two different text mBART models employed as initialisation. The systems presented report competitive performance for most of the language pairs evaluated. Besides, results show a remarkable improvement when initialising our proposed architecture with a model pre-trained with more languages.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#29289;&#32852;&#32593;&#29615;&#22659;&#23450;&#21046;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#22534;&#21472;&#23376;&#32858;&#31867;&#25216;&#26415;&#26469;&#35782;&#21035;&#26410;&#30693;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.07461</link><description>&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#26816;&#27979;&#26410;&#30693;&#25915;&#20987;: &#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07461
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#29289;&#32852;&#32593;&#29615;&#22659;&#23450;&#21046;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#22534;&#21472;&#23376;&#32858;&#31867;&#25216;&#26415;&#26469;&#35782;&#21035;&#26410;&#30693;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#35774;&#22791;&#22312;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#36825;&#24341;&#20837;&#20102;&#20114;&#32852;&#30340;&#26102;&#20195;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24378;&#22823;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23433;&#20840;&#31995;&#32479;&#26159;&#22522;&#20110;&#23553;&#38381;&#19990;&#30028;&#35270;&#35282;&#35774;&#35745;&#30340;&#65292;&#24448;&#24448;&#38754;&#20020;&#19982;&#19981;&#26029;&#21457;&#23637;&#30340;&#23041;&#32961;&#29615;&#22659;&#20013;&#26032;&#30340;&#12289;&#38476;&#29983;&#30340;&#25915;&#20987;&#30456;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29289;&#32852;&#32593;&#29615;&#22659;&#19979;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#20013;&#30340;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#20174;&#32593;&#32476;&#27969;&#37327;&#20013;&#25552;&#21462;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38598;&#25104;&#20102;&#22534;&#21472;&#21644;&#23376;&#32858;&#31867;&#25216;&#26415;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#33391;&#24615;&#34892;&#20026;&#65292;&#23454;&#29616;&#23545;&#26410;&#30693;&#25915;&#20987;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread integration of Internet of Things (IoT) devices across all facets of life has ushered in an era of interconnectedness, creating new avenues for cybersecurity challenges and underscoring the need for robust intrusion detection systems. However, traditional security systems are designed with a closed-world perspective and often face challenges in dealing with the ever-evolving threat landscape, where new and unfamiliar attacks are constantly emerging. In this paper, we introduce a framework aimed at mitigating the open set recognition (OSR) problem in the realm of Network Intrusion Detection Systems (NIDS) tailored for IoT environments. Our framework capitalizes on image-based representations of packet-level data, extracting spatial and temporal patterns from network traffic. Additionally, we integrate stacking and sub-clustering techniques, enabling the identification of unknown attacks by effectively modeling the complex and diverse nature of benign behavior. The empiric
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SC-MAD&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28151;&#21512;&#39640;&#38454;&#32593;&#32476;&#23545;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#12290;&#36890;&#36807;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#28151;&#21512;&#26426;&#21046;&#36820;&#22238;&#29616;&#26377;&#26631;&#35760;&#26679;&#26412;&#30340;&#28151;&#21512;&#29289;&#65292;&#20197;&#21450;&#19968;&#31181;&#20984;&#32858;&#31867;&#28151;&#21512;&#26041;&#27861;&#29992;&#20110;&#25551;&#36848;&#22810;&#20010;&#22797;&#24418;&#22797;&#26434;&#32593;&#32476;&#20043;&#38388;&#30340;&#25968;&#25454;&#39537;&#21160;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22797;&#24418;&#22797;&#26434;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07453</link><description>&lt;p&gt;
SC-MAD: &#28151;&#21512;&#39640;&#38454;&#32593;&#32476;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SC-MAD: Mixtures of Higher-order Networks for Data Augmentation. (arXiv:2309.07453v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SC-MAD&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28151;&#21512;&#39640;&#38454;&#32593;&#32476;&#23545;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#12290;&#36890;&#36807;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#28151;&#21512;&#26426;&#21046;&#36820;&#22238;&#29616;&#26377;&#26631;&#35760;&#26679;&#26412;&#30340;&#28151;&#21512;&#29289;&#65292;&#20197;&#21450;&#19968;&#31181;&#20984;&#32858;&#31867;&#28151;&#21512;&#26041;&#27861;&#29992;&#20110;&#25551;&#36848;&#22810;&#20010;&#22797;&#24418;&#22797;&#26434;&#32593;&#32476;&#20043;&#38388;&#30340;&#25968;&#25454;&#39537;&#21160;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#22797;&#24418;&#22797;&#26434;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#22810;&#22797;&#26434;&#31995;&#32479;&#20013;&#23384;&#22312;&#30528;&#22810;&#32500;&#20132;&#20114;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#22522;&#20110;&#22270;&#30340;&#25104;&#23545;&#36830;&#25509;&#25299;&#23637;&#21040;&#39640;&#38454;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#22797;&#24418;&#22797;&#26434;&#32593;&#32476;&#24050;&#32463;&#21551;&#21457;&#20102;&#22522;&#20110;&#22797;&#24418;&#22797;&#26434;&#27169;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#19978;&#30340;&#23398;&#20064;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#38590;&#20197;&#33719;&#21462;&#25110;&#32773;&#25104;&#26412;&#39640;&#26114;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#28151;&#21512;&#26426;&#21046;&#23545;&#22797;&#24418;&#22797;&#26434;&#32593;&#32476;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36820;&#22238;&#29616;&#26377;&#26631;&#35760;&#26679;&#26412;&#30340;&#28151;&#21512;&#29289;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#25104;&#23545;&#28151;&#21512;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20984;&#32858;&#31867;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#20010;&#22797;&#24418;&#22797;&#26434;&#32593;&#32476;&#20043;&#38388;&#30340;&#25968;&#25454;&#39537;&#21160;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;&#21512;&#25104;&#22797;&#24418;&#22797;&#26434;&#32593;&#32476;&#22312;&#21516;&#24577;&#23494;&#24230;&#26041;&#38754;&#25554;&#20540;&#20102;&#29616;&#26377;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22797;&#24418;&#22797;&#26434;&#20998;&#31867;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The myriad complex systems with multiway interactions motivate the extension of graph-based pairwise connections to higher-order relations. In particular, the simplicial complex has inspired generalizations of graph neural networks (GNNs) to simplicial complex-based models. Learning on such systems requires large amounts of data, which can be expensive or impossible to obtain. We propose data augmentation of simplicial complexes through both linear and nonlinear mixup mechanisms that return mixtures of existing labeled samples. In addition to traditional pairwise mixup, we present a convex clustering mixup approach for a data-driven relationship among several simplicial complexes. We theoretically demonstrate that the resultant synthetic simplicial complexes interpolate among existing data with respect to homomorphism densities. Our method is demonstrated on both synthetic and real-world datasets for simplicial complex classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#35299;&#20915;&#22270;&#31070;&#32463;&#20999;&#21521;&#26680;&#26159;&#21542;&#31561;&#20215;&#20110;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19977;&#20010;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.07452</link><description>&lt;p&gt;
&#35299;&#20915;&#22270;&#31070;&#32463;&#20999;&#21521;&#26680;&#26159;&#21542;&#31561;&#20215;&#20110;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Solving Graph Neural Tangent Kernel Equivalent to Training Graph Neural Network?. (arXiv:2309.07452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35299;&#20915;&#22270;&#31070;&#32463;&#20999;&#21521;&#26680;&#26159;&#21542;&#31561;&#20215;&#20110;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19977;&#20010;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#21407;&#29702;&#30340;&#19968;&#20010;&#26032;&#36235;&#21183;&#26159;&#36890;&#36807;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#26469;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;NTK&#26159;&#19968;&#31181;&#31561;&#20215;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#22810;&#23618;&#26080;&#38480;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#26041;&#27861;&#12290;NTK&#22312;&#29702;&#35770;&#28145;&#24230;&#23398;&#20064;&#20013;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#20256;&#32479;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#36136;&#65292;&#24182;&#20174;&#29702;&#35770;&#35282;&#24230;&#35299;&#37322;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#12290;&#22312;&#22270;&#23398;&#20064;&#20013;&#65292;NTK&#30340;&#19968;&#31181;&#33258;&#28982;&#25512;&#24191;&#26159;&#22270;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;GNTK&#65289;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25552;&#20986;&#20102;GNTK&#30340;&#20844;&#24335;&#65292;&#24182;&#20174;&#23454;&#35777;&#19978;&#34920;&#26126;&#35813;&#26680;&#26041;&#27861;&#22312;&#21508;&#31181;&#29983;&#29289;&#20449;&#24687;&#23398;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#36798;&#21040;&#19982;GNN&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;&#29616;&#22312;&#21097;&#19979;&#30340;&#38382;&#39064;&#26159;&#65292;&#35299;&#20915;GNTK&#22238;&#24402;&#26159;&#21542;&#31561;&#20215;&#20110;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#26080;&#38480;&#23485;&#22810;&#23618;GNN&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A rising trend in theoretical deep learning is to understand why deep learning works through Neural Tangent Kernel (NTK) [jgh18], a kernel method that is equivalent to using gradient descent to train a multi-layer infinitely-wide neural network. NTK is a major step forward in the theoretical deep learning because it allows researchers to use traditional mathematical tools to analyze properties of deep neural networks and to explain various neural network techniques from a theoretical view. A natural extension of NTK on graph learning is \textit{Graph Neural Tangent Kernel (GNTK)}, and researchers have already provide GNTK formulation for graph-level regression and show empirically that this kernel method can achieve similar accuracy as GNNs on various bioinformatics datasets [dhs+19]. The remaining question now is whether solving GNTK regression is equivalent to training an infinite-wide multi-layer GNN using gradient descent. In this paper, we provide three new theoretical results. Fi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;TensorFlow&#24211;&#26469;&#39044;&#27979;&#39640;&#32500;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;TensorFlow&#24211;&#30340;&#38271;&#26399;&#39044;&#27979;&#34892;&#20026;&#26377;&#22833;&#25511;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07450</link><description>&lt;p&gt;
TensorFlow&#28151;&#27788;&#39044;&#27979;&#19982;&#22833;&#25511;
&lt;/p&gt;
&lt;p&gt;
TensorFlow Chaotic Prediction and Blow Up. (arXiv:2309.07450v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;TensorFlow&#24211;&#26469;&#39044;&#27979;&#39640;&#32500;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;TensorFlow&#24211;&#30340;&#38271;&#26399;&#39044;&#27979;&#34892;&#20026;&#26377;&#22833;&#25511;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#28151;&#27788;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#26159;&#31070;&#32463;&#32593;&#32476;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#39033;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#26088;&#22312;&#39044;&#27979;&#19968;&#20010;&#39640;&#32500;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26102;&#31354;&#28151;&#27788;&#21160;&#24577;&#12290;&#22312;&#25105;&#20204;&#30340;&#23581;&#35797;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20195;&#34920;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#39044;&#27979;&#30340;TensorFlow&#24211;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#25152;&#32771;&#34385;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#21487;&#20197;&#22312;&#30701;&#26102;&#38388;&#20869;&#34987;&#39044;&#27979;&#65292;&#20294;&#25105;&#20204;&#20063;&#38388;&#25509;&#21457;&#29616;&#20102;TensorFlow&#24211;&#30340;&#19968;&#20010;&#24847;&#22806;&#21644;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#30001;&#20110;TensorFlow&#24211;&#30340;&#38750;&#30830;&#23450;&#24615;&#34892;&#20026;&#65292;&#31995;&#32479;&#30340;&#38271;&#26399;&#39044;&#27979;&#34892;&#20026;&#36805;&#36895;&#24694;&#21270;&#21644;&#23849;&#28291;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30701;&#26399;&#39044;&#27979;&#33021;&#21147;&#30340;&#25968;&#20540;&#35777;&#25454;&#65292;&#20197;&#21450;&#38271;&#26399;&#21487;&#39044;&#27979;&#24615;&#30340;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting the dynamics of chaotic systems is one of the most challenging tasks for neural networks, and machine learning in general. Here we aim to predict the spatiotemporal chaotic dynamics of a high-dimensional non-linear system. In our attempt we use the TensorFlow library, representing the state of the art for deep neural networks training and prediction. While our results are encouraging, and show that the dynamics of the considered system can be predicted for short time, we also indirectly discovered an unexpected and undesirable behavior of the TensorFlow library. More specifically, the longer term prediction of the system's chaotic behavior quickly deteriorates and blows up due to the nondeterministic behavior of the TensorFlow library. Here we provide numerical evidence of the short time prediction ability, and of the longer term predictability blow up.
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24352;&#37327;&#21644;SVM Trick&#37325;&#26032;&#26500;&#24314;&#20102;&#21333;&#23618;LLM&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#24212;&#30340;&#20248;&#21270;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#22312;&#30697;&#38453;&#20056;&#27861;&#26102;&#38388;&#20869;&#35299;&#20915;&#27880;&#24847;&#21147;&#22238;&#24402;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07418</link><description>&lt;p&gt;
&#22312;&#24352;&#37327;&#21644;SVM Trick&#22522;&#30784;&#19978;&#37325;&#26032;&#26500;&#24314;&#21333;&#23618;LLM&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#22312;&#30697;&#38453;&#20056;&#27861;&#26102;&#38388;&#20869;&#35299;&#20915;&#23427;&#30340;&#24555;&#36895;&#20248;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Fast Optimization View: Reformulating Single Layer Attention in LLM Based on Tensor and SVM Trick, and Solving It in Matrix Multiplication Time. (arXiv:2309.07418v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07418
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24352;&#37327;&#21644;SVM Trick&#37325;&#26032;&#26500;&#24314;&#20102;&#21333;&#23618;LLM&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#24212;&#30340;&#20248;&#21270;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21487;&#20197;&#22312;&#30697;&#38453;&#20056;&#27861;&#26102;&#38388;&#20869;&#35299;&#20915;&#27880;&#24847;&#21147;&#22238;&#24402;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#37117;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#35299;&#20915;LLMs&#20013;&#30340;&#27880;&#24847;&#21147;&#22238;&#24402;&#26159;&#20248;&#21270;LLMs&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#20026;&#19968;&#23618;&#27880;&#24847;&#21147;&#32593;&#32476;&#30446;&#26631;&#20989;&#25968; $L(X,Y) = \sum_{j_0 = 1}^n \sum_{i_0 = 1}^d ( \langle \langle \exp( \mathsf{A}_{j_0} x ) , {\bf 1}_n \rangle^{-1} \exp( \mathsf{A}_{j_0} x ), A_{3} Y_{*,i_0} \rangle - b_{j_0,i_0} )^2$ &#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#36825;&#37324; $\mathsf{A} \in \mathbb{R}^{n^2 \times d^2}$ &#26159;$A_1 \in \mathbb{R}^{n \times d}$ &#21644; $A_2 \in \mathbb{R}^{n \times d}$ &#30340;Kronecker&#31215;&#12290;$A_3$ &#26159;$\mathbb{R}^{n \times d}$ &#20013;&#30340;&#19968;&#20010;&#30697;&#38453;&#65292;$\mathsf{A}_{j_0} \in \mathbb{R}^{n \times d^2}$ &#26159;$\mathsf{A}$ &#30340;&#31532;$j_0$&#20010;&#22359;&#12290;$X, Y \in \mathbb{R}^{d \times d}$ &#26159;&#25105;&#20204;&#35201;&#23398;&#20064;&#30340;&#21464;&#37327;&#12290;$B \in \mathbb{R}^{n \times d}$ &#21644; $b_{j_0,i_0} \in \mathbb{R}$ &#26159;$B$ &#30340;&#31532;$j_0$&#34892;&#21644;&#31532;$i_0$&#21015;&#30340;&#19968;&#20010;&#20803;&#32032;&#65292;$Y_{*,i_0} \in \mathbb{R}^d$ &#26159;$Y$ &#30340;&#31532;$i_0$&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have played a pivotal role in revolutionizing various facets of our daily existence. Solving attention regression is a fundamental task in optimizing LLMs. In this work, we focus on giving a provable guarantee for the one-layer attention network objective function $L(X,Y) = \sum_{j_0 = 1}^n \sum_{i_0 = 1}^d ( \langle \langle \exp( \mathsf{A}_{j_0} x ) , {\bf 1}_n \rangle^{-1} \exp( \mathsf{A}_{j_0} x ), A_{3} Y_{*,i_0} \rangle - b_{j_0,i_0} )^2$. Here $\mathsf{A} \in \mathbb{R}^{n^2 \times d^2}$ is Kronecker product between $A_1 \in \mathbb{R}^{n \times d}$ and $A_2 \in \mathbb{R}^{n \times d}$. $A_3$ is a matrix in $\mathbb{R}^{n \times d}$, $\mathsf{A}_{j_0} \in \mathbb{R}^{n \times d^2}$ is the $j_0$-th block of $\mathsf{A}$. The $X, Y \in \mathbb{R}^{d \times d}$ are variables we want to learn. $B \in \mathbb{R}^{n \times d}$ and $b_{j_0,i_0} \in \mathbb{R}$ is one entry at $j_0$-th row and $i_0$-th column of $B$, $Y_{*,i_0} \in \mathbb{R}^d$ is the $i_
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LRNN&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LRNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37197;&#22791;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#19988;&#36755;&#20837;&#30456;&#20851;&#30340;&#36716;&#31227;&#30697;&#38453;&#65292;&#24182;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22312;&#27491;&#21017;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#30340;LRNN&#12290;</title><link>http://arxiv.org/abs/2309.07412</link><description>&lt;p&gt;
&#22312;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#25512;&#36827;&#27491;&#21017;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Advancing Regular Language Reasoning in Linear Recurrent Neural Networks. (arXiv:2309.07412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07412
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LRNN&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LRNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37197;&#22791;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#19988;&#36755;&#20837;&#30456;&#20851;&#30340;&#36716;&#31227;&#30697;&#38453;&#65292;&#24182;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22312;&#27491;&#21017;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#30340;LRNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LRNN&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#31243;&#24314;&#27169;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#24555;&#36895;&#30340;&#24182;&#34892;&#35757;&#32451;&#21644;&#24658;&#23450;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#22312;&#23545;LRNN&#37325;&#26032;&#20135;&#29983;&#20852;&#36259;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#35757;&#32451;&#24207;&#21015;&#20013;&#30340;&#38544;&#34255;&#35268;&#21017;&#65292;&#20363;&#22914;&#27491;&#21017;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#23545;&#19968;&#20123;&#24050;&#26377;&#30340;LRNN&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27491;&#21017;&#35821;&#35328;&#19978;&#23384;&#22312;&#38480;&#21046;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LRNN&#65292;&#37197;&#22791;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#21644;&#36755;&#20837;&#30456;&#20851;&#30340;&#36716;&#31227;&#30697;&#38453;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22312;&#27491;&#21017;&#35821;&#35328;&#20219;&#21153;&#65288;&#22914;&#27714;&#21644;&#12289;&#20598;&#25968;&#23545;&#12289;&#27169;&#36816;&#31639;&#31561;&#65289;&#19978;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#30340;LRNN&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SemiGCL&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#26368;&#23567;&#26368;&#22823;&#29109;&#35757;&#32451;&#26469;&#35299;&#20915;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#24471;&#34920;&#31034;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23545;&#25239;&#20248;&#21270;&#20943;&#23567;&#22495;&#24046;&#24322;&#12290;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07402</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#26368;&#23567;&#26368;&#22823;&#29109;&#30340;&#22270;&#19978;&#21322;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Domain Adaptation on Graphs with Contrastive Learning and Minimax Entropy. (arXiv:2309.07402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SemiGCL&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#26368;&#23567;&#26368;&#22823;&#29109;&#35757;&#32451;&#26469;&#35299;&#20915;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#24471;&#34920;&#31034;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#23545;&#25239;&#20248;&#21270;&#20943;&#23567;&#22495;&#24046;&#24322;&#12290;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#26631;&#35760;&#25104;&#26412;&#39640;&#26114;&#65292;&#22270;&#20013;&#30340;&#26631;&#31614;&#31232;&#32570;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#36935;&#21040;&#12290;&#20026;&#27492;&#65292;&#22270;&#19978;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;(SSDA)&#26088;&#22312;&#21033;&#29992;&#26631;&#35760;&#28304;&#22270;&#30340;&#30693;&#35782;&#26469;&#24110;&#21161;&#26377;&#38480;&#26631;&#31614;&#30340;&#30446;&#26631;&#22270;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#12290;SSDA&#20219;&#21153;&#38656;&#35201;&#20811;&#26381;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#20043;&#38388;&#30340;&#22495;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#36328;&#22270;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#23578;&#26410;&#27491;&#24335;&#32771;&#34385;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#19978;&#30340;SSDA&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SemiGCL&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#33719;&#30410;&#20110;&#22270;&#23545;&#27604;&#23398;&#20064;&#21644;&#26368;&#23567;&#26368;&#22823;&#29109;&#35757;&#32451;&#12290;SemiGCL&#36890;&#36807;&#23545;&#27604;&#20174;&#22270;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#35270;&#22270;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#26469;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;SemiGCL&#36890;&#36807;&#30446;&#26631;&#22270;&#20013;&#26410;&#26631;&#35760;&#33410;&#28857;&#30340;&#29109;&#25439;&#22833;&#36827;&#34892;&#23545;&#25239;&#20248;&#21270;&#65292;&#20197;&#20943;&#23567;&#22495;&#24046;&#24322;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SemiGCL&#22312;&#22270;&#19978;&#30340;SSDA&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label scarcity in a graph is frequently encountered in real-world applications due to the high cost of data labeling. To this end, semi-supervised domain adaptation (SSDA) on graphs aims to leverage the knowledge of a labeled source graph to aid in node classification on a target graph with limited labels. SSDA tasks need to overcome the domain gap between the source and target graphs. However, to date, this challenging research problem has yet to be formally considered by the existing approaches designed for cross-graph node classification. To tackle the SSDA problem on graphs, a novel method called SemiGCL is proposed, which benefits from graph contrastive learning and minimax entropy training. SemiGCL generates informative node representations by contrasting the representations learned from a graph's local and global views. Additionally, SemiGCL is adversarially optimized with the entropy loss of unlabeled target nodes to reduce domain divergence. Experimental results on benchmark d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#35821;&#20041;&#23545;&#25239;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#26377;&#20004;&#20010;&#21464;&#31181;&#26041;&#27861;&#65306;&#35821;&#20041;&#36716;&#25442;&#26041;&#27861;(ST)&#65292;&#36890;&#36807;&#24494;&#35843;&#28508;&#22312;&#31354;&#38388;&#21644;/&#25110;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#26469;&#29983;&#25104;&#22270;&#20687;&#65307;&#28508;&#22312;&#23631;&#34109;&#26041;&#27861;(LM)&#65292;&#21033;&#29992;&#21478;&#19968;&#30446;&#26631;&#22270;&#20687;&#23631;&#34109;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#23616;&#37096;&#21453;&#21521;&#20256;&#25773;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07398</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#35821;&#20041;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Semantic Adversarial Attacks via Diffusion Models. (arXiv:2309.07398v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#35821;&#20041;&#23545;&#25239;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#26377;&#20004;&#20010;&#21464;&#31181;&#26041;&#27861;&#65306;&#35821;&#20041;&#36716;&#25442;&#26041;&#27861;(ST)&#65292;&#36890;&#36807;&#24494;&#35843;&#28508;&#22312;&#31354;&#38388;&#21644;/&#25110;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#26469;&#29983;&#25104;&#22270;&#20687;&#65307;&#28508;&#22312;&#23631;&#34109;&#26041;&#27861;(LM)&#65292;&#21033;&#29992;&#21478;&#19968;&#30446;&#26631;&#22270;&#20687;&#23631;&#34109;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#23616;&#37096;&#21453;&#21521;&#20256;&#25773;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#23545;&#25239;&#25915;&#20987;&#20027;&#35201;&#36890;&#36807;&#22312;&#20687;&#32032;&#31354;&#38388;&#20013;&#28155;&#21152;&#23545;&#25239;&#24615;&#25200;&#21160;&#26469;&#25805;&#32437;&#24178;&#20928;&#30340;&#26679;&#26412;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#35821;&#20041;&#23545;&#25239;&#25915;&#20987;&#26356;&#21152;&#20851;&#27880;&#25913;&#21464;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#23646;&#24615;&#65292;&#20363;&#22914;&#39068;&#33394;&#12289;&#19978;&#19979;&#25991;&#21644;&#29305;&#24449;&#65292;&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#26356;&#20855;&#21487;&#34892;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#29983;&#25104;&#35821;&#20041;&#23545;&#25239;&#25915;&#20987;&#65292;&#22240;&#20026;&#35821;&#20041;&#20449;&#24687;&#21253;&#21547;&#22312;&#35757;&#32451;&#26377;&#32032;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#65292;&#35813;&#26694;&#26550;&#26377;&#20004;&#20010;&#21464;&#31181;&#65306;1) &#35821;&#20041;&#36716;&#25442;&#26041;&#27861;(ST)&#36890;&#36807;&#24494;&#35843;&#29983;&#25104;&#22270;&#20687;&#30340;&#28508;&#22312;&#31354;&#38388;&#21644;/&#25110;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#65307;2) &#28508;&#22312;&#23631;&#34109;&#26041;&#27861;(LM)&#20351;&#29992;&#21478;&#19968;&#30446;&#26631;&#22270;&#20687;&#23631;&#34109;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#23616;&#37096;&#21453;&#21521;&#20256;&#25773;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;ST&#26041;&#27861;&#21487;&#22312;&#30333;&#30418;&#25110;&#40657;&#30418;&#35774;&#32622;&#20013;&#24212;&#29992;&#12290;&#22312;CelebA-HQ&#21644;AFHQ&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional adversarial attacks concentrate on manipulating clean examples in the pixel space by adding adversarial perturbations. By contrast, semantic adversarial attacks focus on changing semantic attributes of clean examples, such as color, context, and features, which are more feasible in the real world. In this paper, we propose a framework to quickly generate a semantic adversarial attack by leveraging recent diffusion models since semantic information is included in the latent space of well-trained diffusion models. Then there are two variants of this framework: 1) the Semantic Transformation (ST) approach fine-tunes the latent space of the generated image and/or the diffusion model itself; 2) the Latent Masking (LM) approach masks the latent space with another target image and local backpropagation-based interpretation methods. Additionally, the ST approach can be applied in either white-box or black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;EnCodecMAE&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;EnCodec&#29983;&#25104;&#31163;&#25955;&#30446;&#26631;&#65292;&#29992;&#20110;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#23398;&#20064;&#36890;&#29992;&#38899;&#39057;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#28085;&#30422;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#29615;&#22659;&#22768;&#38899;&#30340;&#22810;&#20010;&#38899;&#39057;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;EnCodecMAE&#36798;&#21040;&#20102;&#19982;&#39046;&#20808;&#30340;&#38899;&#39057;&#34920;&#31034;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07391</link><description>&lt;p&gt;
EnCodecMAE: &#21033;&#29992;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#36827;&#34892;&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EnCodecMAE: Leveraging neural codecs for universal audio representation learning. (arXiv:2309.07391v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;EnCodecMAE&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;EnCodec&#29983;&#25104;&#31163;&#25955;&#30446;&#26631;&#65292;&#29992;&#20110;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#23398;&#20064;&#36890;&#29992;&#38899;&#39057;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#28085;&#30422;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#29615;&#22659;&#22768;&#38899;&#30340;&#22810;&#20010;&#38899;&#39057;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#65292;&#21457;&#29616;EnCodecMAE&#36798;&#21040;&#20102;&#19982;&#39046;&#20808;&#30340;&#38899;&#39057;&#34920;&#31034;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#33719;&#24471;&#21487;&#20197;&#29992;&#20110;&#28041;&#21450;&#35821;&#38899;&#12289;&#38899;&#20048;&#25110;&#29615;&#22659;&#22768;&#38899;&#30340;&#21508;&#31181;&#21518;&#32493;&#20219;&#21153;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;&#21463;&#33258;&#30417;&#30563;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38899;&#39057;&#12290;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#25991;&#26412;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#22240;&#27492;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#38899;&#39057;&#38656;&#35201;&#25913;&#21464;&#23398;&#20064;&#30446;&#26631;&#25110;&#23558;&#38899;&#39057;&#20449;&#21495;&#26144;&#23556;&#21040;&#19968;&#32452;&#31163;&#25955;&#31867;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;EnCodec&#29983;&#25104;&#29992;&#20110;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#23398;&#20064;&#36890;&#29992;&#38899;&#39057;&#27169;&#22411;&#30340;&#31163;&#25955;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;EnCodecMAE&#65292;&#22312;&#28085;&#30422;&#35821;&#38899;&#12289;&#38899;&#20048;&#21644;&#29615;&#22659;&#22768;&#38899;&#30340;&#24191;&#27867;&#38899;&#39057;&#20219;&#21153;&#19978;&#65292;&#20854;&#24615;&#33021;&#30456;&#24403;&#25110;&#20248;&#20110;&#39046;&#20808;&#30340;&#38899;&#39057;&#34920;&#31034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of universal audio representation learning is to obtain foundational models that can be used for a variety of downstream tasks involving speech, music or environmental sounds. To approach this problem, methods inspired by self-supervised models from NLP, like BERT, are often used and adapted to audio. These models rely on the discrete nature of text, hence adopting this type of approach for audio processing requires either a change in the learning objective or mapping the audio signal to a set of discrete classes. In this work, we explore the use of EnCodec, a neural audio codec, to generate discrete targets for learning an universal audio model based on a masked autoencoder (MAE). We evaluate this approach, which we call EncodecMAE, on a wide range of audio tasks spanning speech, music and environmental sounds, achieving performances comparable or better than leading audio representation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#20540;&#20989;&#25968;&#36817;&#20284;&#22312;&#29305;&#23450;&#26412;&#22320;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#36816;&#29992;&#31639;&#23376;&#26041;&#31243;&#36827;&#34892;&#31163;&#32447;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#32500;&#36817;&#20284;&#31354;&#38388;&#20013;&#30340;&#21151;&#29575;&#20989;&#25968;&#24471;&#21040;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#21644;&#32454;&#21270;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07383</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#36817;&#20284;&#30340;&#26576;&#20123;&#26412;&#22320;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning. (arXiv:2309.07383v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#20540;&#20989;&#25968;&#36817;&#20284;&#22312;&#29305;&#23450;&#26412;&#22320;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#36816;&#29992;&#31639;&#23376;&#26041;&#31243;&#36827;&#34892;&#31163;&#32447;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#32500;&#36817;&#20284;&#31354;&#38388;&#20013;&#30340;&#21151;&#29575;&#20989;&#25968;&#24471;&#21040;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#21644;&#32454;&#21270;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#32452;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;$H(\Omega)$&#20013;&#20986;&#29616;&#30340;&#19968;&#20123;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#31867;&#30340;&#26412;&#22320;&#31354;&#38388;&#20013;&#26500;&#24314;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31163;&#32447;&#36817;&#20284;&#30340;&#31639;&#23376;&#26041;&#31243;&#30340;&#24378;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#20010;&#31639;&#23376;&#26041;&#31243;&#20986;&#29616;&#22312;&#31574;&#30053;&#36845;&#20195;&#20013;&#12290;&#21033;&#29992;&#26377;&#38480;&#32500;&#36817;&#20284;&#31354;&#38388;$H_N$&#22312;&#26412;&#22320;&#31354;&#38388;$H(\Omega)$&#20013;&#30340;&#21151;&#29575;&#20989;&#25968;$\Pwr_{H,N}$&#65292;&#24471;&#21040;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#35823;&#24046;&#30340;&#26174;&#24335;&#19978;&#30028;&#12290;&#36825;&#20123;&#19978;&#30028;&#20855;&#26377;&#20960;&#20309;&#24615;&#36136;&#65292;&#24182;&#23545;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#26377;&#20102;&#19968;&#20123;&#25913;&#36827;&#21644;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies convergence rates for some value function approximations that arise in a collection of reproducing kernel Hilbert spaces (RKHS) $H(\Omega)$. By casting an optimal control problem in a specific class of native spaces, strong rates of convergence are derived for the operator equation that enables offline approximations that appear in policy iteration. Explicit upper bounds on error in value function approximations are derived in terms of power function $\Pwr_{H,N}$ for the space of finite dimensional approximants $H_N$ in the native space $H(\Omega)$. These bounds are geometric in nature and refine some well-known, now classical results concerning convergence of approximations of value functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#20998;&#20301;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#40065;&#26834;&#25955;&#24230;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#20998;&#20301;&#25968;&#22238;&#24402;&#23545;&#24322;&#24120;&#20540;&#29305;&#24449;&#25935;&#24863;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07374</link><description>&lt;p&gt;
&#24322;&#24120;&#20540;&#23384;&#22312;&#19979;&#30340;&#40065;&#26834;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;Beta&#20998;&#20301;&#25968;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Beta quantile regression for robust estimation of uncertainty in the presence of outliers. (arXiv:2309.07374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#20998;&#20301;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#40065;&#26834;&#25955;&#24230;&#30340;&#27010;&#24565;&#65292;&#35299;&#20915;&#20102;&#20998;&#20301;&#25968;&#22238;&#24402;&#23545;&#24322;&#24120;&#20540;&#29305;&#24449;&#25935;&#24863;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#20301;&#25968;&#22238;&#24402;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#29983;&#25104;&#39044;&#27979;&#21306;&#38388;&#12290;&#22312;&#20020;&#24202;&#35786;&#26029;&#31561;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#29305;&#21035;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#30830;&#23450;&#30142;&#30149;&#29366;&#20917;&#21644;&#21046;&#23450;&#36866;&#24403;&#27835;&#30103;&#26041;&#26696;&#26102;&#65292;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#30495;&#23454;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#20998;&#20301;&#25968;&#22238;&#24402;&#27169;&#22411;&#26368;&#24120;&#35265;&#30340;&#24212;&#29992;&#26159;&#22312;&#26080;&#27861;&#25351;&#23450;&#21442;&#25968;&#20284;&#28982;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#20998;&#20301;&#25968;&#22238;&#24402;&#23545;&#24322;&#24120;&#20540;&#30340;&#21709;&#24212;&#35266;&#23519;&#20855;&#26377;&#36739;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#24322;&#24120;&#20540;&#30340;&#21327;&#21464;&#35266;&#23519;&#65288;&#29305;&#24449;&#65289;&#25935;&#24863;&#12290;&#24322;&#24120;&#20540;&#29305;&#24449;&#21487;&#33021;&#20250;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#22238;&#24402;&#38382;&#39064;&#30340;&#24615;&#33021;&#65292;&#22914;&#39118;&#26684;&#36716;&#25442;&#12289;&#22270;&#20687;&#37325;&#24314;&#21644;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#65292;&#28508;&#22312;&#22320;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#35770;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#20998;&#20301;&#25968;&#22238;&#24402;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#34701;&#21512;&#20102;&#40065;&#26834;&#25955;&#24230;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantile Regression (QR) can be used to estimate aleatoric uncertainty in deep neural networks and can generate prediction intervals. Quantifying uncertainty is particularly important in critical applications such as clinical diagnosis, where a realistic assessment of uncertainty is essential in determining disease status and planning the appropriate treatment. The most common application of quantile regression models is in cases where the parametric likelihood cannot be specified. Although quantile regression is quite robust to outlier response observations, it can be sensitive to outlier covariate observations (features). Outlier features can compromise the performance of deep learning regression problems such as style translation, image reconstruction, and deep anomaly detection, potentially leading to misleading conclusions. To address this problem, we propose a robust solution for quantile regression that incorporates concepts from robust divergence. We compare the performance of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#24179;&#34913;&#26041;&#31243;&#65292;&#35299;&#37322;&#20102;&#25968;&#25454;&#38598;&#20998;&#24067;&#20272;&#35745;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#23610;&#24230;&#26426;&#21046;&#12290;&#32593;&#32476;&#30340;&#36755;&#20986;&#26159;&#25968;&#25454;&#38598;&#30340;&#23616;&#37096;&#24179;&#22343;&#65292;&#24179;&#22343;&#30340;&#23610;&#24230;&#38543;&#30528;&#35757;&#32451;&#36880;&#28176;&#20943;&#23567;&#24182;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07367</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#24179;&#34913;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
The kernel-balanced equation for deep neural networks. (arXiv:2309.07367v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#24179;&#34913;&#26041;&#31243;&#65292;&#35299;&#37322;&#20102;&#25968;&#25454;&#38598;&#20998;&#24067;&#20272;&#35745;&#20013;&#30340;&#19981;&#31283;&#23450;&#24615;&#21644;&#23610;&#24230;&#26426;&#21046;&#12290;&#32593;&#32476;&#30340;&#36755;&#20986;&#26159;&#25968;&#25454;&#38598;&#30340;&#23616;&#37096;&#24179;&#22343;&#65292;&#24179;&#22343;&#30340;&#23610;&#24230;&#38543;&#30528;&#35757;&#32451;&#36880;&#28176;&#20943;&#23567;&#24182;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#23637;&#29616;&#20986;&#35768;&#22810;&#26377;&#30410;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#26377;&#38480;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#32593;&#32476;&#21487;&#20197;&#33719;&#24471;&#24191;&#20041;&#20989;&#25968;&#12290;&#24191;&#20041;&#21270;&#31243;&#24230;&#26159;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#36817;&#20284;&#23610;&#24230;&#30340;&#23454;&#29616;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#25968;&#25454;&#38598;&#22797;&#26434;&#26102;&#65292;&#23610;&#24230;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#38598;&#20998;&#24067;&#20272;&#35745;&#30340;&#32593;&#32476;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20272;&#35745;&#26159;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#19981;&#31283;&#23450;&#24615;&#21462;&#20915;&#20110;&#25968;&#25454;&#23494;&#24230;&#21644;&#35757;&#32451;&#25345;&#32493;&#26102;&#38388;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26680;&#24179;&#34913;&#26041;&#31243;&#65292;&#23427;&#32473;&#20986;&#20102;&#35299;&#30340;&#31616;&#30701;&#29616;&#35937;&#23398;&#25551;&#36848;&#12290;&#35813;&#26041;&#31243;&#21578;&#35785;&#25105;&#20204;&#19981;&#31283;&#23450;&#24615;&#30340;&#21407;&#22240;&#21644;&#23610;&#24230;&#30340;&#26426;&#21046;&#12290;&#32593;&#32476;&#30340;&#36755;&#20986;&#20316;&#20026;&#39044;&#27979;&#26159;&#25968;&#25454;&#38598;&#30340;&#23616;&#37096;&#24179;&#22343;&#65292;&#24179;&#22343;&#30340;&#23610;&#24230;&#27839;&#30528;&#26041;&#31243;&#30830;&#23450;&#12290;&#23610;&#24230;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#65292;&#26368;&#32456;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have shown many fruitful applications in this decade. A network can get the generalized function through training with a finite dataset. The degree of generalization is a realization of the proximity scale in the data space. Specifically, the scale is not clear if the dataset is complicated. Here we consider a network for the distribution estimation of the dataset. We show the estimation is unstable and the instability depends on the data density and training duration. We derive the kernel-balanced equation, which gives a short phenomenological description of the solution. The equation tells us the reason for the instability and the mechanism of the scale. The network outputs a local average of the dataset as a prediction and the scale of averaging is determined along the equation. The scale gradually decreases along training and finally results in instability in our case.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35946;&#22855;&#20998;&#35299;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21333;&#32431;&#22797;&#21512;&#20307;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35889;&#20449;&#24687;&#30340;&#23884;&#20837;&#12290;&#36890;&#36807;&#32534;&#30721;&#25968;&#25454;&#19981;&#21464;&#24615;&#21644;&#35774;&#35745;&#21512;&#36866;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#37325;&#26032;&#26435;&#34913;&#36127;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21453;&#26144;&#35889;&#20449;&#24687;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.07364</link><description>&lt;p&gt;
Hodge-Aware Contrastive Learning&#65288;&#22522;&#20110;&#35946;&#22855;&#20998;&#35299;&#30340;&#23545;&#27604;&#23398;&#20064;&#65289;
&lt;/p&gt;
&lt;p&gt;
Hodge-Aware Contrastive Learning. (arXiv:2309.07364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35946;&#22855;&#20998;&#35299;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21333;&#32431;&#22797;&#21512;&#20307;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35889;&#20449;&#24687;&#30340;&#23884;&#20837;&#12290;&#36890;&#36807;&#32534;&#30721;&#25968;&#25454;&#19981;&#21464;&#24615;&#21644;&#35774;&#35745;&#21512;&#36866;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#37325;&#26032;&#26435;&#34913;&#36127;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21453;&#26144;&#35889;&#20449;&#24687;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#32431;&#22797;&#21512;&#20307;&#22312;&#23545;&#20855;&#26377;&#22810;&#21521;&#20381;&#36182;&#20851;&#31995;&#30340;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20363;&#22914;&#22312;&#32593;&#32476;&#30340;&#36793;&#32536;&#19978;&#23450;&#20041;&#30340;&#25968;&#25454;&#25110;&#20854;&#20182;&#39640;&#38454;&#32467;&#26500;&#20869;&#30340;&#25968;&#25454;&#12290;&#36890;&#36807;&#35946;&#22855;&#20998;&#35299;&#65292;&#21487;&#20197;&#23558;&#20854;&#35889;&#20998;&#35299;&#20026;&#19977;&#20010;&#21487;&#35299;&#37322;&#30340;&#23376;&#31354;&#38388;&#65292;&#36825;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#20855;&#26377;&#22522;&#30784;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#20998;&#35299;&#26469;&#24320;&#21457;&#19968;&#31181;&#23545;&#27604;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#21333;&#32431;&#22797;&#21512;&#20307;&#25968;&#25454;&#24182;&#29983;&#25104;&#34164;&#21547;&#29305;&#23450;&#35889;&#20449;&#24687;&#30340;&#23884;&#20837;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#21333;&#32431;&#31070;&#32463;&#32593;&#32476;&#26469;&#32534;&#30721;&#30456;&#20851;&#25968;&#25454;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#35774;&#35745;&#20986;&#20855;&#26377;&#36866;&#24403;&#35889;&#29305;&#24615;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#27491;&#23545;&#27604;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#36127;&#26679;&#26412;&#30340;&#35946;&#22855;&#20998;&#37327;&#19982;&#38170;&#28857;&#30456;&#20284;&#24615;&#65292;&#37325;&#26032;&#26435;&#34913;&#23545;&#27604;&#25439;&#22833;&#20013;&#36127;&#26679;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21152;&#24378;&#36739;&#23569;&#30456;&#20284;&#23454;&#20363;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#21453;&#26144;&#35889;&#20449;&#24687;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simplicial complexes prove effective in modeling data with multiway dependencies, such as data defined along the edges of networks or within other higher-order structures. Their spectrum can be decomposed into three interpretable subspaces via the Hodge decomposition, resulting foundational in numerous applications. We leverage this decomposition to develop a contrastive self-supervised learning approach for processing simplicial data and generating embeddings that encapsulate specific spectral information.Specifically, we encode the pertinent data invariances through simplicial neural networks and devise augmentations that yield positive contrastive examples with suitable spectral properties for downstream tasks. Additionally, we reweight the significance of negative examples in the contrastive loss, considering the similarity of their Hodge components to the anchor. By encouraging a stronger separation among less similar instances, we obtain an embedding space that reflects the spect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cluster-Bootstrap PLS&#65288;CLUB-PLS&#65289;&#30340;&#22522;&#20110;&#20559;&#26368;&#23567;&#20108;&#20056;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24433;&#20687;&#36951;&#20256;&#23398;&#20013;&#30340;&#32500;&#24230;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#20004;&#20010;&#39046;&#22495;&#30340;&#22823;&#37327;&#36755;&#20837;&#32500;&#24230;&#21644;&#22823;&#26679;&#26412;&#37327;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#33258;&#21161;&#27861;&#25552;&#20379;&#31283;&#20581;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.07352</link><description>&lt;p&gt;
&#20351;&#29992;CLUB-PLS&#35299;&#20915;&#24433;&#20687;&#36951;&#20256;&#23398;&#20013;&#30340;&#32500;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling the dimensions in imaging genetics with CLUB-PLS. (arXiv:2309.07352v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Cluster-Bootstrap PLS&#65288;CLUB-PLS&#65289;&#30340;&#22522;&#20110;&#20559;&#26368;&#23567;&#20108;&#20056;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24433;&#20687;&#36951;&#20256;&#23398;&#20013;&#30340;&#32500;&#24230;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#22788;&#29702;&#20004;&#20010;&#39046;&#22495;&#30340;&#22823;&#37327;&#36755;&#20837;&#32500;&#24230;&#21644;&#22823;&#26679;&#26412;&#37327;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#33258;&#21161;&#27861;&#25552;&#20379;&#31283;&#20581;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#20687;&#36951;&#20256;&#23398;&#21644;&#31867;&#20284;&#39046;&#22495;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#23558;&#19968;&#20010;&#39046;&#22495;&#65288;&#22914;&#36951;&#20256;&#25968;&#25454;&#65289;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#19982;&#21478;&#19968;&#20010;&#39046;&#22495;&#65288;&#22914;&#33041;&#25104;&#20687;&#25968;&#25454;&#65289;&#20013;&#30340;&#39640;&#32500;&#25968;&#25454;&#36827;&#34892;&#20851;&#32852;&#12290;&#35813;&#39046;&#22495;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#36890;&#36807;&#36951;&#20256;&#22240;&#32032;&#21644;&#25104;&#20687;&#34920;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#21333;&#21464;&#37327;&#20998;&#26512;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#27599;&#20010;&#39044;&#23450;&#20041;&#30340;&#25104;&#20687;&#27979;&#37327;&#25191;&#34892;&#19968;&#27425;&#20840;&#22522;&#22240;&#32452;&#20851;&#32852;&#30740;&#31350;&#65288;GWAS&#65289;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#23427;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#34920;&#22411;&#24517;&#39035;&#20107;&#20808;&#23450;&#20041;&#22909;&#12290;&#22240;&#27492;&#65292;&#19981;&#34987;&#38480;&#21046;&#22312;&#39044;&#36873;&#24863;&#20852;&#36259;&#21306;&#22495;&#20869;&#25110;&#21453;&#26144;&#26356;&#22823;&#30340;&#25972;&#20010;&#22823;&#33041;&#27169;&#24335;&#30340;&#25928;&#24212;&#24456;&#23481;&#26131;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20559;&#26368;&#23567;&#20108;&#20056;&#65288;PLS&#65289;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Cluster-Bootstrap PLS&#65288;CLUB-PLS&#65289;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#20004;&#20010;&#39046;&#22495;&#30340;&#22823;&#37327;&#36755;&#20837;&#32500;&#24230;&#20197;&#21450;&#22823;&#26679;&#26412;&#37327;&#12290;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#20351;&#29992;&#32858;&#31867;&#33258;&#21161;&#27861;&#20026;&#21333;&#20010;&#36755;&#20837;&#29305;&#24449;&#25552;&#20379;&#31283;&#20581;&#30340;&#32479;&#35745;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major challenge in imaging genetics and similar fields is to link high-dimensional data in one domain, e.g., genetic data, to high dimensional data in a second domain, e.g., brain imaging data. The standard approach in the area are mass univariate analyses across genetic factors and imaging phenotypes. That entails executing one genome-wide association study (GWAS) for each pre-defined imaging measure. Although this approach has been tremendously successful, one shortcoming is that phenotypes must be pre-defined. Consequently, effects that are not confined to pre-selected regions of interest or that reflect larger brain-wide patterns can easily be missed. In this work we introduce a Partial Least Squares (PLS)-based framework, which we term Cluster-Bootstrap PLS (CLUB-PLS), that can work with large input dimensions in both domains as well as with large sample sizes. One key factor of the framework is to use cluster bootstrap to provide robust statistics for single input features in b
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Reel&#31639;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#21644;&#31232;&#30095;&#20998;&#35299;&#22312;&#20540;&#22495;&#21644;&#20613;&#37324;&#21494;&#22495;&#20013;&#65292;&#39640;&#25928;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21152;&#24555;&#20102;&#31185;&#23398;&#21457;&#29616;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07344</link><description>&lt;p&gt;
&#36890;&#36807;&#27888;&#21202;&#23637;&#24320;&#21644;&#31232;&#30095;&#20998;&#35299;&#22312;&#20540;&#22495;&#21644;&#20613;&#37324;&#21494;&#22495;&#20013;&#39640;&#25928;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of PDEs via Taylor Expansion and Sparse Decomposition into Value and Fourier Domains. (arXiv:2309.07344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07344
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Reel&#31639;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#21644;&#31232;&#30095;&#20998;&#35299;&#22312;&#20540;&#22495;&#21644;&#20613;&#37324;&#21494;&#22495;&#20013;&#65292;&#39640;&#25928;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#21152;&#24555;&#20102;&#31185;&#23398;&#21457;&#29616;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#21152;&#36895;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#23558;&#21152;&#24555;&#31185;&#23398;&#21457;&#29616;&#30340;&#36895;&#24230;&#12290;&#20808;&#21069;&#30340;&#38543;&#26426;&#31639;&#27861;&#21033;&#29992;PDE&#26356;&#26032;&#30340;&#31232;&#30095;&#24615;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#22312;&#20540;&#22495;&#20013;&#20855;&#26377;&#31232;&#30095;&#29305;&#24449;&#30340;&#19968;&#31867;&#21487;&#20998;&#35299;&#30340;PDEs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Reel&#65292;&#36890;&#36807;&#38543;&#26426;&#25237;&#24433;&#21152;&#36895;&#23398;&#20064;PDEs&#65292;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;Reel&#21033;&#29992;&#31232;&#30095;&#24615;&#23558;&#23494;&#38598;&#26356;&#26032;&#20998;&#35299;&#20026;&#20540;&#22495;&#21644;&#39057;&#22495;&#20013;&#30340;&#31232;&#30095;&#26356;&#26032;&#12290;&#36825;&#31181;&#20998;&#35299;&#20351;&#24471;&#22312;&#26356;&#26032;&#28304;&#30001;&#36328;&#36234;&#22823;&#21306;&#22495;&#28176;&#21464;&#30340;&#39033;(&#39057;&#22495;&#31232;&#30095;)&#20197;&#21450;&#22312;&#23569;&#25968;&#8220;&#30028;&#38754;&#8221;&#21306;&#22495;&#38598;&#20013;&#24555;&#36895;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;(&#20540;&#22495;&#31232;&#30095;)&#12290;&#28982;&#21518;&#65292;&#24212;&#29992;&#38543;&#26426;&#25237;&#24433;&#26469;&#21387;&#32553;&#31232;&#30095;&#20449;&#21495;&#36827;&#34892;&#23398;&#20064;&#12290;&#20026;&#20102;&#25193;&#23637;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#65292;&#36824;&#20351;&#29992;&#20102;&#27888;&#21202;&#32423;&#25968;&#23637;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accelerating the learning of Partial Differential Equations (PDEs) from experimental data will speed up the pace of scientific discovery. Previous randomized algorithms exploit sparsity in PDE updates for acceleration. However such methods are applicable to a limited class of decomposable PDEs, which have sparse features in the value domain. We propose Reel, which accelerates the learning of PDEs via random projection and has much broader applicability. Reel exploits the sparsity by decomposing dense updates into sparse ones in both the value and frequency domains. This decomposition enables efficient learning when the source of the updates consists of gradually changing terms across large areas (sparse in the frequency domain) in addition to a few rapid updates concentrated in a small set of "interfacial" regions (sparse in the value domain). Random projection is then applied to compress the sparse signals for learning. To expand the model applicability, Taylor series expansion is use
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37327;&#23376;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21033;&#29992;&#22522;&#20110;&#37327;&#23376;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20648;&#27700;&#27744;&#30340;QRL&#20195;&#29702;&#65292;&#35299;&#20915;&#20102;QRL&#19982;QRNN&#30340;&#20302;&#25928;&#35757;&#32451;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.07339</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#37327;&#23376;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#65306;&#22522;&#20110;&#37327;&#23376;&#20648;&#27700;&#27744;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Efficient quantum recurrent reinforcement learning via quantum reservoir computing. (arXiv:2309.07339v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37327;&#23376;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21033;&#29992;&#22522;&#20110;&#37327;&#23376;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20648;&#27700;&#27744;&#30340;QRL&#20195;&#29702;&#65292;&#35299;&#20915;&#20102;QRL&#19982;QRNN&#30340;&#20302;&#25928;&#35757;&#32451;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#24378;&#21270;&#23398;&#20064;&#65288;QRL&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#37327;&#23376;&#20248;&#21183;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#21457;&#23637;&#26159;&#36890;&#36807;&#37327;&#23376;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QRNN&#65289;&#26469;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#31561;&#20869;&#23384;&#23494;&#38598;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;QRL&#27169;&#22411;&#32467;&#21512;QRNN&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21253;&#25324;QRL&#19982;QRNN&#30340;&#20302;&#25928;&#35757;&#32451;&#65292;&#22240;&#20026;QRNN&#20013;&#30340;&#26799;&#24230;&#35745;&#31639;&#26082;&#32791;&#36153;&#35745;&#31639;&#36164;&#28304;&#21448;&#32791;&#26102;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#21033;&#29992;&#22522;&#20110;QRNN&#30340;&#20648;&#27700;&#27744;&#30340;QRL&#20195;&#29702;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20855;&#20307;&#37319;&#29992;&#37327;&#23376;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;QLSTM&#65289;&#12290;QLSTM&#21442;&#25968;&#26159;&#38543;&#26426;&#21021;&#22987;&#21270;&#24182;&#22266;&#23450;&#19981;&#21464;&#30340;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#24322;&#27493;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;A3C&#65289;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;QLSTM-Reservoir RL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#20854;&#24615;&#33021;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#19978;&#24471;&#21040;&#20102;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum reinforcement learning (QRL) has emerged as a framework to solve sequential decision-making tasks, showcasing empirical quantum advantages. A notable development is through quantum recurrent neural networks (QRNNs) for memory-intensive tasks such as partially observable environments. However, QRL models incorporating QRNN encounter challenges such as inefficient training of QRL with QRNN, given that the computation of gradients in QRNN is both computationally expensive and time-consuming. This work presents a novel approach to address this challenge by constructing QRL agents utilizing QRNN-based reservoirs, specifically employing quantum long short-term memory (QLSTM). QLSTM parameters are randomly initialized and fixed without training. The model is trained using the asynchronous advantage actor-aritic (A3C) algorithm. Through numerical simulations, we validate the efficacy of our QLSTM-Reservoir RL framework. Its performance is assessed on standard benchmarks, demonstrating 
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#38752;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#65292;&#21033;&#29992;&#24402;&#32435;&#24615;&#31526;&#21512;&#39044;&#27979;&#26469;&#32416;&#27491;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38169;&#35823;&#26631;&#35760;&#21644;&#24322;&#24120;&#20540;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#39564;&#35777;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07332</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#22522;&#20110;&#21487;&#38752;&#24615;&#30340;&#22122;&#22768;&#35757;&#32451;&#26631;&#31614;&#28165;&#27927;&#26041;&#27861;&#19982;&#24402;&#32435;&#24615;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining. (arXiv:2309.07332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07332
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#38752;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#65292;&#21033;&#29992;&#24402;&#32435;&#24615;&#31526;&#21512;&#39044;&#27979;&#26469;&#32416;&#27491;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38169;&#35823;&#26631;&#35760;&#21644;&#24322;&#24120;&#20540;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#39564;&#35777;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#26631;&#35760;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21487;&#29992;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21487;&#38752;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#24402;&#32435;&#24615;&#31526;&#21512;&#39044;&#27979;&#65288;ICP&#65289;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#23567;&#37096;&#20998;&#20934;&#30830;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#21033;&#29992;ICP&#35745;&#31639;&#30340;&#21487;&#38752;&#24615;&#25351;&#26631;&#26469;&#32416;&#27491;&#28023;&#37327;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#38169;&#35823;&#26631;&#35760;&#21644;&#24322;&#24120;&#20540;&#12290;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65306;&#20351;&#29992;&#26631;&#39064;&#21644;&#25688;&#35201;&#23545;&#33647;&#29289;&#35825;&#23548;&#30340;&#32925;&#25439;&#20260;&#65288;DILI&#65289;&#25991;&#29486;&#36827;&#34892;&#36807;&#28388;&#65292;&#36890;&#36807;CT&#24433;&#20687;&#23398;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#39044;&#27979;COVID-19&#24739;&#32773;&#30340;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#20837;&#38498;&#24773;&#20917;&#65292;&#20197;&#21450;&#20351;&#29992;RNA&#27979;&#24207;&#25968;&#25454;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20122;&#22411;&#20998;&#22411;&#12290;&#36890;&#36807;&#26631;&#31614;&#25490;&#21015;&#24341;&#20837;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#35757;&#32451;&#26631;&#31614;&#22122;&#22768;&#12290;&#32467;&#26524;&#26174;&#31034;&#20998;&#31867;&#24615;&#33021;&#26174;&#33879;&#25552;&#21319;&#65306;&#20934;&#30830;&#24230;&#25552;&#39640;
&lt;/p&gt;
&lt;p&gt;
Accurately labeling biomedical data presents a challenge. Traditional semi-supervised learning methods often under-utilize available unlabeled data. To address this, we propose a novel reliability-based training data cleaning method employing inductive conformal prediction (ICP). This method capitalizes on a small set of accurately labeled training data and leverages ICP-calculated reliability metrics to rectify mislabeled data and outliers within vast quantities of noisy training data. The efficacy of the method is validated across three classification tasks within distinct modalities: filtering drug-induced-liver-injury (DILI) literature with title and abstract, predicting ICU admission of COVID-19 patients through CT radiomics and electronic health records, and subtyping breast cancer using RNA-sequencing data. Varying levels of noise to the training labels were introduced through label permutation. Results show significant enhancements in classification performance: accuracy enhanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35270;&#35282;&#26469;&#35299;&#37322;&#21464;&#21387;&#22120;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#38416;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#38480;&#21046;&#28508;&#22312;&#29305;&#24449;&#24182;&#22312;&#36229;&#29699;&#38754;&#19978;&#22609;&#36896;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#25506;&#27979;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#35813;&#35270;&#35282;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.07315</link><description>&lt;p&gt;
&#26053;&#34892;&#35789;&#65306;&#19968;&#31181;&#21464;&#21387;&#22120;&#30340;&#20960;&#20309;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20309;&#35270;&#35282;&#26469;&#35299;&#37322;&#21464;&#21387;&#22120;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#38416;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#38480;&#21046;&#28508;&#22312;&#29305;&#24449;&#24182;&#22312;&#36229;&#29699;&#38754;&#19978;&#22609;&#36896;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#25506;&#27979;&#39044;&#35757;&#32451;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#35813;&#35270;&#35282;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#29702;&#35299;&#20854;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#35270;&#35282;&#65292;&#38416;&#26126;&#20102;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35828;&#26126;&#20102;&#23618;&#24402;&#19968;&#21270;&#22914;&#20309;&#23558;&#28508;&#22312;&#29305;&#24449;&#38480;&#21046;&#22312;&#19968;&#20010;&#36229;&#29699;&#38754;&#19978;&#65292;&#20174;&#32780;&#20351;&#27880;&#24847;&#21147;&#33021;&#22815;&#22312;&#35813;&#34920;&#38754;&#19978;&#22609;&#36896;&#21333;&#35789;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#36825;&#31181;&#20960;&#20309;&#35270;&#28857;&#26080;&#32541;&#22320;&#36830;&#25509;&#20102;&#36845;&#20195;&#25913;&#36827;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#31561;&#24050;&#30693;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25506;&#27979;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;124M&#21442;&#25968;&#30340;GPT-2&#27169;&#22411;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#26089;&#26399;&#23618;&#20013;&#28165;&#26224;&#30340;&#26597;&#35810;-&#38190;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#22312;&#26356;&#28145;&#30340;&#23618;&#27425;&#19978;&#24314;&#31435;&#22312;&#20808;&#21069;&#20851;&#20110;&#27880;&#24847;&#22836;&#30340;&#19987;&#38376;&#24615;&#30340;&#35266;&#23519;&#22522;&#30784;&#19978;&#12290;&#21033;&#29992;&#36825;&#20123;&#20960;&#20309;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21464;&#21387;&#22120;&#30340;&#30452;&#35266;&#29702;&#35299;&#65292;&#23558;&#20854;&#25551;&#32472;&#20026;&#22609;&#36896;&#36712;&#36857;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#22686;&#24378;&#30340;&#32908;&#30005;&#20449;&#21495;&#25163;&#21183;&#20998;&#31867;&#30340;&#29992;&#25143;&#35757;&#32451;&#31995;&#32479;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#65292;&#20351;&#29992;&#20462;&#25913;&#21453;&#39304;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25163;&#21183;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#31867;&#21035;&#21306;&#20998;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07289</link><description>&lt;p&gt;
&#22522;&#20110;&#38169;&#35823;&#22686;&#24378;&#30340;&#32908;&#30005;&#20449;&#21495;&#25163;&#21183;&#20998;&#31867;&#30340;&#29992;&#25143;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
User Training with Error Augmentation for Electromyogram-based Gesture Classification. (arXiv:2309.07289v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#38169;&#35823;&#22686;&#24378;&#30340;&#32908;&#30005;&#20449;&#21495;&#25163;&#21183;&#20998;&#31867;&#30340;&#29992;&#25143;&#35757;&#32451;&#31995;&#32479;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22522;&#32447;&#65292;&#20351;&#29992;&#20462;&#25913;&#21453;&#39304;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25163;&#21183;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#31867;&#21035;&#21306;&#20998;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#24182;&#27979;&#35797;&#20102;&#19968;&#20010;&#23454;&#26102;&#25511;&#21046;&#29992;&#25143;&#30028;&#38754;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#33109;&#24102;&#37197;&#32622;&#30340;&#20843;&#20010;&#30005;&#26497;&#20013;&#25552;&#21462;&#34920;&#38754;&#32908;&#30005;&#27963;&#21160;&#65288;sEMG&#65289;&#12290;sEMG&#25968;&#25454;&#34987;&#23454;&#26102;&#27969;&#20837;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#25163;&#21183;&#30340;&#23454;&#26102;&#20998;&#31867;&#12290;&#22312;&#21021;&#22987;&#27169;&#22411;&#26657;&#20934;&#21518;&#65292;&#21442;&#19982;&#32773;&#22312;&#20154;&#31867;&#23398;&#20064;&#38454;&#27573;&#20013;&#34987;&#25552;&#20379;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#21453;&#39304;&#65306;&#30495;&#23454;&#21453;&#39304;&#65292;&#22312;&#20854;&#20013;&#39044;&#27979;&#30340;&#25163;&#21183;&#20998;&#31867;&#31639;&#27861;&#30340;&#27010;&#29575;&#34987;&#26174;&#31034;&#32780;&#19981;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#65307;&#20462;&#25913;&#21453;&#39304;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#23545;&#36825;&#20123;&#27010;&#29575;&#36827;&#34892;&#20102;&#38169;&#35823;&#30340;&#38544;&#34255;&#22686;&#24378;&#22788;&#29702;&#65307;&#21644;&#26080;&#21453;&#39304;&#12290;&#28982;&#21518;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#36855;&#20320;&#28216;&#25103;&#35780;&#20272;&#20102;&#29992;&#25143;&#30340;&#34920;&#29616;&#65292;&#35201;&#27714;&#34987;&#35797;&#20351;&#29992;&#20843;&#20010;&#25163;&#21183;&#26469;&#25805;&#20316;&#28216;&#25103;&#35282;&#33394;&#23436;&#25104;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#20462;&#25913;&#21453;&#39304;&#26465;&#20214;&#19979;&#30340;&#20934;&#30830;&#24615;&#21644;&#25163;&#21183;&#31867;&#21035;&#21306;&#20998;&#24230;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We designed and tested a system for real-time control of a user interface by extracting surface electromyographic (sEMG) activity from eight electrodes in a wrist-band configuration. sEMG data were streamed into a machine-learning algorithm that classified hand gestures in real-time. After an initial model calibration, participants were presented with one of three types of feedback during a human-learning stage: veridical feedback, in which predicted probabilities from the gesture classification algorithm were displayed without alteration, modified feedback, in which we applied a hidden augmentation of error to these probabilities, and no feedback. User performance was then evaluated in a series of minigames, in which subjects were required to use eight gestures to manipulate their game avatar to complete a task. Experimental results indicated that, relative to baseline, the modified feedback condition led to significantly improved accuracy and improved gesture class separation. These 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#20154;&#33080;&#29983;&#25104;&#39046;&#22495;&#30340;&#21151;&#25928;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23457;&#35745;&#22522;&#20110;&#31038;&#20250;&#23646;&#24615;&#26465;&#20214;&#29983;&#25104;&#30340;&#20154;&#33080;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#33080;&#22270;&#20687;&#29983;&#25104;&#23384;&#22312;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#25991;&#26412;&#25552;&#31034;&#30340;&#24544;&#23454;&#24230;&#12289;&#20154;&#21475;&#32479;&#35745;&#24046;&#24322;&#21644;&#20998;&#24067;&#20559;&#31227;&#12290;</title><link>http://arxiv.org/abs/2309.07277</link><description>&lt;p&gt;
&#26080;&#20559;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#20154;&#33080;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unbiased Face Synthesis With Diffusion Models: Are We There Yet?. (arXiv:2309.07277v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#20154;&#33080;&#29983;&#25104;&#39046;&#22495;&#30340;&#21151;&#25928;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23457;&#35745;&#22522;&#20110;&#31038;&#20250;&#23646;&#24615;&#26465;&#20214;&#29983;&#25104;&#30340;&#20154;&#33080;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#33080;&#22270;&#20687;&#29983;&#25104;&#23384;&#22312;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#25991;&#26412;&#25552;&#31034;&#30340;&#24544;&#23454;&#24230;&#12289;&#20154;&#21475;&#32479;&#35745;&#24046;&#24322;&#21644;&#20998;&#24067;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#32780;&#24191;&#21463;&#27426;&#36814;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#20204;&#29983;&#25104;&#21644;&#20462;&#25913;&#20154;&#33080;&#30340;&#33021;&#21147;&#24050;&#32463;&#25512;&#21160;&#20102;&#23545;&#20351;&#29992;&#29983;&#25104;&#30340;&#20154;&#33080;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#20154;&#33080;&#29983;&#25104;&#39046;&#22495;&#30340;&#21151;&#25928;&#21644;&#19981;&#36275;&#20043;&#22788;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23884;&#20837;&#24230;&#37327;&#21644;&#29992;&#25143;&#30740;&#31350;&#31561;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#23457;&#35745;&#22522;&#20110;&#19968;&#32452;&#31038;&#20250;&#23646;&#24615;&#26465;&#20214;&#29983;&#25104;&#30340;&#20154;&#33080;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#20154;&#33080;&#19978;&#24212;&#29992;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#33080;&#22270;&#20687;&#29983;&#25104;&#23384;&#22312;&#20960;&#20010;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#25991;&#26412;&#25552;&#31034;&#30340;&#24544;&#23454;&#24230;&#12289;&#20154;&#21475;&#32479;&#35745;&#24046;&#24322;&#21644;&#20998;&#24067;&#20559;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#65292;&#20197;&#28145;&#20837;&#20102;&#35299;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image diffusion models have achieved widespread popularity due to their unprecedented image generation capability. In particular, their ability to synthesize and modify human faces has spurred research into using generated face images in both training data augmentation and model performance assessments. In this paper, we study the efficacy and shortcomings of generative models in the context of face generation. Utilizing a combination of qualitative and quantitative measures, including embedding-based metrics and user studies, we present a framework to audit the characteristics of generated faces conditioned on a set of social attributes. We applied our framework on faces generated through state-of-the-art text-to-image diffusion models. We identify several limitations of face image generation that include faithfulness to the text prompt, demographic disparities, and distributional shifts. Furthermore, we present an analytical model that provides insights into how training data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;O-RAN&#32593;&#32476;&#20013;&#20351;&#29992;DRL&#31639;&#27861;&#36827;&#34892;&#38381;&#29615;&#25511;&#21046;&#26102;&#36935;&#21040;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07265</link><description>&lt;p&gt;
&#23433;&#20840;&#19988;&#21152;&#36895;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;O-RAN&#20999;&#29255;: &#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach. (arXiv:2309.07265v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;O-RAN&#32593;&#32476;&#20013;&#20351;&#29992;DRL&#31639;&#27861;&#36827;&#34892;&#38381;&#29615;&#25511;&#21046;&#26102;&#36935;&#21040;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#21644;&#24615;&#33021;&#19981;&#31283;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#26550;&#26500;&#25903;&#25345;&#26234;&#33021;&#32593;&#32476;&#25511;&#21046;&#31639;&#27861;&#20316;&#20026;&#20854;&#26680;&#24515;&#33021;&#21147;&#20043;&#19968;&#12290;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;&#36825;&#20123;&#31639;&#27861;&#36890;&#36807;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#26234;&#33021;&#25511;&#21046;&#22120;&#65288;RIC&#65289;&#26469;&#20248;&#21270;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;RAN&#65289;&#21151;&#33021;&#12290;&#22312;O-RAN&#25991;&#29486;&#20013;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#26159;&#35299;&#20915;&#21160;&#24577;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;O-RAN RIC&#24341;&#20837;&#20102;&#35832;&#22810;&#22909;&#22788;&#65292;&#20294;&#22312;&#30495;&#23454;&#32593;&#32476;&#37096;&#32626;&#20013;&#65292;DRL&#31639;&#27861;&#30340;&#23454;&#38469;&#37319;&#29992;&#21364;&#33853;&#21518;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;DRL&#20195;&#29702;&#22312;&#37096;&#32626;&#21644;&#38754;&#23545;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#32593;&#32476;&#26465;&#20214;&#26102;&#25910;&#25947;&#36895;&#24230;&#24930;&#12289;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#20316;&#20026;O-RAN&#21151;&#33021;&#30340;DRL&#22522;&#20110;&#38381;&#29615;&#25511;&#21046;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#27969;&#31243;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;TL&#36741;&#21161;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The open radio access network (O-RAN) architecture supports intelligent network control algorithms as one of its core capabilities. Data-driven applications incorporate such algorithms to optimize radio access network (RAN) functions via RAN intelligent controllers (RICs). Deep reinforcement learning (DRL) algorithms are among the main approaches adopted in the O-RAN literature to solve dynamic radio resource management problems. However, despite the benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms in real network deployments falls behind. This is primarily due to the slow convergence and unstable performance exhibited by DRL agents upon deployment and when facing previously unseen network conditions. In this paper, we address these challenges by proposing transfer learning (TL) as a core component of the training and deployment workflows for the DRL-based closed-loop control of O-RAN functionalities. To this end, we propose and design a hybrid TL-aided a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#20132;&#32467;&#26500;&#21644;&#32447;&#24615;&#25237;&#24433;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07261</link><description>&lt;p&gt;
&#20855;&#26377;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21516;&#26102;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Simultaneous inference for generalized linear models with unmeasured confounders. (arXiv:2309.07261v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#20132;&#32467;&#26500;&#21644;&#32447;&#24615;&#25237;&#24433;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#20013;&#65292;&#24120;&#24120;&#36827;&#34892;&#25104;&#21315;&#19978;&#19975;&#20010;&#21516;&#26102;&#20551;&#35774;&#26816;&#39564;&#65292;&#20197;&#30830;&#23450;&#24046;&#24322;&#34920;&#36798;&#30340;&#22522;&#22240;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#65292;&#35768;&#22810;&#26631;&#20934;&#32479;&#35745;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#22810;&#20803;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#12290;&#22312;&#20219;&#24847;&#28151;&#28102;&#26426;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#27491;&#20132;&#32467;&#26500;&#24182;&#23558;&#32447;&#24615;&#25237;&#24433;&#25972;&#21512;&#21040;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#20013;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#22810;&#20803;&#21709;&#24212;&#21464;&#37327;&#20998;&#31163;&#36793;&#38469;&#21644;&#19981;&#30456;&#20851;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#24674;&#22797;&#28151;&#28102;&#31995;&#25968;&#30340;&#21015;&#31354;&#38388;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;$\ell_1$&#27491;&#21017;&#21270;&#36827;&#34892;&#31232;&#30095;&#24615;&#20272;&#35745;&#65292;&#24182;&#24378;&#21152;&#27491;&#20132;&#24615;&#38480;&#21046;&#20110;&#28151;&#28102;&#31995;&#25968;&#65292;&#32852;&#21512;&#20272;&#35745;&#28508;&#22312;&#22240;&#23376;&#21644;&#20027;&#35201;&#25928;&#24212;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#25237;&#24433;&#21644;&#21152;&#26435;&#20559;&#24046;&#26657;&#27491;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tens of thousands of simultaneous hypothesis tests are routinely performed in genomic studies to identify differentially expressed genes. However, due to unmeasured confounders, many standard statistical approaches may be substantially biased. This paper investigates the large-scale hypothesis testing problem for multivariate generalized linear models in the presence of confounding effects. Under arbitrary confounding mechanisms, we propose a unified statistical estimation and inference framework that harnesses orthogonal structures and integrates linear projections into three key stages. It first leverages multivariate responses to separate marginal and uncorrelated confounding effects, recovering the confounding coefficients' column space. Subsequently, latent factors and primary effects are jointly estimated, utilizing $\ell_1$-regularization for sparsity while imposing orthogonality onto confounding coefficients. Finally, we incorporate projected and weighted bias-correction steps 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#36882;&#24402;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#36882;&#24402;&#27714;&#35299;&#24037;&#20855;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#25104;&#26412;&#20998;&#26512;&#20013;&#12290;&#26041;&#27861;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#29468;&#27979;&#38381;&#24335;&#20989;&#25968;&#65292;&#24182;&#32467;&#21512;SMT-so&#26469;&#27714;&#35299;&#20219;&#24847;&#32422;&#26463;&#36882;&#24402;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.07259</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#36882;&#24402;&#20851;&#31995;&#65292;&#20197;&#21450;&#22312;&#25104;&#26412;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Solving Recurrence Relations using Machine Learning, with Application to Cost Analysis. (arXiv:2309.07259v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#36882;&#24402;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#36882;&#24402;&#27714;&#35299;&#24037;&#20855;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#24212;&#29992;&#20110;&#25104;&#26412;&#20998;&#26512;&#20013;&#12290;&#26041;&#27861;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#29468;&#27979;&#38381;&#24335;&#20989;&#25968;&#65292;&#24182;&#32467;&#21512;SMT-so&#26469;&#27714;&#35299;&#20219;&#24847;&#32422;&#26463;&#36882;&#24402;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#38745;&#24577;&#25104;&#26412;&#20998;&#26512;&#25512;&#26029;&#20851;&#20110;&#31243;&#24207;&#36164;&#28304;&#20351;&#29992;&#24773;&#20917;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#38656;&#35201;&#20351;&#29992;&#20855;&#20307;&#25968;&#25454;&#65292;&#24182;&#23558;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#22823;&#23567;&#30340;&#20989;&#25968;&#21576;&#29616;&#12290;&#36923;&#36753;&#31243;&#24207;&#65288;&#21644;&#20854;&#20182;&#35821;&#35328;&#65289;&#30340;&#22823;&#22810;&#25968;&#20998;&#26512;&#24037;&#20855;&#22522;&#20110;&#24314;&#31435;&#34920;&#31034;&#35859;&#35789;&#35745;&#31639;&#25104;&#26412;&#65288;&#19978;&#30028;&#65289;&#30340;&#36882;&#24402;&#20851;&#31995;&#65292;&#24182;&#35299;&#20915;&#23427;&#20204;&#20197;&#25214;&#21040;&#31561;&#20215;&#20110;&#65288;&#25110;&#19978;&#30028;&#65289;&#30340;&#38381;&#24335;&#20989;&#25968;&#12290;&#24403;&#21069;&#24037;&#20855;&#20013;&#30340;&#36882;&#24402;&#27714;&#35299;&#26159;&#19968;&#20010;&#29942;&#39048;&#65306;&#22312;&#20998;&#26512;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#35768;&#22810;&#36882;&#24402;&#24335;&#26080;&#27861;&#20351;&#29992;&#24403;&#21069;&#27714;&#35299;&#22120;&#65288;&#22914;&#35745;&#31639;&#26426;&#20195;&#25968;&#31995;&#32479;&#65289;&#36827;&#34892;&#27714;&#35299;&#65292;&#22240;&#27492;&#38656;&#35201;&#38024;&#23545;&#19981;&#21516;&#31867;&#21035;&#30340;&#36882;&#24402;&#24335;&#24320;&#21457;&#29305;&#23450;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#29992;&#26041;&#27861;&#26469;&#35299;&#20915;&#20219;&#24847;&#32422;&#26463;&#36882;&#24402;&#20851;&#31995;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31232;&#30095;&#22238;&#24402;&#25216;&#26415;&#29468;&#27979;&#20505;&#36873;&#30340;&#38381;&#24335;&#20989;&#25968;&#65292;&#24182;&#32467;&#21512;SMT-so&#26469;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic static cost analysis infers information about the resources used by programs without actually running them with concrete data, and presents such information as functions of input data sizes. Most of the analysis tools for logic programs (and other languages) are based on setting up recurrence relations representing (bounds on) the computational cost of predicates, and solving them to find closed-form functions that are equivalent to (or a bound on) them. Such recurrence solving is a bottleneck in current tools: many of the recurrences that arise during the analysis cannot be solved with current solvers, such as Computer Algebra Systems (CASs), so that specific methods for different classes of recurrences need to be developed. We address such a challenge by developing a novel, general approach for solving arbitrary, constrained recurrence relations, that uses machine-learning sparse regression techniques to guess a candidate closed-form function, and a combination of an SMT-so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#33258;&#26059;&#32593;&#32476;&#26500;&#24314;SU(2)&#31561;&#20215;&#37327;&#23376;&#30005;&#36335;&#65292;&#36890;&#36807;&#32534;&#30721;&#32676;&#32467;&#26500;&#26469;&#38480;&#21046;&#20248;&#21270;&#31354;&#38388;&#65292; &#20855;&#26377;&#26059;&#36716;&#23545;&#31216;&#24615;&#65292;&#27604;&#20854;&#20182;&#24050;&#30693;&#30340;&#26500;&#36896;&#26356;&#30452;&#25509;&#23454;&#29616;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#12290;</title><link>http://arxiv.org/abs/2309.07250</link><description>&lt;p&gt;
&#20320;&#25152;&#38656;&#35201;&#30340;&#21482;&#26159;&#26059;&#36716;&#65306;&#22522;&#20110;&#33258;&#26059;&#32593;&#32476;&#30340;SU(2)&#31561;&#20215;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
All you need is spin: SU(2) equivariant variational quantum circuits based on spin networks. (arXiv:2309.07250v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#33258;&#26059;&#32593;&#32476;&#26500;&#24314;SU(2)&#31561;&#20215;&#37327;&#23376;&#30005;&#36335;&#65292;&#36890;&#36807;&#32534;&#30721;&#32676;&#32467;&#26500;&#26469;&#38480;&#21046;&#20248;&#21270;&#31354;&#38388;&#65292; &#20855;&#26377;&#26059;&#36716;&#23545;&#31216;&#24615;&#65292;&#27604;&#20854;&#20182;&#24050;&#30693;&#30340;&#26500;&#36896;&#26356;&#30452;&#25509;&#23454;&#29616;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#31639;&#27861;&#35201;&#27714;&#23558;&#20248;&#21270;&#31354;&#38388;&#33258;&#28982;&#22320;&#38480;&#21046;&#22312;&#19968;&#20010;&#26377;&#25928;&#30340;&#33539;&#22260;&#20869;&#12290;&#22312;&#20960;&#20309;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23558;&#32676;&#32467;&#26500;&#32534;&#30721;&#21040;&#21442;&#25968;&#21270;&#30340;&#37327;&#23376;&#30005;&#36335;&#20013;&#65292;&#23558;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#20316;&#20026;&#24402;&#32435;&#20559;&#32622;&#26469;&#32771;&#34385;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36825;&#26679;&#30340;&#30005;&#36335;&#26159;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23578;&#26410;&#20986;&#29616;&#26126;&#30830;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#26059;&#32593;&#32476;&#65292;&#19968;&#31181;&#22312;&#32676;&#21464;&#25442;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#26377;&#21521;&#24352;&#37327;&#32593;&#32476;&#24418;&#24335;&#65292;&#26469;&#35774;&#35745;SU(2)&#31561;&#20215;&#37327;&#23376;&#30005;&#36335;ansatz - &#20855;&#26377;&#26059;&#36716;&#23545;&#31216;&#24615;&#30340;&#30005;&#36335;&#12290;&#36890;&#36807;&#25913;&#21464;&#20351;SU(2)&#32676;&#20316;&#29992;&#22359;&#23545;&#35282;&#21270;&#30340;&#22522;&#30784;&#65292;&#36825;&#20123;&#32593;&#32476;&#20026;&#26500;&#24314;&#21442;&#25968;&#21270;&#31561;&#20215;&#37327;&#23376;&#30005;&#36335;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#26500;&#24314;&#27169;&#22359;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26500;&#36896;&#22312;&#25968;&#23398;&#19978;&#31561;&#25928;&#20110;&#20854;&#20182;&#24050;&#30693;&#30340;&#26500;&#36896;&#65292;&#20363;&#22914;&#22522;&#20110;&#20132;&#38169;&#21644;&#24191;&#20041;&#25490;&#21015;&#30340;&#26500;&#36896;&#65292;&#20294;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#23454;&#29616;&#26356;&#30452;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational algorithms require architectures that naturally constrain the optimisation space to run efficiently. In geometric quantum machine learning, one achieves this by encoding group structure into parameterised quantum circuits to include the symmetries of a problem as an inductive bias. However, constructing such circuits is challenging as a concrete guiding principle has yet to emerge. In this paper, we propose the use of spin networks, a form of directed tensor network invariant under a group transformation, to devise SU(2) equivariant quantum circuit ans\"atze -- circuits possessing spin rotation symmetry. By changing to the basis that block diagonalises SU(2) group action, these networks provide a natural building block for constructing parameterised equivariant quantum circuits. We prove that our construction is mathematically equivalent to other known constructions, such as those based on twirling and generalised permutations, but more direct to implement on quantum hardwa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;TVM&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#65292;&#20351;&#29992;TVM&#24352;&#37327;&#34920;&#36798;&#35821;&#35328;&#23454;&#29616;&#20102;LU&#20998;&#35299;&#12289;Cholesky&#20998;&#35299;&#21644;3mm&#31561;&#32447;&#24615;&#20195;&#25968;&#26680;&#12290;&#22312;GPU&#38598;&#32676;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;AutoTVM&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.07235</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#33258;&#21160;&#35843;&#20248;&#22522;&#20110;Apache TVM&#30340;&#31185;&#23398;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization. (arXiv:2309.07235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;TVM&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#65292;&#20351;&#29992;TVM&#24352;&#37327;&#34920;&#36798;&#35821;&#35328;&#23454;&#29616;&#20102;LU&#20998;&#35299;&#12289;Cholesky&#20998;&#35299;&#21644;3mm&#31561;&#32447;&#24615;&#20195;&#25968;&#26680;&#12290;&#22312;GPU&#38598;&#32676;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20256;&#32479;&#30340;AutoTVM&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Apache TVM&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#26426;&#22120;&#23398;&#20064;&#32534;&#35793;&#22120;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#21508;&#31181;&#30828;&#20214;&#24179;&#21488;&#19978;&#30340;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26032;&#22411;TVM&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;TVM&#24352;&#37327;&#34920;&#36798;&#35821;&#35328;&#23454;&#29616;&#20102;LU&#20998;&#35299;&#12289;Cholesky&#20998;&#35299;&#21644;3mm&#31561;&#32447;&#24615;&#20195;&#25968;&#26680;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#31185;&#23398;&#35745;&#31639;&#26680;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38463;&#36129;&#22269;&#23478;&#23454;&#39564;&#23460;&#30340;GPU&#38598;&#32676;"Swing"&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;&#19982;TVM&#33258;&#21160;&#35843;&#20248;&#26694;&#26550;AutoTVM&#30340;&#22235;&#20010;&#35843;&#20248;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;AutoTVM&#12290;
&lt;/p&gt;
&lt;p&gt;
Apache TVM (Tensor Virtual Machine), an open source machine learning compiler framework designed to optimize computations across various hardware platforms, provides an opportunity to improve the performance of dense matrix factorizations such as LU (Lower Upper) decomposition and Cholesky decomposition on GPUs and AI (Artificial Intelligence) accelerators. In this paper, we propose a new TVM autotuning framework using Bayesian Optimization and use the TVM tensor expression language to implement linear algebra kernels such as LU, Cholesky, and 3mm. We use these scientific computation kernels to evaluate the effectiveness of our methods on a GPU cluster, called Swing, at Argonne National Laboratory. We compare the proposed autotuning framework with the TVM autotuning framework AutoTVM with four tuners and find that our framework outperforms AutoTVM in most cases.
&lt;/p&gt;</description></item><item><title>EarthPT&#26159;&#19968;&#20010;&#22320;&#29699;&#35266;&#27979;&#30340;&#39044;&#35757;&#32451;transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#22320;&#34920;&#21453;&#23556;&#20540;&#65292;&#24182;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#23884;&#20837;&#20449;&#24687;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.07207</link><description>&lt;p&gt;
EarthPT&#65306;&#22320;&#29699;&#35266;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EarthPT: a foundation model for Earth Observation. (arXiv:2309.07207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07207
&lt;/p&gt;
&lt;p&gt;
EarthPT&#26159;&#19968;&#20010;&#22320;&#29699;&#35266;&#27979;&#30340;&#39044;&#35757;&#32451;transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26410;&#26469;&#22320;&#34920;&#21453;&#23556;&#20540;&#65292;&#24182;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#23884;&#20837;&#20449;&#24687;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;EarthPT - &#19968;&#31181;&#22320;&#29699;&#35266;&#27979;(EO)&#39044;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#12290;EarthPT&#26159;&#19968;&#20010;7&#20159;&#21442;&#25968;&#30340;&#35299;&#30721;transformer&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19987;&#38376;&#38024;&#23545;EO&#24212;&#29992;&#36827;&#34892;&#24320;&#21457;&#12290;&#25105;&#20204;&#35777;&#26126;EarthPT&#26159;&#19968;&#20010;&#26377;&#25928;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#26410;&#26469;400-2300 nm&#33539;&#22260;&#20869;&#30340;&#20687;&#32032;&#32423;&#22320;&#34920;&#21453;&#23556;&#20540;&#12290;&#20363;&#22914;&#65292;&#22312;&#19968;&#20010;&#20026;&#26399;&#20116;&#20010;&#26376;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#22320;&#34920;&#26893;&#34987;&#25351;&#25968;&#65288;NDVI&#65289;&#30340;&#28436;&#21464;&#39044;&#27979;&#30340;&#20856;&#22411;&#35823;&#24046;&#32422;&#20026;0.05&#65288;&#22312;-1 -&gt; 1&#30340;&#33258;&#28982;&#33539;&#22260;&#20869;&#65289;&#65292;&#24615;&#33021;&#36229;&#36807;&#22522;&#20110;&#21382;&#21490;&#24179;&#22343;&#30340;&#31616;&#21333;&#30456;&#20301;&#25240;&#21472;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;EarthPT&#23398;&#21040;&#30340;&#23884;&#20837;&#20855;&#26377;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#39640;&#31934;&#24230;&#12289;&#21160;&#24577;&#30340;&#22303;&#22320;&#21033;&#29992;&#20998;&#31867;&#12290;&#20196;&#20154;&#20852;&#22859;&#30340;&#26159;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;EO&#25968;&#25454;&#30340;&#20016;&#23500;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We introduce EarthPT -- an Earth Observation (EO) pretrained transformer. EarthPT is a 700 million parameter decoding transformer foundation model trained in an autoregressive self-supervised manner and developed specifically with EO use-cases in mind. We demonstrate that EarthPT is an effective forecaster that can accurately predict future pixel-level surface reflectances across the 400-2300 nm range well into the future. For example, forecasts of the evolution of the Normalised Difference Vegetation Index (NDVI) have a typical error of approximately 0.05 (over a natural range of -1 -&gt; 1) at the pixel level over a five month test set horizon, out-performing simple phase-folded models based on historical averaging. We also demonstrate that embeddings learnt by EarthPT hold semantically meaningful information and could be exploited for downstream tasks such as highly granular, dynamic land use classification. Excitingly, we note that the abundance of EO data provides us with -- in theor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#30340;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#31995;&#32479;&#26144;&#23556;&#21040;&#31616;&#21270;&#34920;&#31034;&#31354;&#38388;&#24182;&#27169;&#25311;&#26102;&#38388;&#19978;&#30340;&#22823;&#36339;&#36291;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#21407;&#22987;&#36807;&#31243;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#21160;&#21147;&#23398;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#28382;&#21518;&#38477;&#32500;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07200</link><description>&lt;p&gt;
&#36890;&#36807;&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#30340;&#28508;&#22312;&#34920;&#31034;&#21644;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck. (arXiv:2309.07200v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#30340;&#26041;&#27861;&#65292;&#23558;&#22797;&#26434;&#31995;&#32479;&#26144;&#23556;&#21040;&#31616;&#21270;&#34920;&#31034;&#31354;&#38388;&#24182;&#27169;&#25311;&#26102;&#38388;&#19978;&#30340;&#22823;&#36339;&#36291;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#21407;&#22987;&#36807;&#31243;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#21160;&#21147;&#23398;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#28382;&#21518;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#26159;&#25551;&#36848;&#21508;&#20010;&#39046;&#22495;&#20013;&#21160;&#24577;&#31995;&#32479;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#20934;&#30830;&#31215;&#20998;&#30340;&#30701;&#26102;&#38388;&#27493;&#38271;&#65292;&#31934;&#30830;&#27169;&#25311;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#22823;&#35268;&#27169;&#31995;&#32479;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#22797;&#26434;&#31995;&#32479;&#26144;&#23556;&#21040;&#31616;&#21270;&#34920;&#31034;&#31354;&#38388;&#24182;&#27169;&#25311;&#26102;&#38388;&#19978;&#30340;&#22823;&#36339;&#36291;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#21407;&#21017;&#30446;&#26631;-&#26102;&#38388;&#28382;&#21518;&#20449;&#24687;&#29942;&#39048;&#65288;T-IB&#65289;&#65292;&#23427;&#26088;&#22312;&#25429;&#25417;&#30456;&#20851;&#30340;&#26102;&#38388;&#29305;&#24449;&#65292;&#21516;&#26102;&#20002;&#24323;&#39640;&#39057;&#20449;&#24687;&#20197;&#31616;&#21270;&#27169;&#25311;&#20219;&#21153;&#24182;&#26368;&#23567;&#21270;&#25512;&#29702;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;T-IB&#23398;&#20064;&#20102;&#20449;&#24687;&#26368;&#20248;&#30340;&#34920;&#31034;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#27169;&#25311;&#21407;&#22987;&#36807;&#31243;&#22312;&#36873;&#25321;&#30340;&#26102;&#38388;&#28382;&#21518;&#19979;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#21160;&#21147;&#23398;&#65292;&#24182;&#19988;&#20248;&#20110;&#29616;&#26377;&#30340;&#26102;&#38388;&#28382;&#21518;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov processes are widely used mathematical models for describing dynamic systems in various fields. However, accurately simulating large-scale systems at long time scales is computationally expensive due to the short time steps required for accurate integration. In this paper, we introduce an inference process that maps complex systems into a simplified representational space and models large jumps in time. To achieve this, we propose Time-lagged Information Bottleneck (T-IB), a principled objective rooted in information theory, which aims to capture relevant temporal features while discarding high-frequency information to simplify the simulation task and minimize the inference error. Our experiments demonstrate that T-IB learns information-optimal representations for accurately modeling the statistical properties and dynamics of the original process at a selected time lag, outperforming existing time-lagged dimensionality reduction methods.
&lt;/p&gt;</description></item><item><title>&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#21487;&#20197;&#20943;&#36731;&#34987;&#20837;&#20405;&#33410;&#28857;&#21033;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#26469;&#25506;&#27979;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#36825;&#21487;&#20197;&#36991;&#20813;&#21361;&#38505;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#65292;&#22914;&#34987;&#31713;&#25913;&#30340;&#20132;&#36890;&#26631;&#24535;&#23548;&#33268;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38169;&#35823;&#35782;&#21035;&#65292;&#25110;&#32773;&#34987;&#27745;&#26579;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.07197</link><description>&lt;p&gt;
&#22312;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#20013;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Mitigating Adversarial Attacks in Federated Learning with Trusted Execution Environments. (arXiv:2309.07197v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07197
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#21487;&#20197;&#20943;&#36731;&#34987;&#20837;&#20405;&#33410;&#28857;&#21033;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#26469;&#25506;&#27979;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;&#36825;&#21487;&#20197;&#36991;&#20813;&#21361;&#38505;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#65292;&#22914;&#34987;&#31713;&#25913;&#30340;&#20132;&#36890;&#26631;&#24535;&#23548;&#33268;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38169;&#35823;&#35782;&#21035;&#65292;&#25110;&#32773;&#34987;&#27745;&#26579;&#30340;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#30340;&#20027;&#35201;&#21069;&#25552;&#26159;&#26412;&#22320;&#35745;&#31639;&#26426;&#23398;&#20064;&#27169;&#22411;&#26356;&#26032;&#65292;&#20197;&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#12290;&#36825;&#31181;&#26041;&#27861;&#35774;&#35745;&#19978;&#36991;&#20813;&#20102;&#29992;&#25143;&#25968;&#25454;&#31163;&#24320;&#20854;&#35774;&#22791;&#30340;&#24773;&#20917;&#12290;&#19968;&#26086;&#26356;&#26032;&#27719;&#24635;&#65292;&#27169;&#22411;&#23558;&#24191;&#25773;&#21040;&#32852;&#37030;&#20013;&#30340;&#25152;&#26377;&#33410;&#28857;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#36866;&#24403;&#30340;&#38450;&#24481;&#25514;&#26045;&#65292;&#34987;&#20837;&#20405;&#30340;&#33410;&#28857;&#21487;&#20197;&#22312;&#20854;&#26412;&#22320;&#20869;&#23384;&#20013;&#25506;&#27979;&#27169;&#22411;&#65292;&#23547;&#25214;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#21361;&#38505;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#20363;&#22914;&#65292;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#23545;&#25239;&#24615;&#31034;&#20363;&#21253;&#21547;&#24494;&#24494;&#25200;&#21160;&#30340;&#22270;&#20687;&#65292;&#20154;&#30524;&#26080;&#27861;&#23519;&#35273;&#65292;&#20294;&#34987;&#26412;&#22320;&#27169;&#22411;&#38169;&#35823;&#20998;&#31867;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#23545;&#25239;&#24615;&#22270;&#20687;&#31245;&#21518;&#20250;&#34987;&#21576;&#29616;&#32473;&#21463;&#23475;&#33410;&#28857;&#30340;&#23545;&#24212;&#27169;&#22411;&#65292;&#20197;&#37325;&#25918;&#25915;&#20987;&#12290;&#20856;&#22411;&#30340;&#31034;&#20363;&#21033;&#29992;&#20102;&#20256;&#25773;&#31574;&#30053;&#65292;&#22914;&#34987;&#26356;&#25913;&#30340;&#20132;&#36890;&#26631;&#24535;&#65288;&#20462;&#34917;&#25915;&#20987;&#65289;&#65292;&#36825;&#20123;&#26631;&#24535;&#19981;&#20877;&#34987;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#35782;&#21035;&#65292;&#25110;&#32773;&#30475;&#20284;&#26410;&#34987;&#26356;&#25913;&#30340;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#20250;&#30772;&#22351;&#32852;&#37030;&#23398;&#20064;&#26041;&#26696;&#30340;&#40065;&#26834;&#24615;&#65292;&#27745;&#26579;&#26412;&#22320;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main premise of federated learning (FL) is that machine learning model updates are computed locally to preserve user data privacy. This approach avoids by design user data to ever leave the perimeter of their device. Once the updates aggregated, the model is broadcast to all nodes in the federation. However, without proper defenses, compromised nodes can probe the model inside their local memory in search for adversarial examples, which can lead to dangerous real-world scenarios. For instance, in image-based applications, adversarial examples consist of images slightly perturbed to the human eye getting misclassified by the local model. These adversarial images are then later presented to a victim node's counterpart model to replay the attack. Typical examples harness dissemination strategies such as altered traffic signs (patch attacks) no longer recognized by autonomous vehicles or seemingly unaltered samples that poison the local dataset of the FL scheme to undermine its robustn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#31215;&#20998;&#24418;&#24335;&#30340;&#26041;&#27861;&#23454;&#29616;&#40065;&#26834;&#30340;SINDy&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22024;&#26434;&#21644;&#31232;&#32570;&#30340;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#25511;&#21046;&#26041;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.07193</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#31215;&#20998;&#24418;&#24335;&#30340;&#26041;&#27861;&#23454;&#29616;&#40065;&#26834;&#30340;SINDy&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Robust SINDy Approach by Combining Neural Networks and an Integral Form. (arXiv:2309.07193v1 [math.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#21644;&#31215;&#20998;&#24418;&#24335;&#30340;&#26041;&#27861;&#23454;&#29616;&#40065;&#26834;&#30340;SINDy&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#22024;&#26434;&#21644;&#31232;&#32570;&#30340;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#25511;&#21046;&#26041;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#26159;&#19968;&#20010;&#38271;&#26399;&#20197;&#26469;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#19968;&#31181;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#29992;&#20110;&#36825;&#20010;&#30446;&#30340;&#30340;&#26041;&#27861;&#26159;&#31232;&#30095;&#22238;&#24402;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#65292;&#20063;&#31216;&#20026;SINDy&#12290;&#23613;&#31649;&#36827;&#34892;&#20102;&#20960;&#27425;&#23581;&#35797;&#65292;&#20294;&#22024;&#26434;&#21644;&#31232;&#32570;&#30340;&#25968;&#25454;&#20173;&#28982;&#23545;SINDy&#26041;&#27861;&#30340;&#25104;&#21151;&#26500;&#25104;&#20102;&#20005;&#37325;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31181;&#20174;&#22024;&#26434;&#21644;&#31232;&#32570;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#25511;&#21046;&#26041;&#31243;&#30340;&#40065;&#26834;&#26041;&#27861;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26681;&#25454;&#27979;&#37327;&#25968;&#25454;&#23398;&#20064;&#19968;&#20010;&#38544;&#24335;&#34920;&#31034;&#65292;&#19981;&#20165;&#21487;&#20197;&#22312;&#27979;&#37327;&#21306;&#22495;&#20869;&#20135;&#29983;&#36755;&#20986;&#65292;&#32780;&#19988;&#36755;&#20986;&#30340;&#26102;&#38388;&#28436;&#21464;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#19968;&#20010;&#21160;&#21147;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;SINDy&#26694;&#26550;&#30340;&#31934;&#31070;&#20013;&#23398;&#20064;&#36825;&#26679;&#19968;&#20010;&#21160;&#21147;&#31995;&#32479;&#12290;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24335;&#34920;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#24037;&#20855;&#33719;&#24471;&#20102;SINDy&#25152;&#38656;&#30340;&#23548;&#25968;&#20449;&#24687;&#12290;&#20026;&#20102;&#22686;&#24378;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#32435;&#20837;&#20102;
&lt;/p&gt;
&lt;p&gt;
The discovery of governing equations from data has been an active field of research for decades. One widely used methodology for this purpose is sparse regression for nonlinear dynamics, known as SINDy. Despite several attempts, noisy and scarce data still pose a severe challenge to the success of the SINDy approach. In this work, we discuss a robust method to discover nonlinear governing equations from noisy and scarce data. To do this, we make use of neural networks to learn an implicit representation based on measurement data so that not only it produces the output in the vicinity of the measurements but also the time-evolution of output can be described by a dynamical system. Additionally, we learn such a dynamic system in the spirit of the SINDy framework. Leveraging the implicit representation using neural networks, we obtain the derivative information -- required for SINDy -- using an automatic differentiation tool. To enhance the robustness of our methodology, we further incorp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20005;&#26684;&#36981;&#23432;&#25968;&#25454;&#22788;&#29702;&#12289;&#23454;&#39564;&#35774;&#35745;&#21644;&#27169;&#22411;&#35780;&#20272;&#30340;&#26368;&#20339;&#23454;&#36341;&#26469;&#30830;&#20445;&#21487;&#37325;&#22797;&#21644;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#38024;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#23545;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#37319;&#29992;&#20102;&#20132;&#21449;&#39564;&#35777;&#21644;&#22810;&#27425;&#35757;&#32451;&#35797;&#39564;&#26469;&#34917;&#20607;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#21021;&#22987;&#38543;&#26426;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.07192</link><description>&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#21644;3D-CNN&#28145;&#24230;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The effect of data augmentation and 3D-CNN depth on Alzheimer's Disease detection. (arXiv:2309.07192v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20005;&#26684;&#36981;&#23432;&#25968;&#25454;&#22788;&#29702;&#12289;&#23454;&#39564;&#35774;&#35745;&#21644;&#27169;&#22411;&#35780;&#20272;&#30340;&#26368;&#20339;&#23454;&#36341;&#26469;&#30830;&#20445;&#21487;&#37325;&#22797;&#21644;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#38024;&#23545;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#23545;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#37319;&#29992;&#20102;&#20132;&#21449;&#39564;&#35777;&#21644;&#22810;&#27425;&#35757;&#32451;&#35797;&#39564;&#26469;&#34917;&#20607;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#21021;&#22987;&#38543;&#26426;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#21307;&#30103;&#39046;&#22495;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#32988;&#36807;&#20256;&#32479;&#32479;&#35745;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#30830;&#31435;&#20026;&#20020;&#24202;&#23454;&#36341;&#20013;&#21487;&#38752;&#30340;&#24037;&#20855;&#65292;&#36981;&#23432;&#26368;&#20339;&#23454;&#36341;&#65292;&#21253;&#25324;&#25968;&#25454;&#22788;&#29702;&#12289;&#23454;&#39564;&#35774;&#35745;&#21644;&#27169;&#22411;&#35780;&#20272;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#24182;&#20005;&#26684;&#36981;&#23432;&#36825;&#20123;&#23454;&#36341;&#65292;&#20197;&#30830;&#20445;&#21487;&#37325;&#22797;&#21644;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#65292;&#36825;&#20316;&#20026;&#21307;&#30103;&#39046;&#22495;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#30340;&#20856;&#22411;&#31034;&#20363;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#23545;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;ADNI&#25968;&#25454;&#38598;&#20013;&#30340;MRI&#25968;&#25454;&#35299;&#20915;&#20102;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#37319;&#29992;&#20102;&#19977;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#23454;&#39564;&#37319;&#29992;&#20132;&#21449;&#39564;&#35777;&#21644;&#22810;&#27425;&#35757;&#32451;&#35797;&#39564;&#26469;&#34917;&#20607;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#21021;&#22987;&#38543;&#26426;&#21442;&#25968;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;15&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has emerged as a promising approach in healthcare, outperforming traditional statistical techniques. However, to establish ML as a reliable tool in clinical practice, adherence to best practices regarding data handling, experimental design, and model evaluation is crucial. This work summarizes and strictly observes such practices to ensure reproducible and reliable ML. Specifically, we focus on Alzheimer's Disease (AD) detection, which serves as a paradigmatic example of challenging problem in healthcare. We investigate the impact of different data augmentation techniques and model complexity on the overall performance. We consider MRI data from ADNI dataset to address a classification problem employing 3D Convolutional Neural Network (CNN). The experiments are designed to compensate for data scarcity and initial random parameters by utilizing cross-validation and multiple training trials. Within this framework, we train 15 predictive models, considering three dif
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28418;&#31227;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#28418;&#31227;&#65288;LfD&#65289;&#26041;&#27861;&#65292;&#23427;&#33021;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#26377;&#25928;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#38450;&#27490;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2309.07189</link><description>&lt;p&gt;
&#23398;&#20064;&#28418;&#31227;&#65306;&#36890;&#36807;&#28418;&#31227;&#27491;&#21017;&#21270;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning From Drift: Federated Learning on Non-IID Data via Drift Regularization. (arXiv:2309.07189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07189
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28418;&#31227;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#28418;&#31227;&#65288;LfD&#65289;&#26041;&#27861;&#65292;&#23427;&#33021;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#26377;&#25928;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#38450;&#27490;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;IID&#65289;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;Non-IID&#65289;&#25968;&#25454;&#19978;&#21364;&#23384;&#22312;&#36739;&#22823;&#22256;&#38590;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#39033;&#30446;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#23427;&#20204;&#22312;&#19982;IID&#25968;&#25454;&#30340;&#35757;&#32451;&#30456;&#27604;&#20173;&#28982;&#19981;&#22815;&#20248;&#21270;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20180;&#32454;&#20998;&#26512;&#20102;&#22312;&#38750;IID&#29615;&#22659;&#20013;&#23384;&#22312;&#30340;&#26041;&#27861;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#27491;&#21017;&#21270;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#22312;&#38450;&#27490;&#38750;IID&#25968;&#25454;&#24615;&#33021;&#19979;&#38477;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#28418;&#31227;&#65288;LfD&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24322;&#26500;&#29615;&#22659;&#20013;&#26377;&#25928;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#26696;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;&#28418;&#31227;&#20272;&#35745;&#21644;&#28418;&#31227;&#27491;&#21017;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LfD&#39318;&#20808;&#20272;&#35745;&#26412;&#22320;&#27169;&#22411;&#19982;&#20840;&#23616;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65288;&#21363;&#28418;&#31227;&#65289;&#65292;&#28982;&#21518;&#23545;&#26412;&#22320;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#20197;&#36991;&#20813;&#36807;&#24230;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning algorithms perform reasonably well on independent and identically distributed (IID) data. They, on the other hand, suffer greatly from heterogeneous environments, i.e., Non-IID data. Despite the fact that many research projects have been done to address this issue, recent findings indicate that they are still sub-optimal when compared to training on IID data. In this work, we carefully analyze the existing methods in heterogeneous environments. Interestingly, we find that regularizing the classifier's outputs is quite effective in preventing performance degradation on Non-IID data. Motivated by this, we propose Learning from Drift (LfD), a novel method for effectively training the model in heterogeneous settings. Our scheme encapsulates two key components: drift estimation and drift regularization. Specifically, LfD first estimates how different the local model is from the global model (i.e., drift). The local model is then regularized such that it does not fall in t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#23384;&#20998;&#26512;&#26469;&#39044;&#27979;&#28378;&#29664;&#36724;&#25215;&#30340;&#22833;&#25928;&#26102;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#39057;&#22495;&#25968;&#25454;&#21644;&#35757;&#32451;&#22810;&#20010;&#29983;&#23384;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#27169;&#22411;&#21487;&#20197;&#20197;&#26102;&#38388;&#20026;&#22522;&#30784;&#36827;&#34892;&#39118;&#38505;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#24182;&#20801;&#35768;&#27604;&#36739;&#19981;&#21516;&#32452;&#36724;&#25215;&#30340;&#29983;&#23384;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.07188</link><description>&lt;p&gt;
&#22312;&#23384;&#22312;&#25130;&#23614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#28378;&#29664;&#36724;&#25215;&#30340;&#23551;&#21629;
&lt;/p&gt;
&lt;p&gt;
Predicting Survival Time of Ball Bearings in the Presence of Censoring. (arXiv:2309.07188v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#23384;&#20998;&#26512;&#26469;&#39044;&#27979;&#28378;&#29664;&#36724;&#25215;&#30340;&#22833;&#25928;&#26102;&#38388;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#39057;&#22495;&#25968;&#25454;&#21644;&#35757;&#32451;&#22810;&#20010;&#29983;&#23384;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#27169;&#22411;&#21487;&#20197;&#20197;&#26102;&#38388;&#20026;&#22522;&#30784;&#36827;&#34892;&#39118;&#38505;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#24182;&#20801;&#35768;&#27604;&#36739;&#19981;&#21516;&#32452;&#36724;&#25215;&#30340;&#29983;&#23384;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28378;&#29664;&#36724;&#25215;&#22312;&#21508;&#31181;&#21046;&#36896;&#21644;&#26426;&#26800;&#39046;&#22495;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#30417;&#27979;&#30952;&#25439;&#24182;&#22312;&#25925;&#38556;&#21457;&#29983;&#20043;&#21069;&#21457;&#29616;&#32570;&#38519;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#28041;&#21450;&#21040;&#25130;&#23614;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#21363;&#26410;&#35266;&#23519;&#21040;&#25925;&#38556;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#23384;&#20998;&#26512;&#26469;&#39044;&#27979;&#28378;&#29664;&#36724;&#25215;&#30340;&#22833;&#25928;&#26102;&#38388;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#39057;&#22495;&#20013;&#20998;&#26512;&#36724;&#25215;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#24211;&#23572;&#24052;&#36203;-&#33713;&#24067;&#21202;&#25955;&#24230;&#21644;&#20854;&#30772;&#22351;&#39057;&#29575;&#21306;&#38388;&#19982;&#20854;&#30772;&#22351;&#39057;&#29575;&#21306;&#38388;&#20043;&#38388;&#30340;&#26631;&#20934;&#24046;&#26469;&#26631;&#27880;&#36724;&#25215;&#25925;&#38556;&#26102;&#21051;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#22810;&#20010;&#29983;&#23384;&#27169;&#22411;&#65292;&#26681;&#25454;&#26631;&#27880;&#25968;&#25454;&#21644;&#20174;&#26102;&#22495;&#25552;&#21462;&#30340;&#21327;&#21464;&#37327;&#65288;&#22914;&#20559;&#24230;&#12289;&#23792;&#24230;&#21644;&#29109;&#65289;&#26469;&#20272;&#35745;&#22833;&#25928;&#26102;&#38388;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20197;&#26102;&#38388;&#20026;&#22522;&#30784;&#36827;&#34892;&#39118;&#38505;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#27604;&#36739;&#19981;&#21516;&#32452;&#30340;&#36724;&#25215;&#30340;&#29983;&#23384;&#20989;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ball bearings find widespread use in various manufacturing and mechanical domains, and methods based on machine learning have been widely adopted in the field to monitor wear and spot defects before they lead to failures. Few studies, however, have addressed the problem of censored data, in which failure is not observed. In this paper, we propose a novel approach to predict the time to failure in ball bearings using survival analysis. First, we analyze bearing data in the frequency domain and annotate when a bearing fails by comparing the Kullback-Leibler divergence and the standard deviation between its break-in frequency bins and its break-out frequency bins. Second, we train several survival models to estimate the time to failure based on the annotated data and covariates extracted from the time domain, such as skewness, kurtosis and entropy. The models give a probabilistic prediction of risk over time and allow us to compare the survival function between groups of bearings. We demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#22270;&#26102;&#31354;&#21367;&#31215;&#32593;&#32476;&#21644;&#24207;&#21015;&#20998;&#35299;&#30340;&#22810;&#27493;&#21494;&#32511;&#32032;&#27987;&#24230;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25366;&#25496;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#23545;&#29615;&#22659;&#20445;&#25252;&#21644;&#27700;&#20135;&#20859;&#27542;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.07187</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#22270;&#26102;&#31354;&#21367;&#31215;&#32593;&#32476;&#21644;&#24207;&#21015;&#20998;&#35299;&#30340;&#22810;&#27493;&#21494;&#32511;&#32032;&#27987;&#24230;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-step prediction of chlorophyll concentration based on Adaptive Graph-Temporal Convolutional Network with Series Decomposition. (arXiv:2309.07187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#22270;&#26102;&#31354;&#21367;&#31215;&#32593;&#32476;&#21644;&#24207;&#21015;&#20998;&#35299;&#30340;&#22810;&#27493;&#21494;&#32511;&#32032;&#27987;&#24230;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#25366;&#25496;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#23545;&#29615;&#22659;&#20445;&#25252;&#21644;&#27700;&#20135;&#20859;&#27542;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21494;&#32511;&#32032;&#27987;&#24230;&#33021;&#22815;&#24456;&#22909;&#22320;&#21453;&#26144;&#27700;&#20307;&#30340;&#33829;&#20859;&#29366;&#20917;&#21644;&#34299;&#31867;&#27700;&#21326;&#65292;&#26159;&#35780;&#20272;&#27700;&#36136;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#21494;&#32511;&#32032;&#27987;&#24230;&#21464;&#21270;&#36235;&#21183;&#30340;&#39044;&#27979;&#23545;&#29615;&#22659;&#20445;&#25252;&#21644;&#27700;&#20135;&#20859;&#27542;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24433;&#21709;&#21494;&#32511;&#32032;&#27987;&#24230;&#30340;&#22240;&#32032;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#21644;&#19981;&#21487;&#36776;&#35782;&#30340;&#38750;&#32447;&#24615;&#20851;&#31995;&#12290;&#20026;&#20102;&#26377;&#25928;&#25366;&#25496;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#33258;&#36866;&#24212;&#22270;&#26102;&#31354;&#21367;&#31215;&#32593;&#32476;&#65288;AGTCNSD&#65289;&#39044;&#27979;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#31227;&#21160;&#24179;&#22343;&#27861;&#23558;&#21407;&#22987;&#24207;&#21015;&#20998;&#35299;&#20026;&#36235;&#21183;&#20998;&#37327;&#21644;&#21608;&#26399;&#20998;&#37327;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#27700;&#36136;&#21442;&#25968;&#25968;&#25454;&#65292;&#24182;&#23450;&#20041;&#21442;&#25968;&#23884;&#20837;&#30697;&#38453;&#12290;&#21033;&#29992;&#30697;&#38453;&#20998;&#35299;&#30340;&#24605;&#24819;&#20026;&#27599;&#20010;&#33410;&#28857;&#20998;&#37197;&#26435;&#37325;&#21442;&#25968;&#12290;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#23398;&#20064;&#20102;&#26368;&#20339;&#30340;&#33410;&#28857;&#26435;&#37325;&#21442;&#25968;&#21644;&#37051;&#23621;&#33410;&#28857;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chlorophyll concentration can well reflect the nutritional status and algal blooms of water bodies, and is an important indicator for evaluating water quality. The prediction of chlorophyll concentration change trend is of great significance to environmental protection and aquaculture. However, there is a complex and indistinguishable nonlinear relationship between many factors affecting chlorophyll concentration. In order to effectively mine the nonlinear features contained in the data. This paper proposes a time-series decomposition adaptive graph-time convolutional network ( AGTCNSD ) prediction model. Firstly, the original sequence is decomposed into trend component and periodic component by moving average method. Secondly, based on the graph convolutional neural network, the water quality parameter data is modeled, and a parameter embedding matrix is defined. The idea of matrix decomposition is used to assign weight parameters to each node. The adaptive graph convolution learns th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22768;&#38899;&#25968;&#25454;&#35757;&#32451;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;&#21628;&#21560;&#30142;&#30149;&#36827;&#34892;&#20998;&#31867;&#12290;&#26041;&#27861;&#37319;&#29992;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#21644;&#39057;&#35889;&#20998;&#26512;&#26469;&#25552;&#21462;&#19982;&#24515;&#34880;&#31649;&#21644;&#21628;&#21560;&#27169;&#24335;&#30456;&#20851;&#30340;&#29983;&#29702;&#20449;&#21495;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#21644;&#39044;&#27979;&#24314;&#27169;&#23454;&#29616;&#24555;&#36895;&#31579;&#26597;&#21644;&#35786;&#26029;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2309.07183</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#32423;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22768;&#38899;&#30456;&#20851;&#21628;&#21560;&#30142;&#30149;&#20998;&#31867;&#29992;&#20110;&#36741;&#21161;&#35786;&#26029;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Audio-Based Classification of Respiratory Diseases using Advanced Signal Processing and Machine Learning for Assistive Diagnosis Support. (arXiv:2309.07183v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22768;&#38899;&#25968;&#25454;&#35757;&#32451;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;&#21628;&#21560;&#30142;&#30149;&#36827;&#34892;&#20998;&#31867;&#12290;&#26041;&#27861;&#37319;&#29992;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#21644;&#39057;&#35889;&#20998;&#26512;&#26469;&#25552;&#21462;&#19982;&#24515;&#34880;&#31649;&#21644;&#21628;&#21560;&#27169;&#24335;&#30456;&#20851;&#30340;&#29983;&#29702;&#20449;&#21495;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#25552;&#21462;&#21644;&#39044;&#27979;&#24314;&#27169;&#23454;&#29616;&#24555;&#36895;&#31579;&#26597;&#21644;&#35786;&#26029;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20840;&#29699;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#26159;&#23548;&#33268;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#36825;&#31361;&#26174;&#20102;&#36805;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#25512;&#36827;&#21548;&#35786;&#30340;&#24555;&#36895;&#31579;&#26597;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#21033;&#29992;&#20854;&#20013;&#19968;&#20010;&#26368;&#22823;&#30340;&#20844;&#24320;&#30340;&#21628;&#21560;&#38899;&#21307;&#23398;&#25968;&#25454;&#24211;&#26469;&#35757;&#32451;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23545;&#19981;&#21516;&#30340;&#20581;&#24247;&#29366;&#20917;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#32463;&#39564;&#27169;&#24577;&#20998;&#35299;&#65288;EMD&#65289;&#21644;&#39057;&#35889;&#20998;&#26512;&#65292;&#20174;&#22768;&#23398;&#25968;&#25454;&#20013;&#25552;&#21462;&#19982;&#24515;&#34880;&#31649;&#21644;&#21628;&#21560;&#27169;&#24335;&#23494;&#20999;&#30456;&#20851;&#30340;&#29983;&#29702;&#30456;&#20851;&#29983;&#29289;&#20449;&#21495;&#65292;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;&#21151;&#29575;&#35889;&#23494;&#24230;&#20998;&#26512;&#21644;&#28388;&#27874;&#25216;&#26415;&#26469;&#36873;&#25321;&#19982;&#28508;&#22312;&#29983;&#29702;&#29616;&#35937;&#24378;&#30456;&#20851;&#30340;&#22266;&#26377;&#27169;&#24577;&#20989;&#25968;&#65288;IMFs&#65289;&#12290;&#36825;&#20123;&#29983;&#29289;&#20449;&#21495;&#32463;&#36807;&#20840;&#38754;&#30340;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#29992;&#20110;&#39044;&#27979;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37096;&#32626;&#19968;&#20010;&#20108;&#20998;&#31867;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In global healthcare, respiratory diseases are a leading cause of mortality, underscoring the need for rapid and accurate diagnostics. To advance rapid screening techniques via auscultation, our research focuses on employing one of the largest publicly available medical database of respiratory sounds to train multiple machine learning models able to classify different health conditions. Our method combines Empirical Mode Decomposition (EMD) and spectral analysis to extract physiologically relevant biosignals from acoustic data, closely tied to cardiovascular and respiratory patterns, making our approach apart in its departure from conventional audio feature extraction practices. We use Power Spectral Density analysis and filtering techniques to select Intrinsic Mode Functions (IMFs) strongly correlated with underlying physiological phenomena. These biosignals undergo a comprehensive feature extraction process for predictive modeling. Initially, we deploy a binary classification model t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"EEGMobile"&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#65292;&#29305;&#21035;&#22312;N1&#38454;&#27573;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2309.07182</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Sleep Stage Classification Using a Pre-trained Deep Learning Model. (arXiv:2309.07182v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"EEGMobile"&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#65292;&#29305;&#21035;&#22312;N1&#38454;&#27573;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#38556;&#30861;&#26159;&#24120;&#35265;&#30340;&#20154;&#31867;&#30142;&#30149;&#20043;&#19968;&#12290;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#22312;&#35786;&#26029;&#30561;&#30496;&#38556;&#30861;&#12289;&#30417;&#27979;&#27835;&#30103;&#25928;&#26524;&#21644;&#29702;&#35299;&#30561;&#30496;&#38454;&#27573;&#19982;&#21508;&#31181;&#20581;&#24247;&#29366;&#20917;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#36215;&#30528;&#22522;&#30784;&#24615;&#20316;&#29992;&#12290;&#31934;&#30830;&#32780;&#26377;&#25928;&#22320;&#20998;&#31867;&#36825;&#20123;&#38454;&#27573;&#21487;&#20197;&#26174;&#30528;&#25552;&#21319;&#25105;&#20204;&#23545;&#19982;&#30561;&#30496;&#30456;&#20851;&#29616;&#35937;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#26368;&#32456;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#21644;&#30142;&#30149;&#27835;&#30103;&#25928;&#26524;&#12290;&#20854;&#20182;&#25552;&#20986;&#30340;&#27169;&#22411;&#24448;&#24448;&#32791;&#26102;&#19988;&#32570;&#20047;&#36275;&#22815;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;N1&#38454;&#27573;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;"EEGMobile"&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20174;&#33041;&#20449;&#21495;&#30340;&#33041;&#30005;&#22270;&#35889;&#22270;&#23398;&#20064;&#12290;&#22312;&#21517;&#20026;"Sleep-EDF20"&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#27169;&#22411;&#23454;&#29616;&#20102;86.97%&#30340;&#20934;&#30830;&#29575;&#65292;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#30740;&#31350;&#32773;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;N1&#38454;&#27573;&#35760;&#24405;&#20102;56.4%&#30340;&#20934;&#30830;&#29575;&#65292;&#36825;&#26159;&#26356;&#22909;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the common human diseases is sleep disorders. The classification of sleep stages plays a fundamental role in diagnosing sleep disorders, monitoring treatment effectiveness, and understanding the relationship between sleep stages and various health conditions. A precise and efficient classification of these stages can significantly enhance our understanding of sleep-related phenomena and ultimately lead to improved health outcomes and disease treatment.  Models others propose are often time-consuming and lack sufficient accuracy, especially in stage N1. The main objective of this research is to present a machine-learning model called "EEGMobile". This model utilizes pre-trained models and learns from electroencephalogram (EEG) spectrograms of brain signals. The model achieved an accuracy of 86.97% on a publicly available dataset named "Sleep-EDF20", outperforming other models proposed by different researchers. Moreover, it recorded an accuracy of 56.4% in stage N1, which is bette
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#22312;&#19981;&#21516;&#30828;&#20214;&#19978;&#30340;&#21487;&#31227;&#26893;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36716;&#31227;&#21040;&#20854;&#20182;&#30828;&#20214;&#19978;&#26102;&#21487;&#33021;&#20250;&#20002;&#22833;&#36229;&#36807;40%&#30340;&#20851;&#38190;&#21151;&#33021;&#12290;&#36825;&#31181;&#19981;&#21487;&#31227;&#26893;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#21019;&#26032;&#30340;&#36895;&#24230;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.07181</link><description>&lt;p&gt;
&#22823;&#24187;&#35273;&#65306;&#36719;&#20214;&#21487;&#31227;&#26893;&#24615;&#30340;&#31070;&#35805;&#21450;&#20854;&#23545;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Grand Illusion: The Myth of Software Portability and Implications for ML Progress. (arXiv:2309.07181v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07181
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#22312;&#19981;&#21516;&#30828;&#20214;&#19978;&#30340;&#21487;&#31227;&#26893;&#24615;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36716;&#31227;&#21040;&#20854;&#20182;&#30828;&#20214;&#19978;&#26102;&#21487;&#33021;&#20250;&#20002;&#22833;&#36229;&#36807;40%&#30340;&#20851;&#38190;&#21151;&#33021;&#12290;&#36825;&#31181;&#19981;&#21487;&#31227;&#26893;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#21019;&#26032;&#30340;&#36895;&#24230;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#36793;&#30028;&#24448;&#24448;&#38656;&#35201;&#25506;&#32034;&#19981;&#21516;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#33258;&#30001;&#22320;&#22312;&#19981;&#21516;&#30340;&#24037;&#20855;&#22534;&#26632;&#20043;&#38388;&#36827;&#34892;&#23454;&#39564;&#21487;&#33021;&#19982;&#25928;&#29575;&#30340;&#36861;&#27714;&#30456;&#20914;&#31361;&#65292;&#36825;&#23548;&#33268;&#20102;&#36234;&#26469;&#36234;&#19987;&#38376;&#30340;&#20154;&#24037;&#26234;&#33021;&#30828;&#20214;&#30340;&#20986;&#29616;&#65292;&#24182;&#25512;&#21160;&#20102;&#22260;&#32469;&#19968;&#23567;&#37096;&#20998;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#30340;&#25972;&#21512;&#12290;&#22914;&#26524;&#36719;&#20214;&#21644;&#30828;&#20214;&#21516;&#26102;&#21457;&#23637;&#65292;&#37027;&#20040;&#25506;&#32034;&#24615;&#30740;&#31350;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#65292;&#20351;&#24471;&#36828;&#31163;&#27969;&#34892;&#24037;&#20855;&#22534;&#26632;&#30340;&#20027;&#27969;&#24605;&#24819;&#26356;&#21152;&#22256;&#38590;&#12290;&#34429;&#28982;&#36825;&#31181;&#25705;&#25830;&#36234;&#26469;&#36234;&#22810;&#22320;&#24433;&#21709;&#21040;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21019;&#26032;&#36895;&#24230;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#24037;&#20855;&#30340;&#19981;&#21487;&#31227;&#26893;&#24615;&#23578;&#26410;&#24471;&#21040;&#37327;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#36719;&#20214;&#26694;&#26550;&#26377;&#22810;&#21487;&#31227;&#26893;&#65311;&#25105;&#20204;&#23545;&#20027;&#35201;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#22312;&#19981;&#21516;&#30828;&#20214;&#31867;&#22411;&#19978;&#30340;&#21487;&#31227;&#26893;&#24615;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25551;&#32472;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#19981;&#23433;&#30340;&#30011;&#38754;--&#24403;&#36825;&#20123;&#26694;&#26550;&#36716;&#31227;&#21040;&#20854;&#20182;&#30828;&#20214;&#19978;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#20002;&#22833;&#36229;&#36807;40%&#30340;&#20851;&#38190;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pushing the boundaries of machine learning often requires exploring different hardware and software combinations. However, the freedom to experiment across different tooling stacks can be at odds with the drive for efficiency, which has produced increasingly specialized AI hardware and incentivized consolidation around a narrow set of ML frameworks. Exploratory research can be restricted if software and hardware are co-evolving, making it even harder to stray away from mainstream ideas that work well with popular tooling stacks. While this friction increasingly impacts the rate of innovation in machine learning, to our knowledge the lack of portability in tooling has not been quantified. In this work, we ask: How portable are popular ML software frameworks? We conduct a large-scale study of the portability of mainstream ML frameworks across different hardware types. Our findings paint an uncomfortable picture -- frameworks can lose more than 40% of their key functions when ported to ot
&lt;/p&gt;</description></item><item><title>CloudBrain-NMR&#26159;&#19968;&#20010;&#26234;&#33021;&#20113;&#35745;&#31639;&#24179;&#21488;&#65292;&#29992;&#20110;NMR&#20809;&#35889;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#36890;&#36807;&#22312;&#32447;&#35775;&#38382;&#65292;&#26080;&#38656;&#29992;&#25143;&#31471;&#23433;&#35013;&#20219;&#20309;&#31243;&#24207;&#65292;&#20351;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#21152;&#24555;&#22788;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.07178</link><description>&lt;p&gt;
CloudBrain-NMR: &#19968;&#31181;&#26234;&#33021;&#20113;&#35745;&#31639;&#24179;&#21488;&#65292;&#29992;&#20110;NMR&#20809;&#35889;&#22788;&#29702;&#12289;&#37325;&#24314;&#21644;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
CloudBrain-NMR: An Intelligent Cloud Computing Platform for NMR Spectroscopy Processing, Reconstruction and Analysis. (arXiv:2309.07178v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07178
&lt;/p&gt;
&lt;p&gt;
CloudBrain-NMR&#26159;&#19968;&#20010;&#26234;&#33021;&#20113;&#35745;&#31639;&#24179;&#21488;&#65292;&#29992;&#20110;NMR&#20809;&#35889;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#36890;&#36807;&#22312;&#32447;&#35775;&#38382;&#65292;&#26080;&#38656;&#29992;&#25143;&#31471;&#23433;&#35013;&#20219;&#20309;&#31243;&#24207;&#65292;&#20351;&#29992;&#24182;&#34892;&#35745;&#31639;&#26469;&#21152;&#24555;&#22788;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#30913;&#20849;&#25391;(NMR)&#20809;&#35889;&#23398;&#22312;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#20013;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30740;&#31350;&#20998;&#23376;&#32467;&#26500;&#21644;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#23545;&#20174;NMR&#20809;&#35889;&#20202;&#33719;&#21462;&#30340;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#21644;&#21518;&#32493;&#23450;&#37327;&#20998;&#26512;&#28041;&#21450;&#21508;&#31181;&#19987;&#38376;&#24037;&#20855;&#65292;&#36825;&#23601;&#38656;&#35201;&#20840;&#38754;&#25484;&#25569;&#31243;&#24207;&#21644;NMR&#26041;&#38754;&#30340;&#30693;&#35782;&#12290;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#35774;&#32622;&#22797;&#26434;&#65292;&#26032;&#20852;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#22312;NMR&#20013;&#30340;&#24212;&#29992;&#24182;&#19981;&#23481;&#26131;&#12290;&#22240;&#27492;&#65292;NMR&#22788;&#29702;&#23545;&#20110;&#21270;&#23398;&#23478;&#21644;&#29983;&#29289;&#23398;&#23478;&#26469;&#35828;&#24182;&#19981;&#26159;&#19968;&#39033;&#23481;&#26131;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CloudBrain-NMR&#65292;&#19968;&#31181;&#26234;&#33021;&#22312;&#32447;&#20113;&#35745;&#31639;&#24179;&#21488;&#65292;&#29992;&#20110;NMR&#25968;&#25454;&#30340;&#35835;&#21462;&#65292;&#22788;&#29702;&#65292;&#37325;&#24314;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#35813;&#24179;&#21488;&#36890;&#36807;Web&#27983;&#35272;&#22120;&#26041;&#20415;&#22320;&#35775;&#38382;&#65292;&#26080;&#38656;&#29992;&#25143;&#31471;&#23433;&#35013;&#20219;&#20309;&#31243;&#24207;&#12290;CloudBrain-NMR&#20351;&#29992;&#22270;&#24418;&#22788;&#29702;&#21333;&#20803;&#21644;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;&#36827;&#34892;&#24182;&#34892;&#35745;&#31639;&#65292;&#20174;&#32780;&#22823;&#22823;&#32553;&#30701;&#20102;&#22788;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nuclear Magnetic Resonance (NMR) spectroscopy has served as a powerful analytical tool for studying molecular structure and dynamics in chemistry and biology. However, the processing of raw data acquired from NMR spectrometers and subsequent quantitative analysis involves various specialized tools, which necessitates comprehensive knowledge in programming and NMR. Particularly, the emerging deep learning tools is hard to be widely used in NMR due to the sophisticated setup of computation. Thus, NMR processing is not an easy task for chemist and biologists. In this work, we present CloudBrain-NMR, an intelligent online cloud computing platform designed for NMR data reading, processing, reconstruction, and quantitative analysis. The platform is conveniently accessed through a web browser, eliminating the need for any program installation on the user side. CloudBrain-NMR uses parallel computing with graphics processing units and central processing units, resulting in significantly shorten
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#38024;&#23545;&#40723;&#21169;&#25919;&#31574;&#30340;&#26368;&#20248;&#21644;&#20844;&#24179;&#35780;&#20272;&#20197;&#21450;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#20154;&#31867;&#19981;&#36981;&#24490;&#27835;&#30103;&#24314;&#35758;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#35268;&#21017;&#21482;&#26159;&#24314;&#35758;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;&#27835;&#30103;&#30340;&#24322;&#36136;&#24615;&#21644;&#20844;&#24179;&#32771;&#34385;&#22240;&#32032;&#65292;&#20915;&#31574;&#32773;&#30340;&#26435;&#34913;&#21644;&#20915;&#31574;&#35268;&#21017;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#22312;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#65292;&#30740;&#31350;&#26174;&#31034;&#23384;&#22312;&#19968;&#20010;&#20351;&#29992;&#24046;&#36317;&#38382;&#39064;&#65292;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#30340;&#20154;&#21364;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#30410;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.07176</link><description>&lt;p&gt;
&#26368;&#20248;&#21644;&#20844;&#24179;&#30340;&#40723;&#21169;&#25919;&#31574;&#35780;&#20272;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal and Fair Encouragement Policy Evaluation and Learning. (arXiv:2309.07176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#38024;&#23545;&#40723;&#21169;&#25919;&#31574;&#30340;&#26368;&#20248;&#21644;&#20844;&#24179;&#35780;&#20272;&#20197;&#21450;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#20154;&#31867;&#19981;&#36981;&#24490;&#27835;&#30103;&#24314;&#35758;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#35268;&#21017;&#21482;&#26159;&#24314;&#35758;&#12290;&#21516;&#26102;&#65292;&#38024;&#23545;&#27835;&#30103;&#30340;&#24322;&#36136;&#24615;&#21644;&#20844;&#24179;&#32771;&#34385;&#22240;&#32032;&#65292;&#20915;&#31574;&#32773;&#30340;&#26435;&#34913;&#21644;&#20915;&#31574;&#35268;&#21017;&#20063;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#22312;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#65292;&#30740;&#31350;&#26174;&#31034;&#23384;&#22312;&#19968;&#20010;&#20351;&#29992;&#24046;&#36317;&#38382;&#39064;&#65292;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#21463;&#30410;&#30340;&#20154;&#21364;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#30410;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#24378;&#21046;&#20010;&#20307;&#25509;&#21463;&#27835;&#30103;&#36890;&#24120;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#22240;&#27492;&#22312;&#20154;&#31867;&#19981;&#36981;&#24490;&#27835;&#30103;&#24314;&#35758;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#31574;&#30053;&#35268;&#21017;&#21482;&#26159;&#24314;&#35758;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#25509;&#21463;&#27835;&#30103;&#30340;&#20010;&#20307;&#21487;&#33021;&#23384;&#22312;&#24322;&#36136;&#24615;&#65292;&#27835;&#30103;&#25928;&#26524;&#20063;&#21487;&#33021;&#23384;&#22312;&#24322;&#36136;&#24615;&#12290;&#34429;&#28982;&#26368;&#20248;&#27835;&#30103;&#35268;&#21017;&#21487;&#20197;&#26368;&#22823;&#21270;&#25972;&#20010;&#20154;&#32676;&#30340;&#22240;&#26524;&#32467;&#26524;&#65292;&#20294;&#22312;&#40723;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#35775;&#38382;&#24179;&#31561;&#38480;&#21046;&#25110;&#20854;&#20182;&#20844;&#24179;&#32771;&#34385;&#22240;&#32032;&#21487;&#33021;&#26159;&#30456;&#20851;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#65292;&#19968;&#20010;&#25345;&#20037;&#30340;&#38590;&#39064;&#26159;&#37027;&#20123;&#26368;&#26377;&#21487;&#33021;&#20174;&#20013;&#21463;&#30410;&#30340;&#20154;&#20013;&#37027;&#20123;&#33719;&#30410;&#26381;&#21153;&#30340;&#20351;&#29992;&#24046;&#36317;&#12290;&#24403;&#20915;&#31574;&#32773;&#23545;&#35775;&#38382;&#21644;&#24179;&#22343;&#32467;&#26524;&#37117;&#26377;&#20998;&#37197;&#20559;&#22909;&#26102;&#65292;&#26368;&#20248;&#20915;&#31574;&#35268;&#21017;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22240;&#26524;&#35782;&#21035;&#12289;&#32479;&#35745;&#26041;&#24046;&#20943;&#23569;&#20272;&#35745;&#21644;&#31283;&#20581;&#20272;&#35745;&#30340;&#26368;&#20248;&#27835;&#30103;&#35268;&#21017;&#65292;&#21253;&#25324;&#22312;&#36829;&#21453;&#38451;&#24615;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In consequential domains, it is often impossible to compel individuals to take treatment, so that optimal policy rules are merely suggestions in the presence of human non-adherence to treatment recommendations. In these same domains, there may be heterogeneity both in who responds in taking-up treatment, and heterogeneity in treatment efficacy. While optimal treatment rules can maximize causal outcomes across the population, access parity constraints or other fairness considerations can be relevant in the case of encouragement. For example, in social services, a persistent puzzle is the gap in take-up of beneficial services among those who may benefit from them the most. When in addition the decision-maker has distributional preferences over both access and average outcomes, the optimal decision rule changes. We study causal identification, statistical variance-reduced estimation, and robust estimation of optimal treatment rules, including under potential violations of positivity. We c
&lt;/p&gt;</description></item><item><title>MELAGE&#26159;&#19968;&#27454;&#32431;Python&#22522;&#20110;&#26032;&#29983;&#20799;&#31070;&#32463;&#24433;&#20687;&#36719;&#20214;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#20016;&#23500;&#30340;&#21151;&#33021;&#12290;&#20854;&#26680;&#24515;&#29305;&#28857;&#26159;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#30340;&#21322;&#33258;&#21160;&#33041;&#37096;&#25552;&#21462;&#24037;&#20855;&#65292;&#21487;&#31934;&#30830;&#39640;&#25928;&#22320;&#25552;&#21462;MRI&#21644;3D&#36229;&#22768;&#25968;&#25454;&#20013;&#30340;&#33041;&#32467;&#26500;&#12290;&#35813;&#36719;&#20214;&#22312;&#22270;&#20687;&#20998;&#26512;&#12289;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#20197;&#21450;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.07175</link><description>&lt;p&gt;
MELAGE: &#19968;&#31181;&#32431;Python&#22522;&#20110;&#26032;&#29983;&#20799;&#31070;&#32463;&#24433;&#20687;&#36719;&#20214;
&lt;/p&gt;
&lt;p&gt;
MELAGE: A purely python based Neuroimaging software (Neonatal). (arXiv:2309.07175v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07175
&lt;/p&gt;
&lt;p&gt;
MELAGE&#26159;&#19968;&#27454;&#32431;Python&#22522;&#20110;&#26032;&#29983;&#20799;&#31070;&#32463;&#24433;&#20687;&#36719;&#20214;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#36866;&#24212;&#24615;&#21644;&#20016;&#23500;&#30340;&#21151;&#33021;&#12290;&#20854;&#26680;&#24515;&#29305;&#28857;&#26159;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#30340;&#21322;&#33258;&#21160;&#33041;&#37096;&#25552;&#21462;&#24037;&#20855;&#65292;&#21487;&#31934;&#30830;&#39640;&#25928;&#22320;&#25552;&#21462;MRI&#21644;3D&#36229;&#22768;&#25968;&#25454;&#20013;&#30340;&#33041;&#32467;&#26500;&#12290;&#35813;&#36719;&#20214;&#22312;&#22270;&#20687;&#20998;&#26512;&#12289;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#38598;&#25104;&#20197;&#21450;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MELAGE&#26159;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#22522;&#20110;Python&#30340;&#31070;&#32463;&#24433;&#20687;&#36719;&#20214;&#65292;&#25104;&#20026;&#21487;&#35270;&#21270;&#12289;&#22788;&#29702;&#21644;&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#30340;&#22810;&#21151;&#33021;&#24037;&#20855;&#12290;&#26368;&#21021;&#20026;&#20102;&#35299;&#20915;&#26032;&#29983;&#20799;&#26399;&#22788;&#29702;3D&#36229;&#22768;&#21644;MRI&#33041;&#22270;&#20687;&#30340;&#29420;&#29305;&#25361;&#25112;&#32780;&#35774;&#35745;&#65292;MELAGE&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#36866;&#24212;&#24615;&#65292;&#25193;&#23637;&#20102;&#20854;&#22312;&#25104;&#20154;&#20154;&#33041;&#24433;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;MELAGE&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#30001;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#22686;&#24378;&#30340;&#21322;&#33258;&#21160;&#33041;&#37096;&#25552;&#21462;&#24037;&#20855;&#65292;&#30830;&#20445;&#20174;MRI&#21644;3D&#36229;&#22768;&#25968;&#25454;&#20013;&#31934;&#30830;&#39640;&#25928;&#22320;&#25552;&#21462;&#33041;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;MELAGE&#36824;&#25552;&#20379;&#20840;&#38754;&#30340;&#21151;&#33021;&#22871;&#20214;&#65292;&#21253;&#25324;&#21160;&#24577;3D&#21487;&#35270;&#21270;&#12289;&#20934;&#30830;&#30340;&#27979;&#37327;&#21644;&#20132;&#20114;&#24335;&#22270;&#20687;&#20998;&#21106;&#12290;&#36825;&#20010;&#21019;&#26032;&#36719;&#20214;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20020;&#24202;&#21307;&#29983;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#65292;&#25552;&#20379;&#20102;&#31616;&#21270;&#30340;&#22270;&#20687;&#20998;&#26512;&#12289;&#19982;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#26080;&#32541;&#38598;&#25104;&#20197;&#21450;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
MELAGE, a pioneering Python-based neuroimaging software, emerges as a versatile tool for the visualization, processing, and analysis of medical images. Initially conceived to address the unique challenges of processing 3D ultrasound and MRI brain images during the neonatal period, MELAGE exhibits remarkable adaptability, extending its utility to the domain of adult human brain imaging. At its core, MELAGE features a semi-automatic brain extraction tool empowered by a deep learning module, ensuring precise and efficient brain structure extraction from MRI and 3D Ultrasound data. Moreover, MELAGE offers a comprehensive suite of features, encompassing dynamic 3D visualization, accurate measurements, and interactive image segmentation. This transformative software holds immense promise for researchers and clinicians, offering streamlined image analysis, seamless integration with deep learning algorithms, and broad applicability in the realm of medical imaging.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;HurriCast&#65292;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32452;&#21512;ARIMA&#27169;&#22411;&#21644;K-MEANS&#31639;&#27861;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39123;&#39118;&#36235;&#21183;&#65292;&#24182;&#32467;&#21512;Autoencoder&#36827;&#34892;&#25913;&#36827;&#30340;&#39123;&#39118;&#27169;&#25311;&#65292;&#20174;&#32780;&#26377;&#25928;&#27169;&#25311;&#21382;&#21490;&#39123;&#39118;&#34892;&#20026;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#26410;&#26469;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20840;&#38754;&#19988;&#26377;&#36873;&#25321;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20016;&#23500;&#20102;&#23545;&#39123;&#39118;&#27169;&#24335;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.07174</link><description>&lt;p&gt;
HurriCast&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#29992;&#20110;&#39123;&#39118;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
HurriCast: An Automatic Framework Using Machine Learning and Statistical Modeling for Hurricane Forecasting. (arXiv:2309.07174v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;HurriCast&#65292;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#24314;&#27169;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#32452;&#21512;ARIMA&#27169;&#22411;&#21644;K-MEANS&#31639;&#27861;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39123;&#39118;&#36235;&#21183;&#65292;&#24182;&#32467;&#21512;Autoencoder&#36827;&#34892;&#25913;&#36827;&#30340;&#39123;&#39118;&#27169;&#25311;&#65292;&#20174;&#32780;&#26377;&#25928;&#27169;&#25311;&#21382;&#21490;&#39123;&#39118;&#34892;&#20026;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#26410;&#26469;&#39044;&#27979;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20840;&#38754;&#19988;&#26377;&#36873;&#25321;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20016;&#23500;&#20102;&#23545;&#39123;&#39118;&#27169;&#24335;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39123;&#39118;&#30001;&#20110;&#20854;&#28798;&#23475;&#24615;&#24433;&#21709;&#32780;&#22312;&#32654;&#22269;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#24456;&#37325;&#35201;&#65292;&#20445;&#38505;&#19994;&#22312;&#36825;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20351;&#29992;&#22797;&#26434;&#30340;&#32479;&#35745;&#27169;&#22411;&#36827;&#34892;&#39118;&#38505;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#24573;&#35270;&#20851;&#38190;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#39123;&#39118;&#27169;&#24335;&#65292;&#24182;&#21463;&#21040;&#25968;&#25454;&#31232;&#32570;&#30340;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#23558;ARIMA&#27169;&#22411;&#21644;K-MEANS&#30456;&#32467;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#39123;&#39118;&#36235;&#21183;&#65292;&#24182;&#20351;&#29992;Autoencoder&#36827;&#34892;&#25913;&#36827;&#30340;&#39123;&#39118;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#28151;&#21512;&#26041;&#27861;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#21382;&#21490;&#39123;&#39118;&#34892;&#20026;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#28508;&#22312;&#26410;&#26469;&#36335;&#24452;&#21644;&#24378;&#24230;&#30340;&#35814;&#32454;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#20840;&#38754;&#32780;&#26377;&#36873;&#25321;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#25311;&#20016;&#23500;&#20102;&#23545;&#39123;&#39118;&#27169;&#24335;&#30340;&#24403;&#21069;&#29702;&#35299;&#65292;&#24182;&#20026;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hurricanes present major challenges in the U.S. due to their devastating impacts. Mitigating these risks is important, and the insurance industry is central in this effort, using intricate statistical models for risk assessment. However, these models often neglect key temporal and spatial hurricane patterns and are limited by data scarcity. This study introduces a refined approach combining the ARIMA model and K-MEANS to better capture hurricane trends, and an Autoencoder for enhanced hurricane simulations. Our experiments show that this hybrid methodology effectively simulate historical hurricane behaviors while providing detailed projections of potential future trajectories and intensities. Moreover, by leveraging a comprehensive yet selective dataset, our simulations enrich the current understanding of hurricane patterns and offer actionable insights for risk management strategies.
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#26469;&#23545;&#28145;&#23545;&#27969;&#20912;&#39118;&#26292;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#30340;&#21069;&#30651;&#36752;&#23556;&#35745;&#25968;&#25454;&#21644;&#8220;&#31185;&#23398;&#8221;&#38544;&#34255;&#21464;&#37327;&#30340;&#22320;&#29699;&#22823;&#27668;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#26469;&#35757;&#32451;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#31185;&#23398;&#38544;&#34255;&#21464;&#37327;&#36827;&#34892;K&#22343;&#20540;&#32858;&#31867;&#26469;&#29983;&#25104;&#33258;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2309.07173</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#36827;&#34892;&#28145;&#23545;&#27969;&#20912;&#39118;&#26292;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Using Unsupervised and Supervised Learning and Digital Twin for Deep Convective Ice Storm Classification. (arXiv:2309.07173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07173
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#26469;&#23545;&#28145;&#23545;&#27969;&#20912;&#39118;&#26292;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#30340;&#21069;&#30651;&#36752;&#23556;&#35745;&#25968;&#25454;&#21644;&#8220;&#31185;&#23398;&#8221;&#38544;&#34255;&#21464;&#37327;&#30340;&#22320;&#29699;&#22823;&#27668;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#26469;&#35757;&#32451;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#31185;&#23398;&#38544;&#34255;&#21464;&#37327;&#36827;&#34892;K&#22343;&#20540;&#32858;&#31867;&#26469;&#29983;&#25104;&#33258;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20912;&#20113;&#24863;&#30693;&#65288;SMICES&#65289;&#26159;&#19968;&#20010;&#23567;&#21355;&#26143;&#27010;&#24565;&#65292;&#20854;&#20013;&#20027;&#38647;&#36798;&#26681;&#25454;&#21069;&#30651;&#36752;&#23556;&#35745;&#25910;&#38598;&#30340;&#20449;&#24687;&#26234;&#33021;&#22320;&#38024;&#23545;&#20912;&#39118;&#26292;&#30446;&#26631;&#12290;&#20934;&#30830;&#35782;&#21035;&#38647;&#36798;&#35745;&#25910;&#38598;&#30340;&#20843;&#20010;&#27874;&#27573;&#30340;&#20113;&#26292;&#31867;&#22411;&#23545;&#20110;&#26234;&#33021;&#30446;&#26631;&#36873;&#25321;&#38750;&#24120;&#20851;&#38190;&#12290;&#24863;&#20852;&#36259;&#30340;&#20113;&#26292;&#31867;&#22411;&#21253;&#25324;&#65306;&#26228;&#31354;&#12289;&#34180;&#38632;&#23618;&#20113;&#12289;&#23618;&#20113;&#12289;&#22810;&#38632;&#39030;&#21644;&#23545;&#27969;&#26680;&#24515;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22320;&#29699;&#22823;&#27668;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#36827;&#34892;&#36825;&#31181;&#20998;&#31867;&#22120;&#25512;&#23548;&#30340;&#22810;&#27493;&#39588;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#21517;&#20026;&#22825;&#27668;&#30740;&#31350;&#39044;&#25253;&#65288;WRF&#65289;&#30340;&#22320;&#29699;&#22823;&#27668;&#25968;&#23383;&#23402;&#29983;&#29983;&#25104;&#27169;&#25311;&#30340;&#21069;&#30651;&#36752;&#23556;&#35745;&#25968;&#25454;&#20197;&#21450;&#26356;&#28145;&#23618;&#30340;&#8220;&#31185;&#23398;&#8221;&#38544;&#34255;&#21464;&#37327;&#12290;&#25968;&#25454;&#38598;&#27169;&#25311;&#20102;&#21152;&#21202;&#27604;&#22320;&#21306;&#30340;&#28909;&#24102;&#22320;&#21306;&#21644;&#32654;&#22269;&#22823;&#35199;&#27915;&#27839;&#23736;&#30340;&#38750;&#28909;&#24102;&#22320;&#21306;&#12290;&#20154;&#31867;&#19987;&#23478;&#21033;&#29992;&#31185;&#23398;&#38544;&#34255;&#21464;&#37327;&#19978;&#30340;K&#22343;&#20540;&#32858;&#31867;&#29983;&#25104;&#25968;&#25454;&#30340;&#33258;&#21160;&#26631;&#35760;-&#23558;&#27599;&#20010;&#25968;&#25454;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#20113;&#26292;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart Ice Cloud Sensing (SMICES) is a small-sat concept in which a primary radar intelligently targets ice storms based on information collected by a lookahead radiometer. Critical to the intelligent targeting is accurate identification of storm/cloud types from eight bands of radiance collected by the radiometer. The cloud types of interest are: clear sky, thin cirrus, cirrus, rainy anvil, and convection core.  We describe multi-step use of Machine Learning and Digital Twin of the Earth's atmosphere to derive such a classifier. First, a digital twin of Earth's atmosphere called a Weather Research Forecast (WRF) is used generate simulated lookahead radiometer data as well as deeper "science" hidden variables. The datasets simulate a tropical region over the Caribbean and a non-tropical region over the Atlantic coast of the United States. A K-means clustering over the scientific hidden variables was utilized by human experts to generate an automatic labelling of the data - mapping each 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23545;&#40784;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#26377;&#28508;&#21147;&#22312;&#35880;&#24910;&#30340;&#26694;&#26550;&#21644;&#25552;&#31034;&#35774;&#35745;&#19979;&#36229;&#36234;&#29616;&#26377;&#30340;&#26412;&#20307;&#23545;&#40784;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.07172</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23545;&#40784;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Ontology Alignment. (arXiv:2309.07172v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23545;&#40784;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#26377;&#28508;&#21147;&#22312;&#35880;&#24910;&#30340;&#26694;&#26550;&#21644;&#25552;&#31034;&#35774;&#35745;&#19979;&#36229;&#36234;&#29616;&#26377;&#30340;&#26412;&#20307;&#23545;&#40784;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65288;&#22914;GPT&#31995;&#21015;&#21644;Flan-T5&#65289;&#22312;&#26412;&#20307;&#23545;&#40784;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#29992;&#20110;&#35782;&#21035;&#26412;&#20307;&#20043;&#38388;&#30340;&#27010;&#24565;&#31561;&#20215;&#26144;&#23556;&#12290;&#20026;&#20102;&#27979;&#35797;Flan-T5-XXL&#21644;GPT-3.5-turbo&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;OAEI Bio-ML track&#30340;&#20004;&#20010;&#31561;&#20215;&#21305;&#37197;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23376;&#38598;&#65292;&#24182;&#32771;&#34385;&#21040;&#27010;&#24565;&#26631;&#31614;&#21644;&#32467;&#26500;&#19978;&#19979;&#25991;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#26377;&#21487;&#33021;&#22312;&#35880;&#24910;&#30340;&#26694;&#26550;&#21644;&#25552;&#31034;&#35774;&#35745;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#29616;&#26377;&#30340;&#26412;&#20307;&#23545;&#40784;&#31995;&#32479;&#22914;BERTMap&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the applicability of recent generative Large Language Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for identifying concept equivalence mappings across ontologies. To test the zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking into account concept labels and structural contexts. Preliminary findings suggest that LLMs have the potential to outperform existing ontology alignment systems like BERTMap, given careful framework and prompt design.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20351;&#29992;&#21644;HAR&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;HAR&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.07170</link><description>&lt;p&gt;
&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Overview of Human Activity Recognition Using Sensor Data. (arXiv:2309.07170v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20351;&#29992;&#21644;HAR&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;HAR&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22312;&#23478;&#24237;&#33258;&#21160;&#21270;&#12289;&#24037;&#20316;&#22330;&#25152;&#33258;&#21160;&#21270;&#12289;&#23433;&#20840;&#30417;&#25511;&#20197;&#21450;&#21307;&#30103;&#31561;&#22810;&#20010;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20174;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21040;&#26368;&#26032;&#21457;&#23637;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#29289;&#32852;&#32593;&#65292;HAR&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36129;&#29486;&#12290;&#23613;&#31649;&#24050;&#32463;&#21457;&#34920;&#20102;&#20960;&#31687;&#32508;&#36848;&#21644;&#35843;&#30740;&#30740;&#31350;&#65292;&#20294;&#32570;&#20047;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;HAR&#32508;&#36848;&#30740;&#31350;&#65292;&#37325;&#28857;&#24635;&#32467;&#20102;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#21644;&#26234;&#33021;&#23478;&#23621;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20351;&#29992;&#20197;&#21450;HAR&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24635;&#35272;&#20102;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;HAR&#65292;&#35752;&#35770;&#20102;&#20960;&#20010;&#20381;&#36182;&#20110;HAR&#30340;&#37325;&#35201;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#24050;&#32463;&#29992;&#20110;HAR&#30340;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20960;&#20010;HAR&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#24212;&#35813;&#34987;&#35299;&#20915;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;HAR&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) is an essential research field that has been used in different applications including home and workplace automation, security and surveillance as well as healthcare. Starting from conventional machine learning methods to the recently developing deep learning techniques and the Internet of things, significant contributions have been shown in the HAR area in the last decade. Even though several review and survey studies have been published, there is a lack of sensor-based HAR overview studies focusing on summarising the usage of wearable sensors and smart home sensors data as well as applications of HAR and deep learning techniques. Hence, we overview sensor-based HAR, discuss several important applications that rely on HAR, and highlight the most common machine learning methods that have been used for HAR. Finally, several challenges of HAR are explored that should be addressed to further improve the robustness of HAR.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25299;&#25169;&#20449;&#21495;&#22788;&#29702;&#20013;&#22797;&#21512;&#23376;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#26500;&#36896;&#36793;&#38469;&#22797;&#21512;&#23376;&#21644;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#65292;&#30740;&#31350;&#20854;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#35777;&#26126;&#20102;&#22797;&#21512;&#23376;&#25910;&#25947;&#26102;&#23545;&#24212;&#30340;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#20250;&#25910;&#25947;&#21040;&#26497;&#38480;&#22797;&#21512;&#23376;&#30340;&#29305;&#24449;&#20540;&#12290;&#36825;&#20123;&#32467;&#26524;&#25299;&#23637;&#20102;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.07169</link><description>&lt;p&gt;
&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#30340;&#39057;&#29575;&#25910;&#25947;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Frequency Convergence of Complexon Shift Operators. (arXiv:2309.07169v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25299;&#25169;&#20449;&#21495;&#22788;&#29702;&#20013;&#22797;&#21512;&#23376;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#26500;&#36896;&#36793;&#38469;&#22797;&#21512;&#23376;&#21644;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#65292;&#30740;&#31350;&#20854;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#35777;&#26126;&#20102;&#22797;&#21512;&#23376;&#25910;&#25947;&#26102;&#23545;&#24212;&#30340;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;&#30340;&#29305;&#24449;&#20540;&#20250;&#25910;&#25947;&#21040;&#26497;&#38480;&#22797;&#21512;&#23376;&#30340;&#29305;&#24449;&#20540;&#12290;&#36825;&#20123;&#32467;&#26524;&#25299;&#23637;&#20102;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#20449;&#21495;&#22788;&#29702;(TSP)&#21033;&#29992;&#21333;&#32431;&#24418;&#22797;&#21512;&#26469;&#24314;&#27169;&#27604;&#39030;&#28857;&#21644;&#36793;&#26356;&#39640;&#38454;&#30340;&#32467;&#26500;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;TSP&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#22797;&#21512;&#23376;&#30340;&#24191;&#20041;&#39640;&#38454;&#22270;&#30340;&#29256;&#26412;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#22797;&#21512;&#23376;&#30340;&#27010;&#24565;&#65292;&#21363;&#21333;&#32431;&#24418;&#22797;&#21512;&#24207;&#21015;&#30340;&#26497;&#38480;[1]&#12290;&#21463;&#22270;&#31227;&#20301;&#31639;&#23376;&#30340;&#31215;&#20998;&#31639;&#23376;&#24418;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26681;&#25454;&#22797;&#21512;&#23376;&#30340;&#25152;&#26377;&#21487;&#33021;&#23610;&#23544;&#30340;&#32452;&#20214;&#26500;&#36896;&#20102;&#36793;&#38469;&#22797;&#21512;&#23376;&#21644;&#22797;&#21512;&#31227;&#20301;&#31639;&#23376;(CSO)&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;CSO&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#21521;&#37327;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#19968;&#31867;&#26032;&#30340;&#21152;&#26435;&#37051;&#25509;&#30697;&#38453;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#19968;&#20010;&#21333;&#32431;&#24418;&#22797;&#21512;&#24207;&#21015;&#25910;&#25947;&#21040;&#19968;&#20010;&#22797;&#21512;&#23376;&#26102;&#65292;&#30456;&#24212;&#30340;CSO&#30340;&#29305;&#24449;&#20540;&#25910;&#25947;&#21040;&#26497;&#38480;&#22797;&#21512;&#23376;&#30340;&#29305;&#24449;&#20540;&#12290;&#36825;&#20123;&#32467;&#26524;&#26263;&#31034;&#20102;&#22312;&#22823;&#22411;&#21333;&#32431;&#24418;&#22797;&#21512;&#25110;&#21333;&#32431;&#24418;&#22797;&#21512;&#24207;&#21015;&#19978;&#30340;&#23398;&#20064;&#21487;&#36716;&#31227;&#24615;&#65292;&#20174;&#32780;&#25512;&#24191;&#20102;&#22270;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topological signal processing (TSP) utilizes simplicial complexes to model structures with higher order than vertices and edges. In this paper, we study the transferability of TSP via a generalized higher-order version of graphon, known as complexon. We recall the notion of a complexon as the limit of a simplicial complex sequence [1]. Inspired by the integral operator form of graphon shift operators, we construct a marginal complexon and complexon shift operator (CSO) according to components of all possible dimensions from the complexon. We investigate the CSO's eigenvalues and eigenvectors, and relate them to a new family of weighted adjacency matrices. We prove that when a simplicial complex sequence converges to a complexon, the eigenvalues of the corresponding CSOs converge to that of the limit complexon. These results hint at learning transferability on large simplicial complexes or simplicial complex sequences, which generalize the graphon signal processing framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#30446;&#26631;&#31354;&#38388;&#36827;&#34892;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20855;&#26377;&#30456;&#20284;&#20219;&#21153;&#35282;&#33394;&#30340;&#29615;&#22659;&#29366;&#24577;&#38598;&#21512;&#22312;&#19968;&#36215;&#30340;&#32039;&#23494;&#20851;&#32852;&#34920;&#31034;&#26469;&#21457;&#29616;&#23376;&#30446;&#26631;&#65292;&#22312;&#23548;&#33322;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21487;&#35299;&#37322;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07168</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#30446;&#26631;&#31354;&#38388;&#36827;&#34892;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis. (arXiv:2309.07168v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#22312;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#20013;&#23545;&#30446;&#26631;&#31354;&#38388;&#36827;&#34892;&#25277;&#35937;&#30340;&#26041;&#27861;&#65292;&#20197;&#33258;&#21160;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20855;&#26377;&#30456;&#20284;&#20219;&#21153;&#35282;&#33394;&#30340;&#29615;&#22659;&#29366;&#24577;&#38598;&#21512;&#22312;&#19968;&#36215;&#30340;&#32039;&#23494;&#20851;&#32852;&#34920;&#31034;&#26469;&#21457;&#29616;&#23376;&#30446;&#26631;&#65292;&#22312;&#23548;&#33322;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21487;&#35299;&#37322;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#23398;&#20064;&#26497;&#22823;&#22320;&#21463;&#30410;&#20110;&#20351;&#29992;&#31526;&#21495;&#26041;&#27861;&#36827;&#34892;&#30446;&#26631;&#34920;&#31034;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#32467;&#26500;&#21270;&#20026;&#39640;&#25928;&#19988;&#26131;&#20110;&#36801;&#31227;&#30340;&#23398;&#20064;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20381;&#36182;&#20110;&#31526;&#21495;&#25512;&#29702;&#30340;&#29616;&#26377;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#21463;&#38480;&#20110;&#25163;&#21160;&#30446;&#26631;&#34920;&#31034;&#12290;&#33258;&#20027;&#21457;&#29616;&#31526;&#21495;&#30446;&#26631;&#34920;&#31034;&#30340;&#25361;&#25112;&#22312;&#20110;&#23427;&#24517;&#39035;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#65292;&#22914;&#29615;&#22659;&#21160;&#21147;&#23398;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32039;&#23494;&#20851;&#32852;&#65288;&#21363;&#23558;&#20855;&#26377;&#31867;&#20284;&#20219;&#21153;&#35282;&#33394;&#30340;&#29615;&#22659;&#29366;&#24577;&#38598;&#21512;&#22312;&#19968;&#36215;&#65289;&#30340;&#26032;&#20852;&#34920;&#31034;&#21457;&#29616;&#23376;&#30446;&#26631;&#30340;&#21457;&#23637;&#26426;&#21046;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#36880;&#28176;&#23398;&#20064;&#27492;&#34920;&#31034;&#20197;&#21450;&#31574;&#30053;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#23548;&#33322;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#23398;&#21040;&#30340;&#34920;&#31034;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#24182;&#23454;&#29616;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-ended learning benefits immensely from the use of symbolic methods for goal representation as they offer ways to structure knowledge for efficient and transferable learning. However, the existing Hierarchical Reinforcement Learning (HRL) approaches relying on symbolic reasoning are often limited as they require a manual goal representation. The challenge in autonomously discovering a symbolic goal representation is that it must preserve critical information, such as the environment dynamics. In this work, we propose a developmental mechanism for subgoal discovery via an emergent representation that abstracts (i.e., groups together) sets of environment states that have similar roles in the task. We create a HRL algorithm that gradually learns this representation along with the policies and evaluate it on navigation tasks to show the learned representation is interpretable and results in data efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20110;EEG&#30340;&#35748;&#30693;&#36127;&#33655;&#20272;&#35745;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#21457;&#29616;DNN&#22312;&#31163;&#32447;&#21644;&#23454;&#26102;&#20998;&#31867;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30740;&#31350;&#20013;&#36739;&#23569;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.07163</link><description>&lt;p&gt;
EEG&#22522;&#20110;&#35748;&#30693;&#36127;&#33655;&#26816;&#27979;&#30340;&#23454;&#39564;&#33539;&#24335;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Systematic Review of Experimental Paradigms and Deep Neural Networks for Electroencephalography-Based Cognitive Workload Detection. (arXiv:2309.07163v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;EEG&#30340;&#35748;&#30693;&#36127;&#33655;&#20272;&#35745;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#21457;&#29616;DNN&#22312;&#31163;&#32447;&#21644;&#23454;&#26102;&#20998;&#31867;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30740;&#31350;&#20013;&#36739;&#23569;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#33041;&#30005;&#22270;(EEG)&#30340;&#35748;&#30693;&#36127;&#33655;(CWL)&#20272;&#35745;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#12290;&#25991;&#31456;&#30340;&#37325;&#28857;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#35782;&#21035;&#29992;&#20110;&#21487;&#38752;&#22320;&#24341;&#21457;&#31163;&#25955;&#21644;&#21487;&#37327;&#21270;&#30340;&#35748;&#30693;&#36127;&#33655;&#27700;&#24179;&#30340;&#19981;&#21516;&#23454;&#39564;&#33539;&#24335;&#65292;&#20197;&#21450;&#29992;&#20110;&#20449;&#21495;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#20013;&#24120;&#29992;&#36755;&#20837;&#24418;&#24335;&#30340;&#29305;&#23450;&#24615;&#36136;&#21644;&#34920;&#24449;&#32467;&#26500;&#12290;&#20998;&#26512;&#25581;&#31034;&#20102;&#35768;&#22810;&#30740;&#31350;&#20351;&#29992;&#21407;&#29983;&#34920;&#31034;&#30340;EEG&#20449;&#21495;&#30340;&#20108;&#32500;&#30697;&#38453;&#36827;&#34892;&#31163;&#32447;CW&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#37319;&#29992;&#20102;&#29992;&#20110;&#23454;&#26102;CWL&#20272;&#35745;&#30340;&#22312;&#32447;&#25110;&#20266;&#22312;&#32447;&#20998;&#31867;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20221;&#32508;&#36848;&#20013;&#65292;&#21482;&#26377;&#20960;&#20010;&#21487;&#35299;&#37322;&#30340;DNN&#21644;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#34987;&#29992;&#20110;&#35748;&#30693;&#36127;&#33655;&#26816;&#27979;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;DNN&#35777;&#26126;&#26159;&#29992;&#20110;&#20998;&#31867;EEG&#20449;&#21495;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article summarizes a systematic review of the electroencephalography (EEG)-based cognitive workload (CWL) estimation. The focus of the article is twofold: identify the disparate experimental paradigms used for reliably eliciting discreet and quantifiable levels of cognitive load and the specific nature and representational structure of the commonly used input formulations in deep neural networks (DNNs) used for signal classification. The analysis revealed a number of studies using EEG signals in its native representation of a two-dimensional matrix for offline classification of CWL. However, only a few studies adopted an online or pseudo-online classification strategy for real-time CWL estimation. Further, only a couple of interpretable DNNs and a single generative model were employed for cognitive load detection till date during this review. More often than not, researchers were using DNNs as black-box type models. In conclusion, DNNs prove to be valuable tools for classifying EE
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#19988;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#32447;EEG-SimpleConv&#65292;&#29992;&#20110;BCI&#20013;&#30340;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;EEG-SimpleConv&#34920;&#29616;&#33267;&#23569;&#21516;&#26679;&#22909;&#25110;&#26356;&#39640;&#25928;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#65292;&#25512;&#29702;&#26102;&#38388;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.07159</link><description>&lt;p&gt;
&#19968;&#31181;&#24378;&#22823;&#19988;&#31616;&#21333;&#30340;BCI MI&#35299;&#30721;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
A Strong and Simple Deep Learning Baseline for BCI MI Decoding. (arXiv:2309.07159v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#19988;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#32447;EEG-SimpleConv&#65292;&#29992;&#20110;BCI&#20013;&#30340;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;EEG-SimpleConv&#34920;&#29616;&#33267;&#23569;&#21516;&#26679;&#22909;&#25110;&#26356;&#39640;&#25928;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#65292;&#25512;&#29702;&#26102;&#38388;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EEG-SimpleConv&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;BCI&#20013;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#30340;&#30452;&#35266;&#30340;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#25552;&#20986;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#22522;&#32447;&#26469;&#36827;&#34892;&#27604;&#36739;&#65292;&#20165;&#20351;&#29992;&#25991;&#29486;&#20013;&#38750;&#24120;&#26631;&#20934;&#30340;&#20803;&#32032;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;EEG&#36816;&#21160;&#24819;&#35937;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20854;&#24615;&#33021;&#65292;&#21253;&#25324;&#27169;&#25311;&#22312;&#32447;&#35774;&#32622;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;EEG-SimpleConv&#33267;&#23569;&#19982;&#20854;&#20182;&#26041;&#27861;&#19968;&#26679;&#22909;&#29978;&#33267;&#26356;&#39640;&#25928;&#65292;&#22312;&#20027;&#20307;&#38388;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#30693;&#35782;&#20256;&#36882;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20302;&#25512;&#29702;&#26102;&#38388;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#20027;&#24352;&#20351;&#29992;&#29616;&#25104;&#30340;&#20803;&#32032;&#32780;&#19981;&#26159;&#25552;&#20986;&#29305;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#24110;&#21161;BCI&#20013;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#37319;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#21644;&#23454;&#39564;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose EEG-SimpleConv, a straightforward 1D convolutional neural network for Motor Imagery decoding in BCI. Our main motivation is to propose a very simple baseline to compare to, using only very standard ingredients from the literature. We evaluate its performance on four EEG Motor Imagery datasets, including simulated online setups, and compare it to recent Deep Learning and Machine Learning approaches. EEG-SimpleConv is at least as good or far more efficient than other approaches, showing strong knowledge-transfer capabilities across subjects, at the cost of a low inference time. We advocate that using off-the-shelf ingredients rather than coming with ad-hoc solutions can significantly help the adoption of Deep Learning approaches for BCI. We make the code of the models and the experiments accessible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;AI&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20351;&#29992;&#36739;&#20302;&#31934;&#24230;&#36827;&#34892;&#35757;&#32451;&#65292;&#25506;&#35752;&#20102;16&#20301;&#21644;8&#20301;&#21387;&#32553;&#26684;&#24335;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#22312;&#35745;&#31639;&#20043;&#21069;&#35299;&#21387;&#21387;&#32553;&#25805;&#20316;&#25968;&#65292;&#21487;&#20197;&#25552;&#39640;&#24102;&#23485;&#20351;&#29992;&#21644;&#32531;&#23384;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07158</link><description>&lt;p&gt;
AI&#20013;&#30340;&#21387;&#32553;&#23454;&#25968;&#65306;&#20351;&#29992;RISC-V CPU&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Compressed Real Numbers for AI: a case-study using a RISC-V CPU. (arXiv:2309.07158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;AI&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20351;&#29992;&#36739;&#20302;&#31934;&#24230;&#36827;&#34892;&#35757;&#32451;&#65292;&#25506;&#35752;&#20102;16&#20301;&#21644;8&#20301;&#21387;&#32553;&#26684;&#24335;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#22312;&#35745;&#31639;&#20043;&#21069;&#35299;&#21387;&#21387;&#32553;&#25805;&#20316;&#25968;&#65292;&#21487;&#20197;&#25552;&#39640;&#24102;&#23485;&#20351;&#29992;&#21644;&#32531;&#23384;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#22914;&#26368;&#36817;&#30340;&#30740;&#31350;&#25152;&#31034;&#65292;&#36890;&#24120;&#20351;&#29992;&#21333;&#31934;&#24230;IEEE 754&#28014;&#28857;&#25968;&#65288;&#20108;&#36827;&#21046;32&#20301;&#65289;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20063;&#21487;&#20197;&#20351;&#29992;&#36739;&#20302;&#31934;&#24230;&#12290;&#22240;&#27492;&#65292;16&#20301;&#21644;8&#20301;&#30340;&#21387;&#32553;&#26684;&#24335;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#20004;&#20010;&#24050;&#32463;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#21387;&#32553;&#20108;&#36827;&#21046;32&#20301;&#25968;&#21462;&#24471;&#26377;&#36259;&#32467;&#26524;&#30340;&#26684;&#24335;&#26063;&#65306;bfloat&#21644;posit&#12290;&#23613;&#31649;16&#20301;&#21644;8&#20301;&#30340;bfloat/posit&#24120;&#29992;&#20110;&#20943;&#23569;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;DNN&#30340;&#26435;&#37325;/&#20559;&#24046;&#30340;&#23384;&#20648;&#65292;&#20294;&#25512;&#29702;&#20173;&#28982;&#32463;&#24120;&#22312;CPU&#30340;32&#20301;FPU&#19978;&#36827;&#34892;&#65288;&#23588;&#20854;&#26159;&#22914;&#26524;&#27809;&#26377;GPU&#21487;&#29992;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35745;&#31639;&#20043;&#21069;&#35299;&#21387;bfloat/posit&#24352;&#37327;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#21387;&#32553;&#25805;&#20316;&#25968;&#21152;&#36733;&#21040;&#25903;&#25345;&#21521;&#37327;&#30340;&#21521;&#37327;&#23492;&#23384;&#22120;&#20043;&#21518;&#65292;&#20197;&#33410;&#30465;&#24102;&#23485;&#20351;&#29992;&#21644;&#25552;&#39640;&#32531;&#23384;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
As recently demonstrated, Deep Neural Networks (DNN), usually trained using single precision IEEE 754 floating point numbers (binary32), can also work using lower precision. Therefore, 16-bit and 8-bit compressed format have attracted considerable attention. In this paper, we focused on two families of formats that have already achieved interesting results in compressing binary32 numbers in machine learning applications, without sensible degradation of the accuracy: bfloat and posit. Even if 16-bit and 8-bit bfloat/posit are routinely used for reducing the storage of the weights/biases of trained DNNs, the inference still often happens on the 32-bit FPU of the CPU (especially if GPUs are not available). In this paper we propose a way to decompress a tensor of bfloat/posits just before computations, i.e., after the compressed operands have been loaded within the vector registers of a vector capable CPU, in order to save bandwidth usage and increase cache efficiency. Finally, we show the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#32780;&#31283;&#20581;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#21482;&#21033;&#29992;&#30005;&#21387;&#24133;&#20540;&#65292;&#26080;&#38656;&#30456;&#35282;&#25110;&#21151;&#29575;&#27969;&#25968;&#25454;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#23398;&#20064;&#20013;&#26029;&#21518;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;Bregman&#25955;&#24230;&#32422;&#26463;&#26469;&#35299;&#20915;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#21487;&#20197;&#23398;&#20064;&#21040;&#26368;&#20248;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.07157</link><description>&lt;p&gt;
&#26410;&#30693;&#27169;&#24335;&#19979;&#30340;&#37197;&#30005;&#32593;&#32447;&#36335;&#20013;&#26029;&#35782;&#21035;&#19982;&#24615;&#33021;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Distribution Grid Line Outage Identification with Unknown Pattern and Performance Guarantee. (arXiv:2309.07157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#32780;&#31283;&#20581;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#21482;&#21033;&#29992;&#30005;&#21387;&#24133;&#20540;&#65292;&#26080;&#38656;&#30456;&#35282;&#25110;&#21151;&#29575;&#27969;&#25968;&#25454;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#23398;&#20064;&#20013;&#26029;&#21518;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;Bregman&#25955;&#24230;&#32422;&#26463;&#26469;&#35299;&#20915;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#21487;&#20197;&#23398;&#20064;&#21040;&#26368;&#20248;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37197;&#30005;&#32593;&#32447;&#36335;&#20013;&#26029;&#35782;&#21035;&#23545;&#20110;&#21487;&#25345;&#32493;&#30340;&#30005;&#32593;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#32780;&#31283;&#20581;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#21482;&#21033;&#29992;&#21487;&#29992;&#30340;&#30005;&#21387;&#24133;&#20540;&#65292;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#30456;&#35282;&#25110;&#21151;&#29575;&#27969;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#38024;&#23545;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#22522;&#20110;&#21464;&#28857;&#26816;&#27979;&#30340;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#23545;&#20013;&#26029;&#27169;&#24335;&#36827;&#34892;&#20808;&#39564;&#30693;&#35782;&#65292;&#32780;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20013;&#26029;&#22330;&#26223;&#21364;&#26159;&#26410;&#30693;&#30340;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#31181;&#19981;&#20999;&#23454;&#38469;&#30340;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26469;&#23398;&#20064;&#20013;&#26029;&#21518;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#23384;&#22312;&#21487;&#34892;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#28155;&#21152;Bregman&#25955;&#24230;&#32422;&#26463;&#26469;&#20462;&#25913;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#25511;&#21046;&#21442;&#25968;&#26356;&#26032;&#30340;&#36712;&#36857;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#21487;&#34892;&#24615;&#38382;&#39064;&#12290;&#22914;&#20170;&#65292;&#21450;&#26102;&#25805;&#20316;&#26159;&#20851;&#38190;&#65292;&#25105;&#20204;&#35777;&#26126;&#36890;&#36807;&#21033;&#29992;&#25910;&#25947;&#20445;&#35777;&#21487;&#20197;&#23398;&#20064;&#21040;&#26368;&#20248;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Line outage identification in distribution grids is essential for sustainable grid operation. In this work, we propose a practical yet robust detection approach that utilizes only readily available voltage magnitudes, eliminating the need for costly phase angles or power flow data. Given the sensor data, many existing detection methods based on change-point detection require prior knowledge of outage patterns, which are unknown for real-world outage scenarios. To remove this impractical requirement, we propose a data-driven method to learn the parameters of the post-outage distribution through gradient descent. However, directly using gradient descent presents feasibility issues. To address this, we modify our approach by adding a Bregman divergence constraint to control the trajectory of the parameter updates, which eliminates the feasibility problems. As timely operation is the key nowadays, we prove that the optimal parameters can be learned with convergence guarantees via leveragin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;SE-Resnet-Bi-LSTM&#26550;&#26500;&#36827;&#34892;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#22411;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#21644;&#23439;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.07156</link><description>&lt;p&gt;
&#28145;&#20837;&#30740;&#31350;&#30561;&#30496;&#65306;&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#19982;&#27169;&#22411;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability. (arXiv:2309.07156v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;SE-Resnet-Bi-LSTM&#26550;&#26500;&#36827;&#34892;&#30561;&#30496;&#38454;&#27573;&#30340;&#20998;&#31867;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#22411;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#21644;&#23439;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30561;&#30496;&#26159;&#19968;&#31181;&#22522;&#26412;&#30340;&#29983;&#29702;&#36807;&#31243;&#65292;&#22312;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#21344;&#25454;&#30528;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#20934;&#30830;&#20998;&#31867;&#30561;&#30496;&#38454;&#27573;&#26159;&#35780;&#20272;&#30561;&#30496;&#36136;&#37327;&#21644;&#35782;&#21035;&#21487;&#33021;&#30340;&#30561;&#30496;&#38556;&#30861;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;SE-Resnet-Bi-LSTM&#26550;&#26500;&#23558;&#30561;&#30496;&#20998;&#31867;&#20026;&#20116;&#20010;&#19981;&#21516;&#30340;&#38454;&#27573;&#12290;&#20998;&#31867;&#36807;&#31243;&#22522;&#20110;&#23545;&#21333;&#36890;&#36947;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#20998;&#26512;&#12290;&#24314;&#35758;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#22522;&#26412;&#20803;&#32032;&#32452;&#25104;&#65306;&#21033;&#29992;SE-ResNet&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#21033;&#29992;Bi-LSTM&#21333;&#20803;&#22534;&#26632;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#35780;&#20272;&#24471;&#21040;&#35777;&#23454;&#65292;&#20998;&#21035;&#26159;SLeepEDF-20&#12289;SleepEDF-78&#21644;SHHS&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#29575;&#65292;&#20998;&#21035;&#20026;87.5&#65285;&#12289;83.9&#65285;&#21644;87.8&#65285;&#65292;&#24182;&#19988;&#22312;&#23439;F1&#24471;&#20998;&#26041;&#38754;&#20998;&#21035;&#20026;82.5&#12289;78.9&#21644;81.9&#12290;
&lt;/p&gt;
&lt;p&gt;
Sleep, a fundamental physiological process, occupies a significant portion of our lives. Accurate classification of sleep stages serves as a crucial tool for evaluating sleep quality and identifying probable sleep disorders. This work introduces a novel methodology that utilises a SE-Resnet-Bi-LSTM architecture to classify sleep into five separate stages. The classification process is based on the analysis of single-channel electroencephalograms (EEGs). The framework that has been suggested consists of two fundamental elements: a feature extractor that utilises SE-ResNet, and a temporal context encoder that use stacks of Bi-LSTM units.The effectiveness of our approach is substantiated by thorough assessments conducted on three different datasets, namely SLeepEDF-20, SleepEDF-78, and SHHS. Significantly, our methodology attains notable levels of accuracy, specifically 87.5\%, 83.9\%, and 87.8\%, along with macro-F1 scores of 82.5, 78.9, and 81.9 for the corresponding datasets. Notably, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#32534;&#30721;&#22120;&#12289;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#23547;&#25214;&#24433;&#21709;&#32773;&#30340;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26368;&#20339;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07153</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#23547;&#25214;&#24433;&#21709;&#32773;&#65306;&#19968;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finding Influencers in Complex Networks: An Effective Deep Reinforcement Learning Approach. (arXiv:2309.07153v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#32534;&#30721;&#22120;&#12289;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#32593;&#32476;&#20013;&#23547;&#25214;&#24433;&#21709;&#32773;&#30340;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26368;&#20339;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#20013;&#65292;&#26368;&#22823;&#21270;&#22797;&#26434;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#21147;&#26159;&#19968;&#20010;&#23454;&#38469;&#37325;&#35201;&#20294;&#35745;&#31639;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#20010;NP&#38590;&#38382;&#39064;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#36817;&#20284;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#24040;&#22823;&#30340;&#20154;&#24037;&#35774;&#35745;&#24037;&#20316;&#65292;&#35201;&#20040;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#20043;&#38388;&#26080;&#27861;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24179;&#34913;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#23581;&#35797;&#21482;&#27880;&#37325;&#36895;&#24230;&#65292;&#32780;&#32570;&#20047;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#20013;&#65292;&#19982;&#20197;&#21069;&#30340;&#23581;&#35797;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#22312;&#20256;&#32479;&#30340;&#26368;&#20339;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#31639;&#27861;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#32534;&#30721;&#22120;&#65292;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#35299;&#30721;&#22120;&#65292;&#21629;&#21517;&#20026;DREIM&#12290;&#36890;&#36807;&#22312;&#23567;&#35268;&#27169;&#21512;&#25104;&#22270;&#19978;&#36827;&#34892;&#22823;&#37327;&#35757;&#32451;&#65292;DREIM&#22312;&#38750;&#24120;&#22823;&#30340;&#21512;&#25104;&#32593;&#32476;&#21644;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#19978;&#22312;&#35299;&#20915;&#36136;&#37327;&#26041;&#38754;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximizing influences in complex networks is a practically important but computationally challenging task for social network analysis, due to its NPhard nature. Most current approximation or heuristic methods either require tremendous human design efforts or achieve unsatisfying balances between effectiveness and efficiency. Recent machine learning attempts only focus on speed but lack performance enhancement. In this paper, different from previous attempts, we propose an effective deep reinforcement learning model that achieves superior performances over traditional best influence maximization algorithms. Specifically, we design an end-to-end learning framework that combines graph neural network as the encoder and reinforcement learning as the decoder, named DREIM. Trough extensive training on small synthetic graphs, DREIM outperforms the state-of-the-art baseline methods on very large synthetic and real-world networks on solution quality, and we also empirically show its linear sca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#35299;&#30721;&#20154;&#33041;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;EEG&#25968;&#25454;&#36716;&#25442;&#20026;&#39057;&#35889;&#22270;&#24182;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#21512;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#22270;&#20687;&#20998;&#31867;&#25945;&#24072;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#37325;&#24314;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.07149</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21644;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#20174;&#33041;&#30005;&#22270;&#20013;&#35299;&#30721;&#35270;&#35273;&#33041;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models. (arXiv:2309.07149v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33041;&#30005;&#22270;&#25968;&#25454;&#35299;&#30721;&#20154;&#33041;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;EEG&#25968;&#25454;&#36716;&#25442;&#20026;&#39057;&#35889;&#22270;&#24182;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#21512;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#22270;&#20687;&#20998;&#31867;&#25945;&#24072;&#32593;&#32476;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#37325;&#24314;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#35299;&#30721;&#35270;&#35273;&#34920;&#31034;&#24050;&#25104;&#20026;&#19968;&#20010;&#34028;&#21187;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#33041;&#26426;&#25509;&#21475;&#30340;&#32972;&#26223;&#19979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#22270;&#20687;Net&#25968;&#25454;&#38598;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#26469;&#20998;&#31867;&#21644;&#37325;&#26500;&#22270;&#20687;&#65288;&#21363;&#8220;&#33041;&#35299;&#30721;&#8221;&#65289;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26469;&#33258;6&#21517;&#21442;&#19982;&#32773;&#30340;EEG&#35760;&#24405;&#65292;&#27599;&#21517;&#21442;&#19982;&#32773;&#35266;&#30475;&#20102;&#35206;&#30422;40&#20010;&#29420;&#29305;&#35821;&#20041;&#31867;&#21035;&#30340;50&#20010;&#22270;&#20687;&#12290;&#36825;&#20123;EEG&#35835;&#25968;&#34987;&#36716;&#25442;&#20026;&#39057;&#35889;&#22270;&#65292;&#28982;&#21518;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#65292;&#38598;&#25104;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#22270;&#20687;&#20998;&#31867;&#25945;&#24072;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;80%&#30340;&#21069;&#20116;&#20301;&#20934;&#30830;&#29575;&#65292;&#26174;&#33879;&#20248;&#20110;&#26631;&#20934;CNN&#21644;&#21508;&#31181;&#22522;&#20110;RNN&#30340;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22270;&#20687;&#37325;&#26500;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding visual representations from human brain activity has emerged as a thriving research domain, particularly in the context of brain-computer interfaces. Our study presents an innovative method that employs to classify and reconstruct images from the ImageNet dataset using electroencephalography (EEG) data from subjects that had viewed the images themselves (i.e. "brain decoding"). We analyzed EEG recordings from 6 participants, each exposed to 50 images spanning 40 unique semantic categories. These EEG readings were converted into spectrograms, which were then used to train a convolutional neural network (CNN), integrated with a knowledge distillation procedure based on a pre-trained Contrastive Language-Image Pre-Training (CLIP)-based image classification teacher network. This strategy allowed our model to attain a top-5 accuracy of 80%, significantly outperforming a standard CNN and various RNN-based benchmarks. Additionally, we incorporated an image reconstruction mechanism ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#22270;&#33258;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#38750;&#27431;&#20960;&#37324;&#24471;&#29305;&#24449;&#30340; EEG &#20449;&#21495;&#65292;&#24182;&#25552;&#39640;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07147</link><description>&lt;p&gt;
DGSD: &#22522;&#20110;&#21160;&#24577;&#22270;&#33258;&#33976;&#39311;&#30340;&#22522;&#20110;EEG&#30340;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DGSD: Dynamical Graph Self-Distillation for EEG-Based Auditory Spatial Attention Detection. (arXiv:2309.07147v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#22270;&#33258;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#38750;&#27431;&#20960;&#37324;&#24471;&#29305;&#24449;&#30340; EEG &#20449;&#21495;&#65292;&#24182;&#25552;&#39640;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#27880;&#24847;&#21147;&#26816;&#27979; (AAD) &#26088;&#22312;&#20174;&#22810;&#35828;&#35805;&#32773;&#29615;&#22659;&#20013;&#30340;&#33041;&#20449;&#21495;&#20013;&#26816;&#27979;&#30446;&#26631;&#35828;&#35805;&#32773;&#12290;&#23613;&#31649;&#22522;&#20110; EEG &#30340; AAD &#26041;&#27861;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20026;&#22788;&#29702;&#20687;&#22270;&#20687;&#36825;&#26679;&#30340;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#35774;&#35745;&#30340;&#20256;&#32479;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20351;&#24471;&#22788;&#29702;&#20855;&#26377;&#38750;&#27431;&#20960;&#37324;&#24471;&#29305;&#24449;&#30340; EEG &#20449;&#21495;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110; AAD &#30340;&#21160;&#24577;&#22270;&#33258;&#33976;&#39311; (DGSD) &#26041;&#27861;&#65292;&#20854;&#19981;&#38656;&#35201;&#35821;&#38899;&#21050;&#28608;&#20316;&#20026;&#36755;&#20837;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#26377;&#25928;&#34920;&#31034; EEG &#20449;&#21495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#29305;&#24615;&#65292;&#24212;&#29992;&#21160;&#24577;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#34920;&#31034; EEG &#20449;&#21495;&#30340;&#22270;&#32467;&#26500;&#65292;&#35813;&#32593;&#32476;&#36824;&#21487;&#20197;&#25552;&#21462;&#19982;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#30456;&#20851;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640; AAD &#26816;&#27979;&#24615;&#33021;&#65292;&#36827;&#34892;&#20102;&#33258;&#33976;&#39311;&#65292;&#21253;&#25324;&#29305;&#24449;&#33976;&#39311;&#21644;&#20998;&#23618;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auditory Attention Detection (AAD) aims to detect target speaker from brain signals in a multi-speaker environment. Although EEG-based AAD methods have shown promising results in recent years, current approaches primarily rely on traditional convolutional neural network designed for processing Euclidean data like images. This makes it challenging to handle EEG signals, which possess non-Euclidean characteristics. In order to address this problem, this paper proposes a dynamical graph self-distillation (DGSD) approach for AAD, which does not require speech stimuli as input. Specifically, to effectively represent the non-Euclidean properties of EEG signals, dynamical graph convolutional networks are applied to represent the graph structure of EEG signals, which can also extract crucial features related to auditory spatial attention in EEG signals. In addition, to further improve AAD detection performance, self-distillation, consisting of feature distillation and hierarchical distillation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ECG-Text&#39044;&#35757;&#32451;&#65288;ETP&#65289;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;ECG&#20449;&#21495;&#19982;&#25991;&#26412;&#25253;&#21578;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#36328;&#27169;&#24577;ECG&#29305;&#24449;&#23398;&#20064;&#12290;ETP&#22312;&#32447;&#24615;&#35780;&#20272;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#36328;&#27169;&#24577;ECG&#29305;&#24449;&#23398;&#20064;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07145</link><description>&lt;p&gt;
ETP: &#36890;&#36807;ECG-Text&#39044;&#35757;&#32451;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;ECG&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
ETP: Learning Transferable ECG Representations via ECG-Text Pre-training. (arXiv:2309.07145v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ECG-Text&#39044;&#35757;&#32451;&#65288;ETP&#65289;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#23558;ECG&#20449;&#21495;&#19982;&#25991;&#26412;&#25253;&#21578;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#36328;&#27169;&#24577;ECG&#29305;&#24449;&#23398;&#20064;&#12290;ETP&#22312;&#32447;&#24615;&#35780;&#20272;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#36328;&#27169;&#24577;ECG&#29305;&#24449;&#23398;&#20064;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24515;&#34880;&#31649;&#20445;&#20581;&#39046;&#22495;&#20013;&#65292;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20316;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;ECG&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#38656;&#35201;&#27880;&#37322;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#24494;&#35843;&#38454;&#27573;&#38590;&#20197;&#22788;&#29702;&#19981;&#23384;&#22312;&#30340;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ECG-Text&#39044;&#35757;&#32451;&#65288;ETP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#23558;ECG&#20449;&#21495;&#19982;&#25991;&#26412;&#25253;&#21578;&#32852;&#31995;&#36215;&#26469;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#39318;&#27425;&#22312;ECG&#39046;&#22495;&#20013;&#21033;&#29992;&#20102;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;ETP&#20351;&#29992;ECG&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#23558;ECG&#20449;&#21495;&#19982;&#20854;&#30456;&#24212;&#30340;&#25991;&#26412;&#25253;&#21578;&#23545;&#40784;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#32447;&#24615;&#35780;&#20272;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;PTB-XL&#21644;CPSC2018&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#40065;&#26834;&#19988;&#21487;&#36801;&#31227;&#30340;&#36328;&#27169;&#24577;ECG&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of cardiovascular healthcare, the Electrocardiogram (ECG) serves as a critical, non-invasive diagnostic tool. Although recent strides in self-supervised learning (SSL) have been promising for ECG representation learning, these techniques often require annotated samples and struggle with classes not present in the fine-tuning stages. To address these limitations, we introduce ECG-Text Pre-training (ETP), an innovative framework designed to learn cross-modal representations that link ECG signals with textual reports. For the first time, this framework leverages the zero-shot classification task in the ECG domain. ETP employs an ECG encoder along with a pre-trained language model to align ECG signals with their corresponding textual reports. The proposed framework excels in both linear evaluation and zero-shot classification tasks, as demonstrated on the PTB-XL and CPSC2018 datasets, showcasing its ability for robust and generalizable cross-modal ECG feature learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#21644;&#35780;&#20272;&#20050;&#20051;&#29699;&#36816;&#21160;&#21592;&#21160;&#20316;&#25216;&#33021;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25913;&#36827;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#24182;&#21033;&#29992;&#29305;&#24449;&#24037;&#31243;&#12289;&#38477;&#32500;&#21644;&#19981;&#21516;&#35780;&#20272;&#25351;&#26631;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#21160;&#20316;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#23618;&#27425;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.07141</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20050;&#20051;&#29699;&#36816;&#21160;&#21592;&#21160;&#20316;&#25216;&#33021;&#35782;&#21035;&#19982;&#35780;&#20272;&#31995;&#32479;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Design of Recognition and Evaluation System for Table Tennis Players' Motor Skills Based on Artificial Intelligence. (arXiv:2309.07141v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#21644;&#35780;&#20272;&#20050;&#20051;&#29699;&#36816;&#21160;&#21592;&#21160;&#20316;&#25216;&#33021;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#25913;&#36827;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#24182;&#21033;&#29992;&#29305;&#24449;&#24037;&#31243;&#12289;&#38477;&#32500;&#21644;&#19981;&#21516;&#35780;&#20272;&#25351;&#26631;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#20102;&#21160;&#20316;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#23618;&#27425;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#31185;&#23398;&#25216;&#26415;&#30340;&#39134;&#36895;&#21457;&#23637;&#65292;&#23545;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#30740;&#31350;&#19981;&#26029;&#26356;&#26032;&#65292;&#20294;&#30446;&#21069;&#26469;&#30475;&#65292;&#23545;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#35782;&#21035;&#21644;&#20998;&#26512;&#29305;&#23450;&#36816;&#21160;&#30340;&#33021;&#21147;&#36824;&#19981;&#22815;&#20840;&#38754;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25913;&#36827;&#20102;&#20050;&#20051;&#29699;&#36816;&#21160;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#20102;&#20050;&#20051;&#29699;&#36816;&#21160;&#21592;&#21160;&#20316;&#25216;&#33021;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35780;&#20272;&#12290;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#35774;&#22791;&#26469;&#25910;&#38598;&#20050;&#20051;&#29699;&#36816;&#21160;&#21592;&#30340;&#21160;&#20316;&#20449;&#24687;&#65292;&#24182;&#23545;&#23454;&#38469;&#36816;&#21160;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#12290;&#20854;&#27425;&#65292;&#21046;&#20316;&#20102;&#19968;&#20010;&#28369;&#21160;&#31383;&#21475;&#26469;&#23558;&#25910;&#38598;&#21040;&#30340;&#21160;&#20316;&#25968;&#25454;&#20998;&#21106;&#25104;&#20845;&#20010;&#20050;&#20051;&#29699;&#22522;&#20934;&#21160;&#20316;&#30340;&#29305;&#24449;&#25968;&#25454;&#24211;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#29305;&#24449;&#24037;&#31243;&#26500;&#24314;&#20102;&#21160;&#20316;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#38477;&#32500;&#21518;&#30340;&#19981;&#21516;&#27169;&#22411;&#26469;&#35782;&#21035;&#19981;&#21516;&#30340;&#21160;&#20316;&#25216;&#33021;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#19981;&#21516;&#35780;&#20272;&#25351;&#26631;&#30340;&#25439;&#22833;&#20989;&#25968;&#24314;&#31435;&#20102;&#21160;&#20316;&#25216;&#33021;&#30340;&#23618;&#27425;&#21270;&#35780;&#20272;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of electronic science and technology, the research on wearable devices is constantly updated, but for now, it is not comprehensive for wearable devices to recognize and analyze the movement of specific sports. Based on this, this paper improves wearable devices of table tennis sport, and realizes the pattern recognition and evaluation of table tennis players' motor skills through artificial intelligence. Firstly, a device is designed to collect the movement information of table tennis players and the actual movement data is processed. Secondly, a sliding window is made to divide the collected motion data into a characteristic database of six table tennis benchmark movements. Thirdly, motion features were constructed based on feature engineering, and motor skills were identified for different models after dimensionality reduction. Finally, the hierarchical evaluation system of motor skills is established with the loss functions of different evaluation indexes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-SAEDN-Res&#30340;&#30701;&#26399;&#21151;&#29575;&#36127;&#33655;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#21644;&#27531;&#24046;&#20462;&#27491;&#25216;&#26415;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#24102;&#26377;&#38750;&#26102;&#24207;&#22240;&#32032;&#30340;&#36127;&#33655;&#25968;&#25454;&#24182;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.07140</link><description>&lt;p&gt;
&#22522;&#20110;CNN-SAEDN-Res&#30340;&#30701;&#26399;&#21151;&#29575;&#36127;&#33655;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Short-term power load forecasting method based on CNN-SAEDN-Res. (arXiv:2309.07140v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07140
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN-SAEDN-Res&#30340;&#30701;&#26399;&#21151;&#29575;&#36127;&#33655;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#21644;&#27531;&#24046;&#20462;&#27491;&#25216;&#26415;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#24102;&#26377;&#38750;&#26102;&#24207;&#22240;&#32032;&#30340;&#36127;&#33655;&#25968;&#25454;&#24182;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#24102;&#26377;&#38750;&#26102;&#24207;&#22240;&#32032;&#30340;&#36127;&#33655;&#25968;&#25454;&#38590;&#20197;&#36890;&#36807;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;&#36825;&#20010;&#38382;&#39064;&#23548;&#33268;&#20102;&#39044;&#27979;&#30340;&#31934;&#24230;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#65288;SAEDN&#65289;&#21644;&#27531;&#24046;&#20462;&#27491;&#65288;Res&#65289;&#30340;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#26041;&#27861;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#29305;&#24449;&#25552;&#21462;&#27169;&#22359;&#30001;&#19968;&#20010;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#29992;&#20110;&#25366;&#25496;&#25968;&#25454;&#20043;&#38388;&#30340;&#23616;&#37096;&#30456;&#20851;&#24615;&#24182;&#33719;&#21462;&#39640;&#32500;&#25968;&#25454;&#29305;&#24449;&#12290;&#21021;&#22987;&#36127;&#33655;&#39044;&#27979;&#27169;&#22359;&#30001;&#19968;&#20010;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#21644;&#19968;&#20010;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#32452;&#25104;&#12290;&#35813;&#27169;&#22359;&#21033;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#23545;&#39640;&#32500;&#29305;&#24449;&#36827;&#34892;&#32534;&#30721;&#12290;&#36825;&#20010;&#25805;&#20316;&#21487;&#20197;&#33719;&#21462;&#25968;&#25454;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22522;&#20110;&#28151;&#21512;&#20102;&#38750;&#26102;&#24207;&#22240;&#32032;&#30340;&#25968;&#25454;&#20013;&#30340;&#32806;&#21512;&#20851;&#31995;&#20445;&#30041;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In deep learning, the load data with non-temporal factors are difficult to process by sequence models. This problem results in insufficient precision of the prediction. Therefore, a short-term load forecasting method based on convolutional neural network (CNN), self-attention encoder-decoder network (SAEDN) and residual-refinement (Res) is proposed. In this method, feature extraction module is composed of a two-dimensional convolutional neural network, which is used to mine the local correlation between data and obtain high-dimensional data features. The initial load fore-casting module consists of a self-attention encoder-decoder network and a feedforward neural network (FFN). The module utilizes self-attention mechanisms to encode high-dimensional features. This operation can obtain the global correlation between data. Therefore, the model is able to retain important information based on the coupling relationship between the data in data mixed with non-time series factors. Then, self
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#33258;&#32534;&#30721;&#22120;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30450;&#28304;&#20998;&#31163;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#36755;&#20837;&#35299;&#30721;&#21644;&#37325;&#26500;&#65292;&#28982;&#21518;&#21033;&#29992;&#32534;&#30721;&#25513;&#34109;&#25216;&#26415;&#36827;&#34892;&#28304;&#25512;&#26029;&#65292;&#21516;&#26102;&#24341;&#20837;&#36335;&#24452;&#20998;&#31163;&#25439;&#22833;&#20197;&#20419;&#36827;&#31232;&#30095;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07138</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#30450;&#28304;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Blind Source Separation via Multi-Encoder Autoencoders. (arXiv:2309.07138v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#33258;&#32534;&#30721;&#22120;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#30450;&#28304;&#20998;&#31163;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#36755;&#20837;&#35299;&#30721;&#21644;&#37325;&#26500;&#65292;&#28982;&#21518;&#21033;&#29992;&#32534;&#30721;&#25513;&#34109;&#25216;&#26415;&#36827;&#34892;&#28304;&#25512;&#26029;&#65292;&#21516;&#26102;&#24341;&#20837;&#36335;&#24452;&#20998;&#31163;&#25439;&#22833;&#20197;&#20419;&#36827;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30450;&#28304;&#20998;&#31163;&#65288;BSS&#65289;&#30340;&#20219;&#21153;&#26159;&#22312;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20174;&#28151;&#21512;&#20449;&#21495;&#20013;&#20998;&#31163;&#20986;&#28304;&#20449;&#21495;&#21644;&#28151;&#21512;&#31995;&#32479;&#12290;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#38656;&#35201;&#23545;&#28151;&#21512;&#31995;&#32479;&#21644;&#28304;&#20449;&#21495;&#20570;&#20986;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38750;&#32447;&#24615;&#28151;&#21512;&#30340;BSS&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#32534;&#30721;&#22120;&#33258;&#32534;&#30721;&#22120;&#30340;&#33258;&#28982;&#29305;&#24449;&#23376;&#31354;&#38388;&#19987;&#38376;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#23436;&#20840;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#24378;&#20808;&#39564;&#30693;&#35782;&#12290;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#36755;&#20837;&#35299;&#30721;&#25104;&#22810;&#32534;&#30721;&#22120;&#32593;&#32476;&#30340;&#21333;&#29420;&#32534;&#30721;&#31354;&#38388;&#65292;&#28982;&#21518;&#22312;&#35299;&#30721;&#22120;&#20869;&#37325;&#26032;&#28151;&#21512;&#36825;&#20123;&#34920;&#31034;&#20197;&#37325;&#26500;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#36827;&#34892;&#28304;&#25512;&#26029;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#25513;&#34109;&#25216;&#26415;&#65292;&#21363;&#23631;&#34109;&#38500;&#19968;&#20010;&#32534;&#30721;&#22806;&#30340;&#25152;&#26377;&#32534;&#30721;&#65292;&#20351;&#24471;&#35299;&#30721;&#22120;&#33021;&#22815;&#20272;&#35745;&#28304;&#20449;&#21495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;&#36335;&#24452;&#20998;&#31163;&#25439;&#22833;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#32534;&#30721;&#20043;&#38388;&#30340;&#31232;&#30095;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of blind source separation (BSS) involves separating sources from a mixture without prior knowledge of the sources or the mixing system. This is a challenging problem that often requires making restrictive assumptions about both the mixing system and the sources. In this paper, we propose a novel method for addressing BSS of non-linear mixtures by leveraging the natural feature subspace specialization ability of multi-encoder autoencoders with fully self-supervised learning without strong priors. During the training phase, our method unmixes the input into the separate encoding spaces of the multi-encoder network and then remixes these representations within the decoder for a reconstruction of the input. Then to perform source inference, we introduce a novel encoding masking technique whereby masking out all but one of the encodings enables the decoder to estimate a source signal. To this end, we also introduce a so-called pathway separation loss that encourages sparsity betwe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#24341;&#20837;&#21040;JAX&#24211;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;Firedrake&#26377;&#38480;&#20803;&#24211;&#30340;&#25509;&#21475;&#26469;&#23454;&#29616;&#12290;&#32780;&#20351;&#29992;&#27491;&#21521;&#21644;&#36870;&#21521;&#27169;&#24335;&#30340;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#32452;&#21512;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#21644;&#21487;&#24494;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.07137</link><description>&lt;p&gt;
&#20351;&#29992;&#27491;&#21521;&#21644;&#36870;&#21521;&#27169;&#24335;&#30340;&#33258;&#21160;&#24494;&#20998;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#24341;&#20837;JAX
&lt;/p&gt;
&lt;p&gt;
Bringing PDEs to JAX with forward and reverse modes automatic differentiation. (arXiv:2309.07137v1 [cs.MS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07137
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#24341;&#20837;&#21040;JAX&#24211;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;Firedrake&#26377;&#38480;&#20803;&#24211;&#30340;&#25509;&#21475;&#26469;&#23454;&#29616;&#12290;&#32780;&#20351;&#29992;&#27491;&#21521;&#21644;&#36870;&#21521;&#27169;&#24335;&#30340;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#32452;&#21512;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#21644;&#21487;&#24494;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#29992;&#20110;&#25551;&#36848;&#21508;&#31181;&#29289;&#29702;&#29616;&#35937;&#12290;&#36890;&#24120;&#36825;&#20123;&#26041;&#31243;&#27809;&#26377;&#35299;&#26512;&#35299;&#65292;&#32780;&#26159;&#20351;&#29992;&#25968;&#20540;&#36924;&#36817;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#27714;&#35299;PDEs&#30340;&#26041;&#27861;&#26159;&#26377;&#38480;&#20803;&#26041;&#27861;&#12290;&#22312;&#31185;&#23398;&#35745;&#31639;&#20013;&#65292;&#35745;&#31639;&#35299;&#23545;&#36755;&#20837;&#21442;&#25968;&#30340;&#23548;&#25968;&#20449;&#24687;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;JAX&#33258;&#21160;&#24494;&#20998;&#24211;&#25193;&#23637;&#21040;&#19982;Firedrake&#26377;&#38480;&#20803;&#24211;&#30340;&#25509;&#21475;&#12290;PDE&#30340;&#39640;&#32423;&#31526;&#21495;&#34920;&#31034;&#20801;&#35768;&#36890;&#36807;&#32469;&#36807;&#22522;&#30784;&#38750;&#32447;&#24615;&#27714;&#35299;&#22120;&#30340;&#20302;&#32423;&#22810;&#27425;&#36845;&#20195;&#26469;&#36827;&#34892;&#24494;&#20998;&#12290;&#36890;&#36807;Firedrake&#27714;&#35299;&#22120;&#30340;&#24494;&#20998;&#35745;&#31639;&#20351;&#29992;&#20999;&#32447;&#32447;&#24615;&#21644;&#20276;&#38543;&#26041;&#31243;&#26469;&#23436;&#25104;&#12290;&#36825;&#20351;&#24471;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#21487;&#20197;&#19982;&#20219;&#24847;&#21487;&#24494;&#31243;&#24207;&#39640;&#25928;&#32452;&#21512;&#12290;&#20195;&#30721;&#21487;&#22312;github.com/IvanYashchuk/jax-firedrake&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) are used to describe a variety of physical phenomena. Often these equations do not have analytical solutions and numerical approximations are used instead. One of the common methods to solve PDEs is the finite element method. Computing derivative information of the solution with respect to the input parameters is important in many tasks in scientific computing. We extend JAX automatic differentiation library with an interface to Firedrake finite element library. High-level symbolic representation of PDEs allows bypassing differentiating through low-level possibly many iterations of the underlying nonlinear solvers. Differentiating through Firedrake solvers is done using tangent-linear and adjoint equations. This enables the efficient composition of finite element solvers with arbitrary differentiable programs. The code is available at github.com/IvanYashchuk/jax-firedrake.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MTECG&#65292;&#25193;&#23637;&#20102;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#39564;&#35777;&#20102;&#37325;&#26500;&#30446;&#26631;&#30340;&#27874;&#21160;&#24615;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07136</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;Transformer&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Masked Transformer for Electrocardiogram Classification. (arXiv:2309.07136v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MTECG&#65292;&#25193;&#23637;&#20102;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#39564;&#35777;&#20102;&#37325;&#26500;&#30446;&#26631;&#30340;&#27874;&#21160;&#24615;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#20020;&#24202;&#24212;&#29992;&#20013;&#26368;&#37325;&#35201;&#30340;&#35786;&#26029;&#24037;&#20855;&#20043;&#19968;&#12290;&#38543;&#30528;&#20808;&#36827;&#31639;&#27861;&#30340;&#20986;&#29616;&#65292;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#24212;&#29992;&#20110;ECG&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#20854;&#22312;ECG&#25968;&#25454;&#19978;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#23454;&#29616;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#29992;&#30340;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;MTECG&#65292;&#23427;&#23558;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#24212;&#29992;&#25193;&#23637;&#21040;&#20102;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;220,251&#20010;ECG&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#35760;&#24405;&#30001;&#21307;&#23398;&#19987;&#23478;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35786;&#26029;&#27880;&#37322;&#65292;&#20197;&#25506;&#32034;MTECG&#30340;&#29305;&#24615;&#12290;&#22312;&#25552;&#20986;&#30340;&#35757;&#32451;&#31574;&#30053;&#19979;&#65292;&#19968;&#20010;&#21482;&#26377;5.7M&#21442;&#25968;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#65288;5%-75%&#65289;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#12290;&#28040;&#34701;&#30740;&#31350;&#31361;&#20986;&#20102;&#27874;&#21160;&#37325;&#26500;&#30446;&#26631;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;MTECG&#32791;&#26102;&#36739;&#23569;&#19988;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;&#21508;&#31181;&#24515;&#30005;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) is one of the most important diagnostic tools in clinical applications. With the advent of advanced algorithms, various deep learning models have been adopted for ECG tasks. However, the potential of Transformers for ECG data is not yet realized, despite their widespread success in computer vision and natural language processing. In this work, we present a useful masked Transformer method for ECG classification referred to as MTECG, which expands the application of masked autoencoders to ECG time series. We construct a dataset comprising 220,251 ECG recordings with a broad range of diagnoses annoated by medical experts to explore the properties of MTECG. Under the proposed training strategies, a lightweight model with 5.7M parameters performs stably well on a broad range of masking ratios (5%-75%). The ablation studies highlight the importance of fluctuated reconstruction targets, training schedule length, layer-wise LR decay and DropPath rate. The experiments o
&lt;/p&gt;</description></item><item><title>EpiDeNet&#26159;&#19968;&#31181;&#33410;&#33021;&#24615;&#30315;&#30187;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#36731;&#37327;&#32423;&#30315;&#30187;&#26816;&#27979;&#32593;&#32476;&#21644;&#32467;&#21512;&#28789;&#25935;&#24230;&#21644;&#29305;&#24322;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;(SSWCE)&#65292;&#25104;&#21151;&#24212;&#23545;&#20102;&#20005;&#37325;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#25361;&#25112;&#65292;&#24182;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;91.16&#65285;&#21644;92.00&#65285;&#30340;&#30315;&#30187;&#20107;&#20214;&#26816;&#27979;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07135</link><description>&lt;p&gt;
EpiDeNet&#65306;&#29992;&#20110;&#23884;&#20837;&#24335;&#31995;&#32479;&#30340;&#33410;&#33021;&#24615;&#30315;&#30187;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EpiDeNet: An Energy-Efficient Approach to Seizure Detection for Embedded Systems. (arXiv:2309.07135v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07135
&lt;/p&gt;
&lt;p&gt;
EpiDeNet&#26159;&#19968;&#31181;&#33410;&#33021;&#24615;&#30315;&#30187;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#36731;&#37327;&#32423;&#30315;&#30187;&#26816;&#27979;&#32593;&#32476;&#21644;&#32467;&#21512;&#28789;&#25935;&#24230;&#21644;&#29305;&#24322;&#24230;&#30340;&#25439;&#22833;&#20989;&#25968;(SSWCE)&#65292;&#25104;&#21151;&#24212;&#23545;&#20102;&#20005;&#37325;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#25361;&#25112;&#65292;&#24182;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;91.16&#65285;&#21644;92.00&#65285;&#30340;&#30315;&#30187;&#20107;&#20214;&#26816;&#27979;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30315;&#30187;&#26159;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#24433;&#21709;&#30528;&#25968;&#30334;&#19975;&#24739;&#32773;&#30340;&#29983;&#27963;&#65292;&#36830;&#32493;&#30417;&#27979;&#19982;&#33258;&#21160;&#21270;&#30315;&#30187;&#26816;&#27979;&#25104;&#20026;&#26377;&#25928;&#27835;&#30103;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#20026;&#20102;&#22312;&#26085;&#24120;&#29983;&#27963;&#26465;&#20214;&#19979;&#23454;&#29616;&#38271;&#26399;&#25252;&#29702;&#65292;&#38656;&#35201;&#33298;&#36866;&#21644;&#26234;&#33021;&#30340;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#24182;&#19988;&#35201;&#20855;&#22791;&#38271;&#30005;&#27744;&#23551;&#21629;&#65292;&#36825;&#21453;&#36807;&#26469;&#23545;&#36164;&#28304;&#21463;&#38480;&#21644;&#33410;&#33021;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#25552;&#20986;&#20102;&#35201;&#27714;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30315;&#30187;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#38754;&#20020;&#30528;&#20005;&#37325;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#30315;&#30187;&#26816;&#27979;&#32593;&#32476;EpiDeNet&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Sensitivity-Specificity Weighted Cross-Entropy (SSWCE)&#65292;&#23427;&#32467;&#21512;&#20102;&#28789;&#25935;&#24230;&#21644;&#29305;&#24322;&#24230;&#65292;&#20197;&#24212;&#23545;&#20005;&#37325;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#25361;&#25112;&#12290;&#25552;&#20986;&#30340;EpiDeNet-SSWCE&#26041;&#27861;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65288;CHB-MIT&#21644;PEDESITE&#65289;&#19978;&#25104;&#21151;&#26816;&#27979;&#21040;&#20102;91.16&#65285;&#21644;92.00&#65285;&#30340;&#30315;&#30187;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Epilepsy is a prevalent neurological disorder that affects millions of individuals globally, and continuous monitoring coupled with automated seizure detection appears as a necessity for effective patient treatment. To enable long-term care in daily-life conditions, comfortable and smart wearable devices with long battery life are required, which in turn set the demand for resource-constrained and energy-efficient computing solutions. In this context, the development of machine learning algorithms for seizure detection faces the challenge of heavily imbalanced datasets. This paper introduces EpiDeNet, a new lightweight seizure detection network, and Sensitivity-Specificity Weighted Cross-Entropy (SSWCE), a new loss function that incorporates sensitivity and specificity, to address the challenge of heavily unbalanced datasets. The proposed EpiDeNet-SSWCE approach demonstrates the successful detection of 91.16% and 92.00% seizure events on two different datasets (CHB-MIT and PEDESITE, re
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#29109;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26234;&#33021;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#21033;&#29992;&#38745;&#24687;&#29366;&#24577;&#33041;&#30005;&#20449;&#21495;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#30340;&#35786;&#26029;&#21644;&#30417;&#27979;&#12290;&#36890;&#36807;&#35745;&#31639;&#19981;&#21516;&#31867;&#22411;&#30340;&#29109;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#31946;&#29109;&#22312;&#35786;&#26029;&#21644;&#30417;&#27979;PD&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;PD&#35786;&#26029;&#12290;&#26368;&#37325;&#35201;&#30340;&#33041;&#30005;&#20449;&#21495;&#39057;&#29575;&#33539;&#22260;&#21644;&#36890;&#36947;&#20301;&#32622;&#20063;&#34987;&#30830;&#23450;&#12290;</title><link>http://arxiv.org/abs/2309.07134</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#26234;&#33021;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#24085;&#37329;&#26862;&#30149;&#30340;&#35786;&#26029;&#21644;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Entropy-based machine learning model for diagnosis and monitoring of Parkinson's Disease in smart IoT environment. (arXiv:2309.07134v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07134
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26234;&#33021;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#21033;&#29992;&#38745;&#24687;&#29366;&#24577;&#33041;&#30005;&#20449;&#21495;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#30340;&#35786;&#26029;&#21644;&#30417;&#27979;&#12290;&#36890;&#36807;&#35745;&#31639;&#19981;&#21516;&#31867;&#22411;&#30340;&#29109;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#31946;&#29109;&#22312;&#35786;&#26029;&#21644;&#30417;&#27979;PD&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#30340;&#29305;&#24449;&#32452;&#21512;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;PD&#35786;&#26029;&#12290;&#26368;&#37325;&#35201;&#30340;&#33041;&#30005;&#20449;&#21495;&#39057;&#29575;&#33539;&#22260;&#21644;&#36890;&#36947;&#20301;&#32622;&#20063;&#34987;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#21033;&#29992;&#38745;&#24687;&#29366;&#24577;&#33041;&#30005;&#20449;&#21495;&#65288;rs-EEG&#65289;&#23545;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#36827;&#34892;&#35786;&#26029;&#21644;&#30417;&#27979;&#12290;&#25105;&#20204;&#35745;&#31639;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#29109;&#65292;&#24182;&#21457;&#29616;&#27169;&#31946;&#29109;&#22312;&#20351;&#29992;rs-EEG&#36827;&#34892;PD&#35786;&#26029;&#21644;&#30417;&#27979;&#26041;&#38754;&#34920;&#29616;&#26368;&#22909;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#20449;&#21495;&#39057;&#29575;&#33539;&#22260;&#21644;&#33041;&#30005;&#36890;&#36947;&#30340;&#32452;&#21512;&#65292;&#20197;&#20934;&#30830;&#35786;&#26029;PD&#12290;&#26368;&#21518;&#65292;&#22312;&#36739;&#23569;&#30340;&#29305;&#24449;&#65288;11&#20010;&#29305;&#24449;&#65289;&#19979;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#32422;99.9%&#30340;&#26368;&#22823;&#20998;&#31867;&#20934;&#30830;&#24230;&#65288;ARKF&#65289;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#26368;&#26126;&#26174;&#30340;&#33041;&#30005;&#20449;&#21495;&#39057;&#29575;&#33539;&#22260;&#65292;&#24182;&#21457;&#29616;&#39640;&#20998;&#31867;&#20934;&#30830;&#24230;&#21462;&#20915;&#20110;&#20302;&#39057;&#20449;&#21495;&#25104;&#20998;&#65288;0-4 Hz&#65289;&#12290;&#27492;&#22806;&#65292;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20449;&#21495;&#20027;&#35201;&#26469;&#33258;&#22836;&#37096;&#30340;&#21491;&#21322;&#29699;&#65288;F8&#12289;P8&#12289;T8&#12289;FC6&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#38271;&#24230;&#30340;E
&lt;/p&gt;
&lt;p&gt;
The study presents the concept of a computationally efficient machine learning (ML) model for diagnosing and monitoring Parkinson's disease (PD) in an Internet of Things (IoT) environment using rest-state EEG signals (rs-EEG). We computed different types of entropy from EEG signals and found that Fuzzy Entropy performed the best in diagnosing and monitoring PD using rs-EEG. We also investigated different combinations of signal frequency ranges and EEG channels to accurately diagnose PD. Finally, with a fewer number of features (11 features), we achieved a maximum classification accuracy (ARKF) of ~99.9%. The most prominent frequency range of EEG signals has been identified, and we have found that high classification accuracy depends on low-frequency signal components (0-4 Hz). Moreover, the most informative signals were mainly received from the right hemisphere of the head (F8, P8, T8, FC6). Furthermore, we assessed the accuracy of the diagnosis of PD using three different lengths of E
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#32769;&#24180;&#20154;&#27491;&#24120;&#29983;&#27963;&#26465;&#20214;&#19979;&#36830;&#32493;&#30417;&#27979;&#20854;&#35748;&#30693;&#27700;&#24179;&#65292;&#24182;&#33021;&#22815;&#39044;&#27979;&#35748;&#30693;&#33021;&#21147;&#36739;&#24046;&#30340;&#24773;&#20917;&#65292;&#20026;&#26089;&#26399;&#24178;&#39044;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07133</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#35782;&#21035;&#35748;&#30693;&#33021;&#21147;&#36739;&#24046;&#30340;&#32769;&#24180;&#20154;
&lt;/p&gt;
&lt;p&gt;
Using wearable device-based machine learning models to autonomously identify older adults with poor cognition. (arXiv:2309.07133v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07133
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#32769;&#24180;&#20154;&#27491;&#24120;&#29983;&#27963;&#26465;&#20214;&#19979;&#36830;&#32493;&#30417;&#27979;&#20854;&#35748;&#30693;&#27700;&#24179;&#65292;&#24182;&#33021;&#22815;&#39044;&#27979;&#35748;&#30693;&#33021;&#21147;&#36739;&#24046;&#30340;&#24773;&#20917;&#65292;&#20026;&#26089;&#26399;&#24178;&#39044;&#25552;&#20379;&#20102;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#35748;&#30693;&#27979;&#35797;&#23545;&#24739;&#32773;&#21644;&#20020;&#24202;&#21307;&#29983;&#26469;&#35828;&#38750;&#24120;&#32791;&#26102;&#12290;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#39044;&#27979;&#27169;&#22411;&#21487;&#20197;&#22312;&#27491;&#24120;&#29983;&#27963;&#26465;&#20214;&#19979;&#36827;&#34892;&#25345;&#32493;&#30340;&#20581;&#24247;&#30417;&#27979;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#26089;&#26399;&#24178;&#39044;&#35748;&#30693;&#38556;&#30861;&#32769;&#24180;&#20154;&#30340;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#21462;&#20102;&#19982;&#29983;&#29289;&#38047;&#33410;&#24459;&#12289;&#29615;&#22659;&#20809;&#29031;&#26292;&#38706;&#12289;&#36523;&#20307;&#27963;&#21160;&#27700;&#24179;&#12289;&#30561;&#30496;&#21644;&#20449;&#21495;&#22788;&#29702;&#30456;&#20851;&#30340;&#26032;&#39062;&#21487;&#31359;&#25140;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26681;&#25454;&#25968;&#23383;&#31526;&#21495;&#26367;&#20195;&#27979;&#35797;&#65288;DSST&#65289;&#12289;&#24314;&#31435;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30331;&#35760;&#31807;&#23398;&#20064;&#23376;&#27979;&#39564;&#65288;CERAD-WL&#65289;&#21644;&#21160;&#29289;&#27969;&#21033;&#24615;&#27979;&#35797;&#65288;AFT&#65289;&#30340;&#32467;&#26524;&#26469;&#39044;&#27979;&#35748;&#30693;&#33021;&#21147;&#36739;&#24046;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#21253;&#21547;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#25945;&#32946;&#27700;&#24179;&#12289;&#23130;&#23035;&#29366;&#20917;&#12289;&#23478;&#24237;&#25910;&#20837;&#12289;&#31958;&#23615;&#30149;&#29366;&#24577;&#21644;&#25233;&#37057;&#29366;&#20917;&#31561;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#19977;&#20010;&#35748;&#30693;&#32467;&#26524;&#26102;&#20855;&#26377;&#26174;&#30528;&#26356;&#39640;&#30340;AUC&#12290;
&lt;/p&gt;
&lt;p&gt;
Conducting cognitive tests is time-consuming for patients and clinicians. Wearable device-based prediction models allow for continuous health monitoring under normal living conditions and could offer an alternative to identifying older adults with cognitive impairments for early interventions. In this study, we first derived novel wearable-based features related to circadian rhythms, ambient light exposure, physical activity levels, sleep, and signal processing. Then, we quantified the ability of wearable-based machine-learning models to predict poor cognition based on outcomes from the Digit Symbol Substitution Test (DSST), the Consortium to Establish a Registry for Alzheimers Disease Word-Learning subtest (CERAD-WL), and the Animal Fluency Test (AFT). We found that the wearable-based models had significantly higher AUCs when predicting all three cognitive outcomes compared to benchmark models containing age, sex, education, marital status, household income, diabetic status, depressio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#29992;&#20110;&#27604;&#36739;&#26377;&#21521;&#22270;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#22270;&#25968;&#25454;&#21644;&#21333;&#32454;&#32990;RNA-seq&#25968;&#25454;&#25512;&#26029;&#30340;&#23454;&#38469;&#32454;&#32990;&#38388;&#36890;&#35759;&#22270;&#23545;&#20854;&#30456;&#23545;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.07030</link><description>&lt;p&gt;
&#26377;&#21521;&#21152;&#26435;&#22270;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#65306;&#20197;&#32454;&#32990;&#38388;&#36890;&#35759;&#32593;&#32476;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks. (arXiv:2309.07030v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#29992;&#20110;&#27604;&#36739;&#26377;&#21521;&#22270;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#22270;&#25968;&#25454;&#21644;&#21333;&#32454;&#32990;RNA-seq&#25968;&#25454;&#25512;&#26029;&#30340;&#23454;&#38469;&#32454;&#32990;&#38388;&#36890;&#35759;&#22270;&#23545;&#20854;&#30456;&#23545;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#27604;&#36739;&#26368;&#20248;&#36755;&#36816;&#22270;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#26368;&#20248;&#36755;&#36816;&#24341;&#36215;&#30340;&#36317;&#31163;&#26082;&#25552;&#20379;&#20102;&#22270;&#20043;&#38388;&#30340;&#21512;&#29702;&#24230;&#37327;&#65292;&#21448;&#36890;&#36807;&#36755;&#36816;&#35745;&#21010;&#30340;&#21487;&#35299;&#37322;&#25551;&#36848;&#20102;&#22270;&#20043;&#38388;&#30456;&#20851;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#23545;&#31216;&#24615;&#65292;&#36890;&#24120;&#32771;&#34385;&#30340;&#26368;&#20248;&#36755;&#36816;&#36317;&#31163;&#20027;&#35201;&#29992;&#20110;&#26080;&#21521;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#21464;&#20307;&#30340;&#36317;&#31163;&#24230;&#37327;&#26469;&#27604;&#36739;&#26377;&#21521;&#22270;&#65306;&#65288;i&#65289;&#22320;&#29699;&#31227;&#21160;&#36317;&#31163;&#65288;Wasserstein&#65289;&#21644;&#65288;ii&#65289;Gromov-Wasserstein&#65288;GW&#65289;&#36317;&#31163;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#36317;&#31163;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#20223;&#30495;&#22270;&#25968;&#25454;&#21644;&#22522;&#20110;&#21333;&#32454;&#32990;RNA-seq&#25968;&#25454;&#25512;&#26029;&#30340;&#23454;&#38469;&#26377;&#21521;&#32454;&#32990;&#38388;&#36890;&#35759;&#22270;&#19978;&#30340;&#30456;&#23545;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparing graphs of optimal transport has recently gained significant attention, as the distances induced by optimal transport provide both a principled metric between graphs as well as an interpretable description of the associated changes between graphs in terms of a transport plan. As the lack of symmetry introduces challenges in the typically considered formulations, optimal transport distances for graphs have mostly been developed for undirected graphs. Here, we propose two distance measures to compare directed graphs based on variants of optimal transport: (i) an earth movers distance (Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate these two distances and discuss their relative performance for both simulated graph data and real-world directed cell-cell communication graphs, inferred from single-cell RNA-seq data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.06800</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#27979;&#37327;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#20219;&#21153;&#21644;&#20915;&#31574;&#23548;&#21521;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#65292;&#22240;&#20026;&#23427;&#22312;&#20132;&#36890;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#36817;&#26399;&#65292;&#35768;&#22810;&#30740;&#31350;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#39044;&#27979;&#20301;&#32622;&#26377;&#23436;&#25972;&#25110;&#33267;&#23569;&#37096;&#20998;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#19981;&#33021;&#25193;&#23637;&#21040;&#26080;&#21382;&#21490;&#35760;&#24405;&#30340;&#20301;&#32622;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#21644;&#23433;&#35013;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#20256;&#24863;&#22120;&#30340;&#37096;&#32626;&#21487;&#33021;&#21463;&#38480;&#65292;&#36825;&#20351;&#24471;&#22823;&#22810;&#25968;&#24403;&#21069;&#27169;&#22411;&#19981;&#36866;&#29992;&#12290;&#34429;&#28982;&#23569;&#25968;&#25991;&#29486;&#23581;&#35797;&#22312;&#32570;&#22833;&#20301;&#32622;&#19978;&#25554;&#34917;&#20132;&#36890;&#29366;&#24577;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19982;&#20256;&#24863;&#22120;&#20301;&#32622;&#21516;&#26102;&#35266;&#27979;&#30340;&#25968;&#25454;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#39044;&#27979;&#20219;&#21153;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#32570;&#20047;&#23545;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#24471;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#36866;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#20219;&#21153;&#25110;&#28041;&#21450;&#20915;&#31574;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#21463;&#21040;&#20808;&#21069;&#30340;&#24402;&#32435;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction is a crucial topic because of its broad scope of applications in the transportation domain. Recently, various studies have achieved promising results. However, most studies assume the prediction locations have complete or at least partial historical records and cannot be extended to non-historical recorded locations. In real-life scenarios, the deployment of sensors could be limited due to budget limitations and installation availability, which makes most current models not applicable. Though few pieces of literature tried to impute traffic states at the missing locations, these methods need the data simultaneously observed at the locations with sensors, making them not applicable to prediction tasks. Another drawback is the lack of measurement of uncertainty in prediction, making prior works unsuitable for risk-sensitive tasks or involving decision-making. To fill the gap, inspired by the previous inductive graph neural network, this work proposed an uncertainty-awa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38750;&#21442;&#25968;&#20984;&#21270;&#28388;&#27874;&#65288;DNCF&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35745;&#31639;&#25668;&#24433;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#24674;&#22797;&#12290;DNCF&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#22788;&#29702;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2309.06724</link><description>&lt;p&gt;
&#28145;&#24230;&#38750;&#21442;&#25968;&#20984;&#21270;&#28388;&#27874;&#22312;&#35745;&#31639;&#25668;&#24433;&#65292;&#22270;&#20687;&#21512;&#25104;&#21644;&#23545;&#25239;&#24615;&#38450;&#24481;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Nonparametric Convexified Filtering for Computational Photography, Image Synthesis and Adversarial Defense. (arXiv:2309.06724v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#38750;&#21442;&#25968;&#20984;&#21270;&#28388;&#27874;&#65288;DNCF&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35745;&#31639;&#25668;&#24433;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#24674;&#22797;&#12290;DNCF&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#22788;&#29702;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#19981;&#23436;&#32654;&#30340;&#22270;&#20687;&#20013;&#24674;&#22797;&#30495;&#23454;&#22330;&#26223;&#30340;&#35745;&#31639;&#25668;&#24433;&#65292;&#36890;&#36807;&#28145;&#24230;&#38750;&#21442;&#25968;&#20984;&#21270;&#28388;&#27874;&#65288;DNCF&#65289;&#12290;&#23427;&#30001;&#19968;&#20010;&#38750;&#21442;&#25968;&#28145;&#24230;&#32593;&#32476;&#32452;&#25104;&#65292;&#20197;&#27169;&#25311;&#22270;&#20687;&#24418;&#25104;&#32972;&#21518;&#30340;&#29289;&#29702;&#26041;&#31243;&#65292;&#22914;&#38477;&#22122;&#12289;&#36229;&#20998;&#36776;&#29575;&#12289;&#20462;&#22797;&#21644;&#38378;&#20809;&#12290;DNCF&#27809;&#26377;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#21442;&#25968;&#21270;&#65292;&#22240;&#27492;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#22788;&#29702;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#40723;&#21169;&#32593;&#32476;&#21442;&#25968;&#20026;&#38750;&#36127;&#65292;&#24182;&#22312;&#36755;&#20837;&#21644;&#21442;&#25968;&#19978;&#21019;&#24314;&#19968;&#20010;&#21452;&#20984;&#20989;&#25968;&#65292;&#36825;&#36866;&#24212;&#20110;&#36816;&#34892;&#26102;&#38388;&#19981;&#36275;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#30456;&#23545;&#20110;Deep Image Prior&#26377;10&#20493;&#30340;&#21152;&#36895;&#12290;&#36890;&#36807;&#36825;&#20123;&#24037;&#20855;&#65292;&#25105;&#20204;&#22312;&#23454;&#26102;&#20013;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#23545;&#25239;&#22270;&#20687;&#20998;&#31867;&#28145;&#24230;&#32593;&#32476;&#25915;&#20987;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aim to provide a general framework of for computational photography that recovers the real scene from imperfect images, via the Deep Nonparametric Convexified Filtering (DNCF). It is consists of a nonparametric deep network to resemble the physical equations behind the image formation, such as denoising, super-resolution, inpainting, and flash. DNCF has no parameterization dependent on training data, therefore has a strong generalization and robustness to adversarial image manipulation. During inference, we also encourage the network parameters to be nonnegative and create a bi-convex function on the input and parameters, and this adapts to second-order optimization algorithms with insufficient running time, having 10X acceleration over Deep Image Prior. With these tools, we empirically verify its capability to defend image classification deep networks against adversary attack algorithms in real-time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#23618;&#32423;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#36873;&#25321;&#20998;&#24067;&#24335;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24182;&#21516;&#26102;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#33258;&#21160;&#21270;&#21644;&#21327;&#21516;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06604</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#36164;&#28304;&#19978;&#30340;&#28151;&#21512;&#31639;&#27861;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;: &#19968;&#31181;&#22522;&#20110;&#23618;&#32423;&#20195;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach. (arXiv:2309.06604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#23618;&#32423;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#29992;&#20110;&#36873;&#25321;&#20998;&#24067;&#24335;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24182;&#21516;&#26102;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#21487;&#20280;&#32553;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#33258;&#21160;&#21270;&#21644;&#21327;&#21516;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#26159;&#23398;&#26415;&#30028;&#21644;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#20851;&#38190;&#30340;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#36164;&#28304;&#25968;&#37327;&#30340;&#22823;&#24133;&#22686;&#21152;&#12289;&#22810;&#26679;&#24615;&#21644;&#20998;&#24067;&#24615;&#65292;&#36825;&#20123;&#27493;&#39588;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#24403;&#23558;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#30340;&#35774;&#35745;&#26102;&#65292;&#20250;&#24102;&#26469;&#21487;&#20280;&#32553;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#22810;&#20010;&#29420;&#29305;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21644;&#21327;&#21516;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#26426;&#21046;&#65292;&#29992;&#20110;&#36873;&#25321;&#20998;&#24067;&#24335;&#32452;&#32455;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#21516;&#26102;&#35843;&#25972;&#20854;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#23618;&#32423;&#26426;&#22120;&#23398;&#20064;&#24179;&#21488;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#20854;&#26597;&#35810;&#32467;&#26500;&#26469;&#25903;&#25345;&#19978;&#36848;&#21151;&#33021;&#65292;&#32780;&#19981;&#38480;&#20110;&#29305;&#23450;&#30340;&#23398;&#20064;&#12289;&#36873;&#25321;&#21644;&#35843;&#25972;&#26426;&#21046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#35780;&#20272;&#12289;&#24418;&#24335;&#39564;&#35777;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm selection and hyperparameter tuning are critical steps in both academic and applied machine learning. On the other hand, these steps are becoming ever increasingly delicate due to the extensive rise in the number, diversity, and distributedness of machine learning resources. Multi-agent systems, when applied to the design of machine learning platforms, bring about several distinctive characteristics such as scalability, flexibility, and robustness, just to name a few. This paper proposes a fully automatic and collaborative agent-based mechanism for selecting distributedly organized machine learning algorithms and simultaneously tuning their hyperparameters. Our method builds upon an existing agent-based hierarchical machine-learning platform and augments its query structure to support the aforementioned functionalities without being limited to specific learning, selection, and tuning mechanisms. We have conducted theoretical assessments, formal verification, and analytical st
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#26032;&#21152;&#26435;&#20462;&#27491;&#37319;&#26679;&#20559;&#24046;&#30340;&#31354;&#38388;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#23454;&#29616;&#30446;&#26631;&#38169;&#35823;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#20154;&#24037;&#25968;&#25454;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#20351;&#39044;&#27979;&#35823;&#24046;&#20174;7%&#38477;&#33267;2%&#65292;&#19988;&#26679;&#26412;&#37327;&#36234;&#22823;&#25928;&#26524;&#36234;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.04824</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#26032;&#21152;&#26435;&#32416;&#27491;&#37319;&#26679;&#20559;&#24046;&#30340;&#31354;&#38388;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Correcting sampling biases via importancereweighting for spatial modeling. (arXiv:2309.04824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04824
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#26032;&#21152;&#26435;&#20462;&#27491;&#37319;&#26679;&#20559;&#24046;&#30340;&#31354;&#38388;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#23454;&#29616;&#30446;&#26631;&#38169;&#35823;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#20154;&#24037;&#25968;&#25454;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#20351;&#39044;&#27979;&#35823;&#24046;&#20174;7%&#38477;&#33267;2%&#65292;&#19988;&#26679;&#26412;&#37327;&#36234;&#22823;&#25928;&#26524;&#36234;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#30001;&#20110;&#20998;&#24067;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#29615;&#22659;&#30740;&#31350;&#20013;&#30340;&#31354;&#38388;&#25968;&#25454;&#65292;&#23545;&#38169;&#35823;&#30340;&#20272;&#35745;&#36890;&#24120;&#26159;&#22797;&#26434;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#35201;&#24615;&#37319;&#26679;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#30446;&#26631;&#38169;&#35823;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#36890;&#36807;&#32771;&#34385;&#26399;&#26395;&#38169;&#35823;&#21644;&#21487;&#29992;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#26679;&#26412;&#28857;&#19978;&#37325;&#26032;&#21152;&#26435;&#38169;&#35823;&#24182;&#25269;&#28040;&#20559;&#31227;&#12290;&#25105;&#20204;&#20351;&#29992;&#37325;&#35201;&#24615;&#37319;&#26679;&#25216;&#26415;&#21644;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#36827;&#34892;&#37325;&#21152;&#26435;&#12290;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#30495;&#23454;&#19990;&#30028;&#31354;&#38388;&#25968;&#25454;&#38598;&#30340;&#20154;&#24037;&#25968;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#30446;&#26631;&#38169;&#35823;&#20272;&#35745;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#20026;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#24635;&#20307;&#39044;&#27979;&#35823;&#24046;&#20174;7%&#38477;&#20302;&#21040;&#20165;&#20026;2%&#65292;&#19988;&#38543;&#30528;&#26679;&#26412;&#37327;&#30340;&#22686;&#21152;&#32780;&#36827;&#19968;&#27493;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning models, the estimation of errors is often complex due to distribution bias, particularly in spatial data such as those found in environmental studies. We introduce an approach based on the ideas of importance sampling to obtain an unbiased estimate of the target error. By taking into account difference between desirable error and available data, our method reweights errors at each sample point and neutralizes the shift. Importance sampling technique and kernel density estimation were used for reweighteing. We validate the effectiveness of our approach using artificial data that resemble real-world spatial datasets. Our findings demonstrate advantages of the proposed approach for the estimation of the target error, offering a solution to a distribution shift problem. Overall error of predictions dropped from 7% to just 2% and it gets smaller for larger samples.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31867;&#21704;&#24076;&#34920;&#31034;&#21644;&#20998;&#23618;&#24378;&#21270;&#20132;&#21449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20248;&#21270;&#29305;&#24449;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31995;&#32479;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#24847;&#20041;&#12289;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.04612</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#31867;&#21704;&#24076;&#34920;&#31034;&#21644;&#20998;&#23618;&#24378;&#21270;&#20132;&#21449;&#36827;&#34892;&#33258;&#20248;&#21270;&#29305;&#24449;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Self-optimizing Feature Generation via Categorical Hashing Representation and Hierarchical Reinforcement Crossing. (arXiv:2309.04612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04612
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31867;&#21704;&#24076;&#34920;&#31034;&#21644;&#20998;&#23618;&#24378;&#21270;&#20132;&#21449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20248;&#21270;&#29305;&#24449;&#29983;&#25104;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31995;&#32479;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#24847;&#20041;&#12289;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#29983;&#25104;&#26088;&#22312;&#29983;&#25104;&#26032;&#30340;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#21306;&#20998;&#24230;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#24403;&#29983;&#25104;&#30340;&#29305;&#24449;&#26469;&#33258;&#20855;&#26377;&#22266;&#26377;&#29305;&#24449;&#20132;&#20114;&#30340;&#29305;&#24449;&#23545;&#26102;&#65292;&#29983;&#25104;&#30340;&#29305;&#24449;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#32463;&#39564;&#20016;&#23500;&#30340;&#25968;&#25454;&#31185;&#23398;&#23478;&#21487;&#20197;&#35782;&#21035;&#20986;&#28508;&#22312;&#26377;&#29992;&#30340;&#29305;&#24449;-&#29305;&#24449;&#20132;&#20114;&#65292;&#24182;&#20174;&#25351;&#25968;&#32423;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#32500;&#24230;&#65292;&#22312;&#26368;&#20248;&#30340;&#29983;&#25104;&#36335;&#24452;&#19978;&#20197;&#26368;&#20248;&#30340;&#20132;&#21449;&#24418;&#24335;&#12290;&#20294;&#26159;&#65292;&#26426;&#22120;&#30340;&#20154;&#31867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#23558;&#36825;&#31867;&#23398;&#20064;&#20219;&#21153;&#27010;&#25324;&#20026;&#33258;&#20248;&#21270;&#29305;&#24449;&#29983;&#25104;&#12290;&#33258;&#20248;&#21270;&#29305;&#24449;&#29983;&#25104;&#23545;&#29616;&#26377;&#31995;&#32479;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65306;&#26377;&#24847;&#20041;&#12289;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#21644;&#36890;&#29992;&#30340;&#34920;&#31034;&#20132;&#21449;&#26694;&#26550;&#26469;&#35299;&#20915;&#33258;&#20248;&#21270;&#29305;&#24449;&#29983;&#25104;&#12290;&#20026;&#20102;&#23454;&#29616;&#21704;&#24076;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#27493;&#27861;&#65306;&#29305;&#24449;&#31163;&#25955;&#21270;&#12289;&#29305;&#24449;&#21704;&#24076;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Feature generation aims to generate new and meaningful features to create a discriminative representation space.A generated feature is meaningful when the generated feature is from a feature pair with inherent feature interaction. In the real world, experienced data scientists can identify potentially useful feature-feature interactions, and generate meaningful dimensions from an exponentially large search space, in an optimal crossing form over an optimal generation path. But, machines have limited human-like abilities.We generalize such learning tasks as self-optimizing feature generation. Self-optimizing feature generation imposes several under-addressed challenges on existing systems: meaningful, robust, and efficient generation. To tackle these challenges, we propose a principled and generic representation-crossing framework to solve self-optimizing feature generation.To achieve hashing representation, we propose a three-step approach: feature discretization, feature hashing, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;DMI&#25935;&#24863;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#20302;&#20449;&#22122;&#27604;&#21644;&#22833;&#30495;&#30340;DMI FID&#30340;&#20195;&#35874;&#29289;&#27987;&#24230;&#65292;&#24182;&#36890;&#36807;MRI&#30340;&#36793;&#32536;&#20445;&#25252;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#21442;&#25968;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04100</link><description>&lt;p&gt;
DMI (Deuterium Metabolic Imaging) &#30340;&#25935;&#24863;&#24615;&#22686;&#24378;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Method for Sensitivity Enhancement of Deuterium Metabolic Imaging (DMI). (arXiv:2309.04100v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;DMI&#25935;&#24863;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#20272;&#35745;&#20302;&#20449;&#22122;&#27604;&#21644;&#22833;&#30495;&#30340;DMI FID&#30340;&#20195;&#35874;&#29289;&#27987;&#24230;&#65292;&#24182;&#36890;&#36807;MRI&#30340;&#36793;&#32536;&#20445;&#25252;&#27491;&#21017;&#21270;&#36827;&#19968;&#27493;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#21442;&#25968;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;DMI&#65288;Deuterium Metabolic Imaging&#65289;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#21644;&#26368;&#23567;&#25195;&#25551;&#26102;&#38388;&#21463;&#21040;&#21487;&#36798;&#21040;&#30340;&#20449;&#22122;&#27604;&#30340;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;DMI&#25935;&#24863;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#35774;&#35745;&#20102;&#19968;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26469;&#20272;&#35745;&#20302;&#20449;&#22122;&#27604;&#21644;&#22833;&#30495;&#30340;DMI FID&#30340;2H&#26631;&#35760;&#20195;&#35874;&#29289;&#27987;&#24230;&#12290;CNN&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25968;&#25454;&#20195;&#34920;&#36890;&#24120;&#22312;&#20307;&#20869;&#36935;&#21040;&#30340;&#19968;&#31995;&#21015;&#20449;&#22122;&#27604;&#27700;&#24179;&#12290;&#36890;&#36807;&#23545;&#27599;&#20010;DMI&#25968;&#25454;&#38598;&#20351;&#29992;&#22522;&#20110;MRI&#30340;&#36793;&#32536;&#20445;&#25252;&#27491;&#21017;&#21270;&#36827;&#34892;CNN&#30340;&#24494;&#35843;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20272;&#35745;&#31934;&#24230;&#12290;&#25552;&#20986;&#30340;&#22788;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;PRECISE-DMI&#65288;PReserved Edge ConvolutIonal neural network for Sensitivity Enhanced DMI&#65289;&#65292;&#24212;&#29992;&#20110;&#27169;&#25311;&#30740;&#31350;&#21644;&#20307;&#20869;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#22312;SNR&#19978;&#39044;&#26399;&#30340;&#25913;&#36827;&#65292;&#24182;&#30740;&#31350;&#20102;&#21487;&#33021;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#65306;PRECISE-DMI&#22312;&#20302;&#20449;&#22122;&#27604;&#25968;&#25454;&#38598;&#30340;&#20195;&#35874;&#22270;&#20687;&#19978;&#26377;&#30528;&#26126;&#26174;&#30340;&#25913;&#21892;&#65292;&#24182;&#25552;&#39640;&#20102;&#21442;&#25968;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Common to most MRSI techniques, the spatial resolution and the minimal scan duration of Deuterium Metabolic Imaging (DMI) are limited by the achievable SNR. This work presents a deep learning method for sensitivity enhancement of DMI.  Methods: A convolutional neural network (CNN) was designed to estimate the 2H-labeled metabolite concentrations from low SNR and distorted DMI FIDs. The CNN was trained with synthetic data that represent a range of SNR levels typically encountered in vivo. The estimation precision was further improved by fine-tuning the CNN with MRI-based edge-preserving regularization for each DMI dataset. The proposed processing method, PReserved Edge ConvolutIonal neural network for Sensitivity Enhanced DMI (PRECISE-DMI), was applied to simulation studies and in vivo experiments to evaluate the anticipated improvements in SNR and investigate the potential for inaccuracies.  Results: PRECISE-DMI visually improved the metabolic maps of low SNR datasets, and qua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;DKM&#23454;&#29616;&#65292;&#21363;eDKM&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;&#35757;&#32451;&#26102;&#26435;&#37325;&#32858;&#31867;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#20943;&#23567;DKM&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#35299;&#20915;&#20102;LLM&#21387;&#32553;&#20013;&#30340;&#35757;&#32451;&#24320;&#38144;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.00964</link><description>&lt;p&gt;
eDKM:&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;&#35757;&#32451;&#26102;&#26435;&#37325;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models. (arXiv:2309.00964v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;DKM&#23454;&#29616;&#65292;&#21363;eDKM&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;&#35757;&#32451;&#26102;&#26435;&#37325;&#32858;&#31867;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#20943;&#23567;DKM&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#35299;&#20915;&#20102;LLM&#21387;&#32553;&#20013;&#30340;&#35757;&#32451;&#24320;&#38144;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#22797;&#26434;&#35821;&#35328;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#24615;&#33021;&#65292;&#22240;&#27492;&#23558;&#36825;&#20123;LLMs&#24341;&#20837;&#31227;&#21160;&#35774;&#22791;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#21709;&#24212;&#21644;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#35268;&#27169;&#65288;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65289;&#38656;&#35201;&#39640;&#25928;&#30340;&#21387;&#32553;&#25165;&#33021;&#36866;&#24212;&#23384;&#20648;&#26377;&#38480;&#30340;&#35774;&#22791;&#12290;&#22312;&#20247;&#22810;&#21387;&#32553;&#25216;&#26415;&#20013;&#65292;&#26435;&#37325;&#32858;&#31867;&#26159;LLM&#21387;&#32553;&#30340;&#39046;&#20808;&#20505;&#36873;&#26041;&#27861;&#20043;&#19968;&#65292;&#24182;&#24471;&#21040;&#20102;&#29616;&#20195;&#26234;&#33021;&#25163;&#26426;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#20854;&#35757;&#32451;&#24320;&#38144;&#23545;LLM&#30340;&#24494;&#35843;&#26469;&#35828;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#21487;&#24494;&#20998;K&#22343;&#20540;&#32858;&#31867;&#65288;DKM&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#21387;&#32553;&#27604;&#21644;&#20934;&#30830;&#24615;&#22238;&#24402;&#20043;&#38388;&#30340;&#26368;&#20808;&#36827;&#25240;&#34935;&#26041;&#26696;&#65292;&#20294;&#20854;&#36739;&#22823;&#30340;&#20869;&#23384;&#22797;&#26434;&#24615;&#20351;&#20854;&#20960;&#20046;&#19981;&#21487;&#33021;&#24212;&#29992;&#20110;&#35757;&#32451;&#26102;&#30340;LLM&#21387;&#32553;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;DKM&#23454;&#29616;&#65292;&#21363;eDKM&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#25216;&#26415;&#20943;&#23567;&#20102;DKM&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since Large Language Models or LLMs have demonstrated high-quality performance on many complex language tasks, there is a great interest in bringing these LLMs to mobile devices for faster responses and better privacy protection. However, the size of LLMs (i.e., billions of parameters) requires highly effective compression to fit into storage-limited devices. Among many compression techniques, weight-clustering, a form of non-linear quantization, is one of the leading candidates for LLM compression, and supported by modern smartphones. Yet, its training overhead is prohibitively significant for LLM fine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown the state-of-the-art trade-off between compression ratio and accuracy regression, but its large memory complexity makes it nearly impossible to apply to train-time LLM compression. In this paper, we propose a memory-efficient DKM implementation, eDKM powered by novel techniques to reduce the memory footprint of DKM 
&lt;/p&gt;</description></item><item><title>GBE-MLZSL&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#26631;&#31614;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#32676;&#20307;&#21452;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#22312;&#25512;&#29702;&#26410;&#35265;&#31867;&#26102;&#27169;&#22411;&#22833;&#21435;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#25226;&#25569;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.00923</link><description>&lt;p&gt;
GBE-MLZSL: &#19968;&#31181;&#29992;&#20110;&#22810;&#26631;&#31614;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#32676;&#20307;&#21452;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GBE-MLZSL: A Group Bi-Enhancement Framework for Multi-Label Zero-Shot Learning. (arXiv:2309.00923v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00923
&lt;/p&gt;
&lt;p&gt;
GBE-MLZSL&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#26631;&#31614;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#32676;&#20307;&#21452;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#35299;&#20915;&#20102;&#22312;&#25512;&#29702;&#26410;&#35265;&#31867;&#26102;&#27169;&#22411;&#22833;&#21435;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#25226;&#25569;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#22330;&#26223;&#19979;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;MLZSL&#65289;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#24050;&#35265;&#31867;&#21644;&#36741;&#21161;&#30693;&#35782;&#65288;&#20363;&#22914;&#35821;&#20041;&#20449;&#24687;&#65289;&#22312;&#19968;&#20010;&#26679;&#26412;&#65288;&#20363;&#22914;&#19968;&#24352;&#22270;&#29255;&#65289;&#20013;&#35782;&#21035;&#22810;&#20010;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#20998;&#26512;&#26679;&#26412;&#20013;&#21508;&#20010;&#24050;&#35265;&#31867;&#30340;&#20851;&#31995;&#65288;&#20174;&#31354;&#38388;&#25110;&#35821;&#20041;&#29305;&#24449;&#30340;&#35282;&#24230;&#65289;&#65292;&#24182;&#23558;&#23398;&#20064;&#21040;&#30340;&#27169;&#22411;&#36801;&#31227;&#21040;&#26410;&#35265;&#31867;&#19978;&#12290;&#20294;&#23427;&#20204;&#24573;&#30053;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#26377;&#25928;&#25972;&#21512;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#25512;&#29702;&#26410;&#35265;&#31867;&#26102;&#65292;&#20840;&#23616;&#29305;&#24449;&#34920;&#31034;&#22270;&#20687;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20027;&#35201;&#26041;&#21521;&#65292;&#32780;&#23616;&#37096;&#29305;&#24449;&#24212;&#35813;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#20445;&#25345;&#21807;&#19968;&#24615;&#12290;&#36825;&#31181;&#25972;&#21512;&#30340;&#24573;&#35270;&#20250;&#20351;&#27169;&#22411;&#22833;&#21435;&#23545;&#22270;&#20687;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#30340;&#25226;&#25569;&#12290;&#20165;&#22312;&#25512;&#29702;&#38454;&#27573;&#20381;&#36182;&#20110;&#24050;&#35265;&#31867;&#30340;&#23616;&#37096;&#23384;&#22312;&#20250;&#24341;&#20837;&#19981;&#21487;&#36991;&#20813;&#30340;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#32676;&#20307;&#21452;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates a challenging problem of zero-shot learning in the multi-label scenario (MLZSL), wherein, the model is trained to recognize multiple unseen classes within a sample (e.g., an image) based on seen classes and auxiliary knowledge, e.g., semantic information. Existing methods usually resort to analyzing the relationship of various seen classes residing in a sample from the dimension of spatial or semantic characteristics, and transfer the learned model to unseen ones. But they ignore the effective integration of local and global features. That is, in the process of inferring unseen classes, global features represent the principal direction of the image in the feature space, while local features should maintain uniqueness within a certain range. This integrated neglect will make the model lose its grasp of the main components of the image. Relying only on the local existence of seen classes during the inference stage introduces unavoidable bias. In this paper, we pro
&lt;/p&gt;</description></item><item><title>DoRA&#26159;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#30340;&#20302;&#36164;&#28304;&#25151;&#22320;&#20135;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#26410;&#26631;&#35760;&#30340;&#25151;&#22320;&#20135;&#25968;&#25454;&#38598;&#21512;&#26469;&#20943;&#23569;&#20027;&#35266;&#24615;&#65292;&#24182;&#19988;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2309.00855</link><description>&lt;p&gt;
DoRA: &#22522;&#20110;&#39046;&#22495;&#30340;&#20302;&#36164;&#28304;&#25151;&#22320;&#20135;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DoRA: Domain-Based Self-Supervised Learning Framework for Low-Resource Real Estate Appraisal. (arXiv:2309.00855v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00855
&lt;/p&gt;
&lt;p&gt;
DoRA&#26159;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#30340;&#20302;&#36164;&#28304;&#25151;&#22320;&#20135;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#26410;&#26631;&#35760;&#30340;&#25151;&#22320;&#20135;&#25968;&#25454;&#38598;&#21512;&#26469;&#20943;&#23569;&#20027;&#35266;&#24615;&#65292;&#24182;&#19988;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#38656;&#27714;&#21644;&#20379;&#24212;&#30340;&#24066;&#22330;&#31995;&#32479;&#24050;&#34987;&#25506;&#32034;&#29992;&#20110;&#24320;&#21457;&#23545;&#25151;&#22320;&#20135;&#36827;&#34892;&#20844;&#27491;&#30340;&#20915;&#31574;&#12290;&#25151;&#22320;&#20135;&#35780;&#20272;&#26159;&#37329;&#34701;&#26426;&#26500;&#20013;&#19968;&#39033;&#39640;&#25104;&#26412;&#30340;&#36164;&#20135;&#20272;&#20540;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#39046;&#22495;&#19987;&#23478;&#22522;&#20110;&#30456;&#24212;&#30340;&#30693;&#35782;&#21644;&#24066;&#22330;&#30340;&#21028;&#26029;&#26469;&#35780;&#20272;&#20272;&#20215;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#20272;&#20540;&#27169;&#22411;&#20943;&#23569;&#20102;&#39046;&#22495;&#19987;&#23478;&#30340;&#20027;&#35266;&#24615;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#30340;&#20132;&#26131;&#29992;&#20110;&#26377;&#25928;&#35780;&#20272;&#65292;&#36825;&#20027;&#35201;&#21463;&#38480;&#20110;&#20132;&#26131;&#30340;&#26631;&#35760;&#24037;&#20316;&#20197;&#21450;&#23545;&#26032;&#24320;&#21457;&#21644;&#20892;&#26449;&#22320;&#21306;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#23398;&#20064;&#26410;&#26631;&#35760;&#30340;&#25151;&#22320;&#20135;&#38598;&#21512;&#26102;&#65292;&#24573;&#35270;&#20102;&#21508;&#31181;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#19988;&#26080;&#27861;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DoRA&#65292;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#30340;&#20302;&#36164;&#28304;&#25151;&#22320;&#20135;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The marketplace system connecting demands and supplies has been explored to develop unbiased decision-making in valuing properties. Real estate appraisal serves as one of the high-cost property valuation tasks for financial institutions since it requires domain experts to appraise the estimation based on the corresponding knowledge and the judgment of the market. Existing automated valuation models reducing the subjectivity of domain experts require a large number of transactions for effective evaluation, which is predominantly limited to not only the labeling efforts of transactions but also the generalizability of new developing and rural areas. To learn representations from unlabeled real estate sets, existing self-supervised learning (SSL) for tabular data neglects various important features, and fails to incorporate domain knowledge. In this paper, we propose DoRA, a Domain-based self-supervised learning framework for low-resource Real estate Appraisal. DoRA is pre-trained with an
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#20154;&#24037;&#21644;&#31639;&#27861;&#21512;&#20316;&#65292;&#23545;&#20110;&#22810;&#20010;&#22122;&#38899;&#27169;&#22411;&#26469;&#35828;&#65292;&#23558;&#36873;&#25321;&#39033;&#30446;&#30340;&#23376;&#38598;&#22823;&#23567;$k$&#35774;&#32622;&#22312;$[2, n-1]$&#33539;&#22260;&#20869;&#33021;&#22815;&#26368;&#22823;&#21270;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#39033;&#30446;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11721</link><description>&lt;p&gt;
&#20004;&#20010;&#21015;&#34920;&#20160;&#20040;&#26102;&#20505;&#27604;&#19968;&#20010;&#21015;&#34920;&#26356;&#22909;&#65311;&#21512;&#20316;&#20915;&#31574;&#20013;&#30340;&#30410;&#22788;&#21644;&#20260;&#23475;
&lt;/p&gt;
&lt;p&gt;
When Are Two Lists Better than One?: Benefits and Harms in Joint Decision-making. (arXiv:2308.11721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11721
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#20154;&#24037;&#21644;&#31639;&#27861;&#21512;&#20316;&#65292;&#23545;&#20110;&#22810;&#20010;&#22122;&#38899;&#27169;&#22411;&#26469;&#35828;&#65292;&#23558;&#36873;&#25321;&#39033;&#30446;&#30340;&#23376;&#38598;&#22823;&#23567;$k$&#35774;&#32622;&#22312;$[2, n-1]$&#33539;&#22260;&#20869;&#33021;&#22815;&#26368;&#22823;&#21270;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#39033;&#30446;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#24456;&#22823;&#19968;&#37096;&#20998;&#20851;&#27880;&#30340;&#26159;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20294;&#26368;&#36817;&#26356;&#22810;&#22320;&#20851;&#27880;&#20110;&#20248;&#21270;&#20154;&#24037;&#21644;&#31639;&#27861;&#30340;&#32852;&#21512;&#24615;&#33021;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#20154;&#24037;&#21644;&#31639;&#27861;&#21512;&#20316;&#65292;&#22312;&#36825;&#31181;&#21512;&#20316;&#20013;&#65292;&#31639;&#27861;&#21487;&#20197;&#35775;&#38382;&#19968;&#32452;n&#20010;&#39033;&#30446;&#65292;&#24182;&#23558;&#22823;&#23567;&#20026;k&#30340;&#19968;&#20010;&#23376;&#38598;&#21576;&#29616;&#32473;&#20154;&#31867;&#65292;&#28982;&#21518;&#20154;&#31867;&#20174;&#36825;&#20123;k&#20010;&#39033;&#30446;&#20013;&#36873;&#25321;&#19968;&#20010;&#26368;&#32456;&#39033;&#30446;&#12290;&#36825;&#31181;&#24773;&#20917;&#21487;&#20197;&#27169;&#25311;&#20869;&#23481;&#25512;&#33616;&#12289;&#36335;&#24452;&#35268;&#21010;&#25110;&#20219;&#20309;&#31867;&#22411;&#30340;&#26631;&#27880;&#20219;&#21153;&#12290;&#30001;&#20110;&#20154;&#31867;&#21644;&#31639;&#27861;&#37117;&#23545;&#39033;&#30446;&#30340;&#30495;&#23454;&#25490;&#24207;&#26377;&#30528;&#19981;&#23436;&#32654;&#12289;&#26377;&#22122;&#38899;&#30340;&#20449;&#24687;&#65292;&#20851;&#38190;&#38382;&#39064;&#26159;&#65306;&#21738;&#20010;$k$&#20540;&#33021;&#26368;&#22823;&#21270;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#39033;&#30446;&#30340;&#27010;&#29575;&#65311;&#23545;&#20110;$k=1$&#65292;&#31639;&#27861;&#21333;&#29420;&#34892;&#21160;&#26102;&#24615;&#33021;&#26368;&#20248;&#65292;&#32780;&#23545;&#20110;$k=n$&#65292;&#20154;&#31867;&#21333;&#29420;&#34892;&#21160;&#26102;&#24615;&#33021;&#26368;&#20248;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#22810;&#20010;&#22122;&#38899;&#27169;&#22411;&#65292;&#23558;$k$&#35774;&#32622;&#22312;$[2, n-1]$&#33539;&#22260;&#20869;&#26159;&#26368;&#20248;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#21512;&#20316;&#26377;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historically, much of machine learning research has focused on the performance of the algorithm alone, but recently more attention has been focused on optimizing joint human-algorithm performance. Here, we analyze a specific type of human-algorithm collaboration where the algorithm has access to a set of $n$ items, and presents a subset of size $k$ to the human, who selects a final item from among those $k$. This scenario could model content recommendation, route planning, or any type of labeling task. Because both the human and algorithm have imperfect, noisy information about the true ordering of items, the key question is: which value of $k$ maximizes the probability that the best item will be ultimately selected? For $k=1$, performance is optimized by the algorithm acting alone, and for $k=n$ it is optimized by the human acting alone. Surprisingly, we show that for multiple of noise models, it is optimal to set $k \in [2, n-1]$ - that is, there are strict benefits to collaborating,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#22810;&#31934;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#21442;&#25968;&#21270;&#32593;&#26684;&#21270;&#21644;&#27169;&#25311;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34746;&#26059;&#31649;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#26032;&#30340;&#21453;&#24212;&#22120;&#35774;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20811;&#26381;&#20102;&#27809;&#26377;&#26799;&#24230;&#30340;&#38750;&#23616;&#37096;&#20248;&#21270;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2308.08841</link><description>&lt;p&gt;
&#36890;&#36807;CFD&#32806;&#21512;&#22810;&#31934;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#19979;&#30340;&#26032;&#21453;&#24212;&#22120;&#35774;&#35745;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Assisted Discovery of Novel Reactor Designs via CFD-Coupled Multi-fidelity Bayesian Optimisation. (arXiv:2308.08841v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24212;&#29992;&#22810;&#31934;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#21442;&#25968;&#21270;&#32593;&#26684;&#21270;&#21644;&#27169;&#25311;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34746;&#26059;&#31649;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#26032;&#30340;&#21453;&#24212;&#22120;&#35774;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20811;&#26381;&#20102;&#27809;&#26377;&#26799;&#24230;&#30340;&#38750;&#23616;&#37096;&#20248;&#21270;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28155;&#21152;&#21046;&#36896;&#25216;&#26415;&#20351;&#24471;&#26356;&#20808;&#36827;&#30340;&#21453;&#24212;&#22120;&#20960;&#20309;&#32467;&#26500;&#25104;&#20026;&#21487;&#33021;&#65292;&#20026;&#26356;&#22823;&#21644;&#26356;&#22797;&#26434;&#30340;&#35774;&#35745;&#31354;&#38388;&#25552;&#20379;&#20102;&#28508;&#22312;&#26426;&#20250;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#30830;&#23450;&#21644;&#20248;&#21270;&#26377;&#24076;&#26395;&#30340;&#37197;&#32622;&#23545;&#20110;&#29616;&#26377;&#30340;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#26041;&#27861;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#37492;&#20110;&#31639;&#27861;&#25913;&#36827;&#21644;&#28155;&#21152;&#21046;&#36896;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#34746;&#26059;&#31649;&#21442;&#25968;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21464;&#25130;&#38754;&#21644;&#34746;&#26059;&#36335;&#24452;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#31995;&#21015;&#39640;&#32500;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#12290;&#20026;&#20102;&#30830;&#20445;&#21487;&#34892;&#30340;&#38750;&#23616;&#37096;&#20248;&#21270;&#65292;&#22312;&#27809;&#26377;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24212;&#29992;&#22810;&#31934;&#24230;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25551;&#36848;&#20102;&#22810;&#20010;&#36830;&#32493;&#30340;&#31934;&#24230;&#65292;&#24182;&#19982;&#21442;&#25968;&#21270;&#32593;&#26684;&#21270;&#21644;&#27169;&#25311;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20302;&#36136;&#37327;&#12289;
&lt;/p&gt;
&lt;p&gt;
Additive manufacturing has enabled the production of more advanced reactor geometries, resulting in the potential for significantly larger and more complex design spaces. Identifying and optimising promising configurations within broader design spaces presents a significant challenge for existing human-centric design approaches. As such, existing parameterisations of coiled-tube reactor geometries are low-dimensional with expensive optimisation limiting more complex solutions. Given algorithmic improvements and the onset of additive manufacturing, we propose two novel coiled-tube parameterisations enabling the variation of cross-section and coil path, resulting in a series of high dimensional, complex optimisation problems. To ensure tractable, non-local optimisation where gradients are not available, we apply multi-fidelity Bayesian optimisation. Our approach characterises multiple continuous fidelities and is coupled with parameterised meshing and simulation, enabling lower quality, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#20998;&#31867;&#20808;&#39564;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2308.07200</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#20998;&#31867;&#20808;&#39564;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#25511;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Categorical Priors for Physics-Based Character Control. (arXiv:2308.07200v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#20998;&#31867;&#20808;&#39564;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23398;&#20064;&#21487;&#37325;&#29992;&#36816;&#21160;&#20808;&#39564;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#29983;&#25104;&#33258;&#28982;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#29289;&#29702;&#30340;&#35282;&#33394;&#65292;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#36816;&#21160;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#36861;&#36394;&#21644;&#27169;&#20223;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#36816;&#21160;&#21098;&#36753;&#30340;&#36924;&#30495;&#21160;&#20316;&#65292;&#20351;&#29992;&#31163;&#25955;&#20449;&#24687;&#29942;&#39048;&#65292;&#22914;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#20013;&#25152;&#37319;&#29992;&#30340;&#37027;&#26679;&#12290;&#35813;&#32467;&#26500;&#23558;&#26469;&#33258;&#36816;&#21160;&#21098;&#36753;&#30340;&#26368;&#30456;&#20851;&#20449;&#24687;&#21387;&#32553;&#25104;&#19968;&#20010;&#32039;&#20945;&#32780;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#21363;&#19968;&#20010;&#31163;&#25955;&#30340;&#21521;&#37327;&#37327;&#21270;&#30721;&#31354;&#38388;&#12290;&#36890;&#36807;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#20998;&#31867;&#20808;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#31354;&#38388;&#20013;&#30340;&#30721;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#36924;&#30495;&#30340;&#34892;&#20026;&#65292;&#31867;&#20284;&#20110;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20351;&#29992;VQ-VAE&#12290;&#34429;&#28982;&#36825;&#20010;&#20808;&#39564;&#20998;&#24067;&#21487;&#20197;&#36890;&#36807;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advances in learning reusable motion priors have demonstrated their effectiveness in generating naturalistic behaviors. In this paper, we propose a new learning framework in this paradigm for controlling physics-based characters with significantly improved motion quality and diversity over existing state-of-the-art methods. The proposed method uses reinforcement learning (RL) to initially track and imitate life-like movements from unstructured motion clips using the discrete information bottleneck, as adopted in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure compresses the most relevant information from the motion clips into a compact yet informative latent space, i.e., a discrete space over vector quantized codes. By sampling codes in the space from a trained categorical prior distribution, high-quality life-like behaviors can be generated, similar to the usage of VQ-VAE in computer vision. Although this prior distribution can be trained with the supervis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#38454;HSIC&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;Bayesian&#32593;&#32476;&#20013;&#35299;&#20915;&#20102;&#23616;&#37096;&#21464;&#37327;&#21516;&#26102;&#20855;&#26377;&#30452;&#25509;&#21644;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#30830;&#23450;&#23376;&#38598;&#21644;&#20004;&#38454;&#27573;&#31639;&#27861;&#26469;&#36827;&#34892;&#23616;&#37096;&#20462;&#27491;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.05969</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#38454;HSIC&#23398;&#20064;&#20855;&#26377;&#22686;&#37327;&#20449;&#24687;&#30340;&#38750;&#21442;&#25968;DAGs
&lt;/p&gt;
&lt;p&gt;
Learning nonparametric DAGs with incremental information via high-order HSIC. (arXiv:2308.05969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39640;&#38454;HSIC&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;Bayesian&#32593;&#32476;&#20013;&#35299;&#20915;&#20102;&#23616;&#37096;&#21464;&#37327;&#21516;&#26102;&#20855;&#26377;&#30452;&#25509;&#21644;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#30830;&#23450;&#23376;&#38598;&#21644;&#20004;&#38454;&#27573;&#31639;&#27861;&#26469;&#36827;&#34892;&#23616;&#37096;&#20462;&#27491;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BN&#65289;&#30340;&#22522;&#20110;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20840;&#23616;&#35780;&#20998;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#23616;&#37096;&#21464;&#37327;&#21516;&#26102;&#20855;&#26377;&#30452;&#25509;&#21644;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#65292;&#37027;&#20040;&#22522;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#23558;&#24573;&#30053;&#20855;&#26377;&#38388;&#25509;&#20381;&#36182;&#20851;&#31995;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#36793;&#32536;&#65292;&#20854;&#24471;&#20998;&#23567;&#20110;&#20855;&#26377;&#30452;&#25509;&#20381;&#36182;&#20851;&#31995;&#30340;&#36793;&#32536;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30830;&#23450;&#23376;&#38598;&#30340;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#65292;&#20197;&#35782;&#21035;&#28508;&#22312;&#30340;DAG&#12290;&#36890;&#36807;&#21487;&#36776;&#35782;&#24615;&#26465;&#20214;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#21363;&#26368;&#20248;&#35843;&#25972;&#65288;OT&#65289;&#31639;&#27861;&#65292;&#20197;&#22312;&#20840;&#23616;&#20248;&#21270;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#23616;&#37096;&#20462;&#27491;&#12290;&#22312;&#26368;&#20248;&#38454;&#27573;&#65292;&#22522;&#20110;&#19968;&#38454;Hilbert-Schmidt&#29420;&#31435;&#24615;&#20934;&#21017;&#65288;HSIC&#65289;&#30340;&#20248;&#21270;&#38382;&#39064;&#32473;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#30340;&#39592;&#26550;&#20316;&#20026;&#21021;&#22987;&#30830;&#23450;&#30340;&#29238;&#33410;&#28857;&#23376;&#38598;&#12290;&#22312;&#35843;&#25972;&#38454;&#27573;&#65292;&#26681;&#25454;&#39640;&#38454;HSIC&#30340;&#29702;&#35770;&#35777;&#26126;&#22686;&#37327;&#29305;&#24615;&#65292;&#23545;&#39592;&#26550;&#36827;&#34892;&#23616;&#37096;&#35843;&#25972;&#65292;&#21253;&#25324;&#21024;&#38500;&#12289;&#28155;&#21152;&#21644;DAG&#26684;&#24335;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based methods for learning Bayesain networks(BN) aim to maximizing the global score functions. However, if local variables have direct and indirect dependence simultaneously, the global optimization on score functions misses edges between variables with indirect dependent relationship, of which scores are smaller than those with direct dependent relationship. In this paper, we present an identifiability condition based on a determined subset of parents to identify the underlying DAG. By the identifiability condition, we develop a two-phase algorithm namely optimal-tuning (OT) algorithm to locally amend the global optimization. In the optimal phase, an optimization problem based on first-order Hilbert-Schmidt independence criterion (HSIC) gives an estimated skeleton as the initial determined parents subset. In the tuning phase, the skeleton is locally tuned by deletion, addition and DAG-formalization strategies using the theoretically proved incremental properties of high-order HS
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#30340;&#29983;&#29289;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;30&#19975;&#31181;&#20505;&#36873;&#33647;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#25351;&#32441;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.01921</link><description>&lt;p&gt;
&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#29983;&#29289;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats. (arXiv:2308.01921v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01921
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#65292;&#29992;&#20110;&#24555;&#36895;&#24212;&#23545;&#26410;&#26469;&#30340;&#29983;&#29289;&#23041;&#32961;&#12290;&#36890;&#36807;&#21033;&#29992;&#21253;&#21547;30&#19975;&#31181;&#20505;&#36873;&#33647;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#25351;&#32441;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37197;&#20307;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#33647;&#29289;&#20998;&#23376;&#24555;&#36895;&#31579;&#36873;&#26159;&#33647;&#29289;&#21457;&#29616;&#31649;&#32447;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#22270;&#31070;&#32463;&#25351;&#32441;&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#39640;&#36890;&#37327;&#21644;&#39640;&#20934;&#30830;&#24615;&#20998;&#23376;&#23545;&#25509;&#20195;&#29702;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#32422;30&#19975;&#31181;&#33647;&#29289;&#20505;&#36873;&#29289;&#21644;23&#20010;&#20896;&#29366;&#30149;&#27602;&#34507;&#30333;&#38774;&#30340;COVID-19&#33647;&#29289;&#23545;&#25509;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#22270;&#31070;&#32463;&#25351;&#32441;&#23545;&#25509;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#36890;&#37327;&#34394;&#25311;COVID-19&#33647;&#29289;&#31579;&#36873;&#12290;&#22270;&#31070;&#32463;&#25351;&#32441;&#27169;&#22411;&#22312;&#23545;&#25509;&#24471;&#20998;&#19978;&#20855;&#26377;&#24456;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#23545;&#22823;&#22810;&#25968;&#23545;&#25509;&#38774;&#28857;&#30340;&#22343;&#26041;&#35823;&#24046;&#20302;&#20110;0.21 kcal/mol&#65292;&#30456;&#27604;&#20256;&#32479;&#22278;&#24418;&#25351;&#32441;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#20026;&#20102;&#20351;&#31070;&#32463;&#25351;&#32441;&#36866;&#29992;&#20110;&#26410;&#30693;&#30340;&#38774;&#28857;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#38774;&#28857;&#19978;&#35757;&#32451;&#30340;&#21487;&#36716;&#31227;&#30340;&#22270;&#31070;&#32463;&#25351;&#32441;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast screening of drug molecules based on the ligand binding affinity is an important step in the drug discovery pipeline. Graph neural fingerprint is a promising method for developing molecular docking surrogates with high throughput and great fidelity. In this study, we built a COVID-19 drug docking dataset of about 300,000 drug candidates on 23 coronavirus protein targets. With this dataset, we trained graph neural fingerprint docking models for high-throughput virtual COVID-19 drug screening. The graph neural fingerprint models yield high prediction accuracy on docking scores with the mean squared error lower than $0.21$ kcal/mol for most of the docking targets, showing significant improvement over conventional circular fingerprint methods. To make the neural fingerprints transferable for unknown targets, we also propose a transferable graph neural fingerprint method trained on multiple targets. With comparable accuracy to target-specific graph neural fingerprint models, the transf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#30417;&#25511;&#35270;&#39057;&#36755;&#20837;&#36827;&#34892;&#29359;&#32618;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;Jetson&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#36816;&#34892;&#12290;&#36825;&#26159;&#23545;Jetson&#24179;&#21488;&#22312;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#30340;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2307.16834</link><description>&lt;p&gt;
&#20351;&#29992;&#31471;&#21040;&#31471;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#26469;&#22522;&#20934;&#27979;&#35797;Jetson&#36793;&#32536;&#35774;&#22791;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System. (arXiv:2307.16834v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#36890;&#36807;&#20174;&#30417;&#25511;&#35270;&#39057;&#36755;&#20837;&#36827;&#34892;&#29359;&#32618;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#22312;&#22810;&#20010;Jetson&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#36816;&#34892;&#12290;&#36825;&#26159;&#23545;Jetson&#24179;&#21488;&#22312;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#24179;&#21488;&#65292;&#29305;&#21035;&#26159;&#30828;&#20214;&#21152;&#36895;&#65292;&#26174;&#30528;&#24433;&#21709;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#21019;&#26032;&#23558;&#20154;&#31867;&#21171;&#21160;&#36716;&#21270;&#20026;&#33258;&#21160;&#21270;&#30340;&#26234;&#33021;&#31995;&#32479;&#65292;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#29289;&#32852;&#32593;&#21644;&#35768;&#22810;&#20854;&#20182;&#26377;&#37325;&#22823;&#24433;&#21709;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;NVIDIA&#30340;Jetson&#24179;&#21488;&#26159;&#22312;&#25191;&#34892;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26041;&#38754;&#33021;&#22815;&#25552;&#20379;&#33021;&#25928;&#21644;&#21534;&#21520;&#29575;&#26368;&#20339;&#24615;&#33021;&#30340;&#20808;&#39537;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#22823;&#37096;&#20998;&#22522;&#20934;&#27979;&#35797;&#20998;&#26512;&#37117;&#26159;&#22522;&#20110;2D&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27599;&#20010;&#27604;&#36739;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#20174;&#30417;&#25511;&#35270;&#39057;&#36755;&#20837;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#35270;&#39057;&#30340;&#29359;&#32618;&#29616;&#22330;&#24322;&#24120;&#26816;&#27979;&#31995;&#32479;&#65292;&#24182;&#23558;&#35813;&#31995;&#32479;&#37096;&#32626;&#22312;&#22810;&#20010;Jetson&#36793;&#32536;&#35774;&#22791;&#19978;&#65288;Nano&#12289;AGX Xavier&#12289;Orin Nano&#65289;&#12290;&#27604;&#36739;&#20998;&#26512;&#21253;&#25324;&#23558;Torch-TensorRT&#38598;&#25104;&#20026;&#36719;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative enhancement in embedded system platforms, specifically hardware accelerations, significantly influence the application of deep learning in real-world scenarios. These innovations translate human labor efforts into automated intelligent systems employed in various areas such as autonomous driving, robotics, Internet-of-Things (IoT), and numerous other impactful applications. NVIDIA's Jetson platform is one of the pioneers in offering optimal performance regarding energy efficiency and throughput in the execution of deep learning algorithms. Previously, most benchmarking analysis was based on 2D images with a single deep learning model for each comparison result. In this paper, we implement an end-to-end video-based crime-scene anomaly detection system inputting from surveillance videos and the system is deployed and completely operates on multiple Jetson edge devices (Nano, AGX Xavier, Orin Nano). The comparison analysis includes the integration of Torch-TensorRT as a softwar
&lt;/p&gt;</description></item><item><title>GOKU-UI&#26159;&#19968;&#31181;&#26222;&#36866;&#25512;&#29702;&#30340;SciML&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#21644;&#22810;&#23556;&#20987;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21253;&#25324;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#22312;&#20869;&#30340;&#21508;&#31181;&#24494;&#20998;&#26041;&#31243;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#23427;&#20855;&#26377;&#26174;&#33879;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#35777;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.05735</link><description>&lt;p&gt;
GOKU-UI&#65306;&#36890;&#36807;&#20851;&#27880;&#21147;&#21644;&#22810;&#23556;&#20987;&#23454;&#29616;&#36830;&#32493;&#26102;&#38388;&#29983;&#25104;&#27169;&#22411;&#30340;&#26222;&#36866;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models. (arXiv:2307.05735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05735
&lt;/p&gt;
&lt;p&gt;
GOKU-UI&#26159;&#19968;&#31181;&#26222;&#36866;&#25512;&#29702;&#30340;SciML&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#21644;&#22810;&#23556;&#20987;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#21253;&#25324;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#22312;&#20869;&#30340;&#21508;&#31181;&#24494;&#20998;&#26041;&#31243;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#23427;&#20855;&#26377;&#26174;&#33879;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#23454;&#35777;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#26159;&#19968;&#20010;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#23427;&#23558;&#39046;&#22495;&#24863;&#30693;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#19982;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GOKU-UI&#65292;&#36825;&#26159;SciML&#29983;&#25104;&#27169;&#22411;GOKU-nets&#30340;&#19968;&#31181;&#28436;&#36827;&#12290;GOKU-UI&#25193;&#23637;&#20102;&#21407;&#22987;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#23558;&#20854;&#20182;&#31867;&#21035;&#30340;&#24494;&#20998;&#26041;&#31243;&#65292;&#22914;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#65292;&#34701;&#20837;&#20854;&#20013;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#26426;&#21046;&#21644;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#26032;&#22411;&#22810;&#23556;&#20987;&#35757;&#32451;&#31574;&#30053;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#30340;&#12289;&#21363;&#26080;&#22788;&#19981;&#22312;&#30340;&#25512;&#29702;&#12290;&#36825;&#20123;&#25913;&#36827;&#20351;&#20854;&#22312;&#37325;&#24314;&#21644;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#35777;&#25968;&#25454;&#30340;&#35780;&#20272;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21363;&#20351;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#32553;&#23567;&#20102;32&#20493;&#65292;GOKU-UI&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#65292;&#20984;&#26174;&#20854;&#20986;&#33394;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#24403;&#24212;&#29992;&#20110;&#23454;&#35777;&#30340;&#20154;&#33041;&#25968;&#25454;&#26102;&#65292;&#21516;&#26102;&#34701;&#21512;&#20102;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;GOKU-UI&#20063;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific Machine Learning (SciML) is a burgeoning field that synergistically combines domain-aware and interpretable models with agnostic machine learning techniques. In this work, we introduce GOKU-UI, an evolution of the SciML generative model GOKU-nets. The GOKU-UI broadens the original model's spectrum to incorporate other classes of differential equations, such as Stochastic Differential Equations (SDEs), and integrates a distributed, i.e. ubiquitous, inference through attention mechanisms and a novel multiple shooting training strategy in the latent space. These enhancements have led to a significant increase in its performance in both reconstruction and forecast tasks, as demonstrated by our evaluation of simulated and empirical data. Specifically, GOKU-UI outperformed all baseline models on synthetic datasets even with a training set 32-fold smaller, underscoring its remarkable data efficiency. Furthermore, when applied to empirical human brain data, while incorporating stoch
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#21442;&#25968;&#21270;&#30340;BRDFs&#65292;&#20026;&#33402;&#26415;&#24615;&#22320;&#21019;&#20316;3D&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15679</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#21442;&#25968;&#21270;BRDFs
&lt;/p&gt;
&lt;p&gt;
Generating Parametric BRDFs from Natural Language Descriptions. (arXiv:2306.15679v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15679
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#21442;&#25968;&#21270;&#30340;BRDFs&#65292;&#20026;&#33402;&#26415;&#24615;&#22320;&#21019;&#20316;3D&#29615;&#22659;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#24615;&#22320;&#21019;&#20316;3D&#29615;&#22659;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#29087;&#32451;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#35299;&#20915;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#29983;&#25104;&#32593;&#26684;&#12289;&#25490;&#21015;&#20960;&#20309;&#12289;&#21512;&#25104;&#32441;&#29702;&#31561;&#26041;&#38754;&#24050;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25913;&#36827;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#21452;&#21521;&#21453;&#23556;&#20998;&#24067;&#20989;&#25968;&#65288;BRDFs&#65289;&#12290; BRDFs&#26159;&#34920;&#24449;&#20809;&#19982;&#34920;&#38754;&#26448;&#26009;&#30456;&#20114;&#20316;&#29992;&#30340;&#22235;&#32500;&#27010;&#29575;&#20998;&#24067;&#12290;&#23427;&#20204;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#21442;&#25968;&#21270;&#30340;&#24418;&#24335;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#21015;&#20030;&#27599;&#23545;&#20837;&#23556;&#21644;&#20986;&#23556;&#35282;&#24230;&#30340;&#27010;&#29575;&#23494;&#24230;&#26469;&#34920;&#31034;&#12290;&#21069;&#32773;&#36866;&#29992;&#20110;&#33402;&#26415;&#32534;&#36753;&#65292;&#32780;&#21518;&#32773;&#21017;&#29992;&#20110;&#27979;&#37327;&#23454;&#38469;&#26448;&#26009;&#30340;&#22806;&#35266;&#12290;&#35768;&#22810;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#20174;&#26448;&#26009;&#22270;&#20687;&#20551;&#35774;BRDF&#27169;&#22411;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#20174;&#26448;&#26009;&#30340;&#25991;&#26412;&#25551;&#36848;&#21040;&#21442;&#25968;&#21270;BRDF&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
Artistic authoring of 3D environments is a laborious enterprise that also requires skilled content creators. There have been impressive improvements in using machine learning to address different aspects of generating 3D content, such as generating meshes, arranging geometry, synthesizing textures, etc. In this paper we develop a model to generate Bidirectional Reflectance Distribution Functions (BRDFs) from descriptive textual prompts. BRDFs are four dimensional probability distributions that characterize the interaction of light with surface materials. They are either represented parametrically, or by tabulating the probability density associated with every pair of incident and outgoing angles. The former lends itself to artistic editing while the latter is used when measuring the appearance of real materials. Numerous works have focused on hypothesizing BRDF models from images of materials. We learn a mapping from textual descriptions of materials to parametric BRDFs. Our model is f
&lt;/p&gt;</description></item><item><title>EmbodiedGPT&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#39044;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#36171;&#20104;&#20855;&#26377;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#25191;&#34892;&#33021;&#21147;&#30340;&#23454;&#20307;&#20195;&#29702;&#20154;&#12290;</title><link>http://arxiv.org/abs/2305.15021</link><description>&lt;p&gt;
EmbodiedGPT: &#36890;&#36807;&#24605;&#32500;&#38142;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. (arXiv:2305.15021v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15021
&lt;/p&gt;
&lt;p&gt;
EmbodiedGPT&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#39044;&#35757;&#32451;&#30340;&#26041;&#24335;&#65292;&#36171;&#20104;&#20855;&#26377;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#25191;&#34892;&#33021;&#21147;&#30340;&#23454;&#20307;&#20195;&#29702;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36171;&#20104;&#20855;&#26377;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#25191;&#34892;&#33021;&#21147;&#30340;&#23454;&#20307;&#20195;&#29702;&#20154;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;EmbodiedGPT&#65292;&#29992;&#20110;&#23454;&#20307;&#26234;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20570;&#20986;&#20102;&#20197;&#19979;&#21162;&#21147;&#65306;&#65288;i&#65289;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23884;&#20837;&#24335;&#35268;&#21010;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;EgoCOT&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;Ego4D&#25968;&#25454;&#38598;&#20013;&#31934;&#36873;&#30340;&#35270;&#39057;&#21644;&#30456;&#24212;&#30340;&#39640;&#36136;&#37327;&#35821;&#35328;&#25351;&#20196;&#32452;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#27169;&#24335;&#29983;&#25104;&#19968;&#31995;&#21015;&#23376;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#23884;&#20837;&#24335;&#35268;&#21010;&#12290;&#65288;ii&#65289;&#25105;&#20204;&#25512;&#20986;&#20102;&#19968;&#31181;&#39640;&#36136;&#37327;&#35745;&#21010;&#29983;&#25104;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#32512;&#35843;&#20248;&#23558;7B&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35843;&#25972;&#21040;EgoCOT&#25968;&#25454;&#38598;&#19978;&#12290;&#65288;iii&#65289;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;LLM&#29983;&#25104;&#30340;&#35745;&#21010;&#20013;&#25552;&#21462;&#19982;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#30340;&#33539;&#20363;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments. In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the "Chain of Thoughts" mode for effective embodied planning. (ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated pla
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11322</link><description>&lt;p&gt;
SpikeCP: &#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#24310;&#36831;&#33258;&#36866;&#24212;&#21487;&#38752;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11322
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#26497;&#38480;&#39044;&#27979;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#20174;&#32780;&#33410;&#32422;&#33021;&#28304;&#19982;&#25552;&#39640;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#36890;&#36807;&#20869;&#37096;&#20107;&#20214;&#39537;&#21160;&#30340;&#31070;&#32463;&#21160;&#24577;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20854;&#33021;&#37327;&#28040;&#32791;&#21462;&#20915;&#20110;&#36755;&#20837;&#28436;&#31034;&#26399;&#38388;&#31070;&#32463;&#20803;&#20043;&#38388;&#20132;&#25442;&#30340;&#33033;&#20914;&#25968;&#37327;&#12290;&#22312;&#20856;&#22411;&#30340;SNN&#20998;&#31867;&#22120;&#23454;&#29616;&#20013;&#65292;&#20915;&#31574;&#26159;&#22312;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#34987;&#22788;&#29702;&#21518;&#20135;&#29983;&#30340;&#65292;&#23548;&#33268;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#27700;&#24179;&#22312;&#36755;&#20837;&#20043;&#38388;&#26159;&#30456;&#23545;&#22343;&#21248;&#30340;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;SNN&#21487;&#26681;&#25454;&#27599;&#20010;&#31034;&#20363;&#30340;&#38590;&#24230;&#26469;&#23450;&#21046;&#25512;&#26029;&#24310;&#36831; - &#20197;&#21450;&#38543;&#20043;&#32780;&#26469;&#30340;&#33021;&#32791; - &#36890;&#36807;&#22312;SNN&#27169;&#22411;&#36275;&#22815;&#8220;&#33258;&#20449;&#8221;&#26102;&#20135;&#29983;&#26089;&#26399;&#20915;&#31574;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neural networks (SNNs) process time-series data via internal event-driven neural dynamics whose energy consumption depends on the number of spikes exchanged between neurons over the course of the input presentation. In typical implementations of an SNN classifier, decisions are produced after the entire input sequence has been processed, resulting in latency and energy consumption levels that are fairly uniform across inputs. Recently introduced delay-adaptive SNNs tailor the inference latency -- and, with it, the energy consumption -- to the difficulty of each example, by producing an early decision when the SNN model is sufficiently ``confident''. In this paper, we start by observing that, as an SNN processes input samples, its classification decisions tend to be first under-confident and then over-confident with respect to the decision's ground-truth, unknown, test accuracy. This makes it difficult to determine a stopping time that ensures a desired level of accuracy. To add
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;Deep Spatiotemporal Clustering (DSC) &#65292;&#29992;&#20110;&#20351;&#29992;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#39640;&#32500;&#26102;&#31354;&#25968;&#25454;&#30340;&#26102;&#38388;&#32858;&#31867;&#65292;DSC&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#38598;&#25104;CNN-RNN&#23618;&#23398;&#20064;&#26102;&#31354;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20248;&#21270;&#32858;&#31867;&#25439;&#22833;&#21644;&#25968;&#25454;&#37325;&#24314;&#25439;&#22833;&#20197;&#25913;&#21892;&#32858;&#31867;&#20998;&#37197;&#21644;&#38750;&#32447;&#24615;&#37325;&#24314;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14541</link><description>&lt;p&gt;
&#28145;&#24230;&#26102;&#31354;&#32858;&#31867;&#65306;&#22810;&#32500;&#27668;&#20505;&#25968;&#25454;&#30340;&#20020;&#26102;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Spatiotemporal Clustering: A Temporal Clustering Approach for Multi-dimensional Climate Data. (arXiv:2304.14541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14541
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;Deep Spatiotemporal Clustering (DSC) &#65292;&#29992;&#20110;&#20351;&#29992;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#39640;&#32500;&#26102;&#31354;&#25968;&#25454;&#30340;&#26102;&#38388;&#32858;&#31867;&#65292;DSC&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#38598;&#25104;CNN-RNN&#23618;&#23398;&#20064;&#26102;&#31354;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20248;&#21270;&#32858;&#31867;&#25439;&#22833;&#21644;&#25968;&#25454;&#37325;&#24314;&#25439;&#22833;&#20197;&#25913;&#21892;&#32858;&#31867;&#20998;&#37197;&#21644;&#38750;&#32447;&#24615;&#37325;&#24314;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#23545;&#39640;&#32500;&#26102;&#31354;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#26159;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20351;&#29992;&#19981;&#21516;&#30340;&#30456;&#20284;&#24615;&#21644;&#36317;&#31163;&#20989;&#25968;&#65292;&#20294;&#38598;&#20013;&#20110;&#25968;&#25454;&#30340;&#31354;&#38388;&#25110;&#26102;&#38388;&#29305;&#24449;&#12290;&#25105;&#20204;&#38598;&#20013;&#20110;&#20351;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#32852;&#21512;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;Deep Spatiotemporal Clustering (DSC)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#39640;&#32500;&#26102;&#31354;&#25968;&#25454;&#30340;&#26102;&#38388;&#32858;&#31867;&#12290;&#21463;U-net&#26550;&#26500;&#21551;&#21457;&#65292;DSC&#21033;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#38598;&#25104;CNN-RNN&#23618;&#23398;&#20064;&#26102;&#31354;&#25968;&#25454;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290; DSC&#36824;&#21253;&#25324;&#19968;&#20010;&#29420;&#29305;&#30340;&#28508;&#22312;&#34920;&#31034;&#23618;&#65292;&#29992;&#20110;&#20351;&#29992;&#23398;&#29983;t&#20998;&#24067;&#36827;&#34892;&#32858;&#31867;&#20998;&#37197;&#12290;&#36890;&#36807;&#21516;&#26102;&#20248;&#21270;&#32858;&#31867;&#25439;&#22833;&#21644;&#25968;&#25454;&#37325;&#24314;&#25439;&#22833;&#65292;&#35813;&#31639;&#27861;&#36880;&#28176;&#25913;&#21892;&#32858;&#31867;&#20998;&#37197;&#21644;&#38750;&#32447;&#24615;&#37325;&#24314;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering high-dimensional spatiotemporal data using an unsupervised approach is a challenging problem for many data-driven applications. Existing state-of-the-art methods for unsupervised clustering use different similarity and distance functions but focus on either spatial or temporal features of the data. Concentrating on joint deep representation learning of spatial and temporal features, we propose Deep Spatiotemporal Clustering (DSC), a novel algorithm for the temporal clustering of high-dimensional spatiotemporal data using an unsupervised deep learning method. Inspired by the U-net architecture, DSC utilizes an autoencoder integrating CNN-RNN layers to learn latent representations of the spatiotemporal data. DSC also includes a unique layer for cluster assignment on latent representations that uses the Student's t-distribution. By optimizing the clustering loss and data reconstruction loss simultaneously, the algorithm gradually improves clustering assignments and the nonlinea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#38647;&#36798;&#22238;&#27874;&#22806;&#25512;&#31639;&#27861;TempEE&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#26102;&#31354;&#30456;&#20851;&#29305;&#24449;&#21644;Transformer&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#22810;&#24103;&#22238;&#27874;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20934;&#30830;&#22320;&#34920;&#31034;&#20102;&#38477;&#27700;&#30340;&#38750;&#24179;&#31283;&#36816;&#21160;&#36807;&#31243;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2304.14131</link><description>&lt;p&gt;
TempEE&#65306;&#22522;&#20110;&#26102;&#31354;&#24179;&#34892;Transformer&#23454;&#29616;&#38647;&#36798;&#22238;&#27874;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
TempEE: Temporal-Spatial Parallel Transformer for Radar Echo Extrapolation Beyond Auto-Regression. (arXiv:2304.14131v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#38647;&#36798;&#22238;&#27874;&#22806;&#25512;&#31639;&#27861;TempEE&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#26102;&#31354;&#30456;&#20851;&#29305;&#24449;&#21644;Transformer&#25216;&#26415;&#65292;&#36890;&#36807;&#20174;&#22810;&#24103;&#22238;&#27874;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20934;&#30830;&#22320;&#34920;&#31034;&#20102;&#38477;&#27700;&#30340;&#38750;&#24179;&#31283;&#36816;&#21160;&#36807;&#31243;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#35937;&#38647;&#36798;&#21453;&#23556;&#29575;&#25968;&#25454;&#65288;&#20063;&#31216;&#20026;&#22238;&#27874;&#65289;&#22312;&#39044;&#27979;&#38477;&#27700;&#21644;&#36827;&#34892;&#30701;&#26399;&#24378;&#38477;&#38632;&#30340;&#31934;&#30830;&#24555;&#36895;&#39044;&#27979;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38647;&#36798;&#22238;&#27874;&#22806;&#25512;&#31639;&#27861;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#39640;&#24230;&#21487;&#38752;&#19988;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#31639;&#27861;&#30340;&#21457;&#23637;&#21463;&#21040;&#19977;&#20010;&#20027;&#35201;&#29942;&#39048;&#30340;&#21046;&#32422;&#65306;&#32047;&#31215;&#35823;&#24046;&#25193;&#25955;&#12289;&#31232;&#30095;&#22238;&#27874;&#20998;&#24067;&#30340;&#19981;&#31934;&#30830;&#34920;&#31034;&#20197;&#21450;&#38750;&#24179;&#31283;&#36816;&#21160;&#36807;&#31243;&#30340;&#19981;&#20934;&#30830;&#25551;&#36848;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26102;&#31354;&#30456;&#20851;&#29305;&#24449;&#21644;Transformer&#25216;&#26415;&#30340;&#26032;&#22411;&#38647;&#36798;&#22238;&#27874;&#22806;&#25512;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#20174;&#22810;&#24103;&#22238;&#27874;&#22270;&#20687;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20934;&#30830;&#22320;&#34920;&#31034;&#20102;&#38477;&#27700;&#30340;&#38750;&#24179;&#31283;&#36816;&#21160;&#36807;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;Temporal-Spatial Parallel Transformer&#65288;TempEE&#65289;&#65292;&#30830;&#20445;&#20102;&#22312;&#38647;&#36798;&#22238;&#27874;&#22806;&#25512;&#20013;&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#20811;&#26381;&#20102;&#20256;&#32479;NWP&#27169;&#22411;&#21644;&#33258;&#22238;&#24402;&#31639;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#22312;&#22238;&#27874;&#25345;&#32493;&#26102;&#38388;&#20043;&#22806;&#36827;&#34892;&#22806;&#25512;&#12290;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20854;&#23454;&#38469;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The meteorological radar reflectivity data, also known as echo, plays a crucial role in predicting precipitation and enabling accurate and fast forecasting of short-term heavy rainfall without the need for complex Numerical Weather Prediction (NWP) model. Compared to conventional model, Deep Learning (DL)-based radar echo extrapolation algorithms are more effective and efficient. However, the development of highly reliable and generalized algorithms is hindered by three main bottlenecks: cumulative error spreading, imprecise representation of sparse echo distribution, and inaccurate description of non-stationary motion process. To address these issues, this paper presents a novel radar echo extrapolation algorithm that utilizes temporal-spatial correlation features and the Transformer technology. The algorithm extracts features from multi-frame echo images that accurately represent non-stationary motion processes for precipitation prediction. The proposed algorithm uses a novel paralle
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#25104;&#21151;&#22320;&#22312;32&#20301;Cortex-M&#24494;&#25511;&#21046;&#22120;&#19978;&#20351;&#29992;&#28608;&#20809;&#27880;&#20837;&#36827;&#34892;&#20102;&#27604;&#29305;&#32763;&#36716;&#25915;&#20987;&#65292;&#24378;&#35843;&#20102;&#20856;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32570;&#20047;&#24378;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12876</link><description>&lt;p&gt;
&#22522;&#20110;&#28608;&#20809;&#27880;&#20837;&#30340;&#21442;&#25968;&#25915;&#20987;&#23545;&#23884;&#20837;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Parameter-based Attacks against Embedded Neural Networks with Laser Injection. (arXiv:2304.12876v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#25104;&#21151;&#22320;&#22312;32&#20301;Cortex-M&#24494;&#25511;&#21046;&#22120;&#19978;&#20351;&#29992;&#28608;&#20809;&#27880;&#20837;&#36827;&#34892;&#20102;&#27604;&#29305;&#32763;&#36716;&#25915;&#20987;&#65292;&#24378;&#35843;&#20102;&#20856;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32570;&#20047;&#24378;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#35748;&#35777;&#34892;&#21160;&#30340;&#21040;&#26469;&#65292;&#21152;&#19978;&#27169;&#22411;&#22312;&#22810;&#20010;&#30828;&#20214;&#24179;&#21488;&#19978;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#65292;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#35780;&#20272;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24037;&#20316;&#38598;&#20013;&#22312;&#22522;&#20110;API&#30340;&#25915;&#20987;&#19978;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35270;&#20026;&#32431;&#31639;&#27861;&#25277;&#35937;&#12290;&#28982;&#32780;&#65292;&#26032;&#30340;&#22522;&#20110;&#23454;&#29616;&#30340;&#23041;&#32961;&#24050;&#34987;&#25581;&#31034;&#65292;&#24378;&#35843;&#25552;&#20986;&#23454;&#36341;&#21644;&#22522;&#20110;&#20223;&#30495;&#30340;&#26041;&#27861;&#65292;&#20197;&#36866;&#24403;&#35780;&#20272;&#27169;&#22411;&#30340;&#24378;&#20581;&#24615;&#12290;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#22522;&#20110;&#21442;&#25968;&#30340;&#25915;&#20987;&#65288;&#20363;&#22914;&#27604;&#29305;&#32763;&#36716;&#25915;&#20987;&#65289;&#65292;&#24403;&#20869;&#37096;&#23384;&#20648;&#22120;&#20013;&#30340;&#21442;&#25968;&#34987;&#31934;&#30830;&#21644;&#26368;&#20248;&#22320;&#26356;&#25913;&#26102;&#65292;&#24378;&#35843;&#20102;&#20856;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#32570;&#20047;&#24378;&#20581;&#24615;&#12290;&#38024;&#23545;&#23433;&#20840;&#27979;&#35797;&#30446;&#30340;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#22312;32&#20301;Cortex-M&#24494;&#25511;&#21046;&#22120;&#19978;&#20351;&#29992;&#28608;&#20809;&#25925;&#38556;&#27880;&#20837;&#65292;&#23454;&#38469;&#25253;&#21578;&#20102;&#25104;&#21151;&#30340;BFA&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Upcoming certification actions related to the security of machine learning (ML) based systems raise major evaluation challenges that are amplified by the large-scale deployment of models in many hardware platforms. Until recently, most of research works focused on API-based attacks that consider a ML model as a pure algorithmic abstraction. However, new implementation-based threats have been revealed, emphasizing the urgency to propose both practical and simulation-based methods to properly evaluate the robustness of models. A major concern is parameter-based attacks (such as the Bit-Flip Attack, BFA) that highlight the lack of robustness of typical deep neural network models when confronted by accurate and optimal alterations of their internal parameters stored in memory. Setting in a security testing purpose, this work practically reports, for the first time, a successful variant of the BFA on a 32-bit Cortex-M microcontroller using laser fault injection. It is a standard fault injec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#23545;&#27604;&#35843;&#25972;(MAE-CT)&#65292;&#21033;&#29992;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#65288;NNCLR&#65289;&#22312;&#26631;&#35760;&#25968;&#25454;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19979;&#28216;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#22270;&#20687;&#30340;&#20016;&#23500;&#29305;&#24449;&#32858;&#31867;&#25104;&#23545;&#35937;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.10520</link><description>&lt;p&gt;
&#23545;&#27604;&#35843;&#33410;: &#24110;&#21161;&#36951;&#24536;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#19968;&#28857;&#23567;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65306;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#23545;&#27604;&#35843;&#25972;(MAE-CT)&#65292;&#21033;&#29992;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#65288;NNCLR&#65289;&#22312;&#26631;&#35760;&#25968;&#25454;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#19979;&#28216;&#20998;&#31867;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;&#22270;&#20687;&#30340;&#20016;&#23500;&#29305;&#24449;&#32858;&#31867;&#25104;&#23545;&#35937;&#30340;&#35821;&#20041;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#22914;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36755;&#20837;&#30340;&#20016;&#23500;&#34920;&#31034;&#12290;&#20294;&#26159;&#65292;&#20026;&#20102;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#20016;&#23500;&#30340;&#29305;&#24449;&#19981;&#20165;&#25429;&#33719;&#20102;&#23545;&#35937;&#32780;&#19988;&#36824;&#21253;&#25324;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#32972;&#26223;&#65292;&#22240;&#27492;&#23427;&#20204;&#38656;&#35201;&#36275;&#22815;&#25968;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23454;&#20363;&#36776;&#21035;&#26041;&#27861;&#20391;&#37325;&#20110;&#23545;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#23558;MIM&#30340;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#19982;ID&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#32570;&#23569;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#19979;&#28216;&#20998;&#31867;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#23545;&#27604;&#35843;&#25972;&#65288;MAE-CT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39034;&#24207;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#65288;NNCLR&#65289;&#24212;&#29992;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;MAE&#12290;MAE-CT&#35843;&#25972;&#20102;&#20016;&#23500;&#30340;&#29305;&#24449;&#65292;&#20351;&#23427;&#20204;&#24418;&#25104;&#23545;&#35937;&#30340;&#35821;&#20041;&#32858;&#31867;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#26631;&#31614;&#12290;&#24212;&#29992;&#20110;&#22823;&#22411;&#21644;&#24040;&#22411;Vision Transformer&#65288;ViT&#65289;&#27169;&#22411;&#26102;&#65292;MAE-CT&#22312;&#32447;&#24615;&#25506;&#27979;&#65292;k-&#22343;&#20540;&#32858;&#31867;&#21644;&#21322;&#30417;&#30563;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#21305;&#37197;&#25110;&#36229;&#36234;&#20102;&#22312;ImageNet&#19978;&#35757;&#32451;&#30340;&#20808;&#21069;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE), efficiently learn a rich representation of the input. However, for adapting to downstream tasks, they require a sufficient amount of labeled data since their rich features capture not only objects but also less relevant image background. In contrast, Instance Discrimination (ID) methods focus on objects. In this work, we study how to combine the efficiency and scalability of MIM with the ability of ID to perform downstream classification in the absence of large amounts of labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning (MAE-CT), a sequential approach that applies Nearest Neighbor Contrastive Learning (NNCLR) to a pre-trained MAE. MAE-CT tunes the rich features such that they form semantic clusters of objects without using any labels. Applied to large and huge Vision Transformer (ViT) models, MAE-CT matches or excels previous self-supervised methods trained on ImageNet in linear probing, k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.09960</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#28508;&#22312;&#31354;&#38388;&#29702;&#35770;&#23545;&#24212;&#26032;&#20852;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#65292;&#35777;&#26126;&#20102;LLMs&#33021;&#22815;&#23436;&#25104;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24182;&#19981;&#26159;&#38543;&#26426;&#29983;&#25104;&#65292;&#32780;&#26159;&#20026;&#20102;&#20256;&#36882;&#20449;&#24687;&#12290;&#35821;&#35328;&#19982;&#20854;&#24213;&#23618;&#21547;&#20041;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#20851;&#32852;&#65292;&#22312;&#20854;&#30456;&#20851;&#24615;&#26041;&#38754;&#26377;&#30528;&#20005;&#37325;&#20559;&#24046;&#30340;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31232;&#30095;&#24615;&#65292;&#36825;&#20123;&#39640;&#23792;&#20540;&#24688;&#22909;&#19982;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#21305;&#37197;&#12290;&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#22823;&#27169;&#22411;&#19978;&#35757;&#32451;&#30340;LLMs&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#20197;&#31934;&#30830;&#35780;&#20272;&#35821;&#35328;&#30340;&#36793;&#32536;&#20998;&#24067;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#30340;&#25506;&#32034;&#32852;&#21512;&#20998;&#24067;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#26377;&#25928;&#25512;&#29702;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#20998;&#31867;&#20026;&#26126;&#30830;&#19982;{\epsilon}-&#27169;&#31946;&#65292;&#24182;&#25552;&#20986;&#23450;&#37327;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;LLMs&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24605;&#36335;&#21551;&#21457;&#20197;&#21450;&#26377;&#25928;&#25351;&#20196;&#24494;&#35843;&#65289;&#37117;&#21487;&#20197;&#24402;&#22240;&#20110;&#23545;&#31232;&#30095;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages are not created randomly but rather to communicate information. There is a strong association between languages and their underlying meanings, resulting in a sparse joint distribution that is heavily peaked according to their correlations. Moreover, these peak values happen to match with the marginal distribution of languages due to the sparsity. With the advent of LLMs trained on big data and large models, we can now precisely assess the marginal distribution of languages, providing a convenient means of exploring the sparse structures in the joint distribution for effective inferences. In this paper, we categorize languages as either unambiguous or {\epsilon}-ambiguous and present quantitative results to demonstrate that the emergent abilities of LLMs, such as language understanding, in-context learning, chain-of-thought prompting, and effective instruction fine-tuning, can all be attributed to Bayesian inference on the sparse joint distribution of languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21152;&#26435;&#36830;&#20307;&#32593;&#32476;&#23545;&#36827;&#23637;&#24615;MCI&#24739;&#32773;&#36827;&#34892;&#24207;&#25968;&#20998;&#31867;&#65292;&#20197;&#39044;&#27979;&#20182;&#20204;&#36317;&#31163;&#20005;&#37325;AD&#38454;&#27573;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.07097</link><description>&lt;p&gt;
&#26435;&#37325;&#36830;&#20307;&#32593;&#32476;&#29992;&#20110;&#20174;MRI&#22270;&#20687;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21457;&#30149;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Weighted Siamese Network to Predict the Time to Onset of Alzheimer's Disease from MRI Images. (arXiv:2304.07097v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21152;&#26435;&#36830;&#20307;&#32593;&#32476;&#23545;&#36827;&#23637;&#24615;MCI&#24739;&#32773;&#36827;&#34892;&#24207;&#25968;&#20998;&#31867;&#65292;&#20197;&#39044;&#27979;&#20182;&#20204;&#36317;&#31163;&#20005;&#37325;AD&#38454;&#27573;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#30196;&#21574;&#30151;&#26368;&#24120;&#35265;&#30340;&#21407;&#22240;&#65292;&#26159;&#19968;&#31181;&#36827;&#34892;&#24615;&#30142;&#30149;&#65292;&#20250;&#20808;&#20986;&#29616;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;MCI&#65289;&#12290;&#26089;&#26399;&#26816;&#27979;&#23545;&#20110;&#20570;&#20986;&#27835;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#35745;&#31639;&#26426;&#36741;&#21161;&#26816;&#27979;AD&#30340;&#25991;&#29486;&#32858;&#28966;&#20110;&#23558;&#33041;&#22270;&#20687;&#20998;&#31867;&#20026;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#20043;&#19968;&#65306;&#20581;&#24247;&#12289;MCI&#21644;AD&#65307;&#25110;&#23558;MCI&#24739;&#32773;&#20998;&#31867;&#20026;(1)&#36827;&#23637;&#65306;&#22312;&#32473;&#23450;&#30740;&#31350;&#26399;&#20869;&#30340;&#26410;&#26469;&#26816;&#26597;&#26102;&#38388;&#36827;&#23637;&#21040;AD&#30340;&#24739;&#32773;&#65292;&#20197;&#21450;(2)&#31283;&#23450;&#65306;&#25345;&#32493;&#20316;&#20026;MCI&#24739;&#32773;&#32780;&#20174;&#26410;&#36827;&#23637;&#20026;AD&#12290;&#36825;&#31181;&#26041;&#27861;&#38169;&#36807;&#20102;&#20934;&#30830;&#35782;&#21035;&#36827;&#23637;&#24615;MCI&#24739;&#32773;&#36712;&#36857;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;AD&#35782;&#21035;&#30340;&#33041;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#65292;&#23558;&#20854;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#24207;&#25968;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#39044;&#27979;&#24739;&#32773;&#36317;&#31163;&#20005;&#37325;AD&#38454;&#27573;&#26377;&#22810;&#36817;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#35745;&#21010;&#65288;ADNI&#65289;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#36827;&#23637;&#24615;MCI&#24739;&#32773;&#65292;&#24182;&#20351;&#29992;&#21152;&#26435;&#36830;&#20307;&#32593;&#32476;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's Disease (AD), which is the most common cause of dementia, is a progressive disease preceded by Mild Cognitive Impairment (MCI). Early detection of the disease is crucial for making treatment decisions. However, most of the literature on computer-assisted detection of AD focuses on classifying brain images into one of three major categories: healthy, MCI, and AD; or categorising MCI patients into one of (1) progressive: those who progress from MCI to AD at a future examination time during a given study period, and (2) stable: those who stay as MCI and never progress to AD. This misses the opportunity to accurately identify the trajectory of progressive MCI patients. In this paper, we revisit the brain image classification task for AD identification and re-frame it as an ordinal classification task to predict how close a patient is to the severe AD stage. To this end, we select progressive MCI patients from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset and co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21160;&#21147;&#21644;&#36895;&#24230;&#26469;&#20026;&#33258;&#34892;&#36710;&#27604;&#36187;&#20013;&#30340;&#27599;&#20010;&#39569;&#25163;&#25552;&#20379;&#21345;&#36335;&#37324;&#38656;&#27714;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.03778</link><description>&lt;p&gt;
&#38024;&#23545;Jumbo-Visma&#38431;&#21345;&#36335;&#37324;&#39044;&#27979;&#30340;&#31526;&#21512;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Conformal Regression in Calorie Prediction for Team Jumbo-Visma. (arXiv:2304.03778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31526;&#21512;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21160;&#21147;&#21644;&#36895;&#24230;&#26469;&#20026;&#33258;&#34892;&#36710;&#27604;&#36187;&#20013;&#30340;&#27599;&#20010;&#39569;&#25163;&#25552;&#20379;&#21345;&#36335;&#37324;&#38656;&#27714;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
UCI WorldTour&#36187;&#20107;&#26159;&#30007;&#23376;&#31934;&#33521;&#20844;&#36335;&#33258;&#34892;&#36710;&#27604;&#36187;&#30340;&#39030;&#32423;&#36187;&#20107;&#65292;&#23545;&#39569;&#34892;&#32773;&#30340;&#20307;&#33021;&#21644;&#32784;&#21147;&#36827;&#34892;&#32771;&#39564;&#12290;Jumbo-Visma&#38431;&#30340;&#25945;&#32451;&#20204;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#36127;&#36131;&#39044;&#27979;&#27599;&#20010;&#27604;&#36187;&#26085;&#21382;&#20013;&#33655;&#20848;&#38431;&#30340;&#27599;&#20301;&#39569;&#25163;&#30340;&#33021;&#37327;&#38656;&#27714;&#65292;&#20197;&#30830;&#20445;&#39569;&#25163;&#22312;&#27604;&#36187;&#36807;&#31243;&#20013;&#26377;&#36275;&#22815;&#30340;&#33021;&#37327;&#21644;&#36164;&#28304;&#26469;&#20445;&#25345;&#39640;&#27700;&#24179;&#34920;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26356;&#26377;&#25928;&#30340;&#39044;&#27979;&#39569;&#34892;&#27604;&#36187;&#33021;&#37327;&#38656;&#27714;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#36895;&#24230;&#21644;&#21160;&#21147;&#65292;&#20026;&#27599;&#20010;&#38454;&#27573;&#30340;&#27599;&#20010;&#20010;&#20307;&#39569;&#25163;&#25552;&#20379;&#21345;&#36335;&#37324;&#38656;&#27714;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
UCI WorldTour races, the premier men's elite road cycling tour, are grueling events that put riders' physical fitness and endurance to the test. The coaches of Team Jumbo-Visma have long been responsible for predicting the energy needs of each rider of the Dutch team for every race on the calendar. Those must be estimated to ensure riders have the energy and resources necessary to maintain a high level of performance throughout a race. This task, however, is both time-consuming and challenging, as it requires precise estimates of race speed and power output. Traditionally, the approach to predicting energy needs has relied on coaches' judgement and experience, but this method has its limitations and often leads to inaccurate predictions. In this paper, we propose a new, more effective approach to predicting energy needs for cycling races. By predicting the speed and power with regression models, we provide the coaches with calorie needs estimate for each individual rider per stage inst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20316;&#29289;&#20998;&#21106;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#28304;&#22495;&#30340;&#27169;&#22411;&#38598;&#21512;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#20316;&#29289;&#21644;&#29615;&#22659;&#26465;&#20214;&#30340;&#27867;&#21270;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.01029</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#22686;&#24378;&#20316;&#29289;&#20998;&#21106;&#30340;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization for Crop Segmentation with Knowledge Distillation. (arXiv:2304.01029v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20316;&#29289;&#20998;&#21106;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#28304;&#22495;&#30340;&#27169;&#22411;&#38598;&#21512;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#26032;&#30340;&#20316;&#29289;&#21644;&#29615;&#22659;&#26465;&#20214;&#30340;&#27867;&#21270;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31934;&#30830;&#20892;&#19994;&#36880;&#28176;&#26397;&#30528;&#33258;&#21160;&#21270;&#30340;&#26041;&#21521;&#21457;&#23637;&#65292;&#20197;&#25903;&#25345;&#19982;&#30000;&#38388;&#31649;&#29702;&#30456;&#20851;&#30340;&#25152;&#26377;&#27963;&#21160;&#12290;&#26381;&#21153;&#26426;&#22120;&#20154;&#22312;&#36825;&#19968;&#28436;&#21464;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#37096;&#32626;&#33021;&#22815;&#22312;&#30000;&#38388;&#23548;&#33322;&#24182;&#22312;&#26080;&#38656;&#20154;&#20026;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#30340;&#33258;&#20027;&#20195;&#29702;&#65292;&#20363;&#22914;&#30417;&#27979;&#12289;&#21943;&#27922;&#21644;&#25910;&#33719;&#12290;&#20026;&#20102;&#25191;&#34892;&#36825;&#20123;&#31934;&#30830;&#30340;&#21160;&#20316;&#65292;&#31227;&#21160;&#26426;&#22120;&#20154;&#38656;&#35201;&#19968;&#20010;&#23454;&#26102;&#24863;&#30693;&#31995;&#32479;&#65292;&#33021;&#22815;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#24182;&#22312;&#37326;&#22806;&#35782;&#21035;&#30446;&#26631;&#12290;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#65292;&#23545;&#26032;&#30340;&#20316;&#29289;&#21644;&#29615;&#22659;&#26465;&#20214;&#36827;&#34892;&#27867;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#24456;&#23569;&#26377;&#26631;&#35760;&#26679;&#26412;&#21487;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20316;&#29289;&#20998;&#21106;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#22686;&#24378;&#22495;&#27867;&#21270;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#28304;&#22495;&#19978;&#21333;&#29420;&#35757;&#32451;&#30340;&#27169;&#22411;&#38598;&#21512;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#26410;&#35265;&#30446;&#26631;&#22495;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, precision agriculture has gradually oriented farming closer to automation processes to support all the activities related to field management. Service robotics plays a predominant role in this evolution by deploying autonomous agents that can navigate fields while performing tasks without human intervention, such as monitoring, spraying, and harvesting. To execute these precise actions, mobile robots need a real-time perception system that understands their surroundings and identifies their targets in the wild. Generalizing to new crops and environmental conditions is critical for practical applications, as labeled samples are rarely available. In this paper, we investigate the problem of crop segmentation and propose a novel approach to enhance domain generalization using knowledge distillation. In the proposed framework, we transfer knowledge from an ensemble of models individually trained on source domains to a student model that can adapt to unseen target domains. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#23545;&#25968;S&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#26368;&#22810;&#21482;&#26377;&#19977;&#20010;&#19981;&#21160;&#28857;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#21644;&#29702;&#35770;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24517;&#35201;&#30340;&#26725;&#26753;&#12290;</title><link>http://arxiv.org/abs/2303.12814</link><description>&lt;p&gt;
&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#21160;&#28857;
&lt;/p&gt;
&lt;p&gt;
Fixed points of arbitrarily deep 1-dimensional neural networks. (arXiv:2303.12814v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20855;&#26377;&#23545;&#25968;S&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#26368;&#22810;&#21482;&#26377;&#19977;&#20010;&#19981;&#21160;&#28857;&#65292;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#21644;&#29702;&#35770;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24517;&#35201;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;$\mathbb{R}$&#19978;&#20855;&#26377;&#21512;&#25104;&#24615;&#19988;&#21253;&#21547;&#23545;&#25968;S&#22411;&#20989;&#25968;&#30340;&#26032;&#20989;&#25968;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#31867;&#26469;&#35777;&#26126;&#20855;&#26377;&#23545;&#25968;S&#22411;&#28608;&#27963;&#20989;&#25968;&#30340;&#20219;&#24847;&#28145;&#24230;&#30340;&#19968;&#32500;&#31070;&#32463;&#32593;&#32476;&#26368;&#22810;&#21482;&#26377;&#19977;&#20010;&#19981;&#21160;&#28857;&#12290;&#34429;&#28982;&#36825;&#26679;&#30340;&#31070;&#32463;&#32593;&#32476;&#36828;&#31163;&#23454;&#38469;&#24212;&#29992;&#65292;&#20294;&#25105;&#20204;&#33021;&#22815;&#23436;&#20840;&#29702;&#35299;&#23427;&#20204;&#30340;&#19981;&#21160;&#28857;&#65292;&#24182;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#21644;&#29702;&#35770;&#20043;&#38388;&#26500;&#24314;&#20102;&#19968;&#20010;&#24517;&#35201;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new class of functions on $\mathbb{R}$ that is closed under composition, and contains the logistic sigmoid function. We use this class to show that any 1-dimensional neural network of arbitrary depth with logistic sigmoid activation functions has at most three fixed points. While such neural networks are far from real world applications, we are able to completely understand their fixed points, providing a foundation to the much needed connection between application and theory of deep neural networks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#31649;&#29702;&#24494;&#30005;&#32593;&#20013;&#30340;&#33021;&#28304;&#20132;&#26131;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#23567;&#21270;&#30899;&#36275;&#36857;&#65292;&#21516;&#26102;&#24179;&#34913;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#20256;&#32479;&#33021;&#28304;&#30340;&#28040;&#36153;&#21644;&#29983;&#20135;&#65292;&#24182;&#32771;&#34385;&#33021;&#28304;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.08447</link><description>&lt;p&gt;
MAHTM&#65306;&#20998;&#23618;&#21487;&#20132;&#26131;&#24494;&#30005;&#32593;&#30340;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MAHTM: A Multi-Agent Framework for Hierarchical Transactive Microgrids. (arXiv:2303.08447v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#31649;&#29702;&#24494;&#30005;&#32593;&#20013;&#30340;&#33021;&#28304;&#20132;&#26131;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#23567;&#21270;&#30899;&#36275;&#36857;&#65292;&#21516;&#26102;&#24179;&#34913;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#20256;&#32479;&#33021;&#28304;&#30340;&#28040;&#36153;&#21644;&#29983;&#20135;&#65292;&#24182;&#32771;&#34385;&#33021;&#28304;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21487;&#21464;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#24182;&#20837;&#30005;&#32593;&#32473;&#31995;&#32479;&#36816;&#33829;&#21830;&#24102;&#26469;&#25361;&#25112;&#65292;&#20351;&#24471;&#22312;&#33021;&#28304;&#21487;&#29992;&#24615;&#12289;&#25104;&#26412;&#25215;&#21463;&#33021;&#21147;&#21644;&#27745;&#26579;&#21487;&#25511;&#24615;&#20043;&#38388;&#21462;&#24471;&#26368;&#20248;&#25240;&#34935;&#25104;&#20026;&#19968;&#39033;&#38590;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31649;&#29702;&#24494;&#30005;&#32593;&#33021;&#28304;&#20132;&#26131;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#65306;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#30899;&#36275;&#36857;&#30340;&#26041;&#24335;&#20248;&#21270;&#21487;&#29992;&#36164;&#28304;&#30340;&#21033;&#29992;&#65292;&#24182;&#20351;&#25152;&#26377;&#21033;&#30410;&#30456;&#20851;&#26041;&#21463;&#30410;&#12290;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#30001;&#19977;&#23618;&#20195;&#29702;&#32452;&#25104;&#65292;&#27599;&#23618;&#20195;&#29702;&#36861;&#27714;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#31532;&#19968;&#23618;&#30001;&#29983;&#20135;&#32773;&#21644;&#28040;&#36153;&#32773;&#32452;&#25104;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#24635;&#33021;&#28304;&#25104;&#26412;&#12290;&#20854;&#20182;&#20004;&#23618;&#25511;&#21046;&#33021;&#28304;&#20215;&#26684;&#65292;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#65292;&#21516;&#26102;&#24179;&#34913;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#20256;&#32479;&#33021;&#28304;&#30340;&#28040;&#36153;&#21644;&#29983;&#20135;&#12290;&#35813;&#26694;&#26550;&#36824;&#32771;&#34385;&#20102;&#33021;&#28304;&#38656;&#27714;&#21644;&#20379;&#24212;&#30340;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating variable renewable energy into the grid has posed challenges to system operators in achieving optimal trade-offs among energy availability, cost affordability, and pollution controllability. This paper proposes a multi-agent reinforcement learning framework for managing energy transactions in microgrids. The framework addresses the challenges above: it seeks to optimize the usage of available resources by minimizing the carbon footprint while benefiting all stakeholders. The proposed architecture consists of three layers of agents, each pursuing different objectives. The first layer, comprised of prosumers and consumers, minimizes the total energy cost. The other two layers control the energy price to decrease the carbon impact while balancing the consumption and production of both renewable and conventional energy. This framework also takes into account fluctuations in energy demand and supply.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#28151;&#28102;&#22240;&#32032;&#30340;&#26680;&#26465;&#20214;&#30697;&#32422;&#26463;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26680;&#26041;&#27861;&#24471;&#21040;&#30340;&#26465;&#20214;&#30697;&#32422;&#26463;&#30340;&#36817;&#20284;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#25919;&#31574;&#35780;&#20272;&#30340;&#23574;&#38160;&#19979;&#30028;&#20272;&#35745;&#65292;&#24182;&#33021;&#23545;&#32463;&#20856;&#36793;&#38469;&#25935;&#24863;&#24230;&#27169;&#22411;&#36827;&#34892;&#26032;&#39062;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.13348</link><description>&lt;p&gt;
Kernel Conditional Moment Constraints for Confounding Robust Inference. (arXiv:2302.13348v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Kernel Conditional Moment Constraints for Confounding Robust Inference. (arXiv:2302.13348v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#23545;&#28151;&#28102;&#22240;&#32032;&#30340;&#26680;&#26465;&#20214;&#30697;&#32422;&#26463;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26680;&#26041;&#27861;&#24471;&#21040;&#30340;&#26465;&#20214;&#30697;&#32422;&#26463;&#30340;&#36817;&#20284;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#25919;&#31574;&#35780;&#20272;&#30340;&#23574;&#38160;&#19979;&#30028;&#20272;&#35745;&#65292;&#24182;&#33021;&#23545;&#32463;&#20856;&#36793;&#38469;&#25935;&#24863;&#24230;&#27169;&#22411;&#36827;&#34892;&#26032;&#39062;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#31163;&#32447;&#24773;&#22659;&#25512;&#26029;&#20013;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#23545;&#25919;&#31574;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#22312;&#32473;&#23450;&#19981;&#30830;&#23450;&#24615;&#38598;&#19978;&#20272;&#35745;&#26368;&#22351;&#28151;&#28102;&#24773;&#20917;&#19979;&#30340;&#25919;&#31574;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#24448;&#24448;&#20026;&#20102;&#21487;&#34892;&#24615;&#32780;&#37319;&#29992;&#19968;&#20123;&#31895;&#30053;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#25918;&#26494;&#65292;&#23548;&#33268;&#23545;&#25919;&#31574;&#20215;&#20540;&#30340;&#36807;&#24230;&#20445;&#23432;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20272;&#35745;&#22120;&#65292;&#23427;&#25552;&#20379;&#20102;&#25919;&#31574;&#20215;&#20540;&#30340;&#23574;&#38160;&#19979;&#30028;&#12290;&#21487;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#21253;&#21547;&#20102;Dorn&#21644;Guo&#65288;2022&#24180;&#65289;&#26368;&#36817;&#25552;&#20986;&#30340;&#23574;&#38160;&#20272;&#35745;&#22120;&#20316;&#20026;&#19968;&#20010;&#29305;&#20363;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#21017;&#21033;&#29992;f-&#25955;&#24230;&#26469;&#23454;&#29616;&#23545;&#32463;&#20856;&#36793;&#38469;&#25935;&#24863;&#24230;&#27169;&#22411;&#30340;&#26032;&#39062;&#25193;&#23637;&#12290;&#20026;&#20102;&#26500;&#24314;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#21033;&#29992;&#26680;&#26041;&#27861;&#24471;&#21040;&#20102;&#26465;&#20214;&#30697;&#32422;&#26463;&#30340;&#21487;&#34892;&#36817;&#20284;&#65292;&#32780;&#20256;&#32479;&#30340;&#38750;&#23574;&#38160;&#20272;&#35745;&#22120;&#21017;&#26410;&#33021;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26465;&#20214;&#65292;&#29992;&#20110;&#38477;&#20302;&#20256;&#32479;&#38750;&#23574;&#38160;&#20272;&#35745;&#22120;&#21463;&#28151;&#28102;&#22240;&#32032;&#24433;&#21709;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study policy evaluation of offline contextual bandits subject to unobserved confounders. Sensitivity analysis methods are commonly used to estimate the policy value under the worst-case confounding over a given uncertainty set. However, existing work often resorts to some coarse relaxation of the uncertainty set for the sake of tractability, leading to overly conservative estimation of the policy value. In this paper, we propose a general estimator that provides a sharp lower bound of the policy value. It can be shown that our estimator contains the recently proposed sharp estimator by Dorn and Guo (2022) as a special case, and our method enables a novel extension of the classical marginal sensitivity model using f-divergence. To construct our estimator, we leverage the kernel method to obtain a tractable approximation to the conditional moment constraints, which traditional non-sharp estimators failed to take into account. In the theoretical analysis, we provide a condition for the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#32452;&#21512;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#22312;&#33258;&#26059;&#29627;&#29827;&#22522;&#24577;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#25913;&#36827;&#32467;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#22914;&#27169;&#25311;&#36864;&#28779;&#25110;&#24182;&#34892;&#36864;&#28779;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25552;&#20379;&#30456;&#24403;&#36136;&#37327;&#30340;&#32467;&#26524;&#20043;&#21069;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2302.10848</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#22312;&#33258;&#26059;&#29627;&#29827;&#22522;&#24577;&#38382;&#39064;&#19978;&#30340;&#27979;&#35797;&#65306;&#26356;&#24191;&#27867;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Deep reinforced learning heuristic tested on spin-glass ground states: The larger picture. (arXiv:2302.10848v2 [cond-mat.dis-nn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10848
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#32452;&#21512;&#20248;&#21270;&#36807;&#31243;&#65292;&#24182;&#22312;&#33258;&#26059;&#29627;&#29827;&#22522;&#24577;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#25913;&#36827;&#32467;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#22914;&#27169;&#25311;&#36864;&#28779;&#25110;&#24182;&#34892;&#36864;&#28779;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25552;&#20379;&#30456;&#24403;&#36136;&#37327;&#30340;&#32467;&#26524;&#20043;&#21069;&#20943;&#23569;&#20102;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Changjun Fan&#31561;&#20154;&#30340;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#32452;&#21512;&#20248;&#21270;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20182;&#20204;&#38024;&#23545;&#20960;&#20010;&#33258;&#26059;&#29627;&#29827;&#22522;&#24577;&#38382;&#39064;&#23637;&#31034;&#20102;&#32467;&#26524;&#65292;&#20854;&#20013;&#38750;&#24179;&#38754;&#32593;&#32476;&#19978;&#30340;&#23454;&#20363;&#36890;&#24120;&#26159;NP&#22256;&#38590;&#30340;&#65292;&#19982;&#20960;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#22914;&#27169;&#25311;&#36864;&#28779;&#65288;SA&#65289;&#25110;&#24182;&#34892;&#36864;&#28779;&#65288;PT&#65289;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#24378;&#21270;&#23398;&#20064;&#30456;&#23545;&#20110;SA&#25110;PT&#25913;&#36827;&#20102;&#32467;&#26524;&#65292;&#25110;&#32773;&#33267;&#23569;&#22312;&#33719;&#24471;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#24403;&#36136;&#37327;&#30340;&#32467;&#26524;&#20043;&#21069;&#65292;&#20943;&#23569;&#20102;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#20026;&#20102;&#35777;&#26126;&#20182;&#20204;&#30340;&#26041;&#27861;&#8220;&#20248;&#36234;&#8221;&#65292;&#20316;&#32773;&#37319;&#21462;&#20102;&#20004;&#31181;&#22522;&#26412;&#31574;&#30053;&#65306;&#65288;1&#65289;&#35843;&#29992;&#21830;&#19994;GUROBI&#27714;&#35299;&#22120;&#33719;&#21462;&#19968;&#20123;&#31934;&#30830;&#22522;&#24577;&#26679;&#26412;&#20316;&#20026;&#27979;&#35797;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23558;&#20182;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#31574;&#30053;&#36827;&#34892;&#20102;&#30452;&#25509;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Changjun Fan et al. [Nature Communications https://doi.org/10.1038/s41467-023-36363-w (2023)], the authors present a deep reinforced learning approach to augment combinatorial optimization heuristics. In particular, they present results for several spin glass ground state problems, for which instances on non-planar networks are generally NP-hard, in comparison with several Monte Carlo based methods, such as simulated annealing (SA) or parallel tempering (PT). Indeed, those results demonstrate that the reinforced learning improves the results over those obtained with SA or PT, or at least allows for reduced runtimes for the heuristics before results of comparable quality have been obtained relative to those other methods. To facilitate the conclusion that their method is ''superior'', the authors pursue two basic strategies: (1) A commercial GUROBI solver is called on to procure a sample of exact ground states as a testbed to compare with, and (2) a head-to-head comparison between th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07260</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20808;&#39564;&#32593;&#32476;&#30340;&#39640;&#32500;&#36755;&#20986;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#28041;&#21450;&#21040;&#26410;&#30693;&#30340;&#39640;&#32500;&#24230;&#26144;&#23556;&#19968;&#32452;&#21487;&#25511;&#21464;&#37327;&#21040;&#26114;&#36149;&#23454;&#39564;&#32467;&#26524;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#20219;&#21153;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#26102;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#24403;&#22788;&#29702;&#39640;&#32500;&#36755;&#20986;&#26102;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#32500;&#24230;&#20027;&#35201;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#20030;&#38598;&#25104;&#30340;BO&#21644;&#24207;&#36143;&#20915;&#31574;&#21046;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#20351;&#29992;&#36866;&#24403;&#30340;&#20307;&#31995;&#32467;&#26500;&#36873;&#25321;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#36817;&#20284;&#35774;&#35745;&#21464;&#37327;&#21644;&#24863;&#20852;&#36259;&#37327;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#65292;&#21363;&#20351;&#22312;&#21518;&#32773;&#21462;&#20540;&#20110;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#25110;&#29978;&#33267;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22788;&#29702;&#39640;&#32500;&#24230;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#27969;&#27169;&#22411;&#8212;&#8212;&#20027;&#35201;&#27969;&#65288;PF&#65289;&#65292;&#36890;&#36807;&#23545;Hessian&#30697;&#38453;&#29305;&#24449;&#20998;&#35299;&#30340;&#20381;&#36182;&#65292;&#25429;&#25417;&#21040;&#20102;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#21457;&#25955;&#21644;&#25391;&#33633;&#34892;&#20026;&#12289;&#36867;&#36920;&#23616;&#37096;&#26497;&#23567;&#20540;&#21644;&#38797;&#28857;&#30340;&#36830;&#32493;&#24615;&#27969;&#65292;&#24182;&#35299;&#37322;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#12290;&#36890;&#36807;&#23545;&#19981;&#31283;&#23450;&#24615;&#30340;&#26032;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#29575;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#27979;&#35797;&#38598;&#35780;&#20272;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2302.01952</link><description>&lt;p&gt;
&#20851;&#20110;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#21644;&#28145;&#24230;&#23398;&#20064;&#19981;&#31283;&#23450;&#24615;&#30340;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On a continuous time model of gradient descent dynamics and instability in deep learning. (arXiv:2302.01952v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#27969;&#27169;&#22411;&#8212;&#8212;&#20027;&#35201;&#27969;&#65288;PF&#65289;&#65292;&#36890;&#36807;&#23545;Hessian&#30697;&#38453;&#29305;&#24449;&#20998;&#35299;&#30340;&#20381;&#36182;&#65292;&#25429;&#25417;&#21040;&#20102;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#21457;&#25955;&#21644;&#25391;&#33633;&#34892;&#20026;&#12289;&#36867;&#36920;&#23616;&#37096;&#26497;&#23567;&#20540;&#21644;&#38797;&#28857;&#30340;&#36830;&#32493;&#24615;&#27969;&#65292;&#24182;&#35299;&#37322;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#12290;&#36890;&#36807;&#23545;&#19981;&#31283;&#23450;&#24615;&#30340;&#26032;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#29575;&#36866;&#24212;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#27979;&#35797;&#38598;&#35780;&#20272;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25104;&#21151;&#30340;&#31192;&#35776;&#22312;&#20110;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#30340;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#26799;&#24230;&#19979;&#38477;&#30340;&#34892;&#20026;&#65292;&#29305;&#21035;&#26159;&#20854;&#19981;&#31283;&#23450;&#24615;&#65292;&#33853;&#21518;&#20110;&#20854;&#32463;&#39564;&#25104;&#21151;&#12290;&#20026;&#20102;&#22686;&#21152;&#30740;&#31350;&#26799;&#24230;&#19979;&#38477;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20027;&#35201;&#27969;&#65288;PF&#65289;&#65292;&#19968;&#31181;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#30340;&#36830;&#32493;&#26102;&#38388;&#27969;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;PF&#26159;&#21807;&#19968;&#25429;&#25417;&#21040;&#26799;&#24230;&#19979;&#38477;&#30340;&#21457;&#25955;&#21644;&#25391;&#33633;&#34892;&#20026;&#65292;&#21253;&#25324;&#36867;&#36920;&#23616;&#37096;&#26497;&#23567;&#20540;&#21644;&#38797;&#28857;&#30340;&#36830;&#32493;&#24615;&#27969;&#12290;&#36890;&#36807;&#20854;&#23545;&#20110;Hessian&#29305;&#24449;&#20998;&#35299;&#30340;&#20381;&#36182;&#65292;PF&#35299;&#37322;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#36817;&#35266;&#23519;&#21040;&#30340;&#31283;&#23450;&#36793;&#32536;&#29616;&#35937;&#12290;&#36890;&#36807;&#23545;&#19981;&#31283;&#23450;&#24615;&#30340;&#26032;&#29702;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#29575;&#36866;&#24212;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25511;&#21046;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#27979;&#35797;&#38598;&#35780;&#20272;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recipe behind the success of deep learning has been the combination of neural networks and gradient-based optimization. Understanding the behavior of gradient descent however, and particularly its instability, has lagged behind its empirical success. To add to the theoretical tools available to study gradient descent we propose the principal flow (PF), a continuous time flow that approximates gradient descent dynamics. To our knowledge, the PF is the only continuous flow that captures the divergent and oscillatory behaviors of gradient descent, including escaping local minima and saddle points. Through its dependence on the eigendecomposition of the Hessian the PF sheds light on the recently observed edge of stability phenomena in deep learning. Using our new understanding of instability we propose a learning rate adaptation method which enables us to control the trade-off between training stability and test set evaluation performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>TargetCall&#36890;&#36807;&#39044;&#22522;&#35843;&#36807;&#28388;&#65292;&#28040;&#38500;&#20102;basecalling&#20013;&#30340;&#28010;&#36153;&#35745;&#31639;&#65292;&#25552;&#39640;&#20102;&#22522;&#22240;&#32452;&#20998;&#26512;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.04953</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#22522;&#35843;&#36807;&#28388;&#28040;&#38500;basecalling&#20013;&#30340;&#28010;&#36153;&#35745;&#31639;&#30340;TargetCall
&lt;/p&gt;
&lt;p&gt;
TargetCall: Eliminating the Wasted Computation in Basecalling via Pre-Basecalling Filtering. (arXiv:2212.04953v2 [q-bio.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04953
&lt;/p&gt;
&lt;p&gt;
TargetCall&#36890;&#36807;&#39044;&#22522;&#35843;&#36807;&#28388;&#65292;&#28040;&#38500;&#20102;basecalling&#20013;&#30340;&#28010;&#36153;&#35745;&#31639;&#65292;&#25552;&#39640;&#20102;&#22522;&#22240;&#32452;&#20998;&#26512;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Basecalling&#26159;&#32435;&#31859;&#23380;&#27979;&#24207;&#20998;&#26512;&#20013;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#23427;&#23558;&#32435;&#31859;&#23380;&#27979;&#24207;&#20202;&#30340;&#21407;&#22987;&#20449;&#21495;&#36716;&#25442;&#20026;&#26680;&#37240;&#24207;&#21015;&#65292;&#21363;reads&#12290;&#26368;&#20808;&#36827;&#30340;basecallers&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#39640;&#24230;&#30340;basecalling&#20934;&#30830;&#24615;&#12290;&#36825;&#20351;&#24471;basecalling&#22312;&#35745;&#31639;&#19978;&#25928;&#29575;&#20302;&#19979;&#19988;&#20869;&#23384;&#28040;&#32791;&#22823;&#65292;&#25104;&#20026;&#25972;&#20010;&#22522;&#22240;&#32452;&#20998;&#26512;&#27969;&#31243;&#30340;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#26469;&#35828;&#65292;&#22823;&#22810;&#25968;reads&#19982;&#24863;&#20852;&#36259;&#30340;&#21442;&#32771;&#22522;&#22240;&#32452;&#19981;&#21305;&#37197;&#65288;&#21363;&#30446;&#26631;&#21442;&#32771;&#22522;&#22240;&#32452;&#65289;&#65292;&#22240;&#27492;&#20250;&#22312;&#21518;&#32493;&#30340;&#22522;&#22240;&#32452;&#27969;&#31243;&#27493;&#39588;&#20013;&#34987;&#20002;&#24323;&#65292;&#28010;&#36153;&#20102;basecalling&#30340;&#35745;&#31639;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TargetCall&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#28040;&#38500;basecalling&#20013;&#28010;&#36153;&#35745;&#31639;&#30340;&#39044;&#22522;&#35843;&#36807;&#28388;&#22120;&#12290;TargetCall&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;basecalling&#20043;&#21069;&#20002;&#24323;&#19981;&#20250;&#19982;&#30446;&#26631;&#21442;&#32771;&#22522;&#22240;&#32452;&#21305;&#37197;&#30340;reads&#65288;&#21363;&#38750;&#30446;&#26631;reads&#65289;&#12290;TargetCall&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;1&#65289;LightCall&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#31070;&#32463;&#32593;&#32476;basecaller&#65292;&#20135;&#29983;&#22122;&#22768;reads&#65307;
&lt;/p&gt;
&lt;p&gt;
Basecalling is an essential step in nanopore sequencing analysis where the raw signals of nanopore sequencers are converted into nucleotide sequences, i.e., reads. State-of-the-art basecallers employ complex deep learning models to achieve high basecalling accuracy. This makes basecalling computationally-inefficient and memory-hungry; bottlenecking the entire genome analysis pipeline. However, for many applications, the majority of reads do no match the reference genome of interest (i.e., target reference) and thus are discarded in later steps in the genomics pipeline, wasting the basecalling computation. To overcome this issue, we propose TargetCall, the first pre-basecalling filter to eliminate the wasted computation in basecalling. TargetCall's key idea is to discard reads that will not match the target reference (i.e., off-target reads) prior to basecalling. TargetCall consists of two main components: (1) LightCall, a lightweight neural network basecaller that produces noisy reads;
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#34913;&#37327;&#26631;&#20934;&#32780;&#19981;&#20381;&#36182;&#20110;&#22870;&#21169;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#20445;&#25252;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2211.10851</link><description>&lt;p&gt;
&#22870;&#21169;&#24182;&#38750;&#24517;&#35201;&#65306;&#22914;&#20309;&#20026;&#32456;&#36523;&#23398;&#20064;&#21019;&#24314;&#19968;&#20010;&#32452;&#21512;&#24615;&#33258;&#25105;&#20445;&#25252;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning. (arXiv:2211.10851v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10851
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#34913;&#37327;&#26631;&#20934;&#32780;&#19981;&#20381;&#36182;&#20110;&#22870;&#21169;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#33258;&#25105;&#20445;&#25252;&#33021;&#21147;&#30340;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#35748;&#20026;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#36991;&#20813;&#24809;&#32602;&#26159;&#35299;&#37322;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#26680;&#24515;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#29983;&#20013;&#65292;&#29983;&#29289;&#38656;&#35201;&#23398;&#20064;&#20851;&#20110;&#19990;&#30028;&#32467;&#26500;&#30340;&#35768;&#22810;&#19981;&#21516;&#26041;&#38754;&#65306;&#19990;&#30028;&#29366;&#24577;&#21644;&#29366;&#24577;&#36716;&#31227;&#21160;&#21147;&#23398;&#12290;&#38543;&#30528;&#26234;&#33021;&#20307;&#34701;&#20837;&#26032;&#30693;&#35782;&#65292;&#29366;&#24577;&#32452;&#21512;&#30340;&#25968;&#37327;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#24182;&#19988;&#23545;&#20110;&#32473;&#23450;&#30340;&#29366;&#24577;&#32452;&#21512;&#65292;&#27809;&#26377;&#26126;&#26174;&#23450;&#20041;&#30340;&#39044;&#35774;&#22870;&#21169;&#25110;&#25104;&#26412;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#21152;&#26435;&#38656;&#35201;&#22312;&#26234;&#33021;&#20307;&#22312;&#19990;&#30028;&#20013;&#30340;&#32463;&#39564;&#20043;&#21069;&#23545;&#22909;&#30340;&#21644;&#22351;&#30340;&#32452;&#21512;&#36827;&#34892;&#32534;&#30721;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#22312;&#22823;&#29366;&#24577;&#31354;&#38388;&#20013;&#24320;&#21457;&#26356;&#33258;&#28982;&#30340;&#34892;&#20026;&#21644;&#21160;&#26426;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;&#20869;&#22312;&#21160;&#26426;&#34913;&#37327;&#26631;&#20934;&#65288;&#21363;&#36171;&#20104;&#33021;&#21147;&#65289;&#26159;&#21487;&#33021;&#30340;&#65292;&#35813;&#26631;&#20934;&#34913;&#37327;&#26234;&#33021;&#20307;&#22312;&#36716;&#31227;&#25805;&#20316;&#32773;&#19979;&#23454;&#29616;&#35768;&#22810;&#21487;&#33021;&#26410;&#26469;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#36171;&#20104;&#33021;&#21147;&#25193;&#23637;&#21040;&#20998;&#23618;&#29366;&#24577;&#31354;&#38388;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning views the maximization of rewards and avoidance of punishments as central to explaining goal-directed behavior. However, over a life, organisms will need to learn about many different aspects of the world's structure: the states of the world and state-vector transition dynamics. The number of combinations of states grows exponentially as an agent incorporates new knowledge, and there is no obvious weighted combination of pre-existing rewards or costs defined for a given combination of states, as such a weighting would need to encode information about good and bad combinations prior to an agent's experience in the world. Therefore, we must develop more naturalistic accounts of behavior and motivation in large state-spaces. We show that it is possible to use only the intrinsic motivation metric of empowerment, which measures the agent's capacity to realize many possible futures under a transition operator. We propose to scale empowerment to hierarchical state-space
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31354;&#38388;&#31232;&#30095;&#25512;&#29702;&#65288;SSI&#65289;&#30340;&#36890;&#29992;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36873;&#25321;&#24615;&#22320;&#20026;&#32534;&#36753;&#21306;&#22495;&#25191;&#34892;&#35745;&#31639;&#24182;&#21152;&#36895;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#26465;&#20214;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#32531;&#23384;&#21644;&#37325;&#22797;&#20351;&#29992;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#24449;&#22270;&#65292;&#25105;&#20204;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#31232;&#30095;&#22320;&#24212;&#29992;&#20110;&#32534;&#36753;&#21306;&#22495;&#65292;&#24182;&#22312;&#26410;&#32534;&#36753;&#30340;&#21306;&#22495;&#20013;&#37325;&#22797;&#20351;&#29992;&#32531;&#23384;&#29305;&#24449;&#65292;&#20174;&#32780;&#36890;&#36807;&#32422;$1\%$&#30340;&#21306;&#22495;&#32534;&#36753;&#26469;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#30340;&#28010;&#36153;&#12290;</title><link>http://arxiv.org/abs/2211.02048</link><description>&lt;p&gt;
&#26377;&#25928;&#31232;&#30095;&#25512;&#29702;&#29992;&#20110;&#26465;&#20214;GAN&#21644;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models. (arXiv:2211.02048v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02048
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31354;&#38388;&#31232;&#30095;&#25512;&#29702;&#65288;SSI&#65289;&#30340;&#36890;&#29992;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36873;&#25321;&#24615;&#22320;&#20026;&#32534;&#36753;&#21306;&#22495;&#25191;&#34892;&#35745;&#31639;&#24182;&#21152;&#36895;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#26465;&#20214;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#32531;&#23384;&#21644;&#37325;&#22797;&#20351;&#29992;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#24449;&#22270;&#65292;&#25105;&#20204;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#31232;&#30095;&#22320;&#24212;&#29992;&#20110;&#32534;&#36753;&#21306;&#22495;&#65292;&#24182;&#22312;&#26410;&#32534;&#36753;&#30340;&#21306;&#22495;&#20013;&#37325;&#22797;&#20351;&#29992;&#32531;&#23384;&#29305;&#24449;&#65292;&#20174;&#32780;&#36890;&#36807;&#32422;$1\%$&#30340;&#21306;&#22495;&#32534;&#36753;&#26469;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#30340;&#28010;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#32534;&#36753;&#20013;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24448;&#24448;&#20250;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#21512;&#25104;&#25972;&#20010;&#36755;&#20986;&#65292;&#21253;&#25324;&#26410;&#32534;&#36753;&#30340;&#21306;&#22495;&#12290;&#36825;&#23548;&#33268;&#35745;&#31639;&#36164;&#28304;&#30340;&#28010;&#36153;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36739;&#23567;&#30340;&#32534;&#36753;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31354;&#38388;&#31232;&#30095;&#25512;&#29702;&#65288;SSI&#65289;&#30340;&#36890;&#29992;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36873;&#25321;&#24615;&#22320;&#20026;&#32534;&#36753;&#21306;&#22495;&#25191;&#34892;&#35745;&#31639;&#24182;&#21152;&#36895;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#26465;&#20214;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#29992;&#25143;&#20542;&#21521;&#20110;&#36880;&#28176;&#25913;&#21464;&#36755;&#20837;&#22270;&#20687;&#65292;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#32531;&#23384;&#21644;&#37325;&#22797;&#20351;&#29992;&#21407;&#22987;&#22270;&#20687;&#30340;&#29305;&#24449;&#22270;&#30340;&#24819;&#27861;&#12290;&#32473;&#23450;&#32534;&#36753;&#36807;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#31232;&#30095;&#22320;&#23558;&#21367;&#31215;&#28388;&#27874;&#22120;&#24212;&#29992;&#20110;&#32534;&#36753;&#21306;&#22495;&#65292;&#21516;&#26102;&#37325;&#22797;&#20351;&#29992;&#26410;&#32534;&#36753;&#21306;&#22495;&#30340;&#32531;&#23384;&#29305;&#24449;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#31232;&#30095;&#28176;&#36827;&#24335;&#29983;&#25104;&#24341;&#25806;&#65288;SIGE&#65289;&#26469;&#23558;&#35745;&#31639;&#20943;&#23569;&#36716;&#21270;&#20026;&#22312;&#29616;&#25104;&#30828;&#20214;&#19978;&#30340;&#24310;&#36831;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32422;$1\%$&#30340;&#21306;&#22495;&#32534;&#36753;&#65292;&#20943;&#23569;&#20102;&#35745;&#31639;&#36164;&#28304;&#30340;&#28010;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;
During image editing, existing deep generative models tend to re-synthesize the entire output from scratch, including the unedited regions. This leads to a significant waste of computation, especially for minor editing operations. In this work, we present Spatially Sparse Inference (SSI), a general-purpose technique that selectively performs computation for edited regions and accelerates various generative models, including both conditional GANs and diffusion models. Our key observation is that users tend to gradually change the input image. This motivates us to cache and reuse the feature maps of the original image. Given an edited image, we sparsely apply the convolutional filters to the edited regions while reusing the cached features for the unedited areas. Based on our algorithm, we further propose Sparse Incremental Generative Engine (SIGE) to convert the computation reduction to latency reduction on off-the-shelf hardware. With about $1\%$-area edits, our method reduces the comp
&lt;/p&gt;</description></item><item><title>ConSpec&#26159;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20855;&#26377;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#27493;&#39588;&#21644;&#21487;&#35299;&#37322;&#30340;&#20449;&#29992;&#20998;&#37197;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.05845</link><description>&lt;p&gt;
ConSpec: &#31361;&#20986;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05845
&lt;/p&gt;
&lt;p&gt;
ConSpec&#26159;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20855;&#26377;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#27493;&#39588;&#21644;&#21487;&#35299;&#37322;&#30340;&#20449;&#29992;&#20998;&#37197;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#25104;&#21151;&#24448;&#24448;&#21462;&#20915;&#20110;&#22810;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#36825;&#20123;&#27493;&#39588;&#22312;&#26102;&#38388;&#19978;&#30456;&#36317;&#36739;&#36828;&#65292;&#19982;&#26368;&#32456;&#22870;&#21169;&#20063;&#30456;&#36317;&#29978;&#36828;&#12290;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#20381;&#36182;Bellman&#26041;&#31243;&#65292;&#24456;&#38590;&#35782;&#21035;&#36825;&#20123;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20851;&#38190;&#27493;&#39588;&#12290;&#36825;&#20010;&#31639;&#27861;&#34987;&#31216;&#20026;&#23545;&#27604;&#20869;&#30465;&#65288;ConSpec&#65289;&#65292;&#21487;&#20197;&#28155;&#21152;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;ConSpec&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#19982;&#36825;&#20123;&#21407;&#22411;&#20043;&#19968;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#12290;ConSpec&#20013;&#30340;&#21407;&#22411;&#22312;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#65288;1&#65289;&#23427;&#20204;&#20351;&#24471;&#33021;&#22815;&#36805;&#36895;&#35782;&#21035;&#25152;&#26377;&#20851;&#38190;&#27493;&#39588;&#65307;&#65288;2&#65289;&#23427;&#20204;&#20197;&#23481;&#26131;&#35299;&#37322;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20351;&#24471;&#22312;&#24863;&#35273;&#29305;&#24449;&#25913;&#21464;&#26102;&#21487;&#20197;&#36827;&#34892;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#12290;&#19982;&#20854;&#20182;&#24403;&#20195;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;
&lt;/p&gt;
&lt;p&gt;
In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on critical steps. This algorithm, which we call contrastive introspection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of these prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (1) They enable rapid identification of all the critical steps. (2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04688</link><description>&lt;p&gt;
BAFFLE: &#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#35797;&#38169;&#32463;&#39564;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#31163;&#32447;RL&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;RL&#33539;&#20363;&#65292;&#22240;&#20026;&#23427;&#33410;&#30465;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447;RL&#20013;&#65292;&#25968;&#25454;&#25552;&#20379;&#32773;&#20849;&#20139;&#22823;&#35268;&#27169;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20182;&#20154;&#21487;&#20197;&#22312;&#19981;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#33539;&#20363;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#12289;&#33258;&#21160;&#39550;&#39542;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#23569;&#20851;&#27880;&#30740;&#31350;&#31163;&#32447;RL&#31995;&#32479;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#20851;&#27880;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#19968;&#20123;&#25200;&#21160;&#34987;&#28155;&#21152;&#21040;&#25968;&#25454;&#65288;&#35266;&#27979;&#20540;&#65289;&#20013;&#65292;&#20351;&#24471;&#22312;&#32473;&#23450;&#27491;&#24120;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#37319;&#21462;&#39640;&#22870;&#21169;&#30340;&#21160;&#20316;&#65292;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAFFLE&#65288;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research has focused on the Reinforcement Learning (RL) methods which allow the agent to learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL becomes a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach tha
&lt;/p&gt;</description></item><item><title>LambdaKG&#26159;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12289;&#38382;&#31572;&#12289;&#25512;&#33616;&#21644;&#30693;&#35782;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2210.00305</link><description>&lt;p&gt;
LambdaKG:&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#24211;
&lt;/p&gt;
&lt;p&gt;
LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings. (arXiv:2210.00305v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00305
&lt;/p&gt;
&lt;p&gt;
LambdaKG&#26159;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#24211;&#65292;&#25552;&#20379;&#20102;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12289;&#38382;&#31572;&#12289;&#25512;&#33616;&#21644;&#30693;&#35782;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#36890;&#24120;&#20855;&#26377;&#24322;&#26500;&#30340;&#22270;&#32467;&#26500;&#21644;&#25991;&#26412;&#20016;&#23500;&#30340;&#23454;&#20307;/&#20851;&#31995;&#20449;&#24687;&#12290;&#22522;&#20110;&#25991;&#26412;&#30340;KG&#23884;&#20837;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#25551;&#36848;&#36827;&#34892;&#32534;&#30721;&#26469;&#34920;&#31034;&#23454;&#20307;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#19987;&#38376;&#20026;PLM&#19982;KG&#35774;&#35745;&#30340;&#24320;&#28304;&#24211;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LambdaKG&#65292;&#19968;&#20010;&#24102;&#26377;&#22810;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65292;BART&#65292;T5&#65292;GPT-3&#65289;&#24182;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65292;&#38382;&#31572;&#65292;&#25512;&#33616;&#21644;&#30693;&#35782;&#25506;&#32034;&#65289;&#30340;KGE&#24211;&#12290;LambdaKG&#22312;https://github.com/zjunlp/PromptKG/tree/main/lambdaKG&#19978;&#20844;&#24320;&#24320;&#28304;&#65292;&#24182;&#25552;&#20379;&#20102;&#28436;&#31034;&#35270;&#39057;&#21644;&#38271;&#26399;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph structure and text-rich entity/relation information. Text-based KG embeddings can represent entities by encoding descriptions with pre-trained language models, but no open-sourced library is specifically designed for KGs with PLMs at present. In this paper, we present LambdaKG, a library for KGE that equips with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and supports various tasks (e.g., knowledge graph completion, question answering, recommendation, and knowledge probing). LambdaKG is publicly open-sourced at https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at this http URL and long-term maintenance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;Transformer&#30340;&#38544;&#31192;&#23454;&#29992;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;TrojViT&#65292;&#36890;&#36807;&#34917;&#19969;&#32423;&#35302;&#21457;&#22120;&#23558;&#19968;&#20123;&#26131;&#21463;&#25915;&#20987;&#20301;&#32452;&#25104;&#30340;&#21442;&#25968;&#24314;&#31435;&#20026;&#29305;&#27931;&#20234;&#26408;&#39532;&#12290;</title><link>http://arxiv.org/abs/2208.13049</link><description>&lt;p&gt;
TrojViT: &#35270;&#35273;Transformer&#20013;&#30340;&#21518;&#38376;&#25554;&#20837;
&lt;/p&gt;
&lt;p&gt;
TrojViT: Trojan Insertion in Vision Transformers. (arXiv:2208.13049v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;Transformer&#30340;&#38544;&#31192;&#23454;&#29992;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;TrojViT&#65292;&#36890;&#36807;&#34917;&#19969;&#32423;&#35302;&#21457;&#22120;&#23558;&#19968;&#20123;&#26131;&#21463;&#25915;&#20987;&#20301;&#32452;&#25104;&#30340;&#21442;&#25968;&#24314;&#31435;&#20026;&#29305;&#27931;&#20234;&#26408;&#39532;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViT&#65289;&#22312;&#21508;&#31181;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;ViTs&#30340;&#25104;&#21151;&#28608;&#21169;&#20102;&#25915;&#20987;&#32773;&#23545;&#20854;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#65292;&#20294;ViTs&#30340;&#21518;&#38376;&#25915;&#20987;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#19982;&#36890;&#36807;&#21367;&#31215;&#25429;&#25417;&#20687;&#32032;&#32423;&#23616;&#37096;&#29305;&#24449;&#30340;CNNs&#30456;&#27604;&#65292;ViTs&#36890;&#36807;&#34917;&#19969;&#21644;&#20851;&#27880;&#26426;&#21046;&#25552;&#21462;&#20840;&#23616;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#23558;CNN&#29305;&#23450;&#30340;&#21518;&#38376;&#25915;&#20987;&#22825;&#30495;&#22320;&#31227;&#26893;&#21040;ViTs&#21482;&#33021;&#20135;&#29983;&#20302;&#24178;&#20928;&#25968;&#25454;&#20934;&#30830;&#24615;&#21644;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38544;&#31192;&#32780;&#23454;&#29992;&#30340;ViT&#29305;&#23450;&#21518;&#38376;&#25915;&#20987;TrojViT&#12290;TrojViT&#29983;&#25104;&#19968;&#20010;&#34917;&#19969;&#32423;&#30340;&#35302;&#21457;&#22120;&#65292;&#26088;&#22312;&#36890;&#36807;&#34917;&#19969;&#26174;&#33879;&#24615;&#25490;&#24207;&#21644;&#20851;&#27880;&#30446;&#26631;&#25439;&#22833;&#65292;&#22312;DRAM&#23384;&#20648;&#22120;&#20013;&#30340;&#19968;&#20123;&#26131;&#21463;&#25915;&#20987;&#20301;&#32452;&#25104;&#30340;ViT&#30340;&#21442;&#25968;&#19978;&#24314;&#31435;&#29305;&#27931;&#20234;&#26408;&#39532;&#12290; TrojViT&#36824;&#20351;&#29992;&#26368;&#23567;&#21270;&#35843;&#25972;&#21442;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have demonstrated the state-of-the-art performance in various vision-related tasks. The success of ViTs motivates adversaries to perform backdoor attacks on ViTs. Although the vulnerability of traditional CNNs to backdoor attacks is well-known, backdoor attacks on ViTs are seldom-studied. Compared to CNNs capturing pixel-wise local features by convolutions, ViTs extract global context information through patches and attentions. Na\"ively transplanting CNN-specific backdoor attacks to ViTs yields only a low clean data accuracy and a low attack success rate. In this paper, we propose a stealth and practical ViT-specific backdoor attack $TrojViT$. Rather than an area-wise trigger used by CNN-specific backdoor attacks, TrojViT generates a patch-wise trigger designed to build a Trojan composed of some vulnerable bits on the parameters of a ViT stored in DRAM memory through patch salience ranking and attention-target loss. TrojViT further uses minimum-tuned paramet
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#39044;&#27979;&#21644;&#26550;&#26500;&#31639;&#27861;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#22312;&#25429;&#25417;&#29616;&#35937;&#12289;&#30830;&#23450;&#20851;&#38190;&#25968;&#25454;&#28857;&#21644;&#39044;&#27979;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2208.06028</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#27169;&#22411;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Gaussian Process Surrogate Models for Neural Networks. (arXiv:2208.06028v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06028
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#34892;&#20026;&#39044;&#27979;&#21644;&#26550;&#26500;&#31639;&#27861;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#22312;&#25429;&#25417;&#29616;&#35937;&#12289;&#30830;&#23450;&#20851;&#38190;&#25968;&#25454;&#28857;&#21644;&#39044;&#27979;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#27861;&#29702;&#35299;&#21644;&#39044;&#27979;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#34892;&#20026;&#20351;&#24471;&#24456;&#38590;&#20915;&#23450;&#38024;&#23545;&#32473;&#23450;&#38382;&#39064;&#20351;&#29992;&#20160;&#20040;&#26550;&#26500;&#21644;&#31639;&#27861;&#12290;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#24314;&#27169;&#26159;&#19968;&#31181;&#29992;&#20110;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#30340;&#26041;&#27861;&#35770;&#65292;&#20854;&#20869;&#37096;&#36807;&#31243;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#24314;&#27169;&#29992;&#31616;&#21270;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#31616;&#21270;&#31995;&#32479;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#26500;&#24314;&#20102;&#19968;&#31867;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#19981;&#26159;&#20174;&#26080;&#38480;&#31070;&#32463;&#32593;&#32476;&#25512;&#23548;&#20869;&#26680;&#65292;&#32780;&#26159;&#20174;&#26377;&#38480;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#34892;&#20026;&#20013;&#32463;&#39564;&#24615;&#22320;&#23398;&#20064;&#20869;&#26680;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25429;&#25417;&#21040;&#20102;&#19982;&#31070;&#32463;&#32593;&#32476;&#39057;&#35889;&#20559;&#24046;&#30456;&#20851;&#30340;&#29616;&#35937;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#22914;&#30830;&#23450;&#21738;&#20123;&#28857;&#23545;&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#30340;&#34892;&#20026;&#24433;&#21709;&#26368;&#22823;&#65292;&#24182;&#39044;&#27979;&#21738;&#20123;&#26550;&#26500;&#21644;&#31639;&#27861;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Not being able to understand and predict the behavior of deep learning systems makes it hard to decide what architecture and algorithm to use for a given problem. In science and engineering, modeling is a methodology used to understand complex systems whose internal processes are opaque. Modeling replaces a complex system with a simpler, more interpretable surrogate. Drawing inspiration from this, we construct a class of surrogate models for neural networks using Gaussian processes. Rather than deriving kernels for infinite neural networks, we learn kernels empirically from the naturalistic behavior of finite neural networks. We demonstrate our approach captures existing phenomena related to the spectral bias of neural networks, and then show that our surrogate models can be used to solve practical problems such as identifying which points most influence the behavior of specific neural networks and predicting which architectures and algorithms will generalize well for specific datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#34588;&#34562;&#30417;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;&#21644;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2208.00085</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#22312;&#34588;&#34562;&#30417;&#27979;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Computer Vision Techniques in Bee Monitoring Applications. (arXiv:2208.00085v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#22312;&#34588;&#34562;&#30417;&#27979;&#20013;&#30340;&#26368;&#26032;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#30340;&#28508;&#21147;&#65292;&#24182;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;&#21644;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#26159;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24050;&#32463;&#35777;&#26126;&#33021;&#22815;&#35299;&#20915;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#30417;&#27979;&#34588;&#34562;&#32676;&#20307;&#24182;&#26816;&#26597;&#20854;&#20581;&#24247;&#29366;&#20917;&#65292;&#20174;&#32780;&#22312;&#24773;&#20917;&#21464;&#24471;&#20005;&#37325;&#20043;&#21069;&#65292;&#35782;&#21035;&#20986;&#28508;&#22312;&#21361;&#38505;&#29366;&#24577;&#65292;&#25110;&#32773;&#26356;&#22909;&#22320;&#35745;&#21010;&#23450;&#26399;&#34588;&#34562;&#32676;&#20307;&#26816;&#26597;&#65292;&#20174;&#32780;&#33410;&#30465;&#37325;&#35201;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#29992;&#20110;&#34588;&#34562;&#30417;&#27979;&#30340;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#65292;&#24182;&#20197;&#33258;&#21160;&#21270;&#34588;&#34562;&#35745;&#25968;&#31639;&#27861;&#20026;&#20363;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#38754;&#21521;&#20861;&#21307;&#23398;&#21644;&#34588;&#34562;&#23398;&#19987;&#19994;&#20154;&#21592;&#21644;&#19987;&#23478;&#65292;&#26088;&#22312;&#21521;&#20182;&#20204;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#65292;&#22240;&#27492;&#27599;&#20010;&#24212;&#29992;&#31867;&#21035;&#37117;&#20197;&#31616;&#35201;&#30340;&#29702;&#35770;&#20171;&#32461;&#21644;&#19982;&#20854;&#22522;&#26412;&#26041;&#27861;&#30456;&#20851;&#30340;&#21160;&#26426;&#24320;&#31687;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#35770;&#25991;&#33021;&#28608;&#21457;&#20854;&#20182;&#31185;&#23398;&#23478;&#30340;&#28789;&#24863;...
&lt;/p&gt;
&lt;p&gt;
Machine learning and computer vision are dynamically growing fields, which have proven to be able to solve very complex tasks. They could also be used for the monitoring of the honeybee colonies and for the inspection of their health state, which could identify potentially dangerous states before the situation is critical, or to better plan periodic bee colony inspections and therefore save significant costs. In this paper, we present an overview of the state-of-the-art computer vision and machine learning applications used for bee monitoring. We also demonstrate the potential of those methods as an example of an automated bee counter algorithm. The paper is aimed at veterinary and apidology professionals and experts, who might not be familiar with machine learning to introduce to them its possibilities, therefore each family of applications is opened by a brief theoretical introduction and motivation related to its base method. We hope that this paper will inspire other scientists to 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#32852;&#37030;&#30456;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#35299;&#20915;&#20102;&#21516;&#26102;&#34701;&#21512;&#31354;&#38388;&#20449;&#24687;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#21644;&#21160;&#24577;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2206.03420</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#32852;&#37030;&#30456;&#20851;&#26694;&#26550;&#29992;&#20110;&#26102;&#31354;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Federated Relevance Framework for Spatial Temporal Graph Learning. (arXiv:2206.03420v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03420
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#32852;&#37030;&#30456;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#35299;&#20915;&#20102;&#21516;&#26102;&#34701;&#21512;&#31354;&#38388;&#20449;&#24687;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#21644;&#21160;&#24577;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#30456;&#20851;&#24212;&#29992;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26102;&#31354;&#25968;&#25454;&#21253;&#21547;&#20016;&#23500;&#30340;&#20449;&#24687;&#24182;&#36817;&#24180;&#26469;&#21463;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#20363;&#22914;&#65292;&#21307;&#30103;&#26426;&#26500;&#32463;&#24120;&#20351;&#29992;&#38468;&#30528;&#22312;&#24739;&#32773;&#19981;&#21516;&#37096;&#20301;&#30340;&#30005;&#26497;&#65292;&#20998;&#26512;&#24102;&#26377;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#30005;&#29983;&#29702;&#25968;&#25454;&#36827;&#34892;&#20581;&#24247;&#35780;&#20272;&#21644;&#30142;&#30149;&#35786;&#26029;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#25110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#25552;&#21462;&#38544;&#34255;&#30340;&#26102;&#31354;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#34701;&#21512;&#31354;&#38388;&#20449;&#24687;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#21644;&#21160;&#24577;&#30340;&#26102;&#38388;&#21464;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#23545;&#20110;&#19968;&#20010;&#21033;&#29992;&#36825;&#20123;&#26102;&#31354;&#29305;&#24449;&#26469;&#23436;&#25104;&#22797;&#26434;&#39044;&#27979;&#20219;&#21153;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#32852;&#37030;&#30456;&#20851;&#26694;&#26550;&#65292;&#21517;&#20026;.....
&lt;/p&gt;
&lt;p&gt;
Spatial-temporal data contains rich information and has been widely studied in recent years due to the rapid development of relevant applications in many fields. For instance, medical institutions often use electrodes attached to different parts of a patient to analyse the electorencephal data rich with spatial and temporal features for health assessment and disease diagnosis. Existing research has mainly used deep learning techniques such as convolutional neural network (CNN) or recurrent neural network (RNN) to extract hidden spatial-temporal features. Yet, it is challenging to incorporate both inter-dependencies spatial information and dynamic temporal changes simultaneously. In reality, for a model that leverages these spatial-temporal features to fulfil complex prediction tasks, it often requires a colossal amount of training data in order to obtain satisfactory model performance. Considering the above-mentioned challenges, we propose an adaptive federated relevance framework, nam
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;ConDex&#65292;&#29992;&#20110;&#33258;&#20027;&#35782;&#21035;&#20855;&#26377;&#26410;&#30693;&#29289;&#29702;&#23646;&#24615;&#30340;&#38750;&#22343;&#21248;&#23545;&#35937;&#65292;&#24182;&#23454;&#29616;&#31934;&#30830;&#30340;&#25235;&#21462;&#28857;&#20272;&#35745;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ConDex&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#24182;&#19988;&#29983;&#25104;&#20102;&#20004;&#20010;&#26032;&#30340;&#23545;&#35937;&#25968;&#25454;&#38598;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2205.11110</link><description>&lt;p&gt;
&#29289;&#29702;-&#19981;&#21487;&#30693;&#23545;&#35937;&#30340;&#20803;&#23398;&#20064;&#20877;&#25235;&#21462;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Regrasping Strategies for Physical-Agnostic Objects. (arXiv:2205.11110v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;ConDex&#65292;&#29992;&#20110;&#33258;&#20027;&#35782;&#21035;&#20855;&#26377;&#26410;&#30693;&#29289;&#29702;&#23646;&#24615;&#30340;&#38750;&#22343;&#21248;&#23545;&#35937;&#65292;&#24182;&#23454;&#29616;&#31934;&#30830;&#30340;&#25235;&#21462;&#28857;&#20272;&#35745;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;ConDex&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#24182;&#19988;&#29983;&#25104;&#20102;&#20004;&#20010;&#26032;&#30340;&#23545;&#35937;&#25968;&#25454;&#38598;&#29992;&#20110;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#25235;&#21462;&#38750;&#22343;&#21248;&#23545;&#35937;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21407;&#22240;&#26159;&#23384;&#22312;&#26410;&#30693;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#22914;&#36136;&#37327;&#20998;&#24067;&#21644;&#25705;&#25830;&#31995;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ConDex&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#26465;&#20214;&#31070;&#32463;&#36807;&#31243;&#65288;CNP&#65289;&#19982;DexNet-2.0&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#33258;&#20027;&#35782;&#21035;&#23545;&#35937;&#30340;&#28508;&#22312;&#29289;&#29702;&#23646;&#24615;&#12290;ConDex&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#35797;&#39564;&#20013;&#33719;&#21462;&#29289;&#29702;&#23884;&#20837;&#65292;&#23454;&#29616;&#31934;&#30830;&#30340;&#25235;&#21462;&#28857;&#20272;&#35745;&#12290;&#27492;&#22806;&#65292;ConDex&#33021;&#22815;&#20197;&#22312;&#32447;&#26041;&#24335;&#36845;&#20195;&#22320;&#26356;&#26032;&#39044;&#27979;&#30340;&#25235;&#21462;&#36136;&#37327;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#29983;&#25104;&#20004;&#20010;&#38754;&#21521;&#38750;&#22343;&#21248;&#29289;&#29702;&#23646;&#24615;&#30340;&#23545;&#35937;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#36136;&#37327;&#20998;&#24067;&#21644;&#25705;&#25830;&#31995;&#25968;&#12290;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;ConDex&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;DexNet-2.0&#21644;&#29616;&#26377;&#30340;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#25235;&#21462;&#27969;&#27700;&#32447;&#12290;&#27492;&#22806;&#65292;ConDex&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
Grasping inhomogeneous objects in real-world applications remains a challenging task due to the unknown physical properties such as mass distribution and coefficient of friction. In this study, we propose a meta-learning algorithm called ConDex, which incorporates Conditional Neural Processes (CNP) with DexNet-2.0 to autonomously discern the underlying physical properties of objects using depth images. ConDex efficiently acquires physical embeddings from limited trials, enabling precise grasping point estimation. Furthermore, ConDex is capable of updating the predicted grasping quality iteratively from new trials in an online fashion. To the best of our knowledge, we are the first who generate two object datasets focusing on inhomogeneous physical properties with varying mass distributions and friction coefficients. Extensive evaluations in simulation demonstrate ConDex's superior performance over DexNet-2.0 and existing meta-learning-based grasping pipelines. Furthermore, ConDex shows
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#23398;&#20064;&#28176;&#36817;&#31283;&#23450;&#24179;&#34913;&#28857;&#21560;&#24341;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28385;&#36275;&#24490;&#29615;&#21253;&#21547;&#24615;&#27010;&#24565;&#30340;&#38598;&#21512;&#65292;&#24182;&#21033;&#29992;&#24490;&#29615;&#24615;&#36136;&#35745;&#31639;&#21560;&#24341;&#22495;&#30340;&#20869;&#37096;&#36924;&#36817;&#12290;</title><link>http://arxiv.org/abs/2204.10372</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#23398;&#20064;&#36890;&#36807;&#24490;&#29615;&#38598;&#30340;&#21560;&#24341;&#22495;
&lt;/p&gt;
&lt;p&gt;
Model-free Learning of Regions of Attraction via Recurrent Sets. (arXiv:2204.10372v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10372
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#23398;&#20064;&#28176;&#36817;&#31283;&#23450;&#24179;&#34913;&#28857;&#21560;&#24341;&#22495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28385;&#36275;&#24490;&#29615;&#21253;&#21547;&#24615;&#27010;&#24565;&#30340;&#38598;&#21512;&#65292;&#24182;&#21033;&#29992;&#24490;&#29615;&#24615;&#36136;&#35745;&#31639;&#21560;&#24341;&#22495;&#30340;&#20869;&#37096;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#28176;&#36817;&#31283;&#23450;&#24179;&#34913;&#28857;&#30340;&#21560;&#24341;&#22495;&#30340;&#20869;&#37096;&#36924;&#36817;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26356;&#23485;&#26494;&#30340;&#21253;&#21547;&#24615;&#27010;&#24565;&#8212;&#8212;&#24490;&#29615;&#65292;&#32780;&#19981;&#26159;&#21033;&#29992;&#24102;&#26377;&#26377;&#38480;&#19981;&#30830;&#23450;&#24615;&#30340;&#36817;&#20284;&#27169;&#22411;&#26469;&#25214;&#21040;&#21253;&#21547;&#22312;&#21560;&#24341;&#22495;&#20013;&#30340;&#65288;&#31283;&#23450;&#30340;&#65289;&#19981;&#21464;&#38598;&#12290;&#25105;&#20204;&#23450;&#20041;&#19968;&#20010;&#38598;&#21512;&#20026;$\tau$-&#24490;&#29615;&#65288;&#30456;&#24212;&#22320;&#20026;$k$-&#24490;&#29615;&#65289;&#65292;&#22914;&#26524;&#27599;&#20010;&#20174;&#35813;&#38598;&#21512;&#24320;&#22987;&#30340;&#36712;&#36857;&#22312;&#26368;&#22810;$\tau$&#31186;&#65288;&#30456;&#24212;&#22320;$k$&#27493;&#65289;&#21518;&#36820;&#22238;&#21040;&#35813;&#38598;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#21253;&#21547;&#19968;&#20010;&#31283;&#23450;&#24179;&#34913;&#28857;&#30340;$\tau$-&#24490;&#29615;&#38598;&#24517;&#39035;&#26159;&#20854;&#21560;&#24341;&#22495;&#30340;&#23376;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#29305;&#24615;&#24320;&#21457;&#20102;&#20351;&#29992;&#36890;&#36807;&#37319;&#26679;&#26377;&#38480;&#38271;&#24230;&#36712;&#36857;&#33719;&#24471;&#30340;&#24490;&#29615;&#21453;&#20363;&#26469;&#35745;&#31639;&#21560;&#24341;&#22495;&#20869;&#37096;&#36924;&#36817;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#27425;&#22788;&#29702;&#26679;&#26412;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#33021;&#22815;&#22312;&#21021;&#22987;&#31163;&#32447;&#36816;&#34892;&#20043;&#21518;&#32487;&#32493;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning an inner approximation of the region of attraction (ROA) of an asymptotically stable equilibrium point without an explicit model of the dynamics. Rather than leveraging approximate models with bounded uncertainty to find a (robust) invariant set contained in the ROA, we propose to learn sets that satisfy a more relaxed notion of containment known as recurrence. We define a set to be $\tau$-recurrent (resp. $k$-recurrent) if every trajectory that starts within the set, returns to it after at most $\tau$ seconds (resp. $k$ steps). We show that under mild assumptions a $\tau$-recurrent set containing a stable equilibrium must be a subset of its ROA. We then leverage this property to develop algorithms that compute inner approximations of the ROA using counter-examples of recurrence that are obtained by sampling finite-length trajectories. Our algorithms process samples sequentially, which allow them to continue being executed even after an initial offli
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#36339;&#36291;&#24615;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#26368;&#21487;&#33021;&#36716;&#25442;&#36335;&#24452;&#30340;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21644;&#36816;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#20811;&#26381;&#30456;&#20851;&#36895;&#29575;&#20989;&#25968;&#26080;&#27861;&#26126;&#30830;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2203.16874</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#36339;&#36291;&#24615;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#26368;&#21487;&#33021;&#36716;&#25442;&#36335;&#24452;&#30340;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Optimal Control Method to Compute the Most Likely Transition Path for Stochastic Dynamical Systems with Jumps. (arXiv:2203.16874v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#36339;&#36291;&#24615;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#26368;&#21487;&#33021;&#36716;&#25442;&#36335;&#24452;&#30340;&#26368;&#20248;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21644;&#36816;&#29992;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#20811;&#26381;&#30456;&#20851;&#36895;&#29575;&#20989;&#25968;&#26080;&#27861;&#26126;&#30830;&#34920;&#31034;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#29616;&#35937;&#34920;&#29616;&#20986;&#31361;&#28982;&#12289;&#38388;&#27463;&#25110;&#36339;&#36291;&#34892;&#20026;&#65292;&#26356;&#36866;&#21512;&#29992;&#38750;&#39640;&#26031;L\'evy&#22122;&#22768;&#19979;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#25551;&#36848;&#12290;&#22312;&#36825;&#20123;&#22797;&#26434;&#29616;&#35937;&#20013;&#65292;&#36716;&#21464;&#29366;&#24577;&#20043;&#38388;&#30340;&#26368;&#21487;&#33021;&#36716;&#25442;&#36335;&#24452;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#20123;&#31232;&#26377;&#20107;&#20214;&#22312;&#26576;&#20123;&#24773;&#26223;&#19979;&#21487;&#33021;&#20250;&#20135;&#29983;&#24456;&#39640;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#22823;&#20559;&#24046;&#21407;&#29702;&#65292;&#26368;&#21487;&#33021;&#30340;&#36716;&#25442;&#36335;&#24452;&#21487;&#20197;&#34987;&#35270;&#20026;&#36830;&#25509;&#20004;&#20010;&#28857;&#30340;&#36335;&#24452;&#19978;&#30340;&#36895;&#29575;&#20989;&#25968;&#30340;&#26497;&#23567;&#21270;&#12290;&#35745;&#31639;&#38750;&#39640;&#26031;L\'evy&#22122;&#22768;&#19979;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#31995;&#32479;&#30340;&#26368;&#21487;&#33021;&#36716;&#25442;&#36335;&#24452;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#30456;&#20851;&#30340;&#36895;&#29575;&#20989;&#25968;&#26080;&#27861;&#36890;&#36807;&#36335;&#24452;&#26126;&#30830;&#34920;&#31034;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20197;&#33719;&#24471;&#26368;&#21487;&#33021;&#36716;&#25442;&#36335;&#24452;&#30340;&#26368;&#20248;&#29366;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23545;&#39640;&#26031;&#21644;&#38750;&#39640;&#26031;&#24773;&#20917;&#36827;&#34892;&#20102;&#20960;&#20010;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many complex real world phenomena exhibit abrupt, intermittent or jumping behaviors, which are more suitable to be described by stochastic differential equations under non-Gaussian L\'evy noise. Among these complex phenomena, the most likely transition paths between metastable states are important since these rare events may have a high impact in certain scenarios. Based on the large deviation principle, the most likely transition path could be treated as the minimizer of the rate function upon paths that connect two points. One of the challenges to calculate the most likely transition path for stochastic dynamical systems under non-Gaussian L\'evy noise is that the associated rate function can not be explicitly expressed by paths. For this reason, we formulate an optimal control problem to obtain the optimal state as the most likely transition path. We then develop a neural network method to solve this issue. Several experiments are investigated for both Gaussian and non-Gaussian case
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21644;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#27169;&#22411;&#32454;&#35843;&#21363;&#21487;&#22312;&#30446;&#26631;&#39046;&#22495;&#35299;&#20915;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20248;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#20174;&#22836;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2202.10629</link><description>&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#65306;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning. (arXiv:2202.10629v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10629
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#21644;&#37325;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#27169;&#22411;&#32454;&#35843;&#21363;&#21487;&#22312;&#30446;&#26631;&#39046;&#22495;&#35299;&#20915;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#20248;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#20174;&#22836;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#35821;&#38899;&#31561;&#25968;&#25454;&#20016;&#23500;&#30340;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#25552;&#20379;&#39640;&#24615;&#33021;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#26041;&#38754;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#29978;&#33267;&#21487;&#20197;&#23398;&#20064;&#36890;&#29992;&#30340;&#20219;&#21153;&#26080;&#20851;&#34920;&#31034;&#20197;&#20415;&#26377;&#25928;&#22320;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#32454;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#39046;&#22495;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#20173;&#38754;&#20020;&#22810;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;&#65288;i&#65289;&#25968;&#25454;&#26377;&#38480;&#65307;&#65288;ii&#65289;&#27169;&#22411;&#24320;&#21457;&#25104;&#26412;&#21463;&#38480;&#65307;&#65288;iii&#65289;&#32570;&#20047;&#36275;&#22815;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#20415;&#26377;&#25928;&#36827;&#34892;&#32454;&#35843;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#27010;&#24565;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#36890;&#36807;&#20174;&#28304;&#39046;&#22495;&#37325;&#26032;&#21033;&#29992;&#21644;&#37325;&#29992;&#19968;&#20010;&#31934;&#24515;&#24320;&#21457;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#30446;&#26631;&#39046;&#22495;&#35299;&#20915;&#20219;&#21153;&#32780;&#26080;&#38656;&#36827;&#34892;&#27169;&#22411;&#32454;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#26426;&#22120;&#23398;&#20064;&#65292;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#21487;&#20197;&#24046;&#24322;&#24040;&#22823;&#12290;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#20248;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;&#20174;&#22836;&#35757;&#32451;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#27169;&#22411;&#37325;&#26032;&#32534;&#31243;&#30340;&#26041;&#27861;&#35770;&#65292;&#24182;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-rich domains such as vision, language, and speech, deep learning prevails to deliver high-performance task-specific models and can even learn general task-agnostic representations for efficient finetuning to downstream tasks. However, deep learning in resource-limited domains still faces multiple challenges including (i) limited data, (ii) constrained model development cost, and (iii) lack of adequate pre-trained models for effective finetuning. This paper provides an overview of model reprogramming to bridge this gap. Model reprogramming enables resource-efficient cross-domain machine learning by repurposing and reusing a well-developed pre-trained model from a source domain to solve tasks in a target domain without model finetuning, where the source and target domains can be vastly different. In many applications, model reprogramming outperforms transfer learning and training from scratch. This paper elucidates the methodology of model reprogramming, summarizes existing use c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#21518;&#36890;&#36807;&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#22312;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#34920;&#24449;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#20108;&#20803;&#26631;&#31614;&#30340;&#30001;XOR-like&#20989;&#25968;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#23613;&#31649;&#32447;&#24615;&#20998;&#31867;&#22120;&#22312;&#35813;&#20998;&#24067;&#19978;&#26080;&#27861;&#26356;&#22909;&#22320;&#24037;&#20316;&#65292;&#20294;&#35813;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#25509;&#36817;&#20110;&#26631;&#31614;&#22122;&#22768;&#29575;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#35777;&#26126;&#25216;&#24039;&#65292;&#25581;&#31034;&#20102;&#21021;&#22987;&#21270;&#26102;&#22823;&#22810;&#25968;&#31070;&#32463;&#20803;&#20316;&#20026;&#38543;&#26426;&#29305;&#24449;&#65292;&#38543;&#21518;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#23558;&#36825;&#20123;&#24369;&#30340;&#38543;&#26426;&#29305;&#24449;&#25918;&#22823;&#20026;&#26377;&#29992;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2202.07626</link><description>&lt;p&gt;
&#38543;&#26426;&#29305;&#24449;&#25918;&#22823;&#65306;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Random Feature Amplification: Feature Learning and Generalization in Neural Networks. (arXiv:2202.07626v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.07626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#21518;&#36890;&#36807;&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#22312;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#34920;&#24449;&#65292;&#35777;&#26126;&#20102;&#23545;&#20110;&#20855;&#26377;&#20108;&#20803;&#26631;&#31614;&#30340;&#30001;XOR-like&#20989;&#25968;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#23613;&#31649;&#32447;&#24615;&#20998;&#31867;&#22120;&#22312;&#35813;&#20998;&#24067;&#19978;&#26080;&#27861;&#26356;&#22909;&#22320;&#24037;&#20316;&#65292;&#20294;&#35813;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#25509;&#36817;&#20110;&#26631;&#31614;&#22122;&#22768;&#29575;&#12290;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#35777;&#26126;&#25216;&#24039;&#65292;&#25581;&#31034;&#20102;&#21021;&#22987;&#21270;&#26102;&#22823;&#22810;&#25968;&#31070;&#32463;&#20803;&#20316;&#20026;&#38543;&#26426;&#29305;&#24449;&#65292;&#38543;&#21518;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#23558;&#36825;&#20123;&#24369;&#30340;&#38543;&#26426;&#29305;&#24449;&#25918;&#22823;&#20026;&#26377;&#29992;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#30001;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#21518;&#65292;&#36890;&#36807;&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#23545;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#32771;&#34385;&#30001;&#36755;&#20837;&#29305;&#24449;&#30340;XOR-like&#20989;&#25968;&#29983;&#25104;&#30340;&#20855;&#26377;&#20108;&#20803;&#26631;&#31614;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#20801;&#35768;&#19968;&#20010;&#22266;&#23450;&#27604;&#20363;&#30340;&#35757;&#32451;&#26631;&#31614;&#34987;&#23545;&#25163;&#25439;&#22351;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23613;&#31649;&#32447;&#24615;&#20998;&#31867;&#22120;&#23545;&#20110;&#25105;&#20204;&#32771;&#34385;&#30340;&#20998;&#24067;&#32780;&#35328;&#19981;&#27604;&#38543;&#26426;&#29468;&#27979;&#26356;&#22909;&#65292;&#20294;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#25509;&#36817;&#20110;&#26631;&#31614;&#22122;&#22768;&#29575;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35777;&#26126;&#25216;&#24039;&#65292;&#34920;&#26126;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#32477;&#22823;&#22810;&#25968;&#31070;&#32463;&#20803;&#20316;&#20026;&#20165;&#19982;&#26377;&#29992;&#29305;&#24449;&#24369;&#30456;&#20851;&#30340;&#38543;&#26426;&#29305;&#24449;&#65292;&#32780;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#23558;&#36825;&#20123;&#24369;&#30340;&#38543;&#26426;&#29305;&#24449;&#25918;&#22823;&#20026;&#24378;&#26377;&#29992;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we provide a characterization of the feature-learning process in two-layer ReLU networks trained by gradient descent on the logistic loss following random initialization. We consider data with binary labels that are generated by an XOR-like function of the input features. We permit a constant fraction of the training labels to be corrupted by an adversary. We show that, although linear classifiers are no better than random guessing for the distribution we consider, two-layer ReLU networks trained by gradient descent achieve generalization error close to the label noise rate. We develop a novel proof technique that shows that at initialization, the vast majority of neurons function as random features that are only weakly correlated with useful features, and the gradient descent dynamics 'amplify' these weak, random features to strong, useful features.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#26102;&#33021;&#22815;&#24456;&#22909;&#24212;&#23545;&#22122;&#22768;&#25968;&#25454;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23558;&#35757;&#32451;&#35823;&#24046;&#38477;&#33267;&#38646;&#24182;&#23436;&#32654;&#22320;&#36866;&#24212;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#24182;&#21516;&#26102;&#36798;&#21040;&#26368;&#20248;&#30340;&#27979;&#35797;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2202.05928</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#32447;&#24615;&#20851;&#31995;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#65306;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#29992;&#20110;&#22122;&#22768;&#32447;&#24615;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Benign Overfitting without Linearity: Neural Network Classifiers Trained by Gradient Descent for Noisy Linear Data. (arXiv:2202.05928v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#26102;&#33021;&#22815;&#24456;&#22909;&#24212;&#23545;&#22122;&#22768;&#25968;&#25454;&#30340;&#33391;&#24615;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23558;&#35757;&#32451;&#35823;&#24046;&#38477;&#33267;&#38646;&#24182;&#23436;&#32654;&#22320;&#36866;&#24212;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#24182;&#21516;&#26102;&#36798;&#21040;&#26368;&#20248;&#30340;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#24615;&#36807;&#25311;&#21512;&#26159;&#25351;&#25554;&#20540;&#27169;&#22411;&#22312;&#23384;&#22312;&#22122;&#22768;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#30340;&#29616;&#35937;&#65292;&#26368;&#26089;&#20986;&#29616;&#22312;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#23454;&#35777;&#35266;&#23519;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26426;&#21021;&#22987;&#21270;&#21518;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#22312;&#36923;&#36753;&#25439;&#22833;&#20989;&#25968;&#19978;&#36827;&#34892;&#25554;&#20540;&#35757;&#32451;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#26469;&#33258;&#20110;&#26126;&#26174;&#20998;&#31163;&#30340;&#31867;&#26465;&#20214;&#23545;&#25968;&#20985;&#20998;&#24067;&#65292;&#24182;&#20801;&#35768;&#35757;&#32451;&#26631;&#31614;&#20013;&#30340;&#19968;&#23450;&#27604;&#20363;&#34987;&#23545;&#25163;&#31713;&#25913;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#33391;&#24615;&#36807;&#25311;&#21512;&#30340;&#29305;&#28857;&#65306;&#23427;&#20204;&#21487;&#20197;&#34987;&#39537;&#21160;&#21040;&#38646;&#35757;&#32451;&#35823;&#24046;&#65292;&#23436;&#32654;&#22320;&#25311;&#21512;&#20219;&#20309;&#26377;&#22122;&#22768;&#30340;&#35757;&#32451;&#26631;&#31614;&#65292;&#24182;&#21516;&#26102;&#36798;&#21040;&#26497;&#23567;&#21270;&#26368;&#22823;&#21270;&#26368;&#20248;&#27979;&#35797;&#35823;&#24046;&#12290;&#19982;&#20043;&#21069;&#20851;&#20110;&#33391;&#24615;&#36807;&#25311;&#21512;&#38656;&#35201;&#32447;&#24615;&#25110;&#22522;&#20110;&#26680;&#30340;&#39044;&#27979;&#22120;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#22312;&#27169;&#22411;&#21644;&#23398;&#20064;&#21160;&#24577;&#37117;&#26159;&#22522;&#26412;&#38750;&#32447;&#24615;&#30340;&#24773;&#20917;&#19979;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Benign overfitting, the phenomenon where interpolating models generalize well in the presence of noisy data, was first observed in neural network models trained with gradient descent. To better understand this empirical observation, we consider the generalization error of two-layer neural networks trained to interpolation by gradient descent on the logistic loss following random initialization. We assume the data comes from well-separated class-conditional log-concave distributions and allow for a constant fraction of the training labels to be corrupted by an adversary. We show that in this setting, neural networks exhibit benign overfitting: they can be driven to zero training error, perfectly fitting any noisy training labels, and simultaneously achieve minimax optimal test error. In contrast to previous work on benign overfitting that require linear or kernel-based predictors, our analysis holds in a setting where both the model and learning dynamics are fundamentally nonlinear.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PolicyCleanse&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28608;&#27963;&#30340;&#29305;&#27931;&#20234;&#26234;&#33021;&#20307;&#32047;&#31215;&#22870;&#21169;&#30340;&#19979;&#38477;&#29305;&#24615;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#23581;&#35797;&#32531;&#35299;&#20854;&#29305;&#27931;&#20234;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2202.03609</link><description>&lt;p&gt;
PolicyCleanse&#65306;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#26816;&#27979;&#19982;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning. (arXiv:2202.03609v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PolicyCleanse&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#32531;&#35299;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#28608;&#27963;&#30340;&#29305;&#27931;&#20234;&#26234;&#33021;&#20307;&#32047;&#31215;&#22870;&#21169;&#30340;&#19979;&#38477;&#29305;&#24615;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#23581;&#35797;&#32531;&#35299;&#20854;&#29305;&#27931;&#20234;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;RL&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#20581;&#24615;&#20173;&#20540;&#24471;&#26356;&#22810;&#20851;&#27880;&#21644;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#21521;&#21463;&#23475;&#32773;&#26234;&#33021;&#20307;&#65288;&#21363;&#29305;&#27931;&#20234;&#26234;&#33021;&#20307;&#65289;&#27880;&#20837;&#21518;&#38376;&#35302;&#21457;&#21160;&#20316;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#22833;&#36133;&#12290;&#20026;&#30830;&#20445;RL&#26234;&#33021;&#20307;&#23545;&#24694;&#24847;&#21518;&#38376;&#30340;&#23433;&#20840;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#24615;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#21518;&#38376;&#26816;&#27979;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#26816;&#27979;&#29305;&#27931;&#20234;&#26234;&#33021;&#20307;&#20197;&#21450;&#30456;&#24212;&#30340;&#28508;&#22312;&#35302;&#21457;&#21160;&#20316;&#65292;&#24182;&#36827;&#19968;&#27493;&#23581;&#35797;&#32531;&#35299;&#20854;&#29305;&#27931;&#20234;&#34892;&#20026;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28608;&#27963;&#30340;&#29305;&#27931;&#20234;&#26234;&#33021;&#20307;&#32047;&#31215;&#22870;&#21169;&#22312;&#20960;&#20010;&#26102;&#38388;&#27493;&#20043;&#21518;&#26126;&#26174;&#19979;&#38477;&#30340;PolicyCleanse&#12290;&#38500;&#20102;PolicyCleanse&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26426;&#22120;&#26410;&#23436;&#25104;&#30340;&#37096;&#20998;...
&lt;/p&gt;
&lt;p&gt;
While real-world applications of reinforcement learning are becoming popular, the security and robustness of RL systems are worthy of more attention and exploration. In particular, recent works have revealed that, in a multi-agent RL environment, backdoor trigger actions can be injected into a victim agent (a.k.a. Trojan agent), which can result in a catastrophic failure as soon as it sees the backdoor trigger action. To ensure the security of RL agents against malicious backdoors, in this work, we propose the problem of Backdoor Detection in a multi-agent competitive reinforcement learning system, with the objective of detecting Trojan agents as well as the corresponding potential trigger actions, and further trying to mitigate their Trojan behavior. In order to solve this problem, we propose PolicyCleanse that is based on the property that the activated Trojan agents accumulated rewards degrade noticeably after several timesteps. Along with PolicyCleanse, we also design a machine unl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;$S_n$-&#31561;&#21464;&#21367;&#31215;&#37327;&#23376;Ansatze&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#22312;&#20855;&#26377;SU($d$)&#23545;&#31216;&#24615;&#30340;&#24191;&#27867;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#29983;&#25104;&#20219;&#24847;&#24186;&#27491;&#30697;&#38453;&#65292;&#21516;&#26102;&#39564;&#35777;&#20102;4-local SU($d$)&#23545;&#31216;&#24186;&#27491;&#30697;&#38453;&#30340;&#21487;&#23454;&#29616;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.07611</link><description>&lt;p&gt;
&#36890;&#36807;&#32676;&#31561;&#21464;&#21367;&#31215;&#37327;&#23376;Ansatze&#21152;&#36895;&#23398;&#20064;&#37327;&#23376;&#24577;
&lt;/p&gt;
&lt;p&gt;
Speeding up Learning Quantum States through Group Equivariant Convolutional Quantum Ans\"atze. (arXiv:2112.07611v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.07611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;$S_n$-&#31561;&#21464;&#21367;&#31215;&#37327;&#23376;Ansatze&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#22312;&#20855;&#26377;SU($d$)&#23545;&#31216;&#24615;&#30340;&#24191;&#27867;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#29983;&#25104;&#20219;&#24847;&#24186;&#27491;&#30697;&#38453;&#65292;&#21516;&#26102;&#39564;&#35777;&#20102;4-local SU($d$)&#23545;&#31216;&#24186;&#27491;&#30697;&#38453;&#30340;&#21487;&#23454;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22522;&#20110;Schur-Weyl&#23545;&#20598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;$S_n$-&#31561;&#21464;&#21367;&#31215;&#37327;&#23376;&#30005;&#36335;&#21644;SU$(d)$&#23545;&#31216;&#24615;&#65292;&#23558;Jordan&#30340;&#32622;&#25442;&#37327;&#23376;&#35745;&#31639;(PQC)&#24418;&#24335;&#20027;&#20041;&#25193;&#23637;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;Okounkov-Vershik&#26041;&#27861;&#26469;&#35777;&#26126;Harrow&#22312;&#22855;&#24322;&#36793;&#34920;&#31034;&#22522;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#21033;&#29992;Young-Jucys-Murphy(YJM)&#20803;&#32032;&#24314;&#31435;&#20102;$S_n$-&#31561;&#21464;&#21367;&#31215;&#37327;&#23376;&#20132;&#26367;Ansatze($S_n$-CQA)&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;$S_n$-CQA&#33021;&#22815;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;$S_n$ irrep&#25159;&#21306;&#20013;&#29983;&#25104;&#20219;&#24847;&#24186;&#27491;&#30697;&#38453;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#20855;&#26377;SU($d$)&#23545;&#31216;&#24615;&#30340;&#24191;&#27867;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#21478;&#19968;&#31181;&#35777;&#26126;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;(QAOA)&#30340;&#26222;&#36866;&#24615;&#30340;&#26041;&#24335;&#65292;&#24182;&#39564;&#35777;&#20102;4-local SU($d$)&#23545;&#31216;&#24186;&#27491;&#30697;&#38453;&#26159;&#21487;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a theoretical framework for $S_n$-equivariant convolutional quantum circuits with SU$(d)$-symmetry, building on and significantly generalizing Jordan's Permutational Quantum Computing (PQC) formalism based on Schur-Weyl duality connecting both SU$(d)$ and $S_n$ actions on qudits. In particular, we utilize the Okounkov-Vershik approach to prove Harrow's statement (Ph.D. Thesis 2005 p.160) on the equivalence between $\operatorname{SU}(d)$ and $S_n$ irrep bases and to establish the $S_n$-equivariant Convolutional Quantum Alternating Ans\"atze ($S_n$-CQA) using Young-Jucys-Murphy (YJM) elements. We prove that $S_n$-CQA is able to generate any unitary in any given $S_n$ irrep sector, which may serve as a universal model for a wide array of quantum machine learning problems with the presence of SU($d$) symmetry. Our method provides another way to prove the universality of Quantum Approximate Optimization Algorithm (QAOA) and verifies that 4-local SU($d$) symmetric unitaries are su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Pareto Adversarial Robustness&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#31354;&#38388;&#40065;&#26834;&#24615;&#26041;&#27861;&#21644;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#29616;&#20102;&#36890;&#29992;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#20174;&#40065;&#26834;&#34920;&#31034;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#33258;&#28982;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#40065;&#26834;&#24615;&#21644;&#31354;&#38388;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2111.01996</link><description>&lt;p&gt;
Pareto&#23545;&#25239;&#40065;&#26834;&#24615;&#65306;&#24179;&#34913;&#31354;&#38388;&#40065;&#26834;&#24615;&#21644;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Pareto Adversarial Robustness: Balancing Spatial Robustness and Sensitivity-based Robustness. (arXiv:2111.01996v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Pareto Adversarial Robustness&#31574;&#30053;&#65292;&#36890;&#36807;&#25972;&#21512;&#31354;&#38388;&#40065;&#26834;&#24615;&#26041;&#27861;&#21644;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#29616;&#20102;&#36890;&#29992;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#20174;&#40065;&#26834;&#34920;&#31034;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#33258;&#28982;&#20934;&#30830;&#24615;&#12289;&#25935;&#24863;&#24615;&#40065;&#26834;&#24615;&#21644;&#31354;&#38388;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#40065;&#26834;&#24615;&#20027;&#35201;&#21253;&#25324;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#40065;&#26834;&#24615;&#21644;&#31354;&#38388;&#40065;&#26834;&#24615;&#65292;&#22312;&#23454;&#29616;&#40065;&#26834;&#27867;&#21270;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#23454;&#29616;&#36890;&#29992;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#30340;&#31354;&#38388;&#40065;&#26834;&#24615;&#39046;&#22495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#31354;&#38388;&#40065;&#26834;&#24615;&#26041;&#27861;&#25972;&#21512;&#36215;&#26469;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#31354;&#38388;&#30340;&#33030;&#24369;&#24615;&#32467;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#31354;&#38388;&#25915;&#20987;&#21644;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#40065;&#26834;&#34920;&#31034;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#33258;&#28982;&#20934;&#30830;&#24615;&#12289;&#22522;&#20110;&#25935;&#24863;&#24615;&#30340;&#40065;&#26834;&#24615;&#21644;&#31354;&#38388;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20840;&#38754;&#20851;&#31995;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#20851;&#38190;&#26159;&#65292;&#20026;&#20102;&#23558;&#21508;&#31181;&#40065;&#26834;&#24615;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#32435;&#20837;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#23558;&#24085;&#32047;&#25176;&#20934;&#21017;&#24341;&#20837;&#21040;&#23545;&#25239;&#40065;&#26834;&#24615;&#20998;&#26512;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pareto Adversarial Robustness&#30340;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness, which primarily comprises sensitivity-based robustness and spatial robustness, plays an integral part in achieving robust generalization. In this paper, we endeavor to design strategies to achieve universal adversarial robustness. To achieve this, we first investigate the relatively less-explored realm of spatial robustness. Then, we integrate the existing spatial robustness methods by incorporating both local and global spatial vulnerability into a unified spatial attack and adversarial training approach. Furthermore, we present a comprehensive relationship between natural accuracy, sensitivity-based robustness, and spatial robustness, supported by strong evidence from the perspective of robust representation. Crucially, to reconcile the interplay between the mutual impacts of various robustness components into one unified framework, we incorporate the \textit{Pareto criterion} into the adversarial robustness analysis, yielding a novel strategy called Pareto Ad
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25286;&#20998;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(SVQ-VAE)&#26550;&#26500;&#30340;&#31163;&#25955;&#22768;&#23398;&#31354;&#38388;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#26082;&#20445;&#30041;&#20102;&#20351;&#29992;&#35805;&#35821;&#32423;&#38480;&#21046;&#30340;&#22909;&#22788;&#65292;&#21448;&#20855;&#26377;&#36275;&#22815;&#23567;&#30340;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#20197;&#20174;&#25991;&#26412;&#36827;&#34892;&#39640;&#25928;&#39044;&#27979;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SVQ-VAE&#22312;&#33258;&#28982;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20174;&#25991;&#26412;&#36827;&#34892;&#39044;&#27979;&#65292;&#20943;&#23569;&#20102;&#21512;&#25104;&#21644;&#24405;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2110.12539</link><description>&lt;p&gt;
&#29992;&#20110;&#31070;&#32463;&#25991;&#26412;&#21040;&#35821;&#38899;&#30340;&#39640;&#25928;&#37319;&#26679;&#30340;&#31163;&#25955;&#22768;&#23398;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Discrete Acoustic Space for an Efficient Sampling in Neural Text-To-Speech. (arXiv:2110.12539v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.12539
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25286;&#20998;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(SVQ-VAE)&#26550;&#26500;&#30340;&#31163;&#25955;&#22768;&#23398;&#31354;&#38388;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#26082;&#20445;&#30041;&#20102;&#20351;&#29992;&#35805;&#35821;&#32423;&#38480;&#21046;&#30340;&#22909;&#22788;&#65292;&#21448;&#20855;&#26377;&#36275;&#22815;&#23567;&#30340;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#20197;&#20174;&#25991;&#26412;&#36827;&#34892;&#39640;&#25928;&#39044;&#27979;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SVQ-VAE&#22312;&#33258;&#28982;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20174;&#25991;&#26412;&#36827;&#34892;&#39044;&#27979;&#65292;&#20943;&#23569;&#20102;&#21512;&#25104;&#21644;&#24405;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25286;&#20998;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(SVQ-VAE)&#26550;&#26500;&#30340;&#31163;&#25955;&#22768;&#23398;&#31354;&#38388;&#65292;&#20316;&#20026;&#23545;&#30693;&#21517;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#21644;&#21521;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VQ-VAE)&#26550;&#26500;&#30340;&#22686;&#24378;&#12290;&#19982;&#20043;&#21069;&#30340;&#26550;&#26500;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20445;&#30041;&#20102;&#20351;&#29992;&#35805;&#35821;&#32423;&#38480;&#21046;&#30340;&#22909;&#22788;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#26174;&#33879;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#19988;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#23567;&#21040;&#36275;&#22815;&#20174;&#25991;&#26412;&#36827;&#34892;&#39640;&#25928;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#34920;&#36798;&#24615;&#20219;&#21153;&#23545;&#35805;&#39046;&#22495;&#30340;&#24405;&#38899;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;SVQ-VAE&#22312;&#33258;&#28982;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;VAE&#21644;VQ-VAE&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SVQ-VAE&#28508;&#22312;&#22768;&#23398;&#31354;&#38388;&#21487;&#20174;&#25991;&#26412;&#36827;&#34892;&#39044;&#27979;&#65292;&#23558;&#26631;&#20934;&#30340;&#24658;&#23450;&#21521;&#37327;&#21512;&#25104;&#21644;&#22768;&#30721;&#22120;&#24405;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#20943;&#23569;&#20102;32%&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a Split Vector Quantized Variational Autoencoder (SVQ-VAE) architecture using a split vector quantizer for NTTS, as an enhancement to the well-known Variational Autoencoder (VAE) and Vector Quantized Variational Autoencoder (VQ-VAE) architectures. Compared to these previous architectures, our proposed model retains the benefits of using an utterance-level bottleneck, while keeping significant representation power and a discretized latent space small enough for efficient prediction from text. We train the model on recordings in the expressive task-oriented dialogues domain and show that SVQ-VAE achieves a statistically significant improvement in naturalness over the VAE and VQ-VAE models. Furthermore, we demonstrate that the SVQ-VAE latent acoustic space is predictable from text, reducing the gap between the standard constant vector synthesis and vocoded recordings by 32%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21322;&#23450;&#35268;&#21010;&#26469;&#32852;&#21512;&#31038;&#21306;&#26816;&#27979;&#21644;&#26059;&#36716;&#21516;&#27493;&#30340;&#26041;&#27861;&#65292;&#22312;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20934;&#30830;&#24674;&#22797;&#20986;&#26059;&#36716;&#21644;&#32858;&#31867;&#36523;&#20221;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24674;&#22797;&#24615;&#30340;&#30456;&#21464;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2105.06031</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#23450;&#35268;&#21010;&#65292;&#32852;&#21512;&#31038;&#21306;&#26816;&#27979;&#21644;&#26059;&#36716;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Joint Community Detection and Rotational Synchronization via Semidefinite Programming. (arXiv:2105.06031v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.06031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21322;&#23450;&#35268;&#21010;&#26469;&#32852;&#21512;&#31038;&#21306;&#26816;&#27979;&#21644;&#26059;&#36716;&#21516;&#27493;&#30340;&#26041;&#27861;&#65292;&#22312;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20934;&#30830;&#24674;&#22797;&#20986;&#26059;&#36716;&#21644;&#32858;&#31867;&#36523;&#20221;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20934;&#30830;&#24674;&#22797;&#24615;&#30340;&#30456;&#21464;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23384;&#22312;&#24322;&#26500;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#26059;&#36716;&#30340;&#23545;&#35937;&#33853;&#20837;&#22810;&#20010;&#28508;&#22312;&#31867;&#21035;&#20013;&#65292;&#21516;&#26102;&#23545;&#23427;&#20204;&#36827;&#34892;&#32858;&#31867;&#21644;&#22522;&#20110;&#20004;&#20004;&#20851;&#31995;&#36827;&#34892;&#21516;&#27493;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#31038;&#21306;&#26816;&#27979;&#21644;&#21516;&#27493;&#30340;&#32852;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#21322;&#23450;&#26494;&#24347;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#23558;&#33879;&#21517;&#30340;&#38543;&#26426;&#22359;&#27169;&#22411;&#25193;&#23637;&#21040;&#26059;&#36716;&#21644;&#32858;&#31867;&#36523;&#20221;&#37117;&#38656;&#35201;&#30830;&#23450;&#30340;&#26032;&#24773;&#20917;&#26102;&#30340;&#31934;&#30830;&#24674;&#22797;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#34920;&#26126;&#20102;&#20934;&#30830;&#24674;&#22797;&#24615;&#30340;&#26126;&#26174;&#30456;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the presence of heterogeneous data, where randomly rotated objects fall into multiple underlying categories, it is challenging to simultaneously classify them into clusters and synchronize them based on pairwise relations. This gives rise to the joint problem of community detection and synchronization. We propose a series of semidefinite relaxations, and prove their exact recovery when extending the celebrated stochastic block model to this new setting where both rotations and cluster identities are to be determined. Numerical experiments demonstrate the efficacy of our proposed algorithms and confirm our theoretical result which indicates a sharp phase transition for exact recovery.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#20511;&#37492;&#20102;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#30340;&#36866;&#29992;&#24615;&#22495;&#27010;&#24565;&#65292;&#36890;&#36807;&#21028;&#26029;&#26679;&#26412;&#26159;&#21542;&#22312;&#20998;&#31867;&#22120;&#30340;&#22495;&#20869;&#26469;&#38459;&#25377;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2105.00495</link><description>&lt;p&gt;
BAARD&#65306;&#36890;&#36807;&#26816;&#27979;&#36866;&#29992;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#20915;&#31574;&#24615;&#26469;&#38459;&#25377;&#23545;&#25239;&#24615;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
BAARD: Blocking Adversarial Examples by Testing for Applicability, Reliability and Decidability. (arXiv:2105.00495v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.00495
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#20511;&#37492;&#20102;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#30340;&#36866;&#29992;&#24615;&#22495;&#27010;&#24565;&#65292;&#36890;&#36807;&#21028;&#26029;&#26679;&#26412;&#26159;&#21542;&#22312;&#20998;&#31867;&#22120;&#30340;&#22495;&#20869;&#26469;&#38459;&#25377;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#38450;&#24481;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20813;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20294;&#24448;&#24448;&#38024;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#27169;&#22411;&#25110;&#25915;&#20987;&#36827;&#34892;&#23450;&#21046;&#12290;&#32570;&#20047;&#20851;&#20110;&#26410;&#30693;&#28508;&#22312;&#25915;&#20987;&#30340;&#20449;&#24687;&#20351;&#24471;&#26816;&#27979;&#23545;&#25239;&#24615;&#26679;&#26412;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#25915;&#20987;&#32773;&#19981;&#38656;&#35201;&#36981;&#23432;&#38450;&#24481;&#32773;&#21046;&#23450;&#30340;&#35268;&#21017;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#21270;&#23398;&#20449;&#24687;&#23398;&#20013;&#30340;&#36866;&#29992;&#24615;&#22495;&#30340;&#27010;&#24565;&#12290;&#21270;&#23398;&#20449;&#24687;&#23398;&#27169;&#22411;&#20043;&#25152;&#20197;&#38590;&#20197;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#26159;&#22240;&#20026;&#21482;&#26377;&#26377;&#38480;&#25968;&#37327;&#30340;&#21270;&#21512;&#29289;&#34987;&#30693;&#26195;&#24182;&#21487;&#29992;&#20110;&#35757;&#32451;&#12290;&#36866;&#29992;&#24615;&#22495;&#26681;&#25454;&#24050;&#30693;&#21270;&#21512;&#29289;&#23450;&#20041;&#19968;&#20010;&#22495;&#65292;&#24182;&#25298;&#32477;&#20219;&#20309;&#33853;&#22312;&#22495;&#22806;&#30340;&#26410;&#30693;&#21270;&#21512;&#29289;&#12290;&#31867;&#20284;&#22320;&#65292;&#23545;&#25239;&#24615;&#26679;&#26412;&#26368;&#21021;&#26159;&#26080;&#23475;&#30340;&#36755;&#20837;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#31227;&#21160;&#21040;&#20998;&#31867;&#22120;&#22495;&#22806;&#26469;&#25805;&#32437;&#20197;&#36867;&#36991;&#21487;&#38752;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#35782;&#21035;&#20986;&#36866;&#29992;&#24615;&#22495;&#19982;&#23545;&#25239;&#24615;&#26816;&#27979;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#19981;&#20877;&#20851;&#27880;&#26410;&#30693;&#25915;&#20987;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#26816;&#27979;&#26679;&#26412;&#26159;&#21542;&#22312;&#20998;&#31867;&#22120;&#30340;&#22495;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial defenses protect machine learning models from adversarial attacks, but are often tailored to one type of model or attack. The lack of information on unknown potential attacks makes detecting adversarial examples challenging. Additionally, attackers do not need to follow the rules made by the defender. To address this problem, we take inspiration from the concept of Applicability Domain in cheminformatics. Cheminformatics models struggle to make accurate predictions because only a limited number of compounds are known and available for training. Applicability Domain defines a domain based on the known compounds and rejects any unknown compound that falls outside the domain. Similarly, adversarial examples start as harmless inputs, but can be manipulated to evade reliable classification by moving outside the domain of the classifier. We are the first to identify the similarity between Applicability Domain and adversarial detection. Instead of focusing on unknown attacks, we f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#27169;&#22411;&#30340;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#32570;&#22833;&#38750;&#38543;&#26426;&#25130;&#23614;&#25351;&#26631;&#19979;&#30340;&#26465;&#20214;&#29983;&#23384;&#20989;&#25968;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#20272;&#35745;&#22120;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2009.01726</link><description>&lt;p&gt;
&#22522;&#20110;Copula&#27169;&#22411;&#30340;&#32570;&#22833;&#38750;&#38543;&#26426;&#25130;&#23614;&#25351;&#26631;&#30340;&#29983;&#23384;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Survival Estimation for Missing not at Random Censoring Indicators based on Copula Models. (arXiv:2009.01726v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.01726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Copula&#27169;&#22411;&#30340;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#32570;&#22833;&#38750;&#38543;&#26426;&#25130;&#23614;&#25351;&#26631;&#19979;&#30340;&#26465;&#20214;&#29983;&#23384;&#20989;&#25968;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#20272;&#35745;&#22120;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#21327;&#21464;&#37327;&#30340;&#21491;&#25130;&#23614;&#25968;&#25454;&#20013;&#65292;&#26465;&#20214;Kaplan-Meier&#20272;&#35745;&#22120;&#65288;&#20063;&#31216;&#20026;Beran&#20272;&#35745;&#22120;&#65289;&#19968;&#33268;&#20272;&#35745;&#24863;&#20852;&#36259;&#20107;&#20214;&#30340;&#38543;&#26426;&#36861;&#36394;&#30340;&#26465;&#20214;&#29983;&#23384;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#24517;&#35201;&#26465;&#20214;&#26159;&#23545;&#27599;&#20010;&#20010;&#20307;&#26159;&#21542;&#34987;&#25130;&#23614;&#26377;&#26126;&#30830;&#30340;&#20102;&#35299;&#65292;&#32780;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26159;&#19981;&#23436;&#25972;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24403;&#25130;&#23614;&#25351;&#26631;&#26159;&#19968;&#33324;&#38543;&#26426;&#21464;&#37327;&#26102;&#23545;Beran&#20272;&#35745;&#22120;&#36827;&#34892;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;Beran&#20272;&#35745;&#22120;&#25928;&#29575;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#22522;&#20110;&#32570;&#22833;&#38750;&#38543;&#26426;&#65288;MNAR&#65289;&#25130;&#23614;&#25351;&#26631;&#30340;&#26465;&#20214;Copula&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#29983;&#23384;&#20989;&#25968;&#20272;&#35745;&#22120;&#12290;&#38500;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#30740;&#31350;&#23545;&#23567;&#26679;&#26412;&#20013;&#30340;&#20272;&#35745;&#22120;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the presence of right-censored data with covariates, the conditional Kaplan-Meier estimator (also known as the Beran estimator) consistently estimates the conditional survival function of the random follow-up for the event of interest. However, a necessary condition is the unambiguous knowledge of whether each individual is censored or not, which may be incomplete in practice. We therefore propose a study of the Beran estimator when the censoring indicators are generic random variables and discuss necessary conditions for the efficiency of the Beran estimator. From this, we provide a new estimator for the conditional survival function with missing not at random (MNAR) censoring indicators based on a conditional copula model for the missingness mechanism. In addition to the theoretical results, we illustrate how the estimators work for small samples through a simulation study and show their practical applicability by analyzing synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#38500;&#38750;P=NP&#65292;&#21542;&#21017;&#19981;&#23384;&#22312;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#22312;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;$c^n$&#65288;&#23545;&#20110;&#20219;&#24847;&#24120;&#25968;$c \ge 0$&#65289;&#20869;&#30340;&#20108;&#27425;&#20989;&#25968;&#22312;&#22810;&#38754;&#20307;&#19978;&#30340;&#23616;&#37096;&#26368;&#23567;&#21270;&#28857;&#12290;</title><link>http://arxiv.org/abs/2008.05558</link><description>&lt;p&gt;
&#22312;&#22810;&#38754;&#20307;&#19978;&#23547;&#25214;&#20108;&#27425;&#20989;&#25968;&#30340;&#23616;&#37096;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the complexity of finding a local minimizer of a quadratic function over a polytope. (arXiv:2008.05558v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2008.05558
&lt;/p&gt;
&lt;p&gt;
&#38500;&#38750;P=NP&#65292;&#21542;&#21017;&#19981;&#23384;&#22312;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#22312;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;$c^n$&#65288;&#23545;&#20110;&#20219;&#24847;&#24120;&#25968;$c \ge 0$&#65289;&#20869;&#30340;&#20108;&#27425;&#20989;&#25968;&#22312;&#22810;&#38754;&#20307;&#19978;&#30340;&#23616;&#37096;&#26368;&#23567;&#21270;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#65292;&#38500;&#38750;P=NP&#65292;&#21542;&#21017;&#19981;&#23384;&#22312;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#33021;&#22815;&#25214;&#21040;&#19968;&#20010;&#22312;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;$c^n$&#65288;&#23545;&#20110;&#20219;&#24847;&#24120;&#25968;$c \ge 0$&#65289;&#20869;&#30340;&#20108;&#27425;&#20989;&#25968;&#22312;&#22810;&#38754;&#20307;&#19978;&#30340;&#23616;&#37096;&#26368;&#23567;&#21270;&#28857;&#12290;&#36825;&#20010;&#32467;&#26524;&#65288;&#21363;&#20351;&#22312;$c=0$&#30340;&#24773;&#20917;&#19979;&#65289;&#22238;&#31572;&#20102;Pardalos&#21644;Vavasis&#22312;1992&#24180;&#20851;&#20110;&#25968;&#20540;&#20248;&#21270;&#22797;&#26434;&#24615;&#29702;&#35770;&#20013;&#25552;&#20986;&#30340;&#19968;&#20010;&#30001;&#19971;&#20010;&#24320;&#25918;&#38382;&#39064;&#32452;&#25104;&#30340;&#21015;&#34920;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#25216;&#24039;&#36824;&#24847;&#21619;&#30528;&#21028;&#26029;&#19968;&#20010;&#20108;&#27425;&#20989;&#25968;&#26159;&#21542;&#22312;&#65288;&#26080;&#30028;&#65289;&#22810;&#38754;&#20307;&#19978;&#26377;&#23616;&#37096;&#26368;&#23567;&#21270;&#28857;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#21028;&#26029;&#19968;&#20010;&#22235;&#27425;&#22810;&#39033;&#24335;&#26159;&#21542;&#26377;&#23616;&#37096;&#26368;&#23567;&#21270;&#28857;&#30340;&#38382;&#39064;&#37117;&#26159;NP&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that unless P=NP, there cannot be a polynomial-time algorithm that finds a point within Euclidean distance $c^n$ (for any constant $c \ge 0$) of a local minimizer of an $n$-variate quadratic function over a polytope. This result (even with $c=0$) answers a question of Pardalos and Vavasis that appeared in 1992 on a list of seven open problems in complexity theory for numerical optimization. Our proof technique also implies that the problem of deciding whether a quadratic function has a local minimizer over an (unbounded) polyhedron, and that of deciding if a quartic polynomial has a local minimizer are NP-hard.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38750;&#28176;&#36827;&#20998;&#26512;&#26159;&#39318;&#27425;&#36827;&#34892;&#30340;&#12290;</title><link>http://arxiv.org/abs/1906.00331</link><description>&lt;p&gt;
&#20851;&#20110;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems. (arXiv:1906.00331v9 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1906.00331
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#22797;&#26434;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#20989;&#25968;&#30340;&#31283;&#23450;&#28857;&#12290;&#35813;&#30740;&#31350;&#23545;&#20110;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#38750;&#28176;&#36827;&#20998;&#26512;&#26159;&#39318;&#27425;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#21363;$\min_{\mathbf{x}} \max_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y})$&#65292;&#20854;&#20013;$f$&#22312;$\mathbf{x}$&#19978;&#26159;&#38750;&#20984;&#30340;&#20294;&#22312;$\mathbf{y}$&#19978;&#26159;&#20985;&#30340;&#65292;$\mathcal{Y}$&#26159;&#19968;&#20010;&#20984;&#19988;&#26377;&#30028;&#30340;&#38598;&#21512;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#26368;&#24120;&#29992;&#30340;&#31639;&#27861;&#20043;&#19968;&#26159;&#33879;&#21517;&#30340;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;GDA&#65289;&#31639;&#27861;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#25511;&#21046;&#29702;&#35770;&#21644;&#32463;&#27982;&#23398;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#20984;-&#20985;&#35774;&#32622;&#19979;&#26377;&#30528;&#24191;&#27867;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#20294;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#30456;&#21516;&#27493;&#38271;&#30340;GDA&#21487;&#33021;&#20250;&#25910;&#25947;&#21040;&#26497;&#38480;&#29615;&#65292;&#29978;&#33267;&#21457;&#25955;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35299;&#20915;&#38750;&#20984;-&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#20004;&#26102;&#38388;&#23610;&#24230;GDA&#30340;&#22797;&#26434;&#24230;&#32467;&#26524;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#25214;&#21040;&#20989;&#25968;$\Phi(\cdot) := \max_{\mathbf{y} \in \mathcal{Y}} f(\cdot, \mathbf{y})$&#30340;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#30340;&#38750;&#28176;&#36827;&#20998;&#26512;&#32467;&#26524;&#65292;&#25581;&#31034;&#20102;&#26377;&#20851;&#20004;&#26102;&#38388;&#23610;&#24230;GDA&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider nonconvex-concave minimax problems, $\min_{\mathbf{x}} \max_{\mathbf{y} \in \mathcal{Y}} f(\mathbf{x}, \mathbf{y})$, where $f$ is nonconvex in $\mathbf{x}$ but concave in $\mathbf{y}$ and $\mathcal{Y}$ is a convex and bounded set. One of the most popular algorithms for solving this problem is the celebrated gradient descent ascent (GDA) algorithm, which has been widely used in machine learning, control theory and economics. Despite the extensive convergence results for the convex-concave setting, GDA with equal stepsize can converge to limit cycles or even diverge in a general setting. In this paper, we present the complexity results on two-time-scale GDA for solving nonconvex-concave minimax problems, showing that the algorithm can find a stationary point of the function $\Phi(\cdot) := \max_{\mathbf{y} \in \mathcal{Y}} f(\cdot, \mathbf{y})$ efficiently. To the best our knowledge, this is the first nonasymptotic analysis for two-time-scale GDA in this setting, shedding lig
&lt;/p&gt;</description></item></channel></rss>