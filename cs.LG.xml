<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>GeNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12289;&#21033;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#37325;&#24314;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;</title><link>https://arxiv.org/abs/2403.18296</link><description>&lt;p&gt;
GeNet:&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic Communication Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18296
&lt;/p&gt;
&lt;p&gt;
GeNet&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12289;&#21033;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#24182;&#21033;&#29992;&#35299;&#30721;&#22120;&#37325;&#24314;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25239;&#22122;&#22768;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#35821;&#20041;&#36890;&#20449;&#20219;&#21153;&#26041;&#27861;&#20381;&#36182;&#20110;&#20102;&#35299;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#26469;&#20943;&#36731;&#36890;&#36947;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22312;&#29305;&#23450;&#30340;SNR&#26465;&#20214;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GeNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35821;&#20041;&#36890;&#20449;&#33539;&#24335;&#65292;&#26088;&#22312;&#25269;&#25239;&#22122;&#22768;&#65292;&#20174;&#32780;&#20419;&#36827;&#20219;&#21153;&#23548;&#21521;&#36890;&#20449;&#65288;TOC&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#25968;&#25454;&#22270;&#20687;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#12290;&#28982;&#21518;&#21033;&#29992;&#22522;&#20110;GNN&#30340;&#32534;&#30721;&#22120;&#20174;&#28304;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#25552;&#21462;&#30340;&#35821;&#20041;&#20449;&#24687;&#28982;&#21518;&#36890;&#36807;&#36890;&#36947;&#20256;&#36755;&#12290;&#22312;&#25509;&#25910;&#31471;&#65292;&#20351;&#29992;&#22522;&#20110;GNN&#30340;&#35299;&#30721;&#22120;&#20174;&#28304;&#25968;&#25454;&#20013;&#37325;&#24314;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#20197;&#29992;&#20110;TOC&#12290;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GeNet&#22312;&#25239;&#22122;&#22768;TOC&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18296v1 Announce Type: cross  Abstract: Traditional approaches to semantic communication tasks rely on the knowledge of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these methods necessitate training under specific SNR conditions, entailing considerable time and computational resources. In this paper, we propose GeNet, a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at combating noise, thereby facilitating Task-Oriented Communication (TOC). We propose a novel approach where we first transform the input data image into graph structures. Then we leverage a GNN-based encoder to extract semantic information from the source data. This extracted semantic information is then transmitted through the channel. At the receiver's end, a GNN-based decoder is utilized to reconstruct the relevant semantic information from the source data for TOC. Through experimental evaluation, we show GeNet's effectiveness in anti-noise TOC while decoup
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.16967</link><description>&lt;p&gt;
&#29992;&#20110;&#33151;&#24335;&#23450;&#28857;&#26426;&#22120;&#20154;&#36816;&#21160;&#25805;&#20316;&#30340;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Visual Whole-Body Control for Legged Loco-Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16967
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#65292;&#20351;&#33151;&#24335;&#26426;&#22120;&#20154;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#25805;&#20316;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35757;&#32451;&#21644;Sim2Real&#36716;&#31227;&#23454;&#29616;&#20102;&#22312;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#37197;&#22791;&#25163;&#33218;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#36827;&#34892;&#31227;&#21160;&#25805;&#20316;&#30340;&#38382;&#39064;&#65292;&#21363;&#33151;&#24335;&#23450;&#28857;&#25805;&#20316;&#12290;&#23613;&#31649;&#26426;&#22120;&#20154;&#30340;&#33151;&#36890;&#24120;&#29992;&#20110;&#31227;&#21160;&#65292;&#20294;&#36890;&#36807;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#65292;&#21487;&#20197;&#25193;&#22823;&#20854;&#25805;&#20316;&#33021;&#21147;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#21516;&#26102;&#25511;&#21046;&#33151;&#37096;&#21644;&#25163;&#33218;&#65292;&#20197;&#25193;&#23637;&#20854;&#24037;&#20316;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#20351;&#29992;&#35270;&#35273;&#35266;&#27979;&#33258;&#20027;&#36827;&#34892;&#20840;&#36523;&#25511;&#21046;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;\ourFull~(\our)&#65292;&#30001;&#19968;&#20010;&#20302;&#32423;&#31574;&#30053;&#21644;&#19968;&#20010;&#39640;&#32423;&#31574;&#30053;&#32452;&#25104;&#12290;&#20302;&#32423;&#31574;&#30053;&#20351;&#29992;&#25152;&#26377;&#33258;&#30001;&#24230;&#26469;&#36319;&#36394;&#26411;&#31471;&#25191;&#34892;&#22120;&#30340;&#20301;&#32622;&#65292;&#39640;&#32423;&#31574;&#30053;&#26681;&#25454;&#35270;&#35273;&#36755;&#20837;&#25552;&#20986;&#26411;&#31471;&#25191;&#34892;&#22120;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#20013;&#35757;&#32451;&#20102;&#20004;&#20010;&#32423;&#21035;&#30340;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#20102;&#20174;Sim&#21040;&#23454;&#29289;&#30340;&#36716;&#31227;&#20197;&#36827;&#34892;&#23454;&#38469;&#26426;&#22120;&#20154;&#37096;&#32626;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#37197;&#32622;&#19979;&#65288;&#39640;&#24230;&#12289;&#65289;&#25441;&#36215;&#19981;&#21516;&#29289;&#20307;&#26041;&#38754;&#65292;&#30456;&#23545;&#22522;&#32447;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16967v1 Announce Type: cross  Abstract: We study the problem of mobile manipulation using legged robots equipped with an arm, namely legged loco-manipulation. The robot legs, while usually utilized for mobility, offer an opportunity to amplify the manipulation capabilities by conducting whole-body control. That is, the robot can control the legs and the arm at the same time to extend its workspace. We propose a framework that can conduct the whole-body control autonomously with visual observations. Our approach, namely \ourFull~(\our), is composed of a low-level policy using all degrees of freedom to track the end-effector manipulator position and a high-level policy proposing the end-effector position based on visual inputs. We train both levels of policies in simulation and perform Sim2Real transfer for real robot deployment. We perform extensive experiments and show significant improvements over baselines in picking up diverse objects in different configurations (heights,
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;PPO-AIRL + SAC&#20197;&#35299;&#20915;SAC&#31639;&#27861;&#22312;AIRL&#35757;&#32451;&#20013;&#26080;&#27861;&#20840;&#38754;&#35299;&#24320;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14593</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65306;&#20174;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#30340;&#35282;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rethinking Adversarial Inverse Reinforcement Learning: From the Angles of Policy Imitation and Transferable Reward Recovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14593
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;PPO-AIRL + SAC&#20197;&#35299;&#20915;SAC&#31639;&#27861;&#22312;AIRL&#35757;&#32451;&#20013;&#26080;&#27861;&#20840;&#38754;&#35299;&#24320;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;AIRL&#65289;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#26041;&#27861;&#12290;&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;AIRL&#30340;&#20004;&#20010;&#19981;&#21516;&#35282;&#24230;&#65306;&#31574;&#30053;&#27169;&#20223;&#21644;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#12290;&#25105;&#20204;&#20174;&#29992;Soft Actor-Critic&#65288;SAC&#65289;&#26367;&#25442;AIRL&#20013;&#30340;&#20869;&#32622;&#31639;&#27861;&#24320;&#22987;&#65292;&#20197;&#22686;&#24378;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;SAC&#30340;&#31163;&#31574;&#30053;&#24418;&#24335;&#21644;&#30456;&#23545;&#20110;AIRL&#32780;&#35328;&#21487;&#35782;&#21035;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#27169;&#22411;&#12290;&#36825;&#30830;&#23454;&#22312;&#31574;&#30053;&#27169;&#20223;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20294;&#19981;&#24910;&#32473;&#21487;&#36716;&#31227;&#22870;&#21169;&#24674;&#22797;&#24102;&#26469;&#20102;&#32570;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;SAC&#31639;&#27861;&#26412;&#36523;&#22312;AIRL&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#27861;&#20840;&#38754;&#35299;&#24320;&#22870;&#21169;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;PPO-AIRL + SAC&#65292;&#20197;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#36716;&#31227;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29615;&#22659;&#25552;&#21462;&#35299;&#24320;&#30340;&#22870;&#21169;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14593v1 Announce Type: new  Abstract: Adversarial inverse reinforcement learning (AIRL) stands as a cornerstone approach in imitation learning. This paper rethinks the two different angles of AIRL: policy imitation and transferable reward recovery. We begin with substituting the built-in algorithm in AIRL with soft actor-critic (SAC) during the policy optimization process to enhance sample efficiency, thanks to the off-policy formulation of SAC and identifiable Markov decision process (MDP) models with respect to AIRL. It indeed exhibits a significant improvement in policy imitation but accidentally brings drawbacks to transferable reward recovery. To learn this issue, we illustrate that the SAC algorithm itself is not feasible to disentangle the reward function comprehensively during the AIRL training process, and propose a hybrid framework, PPO-AIRL + SAC, for satisfactory transfer effect. Additionally, we analyze the capability of environments to extract disentangled rewa
&lt;/p&gt;</description></item><item><title>&#25209;&#35780;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#23545;&#38750;&#26126;&#26174;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Adversarial Nibbler Challenge&#20197;&#20247;&#21253;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12075</link><description>&lt;p&gt;
Adversarial Nibbler: &#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#22810;&#26679;&#21270;&#21361;&#23475;&#30340;&#24320;&#25918;&#24335;&#32418;&#38431;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12075
&lt;/p&gt;
&lt;p&gt;
&#25209;&#35780;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#23545;&#38750;&#26126;&#26174;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Adversarial Nibbler Challenge&#20197;&#20247;&#21253;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#35780;&#20272;&#27169;&#22411;&#23545;&#20110;&#19981;&#26126;&#26174;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#20197;&#20943;&#23569;&#29983;&#25104;&#20882;&#29359;&#24615;&#22270;&#20687;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;"&#38544;&#24615;&#23545;&#25239;"&#25552;&#31034;&#65288;&#35302;&#21457;T2I&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#30340;&#38750;&#26126;&#26174;&#21407;&#22240;&#65289;&#65292;&#25105;&#20204;&#29420;&#31435;&#36776;&#21035;&#20986;&#19968;&#32452;&#38590;&#20197;&#21457;&#29616;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#20154;&#31867;&#21019;&#36896;&#21147;&#24456;&#36866;&#21512;&#25581;&#31034;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Adversarial Nibbler Challenge&#65292;&#36825;&#26159;&#19968;&#20010;&#32418;&#38431;&#26041;&#27861;&#65292;&#29992;&#20110;&#20247;&#21253;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#38544;&#24615;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#25105;&#20204;&#24050;&#27719;&#24635;&#19968;&#22871;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#65292;&#37319;&#29992;&#31616;&#21333;&#29992;&#25143;&#30028;&#38754;&#26469;&#35782;&#21035;&#21644;&#27880;&#37322;&#21361;&#23475;&#65292;&#24182;&#21560;&#24341;&#24191;&#27867;&#20154;&#32676;&#26469;&#25429;&#25417;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#21487;&#33021;&#34987;&#24573;&#35270;&#30340;&#38271;&#23614;&#23433;&#20840;&#38382;&#39064;&#12290;&#25361;&#25112;&#22312;&#36830;&#32493;&#22238;&#21512;&#20013;&#36827;&#34892;&#65292;&#20197;&#23454;&#29616;&#23545;T2I&#27169;&#22411;&#20013;&#23433;&#20840;&#38544;&#24739;&#30340;&#25345;&#32493;&#21457;&#29616;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12075v1 Announce Type: cross  Abstract: With the rise of text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on ``implicitly adversarial'' prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. The challenge is run in consecutive rounds to enable a sustained discovery and analysis of safety pitfalls in T2I models.   In this pape
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>&#37325;&#35201;&#24615;&#21152;&#26435;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#31243;&#24207;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#20989;&#25968;&#25110;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#21152;&#26435;&#65292;&#21487;&#20197;&#20445;&#35777;&#30417;&#30563;&#23398;&#20064;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#32479;&#35745;&#19978;&#26399;&#26395;&#30340;&#24615;&#36136;</title><link>https://arxiv.org/abs/2403.10175</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#31616;&#35201;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Short Survey on Importance Weighting for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10175
&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#21152;&#26435;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#31243;&#24207;&#65292;&#36890;&#36807;&#23545;&#30446;&#26631;&#20989;&#25968;&#25110;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#21152;&#26435;&#65292;&#21487;&#20197;&#20445;&#35777;&#30417;&#30563;&#23398;&#20064;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#32479;&#35745;&#19978;&#26399;&#26395;&#30340;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#21152;&#26435;&#26159;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#31243;&#24207;&#65292;&#26681;&#25454;&#26576;&#31181;&#24847;&#20041;&#19978;&#23454;&#20363;&#30340;&#37325;&#35201;&#24615;&#23545;&#30446;&#26631;&#20989;&#25968;&#25110;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#21152;&#26435;&#12290;&#36825;&#19968;&#31616;&#21333;&#32780;&#26377;&#29992;&#30340;&#24605;&#24819;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20102;&#35768;&#22810;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#25454;&#30693;&#65292;&#22312;&#20851;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#24046;&#24322;&#30340;&#20551;&#35774;&#19979;&#30340;&#30417;&#30563;&#23398;&#20064;&#65292;&#36890;&#36807;&#23494;&#24230;&#27604;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#21487;&#20197;&#20445;&#35777;&#32479;&#35745;&#19978;&#26399;&#26395;&#30340;&#24615;&#36136;&#12290;&#36825;&#39033;&#35843;&#26597;&#24635;&#32467;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#30456;&#20851;&#30740;&#31350;&#20013;&#37325;&#35201;&#24615;&#21152;&#26435;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10175v1 Announce Type: cross  Abstract: Importance weighting is a fundamental procedure in statistics and machine learning that weights the objective function or probability distribution based on the importance of the instance in some sense. The simplicity and usefulness of the idea has led to many applications of importance weighting. For example, it is known that supervised learning under an assumption about the difference between the training and test distributions, called distribution shift, can guarantee statistically desirable properties through importance weighting by their density ratio. This survey summarizes the broad applications of importance weighting in machine learning and related research.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Occam Plausibility Algorithm for surrogate models (OPAL-surrogate)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#37319;&#29992;&#23618;&#27425;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#27169;&#22411;&#39564;&#35777;&#27979;&#35797;&#26469;&#35780;&#20272;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#39044;&#27979;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08901</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#21457;&#29616;&#21487;&#20449;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#30340;&#25112;&#30053;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08901
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Occam Plausibility Algorithm for surrogate models (OPAL-surrogate)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#37319;&#29992;&#23618;&#27425;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#27169;&#22411;&#39564;&#35777;&#27979;&#35797;&#26469;&#35780;&#20272;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#39044;&#27979;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24320;&#21457;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#39640;&#20445;&#30495;&#20223;&#30495;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#24191;&#27867;&#25972;&#21512;&#65292;&#20984;&#26174;&#20102;&#31283;&#20581;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#20445;&#20195;&#29702;&#27169;&#22411;&#21487;&#21487;&#38752;&#22320;&#29992;&#20110;&#37325;&#35201;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;Occam Plausibility Algorithm&#65288;OPAL-surrogate&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#20197;&#22312;&#22823;&#37327;&#28508;&#22312;&#27169;&#22411;&#65288;&#21253;&#25324;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#20197;&#21450;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#36873;&#25321;&#65289;&#20013;&#25581;&#31034;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#26694;&#26550;&#22522;&#20110;&#23618;&#27425;&#36125;&#21494;&#26031;&#25512;&#29702;&#65292;&#24182;&#37319;&#29992;&#27169;&#22411;&#39564;&#35777;&#27979;&#35797;&#26469;&#35780;&#20272;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#39044;&#27979;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21407;&#21017;&#65292;OPAL-surrogate&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#24615;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08901v1 Announce Type: cross  Abstract: The widespread integration of deep neural networks in developing data-driven surrogate models for high-fidelity simulations of complex physical systems highlights the critical necessity for robust uncertainty quantification techniques and credibility assessment methodologies, ensuring the reliable deployment of surrogate models in consequential decision-making. This study presents the Occam Plausibility Algorithm for surrogate models (OPAL-surrogate), providing a systematic framework to uncover predictive neural network-based surrogate models within the large space of potential models, including various neural network classes and choices of architecture and hyperparameters. The framework is grounded in hierarchical Bayesian inferences and employs model validation tests to evaluate the credibility and prediction reliability of the surrogate models under uncertainty. Leveraging these principles, OPAL-surrogate introduces a systematic and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;</title><link>https://arxiv.org/abs/2403.00858</link><description>&lt;p&gt;
&#30452;&#25509;&#19982;Chat-Fine-Tuned LLMs&#30340;&#33609;&#26696;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00858
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#29983;&#25104;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33258;&#22238;&#24402;&#26412;&#36136;&#12289;&#24040;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#26377;&#38480;&#30340;&#20869;&#23384;&#24102;&#23485;&#32780;&#34987;&#35748;&#20026;&#26159;&#20869;&#23384;&#23494;&#38598;&#22411;&#65292;&#36890;&#24120;&#23548;&#33268;&#20302;&#20196;&#29260;&#36895;&#29575;&#12290;&#29468;&#27979;&#35299;&#30721;&#24050;&#34987;&#25552;&#20986;&#20316;&#20026;LLM&#25512;&#29702;&#21152;&#36895;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#24320;&#28304;LLM&#31995;&#21015;&#20013;&#65292;&#20363;&#22914;Llama 2 7B&#65292;&#30001;&#20110;&#33609;&#26696;&#27169;&#22411;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#22240;&#27492;&#38656;&#35201;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#33609;&#26696;&#27169;&#22411;&#20197;&#36890;&#36807;&#29468;&#27979;&#35299;&#30721;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#33609;&#26696;&#27169;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#30452;&#25509;&#19982;Chat-capable&#30446;&#26631;&#27169;&#22411;&#23545;&#40784;&#12290;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;Llama 2 Chat Drafter 115M&#65292;&#36825;&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;Llama 2 Chat 7B&#25110;&#26356;&#22823;&#27169;&#22411;&#30340;&#33609;&#26696;&#27169;&#22411;&#65292;&#20165;&#21344;&#21407;&#22987;&#22823;&#23567;&#30340;1.64&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26694;&#26550;&#20165;&#21253;&#25324;&#39044;&#35757;&#32451;&#12289;&#33976;&#39311;&#25968;&#25454;&#38598;&#29983;&#25104;&#21644;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#24494;&#35843;&#65292;&#27809;&#26377;&#39069;&#22806;&#30340;&#23545;&#40784;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
&lt;/p&gt;</description></item><item><title>PiShield&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23558;&#38656;&#27714;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#26694;&#26550;&#65292;&#26080;&#35770;&#36755;&#20837;&#22914;&#20309;&#37117;&#33021;&#30830;&#20445;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#24182;&#21487;&#26681;&#25454;&#20174;&#19994;&#32773;&#38656;&#27714;&#22312;&#25512;&#26029;&#21644;/&#25110;&#35757;&#32451;&#26102;&#38598;&#25104;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.18285</link><description>&lt;p&gt;
PiShield&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#20197;&#38656;&#27714;&#20026;&#22522;&#30784;&#23398;&#20064;&#30340;NeSy&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PiShield: A NeSy Framework for Learning with Requirements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18285
&lt;/p&gt;
&lt;p&gt;
PiShield&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23558;&#38656;&#27714;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#26694;&#26550;&#65292;&#26080;&#35770;&#36755;&#20837;&#22914;&#20309;&#37117;&#33021;&#30830;&#20445;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#24182;&#21487;&#26681;&#25454;&#20174;&#19994;&#32773;&#38656;&#27714;&#22312;&#25512;&#26029;&#21644;/&#25110;&#35757;&#32451;&#26102;&#38598;&#25104;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#20854;&#20248;&#21183;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#38590;&#20197;&#28385;&#36275;&#20854;&#36755;&#20986;&#30340;&#23433;&#20840;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PiShield&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20801;&#35768;&#23558;&#38656;&#27714;&#38598;&#25104;&#21040;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#26694;&#26550;&#12290;PiShield&#30830;&#20445;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#26080;&#35770;&#36755;&#20837;&#22914;&#20309;&#12290;&#27492;&#22806;&#65292;&#23427;&#20801;&#35768;&#26681;&#25454;&#20174;&#19994;&#32773;&#30340;&#38656;&#27714;&#22312;&#25512;&#26029;&#21644;/&#25110;&#35757;&#32451;&#26102;&#38598;&#25104;&#38656;&#27714;&#12290;&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#36843;&#20999;&#38656;&#35201;&#20801;&#35768;&#22312;&#21508;&#20010;&#39046;&#22495;&#38598;&#25104;&#38656;&#27714;&#30340;&#26694;&#26550;&#12290;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#20010;&#24212;&#29992;&#22330;&#26223;&#65306;&#21151;&#33021;&#22522;&#22240;&#32452;&#23398;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18285v1 Announce Type: cross  Abstract: Deep learning models have shown their strengths in various application domains, however, they often struggle to meet safety requirements for their outputs. In this paper, we introduce PiShield, the first framework ever allowing for the integration of the requirements into the neural networks' topology. PiShield guarantees compliance with these requirements, regardless of input. Additionally, it allows for integrating requirements both at inference and/or training time, depending on the practitioners' needs. Given the widespread application of deep learning, there is a growing need for frameworks allowing for the integration of the requirements across various domains. Here, we explore three application scenarios: functional genomics, autonomous driving, and tabular data generation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#21644;&#24102;&#23485;&#20998;&#37197;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#36866;&#24212;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#24322;&#26500;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10097</link><description>&lt;p&gt;
&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#20013;&#20855;&#26377;&#29420;&#31435;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Federated Learning in Heterogeneous Wireless Networks with Independent Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10097
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#21644;&#24102;&#23485;&#20998;&#37197;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#36866;&#24212;&#25968;&#25454;&#21644;&#31995;&#32479;&#30340;&#24322;&#26500;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#23458;&#25143;&#31471;&#36827;&#34892;&#38543;&#26426;&#23376;&#38598;&#37319;&#26679;&#26469;&#35299;&#20915;&#36831;&#21040;&#32773;&#38382;&#39064;&#24182;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#32852;&#21512;&#31995;&#32479;&#21644;&#25968;&#25454;&#24322;&#26500;&#35774;&#35745;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21487;&#33021;&#19982;&#23454;&#38469;&#30340;&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#19981;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20513;&#19968;&#31181;&#26032;&#30340;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#23454;&#38469;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#32771;&#34385;&#36890;&#20449;&#21644;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#24102;&#26377;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#30340;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#26032;&#25910;&#25947;&#30028;&#38480;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24102;&#23485;&#20998;&#37197;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25910;&#25947;&#36718;&#25968;&#19978;&#30028;&#21644;&#27599;&#36718;&#39044;&#26399;&#35757;&#32451;&#26102;&#38388;&#30340;&#39640;&#25928;&#29420;&#31435;&#23458;&#25143;&#31471;&#37319;&#26679;&#31639;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#23454;&#38469;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10097v1 Announce Type: new  Abstract: Federated Learning (FL) algorithms commonly sample a random subset of clients to address the straggler issue and improve communication efficiency. While recent works have proposed various client sampling methods, they have limitations in joint system and data heterogeneity design, which may not align with practical heterogeneous wireless networks. In this work, we advocate a new independent client sampling strategy to minimize the wall-clock training time of FL, while considering data heterogeneity and system heterogeneity in both communication and computation. We first derive a new convergence bound for non-convex loss functions with independent client sampling and then propose an adaptive bandwidth allocation scheme. Furthermore, we propose an efficient independent client sampling algorithm based on the upper bounds on the convergence rounds and the expected per-round training time, to minimize the wall-clock time of FL, while consider
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;"&#20934;&#21017;&#23849;&#28291;"&#30340;&#27010;&#24565;&#65292;&#21363;&#20248;&#21270;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#24847;&#21619;&#30528;&#21478;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#30340;&#26368;&#20248;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#23545;&#20110;&#25439;&#22833;&#30340;&#20271;&#21162;&#21033;&#20998;&#24067;&#65292;CVaR&#21644;DRO&#30340;&#32467;&#26524;&#36828;&#36229;&#20986;&#29616;&#26377;&#30740;&#31350;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#21333;&#35843;&#20934;&#21017;&#22914;&#20542;&#26012;ERM&#26080;&#27861;&#36991;&#20813;&#23849;&#28291;&#65292;&#32780;&#38750;&#21333;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#12290;</title><link>https://arxiv.org/abs/2402.09802</link><description>&lt;p&gt;
&#20934;&#21017;&#23849;&#28291;&#21644;&#25439;&#22833;&#20998;&#24067;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Criterion collapse and loss distribution control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;"&#20934;&#21017;&#23849;&#28291;"&#30340;&#27010;&#24565;&#65292;&#21363;&#20248;&#21270;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#24847;&#21619;&#30528;&#21478;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#30340;&#26368;&#20248;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#23545;&#20110;&#25439;&#22833;&#30340;&#20271;&#21162;&#21033;&#20998;&#24067;&#65292;CVaR&#21644;DRO&#30340;&#32467;&#26524;&#36828;&#36229;&#20986;&#29616;&#26377;&#30740;&#31350;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#19968;&#20123;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#21333;&#35843;&#20934;&#21017;&#22914;&#20542;&#26012;ERM&#26080;&#27861;&#36991;&#20813;&#23849;&#28291;&#65292;&#32780;&#38750;&#21333;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;"&#20934;&#21017;&#23849;&#28291;"&#30340;&#27010;&#24565;&#65292;&#21363;&#20248;&#21270;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#24847;&#21619;&#30528;&#21478;&#19968;&#20010;&#24230;&#37327;&#25351;&#26631;&#30340;&#26368;&#20248;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#21508;&#31181;&#23398;&#20064;&#20934;&#21017;&#19979;&#23849;&#28291;&#25104;&#35823;&#24046;&#27010;&#29575;&#26368;&#23567;&#21270;&#22120;&#30340;&#26465;&#20214;&#65292;&#20174;DRO&#21644;OCE&#39118;&#38505;&#65288;CVaR&#12289;&#20542;&#26012;ERM&#65289;&#21040;&#25991;&#29486;&#20013;&#25506;&#32034;&#30340;&#26368;&#26032;&#19978;&#21319;-&#19979;&#38477;&#31639;&#27861;&#30340;&#38750;&#21333;&#35843;&#20934;&#21017;&#65288;&#27946;&#27700;&#12289;SoftAD&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20271;&#21162;&#21033;&#20998;&#24067;&#25439;&#22833;&#30340;&#32972;&#26223;&#19979;&#65292;CVaR&#21644;DRO&#30340;&#29616;&#26377;&#32467;&#26524;&#36828;&#36828;&#36229;&#36234;&#20102;&#23849;&#28291;&#30340;&#33539;&#22260;&#65292;&#28982;&#21518;&#25193;&#22823;&#20102;&#25105;&#20204;&#30340;&#33539;&#22260;&#65292;&#21253;&#25324;&#20195;&#29702;&#25439;&#22833;&#65292;&#23637;&#31034;&#20102;&#20687;&#20542;&#26012;ERM&#36825;&#26679;&#30340;&#21333;&#35843;&#20934;&#21017;&#26080;&#27861;&#36991;&#20813;&#23849;&#28291;&#30340;&#26465;&#20214;&#65292;&#32780;&#38750;&#21333;&#35843;&#30340;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09802v1 Announce Type: cross  Abstract: In this work, we consider the notion of "criterion collapse," in which optimization of one metric implies optimality in another, with a particular focus on conditions for collapse into error probability minimizers under a wide variety of learning criteria, ranging from DRO and OCE risks (CVaR, tilted ERM) to non-monotonic criteria underlying recent ascent-descent algorithms explored in the literature (Flooding, SoftAD). We show how collapse in the context of losses with a Bernoulli distribution goes far beyond existing results for CVaR and DRO, then expand our scope to include surrogate losses, showing conditions where monotonic criteria such as tilted ERM cannot avoid collapse, whereas non-monotonic alternatives can.
&lt;/p&gt;</description></item><item><title>Atlassian&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;CI&#26500;&#24314;&#22833;&#36133;&#23545;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#21644;&#22242;&#38431;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#23558;CI&#26500;&#24314;&#39044;&#27979;&#24037;&#20855;&#38598;&#25104;&#21040;Bitbucket&#29615;&#22659;&#20013;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#21644;&#26399;&#26395;&#12290;</title><link>https://arxiv.org/abs/2402.09651</link><description>&lt;p&gt;
Atlassian&#30340;CI&#26500;&#24314;&#22833;&#36133;&#39044;&#27979;&#30340;&#20174;&#19994;&#32773;&#25361;&#25112;&#21644;&#24863;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Practitioners' Challenges and Perceptions of CI Build Failure Predictions at Atlassian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09651
&lt;/p&gt;
&lt;p&gt;
Atlassian&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;CI&#26500;&#24314;&#22833;&#36133;&#23545;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#21644;&#22242;&#38431;&#30340;&#24433;&#21709;&#65292;&#24182;&#30740;&#31350;&#20102;&#23558;CI&#26500;&#24314;&#39044;&#27979;&#24037;&#20855;&#38598;&#25104;&#21040;Bitbucket&#29615;&#22659;&#20013;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#21644;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#38598;&#25104;&#65288;CI&#65289;&#26500;&#24314;&#22833;&#36133;&#21487;&#33021;&#20250;&#23545;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#21644;&#22242;&#38431;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#65292;&#22914;&#24310;&#36831;&#21457;&#24067;&#26032;&#21151;&#33021;&#21644;&#38477;&#20302;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#12290;&#26412;&#30740;&#31350;&#25253;&#21578;&#20102;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#35843;&#26597;&#20102;Atlassian&#22312;&#20135;&#21697;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;CI&#26500;&#24314;&#22833;&#36133;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#20998;&#26512;&#21457;&#29616;&#65292;&#20195;&#30721;&#24211;&#32500;&#24230;&#26159;&#24433;&#21709;CI&#26500;&#24314;&#22833;&#36133;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23450;&#24615;&#35843;&#26597;&#21457;&#29616;&#65292;Atlassian&#24320;&#21457;&#20154;&#21592;&#35748;&#20026;CI&#26500;&#24314;&#22833;&#36133;&#26159;&#23454;&#36341;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;CI&#26500;&#24314;&#39044;&#27979;&#19981;&#20165;&#21487;&#20197;&#25552;&#20379;&#23545;CI&#26500;&#24314;&#22833;&#36133;&#30340;&#31215;&#26497;&#35265;&#35299;&#65292;&#36824;&#21487;&#20197;&#20419;&#36827;&#22242;&#38431;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#23558;CI&#26500;&#24314;&#39044;&#27979;&#24037;&#20855;&#38598;&#25104;&#21040;Bitbucket&#29615;&#22659;&#20013;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#21644;&#26399;&#26395;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;CI&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09651v1 Announce Type: cross  Abstract: Continuous Integration (CI) build failures could significantly impact the software development process and teams, such as delaying the release of new features and reducing developers' productivity. In this work, we report on an empirical study that investigates CI build failures throughout product development at Atlassian. Our quantitative analysis found that the repository dimension is the key factor influencing CI build failures. In addition, our qualitative survey revealed that Atlassian developers perceive CI build failures as challenging issues in practice. Furthermore, we found that the CI build prediction can not only provide proactive insight into CI build failures but also facilitate the team's decision-making. Our study sheds light on the challenges and expectations involved in integrating CI build prediction tools into the Bitbucket environment, providing valuable insights for enhancing CI processes.
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#35770;&#25991;&#65292;&#20316;&#32773;&#20998;&#26512;&#20102;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#21487;&#20197;&#22312;&#38754;&#23545;&#19981;&#21516;&#31867;&#22411;&#23545;&#25163;&#26102;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.08621</link><description>&lt;p&gt;
&#19968;&#31181;&#24191;&#20041;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Generalized Approach to Online Convex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08621
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31687;&#20851;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#35770;&#25991;&#65292;&#20316;&#32773;&#20998;&#26512;&#20102;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36716;&#21270;&#20026;&#30456;&#24212;&#30340;&#32447;&#24615;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#21487;&#20197;&#22312;&#38754;&#23545;&#19981;&#21516;&#31867;&#22411;&#23545;&#25163;&#26102;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#20309;&#29992;&#20110;&#20855;&#26377;&#23436;&#20840;&#33258;&#36866;&#24212;&#23545;&#25163;&#30340;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#30340;&#31639;&#27861;&#37117;&#26159;&#29992;&#20110;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20219;&#20309;&#38656;&#35201;&#20840;&#20449;&#24687;&#21453;&#39304;&#30340;&#31639;&#27861;&#37117;&#21487;&#20197;&#36716;&#21270;&#20026;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#36951;&#25022;&#30028;&#38480;&#30340;&#21322;&#21305;&#37197;&#21453;&#39304;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20351;&#29992;&#30830;&#23450;&#24615;&#21322;&#21305;&#37197;&#21453;&#39304;&#30340;&#20840;&#33258;&#36866;&#24212;&#23545;&#25163;&#35774;&#35745;&#30340;&#31639;&#27861;&#22312;&#38754;&#23545;&#26080;&#30693;&#23545;&#25163;&#26102;&#21487;&#20197;&#20351;&#29992;&#21482;&#26377;&#38543;&#26426;&#21322;&#21305;&#37197;&#21453;&#39304;&#30340;&#31639;&#27861;&#33719;&#24471;&#30456;&#20284;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#25551;&#36848;&#20102;&#23558;&#19968;&#38454;&#31639;&#27861;&#36716;&#21270;&#20026;&#38646;&#38454;&#31639;&#27861;&#30340;&#36890;&#29992;&#20803;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#22312;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#20840;&#20449;&#24687;&#21453;&#39304;&#12289;&#21322;&#21305;&#37197;&#21453;&#39304;&#12289;&#38543;&#26426;&#36951;&#25022;&#12289;&#23545;&#25239;&#36951;&#25022;&#21644;&#21508;&#31181;&#24418;&#24335;&#30340;&#38750;&#24179;&#31283;&#36951;&#25022;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;
&lt;/p&gt;
&lt;p&gt;
In this paper, we analyze the problem of online convex optimization in different settings. We show that any algorithm for online linear optimization with fully adaptive adversaries is an algorithm for online convex optimization. We also show that any such algorithm that requires full-information feedback may be transformed to an algorithm with semi-bandit feedback with comparable regret bound. We further show that algorithms that are designed for fully adaptive adversaries using deterministic semi-bandit feedback can obtain similar bounds using only stochastic semi-bandit feedback when facing oblivious adversaries. We use this to describe general meta-algorithms to convert first order algorithms to zeroth order algorithms with comparable regret bounds. Our framework allows us to analyze online optimization in various settings, such full-information feedback, bandit feedback, stochastic regret, adversarial regret and various forms of non-stationary regret. Using our analysis, we provide
&lt;/p&gt;</description></item><item><title>MOMENT&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03885</link><description>&lt;p&gt;
MOMENT&#65306;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;
&lt;/p&gt;
&lt;p&gt;
MOMENT: A Family of Open Time-series Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03885
&lt;/p&gt;
&lt;p&gt;
MOMENT&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#65292;&#36890;&#36807;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MOMENT&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22522;&#30784;&#27169;&#22411;&#23478;&#26063;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#22823;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#32570;&#20047;&#19968;&#20010;&#22823;&#32780;&#26377;&#20957;&#32858;&#21147;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#23384;&#20648;&#24211;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22810;&#26679;&#30340;&#26102;&#38388;&#24207;&#21015;&#29305;&#24449;&#20351;&#24471;&#22810;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#39564;&#35780;&#20272;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#36164;&#28304;&#12289;&#26102;&#38388;&#21644;&#30417;&#30563;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#12290;&#65288;3&#65289;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;&#20844;&#20849;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;&#26102;&#38388;&#24207;&#21015;&#22534;&#65292;&#20197;&#31995;&#32479;&#22320;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#20197;&#35299;&#38145;&#22823;&#35268;&#27169;&#30340;&#22810;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20511;&#37492;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#26377;&#38480;&#30417;&#30563;&#22330;&#26223;&#19979;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MOMENT, a family of open-source foundation models for general-purpose time-series analysis. Pre-training large models on time-series data is challenging due to (1) the absence of a large and cohesive public time-series repository, and (2) diverse time-series characteristics which make multi-dataset training onerous. Additionally, (3) experimental benchmarks to evaluate these models, especially in scenarios with limited resources, time, and supervision, are still in their nascent stages. To address these challenges, we compile a large and diverse collection of public time-series, called the Time-series Pile, and systematically tackle time-series-specific challenges to unlock large-scale multi-dataset pre-training. Finally, we build on recent work to design a benchmark to evaluate time-series foundation models on diverse tasks and datasets in limited supervision settings. Experiments on this benchmark demonstrate the effectiveness of our pre-trained models with minimal data 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00564</link><description>&lt;p&gt;
&#19968;&#27425;&#22270;&#21367;&#31215;&#23601;&#22815;&#20102;&#65306;&#39640;&#25928;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#23436;&#25104;&#20219;&#21153;&#65292;&#32780;CNN&#30456;&#27604;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#26356;&#21152;&#24222;&#22823;&#65292;&#36825;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#24102;&#26469;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#36866;&#29992;&#20110;RGB&#21644;&#28784;&#24230;&#25968;&#25454;&#38598;&#65292;&#20294;&#20165;&#20165;&#20351;&#29992;&#28784;&#24230;&#22270;&#20687;&#30340;&#20998;&#31867;&#22120;&#30456;&#23545;&#36739;&#23569;&#35265;&#12290;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;(ATR)&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#20687;&#30340;&#30690;&#37327;&#21270;&#35270;&#22270;&#30340;&#26032;&#22411;&#28784;&#24230;(&#21333;&#36890;&#36947;)&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#23558;&#38382;&#39064;&#35774;&#32622;&#20026;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;MLP&#30340;&#36731;&#37327;&#32423;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25209;&#27425;&#32423;&#21035;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24182;&#20943;&#23567;&#24615;&#33021;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23450;&#21046;&#30340;&#20934;&#30830;&#29575;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized acc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36880;&#27493;&#25193;&#23637;&#21333;&#20010;&#33410;&#28857;&#21040;&#30446;&#26631;&#22270;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#23545;&#25152;&#26377;&#33410;&#28857;&#23545;&#30340;&#25972;&#20010;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22270;&#29983;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#22810;&#23610;&#24230;&#29983;&#25104;&#20445;&#25345;&#20102;&#39640;&#34920;&#36798;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.11529</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#26412;&#22320;&#25193;&#23637;&#23454;&#29616;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient and Scalable Graph Generation through Iterative Local Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11529
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36880;&#27493;&#25193;&#23637;&#21333;&#20010;&#33410;&#28857;&#21040;&#30446;&#26631;&#22270;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#23545;&#25152;&#26377;&#33410;&#28857;&#23545;&#30340;&#25972;&#20010;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#22270;&#29983;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#22810;&#23610;&#24230;&#29983;&#25104;&#20445;&#25345;&#20102;&#39640;&#34920;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#30340;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20195;&#34920;&#25152;&#26377;&#33410;&#28857;&#23545;&#30340;&#25972;&#20010;&#32852;&#21512;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#21516;&#26102;&#25429;&#25417;&#20840;&#23616;&#21644;&#23616;&#37096;&#22270;&#32467;&#26500;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#22411;&#22270;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#23558;&#21333;&#20010;&#33410;&#28857;&#25193;&#23637;&#21040;&#30446;&#26631;&#22270;&#26469;&#29983;&#25104;&#22270;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#20197;&#26412;&#22320;&#21270;&#26041;&#24335;&#28155;&#21152;&#33410;&#28857;&#21644;&#36793;&#65292;&#39318;&#20808;&#26500;&#24314;&#20840;&#23616;&#32467;&#26500;&#65292;&#28982;&#21518;&#32454;&#21270;&#23616;&#37096;&#32454;&#33410;&#12290;&#23616;&#37096;&#29983;&#25104;&#36991;&#20813;&#20102;&#23545;&#25152;&#26377;&#33410;&#28857;&#23545;&#19978;&#30340;&#25972;&#20010;&#32852;&#21512;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#23545;&#20110;&#33410;&#28857;&#25968;&#32780;&#35328;&#23454;&#29616;&#20102;&#22823;&#24133;&#30340;&#35745;&#31639;&#33410;&#32422;&#65292;&#24182;&#36890;&#36807;&#22810;&#23610;&#24230;&#29983;&#25104;&#20445;&#25345;&#20102;&#39640;&#34920;&#36798;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20844;&#35748;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11529v2 Announce Type: replace-cross  Abstract: In the realm of generative models for graphs, extensive research has been conducted. However, most existing methods struggle with large graphs due to the complexity of representing the entire joint distribution across all node pairs and capturing both global and local graph structures simultaneously. To overcome these issues, we introduce a method that generates a graph by progressively expanding a single node to a target graph. In each step, nodes and edges are added in a localized manner through denoising diffusion, building first the global structure, and then refining the local details. The local generation avoids modeling the entire joint distribution over all node pairs, achieving substantial computational savings with subquadratic runtime relative to node count while maintaining high expressivity through multiscale generation. Our experiments show that our model achieves state-of-the-art performance on well-established b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#22522;&#20110;Conformer&#30340;&#31471;&#21040;&#31471;&#27969;&#24335;ASR&#31995;&#32479;&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#26550;&#26500;&#36866;&#37197;&#12289;&#31070;&#32463;&#32593;&#32476;&#22270;&#21464;&#25442;&#21644;&#25968;&#20540;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;5.26&#20493;&#23454;&#26102;&#36895;&#24230;&#30340;&#35821;&#38899;&#35782;&#21035;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#20943;&#23569;&#33021;&#32791;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.10359</link><description>&lt;p&gt;
&#22522;&#20110;Conformer&#30340;&#26497;&#31471;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#19978;&#30340;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Conformer-Based Speech Recognition On Extreme Edge-Computing Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#23454;&#29616;&#22522;&#20110;Conformer&#30340;&#31471;&#21040;&#31471;&#27969;&#24335;ASR&#31995;&#32479;&#30340;&#19968;&#31995;&#21015;&#27169;&#22411;&#26550;&#26500;&#36866;&#37197;&#12289;&#31070;&#32463;&#32593;&#32476;&#22270;&#21464;&#25442;&#21644;&#25968;&#20540;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;5.26&#20493;&#23454;&#26102;&#36895;&#24230;&#30340;&#35821;&#38899;&#35782;&#21035;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#20943;&#23569;&#33021;&#32791;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20170;&#22825;&#35774;&#22791;&#20013;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#36164;&#28304;&#65292;&#20256;&#32479;&#19978;&#22312;&#20113;&#31471;&#25191;&#34892;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#27491;&#20174;&#20113;&#31471;&#36716;&#31227;&#21040;&#35774;&#22791;&#19978;&#20197;&#26356;&#22909;&#22320;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;&#26412;&#22320;ASR&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20363;&#22914;&#26234;&#33021;&#25163;&#26426;&#12289;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#20854;&#20182;&#23567;&#22411;&#23478;&#23621;&#33258;&#21160;&#21270;&#35774;&#22791;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#27169;&#22411;&#26550;&#26500;&#35843;&#25972;&#12289;&#31070;&#32463;&#32593;&#32476;&#22270;&#21464;&#25442;&#21644;&#25968;&#20540;&#20248;&#21270;&#65292;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#36866;&#37197;&#20808;&#36827;&#30340;&#22522;&#20110;Conformer&#30340;&#31471;&#21040;&#31471;&#27969;&#24335;ASR&#31995;&#32479;&#65292;&#32780;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#23567;&#22411;&#21487;&#31359;&#25140;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#36229;&#36807;&#23454;&#26102;5.26&#20493;&#24555;&#65288;0.19 RTF&#65289;&#30340;&#35821;&#38899;&#35782;&#21035;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#33021;&#32791;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24191;&#27867;&#36866;&#29992;&#20110;&#20854;&#20182;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#26080;&#26381;&#21153;&#22120;AI&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10359v2 Announce Type: replace  Abstract: With increasingly more powerful compute capabilities and resources in today's devices, traditionally compute-intensive automatic speech recognition (ASR) has been moving from the cloud to devices to better protect user privacy. However, it is still challenging to implement on-device ASR on resource-constrained devices, such as smartphones, smart wearables, and other small home automation devices. In this paper, we propose a series of model architecture adaptions, neural network graph transformations, and numerical optimizations to fit an advanced Conformer based end-to-end streaming ASR system on resource-constrained devices without accuracy degradation. We achieve over 5.26 times faster than realtime (0.19 RTF) speech recognition on small wearables while minimizing energy consumption and achieving state-of-the-art accuracy. The proposed methods are widely applicable to other transformer-based server-free AI applications. In addition
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20559;&#32622;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#20108;&#38454;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#36712;&#36857;&#37319;&#26679;&#30340;&#26222;&#36890;&#26799;&#24230;&#20272;&#35745;&#22120;&#21644;&#22522;&#20110;&#21452;&#24490;&#29615;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12290;&#23454;&#29616;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20559;&#32622;&#20027;&#35201;&#26469;&#33258;&#20110;&#26377;&#38480;&#26102;&#38388;&#37319;&#26679;&#21644;&#23545;&#20215;&#20540;&#20989;&#25968;&#30340;&#36924;&#36817;&#12290;</title><link>https://arxiv.org/abs/2311.02546</link><description>&lt;p&gt;
&#20851;&#20110;&#20559;&#32622;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#20108;&#38454;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Second-Order Convergence of Biased Policy Gradient Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02546
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20559;&#32622;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#20108;&#38454;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#36712;&#36857;&#37319;&#26679;&#30340;&#26222;&#36890;&#26799;&#24230;&#20272;&#35745;&#22120;&#21644;&#22522;&#20110;&#21452;&#24490;&#29615;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#12290;&#23454;&#29616;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20559;&#32622;&#20027;&#35201;&#26469;&#33258;&#20110;&#26377;&#38480;&#26102;&#38388;&#37319;&#26679;&#21644;&#23545;&#20215;&#20540;&#20989;&#25968;&#30340;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#36890;&#24120;&#26159;&#39640;&#24230;&#38750;&#20984;&#30340;&#65292;&#22240;&#27492;&#24076;&#26395;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#33021;&#22815;&#33073;&#31163;&#38797;&#28857;&#24182;&#36798;&#21040;&#20108;&#38454;&#31283;&#23450;&#28857;&#12290;&#29616;&#26377;&#30340;&#32467;&#26524;&#21482;&#32771;&#34385;&#20102;&#24102;&#26377;&#26080;&#20559;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#26222;&#36890;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20294;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;&#22238;&#25253;&#35774;&#32622;&#19979;&#65292;&#23454;&#38469;&#23454;&#29616;&#26159;&#26377;&#20559;&#30340;&#65292;&#22240;&#20026;&#26377;&#38480;&#26102;&#38388;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#35780;&#35770;&#23478;&#23545;&#20215;&#20540;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#35780;&#35770;&#23478;-&#28436;&#21592;&#26041;&#27861;&#30340;&#20108;&#38454;&#25910;&#25947;&#24615;&#20063;&#26410;&#34987;&#35777;&#23454;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26377;&#20559;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#26032;&#39062;&#30340;&#20108;&#38454;&#20998;&#26512;&#65292;&#21253;&#25324;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36712;&#36857;&#37319;&#26679;&#35745;&#31639;&#24471;&#21040;&#30340;&#26222;&#36890;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#20197;&#21450;&#21452;&#24490;&#29615;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#22312;&#20869;&#24490;&#29615;&#20013;&#65292;&#35780;&#35770;&#23478;&#36890;&#36807;TD(0)&#23398;&#20064;&#25913;&#36827;&#20102;&#23545;&#20215;&#20540;&#20989;&#25968;&#30340;&#36924;&#36817;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;TD(0)&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the objective functions of reinforcement learning problems are typically highly nonconvex, it is desirable that policy gradient, the most popular algorithm, escapes saddle points and arrives at second-order stationary points. Existing results only consider vanilla policy gradient algorithms with unbiased gradient estimators, but practical implementations under the infinite-horizon discounted reward setting are biased due to finite-horizon sampling. Moreover, actor-critic methods, whose second-order convergence has not yet been established, are also biased due to the critic approximation of the value function. We provide a novel second-order analysis of biased policy gradient methods, including the vanilla gradient estimator computed from Monte-Carlo sampling of trajectories as well as the double-loop actor-critic algorithm, where in the inner loop the critic improves the approximation of the value function via TD(0) learning. Separately, we also establish the convergence of TD(0)
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#26500;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#20132;&#21449;&#23376;&#32676;&#20307;&#38388;&#30340;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#31995;&#32479;&#24615;&#33021;&#20272;&#35745;&#65292;&#21363;&#20351;&#23545;&#20110;&#24456;&#23567;&#30340;&#23376;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.14893</link><description>&lt;p&gt;
&#35780;&#20272;&#27169;&#22411;&#22312;&#20132;&#21449;&#23376;&#32676;&#20307;&#38388;&#24615;&#33021;&#30340;&#32467;&#26500;&#22238;&#24402;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A structured regression approach for evaluating model performance across intersectional subgroups. (arXiv:2401.14893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#26500;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#20132;&#21449;&#23376;&#32676;&#20307;&#38388;&#30340;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#25552;&#20379;&#21487;&#38752;&#30340;&#31995;&#32479;&#24615;&#33021;&#20272;&#35745;&#65292;&#21363;&#20351;&#23545;&#20110;&#24456;&#23567;&#30340;&#23376;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20844;&#24179;&#24615;&#35780;&#20272;&#20013;&#65292;&#20998;&#35299;&#24335;&#35780;&#20272;&#26159;&#19968;&#39033;&#26680;&#24515;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#30001;&#20154;&#21475;&#32479;&#35745;&#23398;&#25110;&#20854;&#20182;&#25935;&#24863;&#23646;&#24615;&#32452;&#21512;&#23450;&#20041;&#30340;&#19981;&#21516;&#23376;&#32676;&#20307;&#20013;&#30340;&#24615;&#33021;&#12290;&#26631;&#20934;&#26041;&#27861;&#26159;&#23558;&#35780;&#20272;&#25968;&#25454;&#20998;&#23618;&#21040;&#23376;&#32676;&#20307;&#20013;&#65292;&#24182;&#20998;&#21035;&#35745;&#31639;&#27599;&#20010;&#32452;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#22312;&#32771;&#34385;&#21040;&#20132;&#21449;&#23376;&#32676;&#20307;&#26102;&#26679;&#26412;&#25968;&#37327;&#20063;&#20250;&#36805;&#36895;&#21464;&#23567;&#65292;&#36825;&#22823;&#22823;&#38480;&#21046;&#20102;&#35768;&#22810;&#20998;&#35299;&#35780;&#20272;&#20013;&#23545;&#20132;&#21449;&#32676;&#20307;&#30340;&#32771;&#34385;&#31243;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#26500;&#22238;&#24402;&#26041;&#27861;&#26469;&#36827;&#34892;&#20998;&#35299;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#23567;&#30340;&#23376;&#32676;&#20307;&#65292;&#35813;&#26041;&#27861;&#20063;&#33021;&#20135;&#29983;&#21487;&#38752;&#30340;&#31995;&#32479;&#24615;&#33021;&#20272;&#35745;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#25512;&#26029;&#31574;&#30053;&#26469;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#25506;&#32034;&#20102;&#25311;&#21512;&#20248;&#24230;&#27979;&#35797;&#22914;&#20309;&#25581;&#31034;&#20132;&#21449;&#23376;&#32676;&#20307;&#25152;&#32463;&#21382;&#30340;&#19982;&#20844;&#24179;&#30456;&#20851;&#30340;&#20260;&#23475;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disaggregated evaluation is a central task in AI fairness assessment, with the goal to measure an AI system's performance across different subgroups defined by combinations of demographic or other sensitive attributes. The standard approach is to stratify the evaluation data across subgroups and compute performance metrics separately for each group. However, even for moderately-sized evaluation datasets, sample sizes quickly get small once considering intersectional subgroups, which greatly limits the extent to which intersectional groups are considered in many disaggregated evaluations. In this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate can yield reliable system performance estimates even for very small subgroups. We also provide corresponding inference strategies for constructing confidence intervals and explore how goodness-of-fit testing can yield insight into the structure of fairness-related harms experienced by intersectio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#21512;&#25104;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#38450;&#25252;&#65292;&#29983;&#25104;&#19982;&#21387;&#21147;&#26102;&#21051;&#30456;&#20851;&#30340;&#21512;&#25104;&#24207;&#21015;&#25968;&#25454;&#65292;&#30830;&#20445;&#24739;&#32773;&#20449;&#24687;&#30340;&#20445;&#25252;&#65292;&#24182;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;&#22312;&#21387;&#21147;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13327</link><description>&lt;p&gt;
&#20026;&#38544;&#31169;&#20445;&#25252;&#21487;&#31359;&#25140;&#21387;&#21147;&#26816;&#27979;&#29983;&#25104;&#21512;&#25104;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection. (arXiv:2401.13327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#21512;&#25104;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#38450;&#25252;&#65292;&#29983;&#25104;&#19982;&#21387;&#21147;&#26102;&#21051;&#30456;&#20851;&#30340;&#21512;&#25104;&#24207;&#21015;&#25968;&#25454;&#65292;&#30830;&#20445;&#24739;&#32773;&#20449;&#24687;&#30340;&#20445;&#25252;&#65292;&#24182;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#36136;&#37327;&#35780;&#20272;&#12290;&#22312;&#21387;&#21147;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#25163;&#34920;&#30340;&#20581;&#24247;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#26234;&#33021;&#20581;&#24247;&#24212;&#29992;&#21644;&#24739;&#32773;&#30417;&#27979;&#20013;&#36234;&#26469;&#36234;&#34987;&#20351;&#29992;&#65292;&#21253;&#25324;&#21387;&#21147;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#21307;&#30103;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#25935;&#24863;&#30340;&#20010;&#20154;&#20449;&#24687;&#65292;&#24182;&#19988;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#20197;&#36827;&#34892;&#30740;&#31350;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20851;&#27880;&#38544;&#31169;&#30340;&#21512;&#25104;&#22810;&#20256;&#24863;&#22120;&#26234;&#33021;&#25163;&#34920;&#20581;&#24247;&#35835;&#25968;&#19982;&#21387;&#21147;&#26102;&#21051;&#30456;&#20851;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#29983;&#25104;&#21512;&#25104;&#24207;&#21015;&#25968;&#25454;&#65292;&#24182;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26045;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#38450;&#25252;&#20197;&#20445;&#25252;&#24739;&#32773;&#20449;&#24687;&#12290;&#20026;&#20102;&#30830;&#20445;&#21512;&#25104;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31995;&#21015;&#36136;&#37327;&#35780;&#20272;&#65292;&#24182;&#30417;&#27979;&#21512;&#25104;&#25968;&#25454;&#19982;&#21407;&#22987;&#25968;&#25454;&#20043;&#38388;&#30340;&#21512;&#29702;&#24615;&#12290;&#20026;&#20102;&#27979;&#35797;&#20854;&#26377;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#24120;&#29992;&#20294;&#35268;&#27169;&#36739;&#23567;&#30340;&#21387;&#21147;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#21019;&#24314;&#20102;&#31169;&#26377;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#22522;&#30784;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smartwatch health sensor data is increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprises sensitive personal information and is resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress. Our method involves the generation of synthetic sequence data through Generative Adversarial Networks (GANs), coupled with the implementation of Differential Privacy (DP) safeguards for protecting patient information during model training. To ensure the integrity of our synthetic data, we employ a range of quality assessments and monitor the plausibility between synthetic and original data. To test the usefulness, we create private machine learning models on a commonly used, albeit small, stress detection dataset, exploring strategies for enhancing the existing data foundat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#36817;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24320;&#22987;&#65292;&#25581;&#31034;&#20102;&#26631;&#31614;&#23545;&#26679;&#26412;&#20043;&#38388;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#24102;&#26631;&#31614;&#30340;&#20266;&#31070;&#32463;&#20999;&#21521;&#26680; (lpNTK)&#65292;&#24182;&#25506;&#35752;&#20102;lpNTK&#22914;&#20309;&#24110;&#21161;&#29702;&#35299;&#23398;&#20064;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2401.08808</link><description>&lt;p&gt;
&#23398;&#20064;&#21160;&#21147;&#23398;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#65306;&#20174;&#26679;&#26412;&#20851;&#31995;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
Sample Relationship from Learning Dynamics Matters for Generalisation. (arXiv:2401.08808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08808
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#36817;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24320;&#22987;&#65292;&#25581;&#31034;&#20102;&#26631;&#31614;&#23545;&#26679;&#26412;&#20043;&#38388;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#24102;&#26631;&#31614;&#30340;&#20266;&#31070;&#32463;&#20999;&#21521;&#26680; (lpNTK)&#65292;&#24182;&#25506;&#35752;&#20102;lpNTK&#22914;&#20309;&#24110;&#21161;&#29702;&#35299;&#23398;&#20064;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#65292;&#24050;&#32463;&#26377;&#24456;&#22810;&#20851;&#20110;&#25552;&#20986;&#26032;&#27169;&#22411;&#25110;&#25439;&#22833;&#20989;&#25968;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#35757;&#32451;&#25968;&#25454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#21364;&#20851;&#27880;&#36739;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#36817;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24320;&#22987;&#65292;&#21363;&#23398;&#20064;&#19968;&#20010;&#26679;&#26412;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#23545;&#20854;&#20182;&#26679;&#26412;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#20998;&#26512;&#30417;&#30563;&#23398;&#20064;&#20013;&#28041;&#21450;&#30340;&#26435;&#37325;&#26356;&#26032;&#39033;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#31614;&#20250;&#24433;&#21709;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26631;&#31614;&#30340;&#20266;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;lpNTK&#65289;&#65292;&#22312;&#27979;&#37327;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26102;&#32771;&#34385;&#26631;&#31614;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;lpNTK&#22312;Frobenius&#33539;&#25968;&#19979;&#28176;&#36817;&#25910;&#25947;&#20110;&#32463;&#39564;&#31070;&#32463;&#20999;&#21521;&#26680;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;lpNTK&#22914;&#20309;&#24110;&#21161;&#29702;&#35299;&#20808;&#21069;&#24037;&#20316;&#20013;&#21457;&#29616;&#30340;&#23398;&#20064;&#29616;&#35937;&#65292;&#29305;&#21035;&#26159;&#26679;&#26412;&#23398;&#20064;&#22256;&#38590;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although much research has been done on proposing new models or loss functions to improve the generalisation of artificial neural networks (ANNs), less attention has been directed to the impact of the training data on generalisation. In this work, we start from approximating the interaction between samples, i.e. how learning one sample would modify the model's prediction on other samples. Through analysing the terms involved in weight updates in supervised learning, we find that labels influence the interaction between samples. Therefore, we propose the labelled pseudo Neural Tangent Kernel (lpNTK) which takes label information into consideration when measuring the interactions between samples. We first prove that lpNTK asymptotically converges to the empirical neural tangent kernel in terms of the Frobenius norm under certain assumptions. Secondly, we illustrate how lpNTK helps to understand learning phenomena identified in previous work, specifically the learning difficulty of sample
&lt;/p&gt;</description></item><item><title>TAnet&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#30721;&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#33041;&#30005;&#20449;&#21495;&#20013;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#35299;&#30721;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05819</link><description>&lt;p&gt;
TAnet: &#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#30721;&#30340;&#26102;&#38388;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
TAnet: A New Temporal Attention Network for EEG-based Auditory Spatial Attention Decoding with a Short Decision Window. (arXiv:2401.05819v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05819
&lt;/p&gt;
&lt;p&gt;
TAnet&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#33041;&#30005;&#20449;&#21495;&#30340;&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#35299;&#30721;&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#33041;&#30005;&#20449;&#21495;&#20013;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#35299;&#30721;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#26816;&#27979;&#65288;ASAD&#65289;&#36890;&#36807;&#20998;&#26512;&#30005;&#33041;&#33041;&#30005;&#20449;&#21495;&#26469;&#30830;&#23450;&#21548;&#20247;&#23545;&#35828;&#35805;&#32773;&#30340;&#27880;&#24847;&#26041;&#21521;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25913;&#36827;ASAD&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#36739;&#30701;&#30340;&#20915;&#31574;&#31383;&#21475;&#65288;&#23567;&#20110;1&#31186;&#65289;&#65292;&#32780;&#19981;&#26159;&#20197;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#36739;&#38271;&#20915;&#31574;&#31383;&#21475;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26102;&#38388;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;&#21363;TAnet&#65289;&#12290;TAnet&#37319;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#37319;&#38598;&#21040;&#30340;&#33041;&#30005;&#20449;&#21495;&#20013;&#26102;&#38388;&#27493;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#20026;&#36825;&#20123;&#26102;&#38388;&#27493;&#20998;&#37197;&#30456;&#24212;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;ASAD&#26041;&#27861;&#30456;&#27604;&#65292;TAnet&#22312;KUL&#25968;&#25454;&#38598;&#20013;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#35299;&#30721;&#24615;&#33021;&#65292;&#20351;&#29992;&#36739;&#30701;&#30340;&#20915;&#31574;&#31383;&#21475;&#65288;&#21363;&#23567;&#20110;1&#31186;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#30721;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;92.4%&#65288;&#20915;&#31574;&#31383;&#21475;0.1&#31186;&#65289;&#12289;94.9%&#65288;0.25&#31186;&#65289;&#12289;95.1%&#65288;0.3&#31186;&#65289;&#12289;95.4%&#65288;0.4&#31186;&#65289;&#21644;95.5%&#65288;0.5&#31186;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auditory spatial attention detection (ASAD) is used to determine the direction of a listener's attention to a speaker by analyzing her/his electroencephalographic (EEG) signals. This study aimed to further improve the performance of ASAD with a short decision window (i.e., &lt;1 s) rather than with long decision windows in previous studies. An end-to-end temporal attention network (i.e., TAnet) was introduced in this work. TAnet employs a multi-head attention (MHA) mechanism, which can more effectively capture the interactions among time steps in collected EEG signals and efficiently assign corresponding weights to those EEG time steps. Experiments demonstrated that, compared with the CNN-based method and recent ASAD methods, TAnet provided improved decoding performance in the KUL dataset, with decoding accuracies of 92.4% (decision window 0.1 s), 94.9% (0.25 s), 95.1% (0.3 s), 95.4% (0.4 s), and 95.5% (0.5 s) with short decision windows (i.e., &lt;1 s). As a new ASAD model with a short deci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20272;&#35745;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#26631;&#20934;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29366;&#24577;&#23548;&#25968;&#32423;&#21035;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#65292;&#35299;&#20915;&#20102;&#38544;&#34255;&#29366;&#24577;&#12289;&#38544;&#34255;&#29366;&#24577;&#23548;&#25968;&#20197;&#21450;&#26102;&#38388;&#38388;&#38548;&#30340;&#26631;&#20934;&#21270;&#25361;&#25112;&#12290;&#36873;&#25321;&#36866;&#24403;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#19982;&#24453;&#35782;&#21035;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#30456;&#20851;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#33719;&#24471;&#26377;&#25928;&#26631;&#20934;&#21270;&#24120;&#25968;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02902</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29366;&#24577;&#23548;&#25968;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
State Derivative Normalization for Continuous-Time Deep Neural Networks. (arXiv:2401.02902v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20272;&#35745;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#26631;&#20934;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#29366;&#24577;&#23548;&#25968;&#32423;&#21035;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#65292;&#35299;&#20915;&#20102;&#38544;&#34255;&#29366;&#24577;&#12289;&#38544;&#34255;&#29366;&#24577;&#23548;&#25968;&#20197;&#21450;&#26102;&#38388;&#38388;&#38548;&#30340;&#26631;&#20934;&#21270;&#25361;&#25112;&#12290;&#36873;&#25321;&#36866;&#24403;&#30340;&#26631;&#20934;&#21270;&#24120;&#25968;&#19982;&#24453;&#35782;&#21035;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#30456;&#20851;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#31181;&#33719;&#24471;&#26377;&#25928;&#26631;&#20934;&#21270;&#24120;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#36866;&#24403;&#25968;&#25454;&#26631;&#20934;&#21270;&#30340;&#37325;&#35201;&#24615;&#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#36830;&#32493;&#26102;&#38388;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20272;&#35745;&#20013;&#65292;&#35266;&#23519;&#21040;&#27169;&#22411;&#20272;&#35745;&#30340;&#38544;&#34255;&#29366;&#24577;&#25110;&#38544;&#34255;&#29366;&#24577;&#23548;&#25968;&#65292;&#29978;&#33267;&#26102;&#38388;&#38388;&#38548;&#30340;&#19981;&#36866;&#24403;&#26631;&#20934;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#26102;&#30340;&#25968;&#20540;&#21644;&#20248;&#21270;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#27169;&#22411;&#36136;&#37327;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19977;&#20010;&#26631;&#20934;&#21270;&#20219;&#21153;&#30340;&#20869;&#22312;&#32806;&#21512;&#12290;&#30001;&#20110;&#23384;&#22312;&#36825;&#31181;&#32806;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29366;&#24577;&#23548;&#25968;&#27700;&#24179;&#24341;&#20837;&#26631;&#20934;&#21270;&#24120;&#25968;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36866;&#24403;&#36873;&#25321;&#26631;&#20934;&#21270;&#24120;&#25968;&#19982;&#24453;&#35782;&#21035;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#30456;&#20851;&#65292;&#24182;&#25512;&#23548;&#20102;&#22810;&#31181;&#33719;&#24471;&#26377;&#25928;&#26631;&#20934;&#21270;&#24120;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#23454;&#39564;&#25968;&#25454;&#30340;&#22522;&#20934;&#38382;&#39064;&#19978;&#27604;&#36739;&#21644;&#35752;&#35770;&#20102;&#25152;&#26377;&#26631;&#20934;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of proper data normalization for deep neural networks is well known. However, in continuous-time state-space model estimation, it has been observed that improper normalization of either the hidden state or hidden state derivative of the model estimate, or even of the time interval can lead to numerical and optimization challenges with deep learning based methods. This results in a reduced model quality. In this contribution, we show that these three normalization tasks are inherently coupled. Due to the existence of this coupling, we propose a solution to all three normalization challenges by introducing a normalization constant at the state derivative level. We show that the appropriate choice of the normalization constant is related to the dynamics of the to-be-identified system and we derive multiple methods of obtaining an effective normalization constant. We compare and discuss all the normalization strategies on a benchmark problem based on experimental data from a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#22270;&#27169;&#22411;FoToM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#36827;&#34892;&#22270;&#39044;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#23454;&#29616;&#20102;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.03976</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Graph Model. (arXiv:2311.03976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#22270;&#27169;&#22411;FoToM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#36827;&#34892;&#22270;&#39044;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#23454;&#29616;&#20102;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#22312;&#25968;&#25454;&#25110;&#26631;&#31614;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#65292;&#20445;&#25345;&#39044;&#35757;&#32451;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#19968;&#33268;&#12290;&#36825;&#20351;&#24471;&#26080;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#36827;&#34892;&#36801;&#31227;&#12290;&#33021;&#22815;&#22312;&#20219;&#24847;&#20219;&#21153;&#21644;&#39046;&#22495;&#19978;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#30340;&#27169;&#22411;&#23558;&#25104;&#20026;&#31532;&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#25552;&#20986;&#20102;FoToM&#65292;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#30340;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;FoToM&#22312;&#22810;&#20010;&#22270;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27491;&#21521;&#36801;&#31227;&#12290;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#65292;&#24615;&#33021;&#26368;&#24046;&#26102;&#19982;&#26377;&#30417;&#30563;&#22522;&#32447;&#30456;&#24403;&#65292;76%&#30340;&#25968;&#25454;&#38598;&#22312;95%&#32622;&#20449;&#24230;&#19979;&#37117;&#26174;&#33879;&#20248;&#20110;&#26377;&#30417;&#30563;&#22522;&#32447;&#65288;P&#8804;0.01&#65289;&#65292;&#35823;&#24046;&#20943;&#23569;&#20102;8%&#33267;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
The principal benefit of unsupervised graph representation learning is that a pre-trained model can be fine-tuned where data or labels are scarce. Existing approaches are domain specific, maintaining consistent node and edge attributes across the pre-training and target datasets. This precludes transfer to other domains. A model capable of positive transfer on arbitrary tasks and domains would represent the first foundation graph model.  In this work we use adversarial contrastive learning to present FoToM, a graph pre-training method based on node and edge feature exclusion. We use FoToM to pre-train models over multiple graph domains, producing the first foundation graph models. We demonstrate positive transfer on evaluation datasets from multiple domains, including domains not present in pre-training data. On all datasets performance is at worst on-par and on 76% significantly better than a supervised baseline ($P \leq 0.01$), with an 8 to 40% reduction in error at 95% confidence. C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#38024;&#23545;&#27969;&#24418;&#20540;&#25968;&#25454;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27969;&#24418;&#19978;&#25805;&#20316;&#27010;&#29575;&#20998;&#24067;&#21442;&#25968;&#26469;&#30452;&#25509;&#20272;&#35745;&#20989;&#25968;&#65292;&#20197;&#25913;&#21892;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#31616;&#21270;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.19561</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#22312;&#27969;&#24418;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Non-parametric regression for robot learning on manifolds. (arXiv:2310.19561v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#38024;&#23545;&#27969;&#24418;&#20540;&#25968;&#25454;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27969;&#24418;&#19978;&#25805;&#20316;&#27010;&#29575;&#20998;&#24067;&#21442;&#25968;&#26469;&#30452;&#25509;&#20272;&#35745;&#20989;&#25968;&#65292;&#20197;&#25913;&#21892;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#31616;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#20154;&#23398;&#20064;&#24037;&#20855;&#37117;&#26159;&#38024;&#23545;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#35774;&#35745;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#65292;&#35768;&#22810;&#24212;&#29992;&#28041;&#21450;&#21040;&#27969;&#24418;&#20540;&#25968;&#25454;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#20363;&#23376;&#26159;&#23039;&#24577;&#65307;&#23427;&#21487;&#20197;&#34920;&#31034;&#20026;3x3&#30340;&#26059;&#36716;&#30697;&#38453;&#25110;&#22235;&#20803;&#25968;&#65292;&#20854;&#31354;&#38388;&#26159;&#38750;&#27431;&#20960;&#37324;&#24471;&#27969;&#24418;&#12290;&#22312;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#65292;&#27969;&#24418;&#20540;&#25968;&#25454;&#36890;&#24120;&#36890;&#36807;&#23558;&#27969;&#24418;&#19982;&#21512;&#36866;&#30340;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#30456;&#20851;&#32852;&#26469;&#22788;&#29702;&#65292;&#21487;&#20197;&#36890;&#36807;&#23884;&#20837;&#27969;&#24418;&#25110;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#25110;&#22810;&#20010;&#20999;&#31354;&#38388;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#39044;&#27979;&#20934;&#30830;&#24615;&#19981;&#39640;&#21644;&#31639;&#27861;&#22797;&#26434;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27969;&#24418;&#20869;&#30452;&#25509;&#36827;&#34892;&#22238;&#24402;&#30340;&#8220;&#22266;&#26377;&#8221;&#26041;&#27861;&#12290;&#23427;&#28041;&#21450;&#23545;&#27969;&#24418;&#19978;&#30340;&#36866;&#24403;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#25805;&#20316;&#65292;&#23558;&#20854;&#21442;&#25968;&#20316;&#20026;&#39044;&#27979;&#21464;&#37327;&#65288;&#22914;&#26102;&#38388;&#65289;&#30340;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#8220;&#23616;&#37096;&#20284;&#28982;&#8221;&#26041;&#27861;&#26469;&#38750;&#21442;&#25968;&#20272;&#35745;&#35813;&#20989;&#25968;&#65292;&#20854;&#20013;&#21253;&#21547;&#26680;&#20989;&#25968;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#21629;&#21517;&#20026;&#26680;&#21270;&#20284;&#28982;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many of the tools available for robot learning were designed for Euclidean data. However, many applications in robotics involve manifold-valued data. A common example is orientation; this can be represented as a 3-by-3 rotation matrix or a quaternion, the spaces of which are non-Euclidean manifolds. In robot learning, manifold-valued data are often handled by relating the manifold to a suitable Euclidean space, either by embedding the manifold or by projecting the data onto one or several tangent spaces. These approaches can result in poor predictive accuracy, and convoluted algorithms. In this paper, we propose an "intrinsic" approach to regression that works directly within the manifold. It involves taking a suitable probability distribution on the manifold, letting its parameter be a function of a predictor variable, such as time, then estimating that function non-parametrically via a "local likelihood" method that incorporates a kernel. We name the method kernelised likelihood esti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#20809;&#35889;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#26435;&#37325;&#30697;&#38453;&#21644;&#26356;&#26032;&#30340;&#35889;&#33539;&#25968;&#32553;&#25918;&#20026;$\sqrt{\texttt{fan-out}/\texttt{fan-in}}$&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#29305;&#24449;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23548;&#20986;&#20102;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#30340;&#25512;&#23548;&#65292;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#29702;&#35299;&#26356;&#21152;&#28145;&#20837;&#12290;</title><link>http://arxiv.org/abs/2310.17813</link><description>&lt;p&gt;
&#19968;&#20010;&#29305;&#24449;&#23398;&#20064;&#30340;&#20809;&#35889;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
A Spectral Condition for Feature Learning. (arXiv:2310.17813v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17813
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#20809;&#35889;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#26435;&#37325;&#30697;&#38453;&#21644;&#26356;&#26032;&#30340;&#35889;&#33539;&#25968;&#32553;&#25918;&#20026;$\sqrt{\texttt{fan-out}/\texttt{fan-in}}$&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#29305;&#24449;&#23398;&#20064;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23548;&#20986;&#20102;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#30340;&#25512;&#23548;&#65292;&#26088;&#22312;&#24110;&#21161;&#35835;&#32773;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#23398;&#20064;&#29702;&#35299;&#26356;&#21152;&#28145;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35757;&#32451;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#21160;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#32593;&#32476;&#23485;&#24230;&#19978;&#30340;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23545;&#32593;&#32476;&#30340;&#20869;&#37096;&#34920;&#31034;&#36827;&#34892;&#38750;&#24179;&#20961;&#30340;&#28436;&#21464;&#65292;&#21363;&#29305;&#24449;&#23398;&#20064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32553;&#25918;&#26435;&#37325;&#30697;&#38453;&#21644;&#26356;&#26032;&#30340;&#35889;&#33539;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#29305;&#24449;&#23398;&#20064;&#65292;&#32553;&#25918;&#31995;&#25968;&#20026;$\sqrt{\texttt{fan-out}/\texttt{fan-in}}$&#65292;&#19982;&#22522;&#20110;Frobenius&#33539;&#25968;&#21644;&#20803;&#32032;&#22823;&#23567;&#30340;&#21551;&#21457;&#24335;&#32553;&#25918;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#20809;&#35889;&#32553;&#25918;&#20998;&#26512;&#36824;&#23548;&#20986;&#20102;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#30340;&#22522;&#26412;&#25512;&#23548;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#35835;&#32773;&#25552;&#20379;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#22362;&#23454;&#27010;&#24565;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The push to train ever larger neural networks has motivated the study of initialization and training at large network width. A key challenge is to scale training so that a network's internal representations evolve nontrivially at all widths, a process known as feature learning. Here, we show that feature learning is achieved by scaling the spectral norm of weight matrices and their updates like $\sqrt{\texttt{fan-out}/\texttt{fan-in}}$, in contrast to widely used but heuristic scalings based on Frobenius norm and entry size. Our spectral scaling analysis also leads to an elementary derivation of \emph{maximal update parametrization}. All in all, we aim to provide the reader with a solid conceptual understanding of feature learning in neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;F2GNN&#30340;&#26041;&#27861;&#65292;&#23427;&#26088;&#22312;&#22686;&#24378;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20943;&#36731;&#20559;&#35265;&#30340;&#26032;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.12350</link><description>&lt;p&gt;
&#20026;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#32467;&#26500;&#24863;&#30693;&#32676;&#20307;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Equipping Federated Graph Neural Networks with Structure-aware Group Fairness. (arXiv:2310.12350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;F2GNN&#30340;&#26041;&#27861;&#65292;&#23427;&#26088;&#22312;&#22686;&#24378;&#32852;&#37030;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20943;&#36731;&#20559;&#35265;&#30340;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#21508;&#31181;&#22270;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#20219;&#21153;&#12290;&#30001;&#20110;&#38544;&#31169;&#21644;&#30417;&#31649;&#38480;&#21046;&#65292;&#23545;&#38598;&#20013;&#24335;&#22270;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#36235;&#21183;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;GNN&#21487;&#33021;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#32487;&#25215;&#21382;&#21490;&#20559;&#35265;&#24182;&#23548;&#33268;&#27495;&#35270;&#24615;&#39044;&#27979;&#65292;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#65292;&#23616;&#37096;&#27169;&#22411;&#30340;&#20559;&#35265;&#24456;&#23481;&#26131;&#20256;&#25773;&#21040;&#20840;&#23616;&#27169;&#22411;&#65292;&#36825;&#32473;&#22312;&#32852;&#37030;GNN&#20013;&#20943;&#36731;&#20559;&#35265;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;F2GNN&#65292;&#19968;&#31181;&#22686;&#24378;&#32852;&#37030;GNN&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#20559;&#35265;&#21487;&#33021;&#26469;&#33258;&#25968;&#25454;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;F2GNN&#26088;&#22312;&#22312;&#32852;&#37030;&#29615;&#22659;&#19979;&#20943;&#23569;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have been widely used for various types of graph data processing and analytical tasks in different domains. Training GNNs over centralized graph data can be infeasible due to privacy concerns and regulatory restrictions. Thus, federated learning (FL) becomes a trending solution to address this challenge in a distributed learning paradigm. However, as GNNs may inherit historical bias from training data and lead to discriminatory predictions, the bias of local models can be easily propagated to the global model in distributed settings. This poses a new challenge in mitigating bias in federated GNNs. To address this challenge, we propose $\text{F}^2$GNN, a Fair Federated Graph Neural Network, that enhances group fairness of federated GNNs. As bias can be sourced from both data and learning algorithms, $\text{F}^2$GNN aims to mitigate both types of bias under federated settings. First, we provide theoretical insights on the connection between data bias in a tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#21305;&#37197;&#30340;&#26080;&#35889;&#22270;&#21387;&#32553;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#21457;&#29616;&#65292;GNN&#27880;&#20837;&#21512;&#25104;&#22270;&#20013;&#30340;&#35889;&#20559;&#24046;&#23548;&#33268;&#20102;&#24615;&#33021;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.09202</link><description>&lt;p&gt;
&#22270;&#23884;&#20837;&#19982;&#29305;&#24449;&#21305;&#37197;&#30340;&#22270;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Graph Condensation via Eigenbasis Matching. (arXiv:2310.09202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#21305;&#37197;&#30340;&#26080;&#35889;&#22270;&#21387;&#32553;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#21457;&#29616;&#65292;GNN&#27880;&#20837;&#21512;&#25104;&#22270;&#20013;&#30340;&#35889;&#20559;&#24046;&#23548;&#33268;&#20102;&#24615;&#33021;&#24046;&#24322;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#25968;&#25454;&#37327;&#30340;&#22686;&#21152;&#65292;&#35201;&#27714;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21508;&#31181;&#22270;&#30456;&#20851;&#24212;&#29992;&#20013;&#25552;&#39640;&#25928;&#29575;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#26368;&#36817;&#65292;&#26032;&#20852;&#30340;&#22270;&#21387;&#32553;&#65288;GC&#65289;&#20174;&#25968;&#25454;&#35282;&#24230;&#38477;&#20302;&#20102;GNN&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23427;&#26088;&#22312;&#29992;&#19968;&#20010;&#26126;&#26174;&#36739;&#23567;&#30340;&#21512;&#25104;&#22270;&#26367;&#20195;&#30495;&#23454;&#30340;&#22823;&#22411;&#22270;&#65292;&#20351;&#24471;&#22312;&#36825;&#20004;&#20010;&#22270;&#19978;&#35757;&#32451;&#30340;GNN&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;GC&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#21363;&#22312;&#21516;&#19968;&#20010;&#21512;&#25104;&#22270;&#19978;&#35757;&#32451;&#30340;&#19981;&#21516;GNN&#24615;&#33021;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;&#26159;&#20160;&#20040;&#22240;&#32032;&#38459;&#30861;&#20102;GC&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21457;&#29616;GNN&#20250;&#23558;&#35889;&#20559;&#24046;&#27880;&#20837;&#21512;&#25104;&#22270;&#20013;&#65292;&#23548;&#33268;&#20998;&#24067;&#20559;&#31227;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#29305;&#24449;&#21305;&#37197;&#30340;&#26080;&#35889;&#22270;&#21387;&#32553;&#65292;&#31216;&#20043;&#20026;...
&lt;/p&gt;
&lt;p&gt;
The increasing amount of graph data places requirements on the efficiency and scalability of graph neural networks (GNNs), despite their effectiveness in various graph-related applications. Recently, the emerging graph condensation (GC) sheds light on reducing the computational cost of GNNs from a data perspective. It aims to replace the real large graph with a significantly smaller synthetic graph so that GNNs trained on both graphs exhibit comparable performance. However, our empirical investigation reveals that existing GC methods suffer from poor generalization, i.e., different GNNs trained on the same synthetic graph have obvious performance gaps. What factors hinder the generalization of GC and how can we mitigate it? To answer this question, we commence with a detailed analysis and observe that GNNs will inject spectrum bias into the synthetic graph, resulting in a distribution shift. To tackle this issue, we propose eigenbasis matching for spectrum-free graph condensation, name
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.03722</link><description>&lt;p&gt;
&#26410;&#30693;&#26041;&#24046;&#19979;&#30340;&#39640;&#26031;&#22343;&#20540;&#30340;&#20219;&#24847;&#26377;&#25928;T&#26816;&#39564;&#21644;&#32622;&#20449;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance. (arXiv:2310.03722v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#26041;&#27861;&#65292;&#20998;&#21035;&#36890;&#36807;&#26367;&#25442;Lai&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1976&#24180;&#65292;Lai&#26500;&#36896;&#20102;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#22343;&#20540;$\mu$&#30340;&#39640;&#26031;&#20998;&#24067;&#30340;&#32622;&#20449;&#24207;&#21015;&#65292;&#35813;&#20998;&#24067;&#30340;&#26041;&#24046;$\sigma$&#26159;&#26410;&#30693;&#30340;&#12290;&#20182;&#20351;&#29992;&#20102;&#20851;&#20110;$\sigma$&#30340;&#19981;&#36866;&#24403;&#65288;&#21491;Haar&#65289;&#28151;&#21512;&#21644;&#20851;&#20110;$\mu$&#30340;&#19981;&#36866;&#24403;&#65288;&#24179;&#22374;&#65289;&#28151;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#20182;&#26500;&#24314;&#30340;&#32454;&#33410;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#24191;&#20041;&#30340;&#19981;&#21487;&#31215;&#20998;&#38789;&#21644;&#25193;&#23637;&#30340;&#32500;&#23572;&#19981;&#31561;&#24335;&#12290;&#23613;&#31649;&#36825;&#30830;&#23454;&#20135;&#29983;&#20102;&#19968;&#20010;&#39034;&#24207;T&#26816;&#39564;&#65292;&#20294;&#30001;&#20110;&#20182;&#30340;&#38789;&#19981;&#21487;&#31215;&#20998;&#65292;&#23427;&#24182;&#27809;&#26377;&#20135;&#29983;&#19968;&#20010;&#8220;e-process&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#30456;&#21516;&#30340;&#35774;&#32622;&#24320;&#21457;&#20102;&#20004;&#20010;&#26032;&#30340;&#8220;e-process&#8221;&#21644;&#32622;&#20449;&#24207;&#21015;&#65306;&#19968;&#20010;&#26159;&#22312;&#32553;&#20943;&#28388;&#27874;&#22120;&#20013;&#30340;&#27979;&#35797;&#38789;&#65292;&#21478;&#19968;&#20010;&#26159;&#22312;&#35268;&#33539;&#25968;&#25454;&#28388;&#27874;&#22120;&#20013;&#30340;&#8220;e-process&#8221;&#12290;&#36825;&#20123;&#20998;&#21035;&#26159;&#36890;&#36807;&#23558;Lai&#30340;&#24179;&#22374;&#28151;&#21512;&#26367;&#25442;&#20026;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#23558;&#23545;$\sigma$&#30340;&#21491;Haar&#28151;&#21512;&#26367;&#25442;&#20026;&#22312;&#38646;&#31354;&#38388;&#19979;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#23601;&#20687;&#22312;&#36890;&#29992;&#25512;&#26029;&#20013;&#19968;&#26679;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#25152;&#24471;&#32467;&#26524;&#30340;&#23485;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$ of a Gaussian distribution with unknown variance $\sigma$. Curiously, he employed both an improper (right Haar) mixture over $\sigma$ and an improper (flat) mixture over $\mu$. Here, we elaborate carefully on the details of his construction, which use generalized nonintegrable martingales and an extended Ville's inequality. While this does yield a sequential t-test, it does not yield an ``e-process'' (due to the nonintegrability of his martingale). In this paper, we develop two new e-processes and confidence sequences for the same setting: one is a test martingale in a reduced filtration, while the other is an e-process in the canonical data filtration. These are respectively obtained by swapping Lai's flat mixture for a Gaussian mixture, and swapping the right Haar mixture over $\sigma$ with the maximum likelihood estimate under the null, as done in universal inference. We also analyze the width of resulting 
&lt;/p&gt;</description></item><item><title>RealFill&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#20462;&#22635;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#22330;&#26223;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#33021;&#22815;&#20197;&#30495;&#23454;&#12289;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#20869;&#23481;&#23436;&#25104;&#30446;&#26631;&#22270;&#20687;&#30340;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.16668</link><description>&lt;p&gt;
RealFill&#65306;&#21442;&#32771;&#39537;&#21160;&#30340;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RealFill: Reference-Driven Generation for Authentic Image Completion. (arXiv:2309.16668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16668
&lt;/p&gt;
&lt;p&gt;
RealFill&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#20462;&#22635;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#22330;&#26223;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#33021;&#22815;&#20197;&#30495;&#23454;&#12289;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#20869;&#23481;&#23436;&#25104;&#30446;&#26631;&#22270;&#20687;&#30340;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#22270;&#20687;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#33021;&#22815;&#22312;&#26410;&#30693;&#21306;&#22495;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#22270;&#20687;&#20869;&#23481;&#30340;&#22806;&#25299;&#21644;&#20462;&#22635;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#20869;&#23481;&#26159;&#19981;&#30495;&#23454;&#30340;&#65292;&#22240;&#20026;&#27169;&#22411;&#32570;&#20047;&#20851;&#20110;&#30495;&#23454;&#22330;&#26223;&#30340;&#36275;&#22815;&#32972;&#26223;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#29983;&#25104;&#26041;&#27861;RealFill&#65292;&#23427;&#36890;&#36807;&#22635;&#20805;&#22270;&#20687;&#20013;&#32570;&#22833;&#21306;&#22495;&#20351;&#20854;&#20869;&#23481;&#30495;&#27491;&#24212;&#22312;&#30340;&#20869;&#23481;&#12290;RealFill&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#20462;&#22635;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#20960;&#24352;&#30446;&#26631;&#22330;&#26223;&#30340;&#21442;&#32771;&#22270;&#20687;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#20123;&#21442;&#32771;&#22270;&#20687;&#19981;&#38656;&#35201;&#19982;&#30446;&#26631;&#22270;&#20687;&#23545;&#40784;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#35270;&#35282;&#12289;&#20809;&#29031;&#26465;&#20214;&#12289;&#25668;&#20687;&#26426;&#20809;&#22280;&#25110;&#22270;&#20687;&#39118;&#26684;&#25293;&#25668;&#12290;&#20010;&#24615;&#21270;&#21518;&#65292;RealFill&#33021;&#22815;&#20197;&#35270;&#35273;&#19978;&#24341;&#20154;&#27880;&#30446;&#30340;&#20869;&#23481;&#23436;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#19988;&#24544;&#23454;&#20110;&#21407;&#22987;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20840;&#38754;&#19988;&#20855;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#20462;&#22797;&#22522;&#20934;&#19978;&#23545;RealFill&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions, but the content these models hallucinate is necessarily inauthentic, since the models lack sufficient context about the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.05950</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#65292;&#37319;&#29992;&#32842;&#22825;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;VLMs &#30340;&#24494;&#35843;&#26041;&#27861;&#20027;&#35201;&#22312;&#30333;&#30418;&#29615;&#22659;&#20013;&#25805;&#20316;&#65292;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810; VLMs &#20381;&#36182;&#20110;&#19987;&#26377;&#25968;&#25454;&#19988;&#19981;&#24320;&#28304;&#65292;&#38480;&#21046;&#20102;&#20351;&#29992;&#30333;&#30418;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#12290;&#37492;&#20110;&#20687; ChatGPT &#36825;&#26679;&#30340;&#21463;&#27426;&#36814;&#31169;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20173;&#28982;&#25552;&#20379;&#22522;&#20110;&#35821;&#35328;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340; VLMs &#24494;&#35843;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#35775;&#38382;&#27169;&#22411;&#21442;&#25968;&#12289;&#29305;&#24449;&#23884;&#20837;&#25110;&#36755;&#20986; logits &#30340;&#38656;&#35201;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#32842;&#22825;&#30340; LLMs &#20316;&#20026;&#40657;&#30418;&#20248;&#21270;&#22120;&#65292;&#20197;&#22312;&#20351;&#29992; CLIP &#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#31034;&#20363;&#20219;&#21153;&#20013;&#23547;&#25214;&#26368;&#20339;&#25991;&#26412;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;"&#29228;&#23665;"&#31243;&#24207;&#65292;&#23427;&#33021;&#25910;&#25947;&#21040;&#26377;&#25928;&#30340;&#25552;&#31034;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DECODE&#30340;&#25193;&#24352;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#26497;&#31471;&#36136;&#37327;&#27604;&#28608;&#21457;&#30340;&#20449;&#21495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#39057;&#22495;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#26102;&#38388;&#24310;&#36831;&#24178;&#28041;&#20202;&#20197;&#22788;&#29702;&#22810;&#36890;&#36947;TDI&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.16422</link><description>&lt;p&gt;
DECODE: &#29992;&#20110;&#26816;&#27979;&#26497;&#31471;&#36136;&#37327;&#27604;&#28608;&#21457;&#30340;&#25193;&#24352;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DECODE: DilatEd COnvolutional neural network for Detecting Extreme-mass-ratio inspirals. (arXiv:2308.16422v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16422
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DECODE&#30340;&#25193;&#24352;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#26497;&#31471;&#36136;&#37327;&#27604;&#28608;&#21457;&#30340;&#20449;&#21495;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#39057;&#22495;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#26102;&#38388;&#24310;&#36831;&#24178;&#28041;&#20202;&#20197;&#22788;&#29702;&#22810;&#36890;&#36947;TDI&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#27874;&#24418;&#12289;&#25345;&#20037;&#30340;&#25345;&#32493;&#26102;&#38388;&#21644;&#20302;&#20449;&#22122;&#27604;&#65292;&#26497;&#31471;&#36136;&#37327;&#27604;&#28608;&#21457;(EMRI)&#30340;&#26816;&#27979;&#26159;&#22797;&#26434;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#19982;&#32039;&#20945;&#30340;&#20108;&#36827;&#21046;&#34701;&#21512;&#30456;&#27604;&#26356;&#38590;&#34987;&#35782;&#21035;&#12290;&#34429;&#28982;&#22522;&#20110;&#21305;&#37197;&#28388;&#27874;&#30340;&#25216;&#26415;&#20197;&#20854;&#35745;&#31639;&#35201;&#27714;&#32780;&#38395;&#21517;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#20027;&#35201;&#22788;&#29702;&#26102;&#22495;&#25968;&#25454;&#65292;&#24182;&#19988;&#36890;&#24120;&#21463;&#21040;&#25968;&#25454;&#25345;&#32493;&#26102;&#38388;&#21644;&#20449;&#22122;&#27604;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#24573;&#30053;&#20102;&#26102;&#38388;&#24310;&#36831;&#24178;&#28041;&#20202;(TDI)&#24182;&#22312;&#25506;&#27979;&#22120;&#21709;&#24212;&#35745;&#31639;&#20013;&#24212;&#29992;&#20102;&#38271;&#27874;&#36817;&#20284;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#22788;&#29702;&#28608;&#20809;&#39057;&#29575;&#22122;&#22768;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DECODE&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#39057;&#22495;&#24207;&#21015;&#24314;&#27169;&#20026;&#37325;&#28857;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;EMRI&#20449;&#21495;&#26816;&#27979;&#12290;DECODE&#22260;&#32469;&#30528;&#19968;&#20010;&#20197;&#25193;&#24352;&#22240;&#26524;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20026;&#20013;&#24515;&#65292;&#20351;&#29992;&#32771;&#34385;&#21040;TDI-1.5&#25506;&#27979;&#22120;&#21709;&#24212;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#19968;&#24180;&#30340;&#22810;&#36890;&#36947;TDI&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of Extreme Mass Ratio Inspirals (EMRIs) is intricate due to their complex waveforms, extended duration, and low signal-to-noise ratio (SNR), making them more challenging to be identified compared to compact binary coalescences. While matched filtering-based techniques are known for their computational demands, existing deep learning-based methods primarily handle time-domain data and are often constrained by data duration and SNR. In addition, most existing work ignores time-delay interferometry (TDI) and applies the long-wavelength approximation in detector response calculations, thus limiting their ability to handle laser frequency noise. In this study, we introduce DECODE, an end-to-end model focusing on EMRI signal detection by sequence modeling in the frequency domain. Centered around a dilated causal convolutional neural network, trained on synthetic data considering TDI-1.5 detector response, DECODE can efficiently process a year's worth of multichannel TDI data wi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#65292;&#22312;El Ni\~no Southern Oscillation&#65288;ENSO&#65289;&#20013;&#21457;&#29616;&#20102;&#26032;&#30340;&#26497;&#31471;El Ni\~no&#31867;&#21035;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#19982;&#20856;&#22411;EP El Ni\~no&#19981;&#21516;&#12290;EP El Ni\~nos&#65292;CP La Ni\~nas&#21644;Extreme El Ni\~nos&#23545;&#36328;&#21313;&#24180;&#23610;&#24230;&#30340;ENSO&#26368;&#20855;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11552</link><description>&lt;p&gt;
El Ni\~no Southern Oscillation&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
A multi-modal representation of El Ni\~no Southern Oscillation Diversity. (arXiv:2307.11552v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11552
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20302;&#32500;&#34920;&#31034;&#65292;&#22312;El Ni\~no Southern Oscillation&#65288;ENSO&#65289;&#20013;&#21457;&#29616;&#20102;&#26032;&#30340;&#26497;&#31471;El Ni\~no&#31867;&#21035;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#19982;&#20856;&#22411;EP El Ni\~no&#19981;&#21516;&#12290;EP El Ni\~nos&#65292;CP La Ni\~nas&#21644;Extreme El Ni\~nos&#23545;&#36328;&#21313;&#24180;&#23610;&#24230;&#30340;ENSO&#26368;&#20855;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
El Ni\~no Southern Oscillation (ENSO)&#36890;&#36807;&#36196;&#36947;&#22826;&#24179;&#27915;&#28201;&#26262;&#65288;El Ni\~no&#65289;&#21644;&#23506;&#20919;&#65288;La Ni\~na&#65289;&#28023;&#34920;&#28201;&#24230;&#24322;&#24120;&#65288;SSTA&#65289;&#30340;&#20132;&#26367;&#38454;&#27573;&#26469;&#25551;&#36848;&#12290;&#23613;&#31649;El Ni\~no&#21644;La Ni\~na&#26159;&#26126;&#30830;&#23450;&#20041;&#30340;&#27668;&#20505;&#27169;&#24335;&#65292;&#20294;&#27809;&#26377;&#20004;&#20010;&#20107;&#20214;&#26159;&#30456;&#21516;&#30340;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;ENSO&#22810;&#26679;&#24615;&#20027;&#35201;&#20197;SSTA&#23792;&#20540;&#30340;&#32463;&#24230;&#20301;&#32622;&#26469;&#25551;&#36848;&#65292;&#29992;&#20110;&#22312;&#19996;&#22826;&#24179;&#27915;&#65288;EP&#65289;&#21644;&#20013;&#22826;&#24179;&#27915;&#65288;CP&#65289;&#31867;&#22411;&#20013;&#23450;&#20041;&#21452;&#23792;&#20998;&#31867;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#22826;&#24179;&#27915;SSTA&#30340;&#20302;&#32500;&#34920;&#31034;&#26469;&#35777;&#26126;&#20108;&#36827;&#21046;&#20998;&#31867;&#25104;&#21592;&#23545;&#25551;&#36848;ENSO&#20107;&#20214;&#19981;&#21512;&#36866;&#12290;&#36890;&#36807;&#27169;&#31946;&#26080;&#30417;&#30563;&#32858;&#31867;&#65292;&#25105;&#20204;&#24674;&#22797;&#20102;&#22235;&#20010;&#24050;&#30693;&#30340;ENSO&#31867;&#21035;&#65292;&#20197;&#21450;&#31532;&#20116;&#20010;&#31867;&#21035;&#65306;&#26497;&#31471;El Ni\~no&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26497;&#31471;El Ni\~nos&#22312;&#20854;&#24378;&#24230;&#21644;&#26102;&#38388;&#28436;&#21270;&#26041;&#38754;&#19982;&#20856;&#22411;&#30340;EP El Ni\~nos&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;CP La Ni\~nas&#65292;EP El Ni\~nos&#21644;Extreme El Ni\~nos&#23545;&#36328;&#21313;&#24180;&#23610;&#24230;&#30340;ENSO&#26368;&#20855;&#36129;&#29486;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The El Ni\~no-Southern Oscillation (ENSO) is characterized by alternating periods of warm (El Ni\~no) and cold (La Ni\~na) sea surface temperature anomalies (SSTA) in the equatorial Pacific. Although El Ni\~no and La Ni\~na are well-defined climate patterns, no two events are alike. To date, ENSO diversity has been described primarily in terms of the longitudinal location of peak SSTA, used to define a bimodal classification of events in Eastern Pacific (EP) and Central Pacific (CP) types. Here, we use low-dimensional representations of Pacific SSTAs to argue that binary categorical memberships are unsuitable to describe ENSO events. Using fuzzy unsupervised clustering, we recover the four known ENSO categories, along with a fifth category: an Extreme El Ni\~no. We show that Extreme El Ni\~nos differ both in their intensity and temporal evolution from canonical EP El Ni\~nos. We also find that CP La Ni\~nas, EP El Ni\~nos, and Extreme El Ni\~nos contribute the most to interdecadal ENSO
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#30149;&#20154;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;</title><link>http://arxiv.org/abs/2307.11465</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#25972;&#20307;&#29983;&#23384;&#20998;&#26512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach for Overall Survival Analysis with Missing Values. (arXiv:2307.11465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11465
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#39044;&#27979;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#30149;&#20154;&#30340;&#25972;&#20307;&#29983;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#24212;&#29992;&#20110;&#32954;&#30284;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#38750;&#23567;&#32454;&#32990;&#32954;&#30284;&#65288;NSCLC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#12290;&#23545;&#20110;&#30149;&#20154;&#29366;&#24577;&#30340;&#25972;&#20307;&#29983;&#23384;&#65288;OS&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#25351;&#26631;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#29983;&#23384;&#27010;&#29575;&#19981;&#21516;&#30340;&#20122;&#32452;&#65292;&#20174;&#32780;&#23454;&#29616;&#20010;&#20307;&#21270;&#27835;&#30103;&#21644;&#25913;&#21892;&#25972;&#20307;&#29983;&#23384;&#29575;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#65292;&#38656;&#35201;&#32771;&#34385;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#27599;&#20010;&#30149;&#20154;&#30340;&#21487;&#29992;&#20449;&#24687;&#65292;&#21033;&#29992;&#26410;&#34987;&#23457;&#26597;&#30340;&#65288;&#21363;&#27515;&#20129;&#65289;&#21644;&#34987;&#23457;&#26597;&#30340;&#65288;&#21363;&#24184;&#23384;&#32773;&#65289;&#30149;&#20154;&#30340;&#20449;&#24687;&#65292;&#20063;&#35201;&#32771;&#34385;&#21040;&#27515;&#20129;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;&#19981;&#23436;&#25972;&#25968;&#25454;&#22788;&#29702;&#26159;&#21307;&#23398;&#39046;&#22495;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#25554;&#34917;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#20010;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#34987;&#23457;&#26597;&#21644;&#26410;&#34987;&#23457;&#26597;&#30340;&#30149;&#20154;&#21450;&#20854;&#21487;&#29992;&#29305;&#24449;&#20013;&#26377;&#25928;&#23398;&#20064;&#65292;&#39044;&#27979;NSCLC&#30149;&#20154;&#30340;OS&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most challenging fields where Artificial Intelligence (AI) can be applied is lung cancer research, specifically non-small cell lung cancer (NSCLC). In particular, overall survival (OS) is a vital indicator of patient status, helping to identify subgroups with diverse survival probabilities, enabling tailored treatment and improved OS rates. In this analysis, there are two challenges to take into account. First, few studies effectively exploit the information available from each patient, leveraging both uncensored (i.e., dead) and censored (i.e., survivors) patients, considering also the death times. Second, the handling of incomplete data is a common issue in the medical field. This problem is typically tackled through the use of imputation methods. Our objective is to present an AI model able to overcome these limits, effectively learning from both censored and uncensored patients and their available features, for the prediction of OS for NSCLC patients. We present a novel 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.10053</link><description>&lt;p&gt;
&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Convergence Guarantees for Stochastic Subgradient Methods in Nonsmooth Nonconvex Optimization. (arXiv:2307.10053v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#20013;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;&#20102;&#22810;&#31181;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#12290;&#23545;&#20110;&#26377;&#38480;&#21644;&#24418;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#25214;&#21040;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#21450;&#20854;&#21464;&#31181;&#22312;&#35757;&#32451;&#30001;&#38750;&#24179;&#28369;&#28608;&#27963;&#20989;&#25968;&#26500;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;&#26356;&#26032;&#21160;&#37327;&#39033;&#21644;&#21464;&#37327;&#30340;&#27493;&#38271;&#20998;&#37197;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#12290;&#22312;&#19968;&#20123;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21333;&#26102;&#38388;&#23610;&#24230;&#21644;&#21452;&#26102;&#38388;&#23610;&#24230;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#21547;&#20102;&#24456;&#22810;&#24050;&#30693;&#30340;SGD&#31867;&#22411;&#26041;&#27861;&#65292;&#21253;&#25324;heavy-ball SGD&#12289;SignSGD&#12289;Lion&#12289;normalized SGD&#21644;clipped SGD&#12290;&#27492;&#22806;&#65292;&#24403;&#30446;&#26631;&#20989;&#25968;&#37319;&#29992;&#26377;&#38480;&#21644;&#24418;&#24335;&#26102;&#65292;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;SGD&#31867;&#22411;&#26041;&#27861;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#27493;&#38271;&#21644;&#21021;&#22987;&#28857;&#19978;&#33021;&#22815;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;Clarke&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the convergence properties of the stochastic gradient descent (SGD) method and its variants, especially in training neural networks built from nonsmooth activation functions. We develop a novel framework that assigns different timescales to stepsizes for updating the momentum terms and variables, respectively. Under mild conditions, we prove the global convergence of our proposed framework in both single-timescale and two-timescale cases. We show that our proposed framework encompasses a wide range of well-known SGD-type methods, including heavy-ball SGD, SignSGD, Lion, normalized SGD and clipped SGD. Furthermore, when the objective function adopts a finite-sum formulation, we prove the convergence properties for these SGD-type methods based on our proposed framework. In particular, we prove that these SGD-type methods find the Clarke stationary points of the objective function with randomly chosen stepsizes and initial points under mild assumptions. Preli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#38543;&#26426;Polyak&#27493;&#38271;&#26041;&#27861;&#25193;&#23637;&#21040;&#32852;&#37030;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#21644;&#20960;&#20046;&#26080;&#38656;&#35843;&#21442;&#30340;FedSPS&#21644;FedDecSPS&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#25554;&#20540;&#26465;&#20214;&#28385;&#36275;&#26102;&#65292;FedSPS&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#35299;&#30340;&#37051;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.06306</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;Polyak&#27493;&#38271;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes. (arXiv:2307.06306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#38543;&#26426;Polyak&#27493;&#38271;&#26041;&#27861;&#25193;&#23637;&#21040;&#32852;&#37030;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#21644;&#20960;&#20046;&#26080;&#38656;&#35843;&#21442;&#30340;FedSPS&#21644;FedDecSPS&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#25554;&#20540;&#26465;&#20214;&#28385;&#36275;&#26102;&#65292;FedSPS&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#35299;&#30340;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;FedAvg&#65292;&#38656;&#35201;&#31934;&#24515;&#35843;&#25972;&#30340;&#27493;&#38271;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#29616;&#26377;&#33258;&#36866;&#24212;&#32852;&#37030;&#26041;&#27861;&#25552;&#20986;&#30340;&#25913;&#36827;&#20165;&#28041;&#21450;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22914;&#21160;&#37327;&#21442;&#25968;&#65292;&#24182;&#19988;&#20165;&#32771;&#34385;&#22312;&#26381;&#21153;&#22120;&#32858;&#21512;&#36718;&#27425;&#20013;&#30340;&#36866;&#24212;&#24615;&#65292;&#32780;&#19981;&#26159;&#23616;&#37096;&#30340;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#19979;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36807;&#22810;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#19981;&#33021;&#25429;&#25417;&#23616;&#37096;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#23558;&#26368;&#36817;&#25552;&#20986;&#30340;&#38543;&#26426;Polyak&#27493;&#38271;&#26041;&#27861;&#25193;&#23637;&#21040;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#23616;&#37096;&#33258;&#36866;&#24212;&#21644;&#20960;&#20046;&#26080;&#38656;&#35843;&#21442;&#30340;&#20998;&#24067;&#24335;SPS&#21464;&#20307;&#65288;FedSPS&#21644;FedDecSPS&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#24403;&#25554;&#20540;&#26465;&#20214;&#65288;&#36807;&#21442;&#25968;&#21270;&#65289;&#28385;&#36275;&#26102;&#65292;FedSPS&#22312;&#24378;&#20984;&#21644;&#20984;&#35774;&#32622;&#20013;&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#25910;&#25947;&#21040;&#35299;&#30340;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art federated learning algorithms such as FedAvg require carefully tuned stepsizes to achieve their best performance. The improvements proposed by existing adaptive federated methods involve tuning of additional hyperparameters such as momentum parameters, and consider adaptivity only in the server aggregation round, but not locally. These methods can be inefficient in many practical scenarios because they require excessive tuning of hyperparameters and do not capture local geometric information. In this work, we extend the recently proposed stochastic Polyak stepsize (SPS) to the federated learning setting, and propose new locally adaptive and nearly parameter-free distributed SPS variants (FedSPS and FedDecSPS). We prove that FedSPS converges linearly in strongly convex and sublinearly in convex settings when the interpolation condition (overparametrization) is satisfied, and converges to a neighborhood of the solution in the general case. We extend our proposed method t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DistroFair&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#26816;&#27979;&#21040;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;</title><link>http://arxiv.org/abs/2305.13935</link><description>&lt;p&gt;
&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Distribution-aware Fairness Test Generation. (arXiv:2305.13935v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DistroFair&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#26816;&#27979;&#21040;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#39564;&#35777;&#22270;&#20687;&#35782;&#21035;&#36719;&#20214;&#20013;&#30340;&#32452;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#30340;&#20844;&#24179;&#24615;&#27979;&#35797;&#26041;&#27861;&#65288;&#31216;&#20026;DistroFair&#65289;&#65292;&#36890;&#36807;&#23558;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23545;&#35937;&#24341;&#20837;&#21040;&#22270;&#20687;&#35782;&#21035;&#22120;&#20013;&#65292;&#36890;&#36807;&#19977;&#31181;&#35821;&#20041;&#20445;&#30041;&#22270;&#20687;&#21464;&#25442; - &#23545;&#35937;&#21024;&#38500;&#65292;&#23545;&#35937;&#25554;&#20837;&#21644;&#23545;&#35937;&#26059;&#36716;&#26469;&#31995;&#32479;&#24615;&#22320;&#26292;&#38706;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#30340;&#31867;&#32423;&#21035;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#65288;CityScapes&#21644;MS-COCO&#65289;&#21644;&#19977;&#20010;&#20027;&#35201;&#30340;&#21830;&#19994;&#22270;&#20687;&#35782;&#21035;&#36719;&#20214;&#65288;&#21363;Amazon Rekognition&#65292;Google Cloud Vision&#21644;Azure&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#23545;DistroFair&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;DistroFair&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#65292;&#32422;&#26377;21&#65285;&#36890;&#36807;&#30495;&#23454;&#26631;&#20934;&#25110;&#20803;&#27979;&#35797;&#26631;&#20934;&#26174;&#38706;&#20986;&#20102;&#31867;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#36829;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses how to validate group fairness in image recognition software. We propose a distribution-aware fairness testing approach (called DistroFair) that systematically exposes class-level fairness violations in image classifiers via a synergistic combination of out-of-distribution (OOD) testing and semantic-preserving image mutation. DistroFair automatically learns the distribution (e.g., number/orientation) of objects in a set of images. Then it systematically mutates objects in the images to become OOD using three semantic-preserving image mutations -- object deletion, object insertion and object rotation. We evaluate DistroFair using two well-known datasets (CityScapes and MS-COCO) and three major, commercial image recognition software (namely, Amazon Rekognition, Google Cloud Vision and Azure Computer Vision). Results show that about 21% of images generated by DistroFair reveal class-level fairness violations using either ground truth or metamorphic oracles. DistroFair 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#26368;&#23567;&#21270;&#36890;&#20449;&#28040;&#32791;&#30340;&#19977;&#32500;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#26234;&#33021;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12289;&#26032;&#39062;&#36229;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#36890;&#20449;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;GPU&#31354;&#38386;&#26102;&#38388;&#38477;&#20302;&#20102;50&#65285;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2305.13525</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#36890;&#20449;&#30340;&#24322;&#27493;&#24352;&#37327;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Communication-minimizing Asynchronous Tensor Parallelism. (arXiv:2305.13525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#26368;&#23567;&#21270;&#36890;&#20449;&#28040;&#32791;&#30340;&#19977;&#32500;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#26234;&#33021;&#20998;&#24067;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12289;&#26032;&#39062;&#36229;&#20998;&#35299;&#26041;&#27861;&#20197;&#21450;&#36890;&#20449;&#27169;&#22411;&#65292;&#20351;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;GPU&#31354;&#38386;&#26102;&#38388;&#38477;&#20302;&#20102;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#25193;&#22823;&#21040;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65292;&#35774;&#35745;&#33021;&#22815;&#22312;&#22810;GPU&#38598;&#32676;&#19978;&#39640;&#25928;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#30340;&#24182;&#34892;&#31639;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Tensor3D&#65292;&#19968;&#31181;&#20840;&#26032;&#30340;&#19977;&#32500;&#65288;3D&#65289;&#24352;&#37327;&#35745;&#31639;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#22823;&#22411;&#22810;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24182;&#34892;&#35757;&#32451;&#20013;&#30001;&#36890;&#20449;&#24341;&#36215;&#30340;&#31354;&#38386;&#26102;&#38388;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26234;&#33021;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#20998;&#24067;&#26041;&#24335;&#65292;&#28040;&#38500;&#20102;&#20026;&#28385;&#36275;&#21508;&#23618;&#25968;&#25454;&#20381;&#36182;&#32780;&#38656;&#35201;&#30340;&#36890;&#20449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#35757;&#32451;&#36807;&#31243;&#36229;&#20998;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36890;&#20449;&#19982;&#35745;&#31639;&#30340;&#37325;&#21472;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#31354;&#38386;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#27169;&#22411;&#65292;&#24110;&#21161;&#29992;&#25143;&#20026;&#32473;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#36890;&#20449;&#26368;&#20248;&#30340;&#21487;&#29992;&#30828;&#20214;&#36164;&#28304;&#20998;&#35299;&#12290; &#23545;&#20110;256 A100 GPU&#19978;&#30340;28B&#21442;&#25968;CNN&#65292;&#22312;&#26412;&#25991;&#30340; Tensor3D &#26041;&#27861;&#19979;&#65292;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;3&#20493;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604; GPU &#31354;&#38386;&#26102;&#38388;&#20063;&#38477;&#20302;&#20102;&#32422;50&#65285;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
As state-of-the-art neural networks scale to billions of parameters, designing parallel algorithms that can train these networks efficiently on multi-GPU clusters has become critical. This paper presents Tensor3D, a novel three-dimensional (3D) approach to parallelize tensor computations, that strives to minimize the idle time incurred due to communication in parallel training of large multi-billion parameter models. First, we introduce an intelligent distribution of neural network parameters across GPUs that eliminates communication required for satisfying data dependencies of individual layers. Then, we propose a novel overdecomposition of the parallel training process, using which we achieve significant overlap of communication with computation, thereby reducing GPU idle time. Finally, we present a communication model, which helps users identify communication optimal decompositions of available hardware resources for a given neural network. For a 28B parameter CNN on 256 A100 GPUs, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05215</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20808;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#39044;&#35757;&#32451;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21450;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#26368;&#36817;&#65292;&#36965;&#24863;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#22522;&#30784;&#27169;&#22411;&#22312;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#21442;&#25968;&#65288;&#21253;&#25324;86M&#12289;605.26M&#12289;1.3B&#21644;2.4B&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#30830;&#23450;&#21442;&#25968;&#22686;&#21152;&#26159;&#21542;&#20250;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10&#20159;&#20010;&#36965;&#24863;&#22270;&#20687;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21521;&#30740;&#31350;&#31038;&#21306;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#19988;&#28789;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#12290;&#27492;&#26694;&#26550;&#21487;&#29992;&#20110;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#24050;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12797</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#31639;&#27861;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters. (arXiv:2303.12797v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#19988;&#28789;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#12290;&#27492;&#26694;&#26550;&#21487;&#29992;&#20110;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#24050;&#26377;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#20248;&#21270;&#30456;&#20851;&#30340;&#36229;&#21442;&#25968;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#36827;&#21270;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG)&#65292;&#23450;&#20041;&#20102;&#27604;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;&#25628;&#32034;&#31354;&#38388;&#26356;&#20026;&#28789;&#27963;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#20801;&#35768;&#28151;&#21512;&#20351;&#29992;&#20256;&#32479;&#25805;&#20316;&#65292;&#22914;&#21367;&#31215;&#12289;&#24490;&#29615;&#21644;&#23494;&#38598;&#23618;&#65292;&#20197;&#21450;&#36739;&#20026;&#26032;&#39062;&#30340;&#25805;&#20316;&#65292;&#22914;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22522;&#20110;&#35813;&#25628;&#32034;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#37051;&#22495;&#25628;&#32034;&#31639;&#23376;&#21644;&#28436;&#21270;&#25628;&#32034;&#31639;&#23376;&#65292;&#20197;&#20248;&#21270;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#12290;&#36825;&#20123;&#25628;&#32034;&#31639;&#23376;&#21487;&#19982;&#20219;&#20309;&#33021;&#22815;&#22788;&#29702;&#28151;&#21512;&#25628;&#32034;&#31354;&#38388;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#36827;&#21270;&#31639;&#27861;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#25214;&#21040;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an evolutionary algorithm on a time series prediction benchmark. The results demonstrate that our framework was able to find models outperforming the established baseline on numerous datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#19968;&#31181;&#21333;&#27425;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20027;&#35201;&#32858;&#28966;&#20110;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#22914;&#20309;&#24433;&#21709;&#25972;&#20010;&#24207;&#21015;&#30456;&#20284;&#24615;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#23376;&#24207;&#21015;&#30456;&#20284;&#30340;&#24207;&#21015;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2301.08403</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#29983;&#25104;&#24207;&#21015;&#65306;&#29702;&#35770;&#21450;&#20854;&#22312;&#26080;&#20154;&#26426;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Sequence Generation via Subsequence Similarity: Theory and Application to UAV Identification. (arXiv:2301.08403v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#19968;&#31181;&#21333;&#27425;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#65292;&#20027;&#35201;&#32858;&#28966;&#20110;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#22914;&#20309;&#24433;&#21709;&#25972;&#20010;&#24207;&#21015;&#30456;&#20284;&#24615;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#23376;&#24207;&#21015;&#30456;&#20284;&#30340;&#24207;&#21015;&#26469;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#21512;&#25104;&#24207;&#21015;&#30340;&#33021;&#21147;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21644;&#29983;&#25104;&#26694;&#26550;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#36825;&#19968;&#36807;&#31243;&#12290;&#26412;&#25991;&#20351;&#29992;&#19968;&#31181;&#21333;&#27425;&#29983;&#25104;&#27169;&#22411;&#26469;&#37319;&#26679;&#65292;&#36890;&#36807;&#30456;&#20284;&#24615;&#29983;&#25104;&#23376;&#24207;&#21015;&#65292;&#24182;&#35777;&#26126;&#20102;&#23376;&#24207;&#21015;&#30456;&#20284;&#24615;&#23545;&#25972;&#20010;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#65292;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#19968;&#27425;&#24615;&#29983;&#25104;&#27169;&#22411;&#26469;&#20174;&#21333;&#20010;&#24207;&#21015;&#30340;&#33539;&#22260;&#20869;&#21462;&#26679;&#65292;&#24182;&#29983;&#25104;&#23376;&#24207;&#21015;&#30456;&#20284;&#30340;&#24207;&#21015;&#65292;&#35777;&#26126;&#20102;&#25968;&#25454;&#38598;&#22686;&#24378;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generate synthetic sequences is crucial for a wide range of applications, and recent advances in deep learning architectures and generative frameworks have greatly facilitated this process. Particularly, unconditional one-shot generative models constitute an attractive line of research that focuses on capturing the internal information of a single image or video to generate samples with similar contents. Since many of those one-shot models are shifting toward efficient non-deep and non-adversarial approaches, we examine the versatility of a one-shot generative model for augmenting whole datasets. In this work, we focus on how similarity at the subsequence level affects similarity at the sequence level, and derive bounds on the optimal transport of real and generated sequences based on that of corresponding subsequences. We use a one-shot generative model to sample from the vicinity of individual sequences and generate subsequence-similar ones and demonstrate the improvem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#36882;&#24402;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#26356;&#39640;&#25928;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#26080;&#24179;&#28369;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#22797;&#26434;&#24230;&#20026; $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$&#12290;</title><link>http://arxiv.org/abs/2301.06428</link><description>&lt;p&gt;
&#26080;&#24179;&#28369;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#30340;&#26356;&#24555;&#26080;&#26799;&#24230;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic Optimization. (arXiv:2301.06428v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#36882;&#24402;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#26356;&#39640;&#25928;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#26080;&#24179;&#28369;&#38750;&#20984;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#22797;&#26434;&#24230;&#20026; $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#24418;&#22914; $\min_{x \in \mathbb{R}^d} f(x) \triangleq \mathbb{E}_{\xi} [F(x; \xi)]$ &#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#20998;&#37327; $F(x;\xi)$ &#26159; $L$ &#24179;&#22343;&#22343;&#26041;&#20559;&#24046;&#30340; Lipschitz &#20294;&#21487;&#33021;&#26159;&#38750;&#20984;&#38750;&#20809;&#28369;&#20989;&#25968;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#26080;&#26799;&#24230;&#26041;&#27861;&#26368;&#22810;&#38656;&#35201; $\mathcal{O}( L^4 d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2} \delta^{-1} \epsilon^{-4})$ &#30340;&#38543;&#26426;&#38646;&#38454;&#39044;&#22788;&#29702;&#22120;&#22797;&#26434;&#24230;&#26469;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340; $(\delta,\epsilon)$-Goldstein &#38745;&#27490;&#28857;&#65292;&#20854;&#20013; $\Delta = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$&#65292;$x_0$ &#26159;&#31639;&#27861;&#30340;&#21021;&#22987;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#36882;&#24402;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#23558;&#22797;&#26434;&#24230;&#25913;&#36827;&#20026; $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the optimization problem of the form $\min_{x \in \mathbb{R}^d} f(x) \triangleq \mathbb{E}_{\xi} [F(x; \xi)]$, where the component $F(x;\xi)$ is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth. The recently proposed gradient-free method requires at most $\mathcal{O}( L^4 d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2} \delta^{-1} \epsilon^{-4})$ stochastic zeroth-order oracle complexity to find a $(\delta,\epsilon)$-Goldstein stationary point of objective function, where $\Delta = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$ and $x_0$ is the initial point of the algorithm. This paper proposes a more efficient algorithm using stochastic recursive gradient estimators, which improves the complexity to $\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1} \epsilon^{-3})$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21435;&#20013;&#24515;&#21270;&#36882;&#24402;&#26799;&#24230;&#19978;&#21319;&#27861;&#65288;DREAM&#65289;&#30340;&#31616;&#21333;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21435;&#20013;&#24515;&#21270;&#38750;&#20984;&#26497;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#23547;&#25214;&#21407;&#20989;&#25968;&#30340; $\epsilon$-&#31283;&#23450;&#28857;&#30340;&#26368;&#20339;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2212.02387</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#38750;&#20984;&#26497;&#23567;&#21270;&#38382;&#39064;&#38543;&#26426;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization. (arXiv:2212.02387v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21435;&#20013;&#24515;&#21270;&#36882;&#24402;&#26799;&#24230;&#19978;&#21319;&#27861;&#65288;DREAM&#65289;&#30340;&#31616;&#21333;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21435;&#20013;&#24515;&#21270;&#38750;&#20984;&#26497;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#23547;&#25214;&#21407;&#20989;&#25968;&#30340; $\epsilon$-&#31283;&#23450;&#28857;&#30340;&#26368;&#20339;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#20013;&#24515;&#21270;&#38750;&#20984;&#26497;&#23567;&#21270;&#38382;&#39064;&#30340;&#38543;&#26426;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#21435;&#20013;&#24515;&#21270;&#36882;&#24402;&#26799;&#24230;&#19978;&#21319;&#27861;&#65288;\texttt{DREAM}&#65289;&#65292;&#23427;&#23454;&#29616;&#20102;&#23547;&#25214;&#21407;&#20989;&#25968;&#30340;$\epsilon$-&#31283;&#23450;&#28857;&#30340;&#26368;&#20339;&#24050;&#30693;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#22312;&#32447;&#35774;&#32622;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#38656;&#35201;$\mathcal{O}(\kappa^3\epsilon^{-3})$&#38543;&#26426;&#19968;&#38454;&#39044;&#35328;&#26426;&#65288;SFO&#65289;&#35843;&#29992;&#20197;&#21450;$\mathcal{O}\big(\kappa^2\epsilon^{-2}/\sqrt{1-\lambda_2(W)}\,\big)$&#36890;&#20449;&#36718;&#27425;&#26469;&#25214;&#21040;$\epsilon$-&#31283;&#23450;&#28857;&#65292;&#20854;&#20013;$\kappa$&#26159;&#26465;&#20214;&#25968;&#65292;$\lambda_2(W)$&#26159;&#20843;&#21350;&#30697;&#38453;$W$&#30340;&#27425;&#22823;&#29305;&#24449;&#20540;&#12290;&#23545;&#20110;&#23436;&#20840;&#30001;$N$&#20010;&#20998;&#37327;&#20989;&#25968;&#32452;&#25104;&#30340;&#31163;&#32447;&#35774;&#32622;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#38656;&#35201;$\mathcal{O}\big(\kappa^2 \sqrt{N} \epsilon^{-2}\big)$ SFO &#35843;&#29992;&#21644;&#19982;&#22312;&#32447;&#35774;&#32622;&#30456;&#21516;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the stochastic optimization for decentralized nonconvex-strongly-concave minimax problem. We propose a simple and efficient algorithm, called Decentralized Recursive-gradient descEnt Ascent Method (\texttt{DREAM}), which achieves the best-known theoretical guarantee for finding the $\epsilon$-stationary point of the primal function. For the online setting, the proposed method requires $\mathcal{O}(\kappa^3\epsilon^{-3})$ stochastic first-order oracle (SFO) calls and $\mathcal{O}\big(\kappa^2\epsilon^{-2}/\sqrt{1-\lambda_2(W)}\,\big)$ communication rounds to find an $\epsilon$-stationary point, where $\kappa$ is the condition number and $\lambda_2(W)$ is the second-largest eigenvalue of the gossip matrix~$W$. For the offline setting with totally $N$ component functions, the proposed method requires $\mathcal{O}\big(\kappa^2 \sqrt{N} \epsilon^{-2}\big)$ SFO calls and the same communication complexity as the online setting.
&lt;/p&gt;</description></item></channel></rss>